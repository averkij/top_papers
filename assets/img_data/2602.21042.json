[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Datasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21042/x1.png",
                "caption": "Figure 2:OmniOCR.A represents the processing procedure of the model’s Vision Encoder; B represents the processing procedure of the model’s Text Encoder; C respresents two distinct parameter-efficient fine-tuning methods: Dynamic-Rank Training and Fixed-Rank Training.",
                "position": 194
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21042/x2.png",
                "caption": "Figure 3:Demonstration of Recognition Performance for Tibetan Handwritten Digits via OmniOCR. The results show the accuracy and visual recognition effect of OmniOCR on the Tibetan handwritten digit dataset.",
                "position": 210
            },
            {
                "img": "https://arxiv.org/html/2602.21042/x3.png",
                "caption": "Figure 4:The performance of OmniOCR evaluated on four representative datasets—TibetanMNIST, Shui, Ancient Yi, and Dongba—highlighting its ability to generalize across heterogeneous scripts and writing systems.",
                "position": 302
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Limitation and Future Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]