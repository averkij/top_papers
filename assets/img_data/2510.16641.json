[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16641/x1.png",
                "caption": "",
                "position": 110
            },
            {
                "img": "https://arxiv.org/html/2510.16641/x2.png",
                "caption": "",
                "position": 139
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16641/x3.png",
                "caption": "Figure 2:Examples of multi-turn conversations from MMDU[47]and ConvBench[45], along with their limitations. The graph below compares MMDU, ConvBench, andMultiVersein terms of query lexical diversity, the number of code/math tasks in responses, and the total number of tasks.",
                "position": 154
            },
            {
                "img": "https://arxiv.org/html/2510.16641/x4.png",
                "caption": "Figure 3:An overview ofMultiVerseconstruction pipeline, consisting of five steps: (1) Source Image Collection, gathering images from various benchmarks; (2) Personal Background Generation, creating user personas with specific goals; (3) Multi-Turn Conversation Generation, where an AI assistant analyzes model performance based on evaluation tables; (4) Manual Reviewing, assessing response correctness and relevance; and (5) Checklist Generation, ensuring quality and coherence in AI responses.",
                "position": 157
            },
            {
                "img": "https://arxiv.org/html/2510.16641/x5.png",
                "caption": "Figure 4:Detailed distribution of (a) Task, (b) Interaction Goal, and (c) Image Domain inMultiVerse.",
                "position": 168
            }
        ]
    },
    {
        "header": "2MultiVerse",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16641/x6.png",
                "caption": "Figure 5:Correlation between the checklist completion ratio and quality assessment. The red line represents the best-fit linear regression, indicating a strong positive correlation(R2=0.44)(R^{2}=0.44)between the two metrics.",
                "position": 325
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16641/x7.png",
                "caption": "Figure 6:Average performance of 18 VLMs under two different evaluation settings:OracleandSelf-Prediction.",
                "position": 530
            },
            {
                "img": "https://arxiv.org/html/2510.16641/x8.png",
                "caption": "Figure 7:Comparison performance across 19 different VLMs betweenOracleandSelf-Predictionsettings.",
                "position": 533
            },
            {
                "img": "https://arxiv.org/html/2510.16641/x9.png",
                "caption": "Figure 8:Performance comparison across different model sizes in three model families: InternVL2.5, LLaMA-3.2-Vision, and Qwen2.5-VL. The radar charts show that larger models generally perform better across tasks, but scaling effects vary. Notably, Qwen2.5-VL-72B excels in structured reasoning tasks (e.g., mathematics, coding), while Qwen2.5-VL-7B shows stronger creative abilities, highlighting task-dependent scaling impacts.",
                "position": 546
            },
            {
                "img": "https://arxiv.org/html/2510.16641/graphics/verification.png",
                "caption": "Table 3:Performance of 18 VLMs inMultiVersefor 8 different interaction goals—verification (), analysis (), exploration (), optimization (), calculation (), understanding (), research (), and creation ()—underOraclesetting.",
                "position": 561
            },
            {
                "img": "https://arxiv.org/html/2510.16641/graphics/analysis.png",
                "caption": "",
                "position": 568
            },
            {
                "img": "https://arxiv.org/html/2510.16641/graphics/exploration.png",
                "caption": "",
                "position": 569
            },
            {
                "img": "https://arxiv.org/html/2510.16641/graphics/optimization.png",
                "caption": "",
                "position": 570
            },
            {
                "img": "https://arxiv.org/html/2510.16641/graphics/calculation.png",
                "caption": "",
                "position": 571
            },
            {
                "img": "https://arxiv.org/html/2510.16641/graphics/understanding.png",
                "caption": "",
                "position": 572
            },
            {
                "img": "https://arxiv.org/html/2510.16641/graphics/research.png",
                "caption": "",
                "position": 573
            },
            {
                "img": "https://arxiv.org/html/2510.16641/graphics/creation.png",
                "caption": "",
                "position": 574
            },
            {
                "img": "https://arxiv.org/html/2510.16641/x10.png",
                "caption": "Figure 9:Linear correlation (R2R^{2}) between response length and performance from GPT-4o.",
                "position": 759
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix Contents",
        "images": []
    },
    {
        "header": "Appendix AAdditional Related Works",
        "images": []
    },
    {
        "header": "Appendix BAdditional Details ofMultiVerse",
        "images": []
    },
    {
        "header": "Appendix CFurther Analysis ofMultiVerse",
        "images": []
    },
    {
        "header": "Appendix DMore examples onMultiVerse",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16641/x11.png",
                "caption": "Figure 10:An example ofMultiVerse.",
                "position": 2029
            },
            {
                "img": "https://arxiv.org/html/2510.16641/x12.png",
                "caption": "Figure 11:An example ofMultiVerse.",
                "position": 2032
            },
            {
                "img": "https://arxiv.org/html/2510.16641/x13.png",
                "caption": "Figure 12:An example ofMultiVerse.",
                "position": 2035
            },
            {
                "img": "https://arxiv.org/html/2510.16641/x14.png",
                "caption": "Figure 13:An example ofMultiVerse.",
                "position": 2038
            }
        ]
    },
    {
        "header": "Appendix EPrompt Templates used forMultiVerse",
        "images": []
    }
]