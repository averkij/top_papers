[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16034/x1.png",
                "caption": "Figure 1:VisualLensleverages a user’s task-agnostic visual history to provide personalized recommendations. Our method outperforms GPT-4o by 1.6%∼similar-to\\sim∼4.6% on Hit@3.",
                "position": 216
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16034/x2.png",
                "caption": "Figure 2:VisualLensinference pipeline: the offline process augments images in the visual history with captions and aspect words; the runtime recommendation process retrieves relevant images, generate user profile accordingly, and then predict candidate preferences.",
                "position": 284
            }
        ]
    },
    {
        "header": "3Problem Definition",
        "images": []
    },
    {
        "header": "4Recommendation Method",
        "images": []
    },
    {
        "header": "5Iterative Refinement and Joint Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16034/x3.png",
                "caption": "Table 1:Dataset statistics of Google Review-Vision (GR-V) and Yelp-Vision (Yelp-V).",
                "position": 499
            },
            {
                "img": "https://arxiv.org/html/2411.16034/x3.png",
                "caption": "Table 2:Hit rates and MRRs ofVisualLensvs.multiple baselines on Google Review-V and Yelp-V. The result shows (a)VisualLensoutperforms other baselines, though has a gap with the human oracle; (b) model size greatly affects the performance; (c) simply rank by rating is a worse design than the random baseline.",
                "position": 513
            }
        ]
    },
    {
        "header": "6Benchmarks and Experiments Setups",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16034/x3.png",
                "caption": "Table 3:Ablation study on PaliGemma. Different components ofVisualLensmodel: joint training (Joint), iterative refinement (Iter), aspect words (Asp.), captions (Cap.), image embedding (Img.), and relevant image retrieval (Ret.). MRR difference over 0.4 has a p-value<<<0.04.",
                "position": 643
            },
            {
                "img": "https://arxiv.org/html/2411.16034/x3.png",
                "caption": "(a)",
                "position": 670
            },
            {
                "img": "https://arxiv.org/html/2411.16034/x3.png",
                "caption": "(a)",
                "position": 673
            },
            {
                "img": "https://arxiv.org/html/2411.16034/x4.png",
                "caption": "(b)",
                "position": 678
            },
            {
                "img": "https://arxiv.org/html/2411.16034/x5.png",
                "caption": "Table 4:Transferability: MRR ofVisualLensmodels to different test setups.\nLongHis: train until a certain timestamp and test afterwards.\nCategory: held-out a set of categories for testing per user.\nUse ID: train and test set share no common user ID.",
                "position": 689
            },
            {
                "img": "https://arxiv.org/html/2411.16034/x5.png",
                "caption": "(a)",
                "position": 709
            },
            {
                "img": "https://arxiv.org/html/2411.16034/x5.png",
                "caption": "(a)",
                "position": 712
            },
            {
                "img": "https://arxiv.org/html/2411.16034/x6.png",
                "caption": "(b)",
                "position": 717
            }
        ]
    },
    {
        "header": "7Results and Analysis",
        "images": []
    },
    {
        "header": "8Conclusion and Discussion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "ABenchmark Creation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16034/x7.png",
                "caption": "Figure 5:The Google Review-Vision (Google Review-V) training data consists of 66 categories.",
                "position": 1854
            }
        ]
    },
    {
        "header": "BMore Related Works",
        "images": []
    },
    {
        "header": "CPrompt Template",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16034/x8.png",
                "caption": "Figure 6:The prompt template for aspect word generation.",
                "position": 1916
            },
            {
                "img": "https://arxiv.org/html/2411.16034/x9.png",
                "caption": "Figure 7:The prompt template for candidate matching.",
                "position": 1919
            }
        ]
    },
    {
        "header": "DAdditional Implementation Details",
        "images": []
    }
]