[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07615/x1.png",
                "caption": "",
                "position": 115
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3VLM-R1 Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07615/x2.png",
                "caption": "Figure 2:Flow chart of VLM-R1 framework. This chart exhibits the function transformation of the framework. The key features of VLM-R1 are displyed by thegreenrectangle.",
                "position": 290
            },
            {
                "img": "https://arxiv.org/html/2504.07615/x3.png",
                "caption": "Figure 3:The interaction between Trainer and VLM Module. With the VLM Module, the GRPOTrainer can interact with different VLMs by simply invoking the standardized interfaces without the need to handle model-specific implementations.",
                "position": 308
            }
        ]
    },
    {
        "header": "4Reward Design",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07615/extracted/6350671/images/data2.png",
                "caption": "Table 2:Performance comparison of SFT and RL on in-domain and out-of-domain evaluation datasets. All results are from Qwen2.5VL-3B-Instruct trained on the training split of Refcoco/+/g. Step 0 represents the results from Qwen2.5VL-3B-Instruct itself.ŒîR‚Å¢L‚àíS‚Å¢F‚Å¢TsubscriptŒîùëÖùêøùëÜùêπùëá\\Delta_{RL-SFT}roman_Œî start_POSTSUBSCRIPT italic_R italic_L - italic_S italic_F italic_T end_POSTSUBSCRIPTdenotes the improved value of the RL model compared to the SFT model.",
                "position": 449
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07615/extracted/6350671/images/data2.png",
                "caption": "Figure 4:Difference between in-domain and out-of-domain dataset for REC task. In-domain data only describes the spatial or appearance attribute information for the object, while out-of-domain data require the model to use the open-world knowledge to recognize the role ofsoccer goalkeeperand then locate it.",
                "position": 644
            },
            {
                "img": "https://arxiv.org/html/2504.07615/x4.png",
                "caption": "Figure 5:Performance comparison between the SFT and RL models. The RL model shows significantly better generalization on the out-of-domain evaluation dataset compared to the SFT model.",
                "position": 647
            },
            {
                "img": "https://arxiv.org/html/2504.07615/extracted/6350671/images/length.jpg",
                "caption": "Table 4:Results of OVD task on OVDEval. Base denotes the Qwen2.5VL-3B-Instruct, and Base 7B denotes the 7B model.ŒîR‚Å¢L‚àíS‚Å¢F‚Å¢TsubscriptŒîùëÖùêøùëÜùêπùëá\\Delta_{RL-SFT}roman_Œî start_POSTSUBSCRIPT italic_R italic_L - italic_S italic_F italic_T end_POSTSUBSCRIPTdenotes the improved value of the RL model compared to the SFT model. We also list the performance of OmDet, the current state-of-the-art in specialized open-vocabulary detection, for the comprehensive comparison.",
                "position": 716
            },
            {
                "img": "https://arxiv.org/html/2504.07615/extracted/6350671/images/length.jpg",
                "caption": "Figure 6:Visualization of the completion length across different reward settings on OVD task. It is observed that the model always generates the overlong completion with the nativeAPreward, indicating the redundant predicted objects.",
                "position": 1006
            },
            {
                "img": "https://arxiv.org/html/2504.07615/x5.png",
                "caption": "Figure 7:Comparison of cases with and without the proposedodLengthreward.Left: WithoutodLength, the model generates redundant and duplicated boxes, yet still receives a high reward from nativemAP. Each circle denotes a predicted bounding box, and circles of the same color indicate bounding boxes with identical coordinates.Right: WithodLength, the model exhibits an ‚ÄúOD aha moment‚Äù, first reasoning about object presence before producing accurate bounding boxes.",
                "position": 1009
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]