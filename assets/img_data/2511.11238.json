[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11238/x1.png",
                "caption": "Figure 1:Results from large-scale experiments on a 3.3B-activation MoE using Virtual Width Networks (VWN). We compare the baselineMoE-A3.3BagainstMoE-A3.3B-VWNx8, configured with a virtual width factor ofr=8r{=}8.Left and middle:training loss for next-token and next-two-token prediction versus seen tokens. VWN reaches the same loss as the baseline using2.5√ó2.5\\timesand3.5√ó3.5\\timesfewer tokens, respectively.Right:average accuracy on a collection of open-source benchmarks (see Table3), where scores are aggregated using internally defined task weights. A difference of one point corresponds to a notable performance gap under this weighting scheme.",
                "position": 94
            },
            {
                "img": "https://arxiv.org/html/2511.11238/x2.png",
                "caption": "",
                "position": 103
            },
            {
                "img": "https://arxiv.org/html/2511.11238/x3.png",
                "caption": "",
                "position": 108
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11238/x4.png",
                "caption": "Figure 2:Standard Transformer vs. Virtual Width Network (VWN).(a) A standard Transformer uses the same width for embeddings and backbone.\n(b) Naive width scaling expands both proportionally, causing quadratic growth in parameters and compute.\n(c)VWNdecouples embedding width from backbone width. With Generalized Hyper‚ÄëConnections, over‚Äëwidth embeddings (e.g., 1.5√ó\\times) are coupled to a standard‚Äëwidth backbone, increasing representational capacity with minimal compute overhead.",
                "position": 117
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11238/x5.png",
                "caption": "Figure 3:Overview of Virtual Width Networks (VWN).(a) The standard Transformer maintains a consistent width across input embeddings, intermediate hidden vectors at each layer, and final layer outputs.\n(b) VWN scales the embedding dimension throughover-width embeddingswhile maintaining the layer dimension using lightweightGeneralized Hyper-Connections (GHC). These dimensions interact flexibly through small matricesùêÄl\\mathbf{A}^{l}andùêÅl\\mathbf{B}^{l}(llstands for the layer number).\n(c) We enable multiple token supervision (multi-token prediction), allowing for richer token representations.",
                "position": 196
            }
        ]
    },
    {
        "header": "4A Connectivity Perspective",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11238/x6.png",
                "caption": "Figure 4:Performance of VWN and MTP on 0.4B/4B MoE models.Left:Training loss versus seen tokens (billions). VWN lowers the next-token prediction loss, whereas MTP slightly hurts the NTP loss; combining VWN and MTP (VWN+MTP) yields the lowest final loss among the augmented variants but still shows a small gap ( 0.016) relative to the baseline metric when MTP is included.Right:Average downstream accuracy (%) versus tokens. Both VWN and MTP improve downstream accuracy over the baseline, and their combination delivers the largest gains throughout training. Models: MoE-0.4B/4B (baseline), MoE-0.4B/4B-VWN, MoE-0.4B/4B-MTP, and MoE-0.4B/4B-VWN-MTP.",
                "position": 569
            },
            {
                "img": "https://arxiv.org/html/2511.11238/x7.png",
                "caption": "",
                "position": 578
            },
            {
                "img": "https://arxiv.org/html/2511.11238/x8.png",
                "caption": "Figure 5:Performance of VWN and MTP on 2.5B/25B MoE models.Left:Training loss versus seen tokens (billions). VWN reduces the next‚Äëtoken prediction loss relative to the baseline, and adding MTP on top of VWN does not hurt the loss at this scale, with VWN+MTP reaching the lowest final loss, with a gap of 0.015 versus the baseline at the end of training.Right:Average downstream accuracy (%) versus tokens. Both VWN and VWN+MTP outperform the baseline, and VWN+MTP delivers the highest accuracy throughout training. Models: MoE-2.5B/25B (baseline), MoE-2.5B/25B-VWN, and MoE-2.5B/25B-VWN-MTP.",
                "position": 599
            },
            {
                "img": "https://arxiv.org/html/2511.11238/x9.png",
                "caption": "",
                "position": 608
            },
            {
                "img": "https://arxiv.org/html/2511.11238/x10.png",
                "caption": "Figure 6:Ablation on the fraction ratemmunder different virtual‚Äëwidth factorsrronMoE‚ÄëA0.8B. Each panel plots next‚Äëtoken training loss versus seen tokens (billions) for VWN√ó2 (left), VWN√ó4 (middle), and VWN√ó8 (right). Atr=2r{=}2, increasingmmfrom 2 to 4 produces a modest but visible improvement. Whenr=4r{=}4orr=8r{=}8, varyingmmbetween tested values leads to only minor differences, suggesting that beyondm‚âà4m{\\approx}4the effect of finer hidden partitioning largely saturates at this model scale.",
                "position": 639
            },
            {
                "img": "https://arxiv.org/html/2511.11238/x11.png",
                "caption": "",
                "position": 648
            },
            {
                "img": "https://arxiv.org/html/2511.11238/x12.png",
                "caption": "",
                "position": 653
            },
            {
                "img": "https://arxiv.org/html/2511.11238/x13.png",
                "caption": "Figure 7:Token efficiency of VWN onMoE-A0.8Bwith a fixed fraction ratem=8m=8.\nWe vary the virtual width factor by settingr‚àà{2,4,8}r\\in\\{2,4,8\\}andn=r‚ãÖm={16,32,64}n=r\\cdot m=\\{16,32,64\\}.\nLeft/middle: training loss for next-token and next-2-token prediction versus seen tokens.\nRight: average accuracy onCollection B3versus tokens.\nVWN consistently improves sample efficiency; at 500B tokens, VWN√ó\\times8 yieldsŒî=0.035\\Delta=0.035(next-token loss),Œî=0.058\\Delta=0.058(next-2 loss), and a+4.16+4.16-point accuracy gain (Collection B, Table3) over the non-VWN baseline, by leveraging over-width embeddings and GHC without increasing the backbone width.",
                "position": 665
            },
            {
                "img": "https://arxiv.org/html/2511.11238/x14.png",
                "caption": "",
                "position": 674
            },
            {
                "img": "https://arxiv.org/html/2511.11238/x15.png",
                "caption": "",
                "position": 679
            },
            {
                "img": "https://arxiv.org/html/2511.11238/x16.png",
                "caption": "Figure 8:Scaling law analysis of the relationship between virtual width factorrrand loss. The observed data (red points) are fitted with a log-linear functiony=‚àí0.0069‚ãÖlog2‚Å°(x)+1.6212y=-0.0069\\cdot\\log_{2}(x)+1.6212, with a coefficient of determinationR2=0.9986R^{2}=0.9986.",
                "position": 704
            },
            {
                "img": "https://arxiv.org/html/2511.11238/x16.png",
                "caption": "Figure 8:Scaling law analysis of the relationship between virtual width factorrrand loss. The observed data (red points) are fitted with a log-linear functiony=‚àí0.0069‚ãÖlog2‚Å°(x)+1.6212y=-0.0069\\cdot\\log_{2}(x)+1.6212, with a coefficient of determinationR2=0.9986R^{2}=0.9986.",
                "position": 706
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Contribution",
        "images": []
    },
    {
        "header": "8Detailed Downstream Results forMoE-A0.8BModels",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11238/x17.png",
                "caption": "Figure 9:Performance of VWN on MoE-A0.8B across downstream benchmarks.We compare the non‚ÄëVWN baseline withVWN√ó\\times8(r=8r=8;n=r‚ãÖm=64n=r\\cdot m=64).VWN√ó\\times8consistently outperforms the baseline throughout training;\nat 500B tokens it yields +8.92 (DROP), +2.44 (HumanEval), +4.20 (MATH), +3.95 (MMLU),\n+5.25 (MMLU‚ÄëPro), and +7.45 (TriviaQA) accuracy points.",
                "position": 934
            },
            {
                "img": "https://arxiv.org/html/2511.11238/x17.png",
                "caption": "",
                "position": 937
            },
            {
                "img": "https://arxiv.org/html/2511.11238/x18.png",
                "caption": "",
                "position": 941
            },
            {
                "img": "https://arxiv.org/html/2511.11238/x19.png",
                "caption": "",
                "position": 945
            },
            {
                "img": "https://arxiv.org/html/2511.11238/x20.png",
                "caption": "",
                "position": 950
            },
            {
                "img": "https://arxiv.org/html/2511.11238/x21.png",
                "caption": "",
                "position": 954
            },
            {
                "img": "https://arxiv.org/html/2511.11238/x22.png",
                "caption": "",
                "position": 958
            }
        ]
    },
    {
        "header": "9Implementation of Generalized Hyper-Connections",
        "images": []
    },
    {
        "header": "10Downstream Benchmarks",
        "images": []
    }
]