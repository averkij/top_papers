[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03075/x1.png",
                "caption": "Figure 1:ReMiT on OLMo-1B substantially outperforms baselines trained with the standard mid-training method. (a) At the mid-training stage, ReMiT improves average accuracy across 10 widely-used benchmark tasks by 5.2% and reaches the baseline performance6×6\\timesfaster. (b) The improvements can carry over to post-training: during RL, ReMiT maintains a higher verifiable correct rate than the baseline and achieves better performance.",
                "position": 181
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03075/x2.png",
                "caption": "Figure 2:We identify the mid-training stage as a critical turning point, as it rapidly shifts the base model’s token distribution toward that of a more capable RL model. ReMiT enhances this stage by dynamically reweighting tokens in the mid-training corpus.",
                "position": 194
            }
        ]
    },
    {
        "header": "2Preliminaries and Empirical Motivation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03075/x3.png",
                "caption": "Figure 3:Visualization of the log-probability divergence between the pre-trained base model and the RL model. Background intensity reflects the marginΔ​log⁡p=log⁡pRL−log⁡pbase\\Delta\\log p=\\log p_{\\mathrm{RL}}-\\log p_{\\mathrm{base}}, where deeper red highlights pivotal tokens on which the RL model demonstrates significantly higher confidence than the base model.",
                "position": 279
            }
        ]
    },
    {
        "header": "3The ReMiT Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03075/x4.png",
                "caption": "Figure 4:Overview of the proposed ReMiT framework. The pipeline connects pre-training and post-training, establishing a self-reinforcing flywheel: improvements from the RL stage are retroactively transferred to strengthen the base model foundation, which in turn amplifies performance in subsequent post-training stages.",
                "position": 298
            }
        ]
    },
    {
        "header": "4Theoretical Motivation",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03075/x5.png",
                "caption": "Table 1:Few-shot accuracy across 10 widely used downstream tasks. All improvements\nof ReMiT over baselines are statistically significant. Extended results and the num_shots are provided in AppendixD. The best scores of each model family areboldfaced.",
                "position": 563
            },
            {
                "img": "https://arxiv.org/html/2602.03075/x5.png",
                "caption": "Figure 5:Performance gains of ReMiT on OLMo-1B acquired during mid-training transfer consistently to post-training, regardless of the post-training process (SFT, DPO, or RLVR). Figure (a): applying RL directly to the mid-trained base model. Figure (b): applying the complete post-training procedure to the mid-trained base model.",
                "position": 872
            },
            {
                "img": "https://arxiv.org/html/2602.03075/x6.png",
                "caption": "Figure 6:Robust generalization via ReMiT. (a) Co-improving Loop: Performance gains consistently transfer and amplify through iterative mid-training cycles (Vanilla NTP→\\toReMiT→\\toReMiT2). (b) KL Analysis: While KD forces the model to strictly mirror the RL policy, ReMiT allows for a moderate KL divergence, ensuring better transferability during post-training.",
                "position": 890
            },
            {
                "img": "https://arxiv.org/html/2602.03075/x6.png",
                "caption": "Figure 6:Robust generalization via ReMiT. (a) Co-improving Loop: Performance gains consistently transfer and amplify through iterative mid-training cycles (Vanilla NTP→\\toReMiT→\\toReMiT2). (b) KL Analysis: While KD forces the model to strictly mirror the RL policy, ReMiT allows for a moderate KL divergence, ensuring better transferability during post-training.",
                "position": 893
            },
            {
                "img": "https://arxiv.org/html/2602.03075/x7.png",
                "caption": "Figure 7:Sustained advantages of ReMiT across training stages. ReMiT achieves better generalization with pre-training gains that consistently carry over to post-training, whereas KD’s early advantages fade in later stages.",
                "position": 898
            },
            {
                "img": "https://arxiv.org/html/2602.03075/x8.png",
                "caption": "",
                "position": 934
            }
        ]
    },
    {
        "header": "6RELATED WORK",
        "images": []
    },
    {
        "header": "7CONCLUSION",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ABackground: The Mid-Training Phase",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03075/x9.png",
                "caption": "Figure 9:Overview of the pre-training pipeline. Top: During mid-training, the data distribution shifts from general web text to a higher-quality mixture enriched with code, STEM, and reasoning content. Bottom: The learning rate schedule is synchronized with this data shift, exhibiting rapid decay throughout the mid-training phase.",
                "position": 985
            }
        ]
    },
    {
        "header": "Appendix BMathematical Derivations",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03075/x10.png",
                "caption": "Figure 10:Token distribution divergence between the Base and RL models. Background intensity encodes the per-token log-probability margin (Δ​log⁡p=log⁡pRL−log⁡pbase\\Delta\\log p=\\log p_{\\mathrm{RL}}-\\log p_{\\mathrm{base}}), with deeper red indicating higher confidence in the ground-truth token from the RL model.",
                "position": 1112
            }
        ]
    },
    {
        "header": "Appendix CVisualization of Log-Probability Discrepancies Between Different Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03075/x11.png",
                "caption": "Figure 11:A comparison of token-level preferences between the RL and SFT models. The background color of each ground-truth token indicates the log-probability marginΔ​log⁡p=log⁡pRL−log⁡pSFT\\Delta\\log p=\\log p_{\\mathrm{RL}}-\\log p_{\\mathrm{SFT}}. Deeper red signifies a stronger preference from the RL model, deeper blue from the SFT model, and white denotes a negligible difference(|Δ​log⁡p|<0.1)(|\\Delta\\log p|<0.1).",
                "position": 1132
            }
        ]
    },
    {
        "header": "Appendix DExperimental Configuration",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03075/x12.png",
                "caption": "Figure 12:Comparison of downstream performance between SFT-Guided and ReMiT during mid-training on OLMo-1B. The results demonstrate that our RL-guided approach ReMiT consistently outperforms the SFT-guided baseline throughout the training trajectory.",
                "position": 1168
            }
        ]
    },
    {
        "header": "Appendix EExtended Analysis and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03075/x13.png",
                "caption": "Table 2:Extended evaluation of pre-training models across 10 widely used downstream tasks. The best scores within each model family areboldfaced.",
                "position": 1221
            },
            {
                "img": "https://arxiv.org/html/2602.03075/x13.png",
                "caption": "Table 3:Zero-shot accuracy of post-trained models on OLMo-1B across 10 widely used downstream tasks. The best scores areboldfaced.",
                "position": 1369
            },
            {
                "img": "https://arxiv.org/html/2602.03075/x13.png",
                "caption": "Table 4:Zero-shot accuracy of post-trained models on Youtu-LLM-2B across 10 widely used downstream tasks. The best scores areboldfaced.",
                "position": 1536
            },
            {
                "img": "https://arxiv.org/html/2602.03075/x13.png",
                "caption": "Figure 13:Loss trajectories during SFT and DPO on OLMo-1B.",
                "position": 1656
            },
            {
                "img": "https://arxiv.org/html/2602.03075/x14.png",
                "caption": "Figure 14:Performance comparison at the reinforcement learning stage on OLMo-1B.",
                "position": 1662
            },
            {
                "img": "https://arxiv.org/html/2602.03075/x15.png",
                "caption": "Figure 15:Impact of RL reference quality on mid-training performance across 10 downstream tasks. We compare ReMiT (using a fully converged RL model) against ReMiT (Intermediate) which uses a checkpoint trained for only 10% of RL steps.",
                "position": 1708
            },
            {
                "img": "https://arxiv.org/html/2602.03075/x16.png",
                "caption": "Figure 16:Training efficiency analysis of OLMo-1B on the same 50B mid-training corpus. (a) Wall-clock Convergence: Despite the computational overhead of the reference model, ReMiT demonstrates superior efficiency, converging 3.3x faster than the baseline in terms of GPU hours. (b) Throughput Comparison: While ReMiT incurs a reduction in training throughput (steps/sec) due to the additional forward pass, this cost is effectively amortized by its rapid convergence rate.",
                "position": 1739
            }
        ]
    },
    {
        "header": "Appendix FDiscussion",
        "images": []
    }
]