[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04989/extracted/5985497/imgs/figure1_labeled.jpg",
                "caption": "",
                "position": 117
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04989/extracted/5985497/imgs/semantic_correspondence_labeled.jpg",
                "caption": "Figure 2:Semantic correspondences in video diffusion models.We analyze feature maps in the image-to-video diffusion model SVD(Blattmann et¬†al.,2023a)for three generated video sequences (row 1).\nWe use PCA to visualize the features at diffusion timestep 30 (out of 50) at the output of an upsampling block (row 2), a self-attention layer (row 3), and the same self-attention layer after our alignment procedure (row 4). Although output feature maps of upsampling blocks in image diffusion models are known to encode semantic information(Tang et¬†al.,2023), we only observe weak semantic correspondences across frames in SVD.\nThus, we focus on the self-attention layer and modify it to produce feature maps that are semantically aligned across frames.",
                "position": 179
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04989/x1.png",
                "caption": "Figure 3:Overview of the controllable image-to-video generation framework.To control trajectories of scene elements, we optimize the latentùíõtsubscriptùíõùë°\\bm{z}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTat specific denoising timestepstùë°titalic_tof a pre-trained video diffusion model.\nFirst, we extract semantically aligned feature maps from the denoising U-Net to estimate the video layout.\nNext, we enforce cross-frame feature similarity along the bounding box trajectory to drive the motion of each region.\nTo preserve the visual quality of the generated video, a frequency-based post-processing method is applied to retain high-frequency noise of the original latentùíõtsubscriptùíõùë°\\bm{z}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. The updated latentùíõ~tsubscript~ùíõùë°\\tilde{\\bm{z}}_{t}over~ start_ARG bold_italic_z end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTis input to the next denoising step.",
                "position": 228
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04989/extracted/5985497/imgs/figure_baseline_labeled.jpg",
                "caption": "Figure 4:Qualitative comparison with supervised baselines.We observe that DragNUWA tends to distort objects rather than move them, and DragAnything is weak at part-level control as it is designed for entity-level control.\nIn contrast, our method can generate videos with natural motion for diverse object and camera trajectories.\nPlease seeour project pagefor more comparisons.",
                "position": 419
            },
            {
                "img": "https://arxiv.org/html/2411.04989/x2.png",
                "caption": "Figure 5:Performance across U-Net feature maps used to compute loss inEq.1.For all metrics, lower values are better.Temporalandspatialrefer to the temporal and spatial self-attention layers.\nWe find that features extracted from self-attention layers generally perform better than those from upsampling blocks and temporal attention layers.\nIn addition, using the feature maps of our modified self-attention layer achieves the best results, since they are semantically aligned across frames.\nCorresponding qualitative visuals are presented inFig.13andour project page.",
                "position": 549
            },
            {
                "img": "https://arxiv.org/html/2411.04989/x3.png",
                "caption": "Figure 6:Performance across U-Net layers used to extract feature maps.Lower is better for all metrics.Bottom,mid, andtopindicate the three resolution levels in the U-Net‚Äôs upsampling path, each containing three self-attention layers numbered 1, 2, and 3.\nfor example ‚ÄúM2-3‚Äù means applying the loss to features from both mid-resolution layers 2 and 3.\nWe observe that mid-resolution feature maps perform best for trajectory guidance.\nIn addition, using features from both M2 and M3 leads to the best result.\nSeeour project pagefor visualizations.",
                "position": 569
            },
            {
                "img": "https://arxiv.org/html/2411.04989/extracted/5985497/imgs/figure_fft_labeled.jpg",
                "caption": "Figure 7:Effect of high-frequency preservation in post-processing.Videos without post-processing tend to demonstrate oversmoothing and have artifacts.\nIn contrast, our post-processing technique retains videos with sharp details and eliminates most of the artifacts.\nSeeour project pagefor more examples.",
                "position": 586
            },
            {
                "img": "https://arxiv.org/html/2411.04989/x4.png",
                "caption": "Figure 8:Study of the cut-off frequency in post-processing.Lower is better for all metrics.\nThe valueŒ≥ùõæ\\gammaitalic_Œ≥indicates the cut-off frequency.\nFully keeping the optimized latent (Œ≥=1ùõæ1\\gamma=1italic_Œ≥ = 1) results in degraded video quality, as shown by high FID and FVD values.\nOn the other hand, replacing too many frequency components diminishes motion control, as indicated by the increasing ObjMC.",
                "position": 593
            },
            {
                "img": "https://arxiv.org/html/2411.04989/x5.png",
                "caption": "Figure 9:Ablation on optimization learning rates.Larger learning rates lead to video quality degradation (i.e., higher FID and FVD), while smaller learning rates result in lower motion fidelity (i.e., higher ObjMC). We choose the learning rate considering this tradeoff.",
                "position": 610
            },
            {
                "img": "https://arxiv.org/html/2411.04989/x6.png",
                "caption": "Figure 10:Effect of optimizing latent at individual denoising timesteps.For all metrics, lower values are better.\nHere, we optimizeEq.1on a single denoising timestep (t=50ùë°50t=50italic_t = 50corresponds to standard Gaussian noise), and we find middle timesteps (e.g.t=30ùë°30t=30italic_t = 30) achieve the best motion fidelity while maintaining visual quality.\nMore results on optimizing the latent at multiple timesteps can be found inFig.16.\nSeeFig.15andour project pagefor qualitative comparisons.",
                "position": 621
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04989/extracted/5985497/imgs/correspondence_full_labeled.jpg",
                "caption": "Figure 11:Semantic correspondences in video diffusion models across timesteps.Output feature maps of upsampling blocks have limited semantic correspondences across frames.\nIn contrast, our modified self-attention layers produce semantically aligned feature maps across all the timesteps.",
                "position": 1843
            },
            {
                "img": "https://arxiv.org/html/2411.04989/extracted/5985497/imgs/correspondence_full_feature_labeled.jpg",
                "caption": "Figure 12:Semantic correspondences of different features in video diffusion models.We find features from self-attention layers to be more semantically aligned than that of temporal attention layers and upsampling layers, while our modified self-attention layer produces the most aligned results due to its explicit formulation to attend to the first frame.",
                "position": 1849
            },
            {
                "img": "https://arxiv.org/html/2411.04989/extracted/5985497/imgs/figure_fet_full_labeled.jpg",
                "caption": "Figure 13:Ablation on U-Net feature maps.Applying loss on feature maps extracted from original self/temporal-attention layers or upsampling blocks fails to follow the trajectory due to the semantic misalignment across frames.\nIn contrast, performing optimization with our modified self-attention layers can produce videos consistent with the input trajectory, indicating the importance of using semantically aligned feature maps.\nPlease seeour project pagefor more qualitative results.",
                "position": 1854
            },
            {
                "img": "https://arxiv.org/html/2411.04989/extracted/5985497/imgs/figure_layer_labeled.jpg",
                "caption": "Figure 14:Ablation on U-Net layer to extract feature maps.Consistent with the quantitative results inFig.6, feature maps extracted from the middle resolution level are most useful for trajectory guidance. Optimizing on other feature maps may generate unrealistic videos with low motion fidelity.",
                "position": 1860
            },
            {
                "img": "https://arxiv.org/html/2411.04989/extracted/5985497/imgs/figure_time_labeled.jpg",
                "caption": "Figure 15:Visual comparison of different denoising timesteps.Here we show thelastframe of the generated video. Optimizing latent at later denoising process leads to severe artifacts.",
                "position": 1863
            },
            {
                "img": "https://arxiv.org/html/2411.04989/x7.png",
                "caption": "Figure 16:Effect of optimizing latent at multiple denoising timesteps.Here we perform optimization on multiple denoising timesteps (t=50ùë°50t=50italic_t = 50corresponds to standard Gaussian noise).\nSimilar to performing individual timestepFig.10, performing optimization up to middle timesteps (e.g.50‚àí30503050-3050 - 30) achieves the best motion fidelity while maintaining visual quality.",
                "position": 1868
            }
        ]
    },
    {
        "header": "Appendix BAdditional Results for Ablation Study",
        "images": []
    }
]