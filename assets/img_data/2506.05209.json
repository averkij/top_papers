[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05209/x1.png",
                "caption": "Figure 1:The Common Pile is an 8TB dataset of openly licensed text curated from 30 diverse sources.The sources comprising the Common Pile are shown above, categorized by textual domain.",
                "position": 368
            },
            {
                "img": "https://arxiv.org/html/2506.05209/x2.png",
                "caption": "Figure 2:The Common Pile consistently outperforms other openly licensed corpora as a pre-training dataset.Following the setup fromPenedo et al. [132], we train and evaluate 1.7B parameter models on 28B tokens of data from each dataset. Stars denote benchmarks on which the model trained using the Common Pile outperforms all other models.",
                "position": 604
            },
            {
                "img": "https://arxiv.org/html/2506.05209/x3.png",
                "caption": "Figure 3:Compared to models trained with similar resources (7 billion parameters, 1 trillion tokens), Comma v0.1-1T is the strongest model on several standard benchmarks.To contextualize these results, we include Qwen3 8B (trained on 36 trillion tokens) as a “current best-practices” upper bound. Stars denote benchmarks on which Comma v0.1-1T outperforms all other compute-matched models (i.e., all models other than Qwen3). Full numerical results are provided inLABEL:tab:benchmarkresults(appendix).",
                "position": 660
            },
            {
                "img": "https://arxiv.org/html/2506.05209/x4.png",
                "caption": "Figure 4:Comma v0.1-2T is also competitive with budget-matched models (7 billion parameters, 2 trillion tokens) trained on unlicensed data.We additionally include Qwen3 8B as a higher budget upper bound. Stars denote benchmarks where Comma v0.1-2T outperforms budget-matched models. Full numerical results are provided inLABEL:tab:ablation2tbenchmarkresults(appendix).",
                "position": 702
            },
            {
                "img": "https://arxiv.org/html/2506.05209/x5.png",
                "caption": "Figure 5:Author contributions to this work.Large squares indicate a major contribution and small squares indicate a supporting contribution.",
                "position": 3395
            },
            {
                "img": "https://arxiv.org/html/2506.05209/x6.png",
                "caption": "Figure 6:The amount of openly licensed text grows steadily over time.We visualize the cumulative proportion of data created up to various cutoff dates for sources in the Common Pile with reliable creation date metadata. This includes all sources except for the Caselaw Access Project, Data Provenance Initiative, and the sources covering early 20th century Public Domain books.",
                "position": 4294
            },
            {
                "img": "https://arxiv.org/html/2506.05209/x7.png",
                "caption": "Figure 7:A model trained on the Comma dataset consistently outperforms models trained on other corpora of openly licensed text and outperforms the Pile on all but two tasks.We train identical 1.7B parameter models on 28B tokens from each dataset followingPenedo et al. [132].",
                "position": 6507
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]