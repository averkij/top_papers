[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.17428/x1.png",
                "caption": "Figure 1:Overview of Quantization-aware Walsh-Hadamard Adaptation (QWHA). The weight update from QWHA is formulated asΔ​𝑾=𝑭​𝑯−1\\Delta{\\bm{W}}={\\bm{F}}{\\bm{H}}^{-1}, where𝑯{\\bm{H}}is a predefined Walsh-Hadamard transform (WHT) matrix and𝑭{\\bm{F}}is a trainable sparse coefficient matrix consisting of values𝒄{\\bm{c}}and their indices𝑬{\\bm{E}}. The multiplication𝑭​𝑯−1{\\bm{F}}{\\bm{H}}^{-1}indicates the expansion of learned coefficients (i.e.,𝒄{\\bm{c}}), over the transform basis (i.e., columns of𝑯−1{\\bm{H}}^{-1}). Note that, the coefficients𝒄{\\bm{c}}are the only trainable parameters, and𝑯{\\bm{H}}remains constant. Our key contributions are in the adoption of WHT into the adapter (WHA) and their initialization, particularly𝑬{\\bm{E}}(AdaAlloc) and𝒄{\\bm{c}}(Refinement).",
                "position": 163
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.17428/x2.png",
                "caption": "Figure 2:(a) Comparison of rank in weight updates between low-rank and FT-based adapters across linear layers.\n(b) Cumulative distribution ofℓ2\\ell_{2}norm of singular values and transform coefficients with Pareto hill indexη\\etafor the quantization errorΔ​𝑾Q\\Delta{\\bm{W}}_{Q}in the 14th-layer Value projection.\nThe vertical blue line indicates a point where the adapters have the same number of parameters.",
                "position": 359
            },
            {
                "img": "https://arxiv.org/html/2509.17428/x3.png",
                "caption": "Figure 3:(a) Average coverage of outlier components within the selected parameters. (b)ℓ2\\ell_{2}norm of the layer output error after initialization on the 14th-layer Key projection. The vertical blue lines indicate points where the adapters have the same number of parameters.",
                "position": 364
            },
            {
                "img": "https://arxiv.org/html/2509.17428/figure/rank_selection.png",
                "caption": "Figure 4:Rank of adapter weights.",
                "position": 479
            },
            {
                "img": "https://arxiv.org/html/2509.17428/figure/refine.png",
                "caption": "Figure 5:Layer output error.",
                "position": 595
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "Appendix CQuantization-aware Initialization of WHA",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.17428/x4.png",
                "caption": "Figure 9:Parameter selection patterns and two example zoomed-in results of each method in the 14th-layer Query projection of LLaMA-3.2-3B.",
                "position": 1147
            }
        ]
    },
    {
        "header": "Appendix DExperimental Details and Ablative Study",
        "images": []
    }
]