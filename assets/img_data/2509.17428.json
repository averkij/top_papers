[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.17428/x1.png",
                "caption": "Figure 1:Overview of Quantization-aware Walsh-Hadamard Adaptation (QWHA). The weight update from QWHA is formulated asÎ”â€‹ğ‘¾=ğ‘­â€‹ğ‘¯âˆ’1\\Delta{\\bm{W}}={\\bm{F}}{\\bm{H}}^{-1}, whereğ‘¯{\\bm{H}}is a predefined Walsh-Hadamard transform (WHT) matrix andğ‘­{\\bm{F}}is a trainable sparse coefficient matrix consisting of valuesğ’„{\\bm{c}}and their indicesğ‘¬{\\bm{E}}. The multiplicationğ‘­â€‹ğ‘¯âˆ’1{\\bm{F}}{\\bm{H}}^{-1}indicates the expansion of learned coefficients (i.e.,ğ’„{\\bm{c}}), over the transform basis (i.e., columns ofğ‘¯âˆ’1{\\bm{H}}^{-1}). Note that, the coefficientsğ’„{\\bm{c}}are the only trainable parameters, andğ‘¯{\\bm{H}}remains constant. Our key contributions are in the adoption of WHT into the adapter (WHA) and their initialization, particularlyğ‘¬{\\bm{E}}(AdaAlloc) andğ’„{\\bm{c}}(Refinement).",
                "position": 163
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.17428/x2.png",
                "caption": "Figure 2:(a) Comparison of rank in weight updates between low-rank and FT-based adapters across linear layers.\n(b) Cumulative distribution ofâ„“2\\ell_{2}norm of singular values and transform coefficients with Pareto hill indexÎ·\\etafor the quantization errorÎ”â€‹ğ‘¾Q\\Delta{\\bm{W}}_{Q}in the 14th-layer Value projection.\nThe vertical blue line indicates a point where the adapters have the same number of parameters.",
                "position": 359
            },
            {
                "img": "https://arxiv.org/html/2509.17428/x3.png",
                "caption": "Figure 3:(a) Average coverage of outlier components within the selected parameters. (b)â„“2\\ell_{2}norm of the layer output error after initialization on the 14th-layer Key projection. The vertical blue lines indicate points where the adapters have the same number of parameters.",
                "position": 364
            },
            {
                "img": "https://arxiv.org/html/2509.17428/figure/rank_selection.png",
                "caption": "Figure 4:Rank of adapter weights.",
                "position": 479
            },
            {
                "img": "https://arxiv.org/html/2509.17428/figure/refine.png",
                "caption": "Figure 5:Layer output error.",
                "position": 595
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "Appendix CQuantization-aware Initialization of WHA",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.17428/x4.png",
                "caption": "Figure 9:Parameter selection patterns and two example zoomed-in results of each method in the 14th-layer Query projection of LLaMA-3.2-3B.",
                "position": 1147
            }
        ]
    },
    {
        "header": "Appendix DExperimental Details and Ablative Study",
        "images": []
    }
]