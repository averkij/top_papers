[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.14340/x1.png",
                "caption": "Figure 1:Audio-visual soundscape stylization.We learn through self-supervision to manipulate input speech (middle) such that it sounds as though it were recorded within a given scene (left). Our approach captures both acoustic properties, such as reverberation, as well as the ambient sounds, such as crashing waves (top).\nTo help convey the results of the stylization, we have used source separation to visualize the speech waveform (shown inred) separately from background sound (shown inblue).",
                "position": 159
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Audio-Visual Soundscape Stylization",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.14340/x2.png",
                "caption": "Figure 2:Soundscape stylization by conditional speech de-enhancement. We randomly select two disjoint clips from a video, designating one as aconditionalexample and the other as thetarget. We then separate and enhance the target audio. Our model’s self-supervised pretext task is to remove this enhancement using the other conditional (audio, visual, or audio-visual) signal as a hint. At test time, we stylize an audio clip using a conditional example from the desired scene.",
                "position": 231
            },
            {
                "img": "https://arxiv.org/html/2409.14340/x3.png",
                "caption": "Figure 3:Model architecture. Given input audio derived from an enhancement model, and the conditional audio-visual clip sampled from the same video, we aim to stylize the input to closely resemble the original signal. We encode both the input and target spectrograms to the latent space using a pre-trained latent encoder, and feed them into a latent diffusion model together with the conditional audio-visual embedding. The goal is to harmonize the encoded latent of the input spectrogram with the target one. Finally, we employ a pre-trained latent decoder followed by a pre-trained HiFi-GAN vocoder to reconstruct the waveform from the latent space. Note that the latent encoder for the target spectrogram isnotused at test time.",
                "position": 256
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.14340/x4.png",
                "caption": "Figure 4:Model comparison. We show soundscape stylization results for several models, where each input audio is conditioned on two different audio-visual clips.",
                "position": 765
            },
            {
                "img": "https://arxiv.org/html/2409.14340/x5.png",
                "caption": "Figure 5:Qualitative generalization results. We restyle audio from LRS[85]conditioned on audio-visual (or visual-only) clips taken from AVSpeech[19].",
                "position": 1040
            },
            {
                "img": "https://arxiv.org/html/2409.14340/x6.png",
                "caption": "Figure 6:Generalization to non-speech sounds. We restyle the sounds of dog barks and train chimes like they are in the conditional scenes.",
                "position": 1052
            },
            {
                "img": "https://arxiv.org/html/2409.14340/x6.png",
                "caption": "Figure 6:Generalization to non-speech sounds. We restyle the sounds of dog barks and train chimes like they are in the conditional scenes.",
                "position": 1055
            },
            {
                "img": "https://arxiv.org/html/2409.14340/x7.png",
                "caption": "Figure 7:Failure cases. Our model fails to resemble the soundscapes of the scenes, perhaps due to vocal effort or invisible sounding objects. It also fails to generate impact sound synchronized with the conditional example.",
                "position": 1060
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix A.1Results Video",
        "images": []
    },
    {
        "header": "Appendix A.2Dataset Collection",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.14340/x8.png",
                "caption": "(a)Distribution of video duration",
                "position": 1861
            },
            {
                "img": "https://arxiv.org/html/2409.14340/x8.png",
                "caption": "(a)Distribution of video duration",
                "position": 1864
            },
            {
                "img": "https://arxiv.org/html/2409.14340/x9.png",
                "caption": "(b)Distribution of video categories",
                "position": 1869
            },
            {
                "img": "https://arxiv.org/html/2409.14340/x10.png",
                "caption": "Figure 9:Example frames of theCityWalkdataset. We randomly select 10 different scenes here for showcasing.",
                "position": 1876
            },
            {
                "img": "https://arxiv.org/html/2409.14340/extracted/5870910/figs/mturk.jpg",
                "caption": "Figure 10:Interface for Human Evaluation. We provide a screenshot of the interface designed for evaluatingaudio-visual soundscape stylization. Participants are instructed to listen to each audio at least three times, and complete the last four columns prior to advancing to the next example. Upon clicking the “Submit” button, participants will be navigated to the next question.",
                "position": 1879
            }
        ]
    },
    {
        "header": "Appendix A.3Additional Evaluation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.14340/x11.png",
                "caption": "Figure 11:Qualitative enhancement comparison. We visualize enhanced audio from different enhancement strategies, including separation-only, enhancement-only, and their combination.",
                "position": 1992
            },
            {
                "img": "https://arxiv.org/html/2409.14340/x12.png",
                "caption": "Figure 12:Additional results on non-speech sound stylization. We restyle sounds like baby cries and cat mews to match the soundscapes of the specified scenes.",
                "position": 2108
            },
            {
                "img": "https://arxiv.org/html/2409.14340/x13.png",
                "caption": "Figure 13:Additional qualitative results. We present conditional examples derived from the same video as the input audio, but at different time steps. This is an extension of Figure 4 in the main paper.",
                "position": 2305
            }
        ]
    },
    {
        "header": "Appendix A.4Additional Results",
        "images": []
    }
]