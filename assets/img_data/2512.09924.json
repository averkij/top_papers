[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09924/x1.png",
                "caption": "Figure 1:Current video unified models excel at understanding tasks with a powerful internal VLM, but a significant gap remains in their ability to guide a generator for reason-informed editing. Our ReViSE bridges this gap by enabling the VLM to produce self-corrective feedback, iteratively refining editing direction.",
                "position": 126
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09924/x2.png",
                "caption": "Figure 2:Overview and statistics of RVE-Bench.(a) Examples from two subsets: Reasoning-Informed Video Editing (top) and In-Context Video Generation (bottom). (b) Distribution of reasoning categories across the two subsets. (c) Word cloud of instruction keyword frequencies.",
                "position": 175
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09924/x3.png",
                "caption": "Figure 3:Overview of the data construction pipeline for the In-Context Video Generation subset of RVE-Bench.",
                "position": 230
            },
            {
                "img": "https://arxiv.org/html/2512.09924/x4.png",
                "caption": "Figure 4:Overview of ReViSE’s framework.Given a reasoning instruction and a source video, the internal VLM encodes them into textual tokens and visual features: the textual tokens support understanding, while the visual features pass through a vision head and adapter to condition a DiT for video edits. The proposed Self-Reflective Learning utilizes the internal VLM to evaluate the edited video from four dimensions and produces self-corrective feedback by answering “yes” or “no”, guiding the DiT training and refining the editing results.",
                "position": 291
            },
            {
                "img": "https://arxiv.org/html/2512.09924/x5.png",
                "caption": "Figure 5:Qualitative comparison of ReViSE and other baselines on RVE-Bench.",
                "position": 348
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09924/x6.png",
                "caption": "Figure 6:Qualitative comparisons between ReViSE and other representative methods on conventional video editing.",
                "position": 903
            },
            {
                "img": "https://arxiv.org/html/2512.09924/materials/radar_ori.png",
                "caption": "Figure 7:Comparison results of conventional video editing on Ditto-1M[bai2025scaling].",
                "position": 908
            },
            {
                "img": "https://arxiv.org/html/2512.09924/x7.png",
                "caption": "Figure 8:Qualitative ablation results of training objectives.",
                "position": 984
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Contents",
        "images": []
    },
    {
        "header": "Appendix ADetailed Descriptions of RVE-Bench",
        "images": []
    },
    {
        "header": "Appendix BTraining Details",
        "images": []
    },
    {
        "header": "Appendix CEffect of Self-Reflection Strengthλ\\lambda",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09924/x8.png",
                "caption": "Figure 9:Visualized qualitative comparison under varyingλ\\lambda.",
                "position": 1297
            },
            {
                "img": "https://arxiv.org/html/2512.09924/materials/causal_reasoning.png",
                "caption": "Figure 10:Ablation results of self reflection strengthλ\\lambdaacross causal, spatial, temporal, and commonsense reasoning.",
                "position": 1302
            },
            {
                "img": "https://arxiv.org/html/2512.09924/materials/causal_reasoning.png",
                "caption": "",
                "position": 1305
            },
            {
                "img": "https://arxiv.org/html/2512.09924/materials/spatial_reasoning.png",
                "caption": "",
                "position": 1307
            },
            {
                "img": "https://arxiv.org/html/2512.09924/materials/temporal_reasoning.png",
                "caption": "",
                "position": 1309
            },
            {
                "img": "https://arxiv.org/html/2512.09924/materials/commonsense_reasoning.png",
                "caption": "",
                "position": 1311
            }
        ]
    },
    {
        "header": "Appendix DReliability of Understanding Module",
        "images": []
    },
    {
        "header": "Appendix EMore Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09924/x9.png",
                "caption": "Figure 11:More qualitative comparisons between ReViSE and other representative methods.",
                "position": 1493
            }
        ]
    },
    {
        "header": "Appendix FExamples of Evaluation Results",
        "images": []
    },
    {
        "header": "Appendix GLimitation",
        "images": []
    }
]