[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.15156/x1.png",
                "caption": "Figure 1:A non-parametric continual learning (NPCL) framework schematics (1)Continual experience: incoming documents are processed asynchronously, potentially by different agents. (2)Individual workspaces: each experience is encoded into a Generative Semantic Workspace (GSW).\n(3)Continually learned global workspace: GSWs can be continually consolidated by reconciling entities, events, and actions both across and within documents. Extensive ablation studies (see Table11) show that different combinations of LLM models of different sizes for performing different tasks – GSW generation, and retrieval – lead to consistently robust performance. Thus GSW can be used as a shared meta-representation. (4)Reasoning-grounded inference: The goal is to haveenough reconciliation–but not exhaustive– so thatall latent knowledge supported by the collection of experiences are represented by inference chains/paths.",
                "position": 179
            },
            {
                "img": "https://arxiv.org/html/2602.15156/x2.png",
                "caption": "Figure 2:System overview of PANINI at inference time.Step 1: Planning:A decomposition LLM converts the user query into an ordered sequence of single-hop sub-questions.Step 2: RICR:We perform chain-based retrieval by expanding candidate paths hop-by-hop. The initial seed set is obtained via embedding similarity; therefore, for a query like “Who was Lothair II’s mother?”, retrieval may include bothLothair IIand the semantically nearbyLothair I. From these seeds, RICR follows QA edges to propose intermediate entities (e.g., candidate mothers) and incrementally extends partial chains across GSWs. Candidate chains are scored at each hop, and low-scoring paths are pruned.Step 3: Answer Generation:Top-ranked chains are de-duplicated and provided to the final answering LLM.",
                "position": 184
            }
        ]
    },
    {
        "header": "2Panini",
        "images": []
    },
    {
        "header": "3Experimental Setup",
        "images": []
    },
    {
        "header": "4Results and Discussion",
        "images": []
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Concluding Remarks and Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix APrompts to the LLM",
        "images": []
    },
    {
        "header": "Appendix BGSW Representation Example",
        "images": []
    },
    {
        "header": "Appendix CRICR Example",
        "images": []
    },
    {
        "header": "Appendix DOpen-Source Experiments",
        "images": []
    },
    {
        "header": "Appendix EAblation Studies",
        "images": []
    },
    {
        "header": "Appendix FQualitative Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.15156/x3.png",
                "caption": "Figure 4:Prompt used for factual GSW construction from documents.",
                "position": 3461
            },
            {
                "img": "https://arxiv.org/html/2602.15156/x4.png",
                "caption": "Figure 5:Prompt used for factual GSW construction from documents (continued).",
                "position": 3464
            },
            {
                "img": "https://arxiv.org/html/2602.15156/x5.png",
                "caption": "Figure 6:LLM prompt for 2nd pass GSW Construction.",
                "position": 3467
            },
            {
                "img": "https://arxiv.org/html/2602.15156/x6.png",
                "caption": "Figure 7:Prompt used for multi-hop question decomposition into atomic sub-questions.",
                "position": 3472
            },
            {
                "img": "https://arxiv.org/html/2602.15156/x7.png",
                "caption": "Figure 8:Oracle-style prompt used for answer generation from retrieved GSW evidence.",
                "position": 3475
            },
            {
                "img": "https://arxiv.org/html/2602.15156/x8.png",
                "caption": "Figure 10:Continual-learning under corpus growth (MuSiQue, 200 questions).We fix a held-out set of 200 questions and evaluate retrieval+QA F1 as the corpus is incrementally expanded from 4K to the full MuSiQue collection (∼\\sim12K passages). The set ofrelevantpassages for these questions is contained in the initial 4K subset and remains unchanged across steps; only the number ofdistractorpassages increases as additional (irrelevant) documents are added. This simulates a continuously evolving corpus where the signal stays constant but the retrieval search space grows.",
                "position": 3959
            }
        ]
    },
    {
        "header": "Appendix GComputational Costs and Resources for Building the GSW",
        "images": []
    }
]