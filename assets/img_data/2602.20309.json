[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.20309/figure/icon.png",
                "caption": "",
                "position": 83
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.20309/x1.png",
                "caption": "Figure 1:Comparison of representative VLA efficiency frameworks.(1) TinyVLA focuses on compact multimodal transformers and lightweight diffusion-policy heads for architectural efficiency;\n(2) EfficientVLA accelerates inference by pruning redundant language layers and reusing intermediate representations;\n(3) VLA-Cache improves throughput through key–value reuse and static caching of vision tokens;\n(4) MoLe-VLA adopts mixture-of-layers routing to dynamically skip computation in the language module; and\n(5)QuantVLAintroduces a training-free PTQ framework that low-bit quantizes both language and action modules without altering the model architecture.",
                "position": 117
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.20309/x2.png",
                "caption": "Figure 2:Overview ofQuantVLAfor VLAs with a DiT-based action head. The framework is training-free and preserves the original architecture and operator schedule. It combines: (1) a selective quantization layout that integerizes all linear layers in the LLM and all MLP layers in the DiT while keeping the attention projectionsQQ,KK,VV,OOin floating point; (2)Attention Temperature Matching(ATM), a per-head scalarα\\alphathat aligns teacher–student logits and is folded into dequantization scales; and (3)Output Head Balancing(OHB), a per-layer scalarβ\\betathat matches post-projection energy at the residual interface.",
                "position": 230
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.20309/x3.png",
                "caption": "Figure 3:ATM and OHB effects across attention blocks.(Left)shows logits standard deviation.(Right)shows attention output RMS after the output projection. The figure reports three configurations: the teacher model in floating point without quantization, the quantized baseline with LLM and DiT MLP integerized, and QuantVLA with ATM in the left panel or QuantVLA with OHB in the right panel, which are evaluated on the GR00T N1.5 model.",
                "position": 598
            },
            {
                "img": "https://arxiv.org/html/2602.20309/x3.png",
                "caption": "",
                "position": 601
            },
            {
                "img": "https://arxiv.org/html/2602.20309/x4.png",
                "caption": "",
                "position": 605
            },
            {
                "img": "https://arxiv.org/html/2602.20309/x5.png",
                "caption": "Figure 4:Memory saving of QuantVLA over the baseline on OpenPIπ​0.5\\pi 0.5and GR00T N1.5.",
                "position": 772
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AGeneral Quantization Formulations",
        "images": []
    },
    {
        "header": "Appendix BDuQuant Implementation Details",
        "images": []
    },
    {
        "header": "Appendix CHow logits transfer from the language backbone to DiT in a VLA",
        "images": []
    },
    {
        "header": "Appendix DQuantVLA Parameters",
        "images": []
    },
    {
        "header": "Appendix EComparison with other PTQ Method",
        "images": []
    },
    {
        "header": "Appendix FExtended Benchmark Evaluation",
        "images": []
    },
    {
        "header": "Appendix GApplicability Beyond DiT-Based VLA Models",
        "images": []
    }
]