[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04194/x1.png",
                "caption": "",
                "position": 86
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04194/x2.png",
                "caption": "Figure 2:Overview.For the input meshes of a given scene, we first convert them into 3D-GS representations to enable smooth gradient computation. The converted 3D-GS models are then used to initialize a 4D representation (Sec.3.3). We iteratively refine this 4D representation by sampling camera poses at each iteration, rendering the corresponding videos, and passing them to the video generation model to obtain optimization gradients (Sec.3.2). Additionally, we compute regularization terms (Sec.3.4) to enforce spatial and temporal smoothness during the optimization process.",
                "position": 165
            },
            {
                "img": "https://arxiv.org/html/2601.04194/x3.png",
                "caption": "Figure 3:Illustration of the hierarchical control point representation.We represent the deformation using a spatial hierarchical structure.\nCoarse control points capture large-scale deformations, while fine control points refine local details.",
                "position": 286
            },
            {
                "img": "https://arxiv.org/html/2601.04194/x4.png",
                "caption": "Figure 4:Illustration of the Fenwick Tree representation.Each node stores the cumulative deformation over a temporal range, allowing nearby frames to share parameters and naturally enforcing temporal coherence. For example,(rk[6],Tk[6])(r_{k}^{[6]},T_{k}^{[6]})encodes the accumulated deformation from frames 5â€“6. Queries for frames 6 and 7 then compose their deformations from a small, overlapping set of nodes, as shown in the figure.",
                "position": 354
            },
            {
                "img": "https://arxiv.org/html/2601.04194/x5.png",
                "caption": "Figure 5:Qualitative comparisons.We compare our approach with several mesh animation methods. Our method produces results that better align with the given prompts and exhibit more natural motion. In the figure, A3D refers to Animate3D[28], AAM denotes AnimateAnyMesh[73], MD represents MotionDreamer[64], and TC corresponds to 4D reconstruction results from videos generated by TrajectoryCrafter[84]. For additional comparisons and full animation results, please refer to our supplementary website.",
                "position": 422
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04194/x6.png",
                "caption": "Figure 6:Real-world object animation results.",
                "position": 524
            },
            {
                "img": "https://arxiv.org/html/2601.04194/x7.png",
                "caption": "Figure 7:Robot manipulation guided by our generated dense object flow.Given our generated dense object flow, the robot either grasps or pushes the object of interest in a manner that matches the flow. This allows effective manipulation of rigid objects (first row), articulated objects (second row), and deformable objects (third and fourth rows).",
                "position": 534
            },
            {
                "img": "https://arxiv.org/html/2601.04194/x8.png",
                "caption": "Figure 8:Ablation on noise-level sampling strategy.Removing our noise-level sampling strategy leads to unnatural motion, such as the laptop appearing to float.",
                "position": 555
            },
            {
                "img": "https://arxiv.org/html/2601.04194/x9.png",
                "caption": "Figure 9:Ablations on components in the 4D representation.Removing the Fenwick Tree leads to severe artifacts in later frames; removing fine control points prevents detailed deformation; and removing coarse control points causes distortions.",
                "position": 565
            },
            {
                "img": "https://arxiv.org/html/2601.04194/x10.png",
                "caption": "Figure 10:Ablations on regularization losses.Removing temporal regularization results in flickering, while removing spatial regularization results in distortions.",
                "position": 574
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BDerivation of SDS for Rectified Flow Models",
        "images": []
    },
    {
        "header": "Appendix CMore Experiment Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04194/x11.png",
                "caption": "Figure 11:Qualitative comparisons on single mesh animation.We compare our approach with several mesh animation methods. Our method produces results that better align with the given prompts and exhibit more natural motion. In the figure, A3D refers to Animate3D[28], AAM denotes AnimateAnyMesh[73], MD represents MotionDreamer[64], and TC corresponds to 4D reconstruction results from videos generated by TrajectoryCrafter[84].",
                "position": 1780
            },
            {
                "img": "https://arxiv.org/html/2601.04194/x12.png",
                "caption": "Figure 12:Failure Cases.The failure in the first row is due to limitations of the video generative model: it cannot produce motion that matches the prompt, as evidenced by its inability to sample videos aligned with the described action. The failure in the second row arises because our method cannot generate objects that were not present in the initial static scene. As a result, no liquid can appear when prompted, since the system cannot generate newly emerging objects.",
                "position": 2102
            }
        ]
    },
    {
        "header": "Appendix DLimitation and Future Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04194/figures/X_suppl/User_Study_1.png",
                "caption": "Figure 13:Screenshot of the user study question on Prompt Alignment.",
                "position": 2134
            },
            {
                "img": "https://arxiv.org/html/2601.04194/figures/X_suppl/User_Study_2.png",
                "caption": "Figure 14:Screenshot of the user study question on Motion Realism.",
                "position": 2139
            }
        ]
    },
    {
        "header": "Appendix EUser Study Template",
        "images": []
    }
]