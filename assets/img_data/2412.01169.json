[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01169/x1.png",
                "caption": "",
                "position": 122
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01169/x2.png",
                "caption": "Figure 2:Pipeline of OmniFlow. Previous any-to-any models such as CoDi[46](Top) concatenate multiple modality-specific encoders and decoders, and naively average the embedding of multiple modalities to achieve joint conditioning. By contrast, OmniFlow¬†(Bottom) is a unified, modular multi-modal model, where features from different modalities directly interact with each other through joint attention layers. OmniFlow¬†is inspired by the modular design of Stable Diffusion 3[11](Middle), a text-to-image model.",
                "position": 169
            }
        ]
    },
    {
        "header": "2Backgrounds",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01169/x3.png",
                "caption": "(a)Overall Pipeline of OmniFlow",
                "position": 401
            },
            {
                "img": "https://arxiv.org/html/2412.01169/x3.png",
                "caption": "(a)Overall Pipeline of OmniFlow",
                "position": 404
            },
            {
                "img": "https://arxiv.org/html/2412.01169/x4.png",
                "caption": "(b)Design of Omni-Transformer Block",
                "position": 409
            }
        ]
    },
    {
        "header": "4Setup",
        "images": []
    },
    {
        "header": "5Main Results",
        "images": []
    },
    {
        "header": "6Sampling",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01169/x5.png",
                "caption": "(a)Text-to-Audio Generation.",
                "position": 816
            },
            {
                "img": "https://arxiv.org/html/2412.01169/x5.png",
                "caption": "(a)Text-to-Audio Generation.",
                "position": 819
            },
            {
                "img": "https://arxiv.org/html/2412.01169/x6.png",
                "caption": "(b)Audio-to-Text Generation.",
                "position": 825
            },
            {
                "img": "https://arxiv.org/html/2412.01169/x7.png",
                "caption": "Figure 5:Effect of Multi-Modal Guidance.In this example, the user can flexibly control the alignment between output text and input image, audio independently by varyingŒ±ausubscriptùõºau\\alpha_{\\text{au}}italic_Œ± start_POSTSUBSCRIPT au end_POSTSUBSCRIPTandŒ±imsubscriptùõºim\\alpha_{\\text{im}}italic_Œ± start_POSTSUBSCRIPT im end_POSTSUBSCRIPT. HigherŒ±imsubscriptùõºim\\alpha_{\\text{im}}italic_Œ± start_POSTSUBSCRIPT im end_POSTSUBSCRIPTwill make the output texts resemble image captions, with visual descriptions such aslined up,driving down. HigherŒ±ausubscriptùõºau\\alpha_{\\text{au}}italic_Œ± start_POSTSUBSCRIPT au end_POSTSUBSCRIPTwill make the output texts resemble audio captions, with descriptions such asaccelerating,revving.",
                "position": 835
            },
            {
                "img": "https://arxiv.org/html/2412.01169/x8.png",
                "caption": "Figure 6:Qualitative Comparison with baselines on text-to-image generation.OmniFlow¬†achieves better image quality and prompt alignment when compared to previous generalist models.",
                "position": 838
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01169/x9.png",
                "caption": "Figure 7:Paths encoding different any-to-any generation tasks.(t1,t2,t3)subscriptùë°1subscriptùë°2subscriptùë°3(t_{1},t_{2},t_{3})( italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT )represents the ‚Äúnoise level‚Äù of image, text and audio modalities.(0,0,0)000(0,0,0)( 0 , 0 , 0 )represents clean (image, text, audio) triplets, and(1,1,1)111(1,1,1)( 1 , 1 , 1 )represents pure Gaussian noise.",
                "position": 1639
            },
            {
                "img": "https://arxiv.org/html/2412.01169/x10.png",
                "caption": "Figure 8:Training Pipeline of OmniFlow.We initialize our model with SD3 (Model 1). We then train the model on text-audio pairs to obtain Model 2. We merge Model 1 and Model 2 to obtain Model 3. The final model is obtained by further training Model 3 on any-to-any generation tasks.",
                "position": 1655
            },
            {
                "img": "https://arxiv.org/html/2412.01169/x11.png",
                "caption": "Figure 9:Architecture of Text VAE and Text Encoders in OmniFlow.SD3 (Top) uses three text encoders: CLIP-L, CLIP-G, and T5-XXL. OmniFlow¬†(Middile) replaces the 4.7B T5-XXL with a VAE encoder based on Flan-T5-L. CLIP encoders become optional and are not used for tasks without clean text inputs. The decoder of VAE (Bottom) is based on TinyLlama-1.1B. The VAE embedding is used as the prefix for decoding.",
                "position": 1658
            },
            {
                "img": "https://arxiv.org/html/2412.01169/extracted/6038158/figs/Artboard_6.png",
                "caption": "Figure 10:Discrete Diffusion Variant of OmniFlow.In this setup, we remove the text VAE and directly pass token embedding to the Omni-Transformer layers. ‚Äú[m]‚Äù indicates a mask token.",
                "position": 1726
            }
        ]
    },
    {
        "header": "Appendix BAdditional Discussions",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01169/x12.png",
                "caption": "Figure 11:Synthetic Experiments on three 1D-modalities.We consider the joint distribution of three toy modalities (x1,x2,x3subscriptùë•1subscriptùë•2subscriptùë•3x_{1},x_{2},x_{3}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT), each represented by a vector of dimension 1. Hence, a triplet consisting of three modalities be represented by a point in‚Ñù3superscript‚Ñù3\\mathbb{R}^{3}blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPTWe assume the joint distribution is a uniform distribution in the neighborhood of tetrahedron (Left). We experiment with training OmniFlowusing triplets, pairs, and only individual modalities. Models trained with triplets of three modalities best represent the original distribution.",
                "position": 1749
            }
        ]
    },
    {
        "header": "Appendix CQuantative Text Evaluation",
        "images": []
    },
    {
        "header": "Appendix DAdditional Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01169/x13.png",
                "caption": "Figure 12:Qualitative comparison of OmniFlow¬†with baselines on image-to-text generation.Images are provided from the Midjourney Explore page[38].",
                "position": 1883
            }
        ]
    },
    {
        "header": "Appendix ELimitations",
        "images": []
    },
    {
        "header": "Appendix FMiscellaneous",
        "images": []
    },
    {
        "header": "Appendix GReproducibility Statement",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01169/x14.png",
                "caption": "Figure 13:Examples of failure cases encountered during the text-to-image generation process of OmniFlow.",
                "position": 2272
            },
            {
                "img": "https://arxiv.org/html/2412.01169/x15.png",
                "caption": "Figure 14:Qualitative examples of the text-to-image capability of OmniFlow.",
                "position": 2275
            }
        ]
    },
    {
        "header": "Appendix HFailure Cases",
        "images": []
    }
]