[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18871/x1.png",
                "caption": "Figure 1:Overview of versatile abilities in OmniGen2 and reflection model",
                "position": 113
            }
        ]
    },
    {
        "header": "2Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18871/x2.png",
                "caption": "Figure 2:Architecture of OmniGen2. OmniGen2 employs separate transformer architectures for autoregressive and diffusion. Two distinct image encoders are utilized: ViT encodes images for input into the text transformer, while VAE encodes images for the diffusion transformer.",
                "position": 162
            },
            {
                "img": "https://arxiv.org/html/2506.18871/x3.png",
                "caption": "Figure 3:An illustration of Omni-RoPE. It decomposes position information into three components: (1) a Sequence and Modality Identifier (i‚Å¢ds‚Å¢e‚Å¢qùëñsubscriptùëëùë†ùëíùëûid_{seq}italic_i italic_d start_POSTSUBSCRIPT italic_s italic_e italic_q end_POSTSUBSCRIPT) that is constant for all tokens within a single image (treating it as a semantic unit) but unique across different images; and (2) and (3) 2D spatial coordinates (h,w‚Ñéùë§h,witalic_h , italic_w) that are locally computed from (0,0) for each image entity. This dual mechanism enables the model to unambiguously distinguish different images via their uniquei‚Å¢ds‚Å¢e‚Å¢qùëñsubscriptùëëùë†ùëíùëûid_{seq}italic_i italic_d start_POSTSUBSCRIPT italic_s italic_e italic_q end_POSTSUBSCRIPT, while the shared local spatial coordinates enhance consistency for tasks like image editing.",
                "position": 186
            },
            {
                "img": "https://arxiv.org/html/2506.18871/x4.png",
                "caption": "Figure 4:Multimodal Reflection for image generation.",
                "position": 225
            }
        ]
    },
    {
        "header": "3Dataset Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18871/x5.png",
                "caption": "Figure 5:In-Context Generation Dataset Construction Pipeline. The final input images are outlined with a red border and the target image is marked by a blue boundary.",
                "position": 253
            },
            {
                "img": "https://arxiv.org/html/2506.18871/x6.png",
                "caption": "Figure 6:In-Context Editing Dataset Construction Pipeline. The final input and target images are outlined by red and blue consistent with Figure5.",
                "position": 263
            },
            {
                "img": "https://arxiv.org/html/2506.18871/x7.png",
                "caption": "Figure 7:Create image edit pairs from video dataset. We first filter out frames belonging to different scenes to ensure contextual consistency, and then remove frames that exhibit significant changes in viewpoint.",
                "position": 286
            },
            {
                "img": "https://arxiv.org/html/2506.18871/x8.png",
                "caption": "Figure 8:Overview of OmniContext benchmark.Left:Image genres included in OmniContext.Right:Example images for each genre in OmniContext.",
                "position": 329
            },
            {
                "img": "https://arxiv.org/html/2506.18871/x9.png",
                "caption": "Figure 9:An illustrative example of evaluating the output image in the OmniContext benchmark.",
                "position": 332
            }
        ]
    },
    {
        "header": "4OmniContext Benchmark",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18871/x10.png",
                "caption": "Figure 10:Qualitative text-to-image generation by OmniGen2.Examples showcasing the model‚Äôs high fidelity to various text prompts and its support for diverse aspect ratios.",
                "position": 1012
            },
            {
                "img": "https://arxiv.org/html/2506.18871/x11.png",
                "caption": "Figure 11:Versatile image editing with OmniGen2.The model skillfully handles a wide variety of instructions, from simple object modifications to complex motion change and stylistic alterations.",
                "position": 1556
            },
            {
                "img": "https://arxiv.org/html/2506.18871/x12.png",
                "caption": "Figure 12:Qualitative results of in-context generation and in-context edit.",
                "position": 1567
            },
            {
                "img": "https://arxiv.org/html/2506.18871/x13.png",
                "caption": "Figure 13:Example of generation with reflection using OmniGen2.Leftandmiddle:Successful correction via one round of reflection.Right:an example of failed reflection, where the correct answer is incorrectly judged as wrong due to over-reflection.",
                "position": 2090
            }
        ]
    },
    {
        "header": "6Limitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18871/x14.png",
                "caption": "Figure 14:Visualization of OmniGen2‚Äôs Limitations.Line 1:The model performs poorly when processing Chinese prompts and low-quality images.Line 2:The model often struggles to modify human body shapes accurately.Line 3:The model is sensitive to ambiguous instructions involving multiple image sources.",
                "position": 2133
            }
        ]
    },
    {
        "header": "7Related Works",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]