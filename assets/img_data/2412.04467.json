[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04467/x1.png",
                "caption": "Figure 1:VisionZip Performance and Efficiency.(a) Our VisionZip significantly outperforms the current SOTA EfficientVLM model, like FastV, SparseVLM, achieving nearly 95% of the performance with only 10% of the tokens across 11 benchmarks on LLaVA-1.5. (b) VisionZip could reduce 8×\\times×prefilling time for LLaVA-NeXT 7B. (c) VisionZip reduces GPU inference time by 2×\\times×across 11 benchmarks, enabling the LLaVA-NeXT 13B model to infer faster than the 7B model while achieving better results.",
                "position": 126
            },
            {
                "img": "https://arxiv.org/html/2412.04467/x2.png",
                "caption": "Figure 2:Redundancy Visualization.The visualization and distribution statistics of attention scores show attention concentrated on only a few tokens, while many tokens display very low attention scores, indicating significant redundancy in the visual tokens.",
                "position": 145
            },
            {
                "img": "https://arxiv.org/html/2412.04467/x3.png",
                "caption": "Figure 3:Framework of VisionZip.VisionZip  selects dominant tokens that aggregate substantial information based on visual token attention scores. Remaining tokens are merged based on semantic similarity to produce contextual tokens.  VisionZip is a training-free method significantly reduces the number of image tokens, accelerating inference while maintaining performance. With efficient fine-tuning of the projector, even better results can be achieved with minimal performance loss compared to using the full token.",
                "position": 148
            }
        ]
    },
    {
        "header": "2VisionZip",
        "images": []
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04467/x4.png",
                "caption": "Figure 4:Performance of VisionZip on the Mini-Gemini.",
                "position": 1120
            }
        ]
    },
    {
        "header": "4Analysis and Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04467/x5.png",
                "caption": "Figure 5:Visualization of attention distribution across layers",
                "position": 1263
            },
            {
                "img": "https://arxiv.org/html/2412.04467/x6.png",
                "caption": "Figure 6:Reason of redundancy and feature misalignment",
                "position": 1292
            },
            {
                "img": "https://arxiv.org/html/2412.04467/x7.png",
                "caption": "Figure 7:Example comparison of VisionZip and previous text-relevant method in multi-turn conversation",
                "position": 1524
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AFurther Discussion",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04467/x8.png",
                "caption": "Figure 8:Advantage of  VisionZip in video understanding task.With the same visual token length, using VisionZip allows encoding more frames, significantly enhancing the model’s capacity to understand longer video sequences and capture more detailed information.",
                "position": 4086
            }
        ]
    },
    {
        "header": "Appendix CRelated Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04467/x9.png",
                "caption": "Figure 9:Visualization of Redundancy in the CLIP Model",
                "position": 4632
            },
            {
                "img": "https://arxiv.org/html/2412.04467/x10.png",
                "caption": "Figure 10:Visualization of Redundancy in the CLIP Model",
                "position": 4635
            },
            {
                "img": "https://arxiv.org/html/2412.04467/x11.png",
                "caption": "Figure 11:Visualization of Redundancy in the SigLIP Model",
                "position": 4638
            },
            {
                "img": "https://arxiv.org/html/2412.04467/x12.png",
                "caption": "Figure 12:Visualization of Attention Distribution Change",
                "position": 4641
            },
            {
                "img": "https://arxiv.org/html/2412.04467/x13.png",
                "caption": "Figure 13:Visualization of Attention Distribution Change",
                "position": 4644
            },
            {
                "img": "https://arxiv.org/html/2412.04467/x14.png",
                "caption": "Figure 14:Visualization of Feature Misalignment.The red point represents the selected token, while the heatmaps in the first three columns illustrate the attention relationships to the selected token. The last column displays the attention map for the entire image. The results shows that the attention of the selected token does not focus on semantically similar tokens but instead on dominant tokens, highlighting the phenomenon of feature misalignment.",
                "position": 4647
            },
            {
                "img": "https://arxiv.org/html/2412.04467/x15.png",
                "caption": "Figure 15:Gradio demo to analysis the visual redundancy and the feature misalignment",
                "position": 4650
            }
        ]
    },
    {
        "header": "Appendix DVisualization",
        "images": []
    }
]