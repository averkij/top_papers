[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08026/x1.png",
                "caption": "Figure 1:PEAR reduce the response length by penalizing excessive entropy during thinking phase while allowing moderate exploration at the final answer phase.",
                "position": 116
            }
        ]
    },
    {
        "header": "2Preliminary Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08026/x2.png",
                "caption": "Figure 2:(a) Relationship between average entropy and response length across different models. The dot size indicates accuracy. DS(L) represents DeepSeek-R1-Distill-Qwen/Llama. (b) Comparison of average entropy between the thinking phase and the final answer phase.",
                "position": 178
            },
            {
                "img": "https://arxiv.org/html/2510.08026/x3.png",
                "caption": "",
                "position": 187
            },
            {
                "img": "https://arxiv.org/html/2510.08026/x4.png",
                "caption": "Figure 3:Accuracy and average response length in the entropy filtering experiments on Qwen3-4B.",
                "position": 238
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08026/x5.png",
                "caption": "Table 1:Acc@1 results on four mathematical reasoning benchmarks across three LRMs.↓\\downarrowindicates the relative change with respect to theOriginalrow of each model.\nPEAR consistently achieves the largest reduction in token usage across model scales, while maintaining comparable accuracy.",
                "position": 434
            },
            {
                "img": "https://arxiv.org/html/2510.08026/x5.png",
                "caption": "Figure 4:(a) Entropy changes before and after training with PEAR across thinking and final answer phases. (b) Changes in the number of reasoning steps and average tokens per step for Qwen3-4B. PEAR reduces both the number of reasoning steps and the average tokens per step.",
                "position": 710
            },
            {
                "img": "https://arxiv.org/html/2510.08026/x6.png",
                "caption": "",
                "position": 719
            },
            {
                "img": "https://arxiv.org/html/2510.08026/x7.png",
                "caption": "Figure 5:Average accuracy and response length of Qwen3-4B trained with differentα\\alpha.",
                "position": 749
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AEntropy Filtering Experiments for Qwen3-8B",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08026/x8.png",
                "caption": "Figure 6:Accuracy and average response length in the entropy filtering experiments on Qwen3-8B.",
                "position": 1743
            }
        ]
    },
    {
        "header": "Appendix BExperiment details for baseline methods",
        "images": []
    },
    {
        "header": "Appendix CEvaluation Benchmarks",
        "images": []
    }
]