[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08587/x1.png",
                "caption": "",
                "position": 81
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08587/x2.png",
                "caption": "Figure 2:(a) Reconstruction-based paradigm used in baseline methods. (b) Non-reconstruction-based paradigm used in MoCha.",
                "position": 103
            },
            {
                "img": "https://arxiv.org/html/2601.08587/x3.png",
                "caption": "Figure 3:Overview of MoCha.Training MoCha consists of two stages: (a) In-Context Learning. We apply the condition-aware RoPE to the concatenated tokens and train the DiT backbone. (b) Post-Training. We employ an RL-based strategy to further enhance the facial consistency.",
                "position": 126
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08587/x4.png",
                "caption": "Figure 4:Overview of the data construction pipeline.We propose three methods to construct the training dataset: (I) Rendered data built with Unreal Engine 5. (II) Expression-driven face animation data generated through portrait animation methods. (III) Augmented data synthesized from traditional video-mask pairs.",
                "position": 277
            }
        ]
    },
    {
        "header": "4Dataset",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08587/x5.png",
                "caption": "Figure 5:Comparison with state-of-the-art methods.The results show that MoCha can replace the character with more consistent animation, higher facial expressiveness, and more natural lighting effects.",
                "position": 326
            },
            {
                "img": "https://arxiv.org/html/2601.08587/x6.png",
                "caption": "Figure 6:Ablation on Real-Human Data.The result shows that real-human data improves the overall realism and facial fidelity of the output.",
                "position": 508
            },
            {
                "img": "https://arxiv.org/html/2601.08587/x7.png",
                "caption": "Figure 7:Ablation on Identity-Enhancing Post-Training.The result shows that our post-training strategy is crucial to enhance the facial consistency.",
                "position": 520
            },
            {
                "img": "https://arxiv.org/html/2601.08587/x8.png",
                "caption": "Figure 8:Visualization of the attention score.It shows that MoCha can automatically trace the character with the mask of only one frame.",
                "position": 534
            },
            {
                "img": "https://arxiv.org/html/2601.08587/x9.png",
                "caption": "Figure 9:MoChaâ€™s performance on complex scenarios.MoCha demonstrates its robustness and practical utility on complex cases that are commonly encountered in real-world film and video production.",
                "position": 546
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]