[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22954/x1.png",
                "caption": "Figure 1:Overview of the residual denoising mechanism in Residual Context Diffusion (RCD). Remasking happens during each denoising step, discarding low-confidence token even though they are already explicitly computed. To recycle computation from previous steps, our proposed method\nextracts information before it is discarded, and forwards it to the next denoising step.",
                "position": 115
            },
            {
                "img": "https://arxiv.org/html/2601.22954/images/recall_analysis.png",
                "caption": "Figure 2:Average token recall across LLaDA GSM8K sampling steps. For each stepkk(x-axis), recall@5 (y-axis) is the fraction of final decoded tokens that appear in the step-kktop-5 predictions at the same position.High early-step recall@5 indicates that intermediate distributions contain semantically informative signals.",
                "position": 136
            },
            {
                "img": "https://arxiv.org/html/2601.22954/x2.png",
                "caption": "Figure 3:(Left)In vanilla dLLM inference, tokens not selected are reverted to static mask embeddings for the next step, discarding the predicted probability information.(Center)RCD Inference preserves this information by calculating a weighted summation of the unselected probability distributions (residuals) and injects them into the input embeddings of the subsequent step. This allows the model to aggregate more information and refine ambiguous tokens over time.(Right)RCD Training employs a decoupled strategy. A Frozen Reference Model generates stable probability targets and entropy weights (α\\alpha). These are used to construct the residual vectors, which are then added to the Trainable Target Model’s embeddings, helping the model to effectively leverage residual context for prediction.",
                "position": 170
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3The RCD Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22954/images/sdar_accuracy_vs_tokens_per_step_academic.png",
                "caption": "Figure 4:Accuracy vs. Token per Step Pareto frontiers for SDAR models. Curves are generated by sweeping confidence thresholds from 0.5 to 1.0. Token per Step measures the average number of tokens generated in each parallel iteration; higher Token per Step indicates fewer total steps for a given length. RCD (green dashed) consistently shifts the frontier toward the upper-left, providing significant speedups at identical accuracy levels. Note that curves may not fully overlap on the x-axis as RCD enables stable generation at higher Token per Step regimes where the Sequential Denoising baseline fails to maintain coherence.",
                "position": 718
            },
            {
                "img": "https://arxiv.org/html/2601.22954/images/acc_vs_nts_interpolation_methods.png",
                "caption": "Figure 5:Impact of different interpolation strategiesα\\alphaon the Accuracy vs. Token per Step Pareto frontier. (1) Normalized Entropy (Ours):α\\alphais based on predictive uncertainty; (2) Linear:α\\alphais a fixed-scalar; (3) Confidence:α\\alphais max-probability; and (4) Inverse: reversed mappings of entropy and confidence. Normalized entropy interpolation achieves the optimal trade-off, reaching the highest parallelism (Token per Step) with competitive accuracy.",
                "position": 727
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelated Works",
        "images": []
    },
    {
        "header": "Appendix BFormulation of Residual Context Diffusion",
        "images": []
    },
    {
        "header": "Appendix CDetailed Training Configurations",
        "images": []
    },
    {
        "header": "Appendix DPotential Data Contamination on GSM8K",
        "images": []
    }
]