[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02757/x1.png",
                "caption": "Figure 1:One-Minute Videos Generated by Loong Conditioned on Texts.Loong is an autoregressive LLM-based model that can generate minute-level long videos with consistent appearance, large motion dynamics, and natural scene transitions.",
                "position": 69
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02757/x2.png",
                "caption": "Figure 2:Overall Framework and the Training process of Loong .Given the input text tokens, the model predict video tokens autoregressively. All the text and video information is formulated into a unidirectional discrete token sequence, where the model predicts the next token based on the previous tokens. Video Tokenizer is utlized to convert video frames into discrete tokens. We use different color to represent first frame, short clip and long clip separately. We follow a progressive training pipeline to train on long videos. We omit the special tokens for simplicity.",
                "position": 135
            },
            {
                "img": "https://arxiv.org/html/2410.02757/extracted/5898627/figs/clip_loss_curves.png",
                "caption": "Figure 3:Imbalanced Training Losses When Training Directly on Long Videos.The training loss for late frames (18-65) is smaller than that of early frames (1-17), and the loss for the first frame remains relatively high, leading to suboptimal visual quality in the early frames( despite the model being pretrained on text-to-image).",
                "position": 181
            },
            {
                "img": "https://arxiv.org/html/2410.02757/x3.png",
                "caption": "Figure 4:Inference process of Loong .Given the input text, the model first predicts video tokens (illustrated byv1-v9) for the first 10s. The tokens from the last n frames of this clip are then decoded into video frames and re-encoded by the video tokenizer. These re-encoded tokens (v7-v9), along with the text tokens, serve as conditions to predict the video tokens (v10-v13) for the next clip. This iterative process of token prediction, partial decoding, and re-encoding enables extending videos beyond the training duration while mitigating quality degradation. This process is repeated until the generated video reaches the desired length.",
                "position": 215
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02757/x4.png",
                "caption": "Figure 5:Effectiveness of the Progressive Training with Loss Re-weighting.We sample 4 frames from the 17 earlier frames of the video generation results, to show the performance of models trained with or without our training strategy. The top row shows results of the model trained directly on long video, the appearance of objects degrades largely. The bottom row shows the results model trained with our proposed training approach, the appearance preserves effectively.",
                "position": 310
            },
            {
                "img": "https://arxiv.org/html/2410.02757/x5.png",
                "caption": "Figure 6:Effectiveness of Token Re-encoding during Video Extension.For each sample, the left two images show the results before the extension process, and the right two images show the results after extension. Without token re-encoding, the extension fails to generate visually consistent content.",
                "position": 314
            },
            {
                "img": "https://arxiv.org/html/2410.02757/x6.png",
                "caption": "Figure 7:Study on Sampling Strategies.Results of three different inference sampling strategies. Greedy decoding produces stable results but lacks diversity between frames. Multinomial sampling generates more dynamic and diverse content but with lower quality. Top-kùëòkitalic_ksampling achieves a balance between stability and diversity.kùëòkitalic_kis set to 50 in this experiment.",
                "position": 318
            },
            {
                "img": "https://arxiv.org/html/2410.02757/x7.png",
                "caption": "Figure 8:User Study on 1-min videos.Comparison with the StreamingT2V on SVD model. Our model is more preferred by human raters in terms of both visual text match and content consistency.",
                "position": 384
            },
            {
                "img": "https://arxiv.org/html/2410.02757/x8.png",
                "caption": "Figure 9:Generated Videos from Loong across Various Text-to-video Scenarios.Our model demonstrates diversity and quality across various text-to-video tasks, including short video, long video, and dense text-to-long video generation. The results exhibit rich details, smooth transitions, and strong semantic alignment with input descriptions.",
                "position": 397
            },
            {
                "img": "https://arxiv.org/html/2410.02757/x9.png",
                "caption": "Figure 10:Reconstructed Videos by Our Tokenizer.Each group represents a distinct video sequence, with the top row displaying the original frames and the bottom row presenting the corresponding reconstructions. Despite a high compression ratio of 256, our tokenizer effectively preserves fine details and natural, coherent motion in the reconstructed videos.",
                "position": 410
            }
        ]
    },
    {
        "header": "5Conclusion and Discussions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]