[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01119/x1.png",
                "caption": "",
                "position": 123
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01119/x2.png",
                "caption": "Figure 2:Pipeline ofInstant4D. We use Deep Visual SLAM model and Unidepthpiccinelli2024unidepthto obtain camera parameters, and metric depth. The metrics depth would be further optimized to consistent video depth. After that we back project from consistent depth to get dense point cloud, further voxel filtered to sparse point cloud, as discuss in Section3.2. Based on the 4d Gaussians Initialization, we can reconstruct a scene in 2 minutes. More details about optimization are described in Section3.3.",
                "position": 198
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01119/x3.png",
                "caption": "Figure 3:Visual comparison on the NVIDIA dataset.yoon2020novel",
                "position": 370
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01119/x4.png",
                "caption": "Figure 4:Visual Comparison on the Dycheck dataset.gao2022dynamic",
                "position": 476
            },
            {
                "img": "https://arxiv.org/html/2510.01119/x5.png",
                "caption": "Figure 5:Visualization on the DAVIS Dataset.",
                "position": 695
            }
        ]
    },
    {
        "header": "5Discussion & Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "AAdditional Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01119/x6.png",
                "caption": "Figure A1:Visualization of the Bear scene in the DAVIS[20]dataset.",
                "position": 1407
            },
            {
                "img": "https://arxiv.org/html/2510.01119/x7.png",
                "caption": "Figure A2:Visualization of the Rhino scene in the DAVIS[20]dataset.",
                "position": 1413
            }
        ]
    },
    {
        "header": "BMethods Detail& Ablation Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01119/x8.png",
                "caption": "Figure B3:Visualization for the our binary motion mask from predicted motion probability.",
                "position": 1510
            },
            {
                "img": "https://arxiv.org/html/2510.01119/x9.png",
                "caption": "Figure B4:Reconstruction for Sora[16]generated Video",
                "position": 1516
            }
        ]
    },
    {
        "header": "CVisualization on the AIGC Video",
        "images": []
    },
    {
        "header": "NeurIPS Paper Checklist",
        "images": []
    }
]