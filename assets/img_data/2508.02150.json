[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.02150/x1.png",
                "caption": "Figure 1:Performance comparison of distilled, instruct models and their variants optimized by our method (-IF) on reasoning tasks (AIME2024, GPQA, MMLU-Pro) and instruction following tasks (IFEval, CFBench, ComplexBench).",
                "position": 167
            },
            {
                "img": "https://arxiv.org/html/2508.02150/x2.png",
                "caption": "Figure 2:Overview of our RL framework. We first self-supervisedly construct reward model training data through curriculum decomposition for constraint-wise binary classification. Then, we efficiently train the policy model with both hard constraints via rule-based verification and soft constraints via the reward model.",
                "position": 170
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.02150/x3.png",
                "caption": "Figure 3:The reward and response length of each domain during RL training of Qwen2.5-7B-R (top row) and R1-Distill-Qwen-7B (bottom row).",
                "position": 892
            },
            {
                "img": "https://arxiv.org/html/2508.02150/x4.png",
                "caption": "Figure 4:Reward dynamics comparison.",
                "position": 895
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.02150/x5.png",
                "caption": "Figure 5:Sample Distribution across Task Types, Constraint Categories, and Number of Constraints",
                "position": 1531
            },
            {
                "img": "https://arxiv.org/html/2508.02150/x6.png",
                "caption": "Figure 6:The reward and response length of w/o incremental constraint curriculum.",
                "position": 2353
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]