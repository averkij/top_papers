[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18631/x1.png",
                "caption": "Figure 1:Training Dynamics of Gradient Norm and Reward for Qwen2.5-7B(Qwen et al.,2025)on GSM8K Dataset. Fig.s (a) and (b) compare gradient distributions and reward trends across training steps. The original GRPO method (Fig. (a)) suffers from significant gradient instability—both vanishing (red dots, norms ¡ 0.01) and exploding (purple asterisks, norms ¿ 5). In contrast, ReDit with Gaussian reward smoothing (Fig. (b)) effectively stabilizes optimization throughout training.",
                "position": 120
            },
            {
                "img": "https://arxiv.org/html/2506.18631/x2.png",
                "caption": "Figure 2:The figure illustrates how ReDit of different variances gradually smooth the reward distribution, showing the smoothing effect of perturbations of different variances.",
                "position": 126
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18631/x3.png",
                "caption": "Figure 3:GRPO has unstable performance on the MATH test set. The figure plots the test accuracy achieved for the checkpoints saved during the training run shown in Fig.4(b).",
                "position": 222
            }
        ]
    },
    {
        "header": "3Motivation",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18631/x4.png",
                "caption": "Figure 4:Qwen2.5-7B Gradient norm and reward training dynamics of standard GRPO on GSM8k and MATH datasets. During the whole optimization process, the gradient of standard GRPO is unstable, and there are a lot of gradient vanishing or gradient exploding cases.",
                "position": 235
            },
            {
                "img": "https://arxiv.org/html/2506.18631/x5.png",
                "caption": "Figure 5:Test accuracy across datasets on Qwen2.5-7B-Instruct and Qwen2.5-VL-7B-Instruct. The horizontal dashed line marks ReDit’s performance at 1000 steps, which GRPO fails to match even after 9000 steps.",
                "position": 403
            }
        ]
    },
    {
        "header": "4Reward Dithering (ReDit)",
        "images": []
    },
    {
        "header": "5Empirical Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18631/x6.png",
                "caption": "Figure 6:Accuracy of different LLMs on GSM8K. ReDit improves training efficiency and final performance in various LLMs.",
                "position": 475
            },
            {
                "img": "https://arxiv.org/html/2506.18631/x7.png",
                "caption": "Figure 7:CosineReverse achieves the best performance.",
                "position": 587
            },
            {
                "img": "https://arxiv.org/html/2506.18631/x8.png",
                "caption": "Figure 8:Accuracy of different GRPO variants tested on the GSM8K dataset. The horizontal dashed line highlights the performance of using ReDit at about 1000 training steps, and even after 9000 steps, its accuracy is comparable to the baseline.",
                "position": 590
            },
            {
                "img": "https://arxiv.org/html/2506.18631/x9.png",
                "caption": "Figure 9:Appropriate perturbation achieves the best performance.",
                "position": 600
            },
            {
                "img": "https://arxiv.org/html/2506.18631/x10.png",
                "caption": "Figure 10:ReDit has little effect on improving the performance of GRPO based RM.",
                "position": 607
            },
            {
                "img": "https://arxiv.org/html/2506.18631/x11.png",
                "caption": "Figure 11:Appropriate perturbation achieves the best performance.",
                "position": 614
            }
        ]
    },
    {
        "header": "6Theoretical Insights",
        "images": []
    },
    {
        "header": "7Limitations and Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelated Work",
        "images": []
    },
    {
        "header": "Appendix BTheorems and proofs",
        "images": []
    },
    {
        "header": "Appendix CTraining Dynamic",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18631/x12.png",
                "caption": "Figure 12:Training Dynamics of Gradient Norm and Reward on Math Dataset.",
                "position": 1911
            },
            {
                "img": "https://arxiv.org/html/2506.18631/x13.png",
                "caption": "Figure 13:Training dynamics of gradient norm and reward on the GSM8K dataset, showing the impact of perturbations of different distributions.",
                "position": 1917
            },
            {
                "img": "https://arxiv.org/html/2506.18631/x14.png",
                "caption": "Figure 14:Training dynamics of gradient norm and reward on the Math dataset, showing the impact of perturbations of different distributions.",
                "position": 1920
            }
        ]
    },
    {
        "header": "Appendix DExperimental setting",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18631/x15.png",
                "caption": "Figure 15:ReDit uniform perturbation performance changes with variance.",
                "position": 3199
            },
            {
                "img": "https://arxiv.org/html/2506.18631/x16.png",
                "caption": "Figure 16:ReDit scheduled perturbation Variance trend with training step (taking the original variance as 0.05 as an example)",
                "position": 3395
            }
        ]
    },
    {
        "header": "Appendix EMore result",
        "images": []
    }
]