[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.15603/x1.png",
                "caption": "Figure 1:Visualization of Image-to-Multi-RGBA (I2L) on open-domain images. The leftmost column in each group shows the input image. Qwen-Image-Layered is capable of decomposing diverse images into high-quality, semantically disentangled layers, where each layer can be independently manipulated without affect other content.",
                "position": 79
            },
            {
                "img": "https://arxiv.org/html/2512.15603/x2.png",
                "caption": "Figure 2:Visualization of Image-to-Multi-RGBA (I2L) on images containing texts. The leftmost column in each group shows the input image. Qwen-Image-Layered is capable of accurately decomposing both text and objects into semantically disentangled layers.",
                "position": 82
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.15603/x3.png",
                "caption": "Figure 3:Overview of Qwen-Image-Layered. Left: Illustration of our proposed VLD-MMDiT (Variable Layers Decomposition MMDiT), where the input RGB image and the target RGBA layers are both encoded by our proposed RGBA-VAE. During attention computation, these two sequences are concatenated along the sequence dimension, thereby enhancing inter-layer and intra-layer interactions. Right: Illustration of Layer3D RoPE, where a new layer dimension is introduced to support a variable number of layers.",
                "position": 162
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.15603/figure/numoflayers.png",
                "caption": "(a)Distribution of Layer Counts",
                "position": 289
            },
            {
                "img": "https://arxiv.org/html/2512.15603/figure/numoflayers.png",
                "caption": "(a)Distribution of Layer Counts",
                "position": 292
            },
            {
                "img": "https://arxiv.org/html/2512.15603/figure/numoftypes.png",
                "caption": "(b)Category Distribution",
                "position": 298
            },
            {
                "img": "https://arxiv.org/html/2512.15603/x4.png",
                "caption": "Figure 5:Qualitative comparison of Image-to-Multi-RGBA (I2L). The leftmost column shows the input image; the subsequent columns present the decomposed layers. Notably, LayerD[suzuki2025layerd]exhibits inpainting artifacts (Output Layer 1) and inaccurate segmentation (Output Layer 2 and 3), while our method produces high-quality, semantically disentangled layers, suitable for inherently consistent image editing.",
                "position": 311
            },
            {
                "img": "https://arxiv.org/html/2512.15603/x5.png",
                "caption": "Figure 6:Qualitative comparison of image editing. The leftmost column is the input image; prompts are listed above each row. Qwen-Image-Edit-2509[wu2025qwen]struggles with resizing and repositioning, tasks inherently supported by Qwen-Image-Layered. Meanwhile, Qwen-Image-Edit-2509 introduces pixel-level shifts (last row), while Qwen-Image-Layered can ensure consistency by editing specific layers.",
                "position": 411
            },
            {
                "img": "https://arxiv.org/html/2512.15603/x6.png",
                "caption": "Figure 7:Qualitative comparison of Text-to-Multi-RGBA (T2L). The rightmost column shows the composite image. The second row directly generates layers from text (Qwen-Image-Layered-T2L); the third row first generates a raster image (Qwen-Image-T2I) then decomposes it into layers (Qwen-Image-Layered-I2L). ART[pu2025art]fails to follow the prompt, while Qwen-Image-Layered-T2L produces semantically coherent layers, and Qwen-Image-T2I + Qwen-Image-Layered-I2L further improves visual aesthetics.",
                "position": 691
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    }
]