[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16856/x1.png",
                "caption": "",
                "position": 74
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16856/x2.png",
                "caption": "Figure 2:Overview of Multi-scale VQVAE. Given a 3D model, we leverage multi-view RGB-D(epth) renderings and Pl√ºcker embeddings as the input to our multi-view encoder‚Ñ∞‚Ñ∞\\mathcal{E}caligraphic_E. The encoder predicts a continuous feature map that is then quantized by the multi-scale quantizerùí¨ùí¨\\mathcal{Q}caligraphic_Q, givingR=(r1,r2,‚Ä¶,rK)ùëÖsubscriptùëü1subscriptùëü2‚Ä¶subscriptùëüùêæR=(r_{1},r_{2},\\dots,r_{K})italic_R = ( italic_r start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚Ä¶ , italic_r start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT )of latent tri-plane features. Each code of different scales share the same codebook. The triplane decoder then converts the quantized latent triplane features into the triplane representation through a plane-wise manner. The predicted triplane is multi-view supervised with the ground truth image, depth, and normal.",
                "position": 233
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16856/x3.png",
                "caption": "Figure 3:Overview of 3D Generation and 3D Understanding.Given a 3D model, our 3D VQVAE encodes it into multi-scale discrete tokens for both 3D generation and understanding. In (a) 3D Generation, text or a single image is encoded by CLIPTT{}_{\\text{T}}start_FLOATSUBSCRIPT T end_FLOATSUBSCRIPTor DINOv2, and the encoded condition features are integrated into the decoder-only transformer via cross attention. The transformer then causally predicts each scale of the latent triplane. In (b) 3D Understanding, truncated 3D tokens are first processed with an MLP projector. The large language model receives a multimodal sequence of text and 3D tokens and generates a detailed caption describing the input 3D model.",
                "position": 316
            },
            {
                "img": "https://arxiv.org/html/2411.16856/x4.png",
                "caption": "Figure 4:Qualitative Comparison of Image-conditioned 3D Generation.Here, we compare with the state-of-the-art 3D generative models under different categories. As visualized here, our method achieves superior 3D consistency across views and generates intact objects without distortion. For more comparisons with other methods, please refer to theSupp Mat.",
                "position": 464
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16856/x5.png",
                "caption": "Figure 5:More results of image and text conditioned 3D generation ofSAR3D.",
                "position": 501
            },
            {
                "img": "https://arxiv.org/html/2411.16856/x6.png",
                "caption": "Figure 6:Comparison of Text-conditioned 3D Generation. We present text-conditioned 3D objects generated bySAR3D, displaying two views of each sample.Compared to baseline methods, our approach consistently yields better quality regarding geometry,\ntexture, and text-3D alignment.",
                "position": 504
            },
            {
                "img": "https://arxiv.org/html/2411.16856/x7.png",
                "caption": "Figure 7:Simultaneous 3D Generation and Captioning. Given a single image or text,SAR3D-LLM can generate both a 3D model and a descriptive caption for the model.",
                "position": 536
            },
            {
                "img": "https://arxiv.org/html/2411.16856/x8.png",
                "caption": "Figure 8:3D Object Captioning. Given a 3D model,SAR3D-LLM can generate captions that include bothcategoryanddetails.",
                "position": 539
            }
        ]
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Appendix AMulti-scale quantization and interpolation",
        "images": []
    },
    {
        "header": "Appendix BTransformer blocks",
        "images": []
    },
    {
        "header": "Appendix CMore 3D captioning results",
        "images": []
    },
    {
        "header": "Appendix DMore image-to-3D comparison",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16856/x9.png",
                "caption": "Figure S1:Transformer Blocks in Our 3D Generation Transformer.The CLIP text encoder (CLIPT) or the DINOv2 image encoder processes text and image embeddings, respectively. The pooled tokens are passed through an MLP to compute the scale and shift parameters for the multi-head self-attention and feedforward network (FFN) modules. Additionally, feature vectors are incorporated into multi-head cross-attention blocks to enable cross-modal attention.",
                "position": 694
            },
            {
                "img": "https://arxiv.org/html/2411.16856/x10.png",
                "caption": "Figure S2:Additional 3D Captioning Results.Our method generates detailed descriptions based on the input of 8 scales of latent tri-plane tokens.",
                "position": 697
            },
            {
                "img": "https://arxiv.org/html/2411.16856/x11.png",
                "caption": "Figure S3:More Comparisons of Image-to-3D Generation.Our method consistently produces higher-quality 3D objects without distortion from a single image, excelling in both reference and novel views.",
                "position": 700
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]