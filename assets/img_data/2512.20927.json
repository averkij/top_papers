[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20927/x1.png",
                "caption": "Figure 1:Quantile rendering. (a) Unlike volume rendering[Mildenhall et al.,2021; Kerbl et al.,2023]that densely samples and blends all 3D Gaussians along the rays, (b) our Quantile render selectively samples and blends a sparse set of quantile Gaussians ‚Äì those with dominant influence along the ray, which can efficiently render high-dimensional feature maps from Gaussian features.",
                "position": 195
            }
        ]
    },
    {
        "header": "2Related works",
        "images": []
    },
    {
        "header": "3Preliminary",
        "images": []
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20927/x2.png",
                "caption": "Figure 2:Overview. Given optimized 3D Gaussiansùí¢\\mathcal{G}, our network is trained to predict Gaussian features‚Ñ±\\mathcal{F}that are aligned with the language embedding space from CLIP‚Äôs vision encoder. Typically, the proposed Q-Render accelerates the training and inference speed by transforming predicted Gaussian features into rendered feature maps.",
                "position": 253
            },
            {
                "img": "https://arxiv.org/html/2512.20927/x3.png",
                "caption": "Figure 3:Comparison of transmittance distribution across different Gaussians sampling algorithms. Our Q-Render effectively approximates the distribution of the transmittance distribution of the original 3D-GS. We usedK=10K=10for visualization.",
                "position": 404
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20927/x4.png",
                "caption": "Figure 4:Qualitative results on the open-vocabulary 3D semantic segmentation task.",
                "position": 516
            },
            {
                "img": "https://arxiv.org/html/2512.20927/x5.png",
                "caption": "Figure 5:Our qualitative results in the LeRF-OVS dataset.",
                "position": 595
            },
            {
                "img": "https://arxiv.org/html/2512.20927/x6.png",
                "caption": "Figure 6:Comparison of (a) FPS and (b) mIoU by varying the number of Gaussians participating on rendering. Our Q-Render allows real-time rendering while preserving the mIoU performance. For the fair speed comparison, while using our implementation with the same rasterized Gaussians and 512-D Gaussian features, we only change the sampling strategies among the rasterized Gaussians: V-Render[Mildenhall et al.,2021; Kerbl et al.,2023]uses all intersecting Gaussians, top-KK[Jun-Seong et al.,2025]sorts and selectsKKGaussians, and ours collectKKQuantile Gaussians.",
                "position": 605
            },
            {
                "img": "https://arxiv.org/html/2512.20927/x7.png",
                "caption": "Figure 7:Information loss after voxelization. It shows that ‚Äòa rendered image from de-voxelized Gaussians‚Äô achieved higher fidelity compared to ‚Äòa rendered image directly from sparse voxels‚Äô (w/o de-voxelization).",
                "position": 873
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AResolving up-to-scale with monocular depth",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20927/x8.png",
                "caption": "Figure 8:Visualization of 3D Gaussians with different conditions: (left) 3D Gaussians that are trained from ground truth pointcloud, (middle) 3D Gaussians that are trained from COLMAP points, then are applied with scene scaleaa, (right) 3D Gaussians that are trained from COLMAP points without considering scene scaleaa.",
                "position": 1028
            }
        ]
    },
    {
        "header": "Appendix BData preprocessing",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20927/x9.png",
                "caption": "Figure 9:Generated Gaussian Splatting labels from point cloud labels. Applying both opacity and distance thresholding allows much clearer label generation for Gaussian Splatting.",
                "position": 1056
            }
        ]
    },
    {
        "header": "Appendix CTheoretical Analysis of Quantile Rendering",
        "images": []
    },
    {
        "header": "Appendix DImplementation Details",
        "images": []
    },
    {
        "header": "Appendix EAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20927/x10.png",
                "caption": "Figure 10:Our qualitative results in the MipNeRF360 dataset.",
                "position": 1532
            },
            {
                "img": "https://arxiv.org/html/2512.20927/x11.png",
                "caption": "Figure 11:Comparison between V-Render and Q-Render on RGB reconstruction using a pre-trained 3D-GS model without fine-tuning. Q-Render shows only minor PSNR degradation, demonstrating that it serves as an efficient and accurate approximation of V-Render in the RGB domain.",
                "position": 1752
            }
        ]
    },
    {
        "header": "Appendix FPseudo-code for Voxelization, Voxel Feature Composition, and De-voxelization",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]