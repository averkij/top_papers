[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12574/x1.png",
                "caption": "Figure 1:Estimated memory consumption of inference a Llama-3-8B model with 1 million token on a single GPU.",
                "position": 136
            },
            {
                "img": "https://arxiv.org/html/2502.12574/x2.png",
                "caption": "Figure 2:Demonstrations of KV cache policies in inference. Full KV cache contains two main dimensions: layer and head. Layer-wise offloads KV cache in the layer’s dimension, with a cache budget of all heads per layer.HeadInferfurther reduces GPU memory by adaptively reallocating cache budgets in the head’s dimension, with a cache budget of one head.",
                "position": 144
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3HeadInfer: Head-wise Offload",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12574/x3.png",
                "caption": "Figure 3:Granularity of different methods. Each cube represents the entire attention process along three dimensions: Sequence (S), Layers (L), and Heads (H). Standard inference puts everything on the GPU.\nChunked-prefill fetches only a part of the sequence dimension of all tokens on the GPU at a time.\nLayer-wise offloading fetches a subset of layers on the GPU, offloading the rest.HeadInferintroduces an even finer approach that maintains only selected heads within a layer.",
                "position": 307
            }
        ]
    },
    {
        "header": "4HeadInferfor Memory-Efficient Inference",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12574/x4.png",
                "caption": "Figure 4:HeadInfersnapshot. All parameters are stored on the GPU. Head-wise partitioned KV cache is moved across GPU and CPU with the ping-pong memory.",
                "position": 428
            },
            {
                "img": "https://arxiv.org/html/2502.12574/x5.png",
                "caption": "Figure 5:Flashattention in the roofline plot analysis using the RTX-4090 device setting.",
                "position": 447
            }
        ]
    },
    {
        "header": "5Analysis",
        "images": []
    },
    {
        "header": "6Performance Evaluation",
        "images": []
    },
    {
        "header": "7Ablation Study",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement: Democratizing Access to Advanced AI",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12574/extracted/6208516/figures/7b.png",
                "caption": "Figure 6:HeadInferprovides equal accuracy as standard inference on the Needle-in-a-Haystack benchmark",
                "position": 2024
            }
        ]
    },
    {
        "header": "Appendix BMotivation and Additional Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12574/x6.png",
                "caption": "Figure 7:Token eviction methods cannot work when querying the less relevant information to the main theme. Here, we use a 10K document from LongBench(Bai et al.,2023b)and add one sentence that is not relevant to the main theme. In this case, H2O discards tokens less relevant to the main theme, leading to error generation. StreamingLLM discards tokens based on the query but remaining question tokens, making it Hallucinations.HeadInfercan successfully output the exact information from the lengthy input, even when we compress 75% of the KV cache",
                "position": 2414
            }
        ]
    },
    {
        "header": "Appendix CRoofline Model for head-wise flash attention",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12574/x7.png",
                "caption": "Figure 8:Flashattention in the roofline plot analysis using A100 device setting.",
                "position": 2875
            }
        ]
    },
    {
        "header": "Appendix DExtension:HeadInferImplementation with Head-wise Sparsity",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12574/x8.png",
                "caption": "Figure 9:Demonstrations of KV cache policies in inference from the head-wise view. Upper plots illustrate symbolic plots of an attention map deploying different policies in LLM generation. Lower: the overview ofHeadInfer.",
                "position": 2885
            },
            {
                "img": "https://arxiv.org/html/2502.12574/x9.png",
                "caption": "Figure 10:Workflow ofHeadInfergenerating a model with (n+1) layers and (j+1) attention heads.",
                "position": 2897
            },
            {
                "img": "https://arxiv.org/html/2502.12574/x10.png",
                "caption": "Figure 11:Demonstrations of adaptive head-wise offloading.",
                "position": 2917
            }
        ]
    },
    {
        "header": "Appendix EEasy-to-use and Portable Implementation",
        "images": []
    }
]