[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22975/figures/G.png",
                "caption": "",
                "position": 21
            },
            {
                "img": "https://arxiv.org/html/2601.22975/x1.png",
                "caption": "",
                "position": 79
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22975/x2.png",
                "caption": "Figure 1:Theoldenoose pipeline.We synthesize RLVR tasks from unverifiable text\nby constructing a MCQ version of the fill-in-the-middle\ntask. Given a source text, we prompt an LLM to first identify a contiguous span of crucial reasoning steps and replace it with a[MASK], treating the removed content as the ground-truth answer, and then generate a set of diverse distractors that are plausible and similar to the masked span, yet incorrect.\nFor noisy data sources (e.g., web scrapes), we prompt the LLM to first extract an educationally valuable passage and then construct the MCQ based on it. We further apply difficulty-based filtering to remove easy problems.",
                "position": 144
            },
            {
                "img": "https://arxiv.org/html/2601.22975/x3.png",
                "caption": "Figure 2:Comparison of continued RL training on Qwen-4B-Instruct after data saturation using the originalProRL dataversusadding GooseReason-0.7M. The former exhibits performance plateaus or regression, while the latter yields robust, continuous gains.",
                "position": 150
            },
            {
                "img": "https://arxiv.org/html/2601.22975/x4.png",
                "caption": "",
                "position": 171
            },
            {
                "img": "https://arxiv.org/html/2601.22975/figures/dataset_bar_chart.png",
                "caption": "Figure 3:Comparison betweenGooseReason-0.7Mand existing RLVR datasets used in ProRL(Liuet al.,2025a)in terms of total examples and effective examples, measured relative to ProRL-1.5B-v2. We define an example as effective if it has both successful and failed model rollouts, yielding meaningful learning signal for RL.\nNotably, we increase the number of effective examples in math, code, and STEM by over 450,000, which is a13Ã—\\timesincrease over the total effective examples in the ProRL dataset.",
                "position": 196
            },
            {
                "img": "https://arxiv.org/html/2601.22975/x5.png",
                "caption": "Figure 4:Accuracy distribution ofProRL-1.5B-v2, calculated as the success rate over 16 rollouts per task, onGooseReason-Mathacross different task formulations.\nNotably, with 9-choice MCQ format, the majority of problems fall into a medium-difficulty regime (exhibiting both successful and failed model rollouts), providing the most effective signals for RL training.",
                "position": 201
            }
        ]
    },
    {
        "header": "2Method:oldenoose",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22975/x6.png",
                "caption": "Figure 5:Comparison of continued RL training on ProRL-1.5B-v2 using the originalProRL data,adding GooseReason-0.7M, or usingRLVE(Zenget al.,2025a).\nContinuing with ProRL data yields marginal gains, adding GooseReason-0.7M produces robust, continuous improvements, while RLVE is highly effective in math but less so in STEM and coding.",
                "position": 235
            }
        ]
    },
    {
        "header": "3Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22975/x7.png",
                "caption": "Figure 6:Comparison of RL training from scratch on Qwen-4B-Instruct under a fixed compute budget withProRL data onlyversusjoint training with GooseReason-0.7M. The latter consistently achieves higher performance at the same number\nof steps.",
                "position": 566
            },
            {
                "img": "https://arxiv.org/html/2601.22975/x8.png",
                "caption": "Figure 7:Scaling behavior of continued RL training on Qwen-4B-Instruct withProRL data onlyversusjoint training GooseReason-0.7M, categorized asdiverge(regression vs. gain),outpace(faster gains), andalign(similar trends).",
                "position": 668
            }
        ]
    },
    {
        "header": "4Related works",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Impact Statements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails of Data Synthesis",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22975/x9.png",
                "caption": "Figure 8:Results breakdown for Figure2on six math benchmarks: comparison of continued RL training on Qwen-4B-Instruct after data saturation using the originalProRL dataversusadding GooseReason-0.7M.",
                "position": 1326
            },
            {
                "img": "https://arxiv.org/html/2601.22975/x10.png",
                "caption": "Figure 9:Results breakdown for Figure2on six coding benchmarks: comparison of continued RL training on Qwen-4B-Instruct after data saturation using the originalProRL dataversusadding GooseReason-0.7M.",
                "position": 1329
            },
            {
                "img": "https://arxiv.org/html/2601.22975/x11.png",
                "caption": "Figure 10:Additional results for Figure2on IFEval and GPQA Diamond: comparison of continued RL training on Qwen-4B-Instruct after data saturation using the originalProRL dataversusadding GooseReason-0.7M.",
                "position": 1332
            },
            {
                "img": "https://arxiv.org/html/2601.22975/x12.png",
                "caption": "Figure 11:Results breakdown for Figure5on six math benchmarks: comparison of continued RL training on ProRL-1.5B-v2 using the originalProRL data,adding GooseReason-0.7M, or usingRLVE.(Zenget al.,2025a)",
                "position": 1335
            },
            {
                "img": "https://arxiv.org/html/2601.22975/x13.png",
                "caption": "Figure 12:Results breakdown for Figure5on four coding benchmarks: comparison of continued RL training on ProRL-1.5B-v2 using the originalProRL data,adding GooseReason-0.7M, or usingRLVE.(Zenget al.,2025a)",
                "position": 1338
            },
            {
                "img": "https://arxiv.org/html/2601.22975/x14.png",
                "caption": "Figure 13:Results breakdown for Figure6on six math benchmarks: comparison of RL training from scratch on Qwen-4B-Instruct under a fixed compute budget withProRL data onlyversusjoint training with GooseReason-0.7M.",
                "position": 1341
            },
            {
                "img": "https://arxiv.org/html/2601.22975/x15.png",
                "caption": "Figure 14:Results breakdown for Figure6on four coding benchmarks: comparison of RL training from scratch on Qwen-4B-Instruct under a fixed compute budget withProRL data onlyversusjoint training with GooseReason-0.7M.",
                "position": 1344
            }
        ]
    },
    {
        "header": "Appendix BDetails of Experiments",
        "images": []
    }
]