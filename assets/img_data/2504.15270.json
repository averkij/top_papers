[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15270/x1.png",
                "caption": "",
                "position": 98
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15270/x2.png",
                "caption": "Figure 2:Left: The network architecture of Quicksviewer, that performs unified understanding of videos and images through visual tokens from cascaded modules. Right: The cubing network, that partitions an online video into nonuniform cubes based on Gumbel Softmax.",
                "position": 141
            }
        ]
    },
    {
        "header": "3Training Process",
        "images": []
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15270/x3.png",
                "caption": "Figure 3:(a) Left: Performance of Quicksviewer on particular domains and categories of Video-MME. (b) Right: Distribution of cube lengths across Video-MME videos.",
                "position": 637
            },
            {
                "img": "https://arxiv.org/html/2504.15270/x3.png",
                "caption": "",
                "position": 640
            },
            {
                "img": "https://arxiv.org/html/2504.15270/x4.png",
                "caption": "",
                "position": 644
            },
            {
                "img": "https://arxiv.org/html/2504.15270/x5.png",
                "caption": "Figure 4:The\"Visual Lag\"phenomenon occurring during the modelâ€™s cube-based segmental comprehension, where current cubes incorporate terminal frames from preceding event scenes to enable retrospective understanding.",
                "position": 650
            },
            {
                "img": "https://arxiv.org/html/2504.15270/x6.png",
                "caption": "Figure 5:(a) Left: Gumbel noise progressively anneals to 0.001 following the decaying learning rate with cosine scheduler. (b) Right: Compared to non-annealed training (cyan curve), adding Gumbel noise annealing (purple curve) yields more stable and superior loss convergence.",
                "position": 676
            },
            {
                "img": "https://arxiv.org/html/2504.15270/x6.png",
                "caption": "",
                "position": 679
            },
            {
                "img": "https://arxiv.org/html/2504.15270/extracted/6378287/imgs/train-loss-wo-anneal.png",
                "caption": "",
                "position": 683
            },
            {
                "img": "https://arxiv.org/html/2504.15270/x7.png",
                "caption": "Figure 6:Qualitative analysis showns that Quicksviewer effectively understands lengthy documentary and sports videos, as well as informative single and multiple images.",
                "position": 813
            }
        ]
    },
    {
        "header": "5Related Works",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining Data Details",
        "images": []
    },
    {
        "header": "Appendix BAdditional Evaluations",
        "images": []
    }
]