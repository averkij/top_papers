[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10633/x1.png",
                "caption": "",
                "position": 64
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related works",
        "images": []
    },
    {
        "header": "3Analyzing Hugging Faceâ€™s model atlas",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10633/x2.png",
                "caption": "Figure 2:The Hugging Face atlas:While this is a small subset (63,000 models) of the documented regions of HF, it already reveals significant trends.Depth and structure.The LLM connected component (CC) is deep and complex. It includes almost a third of all models. In contrast, while Flux is also substantial, its structure is much simpler and more uniform.Quantization.Zoom-in (A) highlights quantization practices across vision, language, and vision-language (V&L) models. Vision models barely use quantization, despite Flux containing more parameters (12B) than Llama (8B). Conversely, quantization is commonplace in LLMs, constituting a large proportion of models. VLMs demonstrate a balance between these extremes.Adapter and fine-tuning strategies.A notable distinction exists between discriminative (top) and generative (bottom) vision models. Discriminative models primarily employ fine-tuning, while generative models have widely adopted adapters like LoRA. The evolution of adapter adoption over time is evident: Stable-Diffusion 1.4 (SD) (1) mostly used full fine-tuning, while SD 1.5 (2), SD 2 (3), SD XL (4), and Flux (5) progressively use more adapters. Interestingly, the atlas reveals that audio models rarely use adapters, suggesting gaps in cross-community knowledge transfer. This inter-community variation is particularly evident inmodel merging. LLMs have embraced model merging, with merged models frequently exceeding the popularity of their parents. This raises interesting questions about the limited role of merging in vision models. For enhanced visualization, we display the top 30% most downloaded models.Zoom in to view edges, best viewed in color.",
                "position": 129
            },
            {
                "img": "https://arxiv.org/html/2503.10633/extracted/6277902/figs/depth_distribution.png",
                "caption": "(a)Vision vs. NLP node depth",
                "position": 161
            },
            {
                "img": "https://arxiv.org/html/2503.10633/x3.png",
                "caption": "",
                "position": 173
            }
        ]
    },
    {
        "header": "4Charting the atlas structure",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10633/x4.png",
                "caption": "Figure 4:Quantizations are leaves:Our analysis of over 400,000 documented model relationships reveals that99.41%percent99.4199.41\\%99.41 %of quantized models are leaf nodes. This figure shows this for a subset of the Llama-based models. Indeed, quantized models (magenta) are nearly always leaf nodes, corroborating the statistical finding.",
                "position": 377
            },
            {
                "img": "https://arxiv.org/html/2503.10633/x5.png",
                "caption": "Figure 5:Temporal dynamics indicate edge directionality:We analyzed over 400,000 documented model relationships and observed that in99.73%percent99.7399.73\\%99.73 %of cases, earlier upload times correlate with topologically higher positions in the DAG. Here, we visualize this trend on a subset of the Llama model family. Green nodes indicate models where earlier upload times align with topological order, while red nodes represent exceptions to this trend. The source (in gray) vacuously satisfied this assumption. It is clear that nearly all nodes satisfy our assumption.",
                "position": 399
            },
            {
                "img": "https://arxiv.org/html/2503.10633/x6.png",
                "caption": "Figure 6:Snake vs. Fan patterns:Snake patterns often arise from sequential training checkpoints, while fan patterns typically result from hyperparameter sweeps. In both structures the model weight variance is low. However, in snake patterns the weight distance has high correlation with model upload time, whereas in fan patterns the correlation is lower. Note colors are the same asFig.4",
                "position": 412
            }
        ]
    },
    {
        "header": "5Discussion and limitations",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10633/extracted/6277902/figs/app/neurons_ablation.png",
                "caption": "Figure 7:Number of neurons:Accuracy as a function of the number of neurons, indeed, 100 presents a good tradeoff between performance and resources.",
                "position": 1406
            }
        ]
    },
    {
        "header": "Appendix AAblations",
        "images": []
    },
    {
        "header": "Appendix BAdditional figures of graphs",
        "images": []
    },
    {
        "header": "Appendix CImplementation details",
        "images": []
    },
    {
        "header": "Appendix DDataset details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10633/extracted/6277902/figs/app/base_model_relation_histogram.png",
                "caption": "Figure 8:Relation type in our dataset",
                "position": 1439
            },
            {
                "img": "https://arxiv.org/html/2503.10633/extracted/6277902/figs/app/6_rings.png",
                "caption": "Figure 9:Documentation level in Hugging Face",
                "position": 1442
            },
            {
                "img": "https://arxiv.org/html/2503.10633/extracted/6277902/figs/app/whisper.png",
                "caption": "Figure 10:Individual connected components - 1/5",
                "position": 1467
            },
            {
                "img": "https://arxiv.org/html/2503.10633/extracted/6277902/figs/app/Qwen_VL.png",
                "caption": "Figure 11:Individual connected components - 2/5",
                "position": 1471
            },
            {
                "img": "https://arxiv.org/html/2503.10633/extracted/6277902/figs/app/gemma.png",
                "caption": "Figure 12:Individual connected components - 3/5",
                "position": 1475
            },
            {
                "img": "https://arxiv.org/html/2503.10633/extracted/6277902/figs/app/SM_license.png",
                "caption": "Figure 13:Visualizing other attributes",
                "position": 1510
            },
            {
                "img": "https://arxiv.org/html/2503.10633/extracted/6277902/figs/app/SM_pipeline.png",
                "caption": "",
                "position": 1514
            },
            {
                "img": "https://arxiv.org/html/2503.10633/x7.png",
                "caption": "Figure 14:Atlas prediction errors:Node names are visible by zooming-in. Correctly predicted edges and their target nodes are marked in green. Incorrect edge predictions and their target nodes are marked in red. Ground-truth edges that were not predicted are marked in black.",
                "position": 1519
            },
            {
                "img": "https://arxiv.org/html/2503.10633/x8.png",
                "caption": "",
                "position": 1523
            },
            {
                "img": "https://arxiv.org/html/2503.10633/x9.png",
                "caption": "Figure 15:Atlas prediction errors:Node names are visible by zooming-in. Correctly predicted edges and their target nodes are marked in green. Incorrect edge predictions and their target nodes are marked in red. Ground-truth edges that were not predicted are marked in black.",
                "position": 1528
            },
            {
                "img": "https://arxiv.org/html/2503.10633/extracted/6277902/figs/app/mistral.png",
                "caption": "Figure 16:Individual connected components - 3/5",
                "position": 1531
            },
            {
                "img": "https://arxiv.org/html/2503.10633/extracted/6277902/figs/app/resnet.png",
                "caption": "Figure 17:Individual connected components - 5/5",
                "position": 1535
            },
            {
                "img": "https://arxiv.org/html/2503.10633/extracted/6277902/figs/app/SD_XL.png",
                "caption": "",
                "position": 1539
            },
            {
                "img": "https://arxiv.org/html/2503.10633/extracted/6277902/figs/app/wav2vec.png",
                "caption": "",
                "position": 1541
            }
        ]
    },
    {
        "header": "Appendix EError analysis and visualizing",
        "images": []
    }
]