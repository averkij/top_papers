[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07346/extracted/6194399/figure/languages.png",
                "caption": "",
                "position": 130
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07346/x1.png",
                "caption": "Figure 1:BenchMAX evaluates diverse advanced capabilities of LLMs in multilingual context.",
                "position": 179
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Benchmark Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07346/x2.png",
                "caption": "Table 4:One example in rule-based instruction following task, which includes complex constraints. First, we enclose these constraints with special symbols and then use a machine translation system to translate from English to the target language. Finally, we reform the case structure by extracting the constraint from machine translation for human post-editing.",
                "position": 633
            },
            {
                "img": "https://arxiv.org/html/2502.07346/x2.png",
                "caption": "Figure 2:The construction process of BenchMAX involves three steps: Step 1) translating data from English to non-English; Step 2) post-editing each sample by three human annotators; Step 3) selecting the final translation version.",
                "position": 712
            },
            {
                "img": "https://arxiv.org/html/2502.07346/x3.png",
                "caption": "Figure 3:Flow chart illustrating the constraint extraction and preprocessing pipeline in the first step of our benchmark construction.",
                "position": 729
            }
        ]
    },
    {
        "header": "4Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07346/x4.png",
                "caption": "Figure 4:Performance of several models on the science reasoning task and the domain translation task across different languages. Models show unbalanced multilingual capabilities on these tasks.",
                "position": 1097
            },
            {
                "img": "https://arxiv.org/html/2502.07346/x5.png",
                "caption": "Figure 5:The proportion of tasks where larger models achieve a smaller GAP versus where smaller models perform better across different model families. larger models do not consistently have a smaller GAP, particularly in the Gemma2 family, where the smaller model outperforms the larger one in most cases.",
                "position": 1100
            },
            {
                "img": "https://arxiv.org/html/2502.07346/x6.png",
                "caption": "Figure 6:F1 scores of correct answers in other languages to English across six tasks. Using English correctness as a reference, we treat other languagesâ€™ correctness as predictions to measure their agreement with English. Results show high agreement between English and other languages on these tasks.",
                "position": 1165
            },
            {
                "img": "https://arxiv.org/html/2502.07346/x7.png",
                "caption": "Figure 7:Left:Spearman Correlations between the performance of two models within the same family across different languages.Right:Spearman Correlations between the performance of models on general translation and domain translation.",
                "position": 1385
            }
        ]
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07346/x8.png",
                "caption": "Figure 8:The performance differences between using human-translated data and machine-translated data. The upper figure depicts the performance difference of Llama3.1-70B on the rule-based instruction-following task, while the lower figure presents the performance difference for Qwen2.5-72B on the same task.",
                "position": 1433
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ACapability and Task Data Selection",
        "images": []
    },
    {
        "header": "Appendix BDataset Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07346/x9.png",
                "caption": "Figure 9:The radar charts visualize the performance of models on each subtask in different languages. Most model evaluated have imbalanced performance across different languages.",
                "position": 2500
            }
        ]
    },
    {
        "header": "Appendix CModel Information",
        "images": []
    },
    {
        "header": "Appendix DDetails about Prompt Templates",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07346/x10.png",
                "caption": "Table 10:The native-CoT prompts of the mathematical reasoning task.",
                "position": 2650
            },
            {
                "img": "https://arxiv.org/html/2502.07346/x11.png",
                "caption": "Table 11:The native-CoT prompts of the scientific reasoning task.",
                "position": 2654
            }
        ]
    },
    {
        "header": "Appendix EDetailed results",
        "images": []
    }
]