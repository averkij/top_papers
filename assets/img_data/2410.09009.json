[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.09009/x1.png",
                "caption": "Figure 1:SemanticSDSachieves superior compositional text-to-3d generation results over state-of-the-art baselines, particularly in generating multiple objects with diverse attibutes.",
                "position": 130
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.09009/x2.png",
                "caption": "Figure 2:Overview ofSemanticSDS, comprising of program-aided layout planning (top) and regional denoising with semantic map (bottom).",
                "position": 267
            },
            {
                "img": "https://arxiv.org/html/2410.09009/x3.png",
                "caption": "Figure 3:Illustration of our proposed object-specific view descriptor for global scene optimization.",
                "position": 377
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.09009/x4.png",
                "caption": "Figure 4:Qualitative comparisons of text-to-3D generation.Comparison results demonstrate thatSemanticSDSsynthesizes more precise and realistic multi-object scenes with better visual details, geometric expressiveness, and semantic consistency.",
                "position": 475
            },
            {
                "img": "https://arxiv.org/html/2410.09009/x5.png",
                "caption": "Figure 5:User study results.SemanticSDSis preferred 60% of the time by users than baseline methods.",
                "position": 499
            },
            {
                "img": "https://arxiv.org/html/2410.09009/x6.png",
                "caption": "Figure 6:Qualitative comparisons between without and with our program-aided layout planning.",
                "position": 508
            },
            {
                "img": "https://arxiv.org/html/2410.09009/x7.png",
                "caption": "Figure 7:Qualitative analysis. OurSemanticSDSprovides more precise and fine-grained control and our proposed object-specific view descriptor helps with better multi-view understanding.",
                "position": 527
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.09009/x8.png",
                "caption": "Figure 8:The prompt for scene-level decomposition in program-aided layout planning.",
                "position": 1205
            },
            {
                "img": "https://arxiv.org/html/2410.09009/x9.png",
                "caption": "Figure 9:The prompt for decomposing each object into complementary regions.",
                "position": 1208
            },
            {
                "img": "https://arxiv.org/html/2410.09009/x10.png",
                "caption": "Figure 10:The prompt for guiding GPT-4 as a human-aligned evaluator",
                "position": 1243
            },
            {
                "img": "https://arxiv.org/html/2410.09009/x11.png",
                "caption": "Figure 11:More synthesis results of multiple objects with ourSemanticSDS.",
                "position": 1263
            },
            {
                "img": "https://arxiv.org/html/2410.09009/x12.png",
                "caption": "Figure 12:More synthesis results of single object with diverse attributes with ourSemanticSDS.",
                "position": 1266
            }
        ]
    },
    {
        "header": "Appendix BMore Synthesis Results",
        "images": []
    }
]