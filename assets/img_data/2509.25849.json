[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25849/images/main_framework.png",
                "caption": "Figure 1:Illustration of our framework for allocating exploration budgets among tasks from computational resources. We model each task as an item with learning value and computational cost, then solve the allocation problem using Knapsack optimization.",
                "position": 139
            }
        ]
    },
    {
        "header": "2Preliminary",
        "images": []
    },
    {
        "header": "3Diagnosing Exploration in Homogeneous Budget Allocation",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25849/x1.png",
                "caption": "Figure 2:The ratio of effective gradients and zero gradients during training.",
                "position": 302
            },
            {
                "img": "https://arxiv.org/html/2509.25849/x2.png",
                "caption": "Figure 3:Exploration budget required to ensure non-zero gradients based on success rate. Note that success rates with in the same bins are grouped from real samples, which may not be symmetry, rendering the exploration budget may not be symmetry as the theory suggests.",
                "position": 370
            }
        ]
    },
    {
        "header": "4Proposed Approach: Knapsack-based Budget Allocation",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25849/x3.png",
                "caption": "Figure 4:The interplay between success rate, exploration budget and the value.",
                "position": 519
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25849/x4.png",
                "caption": "Figure 5:Distribution of exploration budgets allocated by knapsack-GRPO for Qwen2.5-Math-7B during training.",
                "position": 751
            },
            {
                "img": "https://arxiv.org/html/2509.25849/x5.png",
                "caption": "(a)DPSK-R1-Distill-1.5B",
                "position": 760
            },
            {
                "img": "https://arxiv.org/html/2509.25849/x5.png",
                "caption": "(a)DPSK-R1-Distill-1.5B",
                "position": 763
            },
            {
                "img": "https://arxiv.org/html/2509.25849/x6.png",
                "caption": "(b)Qwen3-4B",
                "position": 768
            },
            {
                "img": "https://arxiv.org/html/2509.25849/x7.png",
                "caption": "(c)Qwen2.5-Math-7B",
                "position": 773
            },
            {
                "img": "https://arxiv.org/html/2509.25849/x8.png",
                "caption": "(a)GRPO",
                "position": 783
            },
            {
                "img": "https://arxiv.org/html/2509.25849/x8.png",
                "caption": "(a)GRPO",
                "position": 786
            },
            {
                "img": "https://arxiv.org/html/2509.25849/x9.png",
                "caption": "(b)Knapsack-GRPO",
                "position": 791
            },
            {
                "img": "https://arxiv.org/html/2509.25849/x10.png",
                "caption": "Figure 8:Distribution of sample statuses after training.",
                "position": 804
            },
            {
                "img": "https://arxiv.org/html/2509.25849/x11.png",
                "caption": "Figure 9:Performance comparison under different exploration budgets.",
                "position": 817
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProof",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25849/x12.png",
                "caption": "Figure 10:Comparison of exactInfoGainand approximate formula.",
                "position": 1838
            }
        ]
    },
    {
        "header": "Appendix BExtensions",
        "images": []
    },
    {
        "header": "Appendix CExperiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25849/x13.png",
                "caption": "Figure 11:Learning dynamics of randomly selected prompts throughout training, comparing GRPO and Knapsack-GRPO. Each subplot shows the success rate evolution for a specific prompt.",
                "position": 1991
            },
            {
                "img": "https://arxiv.org/html/2509.25849/x14.png",
                "caption": "Figure 12:Evaluation performance of DPSK-R1-Distill-1.5B across training iterations.",
                "position": 2002
            },
            {
                "img": "https://arxiv.org/html/2509.25849/x15.png",
                "caption": "Figure 13:Evaluation performance of Qwen3-4B-Base across training iterations.",
                "position": 2005
            },
            {
                "img": "https://arxiv.org/html/2509.25849/x16.png",
                "caption": "Figure 14:Evaluation performance of Qwen3-4B-Instruct across training iterations.",
                "position": 2008
            },
            {
                "img": "https://arxiv.org/html/2509.25849/x17.png",
                "caption": "Figure 15:Evaluation performance of Qwen2.5-Math-7B across training iterations.",
                "position": 2011
            },
            {
                "img": "https://arxiv.org/html/2509.25849/x18.png",
                "caption": "Figure 16:Effect of the fallback strategy. Without it, exploration budgets are disproportionately allocated to prompts with at least one successful trial, while unsolved tasks are largely ignored.",
                "position": 2138
            },
            {
                "img": "https://arxiv.org/html/2509.25849/x19.png",
                "caption": "Figure 17:Ablation study on the impact ofNlowN_{\\operatorname{low}}andNupN_{\\operatorname{up}}constraints within the knapsack optimization framework.",
                "position": 2147
            },
            {
                "img": "https://arxiv.org/html/2509.25849/x20.png",
                "caption": "Figure 18:Performance of Qwen2.5-Math-7B relative to the number of exploration iterations, demonstrating how effectively the total computation budget is converted into performance gains.",
                "position": 2178
            },
            {
                "img": "https://arxiv.org/html/2509.25849/x21.png",
                "caption": "Figure 19:Performance of Qwen2.5-Math-7B as a function of the number of LLM gradient updates. This figure validates that effective gradients, derived from either dynamic sampling or the knapsack-based approach, lead to greater performance gains for the same number of updates.",
                "position": 2181
            }
        ]
    },
    {
        "header": "Appendix DAdditional Results",
        "images": []
    }
]