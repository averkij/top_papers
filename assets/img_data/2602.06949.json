[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06949/x1.png",
                "caption": "Figure 1:DreamDojooverview.DreamDojoacquires comprehensive physical knowledge from large-scale human datasets by utilizing latent actions as unified labels. After post-training and distillation on the target robots, our model can predict the future world in real time with continuous action controls.DreamDojocan robustly generalize to various objects and environments, facilitating large-scale policy evaluation without real-world deployment. It also enables live teleoperation and online model-based planning.",
                "position": 167
            }
        ]
    },
    {
        "header": "3Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06949/x2.png",
                "caption": "Figure 2:Distribution analysis of DreamDojo-HV.(a)Distribution of the scenarios and random examples from the most frequent categories.(b)[Left]: Distribution of subtask numbers within each video. Most videos involve long-horizon tasks that require multiple interactions to accomplish. [Right]: Representative skills in DreamDojo-HV and their frequencies. Our dataset covers a wide range of interaction types beyond pick-and-place.(c)Visualization of skill verbs and object names based on their frequency of occurrence in language annotations.",
                "position": 198
            },
            {
                "img": "https://arxiv.org/html/2602.06949/x3.png",
                "caption": "Figure 3:Latent action model.[Left]: The information bottleneck design of our latent action model enforces action disentanglement, producing a continuous latent vector that represents actions between frames. [Right]: We retrieve and group the frame pairs from different datasets that share the most similar latent actions. The embodiments are performing the same actions despite the significant differences in context.",
                "position": 739
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06949/x4.png",
                "caption": "Figure 4:Benchmark visualization.We rigorously construct six evaluation benchmarks that reflect the diverse scenarios and actions present in human datasets, while being out-of-distribution for the robot training datasets.",
                "position": 848
            },
            {
                "img": "https://arxiv.org/html/2602.06949/x5.png",
                "caption": "(a)Real vs.DreamDojosuccess rates.",
                "position": 2416
            },
            {
                "img": "https://arxiv.org/html/2602.06949/x5.png",
                "caption": "(a)Real vs.DreamDojosuccess rates.",
                "position": 2419
            },
            {
                "img": "https://arxiv.org/html/2602.06949/x6.png",
                "caption": "(b)Model-based planning results.",
                "position": 2425
            },
            {
                "img": "https://arxiv.org/html/2602.06949/x7.png",
                "caption": "Figure 6:Live teleoperation.We can teleoperate a virtual G1 robot using the PICO VR controller in real time.",
                "position": 2432
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix AAcknowledgement",
        "images": []
    },
    {
        "header": "Appendix BRelated Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06949/x8.png",
                "caption": "Figure 7:Web UI for human preference evaluation.To intuitively compare physics correctness and action controls, we devise a web UI that can display the ground-truth video alongside two videos generated by two different models simultaneously. The order of the generated videos will be randomly switched to avoid any potential bias.",
                "position": 2480
            }
        ]
    },
    {
        "header": "Appendix CHuman Preference Evaluation",
        "images": []
    },
    {
        "header": "Appendix DAdditional Visualizations",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06949/x9.png",
                "caption": "Figure 8:Qualitative comparison of human data pretraining effects.Through pretraining on diverse human interaction data,DreamDojoacquires a generalizable understanding of general physics, resulting in more realistic simulation for objects that are unseen in the target robot dataset.",
                "position": 2507
            },
            {
                "img": "https://arxiv.org/html/2602.06949/x10.png",
                "caption": "Figure 9:Qualitative comparison of our design choices.Applying all our techniques results in the best capabilities for object modeling and action following.",
                "position": 2510
            },
            {
                "img": "https://arxiv.org/html/2602.06949/x11.png",
                "caption": "Figure 10:Long-horizon rollouts for 1 minute.Note that the teacher model generates videos in a chunk-wise manner and operates at a speed (2.72 FPS) that is 4Ã—\\timesslower than that of the student model (10.81 FPS).",
                "position": 2544
            },
            {
                "img": "https://arxiv.org/html/2602.06949/x12.png",
                "caption": "Figure 11:The advantage of student context.The student model exhibits better consistency in handling occlusions and camera shifts, while the teacher model has no way to ensure that due to the missing context.",
                "position": 2547
            },
            {
                "img": "https://arxiv.org/html/2602.06949/x13.png",
                "caption": "Figure 12:Diversity of DreamDojo-HV.We visualize more samples from the curated DreamDojo-HV dataset, which encompasses extremely diverse actions and tool-using scenarios.",
                "position": 2550
            },
            {
                "img": "https://arxiv.org/html/2602.06949/figs/inlab.png",
                "caption": "(a)Post-training PSNR curves on In-lab Eval.",
                "position": 2553
            },
            {
                "img": "https://arxiv.org/html/2602.06949/figs/inlab.png",
                "caption": "(a)Post-training PSNR curves on In-lab Eval.",
                "position": 2556
            },
            {
                "img": "https://arxiv.org/html/2602.06949/figs/egodex.png",
                "caption": "(b)Post-training PSNR curves on EgoDex Eval.",
                "position": 2562
            },
            {
                "img": "https://arxiv.org/html/2602.06949/x14.png",
                "caption": "Figure 14:Value model estimation.We visualize the estimated value and the ground-truth value of two representative episodes. Our value model reliably estimates the number of steps remaining to complete the current subtask.",
                "position": 2569
            },
            {
                "img": "https://arxiv.org/html/2602.06949/x14.png",
                "caption": "",
                "position": 2572
            },
            {
                "img": "https://arxiv.org/html/2602.06949/x15.png",
                "caption": "",
                "position": 2577
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]