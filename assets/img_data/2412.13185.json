[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.13185/x1.png",
                "caption": "Figure 1:2D-conditioned human motion generation.Given an image representing the target scene and a text prompt describing the desired motion, we generate a motion sequence that aligns with the text description and projects naturally onto the scene image.\nThis generated motion then serves as the control signal for the subsequent video generation tasks.",
                "position": 69
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.13185/x2.png",
                "caption": "Figure 2:Overview.The text prompt and background scene image are encoded by the CLIP and DINO encoders, and incorporated into the model via in-context conditioning. The AdaLN layer receives the diffusion timestep as input. Our multi-conditional transformer model then generates a human motion sequence through a diffusion denoising process, aligning the generated motion with both input conditions.",
                "position": 145
            }
        ]
    },
    {
        "header": "3Humans-in-Context Motion Dataset",
        "images": []
    },
    {
        "header": "4Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.13185/x3.png",
                "caption": "Figure 3:Affordance-aware human generation.Our model generates human poses consistent with both text prompts and scene context, such as standing on a cliff. It also supports complex human-scene interactions, including activities like petting a dog.",
                "position": 348
            },
            {
                "img": "https://arxiv.org/html/2412.13185/x4.png",
                "caption": "Figure 4:Motion generation with large dynamics.Our results show motion sequences that are accurately placed and move within scenes, such as playing tennis, enabling the generation of complex human activities that are challenging for video generation models.",
                "position": 352
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.13185/x5.png",
                "caption": "Figure 5:Comparison to state-of-the-art.MDMandSceneDiffproduces implausible poses,MLDgenerates mismatched motion with the scene, andHUMANISEgenerates static poses.\nOur method generates coherent motion aligned with both the scene and text prompts.",
                "position": 408
            },
            {
                "img": "https://arxiv.org/html/2412.13185/x6.png",
                "caption": "Figure 6:Motion-guided human video generation.Our approach generates scene-compatible motion sequences from a scene image and text prompt, which are then used to animate a reference human using Champ[60]or Gen-3[11]. The generated motion ensures accurate human shapes and smooth motion in the resulting videos, outperforming SVD[5]in preserving human geometry and motion consistency.",
                "position": 516
            }
        ]
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]