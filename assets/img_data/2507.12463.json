[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12463/x1.png",
                "caption": "Figure 1:We proposeMMHU, a large-scale dataset for human behavior understanding.\nWe collected 57k human instances with diverse behaviors such as playing mobile phone, holding object, or using mobility devices, from diverse scenes such as in the city, school, park, and alley. We provide rich annotations including motion and trajectory, text descriptions for human motions, and recognize the behaviors that are critical to driving safety.",
                "position": 149
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3The MMHU Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12463/extracted/6623555/figs/pipe.png",
                "caption": "Figure 2:Data Collection and Annotation.(Left) We collect data from three sources: the Waymo dataset, the YouTube videos, and the self-collected or paid driving videos. (Right) We demonstrate the annotation pipeline; we first filter and cut the raw videos based on the rough human detection results. Then we reconstruct the SMPL motion for each detected frame. The missing frames are further recovered by an interpolation procedure. For the labeling of text descriptions, we leverage low-level text as a bridge between the SMPL parameters and the semantic label. Then we generate the high-level text from the low-level ones. We recognize the critical behavior lists leveraging a VLM, based on the visual and text information. We label the behaviors for each human instance using a VLM that is fine-tuned on a small human-labeled subset.",
                "position": 232
            },
            {
                "img": "https://arxiv.org/html/2507.12463/x2.png",
                "caption": "Figure 3:Visualization of the MMHU dataset.For each human instance, the first line shows the video frames that is sampled from the video clips. The human instance is highlighted using a red bounding box. We crop and zoom-in the human instance for a clearer view. Under each of the frames shows the corresponding human motion rendered as mesh, followed by the text description for the human motion and the behavior labels.",
                "position": 256
            },
            {
                "img": "https://arxiv.org/html/2507.12463/extracted/6623555/figs/statistics.jpg",
                "caption": "Figure 4:Statics of MMHU.The average duration of motion sequences is 3s. The most common behavior is crossing the street, while the rarest behavior is using a wheelchair. Behavior definition, please refer to Sec.3.4.",
                "position": 280
            }
        ]
    },
    {
        "header": "4Tasks",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12463/x3.png",
                "caption": "Figure 5:Qualitative comparison of Motion Generation.The baseline model (MotionDiffuse, MD) is not capable to generate proper motions in driving scenes. After fine-tuning on MMHU(second row), the model demonstrates the ability to generate human motions in autonomous driving scenarios.",
                "position": 1033
            }
        ]
    },
    {
        "header": "6Conclusion and Limitation",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendices",
        "images": []
    },
    {
        "header": "Appendix ADataset Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12463/extracted/6623555/figs/annotation-interface.jpg",
                "caption": "Figure 6:Annotation Interface.",
                "position": 2591
            },
            {
                "img": "https://arxiv.org/html/2507.12463/x4.png",
                "caption": "",
                "position": 2611
            },
            {
                "img": "https://arxiv.org/html/2507.12463/x5.png",
                "caption": "",
                "position": 2704
            },
            {
                "img": "https://arxiv.org/html/2507.12463/x6.png",
                "caption": "",
                "position": 2727
            },
            {
                "img": "https://arxiv.org/html/2507.12463/x7.png",
                "caption": "",
                "position": 2750
            }
        ]
    },
    {
        "header": "Appendix BExperiments",
        "images": []
    }
]