[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background and Motivation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.22437/2602.22437v1/x1.png",
                "caption": "Figure 1:The DTensor for flexible communication and computation. Here shows an example of DTensors executing a sharded matrix multiplication on a device. The darken part in each DTensor indicates the materialized local tensor on this device.",
                "position": 173
            },
            {
                "img": "https://arxiv.org/html/2602.22437/2602.22437v1/x2.png",
                "caption": "Figure 2:The fundamental overhead in FSDP2. (AllGather is shown; ReduceScatter is a reverse process.)",
                "position": 243
            }
        ]
    },
    {
        "header": "3Overview",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.22437/2602.22437v1/x3.png",
                "caption": "Figure 3:veScale-FSDP overview.",
                "position": 257
            }
        ]
    },
    {
        "header": "4RaggedShard for Flexibility",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.22437/2602.22437v1/x4.png",
                "caption": "Figure 4:Flexibility comparison of different sharding formats.",
                "position": 278
            },
            {
                "img": "https://arxiv.org/html/2602.22437/2602.22437v1/x5.png",
                "caption": "Figure 5:Composability ofRaggedShardwith existing evenShardfor 2D parallelism like FSDP×\\timesEP (Expert Parallel).",
                "position": 293
            }
        ]
    },
    {
        "header": "5Grouped RaggedShard for Performance",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.22437/2602.22437v1/x6.png",
                "caption": "Figure 6:Grouped communication ofRaggedShardDTensors.",
                "position": 311
            },
            {
                "img": "https://arxiv.org/html/2602.22437/2602.22437v1/x7.png",
                "caption": "Figure 7:Distributed Buffer (DBuffer) for high performance communication. A 2D DBuffer for AllGather’ing parameters is shown; Reversely, a 2D DBuffer redistributing from (Partial,Partial) to (Replicate,Shard) implements 2D gradient reduction with ReduceScatter and AllReduce.",
                "position": 489
            }
        ]
    },
    {
        "header": "6Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.22437/2602.22437v1/x8.png",
                "caption": "Figure 8:FSDP training performance.\nTop row: normalized aggregate throughput (tokens/s). Bottom row: peak per-GPU memory (GB). We sweep FSDP (ZeRO-3) at 128/256 GPUs and HSDP with 2- and 4-way replication (2*256, 4*256 GPUs).",
                "position": 561
            },
            {
                "img": "https://arxiv.org/html/2602.22437/2602.22437v1/x9.png",
                "caption": "(a)",
                "position": 581
            },
            {
                "img": "https://arxiv.org/html/2602.22437/2602.22437v1/x9.png",
                "caption": "(a)",
                "position": 584
            },
            {
                "img": "https://arxiv.org/html/2602.22437/2602.22437v1/x10.png",
                "caption": "(b)",
                "position": 590
            },
            {
                "img": "https://arxiv.org/html/2602.22437/2602.22437v1/x11.png",
                "caption": "(c)",
                "position": 596
            },
            {
                "img": "https://arxiv.org/html/2602.22437/2602.22437v1/x12.png",
                "caption": "(a)",
                "position": 627
            },
            {
                "img": "https://arxiv.org/html/2602.22437/2602.22437v1/x12.png",
                "caption": "(a)",
                "position": 630
            },
            {
                "img": "https://arxiv.org/html/2602.22437/2602.22437v1/x13.png",
                "caption": "(b)",
                "position": 636
            },
            {
                "img": "https://arxiv.org/html/2602.22437/2602.22437v1/x14.png",
                "caption": "(a)",
                "position": 721
            },
            {
                "img": "https://arxiv.org/html/2602.22437/2602.22437v1/x14.png",
                "caption": "(a)",
                "position": 724
            },
            {
                "img": "https://arxiv.org/html/2602.22437/2602.22437v1/x15.png",
                "caption": "(b)",
                "position": 730
            }
        ]
    },
    {
        "header": "7Lessons Learned",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "9Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]