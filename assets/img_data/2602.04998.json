[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04998/x1.png",
                "caption": "Figure 1:Performance of Qwen3-0.6B fine-tuned on mathematical reasoning tasks across learning rates. Different methods reach a similar performance level once the learning rate is properly tuned.",
                "position": 168
            },
            {
                "img": "https://arxiv.org/html/2602.04998/x2.png",
                "caption": "Figure 2:Frequency of advanced LoRA-based PEFT studies, categorized by whether learning rate or batch size tuning was applied and whether comparisons with vanilla LoRA across different ranks were conducted.\nCrucially, only one of the surveyed works simultaneously covered all three dimensions.\nRefer to Appendix Sec.Afor detailed data counts.",
                "position": 193
            },
            {
                "img": "https://arxiv.org/html/2602.04998/x3.png",
                "caption": "Figure 3:Overview of our considered PEFT methods:\n(a) Vanilla LoRA (Sec.3.1), (b) three representative initialization variants (Sec.3.2), and (c) one architectural modification (Sec.3.3).",
                "position": 225
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Background: LoRA PEFT Methods",
        "images": []
    },
    {
        "header": "4Learning Rate Matters, Really",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04998/x4.png",
                "caption": "(a)Mathematical Reasoning",
                "position": 719
            },
            {
                "img": "https://arxiv.org/html/2602.04998/x4.png",
                "caption": "(a)Mathematical Reasoning",
                "position": 722
            },
            {
                "img": "https://arxiv.org/html/2602.04998/x5.png",
                "caption": "(b)Code Generation",
                "position": 728
            },
            {
                "img": "https://arxiv.org/html/2602.04998/x6.png",
                "caption": "(a)Mathematical Reasoning",
                "position": 738
            },
            {
                "img": "https://arxiv.org/html/2602.04998/x6.png",
                "caption": "(a)Mathematical Reasoning",
                "position": 741
            },
            {
                "img": "https://arxiv.org/html/2602.04998/x7.png",
                "caption": "(b)Code Generation",
                "position": 747
            },
            {
                "img": "https://arxiv.org/html/2602.04998/x8.png",
                "caption": "Figure 6:Distributions of the ratios of the top loss Hessian eigenvalues relative to LoRA for Query projection matrices across Transformer layers. Dashed lines indicate the medians. With an adapter rank ofr=128r=128, PiSSA consistently exhibits the largest eigenvalues. In contrast, MiLoRA and Init[AB] exhibit eigenvalues slightly larger than those of LoRA on Qwen, yet remain comparable to LoRA on Gemma and Llama. Detailedλmax\\lambda_{\\max}values and results on other matrix types are provided in Appendix Sec.G.2.",
                "position": 796
            }
        ]
    },
    {
        "header": "5Understanding the Optimal Learning Rate via Hessian Analysis",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Contents",
        "images": []
    },
    {
        "header": "Appendix AComprehensive Study of Hyperparameters in Prior Work",
        "images": []
    },
    {
        "header": "Appendix BFine-tuning Implementation Details",
        "images": []
    },
    {
        "header": "Appendix COn LoRA Scaling Factor",
        "images": []
    },
    {
        "header": "Appendix DAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04998/x9.png",
                "caption": "Figure 7:Best achievable performance of LoRA and its variants across different training sample sizes on mathematical reasoning with Gemma-3-1B (r=128,B=64r=128,B=64). Once the learning rate is properly tuned, all methods exhibit nearly identical improvement trends as the number of training samples increases. Results are reported with mean and standard deviation over three runs.",
                "position": 4157
            },
            {
                "img": "https://arxiv.org/html/2602.04998/x10.png",
                "caption": "(a)Mathematical Reasoning",
                "position": 4168
            },
            {
                "img": "https://arxiv.org/html/2602.04998/x10.png",
                "caption": "(a)Mathematical Reasoning",
                "position": 4171
            },
            {
                "img": "https://arxiv.org/html/2602.04998/x11.png",
                "caption": "(b)Code Generation",
                "position": 4177
            }
        ]
    },
    {
        "header": "Appendix EDetails of Hyperparameter Search Results",
        "images": []
    },
    {
        "header": "Appendix FExample Model Responses",
        "images": []
    },
    {
        "header": "Appendix GHessian Computation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04998/x12.png",
                "caption": "Figure 10:Approximately 500 training samples are sufficient for stable Hessian estimation. The figure reports the estimatedλmax\\lambda_{\\max}for PiSSA\non the first Query projection matrix of Qwen3-0.6B (r=128r=128). We track these estimates across varying sample sizes (NN) from the MetaMathQA dataset, usingN=10​kN=10kas the reference baseline.\nResults represent the mean and standard deviation over 5 randomly selected subsets for eachNN.",
                "position": 6322
            },
            {
                "img": "https://arxiv.org/html/2602.04998/x13.png",
                "caption": "Figure 11:Heatmap of the top eigenvalues of theQuery projection matrixacross Transformer layers, i.e.,λmaxQ,i\\lambda_{\\max}^{Q,i}fori=1,…,Li=1,\\ldots,L, for Qwen3-0.6B on a mathematical reasoning task (r=128r=128). All methods exhibit similar distributional patterns, with PiSSA consistently exhibiting significantly larger values compared to vanilla LoRA.",
                "position": 6556
            },
            {
                "img": "https://arxiv.org/html/2602.04998/x14.png",
                "caption": "Figure 12:Heatmap of the top eigenvalues of theKey projection matrixacross Transformer layers, i.e.,λmaxK,i\\lambda_{\\max}^{K,i}fori=1,…,Li=1,\\ldots,L, for Qwen3-0.6B on a mathematical reasoning task (r=128r=128).",
                "position": 6559
            },
            {
                "img": "https://arxiv.org/html/2602.04998/x15.png",
                "caption": "Figure 13:Heatmap of the top eigenvalues of theValue projection matrixacross Transformer layers, i.e.,λmaxV,i\\lambda_{\\max}^{V,i}fori=1,…,Li=1,\\ldots,L, for Qwen3-0.6B on a mathematical reasoning task (r=128r=128).",
                "position": 6562
            },
            {
                "img": "https://arxiv.org/html/2602.04998/x16.png",
                "caption": "Figure 14:Heatmap of the top eigenvalues of theOutput projection matrixacross Transformer layers, i.e.,λmaxO,i\\lambda_{\\max}^{O,i}fori=1,…,Li=1,\\ldots,L, for Qwen3-0.6B on a mathematical reasoning task (r=128r=128).",
                "position": 6565
            },
            {
                "img": "https://arxiv.org/html/2602.04998/x17.png",
                "caption": "Figure 15:Heatmap of the top eigenvalues of theGate projection matrixacross Transformer layers, i.e.,λmaxGate,i\\lambda_{\\max}^{\\text{Gate},i}fori=1,…,Li=1,\\ldots,L, for Qwen3-0.6B on a mathematical reasoning task (r=128r=128).",
                "position": 6568
            },
            {
                "img": "https://arxiv.org/html/2602.04998/x18.png",
                "caption": "Figure 16:Heatmap of the top eigenvalues of theUp projection matrixacross Transformer layers, i.e.,λmaxUp,i\\lambda_{\\max}^{\\text{Up},i}fori=1,…,Li=1,\\ldots,L, for Qwen3-0.6B on a mathematical reasoning task (r=128r=128).",
                "position": 6571
            },
            {
                "img": "https://arxiv.org/html/2602.04998/x19.png",
                "caption": "Figure 17:Heatmap of the top eigenvalues of theDown projection matrixacross Transformer layers, i.e.,λmaxDown,i\\lambda_{\\max}^{\\text{Down},i}fori=1,…,Li=1,\\ldots,L, for Qwen3-0.6B on a mathematical reasoning task (r=128r=128).",
                "position": 6574
            }
        ]
    },
    {
        "header": "Appendix HLimitations and Future Work",
        "images": []
    }
]