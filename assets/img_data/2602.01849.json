[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01849/x1.png",
                "caption": "Figure 1:Illustrative example of text generation using (a) masked diffusion models and (b) our self-rewarding SMC framework. Here, ‚ÄòM‚Äô represents[ùôºùô∞ùöÇùô∫]\\mathtt{[MASK]}tokens and ‚ÄòResa.‚Äô denotes resampling. SMC maintains multiple diffusion processes, calledparticles, to explore the sampling trajectories in parallel. At each iteration, we take three steps:resample,propagate, andre-weight, to perform as an interactive optimization process. Importantly, traditional diffusion sampling only considers token-level confidence, while our algorithm uses the trajectory-level confidence as importance weights, calculated using Eq.¬†(13), to select globally confident outputs.",
                "position": 238
            }
        ]
    },
    {
        "header": "3Self-Rewarding Sequential Monte Carlo",
        "images": []
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01849/x2.png",
                "caption": "Figure 2:Generative perplexity (‚Üì\\downarrow) comparison of our self-rewarding SMC and the corresponding baselines.",
                "position": 641
            },
            {
                "img": "https://arxiv.org/html/2602.01849/x3.png",
                "caption": "Figure 3:Comparison results of LLaDA 1.5 and Dream-7B using different numbers of particles on four tasks. Each marker denotes the empirical result while the dashed curves indicate first-order polynomial fits used solely to illustrating overall trends asNNincreases.",
                "position": 644
            }
        ]
    },
    {
        "header": "5Discussion and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01849/x4.png",
                "caption": "Figure 5:Effect of sampling temperatureœÑ\\tauon model performance across different benchmarks. We report the accuracy of LLaDA-1.5 and Dream-7B on MBPP and MATH datasets as the temperature varies uniformly from 0.0 to 1.0. The blue circles represent the baseline with standard parallel decoding, while the red stars denote the results using our SR-SMC withN=4N=4particles. SR-SMC consistently demonstrates better robustness across the entire temperature range. Notably, while the baseline performance of Dream-7B collapses at low temperatures (startig from 0.1) due to repetition, SR-SMC maintains stable and high accuracy by effectively exploring the generative space through particle re-weighting and resampling.",
                "position": 909
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASequential Monte Carlo for Diffusion Reverse Process",
        "images": []
    },
    {
        "header": "Appendix BLimitation and Future Work",
        "images": []
    },
    {
        "header": "Appendix CAdditional Experiment",
        "images": []
    }
]