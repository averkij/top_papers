[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08829/x1.png",
                "caption": "",
                "position": 101
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08829/x2.png",
                "caption": "Figure 2:Architecture of InfiniteVL.Visual inputs (images, videos, real-time streams) are embedded by a naïve-resolution ViT and text by a tokenizer, then concatenated and processed by a stack of Hybrid Blocks. Each Hybrid SWA module for local, linear-time modeling and three Gated DeltaNet layers that read from and write to a fixed-size memory cache to capture long-range dependencies, enabling context-length–agnostic inference with constant throughput and GPU memory.",
                "position": 177
            },
            {
                "img": "https://arxiv.org/html/2512.08829/x3.png",
                "caption": "Figure 3:Three-stage training strategy of InfiniteVL.The student model is initialized from a full-attention teacher, replacing its attention layers with Gated DeltaNet while inheriting all remaining parameters. Stage I performs layer-wise and end-to-end distillation to align Gated DeltaNet with the teacher. Stage II applies large-scale supervised fine-tuning on diverse multimodal instruction data to build strong instruction-following and reasoning abilities. Stage III conducts long-sequence SFT with additional high-resolution, document, and video QA/Caption data to enhance length generalization.",
                "position": 189
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08829/x4.png",
                "caption": "Figure 4:Length generalization and inference efficiency of InfiniteVL:(a–b)On Video-MME and LongVideoBench, InfiniteVL delivers stable performance as the number of input frames increases, whereas Qwen2.5-VL-3B(SWA) degrades once the context length exceeds its attention window.(c)InfiniteVL attains over3.6×3.6\\timeslower per-token latency than a transformer-based VLM of similar size.(d)InfiniteVL maintains real-time streaming inference at≈24\\approx 24FPS with 274 tokens per frame, while Qwen2.5-VL-3B rapidly slows down and eventually runs out of memory.",
                "position": 611
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Analysis of Cache Evolution under Streaming Inputs",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08829/x5.png",
                "caption": "Figure 5:L2 norm of the Linear-layer memory cache versus input frame index:the norm increases rapidly at the beginning and then stabilizes.",
                "position": 1037
            }
        ]
    },
    {
        "header": "7Comprehensive Evaluation Setup Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08829/x6.png",
                "caption": "Figure 6:Examples of fundamental Visual-Language Understanding",
                "position": 1060
            },
            {
                "img": "https://arxiv.org/html/2512.08829/x7.png",
                "caption": "Figure 7:Examples of Long-Term Streaming Understanding Capability",
                "position": 1063
            },
            {
                "img": "https://arxiv.org/html/2512.08829/x8.png",
                "caption": "Figure 8:Examples of Long-Term Streaming Understanding Capability",
                "position": 1066
            }
        ]
    },
    {
        "header": "8Case Study across multiple scenarios",
        "images": []
    }
]