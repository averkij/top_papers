[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.14086/x1.png",
                "caption": "Figure 1:3D-LLMs frequently overfit to dataset co-occurrence biases (e.g., white pillows), causing grounding failures.\nWe mitigate these biases through targeted counterfactual edits in 3D scenes and construct aligned QA pairs to strengthen the model’s grounding ability.",
                "position": 97
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.14086/x2.png",
                "caption": "Figure 2:Overview ofDEER-3D. When a grounding error is detected,DEER-3Dperforms targeted visual edits.\nGiven a natural-language instruction, the framework (a) decomposes it into atomic predicates, (b) diagnoses the specific error, and (c) applies predicate-level visual edits. Aligned question–answer pairs are then created to explicitly supervise the failed predicate. Finally, the model (d) iteratively retrains on these counterfactual examples to progressively improve grounding accuracy.",
                "position": 140
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.14086/x3.png",
                "caption": "Figure 3:Examples of targeted visual edits for different grounding errors. Each edit generates aligned question–answer pairs, precisely supervising the model’s visual grounding ability.",
                "position": 236
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.14086/x4.png",
                "caption": "(a)Edit scaling effects.",
                "position": 603
            },
            {
                "img": "https://arxiv.org/html/2511.14086/x4.png",
                "caption": "(a)Edit scaling effects.",
                "position": 606
            },
            {
                "img": "https://arxiv.org/html/2511.14086/x5.png",
                "caption": "(b)Error counts across iterations.",
                "position": 611
            },
            {
                "img": "https://arxiv.org/html/2511.14086/x6.png",
                "caption": "(c)Different types of visual edits.",
                "position": 616
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledgements",
        "images": []
    },
    {
        "header": "Appendix AMore Details about Deer3D Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.14086/x7.png",
                "caption": "Figure 5:Distribution of semantic predicate types from our instruction analysis.",
                "position": 1102
            }
        ]
    },
    {
        "header": "Appendix BMore Details about Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.14086/x8.png",
                "caption": "Figure 6:Semantic error detection followed by targeted text augmentation to disambiguate similar objects.",
                "position": 1317
            }
        ]
    },
    {
        "header": "Appendix CLimitation and Future Work",
        "images": []
    }
]