[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.13154/x1.png",
                "caption": "",
                "position": 102
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "34DNeX-10M",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.13154/x2.png",
                "caption": "Figure 2:Visualization of 4DNeX-10M Dataset.Our dataset spans a wide range of dynamic scenarios, including indoor, outdoor, close-range, far-range, static, high-speed, and human-centric scenes. The word cloud summarizes common visual concepts captured in the dataset, while the 4D point clouds and camera trajectories demonstrate the spatial precision of our pseudo-annotations.",
                "position": 186
            },
            {
                "img": "https://arxiv.org/html/2508.13154/x3.png",
                "caption": "Figure 3:Data Curation Pipeline.The video data is collected from various sources and then selected by video filtering during Data Cleaning. The selected data is captioned via LLaVA-Next-Video model in Video Captioning. The selected data is processed and finally filtered out the video with high-quality annotation during 3D/4D Annotation. Data statistics is also provided in bottom right.",
                "position": 238
            }
        ]
    },
    {
        "header": "44DNeX",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.13154/x4.png",
                "caption": "Figure 4:Comparison of fusion strategies for joint RGB and XYZ modeling.We explore five fusion strategies and analyze their impact on model compatibility and cross-modal alignment.",
                "position": 260
            },
            {
                "img": "https://arxiv.org/html/2508.13154/x5.png",
                "caption": "Figure 5:Comparison of spatial fusion strategies.We compare frame-, height-, and width-wise fusion in terms of the interaction distance between RGB and XYZ tokens.",
                "position": 340
            },
            {
                "img": "https://arxiv.org/html/2508.13154/x6.png",
                "caption": "Figure 6:Overview of 4DNeX.Given a single RGB image and an initialized XYZ map, 4DNeX encodes both inputs with a VAE encoder and fuses them via width-wise concatenation. The fused latent, combined with a noise latent and a guided mask, is processed by a LoRA-tuned Wan-DiT model to jointly generate RGB and XYZ videos. A lightweight post-optimization step recovers camera parameters and depth maps from the predicted outputs.",
                "position": 348
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.13154/x7.png",
                "caption": "Figure 7:Generated RGB and XYZ sequences from single-image input.Each pair of rows shows the output RGB video and its corresponding XYZ sequence.",
                "position": 454
            },
            {
                "img": "https://arxiv.org/html/2508.13154/x8.png",
                "caption": "Figure 8:Novel-view video results on in-the-wild data.",
                "position": 458
            },
            {
                "img": "https://arxiv.org/html/2508.13154/x9.png",
                "caption": "Figure 9:Qualitative comparison.Our method generates results with higher consistency, better aesthetics, and notably larger motion than existing 4D generation methodsZhao et al. (2023); Yu et al. (2024a); Liu et al. (2025).",
                "position": 461
            },
            {
                "img": "https://arxiv.org/html/2508.13154/x10.png",
                "caption": "Figure 10:Ablation study on fusion strategies.We compare channel-wise (a), batch-wise (b), frame-wise (c), height-wise (d), and our width-wise fusion (e) for RGB and XYZ inputs.",
                "position": 464
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ADetails of User Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.13154/x11.png",
                "caption": "Figure A:User study interface.Participants were shown an input prompt and two generated videos from different methods. They were asked to compare the results based onConsistency,DynamicsandAesthetics. Each question allowed skipping if the difference was hard to judge.",
                "position": 1674
            },
            {
                "img": "https://arxiv.org/html/2508.13154/x12.png",
                "caption": "Figure B:Architecture of the Cross-Domain Self-Attention (CDSA) module.The CDSA block is inserted between self-attention and cross-attention layers to facilitate bidirectional interaction between RGB and XYZ modalities. We explore two variants: theFull Version, where all tokens interact densely, and theSparse Version, where attention is restricted to spatially corresponding token pairs. This design enables effective cross-modal alignment with different trade-offs in efficiency and performance.",
                "position": 1679
            }
        ]
    },
    {
        "header": "Appendix BDetails of VBench Metrics",
        "images": []
    }
]