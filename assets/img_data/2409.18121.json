[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.18121/x1.png",
                "caption": "Figure 1:Robot See Robot Do.To visually imitate articulated object motion RSRD first reconstructs a part-aware feature field.\nGiven an input demonstration video, we then track the object part motion using the feature field.\nNext, the robot recognizes the object in its workspace and plans a bimanual trajectory to achieve the demonstrated object motion.",
                "position": 124
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.18121/extracted/5882485/figures/rsrd_results.png",
                "caption": "Figure 2:4D Reconstruction of Articulated Objects. Keyframes from the motion trajectories overlaid over monocular RGB demonstrations with parts colorized, and along with two viewpoints.",
                "position": 152
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.18121/x2.png",
                "caption": "Figure 3:4D Differentiable Part Models (4D-DPM).Left: DINO features and depth are rendered from per-timestep optimizable part pose parameters, and compared with extracted DINO features and monocular depth from the input frame.Right: an ARAP loss penalizes gaussians from deviating too far from their initial configuration with respect to neighbors. Together these losses flow backwards into the part poses and are optimized with gradient descent to recover 3D part motion.",
                "position": 191
            }
        ]
    },
    {
        "header": "3Problem and Assumptions",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.18121/x3.png",
                "caption": "Figure 4:Hand Alignment: RSRD uses HaMeR[74]to detect and align human hand poses to the demonstrations.\nDetections are used to rank part pairs for grasping (Sec4.3).",
                "position": 219
            },
            {
                "img": "https://arxiv.org/html/2409.18121/x4.png",
                "caption": "Figure 5:ARAP Ablation.ARAP is a simple but effective prior for improving 3D motion recovery by preventing small or under-observed parts from drifting.",
                "position": 238
            },
            {
                "img": "https://arxiv.org/html/2409.18121/x5.png",
                "caption": "Figure 6:Example Robot Executions. Arrows indicate direction of motion.",
                "position": 298
            },
            {
                "img": "https://arxiv.org/html/2409.18121/x5.png",
                "caption": "Figure 6:Example Robot Executions. Arrows indicate direction of motion.",
                "position": 301
            }
        ]
    },
    {
        "header": "5Experimental Results",
        "images": []
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]