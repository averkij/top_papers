[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16848/x1.png",
                "caption": "Figure 1:Comparison of RL and Meta-RL training on the MineSweeper environment.Left: Meta-RL training withLaMerretains higher sample diversity from the base model while achieving better success rates, reaching a better trade-off between exploration and exploitation.Right: Distinct trajectories and their empirical probabilities aggregated over multiple sampled trajectories in the MineSweeper environment. Each trajectory corresponds to a sequence of clicks (numbered cell) on the board. Sample diversity is quantified by the entropy of the empirical distribution. The Meta-RL trained model produces more diverse and explorative trajectories.",
                "position": 139
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4LaMer: A Meta-RL Framework for LLM agents",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16848/x2.png",
                "caption": "Figure 2:Comparison between the training processes of RL (top) and Meta-RL used inLaMer(bottom). For a single task, RL generates a group of trajectories independently. In contrast, inLaMerwe use Meta-RL and produce the trajectories sequentially and adapt the policy in-context with self-reflection. Trajectory discount factorγtraj\\gamma_{\\text{traj}}is used for cross-episode credit assignment.",
                "position": 268
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16848/x3.png",
                "caption": "Figure 3:Trajectory diversity of base and trained models. Compared to RL, Meta-RL preserves more diverse trajectories from the base model, striking a better balance between exploration and exploitation.",
                "position": 513
            },
            {
                "img": "https://arxiv.org/html/2512.16848/x4.png",
                "caption": "",
                "position": 516
            },
            {
                "img": "https://arxiv.org/html/2512.16848/x5.png",
                "caption": "",
                "position": 518
            },
            {
                "img": "https://arxiv.org/html/2512.16848/x6.png",
                "caption": "Figure 4:Performance of RL and Meta-RL trained model on the tasks with increased difficulty. For Sokoban, we gradually increase the number of boxes and for MineSweeper, we increase the number of mines in the grid.",
                "position": 532
            },
            {
                "img": "https://arxiv.org/html/2512.16848/x7.png",
                "caption": "",
                "position": 535
            }
        ]
    },
    {
        "header": "6Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16848/x8.png",
                "caption": "Figure 5:Success rates of models trained with differentγtraj\\gamma_{\\text{traj}}. A higher value encourages more exploration during training.",
                "position": 631
            },
            {
                "img": "https://arxiv.org/html/2512.16848/x9.png",
                "caption": "",
                "position": 634
            },
            {
                "img": "https://arxiv.org/html/2512.16848/x10.png",
                "caption": "",
                "position": 636
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ATask Description and Details",
        "images": []
    },
    {
        "header": "Appendix BExample Prompts",
        "images": []
    },
    {
        "header": "Appendix CTraining details",
        "images": []
    },
    {
        "header": "Appendix DAdditional results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16848/x11.png",
                "caption": "Figure 6:Example of trajectories and reflections produced byLaMertrained agents on the MineSweeper game.",
                "position": 1196
            }
        ]
    },
    {
        "header": "Appendix EExamples",
        "images": []
    }
]