[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Empirical Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24372/x1.png",
                "caption": "Figure 1:Mean conciseness reward and mean KL divergence from the base model for each fine-tuning checkpoint across different learning parameters. The Pareto front of ES (blue line) is higher and to the left of the GRPO Pareto front (black line) models, indicating that it found better tradeoffs. ES discovers these solutions without any KL divergence penalty, suggesting that it represents a distinctly different fine-tuning mechanism from the GRPO.",
                "position": 480
            }
        ]
    },
    {
        "header": "5Discussion and Future Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24372/x2.png",
                "caption": "(a)GRPO models results over various learning rates.",
                "position": 2173
            },
            {
                "img": "https://arxiv.org/html/2509.24372/x2.png",
                "caption": "(a)GRPO models results over various learning rates.",
                "position": 2176
            },
            {
                "img": "https://arxiv.org/html/2509.24372/x3.png",
                "caption": "(b)ES and GRPO Pareto fronts.",
                "position": 2181
            },
            {
                "img": "https://arxiv.org/html/2509.24372/x4.png",
                "caption": "(a)Reward distribution withα=2×10−6\\alpha=2\\times 10^{-6}.",
                "position": 2197
            },
            {
                "img": "https://arxiv.org/html/2509.24372/x4.png",
                "caption": "(a)Reward distribution withα=2×10−6\\alpha=2\\times 10^{-6}.",
                "position": 2200
            },
            {
                "img": "https://arxiv.org/html/2509.24372/x5.png",
                "caption": "(b)Reward distribution withα=3×10−6\\alpha=3\\times 10^{-6}.",
                "position": 2205
            },
            {
                "img": "https://arxiv.org/html/2509.24372/x6.png",
                "caption": "(c)Reward distribution withα=4×10−6\\alpha=4\\times 10^{-6}.",
                "position": 2211
            },
            {
                "img": "https://arxiv.org/html/2509.24372/x7.png",
                "caption": "(d)Reward distribution withα=5×10−6\\alpha=5\\times 10^{-6}.",
                "position": 2216
            },
            {
                "img": "https://arxiv.org/html/2509.24372/x8.png",
                "caption": "Figure 5:Accuracy Improvement over Base Models with ES vs RL across Model Families. ES results in consistently largest improvements in all cases.",
                "position": 2223
            },
            {
                "img": "https://arxiv.org/html/2509.24372/x9.png",
                "caption": "Figure 6:Training curves of ES and RL across two model families and six sizes in the countdown task. ES fine-tuning results in significantly better performance in all cases. It is able to improve even the smallest model where RL methods are ineffective. ES is also more sample efficient than RL: in most cases, it only needs less than20%20\\%of the training sample evaluations of RL to achieve similar performance.",
                "position": 2234
            },
            {
                "img": "https://arxiv.org/html/2509.24372/x10.png",
                "caption": "Figure 7:Parameter magnitude shift histograms for the Countdown task in Llama models optimized by ES. The changes are similar to those of a random walk, concentrated around zero, likely due to numerical inaccuracies.",
                "position": 2250
            },
            {
                "img": "https://arxiv.org/html/2509.24372/x11.png",
                "caption": "Figure 8:Parameter magnitude shift histograms for the Countdown task in Qwen models optimized by ES. The results are consistent with those observed in Llama models.",
                "position": 2253
            },
            {
                "img": "https://arxiv.org/html/2509.24372/x12.png",
                "caption": "Figure 9:Parameter magnitude shift histograms in conciseness fine-tuning in Qwen2.5-7B-Instruct model with ES. In this case, the model is large and the fine-tuning goals is different, revealing a potentially significant pattern of primarily small changes. The hypothesis (to be analyzed more thoroughly in future work) is that behavior is coded in large models in a redundant manner, making it possible to achieve this fine-tuning objective through numerous small changes.",
                "position": 2256
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]