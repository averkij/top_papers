[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.17095/extracted/6299363/figures/new_teaser2.png",
                "caption": "",
                "position": 88
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.17095/extracted/6299363/figures/pretrain.png",
                "caption": "Figure 2:Pretraining stage of FFaceNeRF following EG3D[4]and NeRFFaceEditing[18]for disentangled representation.",
                "position": 163
            },
            {
                "img": "https://arxiv.org/html/2503.17095/extracted/6299363/figures/overview_pfnerf.png",
                "caption": "Figure 3:Overview of FFaceNeRF. LMTA is conducted during the training ofÎ¦gâ¢eâ¢osubscriptÎ¦ğ‘”ğ‘’ğ‘œ\\Phi_{geo}roman_Î¦ start_POSTSUBSCRIPT italic_g italic_e italic_o end_POSTSUBSCRIPT.Î¦gâ¢eâ¢osubscriptÎ¦ğ‘”ğ‘’ğ‘œ\\Phi_{geo}roman_Î¦ start_POSTSUBSCRIPT italic_g italic_e italic_o end_POSTSUBSCRIPTtakes as input the concatenation of normalized tri-plane featureFâ€²^tâ¢râ¢isubscript^superscriptğ¹â€²ğ‘¡ğ‘Ÿğ‘–\\hat{F^{\\prime}}_{tri}over^ start_ARG italic_F start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT end_ARG start_POSTSUBSCRIPT italic_t italic_r italic_i end_POSTSUBSCRIPT(yellow box), view directionvdsubscriptğ‘£ğ‘‘v_{d}italic_v start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT(white box), outputs ofÎ¨gâ¢eâ¢osubscriptÎ¨ğ‘”ğ‘’ğ‘œ\\Psi_{geo}roman_Î¨ start_POSTSUBSCRIPT italic_g italic_e italic_o end_POSTSUBSCRIPT, which are segmentation labelsSâ¢eâ¢gğ‘†ğ‘’ğ‘”Segitalic_S italic_e italic_g(blue box), and densityÏƒğœ\\sigmaitalic_Ïƒ(red box). DensityÏƒğœ\\sigmaitalic_Ïƒis directly used from the output ofÎ¨gâ¢eâ¢osubscriptÎ¨ğ‘”ğ‘’ğ‘œ\\Psi_{geo}roman_Î¨ start_POSTSUBSCRIPT italic_g italic_e italic_o end_POSTSUBSCRIPT, without further training usingÎ¦gâ¢eâ¢osubscriptÎ¦ğ‘”ğ‘’ğ‘œ\\Phi_{geo}roman_Î¦ start_POSTSUBSCRIPT italic_g italic_e italic_o end_POSTSUBSCRIPT.",
                "position": 166
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.17095/extracted/6299363/figures/4_05_pfnerf_data.jpg",
                "caption": "Figure 4:Examples of dataset with different segmentation layouts. Green boxes are close-up views of eye regions while red boxes are close-up views of nose regions.",
                "position": 332
            },
            {
                "img": "https://arxiv.org/html/2503.17095/extracted/6299363/figures/4_01_augment2.png",
                "caption": "Figure 5:Semantics-augmentation tradeoff: When mixing earlier layers, semantics and tri-plane feature information change largely (high L1, low mIoU). On the other hand, when mixing later layers, semantics and augmentation change little (low L1, high mIoU).",
                "position": 338
            },
            {
                "img": "https://arxiv.org/html/2503.17095/extracted/6299363/figures/4_04_mixing_vis_b.png",
                "caption": "Figure 6:Visualization of tri-plane mixing N layers with the top L1 metrics. The geometry and the semantic information such as the outline of chin and beard are changed even when only 2 layers are mixed.",
                "position": 352
            },
            {
                "img": "https://arxiv.org/html/2503.17095/extracted/6299363/figures/4_03_mixing_vis_a.png",
                "caption": "Figure 7:Visualization of tri-plane mixing N layers based on the top mIoU metrics. The geometry and the semantic information are not changed while colors are changed.",
                "position": 355
            },
            {
                "img": "https://arxiv.org/html/2503.17095/extracted/6299363/figures/mixing_triplane3.png",
                "caption": "Figure 8:Results of mixing multiple tri-planes. Left: Mixing top N layers based on mIoU, Right: Mixing top N layers based on L1.",
                "position": 367
            },
            {
                "img": "https://arxiv.org/html/2503.17095/extracted/6299363/figures/ours.png",
                "caption": "Figure 9:Examples of multi-view images edited using our method. Edited regions are indicated with red arrows in target masks.",
                "position": 375
            },
            {
                "img": "https://arxiv.org/html/2503.17095/extracted/6299363/figures/comparison.png",
                "caption": "Figure 10:Examples of our results and those of baseline methods in editing tasks. The results of our method faithfully reflect the edited regions.",
                "position": 388
            },
            {
                "img": "https://arxiv.org/html/2503.17095/extracted/6299363/figures/facenerfab.png",
                "caption": "Figure 11:Ablation study results. Our full model best followed the target masks while preserving the original identity. The color shifted when trained without feature injection, hair was not faithfully generated when trained without LMTA, and the source identity changed when trained with mixing all layers.",
                "position": 466
            },
            {
                "img": "https://arxiv.org/html/2503.17095/extracted/6299363/figures/fewnerfab3.png",
                "caption": "Figure 12:Comparisons with percentage-based optimization on enlarging eyes show that our method more faithfully follows the desired eye size modifications.",
                "position": 469
            },
            {
                "img": "https://arxiv.org/html/2503.17095/extracted/6299363/figures/application.png",
                "caption": "Figure 13:Results of full and partial style transfer application.",
                "position": 521
            }
        ]
    },
    {
        "header": "5Application",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.17095/extracted/6299363/figures/app_ffaceGAN.jpg",
                "caption": "Figure 14:Comparison between our FFaceGAN and DatasetGAN in generating segmentation masks based on the target labels.",
                "position": 546
            }
        ]
    },
    {
        "header": "6Limitation and Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]