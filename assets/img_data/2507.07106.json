[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07106/x1.png",
                "caption": "Figure 1:Overview.(Left)We present our full multimodal pipeline. Following LLaVA, we first extract visual features from the frozen diffusion model and pass the question as text-prompt. The LLM then uses these features to generate its answer. Cross-attention maps show that the model can use the question to focus on relevant regions(Right)We show examples on MMVP[1]where diffusion features outperform CLIP.",
                "position": 101
            },
            {
                "img": "https://arxiv.org/html/2507.07106/x2.png",
                "caption": "Figure 2:Inspecting Diffusion Features. (a, b) We visualize spatial features for three image pairs from MMVP-VLM using PCA across blocks and timesteps. For (a), we use T=50; for (b), we fixU-L1-R1-B0-Cross-Q. We observe: (1) different blocks capture either shared semantics or image-specific details; (2) higher timesteps encode coarse layout, while lower timesteps emphasize fine-grained structure; and (3) features likeoutandres-outhave tokens which can act as \"registers\" that act as shared global descriptors across similar images. (c) We plot average cosine similarity, finding thatcross-qrepresentations capture greater similarity compared tooutfeatures.",
                "position": 169
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07106/x3.png",
                "caption": "Figure 3:General Model Performance (Left):We evaluate multimodal reasoning using the LLaVA framework with diffusion features at different layers and timesteps. The table reports accuracy on LLaVA-Bench, MMVP, and NaturalBench under varying feature extraction points.BLINK-val Performance (Right):The plot shows BLINK-val benchmark performance across different timesteps. SD-based models consistently outperform CLIP (in black) across timesteps. We point out notable improvements: +10% on Spatial Reasoning, Multi-view Reasoning, and Art Style.",
                "position": 189
            },
            {
                "img": "https://arxiv.org/html/2507.07106/x3.png",
                "caption": "",
                "position": 329
            }
        ]
    },
    {
        "header": "3What Visual Information Do Diffusion Models Encode?",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07106/x4.png",
                "caption": "Figure 4:Visualizing Cross-Attention Maps.We show a sample from COCO-captions and averaged cross-attention maps at low and high timesteps for a few key words, representing the attention between pixel features and a specific word. We observe that cross-attention maps at higher timesteps show higher focus on background elements (e.g., ‚Äúcourt‚Äù). Attention maps from lower timesteps provide improved localization of both object and action concepts (e.g., ‚Äúracquets‚Äù and ‚Äúholding‚Äù).",
                "position": 355
            }
        ]
    },
    {
        "header": "4How Does Text Guidance Interact with Diffusion Representations?",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07106/x5.png",
                "caption": "Figure 5:Visualizing Text Conditioned Diffusion Features.(a) A few images are sampled from the MMVP-VLM dataset and visualize PCA maps of spatial features extracted from different intermediate layers under both unconditional and text-conditioned settings (t = 50). While the overall structure of the features remains similar after adding text, we find that the difference of these features (ŒîcondsubscriptŒîcond\\Delta_{\\text{cond}}roman_Œî start_POSTSUBSCRIPT cond end_POSTSUBSCRIPT) can highlight specific regions influenced by the text. (b) We show amplifying text guidance highlights relevant object and part regions. (c) Finally, we measure CKA similarity between unconditioned and progressively text-conditioned features across blocks and observe that spatial features extracted fromoutlayers experience greater text modulation compared tocross-qfeatures.",
                "position": 712
            },
            {
                "img": "https://arxiv.org/html/2507.07106/x6.png",
                "caption": "Figure 6:COCO-Captions Performance (Left): We compare model performance on COCO-Captions across different pretraining text-guidance settings. Models trained with stronger ground-truth conditioning outperform the CLIP baseline when given ground-truth captions at inference but degrade significantly when no caption is provided.Evidence of Leakage (Right):We examine whether increasing text-guidance scale causes prompt leakage into the LLM. Training with caption dropout improves robustness when no caption is provided, with minor performance drops when ground-truth captions are used.",
                "position": 735
            },
            {
                "img": "https://arxiv.org/html/2507.07106/x7.png",
                "caption": "",
                "position": 744
            }
        ]
    },
    {
        "header": "5Can We Extract Task-Aware Features for Question-Answering?",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07106/x8.png",
                "caption": "Figure 7:Visualizing Question-Conditioned Features.We sample an image with two questions from NaturalBench and visualize spatial features conditioned on questions via PCA. We see that the difference between conditional and unconditional features finds the relevant regions.",
                "position": 774
            },
            {
                "img": "https://arxiv.org/html/2507.07106/x8.png",
                "caption": "Figure 7:Visualizing Question-Conditioned Features.We sample an image with two questions from NaturalBench and visualize spatial features conditioned on questions via PCA. We see that the difference between conditional and unconditional features finds the relevant regions.",
                "position": 777
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperimental Settings",
        "images": []
    },
    {
        "header": "Appendix BFull Captioning Results",
        "images": []
    },
    {
        "header": "Appendix CFurther Diffusion Feature Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07106/x9.png",
                "caption": "Figure 8:More Diffusion Feature Visualizations.We sample six pairs of images from the NaturalBench benchmark[57]and view the joint PCA maps across different blocks and layers. Please zoom in to see more details.",
                "position": 1699
            },
            {
                "img": "https://arxiv.org/html/2507.07106/x10.png",
                "caption": "Figure 9:CKA Block-Wise Similarity.We compute block-wise CKA similarity using the COCO-Captions test set (5000 images) across various guidance scales (s=ùë†absents=italic_s =0, 1, 4)",
                "position": 1706
            }
        ]
    },
    {
        "header": "Appendix DFurther Cross-Attention Map Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07106/x11.png",
                "caption": "Figure 10:More Cross-Attention Maps.We display more examples of cross-attention maps from images in the COCO-Captions test set. For the image of the skier, we can see that object-attribute binding such as‚Äúblack‚Äù and ‚Äúcoat‚Äù are better aligned at later timesteps compared to earlier ones.",
                "position": 2002
            },
            {
                "img": "https://arxiv.org/html/2507.07106/x12.png",
                "caption": "Figure 11:Cross-Attention Maps Across Layers.We display cross-attention maps at timestep 50 across various layers. We see that cross-attention maps are not uniform and that maps at theup-stageencode more robust image-text alignment.",
                "position": 2005
            }
        ]
    },
    {
        "header": "Appendix EMore Examples of Question-Conditioning in Diffusion Features",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07106/x13.png",
                "caption": "Figure 12:More Visualizations of Question-Conditioned Features.Please zoom in to see smaller regions of focus (e.g., pencil in second row, rightmost column ofU-L2-R1-B0-Cross-Q)",
                "position": 2028
            },
            {
                "img": "https://arxiv.org/html/2507.07106/x14.png",
                "caption": "",
                "position": 2032
            }
        ]
    },
    {
        "header": "Appendix FInvestigating SDXL Architecture",
        "images": []
    }
]