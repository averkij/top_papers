[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.15191/x1.png",
                "caption": "",
                "position": 91
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.15191/x2.png",
                "caption": "Figure 2:Design of the proposed Fusion Block connecting the frozen video and audio backbones. A RoPE-based temporal alignment mechanism establishes correspondences between modalities that are leveraged by self attention. Video and audio features are symmetrically reinjected into the frozen generators. The block is regularly applied multiple times throughout the backbones.",
                "position": 177
            },
            {
                "img": "https://arxiv.org/html/2412.15191/x3.png",
                "caption": "Figure 3:Visualization of Audio-to-Video and Video-to-Audio generation performance for various value of flow timestepstùë°titalic_tfor conditioning features. Best performance is achieved when conditioning features are close to be fully denoised,i.e.t‚àà[0.8,0.98]ùë°0.80.98t\\in[0.8,0.98]italic_t ‚àà [ 0.8 , 0.98 ].",
                "position": 334
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.15191/x4.png",
                "caption": "Figure 4:Qualitative V2A results. Our model achieved the best temporal alignment, matching closely the ‚Äúbouncing‚Äù and ‚Äúdrumming‚Äù sounds entailed by the video modality. See theAppendixandWebsitefor additional results.",
                "position": 852
            },
            {
                "img": "https://arxiv.org/html/2412.15191/x5.png",
                "caption": "Figure 5:Qualitative A2V results. Our model generates semantically and temporally aligned content showing to the ‚Äúexplosions‚Äù and ‚Äúdrumming‚Üí‚Üí\\rightarrow‚Üísilence‚Üí‚Üí\\rightarrow‚Üídrumming‚Äù events implied by the audio modality. We show 3.3s of our samples at 3 FPS while only 2s of TempoTokens samples, hence the difference in frame count. See theAppendixandWebsitefor additional results.",
                "position": 1114
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitations",
        "images": []
    },
    {
        "header": "Appendix BArchitecture Details",
        "images": []
    },
    {
        "header": "Appendix CTraining and Inference Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.15191/x6.png",
                "caption": "Figure 6:Comparison between different parametrizations for the Logit-Normal training distributionsptsubscriptùëùùë°p_{t}italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTfor the flow timesteptùë°titalic_t. When the location (i.e.the mean of the normal distribution) is shifted towards higher noise levels, we observe faster model convergence.",
                "position": 2357
            },
            {
                "img": "https://arxiv.org/html/2412.15191/x7.png",
                "caption": "Figure 7:Qualitative V2A results comparing our method to baselines on in-the-wild videos captured by the authors that require precise temporal alignment. AV-Link produces audio signals that closely align to the visual modalities, while baselines often produce audio that is unrelated or not correctly synchronized with the visual content. See theWebsitefor more results.",
                "position": 2462
            },
            {
                "img": "https://arxiv.org/html/2412.15191/x8.png",
                "caption": "Figure 8:Qualitative A2V results produced by our method. AV-Link produces videos that present a high level of alignment to the conditioning audio signal. See theWebsitefor more results.",
                "position": 2465
            }
        ]
    },
    {
        "header": "Appendix DAdditional Evaluation Results and Details",
        "images": []
    }
]