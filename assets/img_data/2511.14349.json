[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.14349/x1.png",
                "caption": "Figure 1:An illustration of the capabilities of our video chaptering model.Given a video, our model is able to generate timestamped chapters with three-level structured output: 1)Short Title- a concise label summarizing each chapter; 2)Structural Chapter- a detailed, structured annotation for each chapter, including a rewritten comprehensivetitle, anabstractsummarizing the core content, and anintroductiondescribing key details and highlights; and 3)Timestamp-Aligned Video Description- fine-grained descriptions aligned with precise temporal boundaries. This hierarchical structure facilitates an efficient and precise understanding of video content.",
                "position": 136
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Data Collection and Annotation",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.14349/x2.png",
                "caption": "Figure 2:Overview of our automatic video annotation pipeline for hierarchical chaptering and summarization. We extract visual captions (OCR included) from sampled video frames and ASR transcripts from audio. These outputs are temporally aligned and interleaved into a unified multimodal transcript. This transcript, together with original chapter markers, is processed by an LLM to produce structured chapters and timestamp-aligned video descriptions.",
                "position": 193
            },
            {
                "img": "https://arxiv.org/html/2511.14349/x3.png",
                "caption": "(a)Duration distribution",
                "position": 215
            },
            {
                "img": "https://arxiv.org/html/2511.14349/x3.png",
                "caption": "(a)Duration distribution",
                "position": 218
            },
            {
                "img": "https://arxiv.org/html/2511.14349/x4.png",
                "caption": "(b)Categories in dataset",
                "position": 223
            }
        ]
    },
    {
        "header": "4ARC-Chapter",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.14349/x5.png",
                "caption": "Figure 4:Overview of the model architecture for video chaptering.The model inputs include a task-specific prompt, sampled video frames, and timestamped ASR transcripts. Video frames are processed with a frozen vision encoder. The resulting visual features, along with the tokenized prompt and ASR text, are fed into a trainable multimodal large language model (MLLM). Based on the inputs, the model is able to generate chapters in various formats, including timestamped concise title, detailed structural chapters, or comprehensive video description with timestamps.",
                "position": 278
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.14349/x6.png",
                "caption": "Figure 6:Data Scaling property of ARC-Chapter.We report the performance on VidChapter (a sampled subset) and VidAtlas test set with respect to different percentage of training samples.",
                "position": 1739
            },
            {
                "img": "https://arxiv.org/html/2511.14349/x7.png",
                "caption": "Figure 7:Qualitative results on an English video about finance and cryptocurrency.",
                "position": 2156
            },
            {
                "img": "https://arxiv.org/html/2511.14349/x8.png",
                "caption": "Figure 8:Qualitative results on a Chinese video discussing stablecoins.",
                "position": 2159
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    }
]