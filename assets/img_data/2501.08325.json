[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08325/x1.png",
                "caption": "",
                "position": 119
            },
            {
                "img": "https://arxiv.org/html/2501.08325/x2.png",
                "caption": "",
                "position": 137
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08325/x3.png",
                "caption": "Figure 2:A schematic of our GameFactory creating new games based on pre-trained large video generation models. Theupper blue sectionshows the generative capabilities of the pre-trained model in an open-domain, while thelower green sectiondemonstrates how the action control module, learned from a small amount of game data, can be plugged in to create new games.",
                "position": 153
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08325/x4.png",
                "caption": "Figure 3:(a) Integration of Action Control Module into transformer blocks of the video diffusion model.\n(b) Different control mechanisms for continuous mouse signals and discrete keyboard signals (detailed analysis in Sec3.3).\n(c) Due to temporal compression (compression ratior=4ùëü4r=4italic_r = 4), the number of latent features differs from the number of actions, causing granularity mismatch during fusion. Grouping aligns these sequences for fusion. Additionally, theiùëñiitalic_i-th latent feature can fuse with action groups within a previous window (window sizew=3ùë§3w=3italic_w = 3), accounting for delayed action effects (e.g., ‚Äòjump‚Äô key affects several subsequent frames).",
                "position": 226
            },
            {
                "img": "https://arxiv.org/html/2501.08325/x5.png",
                "caption": "Figure 4:Style bias in video game generation. The model tuned on Minecraft data inherits its distinctive pixelated block style, creating a domain gap from the original parameters. This motivates decoupling action control learning from data style learning through specialized training strategies.",
                "position": 424
            },
            {
                "img": "https://arxiv.org/html/2501.08325/x6.png",
                "caption": "Figure 5:Phase #0: pretraining a video generation model on open-domain data.Phase #1: finetuning with LoRA for game video data.Phase #2: training the action control module while fixing other parameters.Phase #3: inference for action-controlled open-domain generation.\nTo decouple style learning from action control,Phase #1learns game-specific style whilePhase #2focuses on style-independent action control. This design preserves the open-domain capabilities fromPhase #0, enabling generalization inPhase #3.",
                "position": 443
            },
            {
                "img": "https://arxiv.org/html/2501.08325/x7.png",
                "caption": "Figure 6:Illustration of autoregressive video generation. The frames from index00tokùëòkitalic_kserve as conditional frames, while the remainingN‚àíkùëÅùëòN-kitalic_N - italic_kframes are for prediction, withkùëòkitalic_krandomly selected. (a) Training stage: Loss computation and optimization focus only on the noise of predicted frames. (b) Inference stage: The model iteratively selects the latestk+1ùëò1k+1italic_k + 1frames as conditions to generateN‚àíkùëÅùëòN-kitalic_N - italic_knew frames, enabling autoregressive generation.",
                "position": 487
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08325/x8.png",
                "caption": "Figure 7:Demonstration of action control capabilities in the Minecraft domain. The model has successfully learned basic atomic actions (WASD keys) and mouse-based yaw and pitch controls. Additionally, it can combine these atomic actions to execute more complex movements. Note that the text below each video frame is a descriptive label of the content, not a text prompt provided to the model.",
                "position": 602
            },
            {
                "img": "https://arxiv.org/html/2501.08325/x9.png",
                "caption": "Figure 8:Demonstration of the learned response to collision, one of the most common interactions in Minecraft navigation. Note that the text below each video frame is a descriptive label of the content, not a text prompt provided to the model.",
                "position": 646
            },
            {
                "img": "https://arxiv.org/html/2501.08325/x10.png",
                "caption": "Figure 9:Our model demonstrates the ability to generalize to a different game type, a racing game. Interestingly, the yaw control learned in Minecraft seamlessly transfers to steering control in the racing game, while unrelated actions, such as moving backward, left, or right, and pitch angle adjustments, automatically diminish.",
                "position": 659
            }
        ]
    },
    {
        "header": "5Potential of Generalizable World Model",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails of GF-Minecraft Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08325/x11.png",
                "caption": "Figure 10:An example of video clip annotation, where words describing scenes and objects are highlighted in red and bolded.",
                "position": 1369
            },
            {
                "img": "https://arxiv.org/html/2501.08325/x12.png",
                "caption": "Figure 11:Qualitative comparison of key input control performance. It can be observed that cross-attention significantly outperforms concatenation in handling discrete key input signals, while concatenation may fail to respond to the key input. The yellow buttons indicate pressed keys.",
                "position": 1522
            },
            {
                "img": "https://arxiv.org/html/2501.08325/x13.png",
                "caption": "Figure 12:Qualitative comparison of multi-phase training with one-phase training for scene generalization. The arrows represent the direction of mouse movements.",
                "position": 1526
            }
        ]
    },
    {
        "header": "Appendix BSupplementary Comparative Experimental Results",
        "images": []
    }
]