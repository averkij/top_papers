[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21025/figures/taxonomy.png",
                "caption": "Figure 1:CaptionQA taxonomies across four domains, the visual information that captions must carry to be useful for downstream tasks.\nThe Natural domain (6 top-level, 22 subcategories) emphasizes object properties, spatial relations, and hallucination; the Document domain (6, 15) targets layout, content, and document-specific structure; the E-commerce domain (7, 16) focuses on product attributes and presentation; and the Embodied AI domain (6, 16) captures perception, spatial understanding, and task-relevant cues for robotics.",
                "position": 179
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Task and Evaluation Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21025/figures/qa_llm.png",
                "caption": "Figure 2:Comparison of text-only QA LLMs (GPT-5, Gemini 2.5 Pro, DeepSeek-R1 Llama 70B, Qwen2.5 72B) along four axes: faithfulness, efficiency (QPS), stability, and performance.",
                "position": 476
            }
        ]
    },
    {
        "header": "4Benchmark Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21025/figures/pipeline.png",
                "caption": "Figure 3:Benchmark construction pipeline.Starting from a human-designed taxonomy and curated images for each domain, we use multiple generators to produce a large pool of taxonomy-guided questions. This pool is then refined by (1) embedding-based deduplication, (2) a text-only blind test to remove questions answerable from priors, (3) dual-VLM quality control to flag ungrounded or reasoning-heavy items, and (4) final human refinement, yielding high-quality, utility-focused QA pairs.",
                "position": 487
            }
        ]
    },
    {
        "header": "5A Gap between Caption and Image Utility",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21025/x1.png",
                "caption": "Figure 4:Overall gap between QA-on-image and QA-on-caption for GPT-5, Gemini-2.5-Pro,\nQwen3-VL-30B-A3B, GLM-4.1V-9B, InternVL3.5-38B, Claude-Sonnet-4.5, and LLaVA-OV-7B.\nEach bar shows the difference in CaptionQA Acc., averaged over the four domains.",
                "position": 1035
            },
            {
                "img": "https://arxiv.org/html/2511.21025/figures/caption_example.png",
                "caption": "Figure 5:Qualitative example of caption under a complex prompt, Taxonomy-Hinted. Although GPT-5 is instructed to describe the image and focus on provided aspects, it outputs in a fill-in-the-blank style and provide much less information than Long prompt.",
                "position": 1134
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Motivation and Overview",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21025/figures/figure1_WIP.png",
                "caption": "Figure 6:Defining and evaluating “useful” captions.Existing practices are either fact-blind (text-similarity metrics) or test a different task with sparse supervision (multimodal benchmarks), or rely on complex, non-deterministic pipelines (caption benchmarks). CaptionQA instead measures how “useful” a caption is by testing whether it can stand in for the image on dense, taxonomy-driven QA, and yields fine-grained diagnostics across domains and aspects.",
                "position": 1194
            }
        ]
    },
    {
        "header": "8Question Characteristics",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21025/figures/choices_distribution.png",
                "caption": "Figure 7:Distribution of answer choices across domains. Most questions are 4-choice (87–92%), making the benchmark more challenging than binary VQA. Natural domain has more binary questions due to attribute verification.",
                "position": 1204
            },
            {
                "img": "https://arxiv.org/html/2511.21025/figures/question_density.png",
                "caption": "Figure 8:Distribution of question density across domains. The violin plots show the distribution of questions per image, with Natural images supporting the highest density of diverse questions and Document images focusing on specific structural and content elements. The consistent distributions within each domain (low variance) demonstrate systematic annotation quality.",
                "position": 1213
            },
            {
                "img": "https://arxiv.org/html/2511.21025/figures/category_coverage_natural.png",
                "caption": "Figure 9:Natural Domain:Question distribution across top-level taxonomy categories. Attribute questions dominate, followed by Object Existence and Hallucination detection.",
                "position": 1219
            },
            {
                "img": "https://arxiv.org/html/2511.21025/figures/category_coverage_document.png",
                "caption": "Figure 10:Document Domain:Question distribution across top-level taxonomy categories. Content-Level Evaluation and Structural Elements are the primary focus.",
                "position": 1222
            },
            {
                "img": "https://arxiv.org/html/2511.21025/figures/category_coverage_ecommerce.png",
                "caption": "Figure 11:E-commerce Domain:Question distribution across top-level taxonomy categories. Questions are evenly distributed across product information, context, and marketing aspects.",
                "position": 1225
            },
            {
                "img": "https://arxiv.org/html/2511.21025/figures/category_coverage_embodiedai.png",
                "caption": "Figure 12:Embodied AI Domain:Question distribution across top-level taxonomy categories. Perception and Spatial Context dominate, reflecting robotics task requirements.",
                "position": 1228
            }
        ]
    },
    {
        "header": "9Caption Prompts",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21025/x2.png",
                "caption": "Figure 13:Average caption length (word count) by prompt type, averaged across all models and domains. The Taxonomy-Hinted prompt produces the longest captions (650 words on average), followed by Long (510 words), Simple (356 words), and Short (22 words).",
                "position": 1270
            }
        ]
    },
    {
        "header": "10Taxonomy Structure",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21025/figures/taxonomy.png",
                "caption": "Figure 14:Taxonomy structure across all four CaptionQA domains.(1) Natural domaincontains 6 top-level and 22 subcategories,\nemphasizing object properties, spatial relationships, and hallucination\ndetection.(2) Document domaincontains 6 top-level and 15\nsubcategories, focusing on structural elements, content evaluation, and\ndocument-specific features.(3) E-commerce domaincontains\n7 top-level and 16 subcategories, covering product attributes, visual\npresentation, and marketing information.(4) Embodied AI domaincontains 6 top-level and 16 subcategories, prioritizing\nperception, spatial understanding, and task-relevant features for robotics\napplications.",
                "position": 1283
            }
        ]
    },
    {
        "header": "11Image Amount Justification",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21025/figures/model_performance_vs_images.png",
                "caption": "Figure 15:Model ranking stability vs. number of images (accuracy-based).Each line represents one model’s accuracy trajectory as more images are randomly sampled (10 trials per sample size). Same color indicates the same model across domains. Performance curves plateau rapidly and maintain relative positions, validating data sufficiency. Top 10 models shown (ranked by average performance across domains).",
                "position": 1316
            },
            {
                "img": "https://arxiv.org/html/2511.21025/figures/model_score_vs_images.png",
                "caption": "Figure 16:Model ranking stability vs. number of images (average score-based).Same analysis using average score (with partial credit for “Cannot answer”:1/nchoices+0.051/n_{\\text{choices}}+0.05). Patterns mirror Figure15, confirming data sufficiency holds across evaluation metrics.",
                "position": 1320
            }
        ]
    },
    {
        "header": "12Cost of Extending CaptionQA to New Domains",
        "images": []
    },
    {
        "header": "13Rationale and Reliability of LLM as QA Reader",
        "images": []
    },
    {
        "header": "14Prompt Transition Analysis: Where Does Length Help?",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21025/figures/transition_short_to_simple_single.png",
                "caption": "Figure 17:Short to Simple effectiveness across all 25 categories.Categories sorted by improvement, color-coded from dark green (+47-56%) to red (+6-21%). Top categories: Document domain-specific evaluation and E-commerce textual elements. Bottom categories: Embodied AI activity context and Natural hallucination. Top categories gain up to 9×\\timesmore than bottom categories.",
                "position": 2890
            },
            {
                "img": "https://arxiv.org/html/2511.21025/figures/transition_simple_to_long.png",
                "caption": "Figure 18:Marginal gains: Simple to Long.All 25 categories show<<2% change (mean +0.35%). Simple and Long achieve nearly identical scores (75.5% vs 75.7%) despite 1.4×\\timeslength difference (355 vs 510 words). Additional length provides minimal gain.",
                "position": 2900
            },
            {
                "img": "https://arxiv.org/html/2511.21025/figures/transition_long_to_taxonomy.png",
                "caption": "Figure 19:Taxonomy-Hinted prompts often degrade performance.23 of 25 categories show losses (mean -10.8%), with 20 losing>>5%. Only 2 categories gain (Visual Appearance +2.0%, Scene-Level Evaluation +0.4%). Largest losses: Document Domain-Specific Evaluation (-33.1%), Embodied AI Perception (-7.8%), Document Structural Elements (-9.3%).",
                "position": 2910
            },
            {
                "img": "https://arxiv.org/html/2511.21025/figures/coverage_accuracy_short_to_long.png",
                "caption": "Figure 20:Coverage vs. Accuracy: Short to Long (r=0.905).Most categories cluster near diagonal. Below diagonal: Natural Spatial (43.9% coverage vs 35.3% accuracy), Document Structural (49.3% vs 40.9%)—more coverage than accuracy. Above diagonal: Natural Object Existence (5.3% coverage vs 26.7% accuracy), E-commerce Contextual (14.4% vs 28.3%)—more accuracy than coverage.",
                "position": 2929
            },
            {
                "img": "https://arxiv.org/html/2511.21025/figures/coverage_accuracy_short_to_simple.png",
                "caption": "Figure 21:Coverage vs. Accuracy: Short to Simple (r=0.902).Mean coverage gain 32.8%, mean accuracy gain 33.8%. Comparing to Short to Long (33.1% coverage, 34.2% accuracy), this transition achieves 99% of Long’s gains at 70% of the length.",
                "position": 2939
            },
            {
                "img": "https://arxiv.org/html/2511.21025/figures/coverage_accuracy_simple_to_long.png",
                "caption": "Figure 22:Coverage vs. Accuracy: Simple to Long (r=0.837).Clustered near origin. Mean coverage change +0.3%, mean accuracy change +0.4%. E-commerce Visual Appearance is outlier with +2.6% coverage and +2.0% accuracy. Near-zero changes confirm diminishing returns.",
                "position": 2949
            },
            {
                "img": "https://arxiv.org/html/2511.21025/figures/coverage_accuracy_long_to_taxonomy.png",
                "caption": "Figure 23:Coverage vs. Accuracy: Long to Taxonomy-Hinted (r=0.966).Strong negative correlation. Mean coverage change -8.8%, mean accuracy change -10.8%. Document Domain-Specific is worst outlier (-27.6% coverage, -33.1% accuracy). Only 2 categories show gains. Bottom-left quadrant: Taxonomy-Hinted prompts add wrong content that reduces both coverage and accuracy.",
                "position": 2959
            }
        ]
    },
    {
        "header": "15Category-Level Statistical Summary",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21025/figures/radar_natural_subcategories.png",
                "caption": "Figure 24:Natural domain: 22 subcategories.Models perform best on scene-level evaluation (80-92%) and object existence (75-90%), but struggle with spatial reasoning (40-65%) and fine-grained attributes (50-70%). GPT-5 leads on most categories. Spatial subcategories (distance, orientation, relative position) show the largest performance gaps and highest variance.",
                "position": 3262
            },
            {
                "img": "https://arxiv.org/html/2511.21025/figures/radar_document_subcategories.png",
                "caption": "Figure 25:Document domain: 15 subcategories.Models excel on high-level evaluation (80-93%) but struggle with structural elements (50-75%). Gemini models show relative strength on table/chart parsing. Variance is high across subcategories, with chart-specific elements (axis labels, legends) being particularly challenging.",
                "position": 3272
            },
            {
                "img": "https://arxiv.org/html/2511.21025/figures/radar_ecommerce_subcategories.png",
                "caption": "Figure 26:E-commerce domain: 16 subcategories.Models achieve highest overall scores (70-96%) across all domains. Contextual understanding (85-96%) and product-level information (82-94%) are strengths. Visual appearance details (color matching, style) are harder (60-75%). Text extraction varies by text type.",
                "position": 3282
            },
            {
                "img": "https://arxiv.org/html/2511.21025/figures/radar_embodiedai_subcategories.png",
                "caption": "Figure 27:Embodied AI domain: 16 subcategories.Most challenging domain overall (50-85% range). Activity/task understanding (80-93%) is relatively strong, but perception subcategories (object properties, affordances, manipulation) drop to 40-70%. Sensor-specific information (depth, embodiment viewpoint) is systematically under-described.",
                "position": 3292
            }
        ]
    },
    {
        "header": "16Detailed Model Performance Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21025/figures/radar_unified_everything_filtered_Subcategories.png",
                "caption": "Figure 28:Comprehensive model performance across all 69 subcategories and 4 domains.Each colored line represents one of 24 evaluated models across 69 fine-grained subcategories.\nThe chart is divided into four domain sections (Natural, Document, E-commerce, Embodied AI) separated by black radial lines.\nConcentric circles indicate accuracy levels from 0-100% at 20% intervals.\nTop-performing models includeGPT-5andGemini 2.5 Flashamong proprietary models, andQwen3-VL 30B-A3BandGLM-4.1V 9Bamong open-source models, though performance varies dramatically across categories within each model (30-95% range), revealing that no single model excels uniformly across all task types.\nNotably, categories requiring spatial reasoning (Embodied AI section) show consistently depressed performance across all models (40-70%) compared to perceptual categories (70-90%), indicating a systematic capability gap in current vision-language models.",
                "position": 3307
            }
        ]
    },
    {
        "header": "17Question Difficulty Distribution",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21025/figures/difficulty_distribution_stacked.png",
                "caption": "Figure 29:Question Difficulty Distribution Across Domains.Each domain exhibits a different difficulty profile: E-commerce has more easy questions (64%), while Embodied AI is the most challenging (32% hard questions). This diversity ensures CaptionQA can discriminate between models at different capability levels.",
                "position": 3358
            },
            {
                "img": "https://arxiv.org/html/2511.21025/figures/example_questions_clean.png",
                "caption": "Figure 30:Example Questions with Images and Answer Choices: Hardest vs Easiest.Left column shows hardest questions (0-10% of models answered correctly), highlighting challenges in fine-grained spatial reasoning, technical detail recognition, and complex relational understanding. Right column shows easiest questions (90-100% of models answered correctly), typically involving basic object presence or simple binary attributes. For each example, the image is shown on the left with the question text and multiple-choice options displayed together in a white box with domain-colored borders (green=Natural, blue=Document, red=E-commerce, orange=Embodied AI). The correct answer is marked with a checkmark (✓). Note that different images are used for hardest vs easiest questions to avoid confounding factors.",
                "position": 3412
            }
        ]
    },
    {
        "header": "18Full Results",
        "images": []
    }
]