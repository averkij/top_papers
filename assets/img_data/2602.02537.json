[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02537/x1.png",
                "caption": "Figure 1:Overall Model Accuracy on WorldVQA. While the Gemini-3-pro (47.4%) and Kimi K2.5 (46.3%) currently lead the field, no evaluated model surpasses the 50% accuracy threshold, underscoring the significant challenge of grounding atomic visual knowledge.",
                "position": 87
            },
            {
                "img": "https://arxiv.org/html/2602.02537/images/main_figure.jpg",
                "caption": "Figure 2:A visual overview of theWorldVQAdataset. The benchmark is organized into nine categories: Nature & Environment (Nature); Locations & Architecture (Geography); Culture, Arts & Crafts (Culture); Objects & Products (Objects); Vehicles, Craft & Transportation (Transportation); Entertainment, Media & Gaming (Entertainment); Brands, Logos & Graphic Design (Brands); Sports, Gear & Venues (Sports); Notable People & Public Figures (People). The visual entities curated to evaluate atomic world knowledge range from globally recognized \"head-class\" landmarks and logos to specific \"long-tail\" biological species and artisanal artifacts. To maintainatomic isolation, each image serves as an unambiguous visual stimulus for entity naming, strictly decoupled from complex reasoning or OCR dependencies.",
                "position": 90
            }
        ]
    },
    {
        "header": "2Data Collection and Verification",
        "images": []
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02537/x2.png",
                "caption": "Figure 3:Category-wise F-score comparison on WorldVQA.This radar chart illustrates the performance profiles of frontier close-source and open-source MLLMs across the 8 semantic categories. The visualization highlights the relative proficiency in high-frequency domains likeSportsandBrands, while revealing significant performance troughs in specialized domains such asNatureandCulture.",
                "position": 747
            },
            {
                "img": "https://arxiv.org/html/2602.02537/x3.png",
                "caption": "Figure 4:Entity Difficulty Distribution vs. MetaCLIP Frequency Rank Percentile across Categories. These plots illustrate the relationship between real-world entity frequency (proxied by MetaCLIP vocabulary rank percentile) and their assigned difficulty in WorldVQA.\nThe x-axis represents the percentile rank of the entity’s frequency in the MetaCLIP vocabulary, where values closer to 0 indicate high-frequency (common) entities, and higher values indicate lower-frequency (rare) entities. The left y-axis corresponds to the grey line, showing the underlying exponential density distribution of MetaCLIP word frequencies, highlighting the long-tail nature of real-world knowledge. The right y-axis shows the probability density for the fitted normal distributions of the four difficulty tiers: Trivial, Easy, Medium, and Hard.",
                "position": 763
            },
            {
                "img": "https://arxiv.org/html/2602.02537/x4.png",
                "caption": "Figure 5:Calibration and Confidence Distribution Analysis.Left:Reliability diagrams plotting Actual Accuracy against Stated Confidence. To ensure statistical significance, only bins containing more than 20 samples are visualized. The size of each data point is proportional to the number of samples in that bin. The black dashed diagonal (y=x) represents perfect calibration, while colored dashed lines indicate the weighted average slope for each model.Right:The distribution of stated confidence scores across the full dataset (without sample thresholding). The plots reveal a severe overconfidence trend, with most models concentrating their predictions in the 90-100% confidence range.",
                "position": 781
            },
            {
                "img": "https://arxiv.org/html/2602.02537/x5.png",
                "caption": "",
                "position": 784
            }
        ]
    },
    {
        "header": "4Related Work and Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix APrompts",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02537/x6.png",
                "caption": "",
                "position": 1407
            },
            {
                "img": "https://arxiv.org/html/2602.02537/x7.png",
                "caption": "",
                "position": 1414
            },
            {
                "img": "https://arxiv.org/html/2602.02537/x8.png",
                "caption": "",
                "position": 1421
            },
            {
                "img": "https://arxiv.org/html/2602.02537/x9.png",
                "caption": "",
                "position": 1428
            },
            {
                "img": "https://arxiv.org/html/2602.02537/x10.png",
                "caption": "",
                "position": 1435
            },
            {
                "img": "https://arxiv.org/html/2602.02537/x11.png",
                "caption": "",
                "position": 1442
            },
            {
                "img": "https://arxiv.org/html/2602.02537/x12.png",
                "caption": "",
                "position": 1449
            },
            {
                "img": "https://arxiv.org/html/2602.02537/x13.png",
                "caption": "",
                "position": 1456
            }
        ]
    },
    {
        "header": "Appendix BWorldVQA Showcases",
        "images": []
    }
]