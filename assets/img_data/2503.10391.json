[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10391/x1.png",
                "caption": "",
                "position": 103
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10391/x2.png",
                "caption": "Figure 2:Our overall pipeline consists of a Multimodal Large Language Model, a semantic alignment network (AlignerNet), a visual entity encoding network, and a Diffusion Transformer that integrates the embeddings encoded by these two modules.⊕direct-sum\\oplus⊕denotes concatenation.",
                "position": 183
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10391/x3.png",
                "caption": "Figure 3:Our language instruction template for MLLM.",
                "position": 243
            },
            {
                "img": "https://arxiv.org/html/2503.10391/x4.png",
                "caption": "Figure 4:An example of reference images, along with the corresponding video and caption, from our training set.",
                "position": 338
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10391/x5.png",
                "caption": "Figure 5:Qualitative evaluation of our method. The reference images are shown on the left, along with the text prompt at the bottom. In each case, we show four frames uniformly sampled from the generated 45-frame video.",
                "position": 378
            },
            {
                "img": "https://arxiv.org/html/2503.10391/x6.png",
                "caption": "Figure 6:Qualitative evaluation of our method dealing with multiple concepts. Our model is capable of encoding and understanding multiple subjects based on the reference images.",
                "position": 409
            },
            {
                "img": "https://arxiv.org/html/2503.10391/x7.png",
                "caption": "Figure 7:Qualitative comparison for ablation studies. The reference images and text prompt are shown on the top. The result of the full model is in the first line, followed by those from different ablation experiments. We show four frames uniformly sampled from the generated 45-frame video of each method.",
                "position": 420
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10391/x8.png",
                "caption": "Figure 8:A failure case of our method. The identities of the two persons are not clearly distinguishable or well-maintained in the generated video.",
                "position": 459
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]