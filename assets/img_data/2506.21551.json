[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Grokking (Delayed Generalization) in LLM Pretraining",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21551/x1.png",
                "caption": "Figure 1:Converged groups of pretraining samples (accumulated) over training steps and test accuracyon downstream tasks across four domains.\nIt shows the asynchronous memorization of local data groups and how it affects the global generalization: the test accuracy starts lately to rise sharply with the increasing converged training samples. But the starting time of grokking the induced gain vary across domains, explaining the difficulty to allocate global grokking.\nMath and coding tasks require more knowledge accumulation to generalize than commonsense and domain specific reasoning.",
                "position": 192
            },
            {
                "img": "https://arxiv.org/html/2506.21551/x2.png",
                "caption": "Figure 2:Training loss and test accuracy of four data groups over pretraining steps with local grokking.The pretraining data and test samples in each group are matched based on their semantic similarities.\nWe observe local grokking, i.e., delayed rise of test accuracy after training loss plateaued, on every group, though their delayed steps and grokking period lengths vary.\nFrom left to right, later converged and memorized (harder) samples exhibit systematically longer delays.",
                "position": 225
            }
        ]
    },
    {
        "header": "4Memorization-to-Generalization: Analysis of Routing Pathway Dynamics",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21551/x3.png",
                "caption": "Figure 3:Pathway Complexity Metrics to Monitor Grokking.Left: Pathway similarity between samples is measured by edit distance on their sequences of expert choices. Right: Pathway consistency quantifies the smoothness of expert transitions between subsequent layers via cosine similarity of their weighted expert embeddings.",
                "position": 273
            },
            {
                "img": "https://arxiv.org/html/2506.21551/x4.png",
                "caption": "Figure 4:Pathway edit distance and training loss of two data groups(converged at125‚Å¢k125ùëò125k125 italic_kand245‚Å¢k245ùëò245k245 italic_ksteps) over pretrianing steps across four domains.\nWhile training loss plateaued early, the edit distance between pathways keeps declining beyond convergence, indicating a declining complexity of internal memorization and a continuous transition from memorization to generalization.",
                "position": 323
            },
            {
                "img": "https://arxiv.org/html/2506.21551/x5.png",
                "caption": "Figure 5:Layer-wise evolution of pathway edit distance during pretraining.The overall distance drops from early to later layers, indicating higher complexity in early layers.\nFor each layer, the distance initially rises, then sharply declines at grokking onset. The shift is depth-dependent‚Äîearly layers‚Äô expert choices rapidly converges, later layers change more gradually, and the final layer diversifies after grokking.",
                "position": 327
            },
            {
                "img": "https://arxiv.org/html/2506.21551/x6.png",
                "caption": "Figure 6:Pathway consistency and training loss of two data groups(converged at490‚Å¢k490ùëò490k490 italic_kand615‚Å¢k615ùëò615k615 italic_ksteps) over pretrianing steps across four domains.\nWhile training loss converges early, pathway consistency continues to improve, revealing continuous refinement and reducing complexity of expert choices beyond memorization, which indicates a transition to generalization.",
                "position": 376
            },
            {
                "img": "https://arxiv.org/html/2506.21551/x7.png",
                "caption": "Figure 7:Correlation between generalization-monitor metrics and test accuracy, and linear regression on five checkpoints.\nPathway edit distance is negatively correlated with test accuracy, while pathway consistency is positively correlated‚Äîboth aligning with expectations.\nIn contrast, training loss and its moving average show weak or even misleading correlations with test accuracy, underscoring that lower training loss does not reliably indicate better generalization.",
                "position": 521
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AGeneralization Bound for MoE with Routing Kernel",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21551/x8.png",
                "caption": "Figure A.1:Correlation of Edit Distance with Effective Dimension.Each point represents an independent experimental run where the router is initialized with a different random seed and then fixed for expert training. This varying initialization leads to diverse fixed routing patterns. We observe a strong positive correlation. This suggests that the structural diversity of pre-defined routing significantly impacts the complexity of the function space learned by experts.",
                "position": 1213
            }
        ]
    },
    {
        "header": "Appendix BDataset Details",
        "images": []
    },
    {
        "header": "Appendix CExperiment Details",
        "images": []
    }
]