[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15271/x1.png",
                "caption": "Figure 1:Performance comparison of Eagle 2.5 with leading vision-language models on the Video-MME benchmark.Eagle 2.5 demonstrates consistent improvement as the number of input frames increases.",
                "position": 195
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Eagle 2.5",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15271/x2.png",
                "caption": "Figure 2:Tiling-based general multimodal system.",
                "position": 276
            },
            {
                "img": "https://arxiv.org/html/2504.15271/x3.png",
                "caption": "Figure 3:Image area preservation.Compared to the tiling strategy (a) from InternVLTeam (2024a), our method (b) effectively retains a larger portion of the original image, especially for high-resolution inputs. This ensures that more comprehensive visual information is preserved, benefiting tasks that require fine-grained details.",
                "position": 297
            },
            {
                "img": "https://arxiv.org/html/2504.15271/x4.png",
                "caption": "Figure 4:Comparison of video duration between open-source data and Eagle-Video-110K.",
                "position": 610
            },
            {
                "img": "https://arxiv.org/html/2504.15271/x5.png",
                "caption": "Figure 5:Overview of our video annotation framework combining bottom-up clip-level and top-down story-level approaches. The diagram illustrates our dual annotation strategy. In the bottom-up approach (left), short video clips are processed by GPT-4o to generate clip-level QA pairs enhanced with time anchors and textural context anchors. In the top-down approach (right), human annotators create story-level segmentations of longer videos, which are then captioned and processed by GPT-4 to generate comprehensive story-level QA pairs. This hierarchical methodology enables both fine-grained temporal understanding and high-level semantic comprehension of video content.",
                "position": 651
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15271/x6.png",
                "caption": "Figure 6:The impact of Eagle-Video-110K dataset and different post-training schedules on the performance of Video-MME.",
                "position": 1560
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix AAcknowledgments",
        "images": []
    },
    {
        "header": "Appendix BTraining and Inference",
        "images": []
    },
    {
        "header": "Appendix CAdditional Benchmarks",
        "images": []
    },
    {
        "header": "Appendix DTraining Data",
        "images": []
    },
    {
        "header": "Appendix EEagle-Video-110K",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]