[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09551/x1.png",
                "caption": "",
                "position": 200
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09551/x2.png",
                "caption": "Figure 2:(a)Overview ofÎ´ğ›¿\\deltaitalic_Î´-Diffusion.The context frameIğ¼Iitalic_Iis provided to the generation modelğ’¢ğ’¢\\mathcal{G}caligraphic_Galong with the action latentsÎ´Vsubscriptğ›¿ğ‘‰\\delta_{V}italic_Î´ start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPTextracted from the demonstration videoVğ‘‰Vitalic_V.\n(b)Extracting action latents.A spatial-temporal vision encoder is applied to extract temporally-aggregated spatiotemopral representationsğ³ğ³\\mathbf{z}bold_zfrom an input videoVğ‘‰Vitalic_V, withtğ‘¡titalic_tdenoting the temporal dimension.\nIn parallel, a spatial vision encoder extracts per-frame representations fromVğ‘‰Vitalic_V, which is aligned toğ³ğ³\\mathbf{z}bold_zby feature predictorğ’«ğ’«\\mathcal{P}caligraphic_Pasğ¡ğ¡\\mathbf{h}bold_h.\nThe appearance bottleneck then computes the action latentsÎ´Vsubscriptğ›¿ğ‘‰\\delta_{V}italic_Î´ start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPTby subtracting the aligned spatial representationsğ¡ğ¡\\mathbf{h}bold_hfrom the spatiotemporal representations.",
                "position": 305
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09551/x3.png",
                "caption": "Figure 3:Qualitative results for bottleneck ablationon the Something-Something v2 dataset(Goyal etÂ al.,2017).\nApplying no or temporal normalization bottleneck suffers from appearance leakage, while generation based on our appearance bottleneck preserves the input context.",
                "position": 474
            },
            {
                "img": "https://arxiv.org/html/2412.09551/x4.png",
                "caption": "Figure 4:Qualitative comparisonsofÎ´ğ›¿\\deltaitalic_Î´-Diffusion against MotionDirector(Zhao etÂ al.,2024b)and WALT(Gupta etÂ al.,2024)on (a) Something-Something v2(Goyal etÂ al.,2017), (b) Epic Kitchens 100(Damen etÂ al.,2018), and (c) Fractal(Brohan etÂ al.,2022)datasets.",
                "position": 635
            },
            {
                "img": "https://arxiv.org/html/2412.09551/x5.png",
                "caption": "Figure 5:Auto-regressive generationcontrolled via a concatenation of three different demonstration videos of varying lengths. The sequence of demonstrated action concepts (â€œpicking something from a drawer and placing it on the tableâ€, â€œclosing a drawerâ€, and â€œopening a drawerâ€) are coherently transferred to the input context.",
                "position": 638
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Broader Impact",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AAuthor Contributions",
        "images": []
    },
    {
        "header": "Appendix BImproving Generation Quality",
        "images": []
    },
    {
        "header": "Appendix CHuman Evaluation Setup",
        "images": []
    },
    {
        "header": "Appendix DDetails on Baselines",
        "images": []
    },
    {
        "header": "Appendix EQualitative Results",
        "images": []
    },
    {
        "header": "Appendix FControllability by Demonstration",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09551/x6.png",
                "caption": "Figure 6:Qualitative results of driving alternative generation from the same context image with different demonstration videos from the Something-Something v2 dataset(Goyal etÂ al.,2017).",
                "position": 1782
            },
            {
                "img": "https://arxiv.org/html/2412.09551/x7.png",
                "caption": "Figure 7:Qualitative results of driving alternative generation from the same context image with different demonstration videos from the Something-Something v2 dataset(Goyal etÂ al.,2017).",
                "position": 1786
            },
            {
                "img": "https://arxiv.org/html/2412.09551/x8.png",
                "caption": "Figure 8:Qualitative results of driving alternative generation from the same context image with different demonstration videos from the Fractal dataset(Brohan etÂ al.,2022).",
                "position": 1790
            },
            {
                "img": "https://arxiv.org/html/2412.09551/x9.png",
                "caption": "Figure 9:Qualitative results of driving alternative generation from the same context image with different demonstration videos from the Fractal dataset(Brohan etÂ al.,2022).",
                "position": 1794
            },
            {
                "img": "https://arxiv.org/html/2412.09551/x10.png",
                "caption": "Figure 10:Failure cases generated byÎ´ğ›¿\\deltaitalic_Î´-Diffusion.",
                "position": 1805
            }
        ]
    },
    {
        "header": "Appendix GFailure Cases",
        "images": []
    }
]