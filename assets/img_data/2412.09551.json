[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09551/x1.png",
                "caption": "",
                "position": 200
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09551/x2.png",
                "caption": "Figure 2:(a)Overview ofδ𝛿\\deltaitalic_δ-Diffusion.The context frameI𝐼Iitalic_Iis provided to the generation model𝒢𝒢\\mathcal{G}caligraphic_Galong with the action latentsδVsubscript𝛿𝑉\\delta_{V}italic_δ start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPTextracted from the demonstration videoV𝑉Vitalic_V.\n(b)Extracting action latents.A spatial-temporal vision encoder is applied to extract temporally-aggregated spatiotemopral representations𝐳𝐳\\mathbf{z}bold_zfrom an input videoV𝑉Vitalic_V, witht𝑡titalic_tdenoting the temporal dimension.\nIn parallel, a spatial vision encoder extracts per-frame representations fromV𝑉Vitalic_V, which is aligned to𝐳𝐳\\mathbf{z}bold_zby feature predictor𝒫𝒫\\mathcal{P}caligraphic_Pas𝐡𝐡\\mathbf{h}bold_h.\nThe appearance bottleneck then computes the action latentsδVsubscript𝛿𝑉\\delta_{V}italic_δ start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPTby subtracting the aligned spatial representations𝐡𝐡\\mathbf{h}bold_hfrom the spatiotemporal representations.",
                "position": 305
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09551/x3.png",
                "caption": "Figure 3:Qualitative results for bottleneck ablationon the Something-Something v2 dataset(Goyal et al.,2017).\nApplying no or temporal normalization bottleneck suffers from appearance leakage, while generation based on our appearance bottleneck preserves the input context.",
                "position": 474
            },
            {
                "img": "https://arxiv.org/html/2412.09551/x4.png",
                "caption": "Figure 4:Qualitative comparisonsofδ𝛿\\deltaitalic_δ-Diffusion against MotionDirector(Zhao et al.,2024b)and WALT(Gupta et al.,2024)on (a) Something-Something v2(Goyal et al.,2017), (b) Epic Kitchens 100(Damen et al.,2018), and (c) Fractal(Brohan et al.,2022)datasets.",
                "position": 635
            },
            {
                "img": "https://arxiv.org/html/2412.09551/x5.png",
                "caption": "Figure 5:Auto-regressive generationcontrolled via a concatenation of three different demonstration videos of varying lengths. The sequence of demonstrated action concepts (“picking something from a drawer and placing it on the table”, “closing a drawer”, and “opening a drawer”) are coherently transferred to the input context.",
                "position": 638
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Broader Impact",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AAuthor Contributions",
        "images": []
    },
    {
        "header": "Appendix BImproving Generation Quality",
        "images": []
    },
    {
        "header": "Appendix CHuman Evaluation Setup",
        "images": []
    },
    {
        "header": "Appendix DDetails on Baselines",
        "images": []
    },
    {
        "header": "Appendix EQualitative Results",
        "images": []
    },
    {
        "header": "Appendix FControllability by Demonstration",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09551/x6.png",
                "caption": "Figure 6:Qualitative results of driving alternative generation from the same context image with different demonstration videos from the Something-Something v2 dataset(Goyal et al.,2017).",
                "position": 1782
            },
            {
                "img": "https://arxiv.org/html/2412.09551/x7.png",
                "caption": "Figure 7:Qualitative results of driving alternative generation from the same context image with different demonstration videos from the Something-Something v2 dataset(Goyal et al.,2017).",
                "position": 1786
            },
            {
                "img": "https://arxiv.org/html/2412.09551/x8.png",
                "caption": "Figure 8:Qualitative results of driving alternative generation from the same context image with different demonstration videos from the Fractal dataset(Brohan et al.,2022).",
                "position": 1790
            },
            {
                "img": "https://arxiv.org/html/2412.09551/x9.png",
                "caption": "Figure 9:Qualitative results of driving alternative generation from the same context image with different demonstration videos from the Fractal dataset(Brohan et al.,2022).",
                "position": 1794
            },
            {
                "img": "https://arxiv.org/html/2412.09551/x10.png",
                "caption": "Figure 10:Failure cases generated byδ𝛿\\deltaitalic_δ-Diffusion.",
                "position": 1805
            }
        ]
    },
    {
        "header": "Appendix GFailure Cases",
        "images": []
    }
]