[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07833/x1.png",
                "caption": "Figure 1:(a, b) Next-token vs. multi-token training.Language models are typically trained using the next-token objective (left), where each token is an artificial text fragment. At every forward pass, models are tasked to predict the next immediate token. However, in the multi-token setting (right), models are tasked to predict the nextnùëõnitalic_ntokens in parallel in each forward pass, thus facilitating conceptual understanding across tokens. In this illustration, the relevant concept is ‚Äúribonucleic acid\", which is segmented into 5 tokens by the Llama 3 tokenizer.(c) Concept-aware fine-tuning (CAFT) architecture.Next-token models are modified into multi-token ones by training task-agnostic auxiliary headsFhksubscriptùêπsubscript‚ÑéùëòF_{h_{k}}italic_F start_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT(blue) that predict the nextkùëòkitalic_k-th token (Section2.2) forn=5ùëõ5n=5italic_n = 5. During multi-token fine-tuning, the auxiliary heads augment the cross-entropy loss to enable the multi-token training objective (Section2.3). Finally, only the original model layers (in yellow) are used for inference.(d) Examples of multi-token concepts.Our proposed method increases coherence across domains and modalities, including text, code, and mathematical expressions, given the prevalence of multi-token concepts.",
                "position": 104
            }
        ]
    },
    {
        "header": "2Concept-Aware Fine-Tuning (CAFT)",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07833/x2.png",
                "caption": "Figure 2:(a) Sample code implementation.Building on top of the industry-standard Transformers library(Wolf et¬†al.,2020), researchers and practitioners can incorporate CAFT into their existing Transformers training pipelines with just a few lines of code using our open-source librarycaft.(b) Downstream Tasks.These tasks empirically underscore the effectiveness and broad applicability of CAFT. The examples are adapted from the HumanEval, MATH-500, MIMIC-IV-BHC, L+M-24, and Mol-Instructions datasets respectively, which are used for the evaluations in Section3.2.",
                "position": 135
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07833/extracted/6525978/images/aux_head_ppl.png",
                "caption": "Figure 3:Perplexities of auxiliary heads over four epochs. Note that headFh1subscriptùêπsubscript‚Ñé1F_{h_{1}}italic_F start_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPTis not adapted and is only displayed for reference.",
                "position": 247
            },
            {
                "img": "https://arxiv.org/html/2506.07833/x3.png",
                "caption": "Figure 4:(a) Examples of CAFT‚Äôs concept-informed generation.A comparison of the ground truth, next-token fine-tuned model generation, and CAFT model generation for two questions from the HumanEval (top) and L+M-24 (bottom) datasets. The red boxes show the relevant proxy concepts, derived using the method described in Section3.3, and how CAFT is able to identify them.(b) Coding (HumanEval) Ablation.The full CAFT model performs disproportionately better on coding questions with a high number of concepts.(c) Molecular Generation (L+P-24) Ablation.The full CAFT model is substantially better at generating the relevant functional groups when required.",
                "position": 640
            }
        ]
    },
    {
        "header": "4Related Works",
        "images": []
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAuxiliary head training dataset details",
        "images": []
    },
    {
        "header": "Appendix BUnderstanding the CAFT hyperparameters",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07833/extracted/6525978/images/hyperparams.png",
                "caption": "Figure 5:Scaling of training losses of auxiliary heads foriterations=1000iterations1000\\textrm{iterations}=1000iterations = 1000,Œ±=0.8ùõº0.8\\alpha=0.8italic_Œ± = 0.8,Œ≤=0.01ùõΩ0.01\\beta=0.01italic_Œ≤ = 0.01, andŒ≥ùõæ\\gammaitalic_Œ≥.",
                "position": 1472
            }
        ]
    },
    {
        "header": "Appendix CExperiment Settings",
        "images": []
    }
]