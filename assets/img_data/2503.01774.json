[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.01774/x1.png",
                "caption": "",
                "position": 138
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.01774/x2.png",
                "caption": "Figure 2:Difix3D+pipeline.The overall pipeline of theDifix3D+model involves the following stages:Step 1: Given a pretrained 3D representation, we render novel views and feed them toDifixwhich acts as a neural enhancer, removing the artifacts and improving the quality of the noisy rendered views (Sec.4.1). The camera poses selected to render the novel views are obtained through pose interpolation, gradually approaching the target poses from the reference ones.Step 2: The cleaned novel views are distilled back to the 3D representation to improve its quality (Sec.4.2). Steps 1 and 2 are applied in several iterations to progressively grow the spatial extent of the reconstruction and hence ensure strong conditioning of the diffusion model (Difix3D).Step 3:Difixadditional acts as a real-time neural enhancer, further improving the quality of the rendered novel views.",
                "position": 149
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Background",
        "images": []
    },
    {
        "header": "4Boosting 3D Reconstruction with DM priors",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.01774/x3.png",
                "caption": "Figure 3:Difixarchitecture.Difixtakes a noisy rendered image and a reference views as input (left), and outputs an enhanced version of the input image with reduced artifacts (right).Difixalso generates identical reference views, which we discard in practice and hence depict transparent. The model architecture consists of a U-Net structure with a cross-view reference mixing layer (Sec.4.1) to maintain consistency across reference views.Difixis fine-tuned from SD-Turbo, using a frozen VAE encoder and a LoRA fine-tuned decoder.",
                "position": 276
            },
            {
                "img": "https://arxiv.org/html/2503.01774/x4.png",
                "caption": "Figure 4:Noise level.To validate our hypothesis that the distribution of images with NeRF/3DGS artifacts is similar to the distribution of noisy images used to train SD-Turbo[49], we perform single-step ‚Äúdenoising‚Äù at varying noise levels. At higher noise levels (e.g.,œÑ=600ùúè600\\tau=600italic_œÑ = 600), the model effectively removes artifacts but also alters the image context. At lower noise levels (e.g.,œÑ=10ùúè10\\tau=10italic_œÑ = 10), the model makes only minor adjustments, leaving most artifacts intact.œÑ=200ùúè200\\tau=200italic_œÑ = 200strikes a good balance, removing artifacts while preserving context, and achieves the highest metrics.",
                "position": 319
            },
            {
                "img": "https://arxiv.org/html/2503.01774/x5.png",
                "caption": "Figure 5:In-the-wild artifact removal.We show comparisons on held-out scenes from the DL3DV dataset[23](top, above the dashed line) and the Nerfbusters[70]dataset (bottom).Difix3D+corrects significantly more artifacts that other methods.",
                "position": 444
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.01774/x6.png",
                "caption": "Figure 6:Qualitative results on the RDS dataset.Difixfor RDS was trained on 40 scenes and 100,000 paired data samples.",
                "position": 670
            },
            {
                "img": "https://arxiv.org/html/2503.01774/x7.png",
                "caption": "Figure 7:Qualitative ablation of real-time post-render processing:Difix3D+uses an additional neural enhancer step that effectively removes residual artifacts, resulting in higher PSNR and lower LPIPS scores. The images displayed in green or red boxes correspond to zoomed-in views of the bounding boxes drawn in the main images.",
                "position": 725
            },
            {
                "img": "https://arxiv.org/html/2503.01774/x8.png",
                "caption": "Figure 8:Qualitative ablation results ofDifix3D+: The columns, labeled by method name, correspond to the rows inTab.4.",
                "position": 728
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "AAdditional Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.01774/x9.png",
                "caption": "Figure S1:Visual comparison ofDifixcomponents.Reducing the noise levelœÑùúè\\tauitalic_œÑ((c)vs.(d)), incorporating Gram loss ((b)vs.(c)), and conditioning on reference views ((a)vs.(b)) all improve our model.",
                "position": 2240
            },
            {
                "img": "https://arxiv.org/html/2503.01774/x10.png",
                "caption": "Figure S2:Visualization of the paired dataset:We utilize a variety of strategies to simulate corrupted training data, including sparse reconstruction, cycle reconstruction, cross-referencing, and intentional model underfitting. The curated paired dataset provides a strong learning signal for theDifixmodel.",
                "position": 2319
            }
        ]
    },
    {
        "header": "BAdditional Results",
        "images": []
    },
    {
        "header": "CLimitation and Future Work",
        "images": []
    }
]