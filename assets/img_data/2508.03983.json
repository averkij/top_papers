[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03983/figures/github-mark.png",
                "caption": "",
                "position": 95
            },
            {
                "img": "https://arxiv.org/html/2508.03983/figures/hf-logo.png",
                "caption": "",
                "position": 95
            },
            {
                "img": "https://arxiv.org/html/2508.03983/x1.png",
                "caption": "",
                "position": 97
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03983/x2.png",
                "caption": "Figure 1:Training cross entropy loss (next token) curves between ASR and caption based pretraining. General captions utilize the ACAVCaps (Table˜15) dataset, while ASR uses ACAV100M-Speech (Table˜14). ACAV100M-Speech contains up to 90 different languages, while captions are English only.",
                "position": 188
            }
        ]
    },
    {
        "header": "2Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03983/x3.png",
                "caption": "Figure 2:Proposed MiDashengLM framework. For all three stages, training is done with standard next-token prediction loss. Stage 1 aligns the audio encoder with the text modality, after which the audio encoder is taken and initialized for Stage 2.",
                "position": 196
            },
            {
                "img": "https://arxiv.org/html/2508.03983/x4.png",
                "caption": "Figure 3:Histrogram plot of training data sample lengths.",
                "position": 227
            }
        ]
    },
    {
        "header": "3Datasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03983/x5.png",
                "caption": "Figure 4:Our proposed data curation pipeline. We filter ACAV100M with an automatic pipeline, that predicts transcripts, sound events, sound quality and other meta information. A reasoning-LLM is then used to generate a caption from the provided meta information. The resulting curated dataset is then split into a training set (ACAVCaps) and a novel evaluation set Multi-Expert Chain for Audio Tasks (MECAT).",
                "position": 271
            },
            {
                "img": "https://arxiv.org/html/2508.03983/x6.png",
                "caption": "Figure 5:Subtasks of the proposed MECAT-QA testset.",
                "position": 358
            },
            {
                "img": "https://arxiv.org/html/2508.03983/x7.png",
                "caption": "Figure 6:Pretraining and SFT sampling across datasets.",
                "position": 473
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03983/x8.png",
                "caption": "Figure 7:Time to first token (TTFT) and Giga Multiply-Add Operations per Second (GMACs) comparison between MiDashengLM-7B and Qwen2.5-Omni-7B.",
                "position": 1520
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AData sources",
        "images": []
    },
    {
        "header": "Appendix BContributors",
        "images": []
    }
]