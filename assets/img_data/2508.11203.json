[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.11203/figures/i2i_comparisons.png",
                "caption": "Figure 1:Comparison between EAS and baseline text based i2i methods. Top row: Disney character, Bottom row: green Orc. See Section4.3.2for experimental details.",
                "position": 275
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.11203/figures/overview3rm.png",
                "caption": "Figure 2:StyleMM fine-tunes Deformation networkDs​r​cD_{src}and texture generatorGs​r​cG_{src}pre-trained on FLAME into stylized modelsDs​t​y​l​eD_{style}andGs​t​y​l​eG_{style}, respectively using Explicit Attribute-preserving Stylization (EAS). In this process, the Explicit Attribute-preserving Module (EAM) is a component of EAS that enables the preservation of alignments.",
                "position": 357
            },
            {
                "img": "https://arxiv.org/html/2508.11203/figures/EAS_rev.png",
                "caption": "Figure 3:Overview of Explicit Attribute-preserving Stylization.Left: training process of EAM. Right: Inference process of EAS, equipped with EAM.",
                "position": 363
            },
            {
                "img": "https://arxiv.org/html/2508.11203/figures/finetuning.png",
                "caption": "Figure 4:Overview of the StyleMM training pipeline.Our method stylizes a 3D morphable face model through a three-stage process that leverages distinct loss functions. A deformation networkDstyleD_{\\text{style}}and a texture generatorGstyleG_{\\text{style}}are optimized using style-supervised 2D image pairs rendered using random shapes, textures, and viewpoints. The proposed Consistent Displacement LossℒCDL\\mathcal{L}_{\\text{CDL}}encourages locally consistent deformation patterns across different identities as shown in the bottom right of the figure.",
                "position": 473
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.11203/figures/correspondance.png",
                "caption": "Figure 5:Stylized faces rendered with the same UV pattern to visualize vertex correspondence. Key regions remain aligned across identities.",
                "position": 631
            },
            {
                "img": "https://arxiv.org/html/2508.11203/figures/separatecontrol_big.png",
                "caption": "Figure 6:Disentangled parametric control of shape (left), expression (middle), and texture (right), with other factors fixed.",
                "position": 634
            },
            {
                "img": "https://arxiv.org/html/2508.11203/figures/beyondreal.png",
                "caption": "Figure 7:The t-SNE projection of randomly sampled faces for each style. Our model extends beyond the realistic representation, distinctly diverging from the representation space of FLAME.",
                "position": 637
            },
            {
                "img": "https://arxiv.org/html/2508.11203/figures/textre_style.png",
                "caption": "Figure 8:Generated Texture fromGs​r​cG_{src}(Realistic) and variants ofGs​t​y​l​eG_{style}. Each row shares the same texture code.",
                "position": 640
            },
            {
                "img": "https://arxiv.org/html/2508.11203/figures/qualrm.png",
                "caption": "Figure 9:Qualitative comparison of randomly sampled faces generated from the realistic 3DMM and the stylized parametric face models.",
                "position": 658
            },
            {
                "img": "https://arxiv.org/html/2508.11203/figures/stylemm_ablationrm.png",
                "caption": "Figure 10:Qualitative comparison for ablation study. Each row was are rendered using the same parameters. Close-up views of the eyes and mouth are provided for visualization.",
                "position": 812
            },
            {
                "img": "https://arxiv.org/html/2508.11203/figures/init_ablation.png",
                "caption": "Figure 11:Ablation study on adjusted initialization for EAS.",
                "position": 859
            },
            {
                "img": "https://arxiv.org/html/2508.11203/figures/user_study.png",
                "caption": "Figure 12:User study results. Ours showed superior results compared to baselines in Diversity, Fidelity, and Naturalness.",
                "position": 869
            }
        ]
    },
    {
        "header": "5Applications",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.11203/figures/videodriven.png",
                "caption": "Figure 13:Video-driven animation results of StyleMM.",
                "position": 877
            },
            {
                "img": "https://arxiv.org/html/2508.11203/figures/stylemm_stylize_pipeline.png",
                "caption": "Figure 14:Two-staged stylization pipeline.",
                "position": 891
            },
            {
                "img": "https://arxiv.org/html/2508.11203/figures/stylemm_stylize.png",
                "caption": "Figure 15:Stylization results from input images. When an input image is provided, the reconstruction stage generates a coarse face model (middle), and the refinement stage is applied to produce the final stylized 3D model (right).",
                "position": 894
            },
            {
                "img": "https://arxiv.org/html/2508.11203/figures/eyeball.png",
                "caption": "Figure 16:Left: original StyleMM output; Right: eyeball post-processing results.",
                "position": 911
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.11203/figures/intersection.png",
                "caption": "Figure 17:Self-intersection under exaggerated expressions",
                "position": 929
            }
        ]
    },
    {
        "header": "Acknowledgements",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.11203/figures/ours_all2.png",
                "caption": "Figure 18:Results of StyleMM, shape and textures are randomly sampled fromDs​t​y​l​eD_{style}andGs​t​y​l​eG_{style}.",
                "position": 1823
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]