[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08532/x1.png",
                "caption": "",
                "position": 132
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08532/x2.png",
                "caption": "Figure 2:Kontinuous Kontextenables finer control across diverse edits. It can do simultaneous changes in attributes hair color and structure, highly localized changes such as editing the panda’s mouth and geometric edits such as changing the size of the car.",
                "position": 142
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08532/x3.png",
                "caption": "Figure 3:Data generation.Our pipeline consists of three steps: (a) We generate an edit instruction for each source image using a pretrained VLM, then apply Flux Kontext, an instruction-driven editing model, to produce a full-strength edit. (b) We synthesize intermediate-strength edits using a diffusion-based morphing method[6], which inverts both the source and edited images into the diffusion latent space and interpolates their features. (c) To compensate for inconsistencies in the morphing sequence (Fig.5), we filter the samples based on the inversion quality and uniformity of the sequence.",
                "position": 189
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08532/x4.png",
                "caption": "Figure 4:Samples from diverse image editing categories in our synthesized dataset. We cover a wide range of global edits, including stylization, reimagination, and environment changes, as well as local edits such as appearance changes, material changes, attribute editing, and object morphing.",
                "position": 218
            },
            {
                "img": "https://arxiv.org/html/2510.08532/x5.png",
                "caption": "Figure 5:Generating intermediate images with Freemorph can introduce inconsistencies such as incomplete objects, abrupt jumps, or errors from diffusion inversion. We filter such cases to obtain a clean dataset with smooth trajectories.",
                "position": 232
            },
            {
                "img": "https://arxiv.org/html/2510.08532/x6.png",
                "caption": "Figure 6:Model architecture.(a) In a simple experiment, we scale the text-token modulation parameters in Flux Kontext with a scalar to generate edit variations. This perturbation produces edits of varying strengths, revealing that modulation parameters can govern edit strength. (b) Building on this insight, we design a lightweight projector network that maps a scalar edit strengthssto offsets of the text modulation parameters, enabling precise control over edit strength.",
                "position": 235
            },
            {
                "img": "https://arxiv.org/html/2510.08532/x7.png",
                "caption": "Figure 7:Adding text embeddings into the slider projector improves smoothness of edit transitions.",
                "position": 272
            },
            {
                "img": "https://arxiv.org/html/2510.08532/x8.png",
                "caption": "Figure 8:Our method enables continuous control for challenging geometric edits, including smooth transformations between animal shapes and seamless shape–color blending for eyeglass transition.",
                "position": 275
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08532/x9.png",
                "caption": "Figure 9:Visual Comparison.We evaluate against (a) image interpolation methods, where we first generate a full strength edit with Flux-Kontext and interpolate to obtain intermediate edits, and (b) domain-specific methods, which train separate LoRAs/Adapters for each attribute. Our generalized method achieves superior slider control with consistent image identity and smooth edit transitions.",
                "position": 331
            },
            {
                "img": "https://arxiv.org/html/2510.08532/x10.png",
                "caption": "Figure 10:User study win-rates (%) of our method against baselines in pairwise comparisons.",
                "position": 484
            }
        ]
    },
    {
        "header": "5Discussion and Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08532/x11.png",
                "caption": "Figure 11:Kontinuous Kontextcan enable fine-grained control over the edit strength for diverse instruction-driven image editing operations.",
                "position": 1163
            },
            {
                "img": "https://arxiv.org/html/2510.08532/x12.png",
                "caption": "Figure 12:Samples for generated edit instructions and the generated edits from Flux Kontext",
                "position": 1259
            },
            {
                "img": "https://arxiv.org/html/2510.08532/x13.png",
                "caption": "Figure 13:Samples trajectories from our synthesized dataset",
                "position": 1275
            },
            {
                "img": "https://arxiv.org/html/2510.08532/x14.png",
                "caption": "Figure 14:Inference time control in modulation space.We conducted a simple experiment by scaling the text modulation parameters with values ofv∈(0.5,1.3)v\\in(0.5,1.3)to generate multiple edits. While these edits varied across different scales, the variations did not consistently correlate with the intended edit strength. This highlights the need for a dedicated learning module that can translate such variations into user-interpretable strength control by accurately manipulating the modulation parameters.",
                "position": 1293
            },
            {
                "img": "https://arxiv.org/html/2510.08532/x15.png",
                "caption": "Figure 15:We performed one user study where we compute the alignment of the users scores given for smoothness of the sequence with the different variations of smoothness metrics. We foundδs​m​o​o​t​h(2)\\delta^{(2)}_{smooth}aligns well with the user preferences for smoothness indicating that it is a good metric to measure the smoothness.",
                "position": 1397
            },
            {
                "img": "https://arxiv.org/html/2510.08532/x16.png",
                "caption": "Figure 16:Qualitative interpretation for first order and second order smoothness.For slider-based image editing, second-order smoothness is more important than first-order smoothness, as it captures the local consistency needed for gradual, nuanced changes with slider controls.",
                "position": 1426
            },
            {
                "img": "https://arxiv.org/html/2510.08532/x17.png",
                "caption": "Figure 17:Comparison for identity preservation of our method against baselines.Our method smoothly transforms the image into target edit over different edit strengths, resulting in close to linear decay in identity change and preserving identity well in lower strengths. In contrast, baselines change the identity of the subject significantly even with small edit strengths and don’t change the image for stronger edits.",
                "position": 1436
            },
            {
                "img": "https://arxiv.org/html/2510.08532/x18.png",
                "caption": "Figure 18:Ablation over architecture ofKontinuous Kontext.",
                "position": 1447
            },
            {
                "img": "https://arxiv.org/html/2510.08532/x19.png",
                "caption": "Figure 19:Comparison with interpolation baselines.Morphing-based methods generate smooth transitions; however, they often introduce artifacts in the intermediate images or omit details such as leaves. Similarly, the video inbetweening model WAN produces strong artifacts in intermediate frames, as these appearance transitions are out of domain for an inbetweening model trained only on real data.",
                "position": 1450
            },
            {
                "img": "https://arxiv.org/html/2510.08532/x20.png",
                "caption": "Figure 20:Comparison with interpolation baselines.DiffMorpher and FreeMorph remove objects in the intermediate edits of the first examples. Moreover, DiffMorpher produces blurred outputs even for simple stylization transitions. The WAN inbetweening model generates transitions with abrupt jumps in both examples. In contrast, our method produces smooth transitions while preserving image identity.",
                "position": 1453
            },
            {
                "img": "https://arxiv.org/html/2510.08532/x21.png",
                "caption": "Figure 21:Comparison with MARBLE for material control",
                "position": 1456
            },
            {
                "img": "https://arxiv.org/html/2510.08532/x22.png",
                "caption": "Figure 22:Comparison with Concept Sliders for diverse attribute editing.",
                "position": 1459
            },
            {
                "img": "https://arxiv.org/html/2510.08532/x23.png",
                "caption": "Figure 23:We compare with additional inference time baselines.",
                "position": 1505
            },
            {
                "img": "https://arxiv.org/html/2510.08532/x24.png",
                "caption": "Figure 24:Extrapolation of edit strengths. One of the failure case of our method is it cannot generate edits with extrapolation well. In most cases, either it recreates the full edit image (s=1s=1), or reduce the extent of edit in extrapolation region.",
                "position": 1516
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]