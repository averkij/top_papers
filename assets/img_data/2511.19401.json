[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19401/x1.png",
                "caption": "",
                "position": 92
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3In-Video Instructions",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19401/x2.png",
                "caption": "Figure 2:In-Video Instruction controls generation by placing the instruction directly on the first frame, providing explicit spatial grounding for the instruction’s scope. This enables assigning independent, less ambiguous, and even multi-step sequential commands to different targets. During generation, we fix the textual prompt to “follow the instructions step by step” and rely solely on in-frame visual signals for control.",
                "position": 163
            },
            {
                "img": "https://arxiv.org/html/2511.19401/x3.png",
                "caption": "Figure 3:Spatial Localization Ability of In-Video Instructions. We use In-Video Instructions to localize a target object among multiple entities and execute the corresponding action. For the prompt-based baseline, we rely on ChatGPT-generated textual descriptions such as “the N-th object from the left” for locating. As shown, In-Video Instructions enable precise and unambiguous localization, whereas text-only prompts exhibit noticeable limitations in resolving object positions.",
                "position": 166
            }
        ]
    },
    {
        "header": "4Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19401/x4.png",
                "caption": "Figure 4:Controlling object motions or trajectories with in-video instructions.",
                "position": 262
            },
            {
                "img": "https://arxiv.org/html/2511.19401/x5.png",
                "caption": "Figure 5:Controlling camera motion with In-Video Instructions. We visualize the initial frame and the final output for seven camera-motion types: static, pan left, pan right, tilt down, tilt up, zoom in, and zoom out.",
                "position": 279
            },
            {
                "img": "https://arxiv.org/html/2511.19401/x6.png",
                "caption": "Figure 6:In-Video Instructions with Multiple Objects and Commands, enabling both sequential instructions that involve a series of actions and parallel instructions that manipulate different objects independently.",
                "position": 313
            },
            {
                "img": "https://arxiv.org/html/2511.19401/x7.png",
                "caption": "Figure 7:Synthesizing videos by manipulating multiple seed frames. We generate videos from several initial frames and use visual instructions to coordinate interactions across them; all videos in this setting are produced using Kling-2.5.",
                "position": 395
            }
        ]
    },
    {
        "header": "5Limitations",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    }
]