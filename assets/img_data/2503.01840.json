[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.01840/extracted/6248979/figs/logo.png",
                "caption": "",
                "position": 53
            },
            {
                "img": "https://arxiv.org/html/2503.01840/x1.png",
                "caption": "Figure 1:Scaling law evaluated on the MT-bench using LLaMA-Instruct 3.1 8B as the target model, with the x-axis representing the data scale relative to ShareGPT. The new architectural designs in EAGLE-3 enable an increasing scaling curve, which was never observed in the previous works.",
                "position": 98
            },
            {
                "img": "https://arxiv.org/html/2503.01840/x1.png",
                "caption": "",
                "position": 101
            },
            {
                "img": "https://arxiv.org/html/2503.01840/x2.png",
                "caption": "",
                "position": 106
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.01840/x3.png",
                "caption": "Figure 2:Speedup ratios of different methods at temperature=0. For the standard speculative sampling, Vicuna-13B uses Vicuna-68M as the draft model. In Table1, we present comparisons with additional methods, but this figure only showcases a subset. Chat model‚Äôs evaluation dataset is MT-bench, and the reasoning model‚Äôs evaluation dataset is GSM8K. DeepSeek R1 LLaMA 8B refers to DeepSeek-R1-Distill-LLaMA 8B.",
                "position": 115
            },
            {
                "img": "https://arxiv.org/html/2503.01840/x4.png",
                "caption": "Figure 3:Illustration oftraining-time test(the bottom part) and its comparison with other draft methods (the upper and middle parts).fùëìfitalic_fdenotes the feature,tùë°titalic_tdenotes the token, andaùëéaitalic_arepresents the unconstrained vectors. We use the hat to denote the predictions from models. All the methods shown in the figure use the token sequence from the previous time step, but for simplicity, this is not depicted in the figure. The input to EAGLE-3 is not actuallyfùëìfitalic_f, but it is not shown in this figure. We will provide a detailed explanation in the following section.",
                "position": 132
            },
            {
                "img": "https://arxiv.org/html/2503.01840/x5.png",
                "caption": "Figure 4:Comparison of acceptance rates across different methods, with the x-axis representing the data scale relative to ShareGPT.",
                "position": 135
            },
            {
                "img": "https://arxiv.org/html/2503.01840/x5.png",
                "caption": "",
                "position": 138
            },
            {
                "img": "https://arxiv.org/html/2503.01840/x6.png",
                "caption": "",
                "position": 142
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3EAGLE-3",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.01840/x7.png",
                "caption": "Figure 5:Diagram of the EAGLE-3 inference pipeline, illustrating the three steps of the draft model.lùëôlitalic_l,mùëömitalic_m, andh‚Ñéhitalic_hrepresent the low, middle, and high-level features of the target model, respectively.eùëíeitalic_edenotes the embedding.",
                "position": 219
            },
            {
                "img": "https://arxiv.org/html/2503.01840/x8.png",
                "caption": "Figure 6:Diagram of the attention causal masks during training-time test. It sequentially shows a native training step (the first step) and two simulated training steps (the second and third steps). The arrows between tokens represent contextual relationships. Thegraytokens represent the training data while theblueandyellowtokens represent the first- and second-round predictions by the draft model, respectively.",
                "position": 238
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.01840/x9.png",
                "caption": "Figure 7:Acceptance rate of EAGLE and EAGLE-3 on MT-bench, with the target model being LLaMA-Instruct 3.1 8B. Hereby,nùëõnitalic_n-Œ±ùõº\\alphaitalic_Œ±refers to the acceptance rate when the input containsnùëõnitalic_nestimated features, under the condition that the previous estimated tokens are all accepted by the target model.",
                "position": 715
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    }
]