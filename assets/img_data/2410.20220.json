[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.20220/extracted/5948805/fig/ORCID-iD_icon-128x128.png",
                "caption": "",
                "position": 97
            },
            {
                "img": "https://arxiv.org/html/2410.20220/extracted/5948805/fig/robonerf_survey_teaser.png",
                "caption": "",
                "position": 139
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.20220/extracted/5948805/fig/neural_field_papers.png",
                "caption": "Figure 2:Growth of Neural Fields in Robotics:plotted as a rough number of publications vs. % of total neural field publications per year.",
                "position": 198
            },
            {
                "img": "https://arxiv.org/html/2410.20220/x1.png",
                "caption": "Figure 3:Timelineof Neural Fields in Robotics paper showing key papers over the years divided into 5 major application areas.",
                "position": 201
            }
        ]
    },
    {
        "header": "2Formulation of Neural Fields",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.20220/extracted/5948805/fig/nf_representations.png",
                "caption": "Figure 4:Neural Field Representations:Section2discusses four core Neural Field representations — Occupancy Networks[42], Signed Distance Fields[23], Neural Radiance Fields[22], and 3D Gaussian Splatting[49].",
                "position": 343
            }
        ]
    },
    {
        "header": "3Neural Fields for Robotics",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.20220/extracted/5948805/fig/spalttam.png",
                "caption": "Figure 6:Mapping and tracking results from SplaTam[121].",
                "position": 549
            },
            {
                "img": "https://arxiv.org/html/2410.20220/extracted/5948805/fig/niceslam.png",
                "caption": "Figure 7:Network architecture of Nice-SLAM[118].",
                "position": 563
            },
            {
                "img": "https://arxiv.org/html/2410.20220/extracted/5948805/fig/bundlesdf.png",
                "caption": "Figure 8:BundleSDF[58]for object tracking and reconstruction.",
                "position": 584
            },
            {
                "img": "https://arxiv.org/html/2410.20220/extracted/5948805/fig/nerfdet.png",
                "caption": "Figure 9:NeRF-Det’s[57]3D detection pipeline using NeRFs.",
                "position": 750
            },
            {
                "img": "https://arxiv.org/html/2410.20220/extracted/5948805/fig/f3rm.png",
                "caption": "Figure 10:Distilled feature fields[4]distill foundation model features into a feature field along with modeling a NeRF.",
                "position": 800
            },
            {
                "img": "https://arxiv.org/html/2410.20220/extracted/5948805/fig/dexnerf.png",
                "caption": "Figure 11:Dex-NeRF[63]leverages NeRF’s depthmap rendering for transparent object grasping.",
                "position": 820
            },
            {
                "img": "https://arxiv.org/html/2410.20220/extracted/5948805/fig/teaser_graspnerf.png",
                "caption": "Figure 12:Generalizable grasping with sparse multi-view images using GraspNeRF[68].",
                "position": 837
            },
            {
                "img": "https://arxiv.org/html/2410.20220/extracted/5948805/fig/autonerf.png",
                "caption": "Figure 13:AutoNeRF[80]generates 3D models of a scene by training NeRFs from data collected by\nautonomous agents.",
                "position": 847
            },
            {
                "img": "https://arxiv.org/html/2410.20220/extracted/5948805/fig/active_neural_mapping.png",
                "caption": "Figure 14:Active exploration of a mobile robot to minimize prediction uncertainty[81].",
                "position": 861
            },
            {
                "img": "https://arxiv.org/html/2410.20220/extracted/5948805/fig/clipfields.png",
                "caption": "Figure 15:Clip-Fields’s[85]semantic representation enables 3D spatial memory for mobile robots.",
                "position": 874
            },
            {
                "img": "https://arxiv.org/html/2410.20220/extracted/5948805/fig/DRD_teaser.png",
                "caption": "Figure 16:Differentiable Robot rendering pipeline[10].",
                "position": 904
            },
            {
                "img": "https://arxiv.org/html/2410.20220/extracted/5948805/fig/pac_nerf_materials.png",
                "caption": "Figure 17:An overview of the different materials model-based NFs are able to simulate[89].",
                "position": 918
            },
            {
                "img": "https://arxiv.org/html/2410.20220/extracted/5948805/fig/streetgaussians.png",
                "caption": "Figure 18:The compositional pipeline for Street Gaussians[213].",
                "position": 972
            },
            {
                "img": "https://arxiv.org/html/2410.20220/extracted/5948805/fig/unisim2.png",
                "caption": "Figure 19:Photorealistic editing results from UniSim[92].",
                "position": 978
            },
            {
                "img": "https://arxiv.org/html/2410.20220/extracted/5948805/fig/ngp.png",
                "caption": "Figure 20:An overview of Neural Groundplans approach.[94]",
                "position": 991
            }
        ]
    },
    {
        "header": "4Open Challenges of Neural Fields in Robotics",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]