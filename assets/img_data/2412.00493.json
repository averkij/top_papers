[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.00493/x1.png",
                "caption": "Figure 1:Comparison of previous work and our method: (a) Previous 3D LLMs are initialized on MLLMs trained solely on image-text pairs, and learn point cloud or voxel representations via fine-tuning on 3D scenes. The 3D point clouds are reconstructed from RGB-D videos.\n(b) Our method directly utilizes video frames and 3D coordinates as input, where the 3D coordinates are converted from depths through coordinate transformation.\nWe then transfer the ability of video understanding to 3D scene understanding by injecting position information into video representations.",
                "position": 73
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.00493/x2.png",
                "caption": "Figure 2:The overview of the model architecture.\n(a) shows the integration of video sequence and global coordinates for creating position-aware video representations.\n(b) and (c) detail the examples of 3D dense captioning and 3D visual grounding, respectively.\nOur approach can generalize well to other 3D tasks.",
                "position": 124
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.00493/x3.png",
                "caption": "Figure 3:The visualization results on ScanRefer.\nThe green/red/blue colors indicate the correct/incorrect/ground truth boxes.",
                "position": 1167
            },
            {
                "img": "https://arxiv.org/html/2412.00493/x4.png",
                "caption": "Figure 4:The visualization results on Scan2Cap.\nThe input boxes are marked in blue.",
                "position": 1173
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BDetailed Comparison",
        "images": []
    }
]