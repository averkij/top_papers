[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05172/x1.png",
                "caption": "Figure 2:Video VLM vs. CoV.Unlike prior approaches(top)that rely on fixed-frame video inputs and answer from a limited temporal window, our chain-of-view framework(bottom)explores an open-ended view space constructed from a 3D scene. CoV dynamically selects informative viewpoints and performs step-by-step reasoning during inference, enabling more complete and grounded answers without additional training.",
                "position": 133
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05172/x2.png",
                "caption": "Figure 3:Action-reasoning chain of the CoV agent.The CoV agent executes an iterative action–reasoning chain. For the question “What should I do to cool down?”, the agent first selects view 6 from the input images as an anchor. It then adjusts the viewpoint at each reasoning step to acquire new observations. Once the agent determines that sufficient information has been obtained, it outputs the answer “turn on the air conditioner.”",
                "position": 209
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05172/figures/Figure4.png",
                "caption": "Figure 4:Visualization of CoV reasoning results. Our method selects informative views and produces coherent multi-step answers grounded in the spatial context.",
                "position": 327
            },
            {
                "img": "https://arxiv.org/html/2601.05172/x3.png",
                "caption": "Figure 5:Action Step Analysis.Distribution of action steps for Qwen3-VL-Flash(Bai et al.,2025)on the OpenEQA(Majumdar et al.,2024)dataset.",
                "position": 725
            },
            {
                "img": "https://arxiv.org/html/2601.05172/x4.png",
                "caption": "Figure 6:Test-time scaling ability of CoV.On OpenEQA, we evaluate different VLMs and observe that the performance of all models gradually improves as the number of action steps increases.",
                "position": 728
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APrompt Templates",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05172/x5.png",
                "caption": "Figure 7:Prompt template for baseline.",
                "position": 1445
            },
            {
                "img": "https://arxiv.org/html/2601.05172/x6.png",
                "caption": "Figure 8:Prompt template for coarse-grained view selection agent.",
                "position": 1448
            },
            {
                "img": "https://arxiv.org/html/2601.05172/x7.png",
                "caption": "Figure 9:Prompt template for fine-grained CoV agent.",
                "position": 1451
            },
            {
                "img": "https://arxiv.org/html/2601.05172/x8.png",
                "caption": "Figure 10:CoV identifies the soap and paper towel dispensers between two sinks and reasons about material and spatial attributes such as the metal trash can and wall-mounted sink.",
                "position": 1463
            },
            {
                "img": "https://arxiv.org/html/2601.05172/x9.png",
                "caption": "Figure 11:CoV accurately localizes furniture such as brown cushion sofas, chairs beside a table, and a tall bookshelf positioned near the chalkboard, demonstrating spatial alignment and object identification.",
                "position": 1466
            },
            {
                "img": "https://arxiv.org/html/2601.05172/x10.png",
                "caption": "Figure 12:Given a desk scene, CoV correctly identifies multiple items on the desk and reasons about their positions, such as the shelf to the right of the monitor and office chairs nearby.",
                "position": 1469
            },
            {
                "img": "https://arxiv.org/html/2601.05172/x11.png",
                "caption": "Figure 13:CoV answers layout and appearance questions, including identifying brown-toned wooden chairs, the refrigerator’s proximity to glass doors, and counting the number of chairs facing the table.",
                "position": 1472
            }
        ]
    },
    {
        "header": "Appendix BResult Visualization",
        "images": []
    }
]