[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18759/x1.png",
                "caption": "Figure 1.An overview of\\sys, an example-driven assistant for mobile UI design featuring three main panels: (A)Conversation Panel: Users describe their design goals and interact with the system through three modes: Chat (generate and refine UI designs), Search (retrieve relevant UIs for inspiration), and Apply (adapt entire design throughglobal remixor specific components throughlocal remixfrom selected examples).\n(B)Example Gallery: Displays retrieved real-world UI examples along with source transparency cues (e.g., ratings, download counts, developer information) to help users assess credibility. (C)Editable Canvas: Presents a live preview of the current design, supporting toggling between visual and code views.",
                "position": 189
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": []
    },
    {
        "header": "2.Related Work",
        "images": []
    },
    {
        "header": "3.\\sysSystem Design",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18759/x2.png",
                "caption": "Figure 2.MMRAGPipeline. The process begins with preprocessing UI screenshots from the Mobbin(Mobbin,2025), Interaction Mining(Interactionmining,2025), and MobileViews(Gaoet al.,2024)datasets, along with metadata from the Google Play Store. Each screenshot is linked to its corresponding metadata through indexing.\nNext, the UI screenshots are passed through an encoder to generate embeddings. The embeddings, together with the corresponding image content, are stored in ChromaDB. When a user submits a query, it is converted into embeddings, and the system retrieves and ranks relevant screenshots using cosine similarity.\nThe retrieved UIs, along with their metadata, are then displayed in the Example Gallery for the user to browse and select desirable designs.\nOnce a user selects a UI, the selected design — combined with the user’s query, current UI code, and target area (used only for local remix) — is formatted into a complete prompt and sent to theMLLM, which generates updated UI code in a diff-patch format. The system backend parses these responses using theunidiffanddifflibpackages, updates the UI code, and renders them as UI previews.",
                "position": 1009
            }
        ]
    },
    {
        "header": "4.User Study",
        "images": []
    },
    {
        "header": "5.Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18759/figures/source/figure1_1.png",
                "caption": "Table 4.Results of post-task questionnaire comparingAlphaandBeta. Wilcoxon signed-rank tests with Bonferroni correction were conducted for each statement. Reported are Bonferroni-corrected p-values (p<.05p<.05*;p<.01p<.01**).",
                "position": 2089
            },
            {
                "img": "https://arxiv.org/html/2601.18759/figures/source/figure1_2.png",
                "caption": "",
                "position": 2214
            },
            {
                "img": "https://arxiv.org/html/2601.18759/figures/source/figure1_3.png",
                "caption": "",
                "position": 2269
            },
            {
                "img": "https://arxiv.org/html/2601.18759/figures/source/figure1_4.png",
                "caption": "",
                "position": 2324
            },
            {
                "img": "https://arxiv.org/html/2601.18759/figures/source/figure1_5.png",
                "caption": "",
                "position": 2379
            },
            {
                "img": "https://arxiv.org/html/2601.18759/figures/source/figure1_6.png",
                "caption": "",
                "position": 2434
            },
            {
                "img": "https://arxiv.org/html/2601.18759/figures/source/figure1_7.png",
                "caption": "",
                "position": 2489
            },
            {
                "img": "https://arxiv.org/html/2601.18759/figures/source/figure1_8.png",
                "caption": "",
                "position": 2544
            },
            {
                "img": "https://arxiv.org/html/2601.18759/figures/source/figure1_9.png",
                "caption": "",
                "position": 2599
            }
        ]
    },
    {
        "header": "6.Discussion and Future Work",
        "images": []
    },
    {
        "header": "7.Conclusion",
        "images": []
    },
    {
        "header": "8.GenAI Usage Disclosure",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18759/figures/source/baseline.png",
                "caption": "Figure 3.A screenshot of GPT-Canvas baseline system.",
                "position": 3992
            }
        ]
    },
    {
        "header": "Appendix AGPT-Canvas baseline Configurations",
        "images": []
    }
]