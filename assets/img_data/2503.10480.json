[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10480/x1.png",
                "caption": "Figure 1:Overview of D2PO: World modeling enables better embodied task planning through joint preference optimization of state prediction and action selection.",
                "position": 295
            }
        ]
    },
    {
        "header": "2Relate Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10480/x2.png",
                "caption": "Figure 2:Our method consists of two dimensions: (a) Data Exploration via Step-wise Tree Search (Sec3.2), which collects preference data through sampling and selecting potential actions, iterative tree expansion, and trajectory backtracking; (b) Dual Preference Optimization (D2PO) framework (Sec3.3) that leverages the collected preference pairs to jointly optimize action selection and state prediction.",
                "position": 406
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": []
    },
    {
        "header": "5Further Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10480/x3.png",
                "caption": "(a)Impact of data scale on performance (SR).",
                "position": 1320
            },
            {
                "img": "https://arxiv.org/html/2503.10480/x3.png",
                "caption": "(a)Impact of data scale on performance (SR).",
                "position": 1322
            },
            {
                "img": "https://arxiv.org/html/2503.10480/x4.png",
                "caption": "(b)Impact of model scale on performance (SR).",
                "position": 1325
            },
            {
                "img": "https://arxiv.org/html/2503.10480/x5.png",
                "caption": "Figure 4:Success rates (SR) of action-conditioned and goal-directed world models across seen and unseen scenarios.",
                "position": 1348
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AVoTa-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10480/x6.png",
                "caption": "(a)ALFRED (high-level planning)(Shridhar et al.,2019)",
                "position": 2296
            },
            {
                "img": "https://arxiv.org/html/2503.10480/x6.png",
                "caption": "(a)ALFRED (high-level planning)(Shridhar et al.,2019)",
                "position": 2298
            },
            {
                "img": "https://arxiv.org/html/2503.10480/x7.png",
                "caption": "(b)LoTa-Bench(Choi et al.,2024)",
                "position": 2301
            },
            {
                "img": "https://arxiv.org/html/2503.10480/x8.png",
                "caption": "(c)VoTa-Bench (ours)",
                "position": 2304
            },
            {
                "img": "https://arxiv.org/html/2503.10480/x9.png",
                "caption": "(a)Seen Scenes",
                "position": 2415
            },
            {
                "img": "https://arxiv.org/html/2503.10480/x9.png",
                "caption": "(a)Seen Scenes",
                "position": 2417
            },
            {
                "img": "https://arxiv.org/html/2503.10480/x10.png",
                "caption": "(b)Unseen Scenes",
                "position": 2420
            }
        ]
    },
    {
        "header": "Appendix BDetails of Preference Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10480/x11.png",
                "caption": "Figure 7:Distribution of the SFT and DPO dataset across different task types.",
                "position": 2660
            }
        ]
    },
    {
        "header": "Appendix CError Analysis",
        "images": []
    },
    {
        "header": "Appendix DCase Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10480/extracted/6278086/imgs/trial_T20190909_112854_740612_1_fail.png",
                "caption": "(a)SFT Trajectory (Fail)",
                "position": 2752
            },
            {
                "img": "https://arxiv.org/html/2503.10480/extracted/6278086/imgs/trial_T20190909_112854_740612_1_fail.png",
                "caption": "(a)SFT Trajectory (Fail)",
                "position": 2754
            },
            {
                "img": "https://arxiv.org/html/2503.10480/extracted/6278086/imgs/trial_T20190909_112854_740612_1_success.png",
                "caption": "(b)D2PO Trajectory (Success)",
                "position": 2757
            },
            {
                "img": "https://arxiv.org/html/2503.10480/extracted/6278086/imgs/trial_T20190908_070946_578973_2_success.png",
                "caption": "(a)SFT Trajectory (Success)",
                "position": 2763
            },
            {
                "img": "https://arxiv.org/html/2503.10480/extracted/6278086/imgs/trial_T20190908_070946_578973_2_success.png",
                "caption": "(a)SFT Trajectory (Success)",
                "position": 2765
            },
            {
                "img": "https://arxiv.org/html/2503.10480/extracted/6278086/imgs/trial_T20190908_070946_578973_2_success_1.png",
                "caption": "(b)D2PO Trajectory (Success)",
                "position": 2768
            }
        ]
    },
    {
        "header": "Appendix EPrompt Template",
        "images": []
    }
]