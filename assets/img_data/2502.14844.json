[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14844/extracted/6219306/images/teaser.jpg",
                "caption": "Figure 1.We personalize a video model to capturedynamic concepts– entities defined not only by their appearance but also by their unique motion patterns, such as the fluid motion of ocean waves or the flickering dynamics of a bonfire (left). This enables high-fidelity generation, editing, and the composition of these dynamic elements into a single video, where they interact naturally (right).",
                "position": 102
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14844/x1.png",
                "caption": "Figure 2.Set-and-Sequenceframework operates in two stages:\n(i) Identity Basis: We trainLoRA Set Encodingon a unordered set of frames extracted from the video, focusing only on the appearance of the dynamic concept to achieve high fidelity without temporal distractions.\n(ii) Motion Residuals: The Basis of the Identity LoRAs is frozen and the coefficient part is augmented with coefficients ofLoRA Sequence Encodingtrained on thetemporal sequenceof full video clip, allowing the model to capture the motion dynamics of the concept.",
                "position": 118
            }
        ]
    },
    {
        "header": "2.Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14844/extracted/6219306/images/editing.jpg",
                "caption": "Figure 3.Local and Global Editing.OurSet-and-Sequenceframework enables text-driven edits of dynamic concepts while preserving both their appearance and motion. Edits can be global (e.g., background and lighting) or local (e.g., clothing and object replacement), ensuring high fidelity to the original dynamic concepts.",
                "position": 151
            }
        ]
    },
    {
        "header": "3.Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14844/extracted/6219306/images/pixar.jpg",
                "caption": "Figure 4.Stylization.Top: Stylization of dynamic concepts achieved by reweighting the identity basis. Bottom: Stylization and motion editing performed using prompt derived from the video in the top row.",
                "position": 196
            },
            {
                "img": "https://arxiv.org/html/2502.14844/extracted/6219306/images/compose.jpg",
                "caption": "Figure 5.Dynamic Concepts Composition.Composition results achieved by our framework showcasing seamless integration of dynamic concepts. with each concept color-coded for clarity. For a more comprehensive demonstration, refer to the supplementary videos.",
                "position": 199
            },
            {
                "img": "https://arxiv.org/html/2502.14844/extracted/6219306/images/compare.jpg",
                "caption": "Figure 6.Comparison with baselines.Comparison of our method with baseline approaches (NewMove(Materzyńska et al.,2024), DreamVideo(Wei et al.,2024), DB-LoRA(Ryu,2023; Ruiz et al.,2023a), and DreamMix(Molad et al.,2023)) on two editing scenarios: changing the background and shirt, and adding a glass. Our method demonstrates superior adherence to the prompt while preserving the subject identity, outperforming the baselines.",
                "position": 293
            }
        ]
    },
    {
        "header": "4.Experiment Settings",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14844/extracted/6219306/images/ablation.jpg",
                "caption": "Figure 7.Ablation.Ablation of design choices on the editing task of adding a different shirt and background. Low-rank LoRA (LoRA-1) results in underfitting, failing to capture sufficient detail, while high-rank LoRA (LoRA-8) overfits, compromising adaptability. Our two-stage approach with added regularization achieves a balanced trade-off, preserving both fidelity and editability.",
                "position": 402
            }
        ]
    },
    {
        "header": "5.Experiment Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14844/extracted/6219306/images/stitch.jpg",
                "caption": "Figure 8.Stitched Example.To address identity leakage when generating multiple identities and motions from multiple videos (Second Column), we augment training with stitched examples by combining videos side by side to generate new compositions with preserved identities (Third Column).",
                "position": 625
            }
        ]
    },
    {
        "header": "6.Limitations",
        "images": []
    },
    {
        "header": "7.Conclusion",
        "images": []
    },
    {
        "header": "8.Architecture and Training Details",
        "images": []
    },
    {
        "header": "9.Prompts",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]