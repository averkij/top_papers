[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.05902/x1.png",
                "caption": "Figure 1:We provide a timeline of representative visual autoregressive models, which illustrates the rapid evolution of visual autoregressive models from early pixel-based approaches like PixelRNN in 2016 to various advanced systems recently. We are excitedly witnessing the rapid growth in this field.",
                "position": 180
            }
        ]
    },
    {
        "header": "2Autoregressive Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.05902/x2.png",
                "caption": "Figure 3:Illustration of three types of visual autoregressive models general frameworks based on their sequence representation strategies.Next-Pixel Predictionflattens the image into apixel sequence.Next-Token Predictionconverts the image into atoken sequencevia a visual tokenizer.Next-Scale Predictionemploys a multi-scale tokenizer to generate amulti-scale sequence.",
                "position": 397
            },
            {
                "img": "https://arxiv.org/html/2411.05902/x2.png",
                "caption": "Figure 3:Illustration of three types of visual autoregressive models general frameworks based on their sequence representation strategies.Next-Pixel Predictionflattens the image into apixel sequence.Next-Token Predictionconverts the image into atoken sequencevia a visual tokenizer.Next-Scale Predictionemploys a multi-scale tokenizer to generate amulti-scale sequence.",
                "position": 400
            },
            {
                "img": "https://arxiv.org/html/2411.05902/x3.png",
                "caption": "Figure 4:Core components in visual autoregressive models. (a)Sequence Representationencodes visual data into the discrete visual sequence, followed by reconstruction. (b)Autoregressive Sequence Modelingpredicts each element sequentially.",
                "position": 405
            }
        ]
    },
    {
        "header": "3Visual Autoregressive Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.05902/x4.png",
                "caption": "Figure 6:Illustration ofdifferent frameworks for auto-regressive text-to-image generation. (a) Generally, autoregressive text-to-image generation involves tokenizing text and images, and model all the tokens with a autoregressive transformer. (b) With the flexibility of autoregressive transformers, the autoregressive generation can be combined with other process, such as the denoising process or multi-scale generation process. (c) With the bi-directional transformer, the autoregressive generation sequence does not need to be a pre-defined order (such as raster scan), but can be any random order.",
                "position": 848
            },
            {
                "img": "https://arxiv.org/html/2411.05902/x5.png",
                "caption": "Figure 7:Frameworks for image painting. (a)BAT-Fill(Yu et al.,2021b)inpaints the image by rearranging the input tokens and auto-regressively generate output. (b)Query-OTR(Yao et al.,2022)does image outpainting by expanding input tokens into query tokens.",
                "position": 928
            },
            {
                "img": "https://arxiv.org/html/2411.05902/x6.png",
                "caption": "Figure 8:For autoregressive image editing,make-a-scenehandles text tokens, scene tokens and image tokens with a autoregressive transformer.",
                "position": 948
            },
            {
                "img": "https://arxiv.org/html/2411.05902/x7.png",
                "caption": "Figure 9:The pipeline of the auto-regressive video generation. The multi-modal input tokens are tokenized by different encoders, and set as prefix conditions for autoregressive models. The output tokens are autoregressively generated.",
                "position": 995
            },
            {
                "img": "https://arxiv.org/html/2411.05902/x8.png",
                "caption": "Figure 10:The overview of auto-regressive multi-modal LLMs’ application.The diagram highlights a high-level architecture combining both text and image modalities within auto-regressive LLMs. The image and text encoders process input data, which is subsequently integrated by the multimodal model to generate text and image outputs. Two image decoding strategies are shown: (a) Diffusion-based image decoder, where continuous image representations are progressively refined using a transformer-based diffusion process, and (b) VQ-based image decoder, where discrete image tokens are transformed into visual outputs. The framework is capable of handling a range of tasks, including pure text-based tasks (e.g., translation), vision-language tasks (e.g., VQA), text-to-image generation, image editing, and multi-modal conversational applications.",
                "position": 1154
            }
        ]
    },
    {
        "header": "4Evaluation Metrics",
        "images": []
    },
    {
        "header": "5Challenges and Future Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]