[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18450/x1.png",
                "caption": "Figure 1:Proposed3DGraphLLMapproach leverages 3D semantic scene graph learnable representation supplied as input to an LLM to perform various 3D vision-language tasks.",
                "position": 78
            }
        ]
    },
    {
        "header": "2Related works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18450/x2.png",
                "caption": "Figure 2:The overall architecture of our approach. 3DGraphLLM leverages pre-trained encoders for 3D object point clouds and semantic relationships between objects. We introduce trainable layers to map the extracted graph node and edge features into the token embedding space of a pre-trained LLM. The scene graph is flattened for input into the LLM, with each object represented by a subgraph of itsknearest neighbors. To further adapt the LLM to 3D vision-language tasks, we add new object tokens to the LLMâ€™s vocabulary and fine-tune it using LoRa.",
                "position": 144
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18450/x3.png",
                "caption": "Figure 3:Qualitative examples of 3DGraphLLM performance on the ScanRefer dataset. For each query, we provide an RGB image from the ScanNet dataset showing the selected object, along with a visualization of the RGB point cloud. In the point cloud, green points indicate the points that 3DGraphLLM identified as corresponding to the object from the text query, while the green box highlights the ground truth (GT) box for the query.",
                "position": 1325
            },
            {
                "img": "https://arxiv.org/html/2412.18450/x4.png",
                "caption": "Figure 4:Comparison of Uni3D object features and VL-SAT semantic edge features for the two nearest neighbors (NNs) based on ground-truth (GT) scene segmentation and Mask3D scene segmentation within the ScanNet training set.Left: Uni3D object features are relatively close for GT point clouds and Mask3D point clouds.Center: using the standard approach for selecting NNs to generate VL-SAT features, the features for pairs of Mask3D point clouds differ significantly from those of GT point clouds.Right: after applying a minimum neighbor distance filter for selecting NNs, the VL-SAT features for object pairs from Mask3D instance segmentation align more closely with those from GT instance segmentation.",
                "position": 1583
            },
            {
                "img": "https://arxiv.org/html/2412.18450/extracted/6093446/nn_ablation.png",
                "caption": "Figure 5:Dependence of inference speed and visual grounding accuracy on the number of nearest neighbors in the object subgraph. This experiment utilizes the RioRefer dataset along with GT instance segmentation.",
                "position": 1601
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ACommon Failure Cases",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18450/x5.png",
                "caption": "Figure 6:Common failure cases of 3DGraphLLM related to spatial relationships.Left: In the ScanQA dataset, 3DGraphLLM incorrectly identifies the front/back and left.right directions relative to the observer.Right: In the ScanRefer dataset, 3DGraphLLM confuses left and right. The GT object is highlighted in green, and the 3DGraphLLM prediction is highlighted in red.",
                "position": 2512
            },
            {
                "img": "https://arxiv.org/html/2412.18450/x6.png",
                "caption": "Figure 7:Functional queries about the room and objects to the 3DGraphLLM.Left: 3DGraphLLM is capable of answering questions about functional properties of the room and its room type.Right: 3DGraphLLM is capable of answering questions about the functional properties of objects in a room.",
                "position": 2524
            }
        ]
    },
    {
        "header": "Appendix BFunctional Queries",
        "images": []
    }
]