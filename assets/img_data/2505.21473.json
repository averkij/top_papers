[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21473/x1.png",
                "caption": "Figure 1:(a) Progressive generation results from DetailFlow. Our proposed 1D tokenizer encodes tokens with an inherent semantic ordering, where each subsequent token contributes additional high-resolution information. The sequences illustrate how image resolution and inferred 1D tokens incrementally increase from left to right. (b) Comparison of our DetailFlow approach with existing methods, showing that DetailFlow achieves better image quality with fewer tokens and times.",
                "position": 93
            },
            {
                "img": "https://arxiv.org/html/2505.21473/x2.png",
                "caption": "",
                "position": 96
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21473/x3.png",
                "caption": "Figure 2:Comparison of different prediction strategies in image generation. (a) Traditional 2D raster-scan next-token/next-patch prediction. (b) Next-scale prediction in VARtian2024visual. (c) Our proposed next-detail prediction, which predicts 1D tokens encoding fine-grained details for high-resolution image generation.",
                "position": 106
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21473/x4.png",
                "caption": "Figure 3:(a) Coarse-to-fine tokenizer training. The encoder maps high-resolution images to 1D latent token sequences. Decoding with more tokens yields higher-resolution outputs, with earlier tokens capturing global structure and later ones refining details. (b) Self-correction training. Randomly perturbed tokens are re-encoded, and encourages subsequent tokens to correct errors from earlier noisy tokens. (c) Autoregressive (AR) model training and decoding. AR model predicts the first group of tokens in a next-token prediction manner, followed by parallel prediction of subsequent groups. At inference, more predicted tokens lead to higher-resolution outputs.",
                "position": 192
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21473/x5.png",
                "caption": "Figure 4:(a) Reconstruction metrics before and after self-correction when adding noise to latent tokens of a group (tokenizer with 128 tokens, group size 8, trained for 200 epochs). (b)\nImpact of token count on image resolution, reconstruction quality (rFID), and generation quality (gFID), with all evaluations conducted on images resized to256√ó256256256256\\times 256256 √ó 256. The tokenizer is identical to (a). (c) Influence of the hyperparameterŒ±ùõº\\alphaitalic_Œ±in the mapping function‚Ñõ‚Å¢(n)‚Ñõùëõ\\mathcal{R}(n)caligraphic_R ( italic_n )on generation metrics, using tokenizers trained for 50 epochs.",
                "position": 628
            },
            {
                "img": "https://arxiv.org/html/2505.21473/x6.png",
                "caption": "",
                "position": 631
            },
            {
                "img": "https://arxiv.org/html/2505.21473/x7.png",
                "caption": "",
                "position": 632
            },
            {
                "img": "https://arxiv.org/html/2505.21473/x8.png",
                "caption": "Figure 5:Qualitative comparison of AR model outputs with (w/) and without (w/o) self-correction training.",
                "position": 732
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21473/x9.png",
                "caption": "Figure 6:The attention mask in the proposed method. We use a group sizeg=2ùëî2g=2italic_g = 2for latent tokens as an example.",
                "position": 1409
            },
            {
                "img": "https://arxiv.org/html/2505.21473/x10.png",
                "caption": "Figure 7:Progressive reconstruction results from DetailFlow-16. Our method encodes 2D image content into a coarse-to-fine 1D token sequence, with early tokens capturing global structure and later tokens introducing details necessary for both finer texture and higher resolution. As the number of tokens increases, the reconstructed images exhibit progressive improvements in both detail and resolution. The last column is the original Ground Truth (GT) image.",
                "position": 1783
            },
            {
                "img": "https://arxiv.org/html/2505.21473/x11.png",
                "caption": "Figure 8:Progressive generation results from DetailFlow-16. Our proposed 1D tokenizer encodes tokens with an inherent semantic ordering, where each subsequent token contributes additional high-resolution information. The sequences illustrate how image resolution and inferred 1D tokens incrementally increase from left to right.",
                "position": 1786
            }
        ]
    },
    {
        "header": "Appendix ATechnical Appendices and Supplementary Material",
        "images": []
    }
]