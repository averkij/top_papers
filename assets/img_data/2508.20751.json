[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20751/x1.png",
                "caption": "Figure 1:Method Overview.(a) Existing pointwise reward functions assign minimal score differences between generated images, which result in illusory advantage and ultimately lead to reward hacking. (b)Pref-GRPOshifts the training focus from reward score maximization to pairwise preference fitting, enabling stable optimization for T2I generation.",
                "position": 109
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20751/x2.png",
                "caption": "Figure 2:Reward Hacking Visualization.",
                "position": 128
            },
            {
                "img": "https://arxiv.org/html/2508.20751/x3.png",
                "caption": "Figure 3:Benchmark Statistics and Evaluation Results. This figure presents (a) prompt themes, (b) subject distribution, and evaluation dimensions (testpoints) ofUniGenBench, along with benchmarking results for both open-source and closed-source T2I models.",
                "position": 146
            },
            {
                "img": "https://arxiv.org/html/2508.20751/x4.png",
                "caption": "Figure 4:Benchmark Comparison. While current methods only support scoring at the primary dimensions, our benchmark provides fine-grained evaluation acrossboth primary and sub dimensions.",
                "position": 149
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Pref-GRPO",
        "images": []
    },
    {
        "header": "4UniGenBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20751/x5.png",
                "caption": "Figure 5:UniGenBenchConstruction and Evaluation Pipeline. We leverage powerful MLLM for (a) large-scale and diverse prompts generation, and (b) scalable and fine-grained T2I evaluation.",
                "position": 344
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20751/x6.png",
                "caption": "Figure 6:Qualitative Comparison. We comparePref-GRPOwith several pointwise RM-based GRPO methods, demonstrating its superior performance and effectiveness.",
                "position": 415
            },
            {
                "img": "https://arxiv.org/html/2508.20751/iclr2026/figures/bro_medal.png",
                "caption": "Table 3:Benchmarking Results of T2I models onUniGenBench.Gemini2.5-prois used as the VLM for evaluation. Best scores are inbold, second-best inunderlined.",
                "position": 577
            },
            {
                "img": "https://arxiv.org/html/2508.20751/iclr2026/figures/medal.png",
                "caption": "",
                "position": 670
            },
            {
                "img": "https://arxiv.org/html/2508.20751/iclr2026/figures/winner.png",
                "caption": "",
                "position": 685
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APref-GRPO",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20751/x7.png",
                "caption": "Figure 7:Qualitative Results of UnifiedReward Score-based Winrate as Reward: We convert UnifiedReward scores into win rates as rewards for GRPO, and observe that this effectively mitigates the reward hacking issue.",
                "position": 1524
            },
            {
                "img": "https://arxiv.org/html/2508.20751/x8.png",
                "caption": "Figure 8:Reward Hacking Visualization of HPS. At around step 160, the image quality begins to degrade, even though the reward score continues to rise, indicating the occurrence of reward hacking.",
                "position": 1662
            },
            {
                "img": "https://arxiv.org/html/2508.20751/x9.png",
                "caption": "Figure 9:Qualitative Results of Joint Optimization. Joint training with CLIP improves semantic consistency while slightly degrading perceptual quality, reflecting the inherent trade-off.",
                "position": 1824
            },
            {
                "img": "https://arxiv.org/html/2508.20751/x10.png",
                "caption": "Figure 10:More Qualitative Comparison. We comparePref-GRPOwith several pointwise RM-based GRPO methods, demonstrating its superior performance and effectiveness.",
                "position": 1841
            },
            {
                "img": "https://arxiv.org/html/2508.20751/x11.png",
                "caption": "Figure 11:Fine-grained Benchmarking Results of T2I models onUniGenBench. Best scores are in green, second-best in yellow.",
                "position": 1844
            }
        ]
    },
    {
        "header": "Appendix BUniGenBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20751/x12.png",
                "caption": "Figure 12:Prompt Themes ofUniGenBench. We provide representative prompt examples for each theme to facilitate understanding.",
                "position": 1870
            },
            {
                "img": "https://arxiv.org/html/2508.20751/x13.png",
                "caption": "Figure 13:Evaluation Dimensions ofUniGenBench. We provide representative prompt examples for each evaluation dimension to facilitate understanding.",
                "position": 1887
            },
            {
                "img": "https://arxiv.org/html/2508.20751/x14.png",
                "caption": "Figure 14:Distribution of Testpoint Counts in Prompts. This figure presents the distribution of the number of testpoints per prompt inUniGenBench.",
                "position": 1952
            }
        ]
    },
    {
        "header": "Appendix CEthical Statement",
        "images": []
    }
]