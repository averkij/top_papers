[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18194/extracted/5949657/diagrams/zip-fit-final-schematic.png",
                "caption": "Figure 1:ZIP-FITselects task-specific data for efficient finetuning.(0) Obtain both the source and target datasets. (1) CalculateZIP-FITAlignment of each source example with the target dataset usinggzipcompression. (2) Rank all source examples based on these alignment scores. (3) Select the top-K most aligned examples for fine-tuning. (4) Fine-tune a large language model using the selected top-K examples to improve performance on the target task.",
                "position": 130
            },
            {
                "img": "https://arxiv.org/html/2410.18194/extracted/5949657/plots/code_all_models.png",
                "caption": "Figure 2:Code Generation:ZIP-FITaccelerates cross-entropy loss reduction, even in code-specialized models like CodeGemma-2B.The plots show cross-entropy test loss versus the number of training tokens for Gemma2-2B (top row) and CodeGemma-2B (bottom row) across different token selection sizes.ZIP-FIT(blue) consistently reduces loss faster than DSIR (green) and D4 (red), achieving up to85.11%percent85.1185.11\\%85.11 %speed improvement at lower token counts. These results demonstrateZIP-FIT’s efficiency in data selection for fine-tuning models on code-geneation tasks.",
                "position": 134
            }
        ]
    },
    {
        "header": "2ZIP-FIT: an Embedding-Free Data Selection Algorithm via Compression-Based Alignment for LM Fine-Tuning",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18194/extracted/5949657/plots/ce_loss_combined.png",
                "caption": "Figure 3:HigherZIP-FITalignment correlates with lower cross-entropy loss.The relationship betweenZIP-FITalignment and cross-entropy (CE) loss for (a) GPT-2 trained on 22k tokens (R2=0.90,p=0.001formulae-sequencesuperscript𝑅20.90𝑝0.001R^{2}=0.90,p=0.001italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 0.90 , italic_p = 0.001) and (b) Mistral7B trained on 22k tokens (R2=0.75,p=0.025formulae-sequencesuperscript𝑅20.75𝑝0.025R^{2}=0.75,p=0.025italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 0.75 , italic_p = 0.025). Each point represents a dataset, with its position reflecting the dataset’sZIP-FITalignment score against the ProofNet test set and the resulting CE loss. The dashed red line indicates the linear regression fit, while the dashed grey line shows the pretrained CE loss. Higher alignment scores correspond to lower CE losses, demonstrating that training on better aligned data yields better performance.",
                "position": 182
            }
        ]
    },
    {
        "header": "3Higher alignment interventionally achieves better model performance",
        "images": []
    },
    {
        "header": "4Higher Alignment leads to more efficient training",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18194/extracted/5949657/plots/more_alignment_lower_loss.png",
                "caption": "Figure 4:Highly aligned data lowers cross-entropy loss more efficiently.The x-axis shows the number of training tokens, and the y-axis represents the cross-entropy (CE) test loss. Different curves correspond to datasets filtered by different alignment scores, indicating their relevance to the target domain. The most aligned data reduce Test CE loss significantly faster than less aligned data. The left panel depicts results using GPT-2, and the right panel uses Mistral7B, demonstrating that using highly aligned data not only accelerates training but also achieves better model performance, validating the effectiveness ofZIP-FITfor data selection in fine-tuning.",
                "position": 306
            }
        ]
    },
    {
        "header": "5Comparative Evaluation ofZIP-FITfor Efficient Fine-Tuning",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18194/extracted/5949657/plots/af_all_models3.png",
                "caption": "Figure 5:AutoFormalization:ZIP-FITconsistently achieves lower test loss more quickly thanD4andDSIR, demonstrating its efficiency in data selection.The plots show cross-entropy test loss versus the number of training tokens for three models (InterLM-Math-Plus-1.8B, Gemma2-2B, and Mistral7B) across different token selection sizes.ZIP-FIT(blue line) consistently outperforms bothDSIR(green line) and D4 (red line) across all model and token size configurations, highlighting its ability to process data more efficiently. The percentage labels in each plot indicate the relative speedup ofZIP-FIToverDSIRin reaching the lowest cross-entropy loss, reinforcing the method’s scalability and adaptability for domain-specific fine-tuning.",
                "position": 314
            }
        ]
    },
    {
        "header": "6Impact of Data Misalignment on Model Performance",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18194/extracted/5949657/plots/remove_misaligned_data2.png",
                "caption": "Figure 6:Selective data filtering withZIP-FITallows us to achieve better cross-entropy test loss faster than training on all the data, resulting in improved performance and efficiency.The x-axis represents the number of training tokens, while the y-axis shows the cross-entropy test loss.\nThe curves represent models fine-tuned (FT) on datasets filtered by varying alignment thresholds (>>>0.1,>>>0.2,>>>0.3).\nThe dashed line indicates the baseline performance of the pretrained Mistral7B model.\nTraining on data filtered with higher alignment thresholds leads to superior performance, demonstrating the effectiveness of removing misaligned data in fine-tuning.",
                "position": 397
            }
        ]
    },
    {
        "header": "7Related Works",
        "images": []
    },
    {
        "header": "8Limitations",
        "images": []
    },
    {
        "header": "9Discussion and Future Work",
        "images": []
    },
    {
        "header": "10Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AgzipCompression Details",
        "images": []
    },
    {
        "header": "Appendix BRationale for the Method NameZIP-FIT",
        "images": []
    },
    {
        "header": "Appendix CAdditional Experimental Results: data selection for efficient fine-tuning usingZIP-FIT",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18194/extracted/5949657/plots/af_remaining_plot.png",
                "caption": "Figure 7:ZIP-FITconsistently achieves a lower test loss at a faster rate compared to D4 and DSIR for Autoformalization. The plots show the cross-entropy test loss against the number of training tokens for three models (InterLM-Math-Plus-1.8B, Gemma2-2B, and Mistral7B) across various token selection sizes.ZIP-FIT(blue line) consistently surpasses both DSIR (green line) and D4 (red line) across all model and token size configurations, emphasizing its superior data processing efficiency. The percentage labels in each plot denote the relative speedup ofZIP-FITover DSIR in attaining the lowest cross-entropy loss, further underscoring the method’s scalability and adaptability for domain-specific fine-tuning.",
                "position": 1431
            }
        ]
    },
    {
        "header": "Appendix DData Selection Profiling (Run Times)",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18194/extracted/5949657/plots/runtime_zip_fit_vs_dsir_code.png",
                "caption": "Figure 8:ZIP-FIPdemonstrates lower cross-entropy and lower run time during data selection than competing DSIR and D4 methods.ZIP-FITis cheaper, faster, and better performing.\nThe run times do no include fine-tuning time, since it’s a constant offset across all models.\nD4’s data selection (not shown) takes 5hs because it uses an embedding model (opt-125mZhang et al. (2022)), the same one as the original paperTirumala et al. (2023).",
                "position": 1442
            }
        ]
    },
    {
        "header": "Appendix EQualitative Analysis",
        "images": []
    },
    {
        "header": "Appendix FFuture Work (Cont.)",
        "images": []
    }
]