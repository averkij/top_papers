[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21395/x1.png",
                "caption": "Figure 1:Method overview.Left: During inference, Monet can automatically decide when to start latent reasoning by outputting a special start embedding. We fix the output length of the latent embeddings.Right: We propose a three-stage SFT (Section3.3) and RL (Section3.4) framework. The SFT stages progressively warm up the model, generate high-quality latent embeddings, and distill latent reasoning ability. The RL stage further refines the model using our VLPO algorithm, specifically designed for latent reasoning.",
                "position": 176
            },
            {
                "img": "https://arxiv.org/html/2511.21395/x2.png",
                "caption": "Figure 2:Construction pipeline of Monet-SFT-125K. Stage 1 filters hard samples (unsolvable from the original image). Stage 2 keeps those where auxiliary images lead to correct answers, ensuring their necessity and correctness. Stage 3 highlights key visual-observation tokens using advanced LLM judges, providing strong supervision for learning latent embeddings.",
                "position": 216
            },
            {
                "img": "https://arxiv.org/html/2511.21395/x3.png",
                "caption": "Figure 3:The proposed three-stage SFT pipeline:warm-up, supervised latent–observation alignment with controlled attention flow, and latent generation without auxiliary-image access.",
                "position": 289
            },
            {
                "img": "https://arxiv.org/html/2511.21395/x4.png",
                "caption": "Figure 4:Prediction accuracy of the observation tokens during warm-up.Training on image–text interleaved data encourages the model to utilize intermediate visual cues.",
                "position": 308
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21395/x5.png",
                "caption": "Figure 5:Effect of the number of abstract visual embeddings used during training and inference on test accuracy.The dashed line marks the accuracy of Qwen2.5-VL-7B.",
                "position": 830
            }
        ]
    },
    {
        "header": "5Conclusion and Limitations",
        "images": []
    },
    {
        "header": "AAdditional Experimental Results",
        "images": []
    },
    {
        "header": "BImplementation Details",
        "images": []
    },
    {
        "header": "CDetailed Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21395/figures/training_data_examples/10.png",
                "caption": "Figure 6:Example of Monet-SFT-125K: cropping the crucial region.",
                "position": 1319
            },
            {
                "img": "https://arxiv.org/html/2511.21395/figures/training_data_examples/11.png",
                "caption": "",
                "position": 1329
            },
            {
                "img": "https://arxiv.org/html/2511.21395/figures/training_data_examples/1.png",
                "caption": "Figure 7:Example of Monet-SFT-125K: creating new visual states.",
                "position": 1334
            },
            {
                "img": "https://arxiv.org/html/2511.21395/figures/training_data_examples/2.png",
                "caption": "",
                "position": 1349
            },
            {
                "img": "https://arxiv.org/html/2511.21395/figures/training_data_examples/3.png",
                "caption": "",
                "position": 1350
            },
            {
                "img": "https://arxiv.org/html/2511.21395/figures/training_data_examples/5.png",
                "caption": "Figure 8:Example of Monet-SFT-125K: drawing auxiliary lines and bounding boxes.",
                "position": 1356
            },
            {
                "img": "https://arxiv.org/html/2511.21395/figures/training_data_examples/6.png",
                "caption": "",
                "position": 1366
            },
            {
                "img": "https://arxiv.org/html/2511.21395/figures/training_data_examples/7.png",
                "caption": "",
                "position": 1367
            },
            {
                "img": "https://arxiv.org/html/2511.21395/figures/training_data_examples/19.png",
                "caption": "Figure 9:Inference example: 3D spatial reasoning.Instead of describing the angles between the chairs in language, Monet directly reasons with latent embeddings before giving the final answer.",
                "position": 1383
            },
            {
                "img": "https://arxiv.org/html/2511.21395/figures/training_data_examples/20.png",
                "caption": "Figure 10:Inference example: 2D transformation.By generating latent embeddings, Monet successfully identifies the flipping rule of the number.",
                "position": 1399
            },
            {
                "img": "https://arxiv.org/html/2511.21395/figures/training_data_examples/21.png",
                "caption": "Figure 11:Inference example: complex diagram reasoning.Monet-7B exhibit ahierarchicalreasoning pattern. Firstlt, it focus on the relevant section in the image by generating latent embeddings: “The highlighted area in the image clearly shows the “Top Sales Countries” section.” Then, it accurately identifies the contents in the “Top Sales Countries” section and gives the correct answer.",
                "position": 1414
            },
            {
                "img": "https://arxiv.org/html/2511.21395/figures/training_data_examples/22.png",
                "caption": "Figure 12:Inference example: commonsense QA.Monet-7B correctly identifies the connection between the “Origin” and the “Analogy” image, showing its commonsense reasoning capability.",
                "position": 1434
            },
            {
                "img": "https://arxiv.org/html/2511.21395/figures/training_data_examples/23.png",
                "caption": "Figure 13:Inference example: fine-grained OCR.Monet-7B accurately identifies the key information, which is located in the middle of the rightmost region of the image.",
                "position": 1449
            }
        ]
    },
    {
        "header": "DCase Studies",
        "images": []
    }
]