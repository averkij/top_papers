[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13424/extracted/6283074/pics/teaser.png",
                "caption": "",
                "position": 120
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13424/x1.png",
                "caption": "Figure 2:The whole pipeline can be devided into four parts: articulation tree structure generation, geometry generation, material generation and joint generation.",
                "position": 242
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13424/x2.png",
                "caption": "Figure 3:Structure of the URDF file.\nEach link is a part of the object, which is represented as a textured mesh in our case.\nEach joint connects two links and describes the articulation structure between them.",
                "position": 310
            },
            {
                "img": "https://arxiv.org/html/2503.13424/x3.png",
                "caption": "Figure 4:We implement6666kinds of joints in our articulated objects.\nFirst three are simple joints, and the last three are compound joints.",
                "position": 326
            },
            {
                "img": "https://arxiv.org/html/2503.13424/extracted/6283074/pics/res_1.png",
                "caption": "Figure 5:Examples of our generated cabinets, chairs, lamps and windows.\nFor each object, we display the textured mesh with the corresponding articulation tree above.\nThe generated objects are diverse in both shapes and articulation structures.",
                "position": 384
            },
            {
                "img": "https://arxiv.org/html/2503.13424/x4.png",
                "caption": "Figure 6:We use two methods to obtain the geometry of each part of an object. 1) Blender python API created meshes. 2) Parts retrieved from our carefully curated and processed dataset.\nFor each group of objects, the left one is generated by Blender python API, while the right3333objects are obtained by retrieving parts from curated PartNet-Moblity.\nParts obtained from both methods can be seamlessly joined, and it improves the diversity of generated shapes by adopting both methods to obtain part geometry.",
                "position": 408
            },
            {
                "img": "https://arxiv.org/html/2503.13424/x5.png",
                "caption": "Figure 7:Original parts from PartNet and ShapeNet bear many back faces and thus their normals are highly irregular.\nWe flip these back faces in Blender using recalculate normal function followed by human repair and ensure the meshes bear consistent outward-facing normals.",
                "position": 415
            },
            {
                "img": "https://arxiv.org/html/2503.13424/x6.png",
                "caption": "Figure 8:We adopt support point-based placement to position the retrieved part on the object.\nNaive bounding box-based placement may create a gap between the retrieved part and the object. Our approach guarantees a seamless connection.",
                "position": 433
            },
            {
                "img": "https://arxiv.org/html/2503.13424/extracted/6283074/pics/comparison.png",
                "caption": "Figure 9:We carefully position each part and optimally configure the joint parameters to guarantee that no collisions occur either among the parts themselves or between the object and the ground during its articulation.\nThe top row is a dishwasher from PartNet-Mobility.\nWhen the dishwasher door is opened, its base collides with the ground, forcing the main body to tilt upwards.\nWhile the dishwasher generated by our method in the bottom row does not have this problem.",
                "position": 476
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13424/extracted/6283074/pics/joint_evaluation.png",
                "caption": "Figure 10:Human evaluators observe textureless videos of our generated articulated objects (lower row) and those from PartNet-Mobility (upper row), subsequently determining which exhibits superior motion structure.",
                "position": 515
            },
            {
                "img": "https://arxiv.org/html/2503.13424/extracted/6283074/pics/table_result.png",
                "caption": "Figure 11:Examples of articulated objects generated by CAGE trained on our results.\nThe generated object has accurate geometry and reasonable motion structure.",
                "position": 520
            },
            {
                "img": "https://arxiv.org/html/2503.13424/extracted/6283074/pics/embodied.png",
                "caption": "Figure 12:Kinematic sequences demonstrating a bimanual robotic manipulator interacting with our generated articulated objects within the Isaac Sim simulation environment.",
                "position": 948
            }
        ]
    },
    {
        "header": "5Limitations",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]