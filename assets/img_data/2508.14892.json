[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.14892/x1.png",
                "caption": "Figure 1:Under the setting of two input images, we propose a feed-forward framework named Snap-Snap which can directly predict 3D human Gaussians in milliseconds.",
                "position": 122
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.14892/x2.png",
                "caption": "Figure 2:The framework of Snap-Snap. With the input front and back view imagesIfI^{f}andIbI^{b}, the point cloud prediction modelℛp\\mathcal{R}_{p}generate the human point clouds from the frontPf,fP^{f,f}, backPb,fP^{b,f}, leftPl,fP^{l,f}, and rightPr,fP^{r,f}views.\nSide-view color information is supplied by the side-view enhancement module. With the enhanced imagesIl,IrI^{l},I^{r}and the input imagesIf,IbI^{f},I^{b}, we obtain fianl human Gaussians through Gaussian attribute regressionℱg\\mathcal{F}^{g}.",
                "position": 193
            },
            {
                "img": "https://arxiv.org/html/2508.14892/x3.png",
                "caption": "Figure 3:The framework of point cloud prediction network.",
                "position": 239
            },
            {
                "img": "https://arxiv.org/html/2508.14892/x4.png",
                "caption": "Figure 4:Visual comparisons with GPS-Gaussian[59]and GHG[20].",
                "position": 525
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.14892/x5.png",
                "caption": "Figure 5:Qualitative comparisons with SiTH[8].",
                "position": 675
            },
            {
                "img": "https://arxiv.org/html/2508.14892/x6.png",
                "caption": "Figure 6:Visual comparisons of Snap-Snap with single-view reconstruction methods.",
                "position": 678
            },
            {
                "img": "https://arxiv.org/html/2508.14892/x7.png",
                "caption": "Figure 7:Reconstruction results from in-the-wild data.",
                "position": 681
            },
            {
                "img": "https://arxiv.org/html/2508.14892/x8.png",
                "caption": "Figure 8:We present the visual results of ablation studies on the additional side-view head in the point cloud prediction network and the side-view enhancement module.",
                "position": 684
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.14892/sup_figs/data1.png",
                "caption": "Figure 9:Data collection setup visualization.",
                "position": 1593
            },
            {
                "img": "https://arxiv.org/html/2508.14892/x9.png",
                "caption": "Figure 10:Visual comparisons of Snap-Snap with TRELLIS[48].",
                "position": 1609
            },
            {
                "img": "https://arxiv.org/html/2508.14892/x10.png",
                "caption": "Figure 11:Comparison of pipelines with different methods.",
                "position": 1612
            },
            {
                "img": "https://arxiv.org/html/2508.14892/x11.png",
                "caption": "Figure 12:Visualization of different texture introduced by generative methods.",
                "position": 1689
            },
            {
                "img": "https://arxiv.org/html/2508.14892/x12.png",
                "caption": "Figure 13:Reconstruction results based on arbitrary image inputs.",
                "position": 1750
            },
            {
                "img": "https://arxiv.org/html/2508.14892/x13.png",
                "caption": "Figure 14:We present the visual results of the impact of the additional side-view heads on the human construction.",
                "position": 1801
            },
            {
                "img": "https://arxiv.org/html/2508.14892/x14.png",
                "caption": "Figure 15:Reconstruction results of the same human in different poses.",
                "position": 1804
            },
            {
                "img": "https://arxiv.org/html/2508.14892/x15.png",
                "caption": "Figure 16:The visual results of the time consumption for different modules in the inference process.",
                "position": 1807
            },
            {
                "img": "https://arxiv.org/html/2508.14892/x16.png",
                "caption": "Figure 17:The network architecture of the Gaussian attribute regression network.",
                "position": 1835
            },
            {
                "img": "https://arxiv.org/html/2508.14892/x17.png",
                "caption": "Figure 18:We visualize the comparison of reconstruction results between our method and GPS-Gaussian[59]with five input views.",
                "position": 1838
            },
            {
                "img": "https://arxiv.org/html/2508.14892/x18.png",
                "caption": "Figure 19:We visualize the reconstruction results of GHG[20]based on SMPL-X[32]parameters predicted by EasyMocap[1], as well as our reconstruction results in the same scenario. The first column shows the input views, the second column shows the detected keypoints, and the third column shows the corresponding predicted SMPL-X mesh visualization results. Columns 4-9 show the reconstruction results of GHG, our reconstruction results, and the ground truth, respectively.",
                "position": 1841
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]