[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.10818/x1.jpg",
                "caption": "",
                "position": 77
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.10818/x2.png",
                "caption": "Figure 2:Model Overview: (i) During setup, we invert the input sketch to act as the reference noise for the first frame, sampling from a standard normal for the rest. (ii) For timesteps within threshold픣1subscript洧랦1\\tau_{1}italic_픣 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, we iteratively refine sampled noise for our reference noise (first frame) is denoised to the input sketch. (iii) We further compose attention maps for joint denoising of reference and sampled noise to influence all frames with first-frame information.",
                "position": 138
            }
        ]
    },
    {
        "header": "3Background",
        "images": []
    },
    {
        "header": "4Proposed Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.10818/x3.png",
                "caption": "Figure 3:We parallelly perform denoising of reference noisextrsubscriptsuperscript洧논洧洧노x^{r}_{t}italic_x start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTand that of all framesftisubscriptsuperscript洧녭洧녰洧노f^{i}_{t}italic_f start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. Query-key pairs from reference frame denoising (qtr,ktrsubscriptsuperscript洧륋롐洧노subscriptsuperscript洧녲洧洧노q^{r}_{t},k^{r}_{t}italic_q start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_k start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT) are used to influence video generation through cross-attention with (qtg,ktgsubscriptsuperscript洧륋롐덣롐몁ubscriptsuperscript洧녲洧녮洧노q^{g}_{t},k^{g}_{t}italic_q start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_k start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT).",
                "position": 198
            },
            {
                "img": "https://arxiv.org/html/2411.10818/extracted/6002028/fig/qualitative_comparisons.jpg",
                "caption": "Figure 5:Qualitative comparison of our method against vector animation algorithm Live-Sketch[22]and raster video generation methods SVD[10]and DynamiCrafter (DC)[61]. Live-Sketch[22]preserves sketch identity by constraining local animations between vectors, but has limited motion capacity. SVD[10]and DC[61]cannot preserve sketch identity, suffering from sketch-photo domain gap. Our method performs dynamic animations that align with text prompts, without losing sketch identity.",
                "position": 431
            }
        ]
    },
    {
        "header": "5Results and Comparisons",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.10818/extracted/6002028/fig/ballerina_extra.jpeg",
                "caption": "Figure 6:Frame extrapolation allows us to construct complex animations by stitching multiple videos with different text prompts.",
                "position": 460
            },
            {
                "img": "https://arxiv.org/html/2411.10818/extracted/6002028/fig/typography.png",
                "caption": "Figure 7:Qualitative comparison against vector animations from Dynamic Typography[32]for animating words with text prompts.",
                "position": 466
            },
            {
                "img": "https://arxiv.org/html/2411.10818/x4.png",
                "caption": "Figure 8:Qualitative comparison of ablative configurations.",
                "position": 539
            },
            {
                "img": "https://arxiv.org/html/2411.10818/extracted/6002028/fig/real_vids.jpg",
                "caption": "Figure 9:We construct high resolution realistic videos using text prompts and skeleton-like guidance from generated raster frames.",
                "position": 600
            }
        ]
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]