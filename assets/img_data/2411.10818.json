[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.10818/x1.jpg",
                "caption": "",
                "position": 77
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.10818/x2.png",
                "caption": "Figure 2:Model Overview: (i) During setup, we invert the input sketch to act as the reference noise for the first frame, sampling from a standard normal for the rest. (ii) For timesteps within thresholdτ1subscript𝜏1\\tau_{1}italic_τ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, we iteratively refine sampled noise for our reference noise (first frame) is denoised to the input sketch. (iii) We further compose attention maps for joint denoising of reference and sampled noise to influence all frames with first-frame information.",
                "position": 138
            }
        ]
    },
    {
        "header": "3Background",
        "images": []
    },
    {
        "header": "4Proposed Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.10818/x3.png",
                "caption": "Figure 3:We parallelly perform denoising of reference noisextrsubscriptsuperscript𝑥𝑟𝑡x^{r}_{t}italic_x start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTand that of all framesftisubscriptsuperscript𝑓𝑖𝑡f^{i}_{t}italic_f start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. Query-key pairs from reference frame denoising (qtr,ktrsubscriptsuperscript𝑞𝑟𝑡subscriptsuperscript𝑘𝑟𝑡q^{r}_{t},k^{r}_{t}italic_q start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_k start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT) are used to influence video generation through cross-attention with (qtg,ktgsubscriptsuperscript𝑞𝑔𝑡subscriptsuperscript𝑘𝑔𝑡q^{g}_{t},k^{g}_{t}italic_q start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_k start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT).",
                "position": 198
            },
            {
                "img": "https://arxiv.org/html/2411.10818/extracted/6002028/fig/qualitative_comparisons.jpg",
                "caption": "Figure 5:Qualitative comparison of our method against vector animation algorithm Live-Sketch[22]and raster video generation methods SVD[10]and DynamiCrafter (DC)[61]. Live-Sketch[22]preserves sketch identity by constraining local animations between vectors, but has limited motion capacity. SVD[10]and DC[61]cannot preserve sketch identity, suffering from sketch-photo domain gap. Our method performs dynamic animations that align with text prompts, without losing sketch identity.",
                "position": 431
            }
        ]
    },
    {
        "header": "5Results and Comparisons",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.10818/extracted/6002028/fig/ballerina_extra.jpeg",
                "caption": "Figure 6:Frame extrapolation allows us to construct complex animations by stitching multiple videos with different text prompts.",
                "position": 460
            },
            {
                "img": "https://arxiv.org/html/2411.10818/extracted/6002028/fig/typography.png",
                "caption": "Figure 7:Qualitative comparison against vector animations from Dynamic Typography[32]for animating words with text prompts.",
                "position": 466
            },
            {
                "img": "https://arxiv.org/html/2411.10818/x4.png",
                "caption": "Figure 8:Qualitative comparison of ablative configurations.",
                "position": 539
            },
            {
                "img": "https://arxiv.org/html/2411.10818/extracted/6002028/fig/real_vids.jpg",
                "caption": "Figure 9:We construct high resolution realistic videos using text prompts and skeleton-like guidance from generated raster frames.",
                "position": 600
            }
        ]
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]