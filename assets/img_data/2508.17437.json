[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.17437/x1.png",
                "caption": "Figure 1:We introducePixie, a novel method for learning simulatable physics of 3D scenes from visual features. Trained on a curated dataset of paired 3D objects and physical material annotations,Pixiecan predict both the discrete material types (e.g., rubber) and continuous values including Young’s modulus, Poisson’s ratio, and density for a variety of materials, including elastic, plastic, and granular. The predicted material parameters can then be coupled with a learned static 3D model such as Gaussian splats and a physics solver such as the Material Point Method (MPM) to produce realistic 3D simulation under physical forces such as gravity and wind.",
                "position": 167
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.17437/x2.png",
                "caption": "Figure 2:Method Overview. From posed multi-view RGB images of a static scene,Pixiefirst reconstructs a 3D model with NeRF and distilled CLIP features[Shen et al.,2023]. Then, we voxelize the features into a regularN×N×N×DN\\times N\\times N\\times Dgrid whereNNis the grid size andDDis the CLIP feature dimension. A U-Net neural network[Dhariwal and Nichol,2021]is trained to map the feature grid to the material fieldℳ^G\\hat{\\mathcal{M}}_{G}which consists of a discrete material model ID and continuous Young’s modulus, Poisson’s ratio, and density value for each voxel. Coupled with a separately trained Gaussian splatting model,ℳ^G\\hat{\\mathcal{M}}_{G}can be used to simulate physics with a physics solver such as MPM.",
                "position": 237
            },
            {
                "img": "https://arxiv.org/html/2508.17437/x3.png",
                "caption": "Figure 3:PixieVerseDataset Overview.We collect16241624high-quality single-object assets, spanning 10 semantic classes (a), and 5 constitutive material types (b). The dataset is annotated with detailed physical properties including spatially varying discrete material types (b), Young’s modulus (c), Poisson’s ratio (d), and mass density (e). The left figure shows representative examples from the dataset: organic matter (tree, shrubs, grass, flowers), deformable toys (rubber ducks), sports equipment (sport balls), granular media (sand, snow & mud), and hollow containers (soda cans, metal crates).",
                "position": 300
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.17437/x4.png",
                "caption": "Figure 4:Main VLM Results.(a)VLM score versus wall-clock time:Pixieis three orders of magnitude faster than previous works while achieving 1.46-4.39x improvement in realism. Test-time optimization methods are run with varying numbers of epochs i.e.,1,25,501,25,50for DreamPhysics and1,2,51,2,5for OmniPhysGS while inference methods are only run once. (b)Per-class VLM score:Our method leads on most object classes. Standard errors are also included.",
                "position": 596
            },
            {
                "img": "https://arxiv.org/html/2508.17437/x5.png",
                "caption": "Figure 5:Qualitative comparison on synthetic scenes.We visualized the predicted material class andEEpredictions (left, right respectively) forPixieand Nerf2Physics,EEfor DreamPhysics (right), and the plasticity and hyperelastic function classes predicted by OmniPhysGS.Pixieproduces stable, physically plausible motion while DreamPhysics remains overly stiff due to inaccurate fine-grainedEEprediction or too highEE(e.g., see tree (C)), OmniPhysGS collapses under load due to unrealistic combination of plasticity and hyperelastic functions, and NeRF2Physics exhibits noisy artifacts. Please seehttps://pixie-3d.github.io/for the videos.",
                "position": 605
            },
            {
                "img": "https://arxiv.org/html/2508.17437/x6.png",
                "caption": "",
                "position": 609
            },
            {
                "img": "https://arxiv.org/html/2508.17437/x7.png",
                "caption": "Figure 6:Pixie’s Zero-shot Real-scene Generalization.Trained only on syntheticPixieVerse,Pixiecan predict plausible physic properties, enabling realistic MPM simulation of real scenes. Here, we visualize the material types (left) and Young’s modulus (right) prediction in the first frame, and subsequent frames impacted by a wind force. Please see the videos in our websitehttps://pixie-3d.github.io/.",
                "position": 614
            }
        ]
    },
    {
        "header": "5Conclusion and Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APreliminaries",
        "images": []
    },
    {
        "header": "Appendix BPixieVerseDataset Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.17437/x8.png",
                "caption": "Figure 9:Manual correction for object filtering.The web interface for quickly inspecting and manually correcting VLM results.",
                "position": 1451
            },
            {
                "img": "https://arxiv.org/html/2508.17437/x9.png",
                "caption": "",
                "position": 1455
            },
            {
                "img": "https://arxiv.org/html/2508.17437/x10.png",
                "caption": "Figure 10:CLIP Semantic Segmentation.CLIP features can be noisy for various objects and different text queries vary greatly in segmentation quality. Thus, we prompt a VLM actor to generate several candidate queries for each object, render all candidates, and prompt another VLM critic to select the best query terms from the rendered 3D segmentation images. Some candidates are provided and proposals chosen by the critic are highlighted. Note that a high-performing query proposal (e.g., “leaves,pot,trunk\") in one object is not necessary high-performant in another. The PCA visualization of the CLIP feature fields is also provided.",
                "position": 1466
            },
            {
                "img": "https://arxiv.org/html/2508.17437/x11.png",
                "caption": "",
                "position": 1470
            },
            {
                "img": "https://arxiv.org/html/2508.17437/x12.png",
                "caption": "Figure 11:VLM Actor’s Physics and Segmentation Proposal.",
                "position": 1484
            }
        ]
    },
    {
        "header": "Appendix CThe Effects of Human Prior onPixieVerse",
        "images": []
    },
    {
        "header": "Appendix DVLM As a Physics Judge",
        "images": []
    },
    {
        "header": "Appendix EModel architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.17437/x13.png",
                "caption": "Figure 16:PixiePrediction Visualization.Pixiesimultaneously recovers discrete material class, continuous Young’s modulus (E), Poisson’s ratio (ν\\nu), and mass density (ρ\\rho) with a high degree of accuracy. For example, the model correctly labels foliage as elastic and the metal can as rigid, while recovering realistic stiffness and density gradients within each object.",
                "position": 2740
            },
            {
                "img": "https://arxiv.org/html/2508.17437/x14.png",
                "caption": "",
                "position": 2744
            },
            {
                "img": "https://arxiv.org/html/2508.17437/x15.png",
                "caption": "Figure 17:PixieAblation’s Per-class Accuracy on synthetic scenes. CLIP features generalizes in synthetic scenes, outperforming RGB and occupancy on all classes.",
                "position": 2748
            },
            {
                "img": "https://arxiv.org/html/2508.17437/x16.png",
                "caption": "Figure 18:Pixie’s Feature Type Ablation on Real Scenes.Replacing CLIP features with RGB or occupancy severely degrades the material prediction. Incorrect predictions such as leave mislaballed as metal or Young’s modulus being uniform within an object are marked with question marks. This highlights the power of pretrained visual features in bridging the sim2real gap.",
                "position": 2751
            },
            {
                "img": "https://arxiv.org/html/2508.17437/x17.png",
                "caption": "",
                "position": 2755
            }
        ]
    },
    {
        "header": "Appendix FAdditional Results",
        "images": []
    }
]