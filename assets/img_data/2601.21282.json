[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21282/x1.png",
                "caption": "",
                "position": 131
            },
            {
                "img": "https://arxiv.org/html/2601.21282/x2.png",
                "caption": "Figure 2:Overview of our generation and evaluation process. For generation (top), we use Kubric, which uses PyBullet and Blender under the hood. During evaluation (bottom), we first pass the initial frames of the generated video to the world foundation model which completes the video. The completed video is passed to SAM2 along with bounding boxes based on ground truth masks. The segmentations outputted by SAM2 are compared to ground truth segmentations to obtain the final metrics.",
                "position": 137
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21282/x3.png",
                "caption": "Figure 3:Overview of our physical parameter estimation pipeline.Given an input video, we first use checkerboard detection and SAM2 to extract 3D positions for objects in the video. We then fit curves to these parameters to estimate relevant physical properties such as acceleration or terminal velocity. These are then post-processed, if needed, to calculate the relevant physical parameters.",
                "position": 193
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Benchmark",
        "images": []
    },
    {
        "header": "4Discussion",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Benchmark Details",
        "images": []
    },
    {
        "header": "Appendix BText-Enhanced Subset",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21282/x4.png",
                "caption": "Figure 4:Qualitative Examples of the Language-based subset of WorldBench. VLMs are given access to a 9 frame video (same as what is inputted to COSMOS) and ask to answer a True/False or multiple choice question based on the video and future predictions.",
                "position": 2404
            },
            {
                "img": "https://arxiv.org/html/2601.21282/x5.png",
                "caption": "Figure 5:Qualitative examples for the Motion Physics scenario of the intuitive physics subset.For the motion physics example, two objects (a vase and a knot) are thrown at each other, collide, and then fall to the floor.",
                "position": 2407
            },
            {
                "img": "https://arxiv.org/html/2601.21282/x6.png",
                "caption": "Figure 6:Qualitative examples for the Support Relations scenario of the intuitive physics subsetA ball is rolled down a ramp towards a solid block near the bottom.",
                "position": 2410
            },
            {
                "img": "https://arxiv.org/html/2601.21282/x7.png",
                "caption": "Figure 7:Qualitative examples for the Object Permanence scenario of the intuitive physics subset. An object is thrown behind a sequence of thin columns, appearing and dis-appearing as it goes.",
                "position": 2413
            },
            {
                "img": "https://arxiv.org/html/2601.21282/x8.png",
                "caption": "Figure 8:Qualitative examples for the Scale/Perspective scenario of the intuitive physics subset.A metallic sphere is rolling away from the camera. All models perform well on this sample.",
                "position": 2416
            },
            {
                "img": "https://arxiv.org/html/2601.21282/x9.png",
                "caption": "Figure 9:Qualitative examples for the Friction scenario of the physical parameter estimation subset.A steel block is released from rest down a ramp at a pre-set elevation and with a specific friction coefficient.",
                "position": 2419
            },
            {
                "img": "https://arxiv.org/html/2601.21282/x10.png",
                "caption": "Figure 10:Qualitative examples for the Motion Physics (top) and Scale/Perspective (bottom) scenarios.For the motion physics example, two objects (a vase and a knot) are thrown at each other, collide, and then fall to the floor. The auto-regressive model greatly distorts the object shapes, while the diffusion model hallucinates the vase into a tank and adds a human hand. For the scale example, a metallic sphere is rolling away from the camera. Both models perform well on this sample.",
                "position": 2422
            }
        ]
    },
    {
        "header": "Appendix CAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21282/figures/metrics_over_time.png",
                "caption": "Figure 11:mIoU and background RMSE results over time. The foreground mIoU is inversely related with how far in the future the model is predicting. There is a sharp drop off after frame 5 or frame 9 when the model first begins predicting (depending on the model) The shaded region shows 1 standard deviation.",
                "position": 2589
            },
            {
                "img": "https://arxiv.org/html/2601.21282/x11.png",
                "caption": "Figure 12:SAM2 tracking an object in the object permanence scenario. Despite the fact that the object disappears for 20+ frames, SAM2 is able to track it when it re-appears.",
                "position": 2824
            }
        ]
    },
    {
        "header": "Appendix DSAM Validation",
        "images": []
    }
]