[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.10238/x1.png",
                "caption": "",
                "position": 92
            },
            {
                "img": "https://arxiv.org/html/2505.10238/x2.png",
                "caption": "Figure 1:We propose MTVCrafter,\nwhich can effectively transfer pose sequences from a driven video to diverse, unseen single or multiple characters in either full-body or half-body settings across various styles such as anime, pixel art, ink drawings, and photorealism,\noutperforming existing state-of-the-art methods in generation robustness and generalizability to open-world scenarios.",
                "position": 134
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.10238/x3.png",
                "caption": "Figure 2:Our motivation is that directly tokenizing 4D motion captures more faithful and expressive information than traditional 2D-rendered pose images derived from the driven video.",
                "position": 215
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.10238/x4.png",
                "caption": "Figure 3:Our 4D motion tokenizer consists of an encoder-decoder framework to learn spatio-temporal latent representations of SMPL sequences,\nand a vector quantizer to learn 4D compact yet expressive tokens in a unified space.\nAll operations are in 2D space along the frame and joint axes.",
                "position": 402
            },
            {
                "img": "https://arxiv.org/html/2505.10238/x5.png",
                "caption": "Figure 4:Based on the video DiT model,\nwe design unique 4D motion attention to leverage 4D motion tokens as context for vision generation.\nTo better capture spatio-temporal relationships,\nwe apply 4D RoPE over (t, x, y, z) coordinates.\nTo further improve the generation quality and generalization,\nwe use learnable unconditional tokens for motion-aware classifier-free guidance.",
                "position": 506
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.10238/x6.png",
                "caption": "Figure 5:Qualitative comparison with existing methods.\nOur MTVCrafter consistently demonstrates the best identity preservation, motion accuracy, visual realism,\nand frame smoothness.",
                "position": 882
            },
            {
                "img": "https://arxiv.org/html/2505.10238/x7.png",
                "caption": "Figure 6:Visual comparison of ablated choices.\nThe numbers below images correspond to the rows in Table2.\nThe original design (denoted as the last image) achieves the best visual performance.",
                "position": 999
            },
            {
                "img": "https://arxiv.org/html/2505.10238/x8.png",
                "caption": "Figure 7:Ablation of motion-aware CFG scalewùë§witalic_w.\nHigher CFG scalewùë§witalic_wleads to better pose alignment,\nbut also introduce more artifacts.\nIn our experiments,\na scale of 3.0 achieves the best trade-off.",
                "position": 1006
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Implementation Details",
        "images": []
    },
    {
        "header": "Appendix BMore Details of Dataset Curation",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.10238/x9.png",
                "caption": "Figure 8:Limitations and failure cases.\n(1) Incorrect generation occurs when the reference character‚Äôs proportions deviate significantly from normal human anatomy,\nas the training data lacks non-human figures.\n(2) Precise hand control is challenging due to insufficient detailed hand supervision.",
                "position": 2372
            }
        ]
    },
    {
        "header": "Appendix CLimitations and Discussion",
        "images": []
    },
    {
        "header": "Appendix DSystematic Analysis of 4D Motion Tokenizer",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.10238/x10.png",
                "caption": "Figure 9:Quantitative analysis of the VQVAE codebook.\nThe left panel demonstrates that up to 69.7% of the codes remain active during inference, indicating efficient utilization of the encoding space.\nThe right figure shows that the cosine similarity of most code pairs is close to 0, confirming the model‚Äôs ability to learn a discrete latent space characterized by highly decorrelated representations.",
                "position": 2419
            },
            {
                "img": "https://arxiv.org/html/2505.10238/x11.png",
                "caption": "Figure 10:Reconstruction performance of our motion VQVAE on unseen Gymnastics data.\nEach group consists of three images: the original image (first column),\nthe extracted original pose (second column), and the reconstructed pose (third column).\nAll poses are visualized as 3D joint skeletons, projected into 2D image space using joint coordinates.\nOur motion VQVAE demonstrates strong generalization to unseen motion data and achieves accurate and robust reconstruction quality.",
                "position": 2426
            },
            {
                "img": "https://arxiv.org/html/2505.10238/x12.png",
                "caption": "Figure 11:(a) We visualize the cross-attention maps at different Transformer blocks. Our 4D RoPE enables effective and structured interactions between motion and vision tokens.\n(b) We visualize the mean 3D joint positions across the dataset, which are used to compute the 4D RoPE. This averaged representation provides typical spatial cues that facilitate consistent cross-modal modulation.",
                "position": 2441
            }
        ]
    },
    {
        "header": "Appendix EMore Details of 4D Motion RoPE",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.10238/x13.png",
                "caption": "Figure 12:Training loss curves of the 4D motion tokenizer and 4D motion-guided video DiT model.\nThe tokenizer demonstrates smooth convergence with decreasing reconstruction and commitment loss, while the video DiT model gradually learns motion-aware video generation.",
                "position": 2547
            }
        ]
    },
    {
        "header": "Appendix FTraining Curves",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.10238/x14.png",
                "caption": "Figure 13:More visualization results (1) on the test set of real humans.\nEach row shows an animation conditioned on a different motion sequence. The first column shows the reference image, while the remaining columns present the animated frames.\nOur MTVCrafter consistently preserves both identity and motion accuracy across a wide variety of scenarios and diverse real, human characters. These results highlight our strong robustness and high-quality open-world animation capability.",
                "position": 2588
            },
            {
                "img": "https://arxiv.org/html/2505.10238/x15.png",
                "caption": "Figure 14:More visualization results (2) on the test set of diverse-style characters.\nEach row shows an animation conditioned on a different motion sequence. The first column shows the reference image, while the remaining columns present the animated frames.\nThese visualizations showcase open-world animation results featuring virtual human characters.\nOur MTVCrafter consistently achieves high identity consistency and motion accuracy across various styles and single/multiple characters.",
                "position": 2595
            },
            {
                "img": "https://arxiv.org/html/2505.10238/x16.png",
                "caption": "Figure 15:More comparisons with SOTA methods.\nOur MTVCrafter consistently demonstrates the best performance with high-quality human motion and high-fidelity appearance across diverse scenes.",
                "position": 2603
            }
        ]
    },
    {
        "header": "Appendix GMore Comparisons and Visualization Results",
        "images": []
    }
]