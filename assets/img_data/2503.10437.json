[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10437/x1.png",
                "caption": "",
                "position": 73
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10437/x2.png",
                "caption": "Figure 2:The framework of constructing a time-varying semantic field in 4D LangSplat. We first use multimodal object-wise prompting to convert a video into pixel-aligned object-level caption features. Then, we learn a 4D language field with a status deformable network.",
                "position": 196
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10437/x3.png",
                "caption": "Figure 3:Visualization of time-sensitive querying results between Deformable CLIP and ours. The bottom row depicts the cosine similarity across frames, rescaled to (0,1) for direct comparison, while the horizontal bars indicate frames identified as relevant time segments. We observed that the CLIP-based method cannot understand dynamic semantics correctly, while our method recognizes them.",
                "position": 550
            },
            {
                "img": "https://arxiv.org/html/2503.10437/x4.png",
                "caption": "Figure 4:Comparison of time-sensitive query mask. We compare time-sensitive query masks between Deformable CLIP and ours. The CLIP-based method fails to identify time segments accurately, especially at the demarcation points during state transitions.",
                "position": 553
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADatasets",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    },
    {
        "header": "Appendix CMore Quantitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10437/x5.png",
                "caption": "Figure 5:Visualization of time-agnostic querying results on HyperNeRF[37]and Neu3D[27]datasets.",
                "position": 1851
            }
        ]
    },
    {
        "header": "Appendix DMore Visualization Results",
        "images": []
    },
    {
        "header": "Appendix EMLLM-based Embeddings",
        "images": []
    }
]