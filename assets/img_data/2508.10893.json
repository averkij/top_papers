[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.10893/x1.png",
                "caption": "Figure 1:STream3R. Given a stream of input images, our method estimates dense 3D geometry for each incoming frame using a causal Transformer. Features from previously observed frames are cached as context for future inference.",
                "position": 79
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries: DUSt3R",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.10893/x2.png",
                "caption": "Figure 2:Method Overview.Built on a causal transformer,STream3Rprocesses streaming images sequentially for 3D reconstruction. Each input image is first tokenized using a shared-weight ViT encoder, and the resulting tokens are passed to our causal decoder. Each decoder layer begins with frame-wise self-attention. For subsequent views, the model applies causal attention to the memory tokens cached from previous observations. The outputs include point maps and confidence maps in both world and camera coordinate systems, as long as the camera pose as shown on the right. Note that we visualize the point cloud of theHeadlocal\\mathrm{Head}_{\\text{local}}with its depth map.",
                "position": 209
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.10893/x3.png",
                "caption": "Figure 3:Qualitative results on in-the-wild Images.We compare our method with approaches MonST3R, Fast3R, and CUT3R, and demonstrate that it achieves superior visual quality.",
                "position": 334
            },
            {
                "img": "https://arxiv.org/html/2508.10893/x4.png",
                "caption": "(a)Overall Training Curve",
                "position": 2002
            },
            {
                "img": "https://arxiv.org/html/2508.10893/x4.png",
                "caption": "(a)Overall Training Curve",
                "position": 2005
            },
            {
                "img": "https://arxiv.org/html/2508.10893/x5.png",
                "caption": "(b)Training Curve of Local Branch",
                "position": 2010
            },
            {
                "img": "https://arxiv.org/html/2508.10893/x6.png",
                "caption": "(c)Training Curve of Global Branch",
                "position": 2015
            }
        ]
    },
    {
        "header": "6Conclusion and Discussions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADataset Details",
        "images": []
    },
    {
        "header": "Appendix BMore Implementation details",
        "images": []
    },
    {
        "header": "Appendix CMore Comparisons",
        "images": []
    }
]