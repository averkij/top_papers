[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/icon.jpg",
                "caption": "",
                "position": 72
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/framework.png",
                "caption": "Figure 1:Overview of the VMB framework.We employ text and music as two explicit bridges for multimodal music generation. Text-form music description is obtained with the Multimodal Music Description model. Reference music is retrieved with the Dual-track Music Retrieval module. The two bridges are fed into the Explicitly Conditioned Music Generation module to generate output music.",
                "position": 102
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/3.1.png",
                "caption": "Figure 2:Pipeline of the Multimodal Music Description Model (MMDM).This process starts with the collection of music videos, followed by automated tagging to refine audio annotations using CLAP embedding similarities. Metadata and thematic descriptions are synthesized by the Llama 3.1 model to create training targets. The training utilizes LoRA fine-tuning in the MMDM to transform multimodal inputs into targeted music descriptions that align with the visual content’s themes.",
                "position": 150
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/3.2.png",
                "caption": "Figure 3:Framework of Dual-track Music Retrieval and Explicitly Conditioned Music Generation.The left part illustrates the Dual-track Music Retrieval process, which leverages our multimodal dataset to perform both broad and targeted retrieval. The right part shows the Explicitly Conditioned Music Generation pathway, where music is generated through a ControlFormer block integrating embeddings from selected music bridge, text bridge, and noisy inputs.",
                "position": 184
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Broader Impacts, Limitations, and Future Works",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADataset Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/Distribution_of_PAM_Scores.png",
                "caption": "Figure 4:Distribution of PAM Scores across the raw training dataset.",
                "position": 1642
            },
            {
                "img": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/dataset_distributions.png",
                "caption": "Figure 5:Histogram of music duration in the training dataset.",
                "position": 1657
            },
            {
                "img": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/dataset_distributions2.png",
                "caption": "Figure 6:Histogram of text word counts in the training dataset.",
                "position": 1660
            },
            {
                "img": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/mood_distribution.png",
                "caption": "Figure 7:Distribution of mood tags across the retrieval dataset. This histogram shows the frequency of various mood categories, illustrating the emotional diversity captured in our data.",
                "position": 1679
            },
            {
                "img": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/genre_distribution.png",
                "caption": "Figure 8:Genre distribution within the retrieval dataset. This bar graph reflects the variety of music genres represented, indicating the dataset’s broad applicability for genre-specific retrieval tasks.",
                "position": 1682
            },
            {
                "img": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/instrument_distribution.png",
                "caption": "Figure 9:Histogram of instrument tags in our retrieval dataset. This figure shows the range of musical instruments represented, underscoring the dataset’s comprehensive coverage of instrumental music.",
                "position": 1685
            },
            {
                "img": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/bpm_distribution.png",
                "caption": "Figure 10:Density curve of BPM across the retrieval dataset. This plot illustrates the distribution of Beats Per Minute, showcasing the tempo range covered in our collection.",
                "position": 1688
            },
            {
                "img": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/hist_audio_duration.png",
                "caption": "Figure 11:Histogram of audio durations in retrieval dataset. This shows the distribution of song lengths in the dataset.",
                "position": 1694
            },
            {
                "img": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/hist_text_word_count.png",
                "caption": "Figure 12:Histogram of text word counts in retrieval dataset. This represents the distribution of word counts in the associated text data.",
                "position": 1697
            },
            {
                "img": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/hist_lexical_diversity.png",
                "caption": "Figure 13:Histogram of lexical diversity scores in retrieval dataset. This shows the variation in vocabulary usage across text samples.",
                "position": 1700
            },
            {
                "img": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/survey.jpg",
                "caption": "Figure 14:Screenshot of the user study questionnaire in subjective evaluation.",
                "position": 1747
            }
        ]
    },
    {
        "header": "Appendix BExperiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/what_samba_dancers_wear_1.jpg",
                "caption": "Table 8:Samples of visual-to-description generation.",
                "position": 1912
            },
            {
                "img": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/02091449s53c__1_.jpg",
                "caption": "",
                "position": 1968
            },
            {
                "img": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/images.jpeg",
                "caption": "",
                "position": 1991
            },
            {
                "img": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/wa2.png",
                "caption": "",
                "position": 2014
            },
            {
                "img": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/863edf85eae2f4043a2f202197ce9145.jpg",
                "caption": "",
                "position": 2037
            },
            {
                "img": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/ins.jpg",
                "caption": "",
                "position": 2060
            },
            {
                "img": "https://arxiv.org/html/2412.09428/extracted/6065175/figs/aaaa.jpg",
                "caption": "",
                "position": 2083
            }
        ]
    },
    {
        "header": "Appendix CDemos",
        "images": []
    }
]