[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01680/x1.png",
                "caption": "Figure 1:Illustration of the modified agglomerative model training procedure. Instead of the student model learning to match the original teacher distributions, it learns to match the normalized distributions (our proposed PHI-S is shown). We show the real distributions forDFN CLIP,DINOv2,SigLIP, andSAMby projecting them down to 2D using PCA. In the original space, the variance of DFN CLIP is so small that it appears as a single point. During inference, we can estimate the original teacher distributions using the inverse normalization process on the student predictions.",
                "position": 117
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01680/x2.png",
                "caption": "Figure 2:Teacher activation histograms. We show the global histogram, as well as the histograms for the channels associated with the minimum mean, maximum mean, minimum variance, and maximum variance. While all being roughly normal, they have very different centers and scales. We provide specific values in table7in the appendix.",
                "position": 172
            },
            {
                "img": "https://arxiv.org/html/2410.01680/x3.png",
                "caption": "Figure 3:The loss curves for each of the four teachers that the ViT-B/16 student is learning to match simultaneously in original teacher space (e.g. denormalized). We emphasize â€œBaseline - MSEâ€ (Blue) and â€œPHI Standardizeâ€ (PHI-S, Red) as they generally set the upper and lower bounds.",
                "position": 177
            }
        ]
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01680/x4.png",
                "caption": "Figure 4:Visualization of how standardization affects the resulting data distribution. We start with the same distribution, and rotate the data by some angle. Regular standardizationâ€™s effect is directly tied to the distribution orientation. Conversely, PHI-S is invariant to any data rotation, and will produce an identical transform up to sign along each dimension. We can make the sign consistent by negating the rows ofğ‡ğ‡\\mathbf{H}bold_Handğ”ğ”\\mathbf{U}bold_Uwhich have a negative value in the diagonal position. Similarly, regular standardization will distort each dimension (shown with red/blue lines), which will have the effect of reducing the importance of high variance axis-aligned dimensions, and increasing the importance of low-variance dimensions. PHI-S is isotropic, so the change in scale is uniform.",
                "position": 863
            },
            {
                "img": "https://arxiv.org/html/2410.01680/x5.png",
                "caption": "Figure 5:Visualization of normalization procedures. We display two axis lines in red and blue. In the original space, theyâ€™re both 2 units long, and aligned with the plot coordinate system. We also display an â€œerror circleâ€ which is a unit circle in the normalized coordinate system. For the three whitening transforms you can see how they only differ by rotation. We also specifically draw colored dots on the error circle corresponding to the extremal points of the error circle when denormalized into an ellipse. PCA-W places the largest error magnitude on the x-axis, given that itâ€™s the dimension with largest eigenvalue thus estimation errors along the x dimension will have a much larger impact in the denormalized space. As we show in equation16, the error for ZCA will be proportional to the original distributionâ€™s covariance matrix, and thus, the extremal points are along the eigenvectors of the covariance matrix. Hadamard whitening has the extremal points at|x1|=|x2|=â€¦=|xC|subscriptğ‘¥1subscriptğ‘¥2â€¦subscriptğ‘¥ğ¶\\left|x_{1}\\right|=\\left|x_{2}\\right|=...=\\left|x_{C}\\right|| italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | = | italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | = â€¦ = | italic_x start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT |. Global Standardization and PHI-S are both isotropic, which means that thereâ€™s an infinite number of extremal points, so we instead show the points as they relate to the distribution itself. Similar to ZCA, for Global Standardization these points are along the principal axes. And similar to HCA, the aligned points for PHI-S are when|x1|=|x2|=â€¦=|xC|subscriptğ‘¥1subscriptğ‘¥2â€¦subscriptğ‘¥ğ¶\\left|x_{1}\\right|=\\left|x_{2}\\right|=...=\\left|x_{C}\\right|| italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | = | italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | = â€¦ = | italic_x start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT |.",
                "position": 877
            },
            {
                "img": "https://arxiv.org/html/2410.01680/x6.png",
                "caption": "Figure 6:Related to figure5and equation31, we visualize what happens to the one-hot error vectors when projecting back to the original space for HCA. We retain the original|x|=|y|ğ‘¥ğ‘¦|x|=|y|| italic_x | = | italic_y |dots, and add the one-hot dots demonstrating how their mapping remains equidistant from the origin relative to each other. In particular, sinceÎ´=1ğ›¿1\\delta=1italic_Î´ = 1, thenâˆ¥(ğ”â¢ğš²12â¢ğ‡âŠº)â¢ğš«râˆ¥=1Câ¢âˆ‘cÎ»câ‰ˆ1.400892delimited-âˆ¥âˆ¥ğ”superscriptğš²12superscriptğ‡âŠºsubscriptğš«ğ‘Ÿ1ğ¶subscriptğ‘subscriptğœ†ğ‘1.400892\\left\\lVert\\left(\\mathbf{U\\Lambda}^{\\frac{1}{2}}\\mathbf{H}^{\\intercal}\\right)%\n\\mathbf{\\Delta}_{r}\\right\\rVert=\\sqrt{\\frac{1}{C}\\sum_{c}\\lambda_{c}}\\approx 1%\n.400892âˆ¥ ( bold_U bold_Î› start_POSTSUPERSCRIPT divide start_ARG 1 end_ARG start_ARG 2 end_ARG end_POSTSUPERSCRIPT bold_H start_POSTSUPERSCRIPT âŠº end_POSTSUPERSCRIPT ) bold_Î” start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT âˆ¥ = square-root start_ARG divide start_ARG 1 end_ARG start_ARG italic_C end_ARG âˆ‘ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT italic_Î» start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT end_ARG â‰ˆ 1.400892for any choice ofrğ‘Ÿritalic_r. For reference,ğš²â‰ˆ[3.8356,0.0894]ğš²3.83560.0894\\mathbf{\\Lambda}\\approx\\left[3.8356,0.0894\\right]bold_Î› â‰ˆ [ 3.8356 , 0.0894 ].",
                "position": 884
            },
            {
                "img": "https://arxiv.org/html/2410.01680/x7.png",
                "caption": "",
                "position": 885
            },
            {
                "img": "https://arxiv.org/html/2410.01680/x8.png",
                "caption": "Figure 7:Similar to figure6and figure5, we visualize PHI-S in the normalized and denormalized spaces. This visualizes how equation27maintains errors along a circle in both spaces, owing to the isotropic nature of the transform. It also can be seen how the|x|=|y|ğ‘¥ğ‘¦|x|=|y|| italic_x | = | italic_y |error dots in normalized space map to the principal directions of the distribution, and also how the one-hot dots capture identical probability density.",
                "position": 895
            },
            {
                "img": "https://arxiv.org/html/2410.01680/x9.png",
                "caption": "",
                "position": 896
            },
            {
                "img": "https://arxiv.org/html/2410.01680/x10.png",
                "caption": "Figure 8:Following from figure5, we visualize the radius of the denormalized error circle at every angle between 0 and2â¢Ï€2ğœ‹2\\pi2 italic_Ï€. Because Global Standardization and PHI-S are isotropic, and because the distribution is mean centered (sectionA.5.2), they scale the error circle uniformly by the same amount. As predicted, forÎ¸=zâ¢Ï€2ğœƒğ‘§ğœ‹2\\theta=z\\frac{\\pi}{2}italic_Î¸ = italic_z divide start_ARG italic_Ï€ end_ARG start_ARG 2 end_ARGwithzâˆˆâ„¤ğ‘§â„¤z\\in\\mathbb{Z}italic_z âˆˆ blackboard_Z(e.g. wheny=0ğ‘¦0y=0italic_y = 0orx=0ğ‘¥0x=0italic_x = 0) we have the same error magnitude for HCA, and also where PHI-S and HCA have identical magnitude. HCA has extremal values atÎ¸hcaex=zâ¢Ï€2+Ï€4superscriptsubscriptğœƒhcaexğ‘§ğœ‹2ğœ‹4\\theta_{\\text{hca}}^{\\text{ex}}=z\\frac{\\pi}{2}+\\frac{\\pi}{4}italic_Î¸ start_POSTSUBSCRIPT hca end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ex end_POSTSUPERSCRIPT = italic_z divide start_ARG italic_Ï€ end_ARG start_ARG 2 end_ARG + divide start_ARG italic_Ï€ end_ARG start_ARG 4 end_ARG. PCA-W has extremal values atÎ¸pca-wex=zâ¢Ï€2superscriptsubscriptğœƒpca-wexğ‘§ğœ‹2\\theta_{\\text{pca-w}}^{\\text{ex}}=z\\frac{\\pi}{2}italic_Î¸ start_POSTSUBSCRIPT pca-w end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ex end_POSTSUPERSCRIPT = italic_z divide start_ARG italic_Ï€ end_ARG start_ARG 2 end_ARG. We also have that ZCA will have extremal valuesÎ¸pca-wexâ¢(z)â‰¤Î¸zcaexâ¢(z)â‰¤Î¸hcaexâ¢(z)superscriptsubscriptğœƒpca-wexğ‘§superscriptsubscriptğœƒzcaexğ‘§superscriptsubscriptğœƒhcaexğ‘§\\theta_{\\text{pca-w}}^{\\text{ex}}(z)\\leq\\theta_{\\text{zca}}^{\\text{ex}}(z)\\leq%\n\\theta_{\\text{hca}}^{\\text{ex}}(z)italic_Î¸ start_POSTSUBSCRIPT pca-w end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ex end_POSTSUPERSCRIPT ( italic_z ) â‰¤ italic_Î¸ start_POSTSUBSCRIPT zca end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ex end_POSTSUPERSCRIPT ( italic_z ) â‰¤ italic_Î¸ start_POSTSUBSCRIPT hca end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ex end_POSTSUPERSCRIPT ( italic_z ).",
                "position": 902
            }
        ]
    },
    {
        "header": "3Implementation Details",
        "images": []
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01680/x11.png",
                "caption": "Figure 9:Loss distributions for various normalization methods. The x-axis range is based on the minimum and maximum losses seen for each method over the course of 1,000 samples after training for 100k iterations. The â€œLargest Max Loss Channelâ€ shows the distribution for the channel that had the highest loss value. It helps us understand how vulnerable our learning process is to outliers. The â€œGlobalâ€ curve shows the distribution by combining all of the channels.",
                "position": 1157
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]