[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04519/x1.png",
                "caption": "",
                "position": 100
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04519/x2.png",
                "caption": "Figure 2:Comparison of DiT block architectures for autoregressive video generation.(a)Standard DiT block with full self-attention, which supports long-context modeling but lacks causality and streaming capability.(b)AR DiT block with masked causal attention, enabling autoregressive and streaming generation at the cost of weakened long-context consistency.(c)Our AR DiT block with a hybrid memory module and router, which combines local causal attention with a learnable global memory to achieve causal generation, streaming, and long-context support.",
                "position": 141
            },
            {
                "img": "https://arxiv.org/html/2512.04519/x3.png",
                "caption": "Figure 3:Illustration of attention mechanisms in AR DiT.LetTTbe the video token length andLLthe sliding-window size.(a) Causal Attention: Each query attends to all past tokens. It captures the full context with quadratic O(T²) complexity, impractical for long sequences.(b) Window Attention:Localized attention within a local sliding window. It enables efficient O(TL) complexity for streaming but causes information drift as early tokens are evicted.(c) Attention Sink:Adds fixed initial “sink” tokens to the window. It improves long-range consistency with O(TL) complexity, but the static memory leads to repetitive generation and fails to adapt to new content(d) Ours (Hybrid Memory):Augments window attention with a learnable memory that compresses evicted tokens. This maintains O(TL) efficiency while providing a dynamic global context, balancing long-term consistency and adaptability.",
                "position": 160
            }
        ]
    },
    {
        "header": "3Preliminary: From DiT to AR DiT",
        "images": []
    },
    {
        "header": "4VideoSSM",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04519/x4.png",
                "caption": "Figure 4:Illustration of how sink, evicted, and window tokens are arranged at different timesteps in a causal DiT with sliding-window attention. Here window lengthL=3L=3.",
                "position": 292
            },
            {
                "img": "https://arxiv.org/html/2512.04519/x5.png",
                "caption": "Figure 5:Architecture of the proposed hybrid memory module. The inputHtinH^{\\text{in}}_{t}is processed in two streams. Thelocal path(top) uses windowed attention with a sliding KV cache to computeHtlocalH^{\\text{local}}_{t}. Theglobal path(bottom) uses a State-Space Model (SSM) to recurrently compress historical information into a memory stateMM, which is retrieved to produceHtglobalH^{\\text{global}}_{t}. Arouterthen dynamically fuses the local and global outputs.",
                "position": 308
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04519/x6.png",
                "caption": "Figure 6:Qualitative comparison of 60s-long video generation. Baseline methods with windowed attention often suffer from error accumulation, leading to drifting artifacts, while methods using attention sink may produce repeated content or nearly static scenes. Our approach generates videos with more coherent motion and stable temporal consistency.",
                "position": 489
            },
            {
                "img": "https://arxiv.org/html/2512.04519/x7.png",
                "caption": "Table 1:Comparison with relevant baselines. We compare VideoSSM with representative open-source video generation models of similar parameter sizes and resolutions.222LongLive results are based on our implementation; other results are taken from[huang2025self,cui2025self,liu2025rolling].",
                "position": 492
            },
            {
                "img": "https://arxiv.org/html/2512.04519/x7.png",
                "caption": "Figure 7:Demonstration of interactive long video generation.",
                "position": 750
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    }
]