[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19817/x1.png",
                "caption": "",
                "position": 70
            },
            {
                "img": "https://arxiv.org/html/2510.19817/x2.png",
                "caption": "",
                "position": 83
            },
            {
                "img": "https://arxiv.org/html/2510.19817/logos/ai2_logo.png",
                "caption": "",
                "position": 93
            },
            {
                "img": "https://arxiv.org/html/2510.19817/x3.png",
                "caption": "",
                "position": 99
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19817/x4.png",
                "caption": "Table 1:Comparison ofolmOCR 2and other OCR systems.olmOCR 2achieves state-of-the-art performance while maintaining fully open data, model, and code.\nOpen-source licenses:Apache 2.01{}^{\\text{1}}\\text{Apache 2.0},MIT2{}^{\\text{2}}\\text{MIT};\nopen licenses with usage restrictions:OpenRAIL-M3{}^{\\text{3}}\\text{OpenRAIL-M},AGPL v34{}^{\\text{4}}\\text{AGPL v3};license not specified5{}^{\\text{5}}\\text{license not specified};API access only after accepting ToS6{}^{\\text{6}}\\text{API access only after accepting ToS}.\nResults are fully reproduced by ourselves, except those marked with * which are reported by their authors.",
                "position": 142
            },
            {
                "img": "https://arxiv.org/html/2510.19817/x5.png",
                "caption": "",
                "position": 210
            },
            {
                "img": "https://arxiv.org/html/2510.19817/x6.png",
                "caption": "",
                "position": 255
            },
            {
                "img": "https://arxiv.org/html/2510.19817/x7.png",
                "caption": "",
                "position": 265
            }
        ]
    },
    {
        "header": "2Why Unit Tests?",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19817/x8.png",
                "caption": "Figure 1:Binary unit test vs edit distance for reading order errors. Thecaptionis floating and can be correctly represented either before or after the section that contains thegreenandyellowpassages.\nA unit test that checks the presence of text ordering‘‘green, then yellow, uninterrupted by red’’will place an equivalent score to OCR output that places caption before or after the main passage.\nYet, edit distance highly penalizes cases where the caption occursafterthe yellow text.\nFurthermore, edit distance sometimes partially rewards cases which should be considered a severe reading order failure, such as when the caption occursin-betweenthe green and yellow texts or the green then yellow text ordering is flipped.",
                "position": 472
            },
            {
                "img": "https://arxiv.org/html/2510.19817/x9.png",
                "caption": "Figure 2:Binary unit test vs edit distance for math equation parsing. For a given equation and its reference LaTeX, model A produces a text output that is more dissimilar to the reference LaTeX than model B; however, after rendering and comparing the relative bounding box positions of rendered equation DOM elements, model A passes the unit test, while model B fails. Limitations of edit distance for math formulas are explored further in CDM(Wang et al.,2025b).",
                "position": 478
            }
        ]
    },
    {
        "header": "3Scaling Unit Test Generation for RLVR",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19817/x10.png",
                "caption": "Figure 3:HTML page generation for ourolmOCR 2synthetic data pipeline. We sample a page from a real document (left) and prompt a general VLM to generate a highly similar HTML page (right). The rendered HTML page image paired with the raw HTML serves as supervision for our OCR-specialized VLM.",
                "position": 485
            },
            {
                "img": "https://arxiv.org/html/2510.19817/x11.png",
                "caption": "Figure 4:Unit test rewards forolmOCR 2’s RLVR training. Given a generated HTMl page and its unit tests (left), we can easily score a generated Markdown page (right) according to these unit tests. Each test contributes a binary reward which is aggregated at a page-level as a pass rate. For example, with 4 of 6 passes, the page level reward is 0.67.",
                "position": 665
            }
        ]
    },
    {
        "header": "4Results",
        "images": []
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]