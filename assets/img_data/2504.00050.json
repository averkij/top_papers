[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.00050/extracted/6320189/figure/linear_regression_with_sizes_sftvsbases_swapped.png",
                "caption": "Figure 1:Judgment performance improvement vs. reasoning requirement across domainY-axis shows F1 score improvement (SFT ‚Äì base) based on Qwen2.5-7B-instruct; X-axis shows the proportion of tasks requiring reasoning. Each point represents a domain. A negative linear trend (y = -0.41x + 16.72, R¬≤ = 0.53) suggests that domains with more reasoning-heavy tasks benefit less from SFT alone. Sample counts across domains: 105 (Office_Productivity), 108 (Search_Information_Retrieval), 195 (Entertainment_Media), and estimated 108 (Social_Professional_Networking), 190 (Life_Utility).",
                "position": 119
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Judge-wise Outcome Reward and RL Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.00050/extracted/6320189/figure/linear_regression_with_sizes_sftvsours.png",
                "caption": "Table 1:Performance on JudgeLM and PandaLM. Qwen2.5-Instruct-Judge-SFT means the finetuned Qwen2.5-Instruct on JudgeLM train set. As pairwise comparisons rarely yield ties, we exclude tie cases (‚àºsimilar-to\\sim‚àº10% of the test set) for more interpretable evaluation, which JudgeLRM-7B‚Äôs F1 reaches 83.47, all metrics on PandaLM surpass DeepSeek-R1. (see TableBfor details). Notably,JudgeLRM-7B matches or even surpasses Deepseek-R1on PandaLM.",
                "position": 333
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.00050/extracted/6320189/figure/linear_regression_with_sizes_sftvsours.png",
                "caption": "Figure 3:Judgment performance improvement vs. reasoning requirement across domains. The Y-axis indicates the F1 score improvement of JudgeLRM-7B over the Qwen2.5-7B-Instruct-Judge-SFT baseline; the X-axis represents the proportion of tasks within each domain that require reasoning. Each point corresponds to a domain. A negative linear trend (y=0.2‚Å¢x‚àí1.05ùë¶0.2ùë•1.05y=0.2x-1.05italic_y = 0.2 italic_x - 1.05,R2=0.95superscriptùëÖ20.95R^{2}=0.95italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 0.95) suggests that domains with a higher proportion of reasoning-intensive tasks see greater performance gains from JudgeLRM-7B.",
                "position": 621
            },
            {
                "img": "https://arxiv.org/html/2504.00050/extracted/6320189/figure/preliminary_3b/response_length_mean_322.png",
                "caption": "Figure 4:Response Length of JudgeLRM-3B and JudgeLRM-7B by Steps.",
                "position": 654
            },
            {
                "img": "https://arxiv.org/html/2504.00050/extracted/6320189/figure/7blength.png",
                "caption": "",
                "position": 663
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    },
    {
        "header": "Details of PandaLM Category Analysis",
        "images": []
    },
    {
        "header": "Prompt for Accessing the Necessity of Reasoning When Judging",
        "images": []
    },
    {
        "header": "Appendix BPerformance on PandaLM testset excluding ties.",
        "images": []
    },
    {
        "header": "Appendix CCase Study 1",
        "images": []
    },
    {
        "header": "Appendix DCase study 2",
        "images": []
    }
]