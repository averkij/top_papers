[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22312/extracted/6488678/figures/Skywork-OR1-32B_math_detail.png",
                "caption": "Figure 1:The performance curve of Skywork-OR1-32B during RL training for AIME 2024 and AIME 2025. The red stars indicate the selected final checkpoints.",
                "position": 235
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22312/extracted/6488678/figures/32b_eval.png",
                "caption": "Figure 2:Performance of Skywork-OR1-32B on challenging mathematics and coding benchmarks.",
                "position": 421
            },
            {
                "img": "https://arxiv.org/html/2505.22312/extracted/6488678/figures/7b_eval.png",
                "caption": "Figure 3:Performance of Skywork-OR1-7B on challenging mathematics and coding benchmarks.",
                "position": 424
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3MAGIC in Skywork-OR1",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22312/extracted/6488678/figures/adv_mask_stage1.png",
                "caption": "Figure 5:Training accuracy and clip ratio during RL training of Skywork-OR1-Math-7B inStage I.accuracy: Mean accuracy reward on training batch.accuracy_nontruncated: Mean accuracy of non-truncated samples.clip_ratio: Ratio of truncated responses.",
                "position": 821
            },
            {
                "img": "https://arxiv.org/html/2505.22312/extracted/6488678/figures/adv_mask_compare_scaling_law.png",
                "caption": "Figure 8:AIME24 avg@32 performance vs. context length for different advantage mask strategies in Ablation Experiments 3. All strategies achieve the same accuracy at the 32K context length. The accuracy was further improved after the training ofStage IIeven though the noisy training signals from truncated responses were introduced inStage I.",
                "position": 1003
            },
            {
                "img": "https://arxiv.org/html/2505.22312/extracted/6488678/figures/impact_of_tau.png",
                "caption": "Figure 9:AIME25 avg@8 performance and entropy versus the number of training steps in Ablation Experiments 3. Training with a temperature of 0.6 starts with the lowest entropy and learns more slowly than at a temperature of 1.0. Note that the entropy in the right plot remains around 0.2 because adaptive entropy control is enabled. This experiment was conducted on an earlier version of the 32B variant using only math data. Note also that in the left plot, the two temperatures indicate the rollout temperatures used during training. The scores of AIME25 were obtained by evaluating both models at a temperature of 0.6 to ensure a fair comparison.",
                "position": 1010
            },
            {
                "img": "https://arxiv.org/html/2505.22312/extracted/6488678/figures/skywork_or1_math_7b_curve.png",
                "caption": "Figure 10:Entropy of generated responses (left) and avg@8 performance on AIME24 (right) of Skywork-OR1-Math-7B across all stages. We use adaptive entropy control withtgt-ent=0.2 andŒî=0.005Œî0.005\\Delta=0.005roman_Œî = 0.005. Under adaptive entropy control, the entropy of Skywork-OR1-Math-7B is generally lower-bounded by the target entropy 0.2 and the performance on AIME24 has been steadily improving.",
                "position": 1086
            }
        ]
    },
    {
        "header": "4Empirical Studies on Mitigating Policy Entropy Collapse",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22312/x1.png",
                "caption": "Figure 12:Overview of our empirical studies on mitigating policy entropy collapse.Gray‚Äå and green blocks:The potential benefits and possible approaches to enhance the model‚Äôs exploration capability and mitigate entropy collapse.Yellow blocks:The experimental variables in our empirical studies on keeping the model‚Äôs exploration capability and maintaining high plasticity.",
                "position": 1155
            },
            {
                "img": "https://arxiv.org/html/2505.22312/extracted/6488678/figures/ent_motivation.png",
                "caption": "Figure 13:Preliminary experiments on mitigating entropy collapse by introducing entropy loss. We tested two different coefficientsŒ±k=subscriptùõºùëòabsent\\alpha_{k}=italic_Œ± start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT =1e-3 and 5e-3, and found that the entropy loss with the higher coefficientŒ±ksubscriptùõºùëò\\alpha_{k}italic_Œ± start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, i.e., 5e-3, more effectively prevents entropy collapse and achieves higher test performance.Left: Accuracy curves on test benchmarks during RL training.Right: Entropy of generated responses during RL training.",
                "position": 1292
            },
            {
                "img": "https://arxiv.org/html/2505.22312/extracted/6488678/figures/entropy_of_bsz_and_gs.png",
                "caption": "Figure 14:Entropy of generated responses during on-policy updates with different rollout batch sizesDRsubscriptùê∑ùëÖD_{R}italic_D start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT(left) and group sizegs(right). All the experiments exhibit similar entropy dynamics.",
                "position": 1304
            },
            {
                "img": "https://arxiv.org/html/2505.22312/x2.png",
                "caption": "Figure 15:Illustration of on-policy vs. off-policy update in PPO-style policy loss. On-policy update applies a single SGD step to the entire rollout batch, whereas off-policy update implements multiple SGD steps through rollout batch decomposition and reuse. The rollout batch is partitioned intoDRDTsubscriptùê∑ùëÖsubscriptùê∑ùëá\\frac{D_{R}}{D_{T}}divide start_ARG italic_D start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT end_ARG start_ARG italic_D start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_ARGmini-batches, with each mini-batch undergoing an independent SGD step. Then, one can iterate over the rollout batchNreusesubscriptùëÅreuseN_{\\text{reuse}}italic_N start_POSTSUBSCRIPT reuse end_POSTSUBSCRIPTtimes. Thus, the total number of SGD steps performed on one rollout batch isDRDT√óNreusesubscriptùê∑ùëÖsubscriptùê∑ùëásubscriptùëÅreuse\\frac{D_{R}}{D_{T}}\\times N_{\\text{reuse}}divide start_ARG italic_D start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT end_ARG start_ARG italic_D start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_ARG √ó italic_N start_POSTSUBSCRIPT reuse end_POSTSUBSCRIPT.",
                "position": 1312
            },
            {
                "img": "https://arxiv.org/html/2505.22312/extracted/6488678/figures/n_sgd_compare_1.png",
                "caption": "Figure 16:Results of Ablation Experiments 6. Off-policy training with increasedNSGDsubscriptùëÅSGDN_{\\text{SGD}}italic_N start_POSTSUBSCRIPT SGD end_POSTSUBSCRIPTby either decreasingDTsubscriptùê∑ùëáD_{T}italic_D start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPTor increasingNreusesubscriptùëÅreuseN_{\\text{reuse}}italic_N start_POSTSUBSCRIPT reuse end_POSTSUBSCRIPTaccelerates entropy collapse and exhibits worse test performance.Left: Entropy of generated responses during RL training.Right: Test performance during RL training.",
                "position": 1357
            },
            {
                "img": "https://arxiv.org/html/2505.22312/extracted/6488678/figures/n_sgd_compare_2.png",
                "caption": "Figure 17:Results of Ablation Experiments 7. On-policy experiments, i.e.NSGD=1subscriptùëÅSGD1N_{\\text{SGD}}=1italic_N start_POSTSUBSCRIPT SGD end_POSTSUBSCRIPT = 1, do not exhibit premature entropy collapse and finally outperform the off-policy counterparts with the sameDTsubscriptùê∑ùëáD_{T}italic_D start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPTwhen training step is sufficiently large.Left: Entropy of generated responses during RL training.Right: Test performance at temperature 1 during RL training.",
                "position": 1391
            },
            {
                "img": "https://arxiv.org/html/2505.22312/extracted/6488678/figures/n_sgd_compare_3.png",
                "caption": "Figure 18:KeepingDRDT=4subscriptùê∑ùëÖsubscriptùê∑ùëá4\\frac{D_{R}}{D_{T}}=4divide start_ARG italic_D start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT end_ARG start_ARG italic_D start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_ARG = 4andNreuse=1subscriptùëÅreuse1N_{\\text{reuse}}=1italic_N start_POSTSUBSCRIPT reuse end_POSTSUBSCRIPT = 1, off-policy training with a largerDRsubscriptùê∑ùëÖD_{R}italic_D start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT, i.e.,DR=256subscriptùê∑ùëÖ256D_{R}=256italic_D start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT = 256, does not prevent the premature entropy collapse. Both off-policy experiments, i.e.NSGD=4subscriptùëÅSGD4N_{\\text{SGD}}=4italic_N start_POSTSUBSCRIPT SGD end_POSTSUBSCRIPT = 4, exhibit faster entropy convergence compared with the on-policy experiment withNSGD=1subscriptùëÅSGD1N_{\\text{SGD}}=1italic_N start_POSTSUBSCRIPT SGD end_POSTSUBSCRIPT = 1.",
                "position": 1401
            },
            {
                "img": "https://arxiv.org/html/2505.22312/extracted/6488678/figures/different_ent_coeff_comparison.png",
                "caption": "Figure 19:The results of Ablation Experiments 8.Left:The entropy of generated responses during RL training.Right:The AIME24 avg@8 performance at temperature 1 during RL training.",
                "position": 1451
            },
            {
                "img": "https://arxiv.org/html/2505.22312/extracted/6488678/figures/ent_sensitive_to_data.png",
                "caption": "Figure 20:Preliminary experiments investigating how training data affects the entropy during RL training. Both experiments used the same hyperparameter configurations withŒ±ksubscriptùõºùëò\\alpha_{k}italic_Œ± start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT=1e-3 but differed in the training data. Both datasets are in math domain. simply switching the dataset resulted in dramatically different entropy evolution patterns",
                "position": 1477
            },
            {
                "img": "https://arxiv.org/html/2505.22312/extracted/6488678/figures/target_entropy.png",
                "caption": "Figure 21:The results of Ablation Experiments 9. Applying adaptive entropy control prevents the entropy collapse, leading to a better test performance.Left:Entropy of generated responses during RL training.Right:Test performance during RL training.",
                "position": 1499
            },
            {
                "img": "https://arxiv.org/html/2505.22312/extracted/6488678/figures/clip_higher_compare.png",
                "caption": "Figure 22:The results of Ablation Experiments 10. Increasing the higher-clip ratio to an adequate value (e.g., 0.25 and 0.265) yields slower convergence and better test performance. However, we find that when the higher-clip ratio is set to 0.28 as recommended in[34], then entropy rises sharply and test performance is not improved.Left:Entropy of generated responses during RL training.Right:Test performance during RL training.",
                "position": 1520
            }
        ]
    },
    {
        "header": "5Empirical Studies on Training Resource Allocation",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22312/x3.png",
                "caption": "Figure 23:Overview of empirical studies on improving training efficiency given fixed computational resources.Grey‚Äå blocks:Potential approaches to enhance training efficiency and their underlying principles.Yellow blocks:Experimental variables in the empirical studies",
                "position": 1565
            },
            {
                "img": "https://arxiv.org/html/2505.22312/x4.png",
                "caption": "Figure 24:Overview of empirical studies on the effect of an increased rollout budget when more training resources are available.Grey and green blocks: The motivation of the empirical studies.Yellow blocks: The experimental variables in the empirical studies.",
                "position": 1742
            },
            {
                "img": "https://arxiv.org/html/2505.22312/extracted/6488678/figures/batch_size_time_compare.png",
                "caption": "Figure 25:Results of Ablation Experiments 11. Given more training resources, increasing the rollout budget by increasingDRsubscriptùê∑ùëÖD_{R}italic_D start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPTachieves better test performance with similar total training hours.",
                "position": 1763
            },
            {
                "img": "https://arxiv.org/html/2505.22312/extracted/6488678/figures/group_size_time_comparsion.png",
                "caption": "Figure 26:Results of Ablation Experiments 12. Given more training resources, increasing rollout budget by increasing the group size can achieve better test performance with similar total training hours.",
                "position": 1784
            }
        ]
    },
    {
        "header": "6Dataset Preparation",
        "images": []
    },
    {
        "header": "7Math & Code Verifiers",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22312/extracted/6488678/figures/reward_models_1.png",
                "caption": "Figure 27:Distributions of the number of correct rollouts from DeepSeek-R1-Distill-Qwen-7B, obtained using four different verifiers on a subset of NuminaMath-1.5 problems. The numbers 0‚Äì8 indicate difficulty levels. The size of each sector represents the number of problems at a specific difficulty level.",
                "position": 2043
            }
        ]
    },
    {
        "header": "8Experiments",
        "images": []
    },
    {
        "header": "9Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]