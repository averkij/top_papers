[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background and Related Work",
        "images": []
    },
    {
        "header": "3Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.05408/x1.png",
                "caption": "Figure 1:Crosslingual test-time scaling of s1 and Qwen models on the MGSM benchmark (excluding English) across different model sizes. In subfigure (a) we enforce a hard limit of maximum thinking token, and in (b) we measure their inference FLOP compute for a Pareto frontier analysis.ΔΔ\\Deltaroman_Δmeasures the absolute difference between average accuracy scores at 0.5k and 8k maximum thinking tokens. Dash lines indicate the best few-shot prompting baseline performance of Qwen.",
                "position": 358
            }
        ]
    },
    {
        "header": "4Crosslingual Test-Time Scaling",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.05408/x2.png",
                "caption": "Table 1:MGSM performance comparison against 14B-sized s1 model with maximum 8k thinking tokens.\nWe report the language-breakdown accuracy from cited papers if available; otherwise, we reproduce using their open-sourced models without any inference budget constraint. We report the average length of the generations (avg. len) and the relative accuracy difference (green text) between s1-14B under extrapolation budget forcing and its baseline Qwen2.5-14B-Instruct.\nWeboldboth s1 performance and baseline models that outperform s1.",
                "position": 401
            }
        ]
    },
    {
        "header": "5Language-Mixing Behaviors",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.05408/x2.png",
                "caption": "Figure 2:Proportion of dominant languages in models’ entire responses when queried with multilingual math questions. “same” indicates that the response language is the same as query language.",
                "position": 804
            },
            {
                "img": "https://arxiv.org/html/2505.05408/x2.png",
                "caption": "Figure 2:Proportion of dominant languages in models’ entire responses when queried with multilingual math questions. “same” indicates that the response language is the same as query language.",
                "position": 807
            },
            {
                "img": "https://arxiv.org/html/2505.05408/x3.png",
                "caption": "Figure 3:Breakdown of language-mixing patterns in s1’s reasoning. Percentage indicates the probability of a sentence being English only, quoting non-English phrases (quote-and-think), entirely being in a different language (intersentential), or mixing different languages within the same sentence (intrasentential).",
                "position": 812
            }
        ]
    },
    {
        "header": "6Language Forcing",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.05408/x4.png",
                "caption": "Figure 4:Language and domain breakdown for Global-MMLU benchmark. Dashed lines indicate the performance of zero-shot prompting of Qwen-32B-Instruct models.",
                "position": 1375
            },
            {
                "img": "https://arxiv.org/html/2505.05408/x5.png",
                "caption": "Figure 5:MGSM accuracy against number of thinking tokens in s1 models’ outputs in different reasoning languages.",
                "position": 1378
            }
        ]
    },
    {
        "header": "7Cross-Domain Generalization",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.05408/x6.png",
                "caption": "Figure 6:Effects of thinking time for s1 models on different domains of Global-MMLU benchmark (subfigure (a)) and cultural commonsense knowledge (FORK) and reasoning (COPAL-ID) benchmarks (subfigure (b)). Similar toFigure4, we added dashed lines as zero-shot prompting of Qwen-32B as baselines in (b).",
                "position": 1409
            },
            {
                "img": "https://arxiv.org/html/2505.05408/x7.png",
                "caption": "Figure 7:Performance comparison between s1 and zero-shot prompting of Qwen baseline for culturally-agnostic (C-Agnostic) and culturally-specific (C-Specific) questions in different domains. Results are average across languages.",
                "position": 1428
            }
        ]
    },
    {
        "header": "8Discussion and Future Work",
        "images": []
    },
    {
        "header": "9Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments and Disclosure of Funding",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AAuthors’ Contributions",
        "images": []
    },
    {
        "header": "Appendix BFurther Details on Crosslingual Test-Time Scaling",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.05408/x8.png",
                "caption": "Table 4:MGSM performance comparison against 32B-sized s1 model with maximum 8k thinking tokens.",
                "position": 2593
            },
            {
                "img": "https://arxiv.org/html/2505.05408/x8.png",
                "caption": "Table 5:MGSM performance comparison against 7B-sized s1 model with maximum 8k thinking tokens.",
                "position": 2722
            },
            {
                "img": "https://arxiv.org/html/2505.05408/x8.png",
                "caption": "Table 6:MGSM performance comparison against 3B-sized s1 model with maximum 8k thinking tokens.",
                "position": 2851
            },
            {
                "img": "https://arxiv.org/html/2505.05408/x8.png",
                "caption": "Table 7:MGSM performance comparison against 1.5B-sized s1 model with maximum 8k thinking tokens. We didn’t run extrapolation budget forcing since without it, s1 already generates extremely long CoTs.",
                "position": 2980
            }
        ]
    },
    {
        "header": "Appendix CFurther Details on Language-Mixing Behaviors",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.05408/x8.png",
                "caption": "Figure 8:Proportion of dominant languages used by 14B-sized models’ responses when queried with Japanese (ja), Russian (ru), Thai (th), and Mandarin Chinese (zh) languages. “same” indicates that the response language is the same as query language.",
                "position": 3122
            },
            {
                "img": "https://arxiv.org/html/2505.05408/x9.png",
                "caption": "Figure 9:Breakdown of s1-32B’s finegrained intrasentential language mixing patterns.",
                "position": 3139
            }
        ]
    },
    {
        "header": "Appendix DFurther Details on Language Forcing",
        "images": []
    },
    {
        "header": "Appendix EFurther Details on Cross-Domain Generalization",
        "images": []
    }
]