[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.00279/x1.png",
                "caption": "Figure 1:Benchmark performance of LongCat-Flash-Omni.",
                "position": 170
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.00279/x2.png",
                "caption": "Figure 2:An overview of the LongCat-Flash-Omni model architecture. The model is fully end-to-end and unifies multimodal understanding and generation across text, image, video, and audio within a single large language model framework. An vision encoder and an audio encoder are used to obtain vision features and audio features, respectively, which are then projected into a shared latent token space and fed into the LongCat-Flash LLM backbone. The LLM decoder directly generates multi-codebook speech tokens, parallel to generated text tokens, which are then converted to audio waveforms by an audio decoder. Shortcut-connected MoE (ScMoE) with zero-computation experts module proposed in LongCat-Flash is employed to achieve efficient multimodal fusion. Vision and audio features are chunk-wisely interleaved to support streaming audio-visual input.",
                "position": 377
            },
            {
                "img": "https://arxiv.org/html/2511.00279/x3.png",
                "caption": "Figure 3:Architecture of the audio decoder.",
                "position": 452
            },
            {
                "img": "https://arxiv.org/html/2511.00279/x3.png",
                "caption": "Figure 3:Architecture of the audio decoder.",
                "position": 455
            },
            {
                "img": "https://arxiv.org/html/2511.00279/x4.png",
                "caption": "Figure 4:Architecture of the audio encoder.",
                "position": 460
            }
        ]
    },
    {
        "header": "3Pre-Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.00279/x5.png",
                "caption": "Figure 5:Pre-training stages.",
                "position": 664
            },
            {
                "img": "https://arxiv.org/html/2511.00279/x6.png",
                "caption": "Figure 6:Schematic diagram of pre-training stage-1, where large-scale speech and text data is employed for the training process.",
                "position": 716
            }
        ]
    },
    {
        "header": "4Post-Training",
        "images": []
    },
    {
        "header": "5Training Infrastructures",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.00279/figs/data_distrubtion.png",
                "caption": "Table 2:Computation distribution per micro-batch across different modalities during the SFT stage.",
                "position": 931
            },
            {
                "img": "https://arxiv.org/html/2511.00279/figs/data_distrubtion.png",
                "caption": "Figure 7:The distribution of sequence token lengths across different modalities during the SFT stage.",
                "position": 968
            },
            {
                "img": "https://arxiv.org/html/2511.00279/x7.png",
                "caption": "Figure 8:Overview of modality-decoupled parallelism (MDP).The modality encoders and the LLM backbone are fully decoupled at the distributed level, enabling independent scheduling and improved computational efficiency.",
                "position": 984
            },
            {
                "img": "https://arxiv.org/html/2511.00279/x8.png",
                "caption": "Figure 9:Overview of the chunk-based ModalityBridge. The example configuration with num_chunk = 3, processing 4 microbatches of 8 images each across 4 GPUs (DP = 1, CP = 2, PP = 2)",
                "position": 1008
            },
            {
                "img": "https://arxiv.org/html/2511.00279/figs/moe_mfu_ep_cp_log2.png",
                "caption": "Figure 10:Benchmark results of MoE GEMMs under different CP/EP configurations with an 8K sequence length.",
                "position": 1062
            }
        ]
    },
    {
        "header": "6Inference and Deployment",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.00279/x9.png",
                "caption": "Figure 11:The asynchronous streaming pipeline for minimizing first packet latency.",
                "position": 1243
            }
        ]
    },
    {
        "header": "7Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.00279/figs/qualitative_analysis.png",
                "caption": "Figure 12:Qualitative analysis of real-time audio-visual interaction.",
                "position": 3175
            }
        ]
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "9Contributions",
        "images": []
    }
]