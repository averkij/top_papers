[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.00912/x1.png",
                "caption": "",
                "position": 177
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.00912/x2.png",
                "caption": "Figure 2:Illustration ofSlidesBench.Each example ofSlidesBenchconsists of three instructions: Detailed Instructions with Images, Detailed Instructions Only, and High-Level Instructions. The model is tasked to generate a slide based on the instruction, and the generated slide is evaluated on the metrics suite, which contains both the reference-free metrics and the reference-based metrics.",
                "position": 238
            }
        ]
    },
    {
        "header": "2SlidesBench",
        "images": []
    },
    {
        "header": "3Evaluation Metrics",
        "images": []
    },
    {
        "header": "4Method",
        "images": []
    },
    {
        "header": "5Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.00912/x3.png",
                "caption": "Figure 3:Examples of slides generated by different methods in three scenarios.End-to-end image generation methods fail to generate structured and clear slides. Small open-sourced models likeLlaMaandLlaVacan barely generate any usable slides, whileAutoPresentproduces quality slides. AddingSlidesLibimproves GPT-4oâ€™s performance ondetailed instruction onlyandhigh-level instructiontasks.",
                "position": 814
            },
            {
                "img": "https://arxiv.org/html/2501.00912/x4.png",
                "caption": "Figure 4:Perceptual evaluation results on detailed instruction (1) with images and (2) only settings.We ask the users to score the quality of each slide from 1-5 and report the average score of each model. The user reported preference on GPT-4o andAutoPresentcompared withLlaMa, while still having a gap with human-designed slides.",
                "position": 1070
            },
            {
                "img": "https://arxiv.org/html/2501.00912/x5.png",
                "caption": "Figure 5:Auto-refinement results with GPT-4o, where the model further addresses some previously neglected instructions (marked in green), such as shape, background color, and text.",
                "position": 1111
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion and Limitations",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASlidesBenchDetails",
        "images": []
    },
    {
        "header": "Appendix BSlidesLibDetails",
        "images": []
    },
    {
        "header": "Appendix CTraining Details forAutoPresent",
        "images": []
    },
    {
        "header": "Appendix DRefinement Details",
        "images": []
    },
    {
        "header": "Appendix EDetailed Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.00912/extracted/6105967/figures/user_example.png",
                "caption": "Figure 11:An example of the perceptual analysis question.We ask the human to score the quality of the slide from 1-5.",
                "position": 3622
            }
        ]
    },
    {
        "header": "Appendix FPerceptual Analysis",
        "images": []
    }
]