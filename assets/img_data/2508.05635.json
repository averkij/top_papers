[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05635/figs/logo.png",
                "caption": "",
                "position": 89
            },
            {
                "img": "https://arxiv.org/html/2508.05635/x1.png",
                "caption": "Figure 1:Overview of the Genie Envisioner World Foundation Platform.Genie Envisioner is a unified world foundation platform that integrates manipulation policy learning and evaluation within a single video-generative framework. At its core lies GE-Base, a large-scale world model that encodes the spatial, temporal, and semantic structure of robotic interactions. Built around it are two key functional modules: GE-Act, a world action model that infers instruction-conditioned policies, and GE-Sim, a video-basedworld simulatorthat enables closed-loop execution through action-conditioned generation. The platform is complemented by EWMBench, an integrated evaluation suite that assesses visual fidelity, physical plausibility, and instruction-policy alignment. GE thus provides a practical and scalable foundation for general intelligence embodiment.",
                "position": 109
            },
            {
                "img": "https://arxiv.org/html/2508.05635/x2.png",
                "caption": "Figure 2:Real-world demonstration of GE-Act on a novel robot embodiment, Agilex Cobot Magic, unseen during pretraining.With only one hour of embodiment- and task-specific teleoperation data for post-training, GE-Act successfully executes a complex manipulation task involving fine-grained control of deformable objects and memory-based decision making.\nGiven a general packaging rule, the robot is required to complete the packing process for each item accordingly. Here, we showcase the detailed execution of the first packing cycle.\nThe robot first stacks a deformable box, places a target object inside based on instruction, and closes the lid,rendering the object no longer visible. It then correctly selects and applies the appropriate stamp, matching the object type, relying solely on internal memory. This showcases GE’s generalization to new embodiments, its precise handling of deformable materials, and its ability to retain task-relevant memory across steps.\n.",
                "position": 114
            }
        ]
    },
    {
        "header": "Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05635/x3.png",
                "caption": "Figure 3:Overview of the GE-Base World Foundation Model.(a) An illustration of the autoregressive video generation process. Given multi-view visual conditions, including the initial observation and sparse memory, along with corresponding noise and positional embeddings, the model generates the next multi-view video chunk conditioned on a language instruction.\n(b) A dedicated causal block facilitates information exchange across different views, ensuring spatial consistency during multi-view video chunk generation.",
                "position": 149
            }
        ]
    },
    {
        "header": "GE-Base: World Foundation Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05635/x4.png",
                "caption": "Figure 4:Overview of the GE-Base Training Process.GE-Base is pre-trained on AgiBot-World-Beta, a large-scale real-world dual-arm robotic manipulation dataset containing 1 million instruction-aligned, multi-view video sequences. The training begins with a domain adaptation phase, transferring general video generation capabilities into the robotic domain using high-frame-rate sequences and mixed sampling strategies to enhance robustness. This is followed by a low-frame-rate fine-tuning stage designed to align the model with the temporal resolution required for downstream action policy training. Throughout the process, the video encoder and video decoder remain fixed.",
                "position": 222
            },
            {
                "img": "https://arxiv.org/html/2508.05635/x5.png",
                "caption": "Figure 5:Multi-View Robotic Manipulation Videos Generated on AgiBot G1 by GE-Base.We visualize robotic manipulation sequences generated by GE-Base across two tasks involving varied objects and environments. For each example, videos from three views are presented,i.e., the head-mounted, left-, and right-arm cameras, respectively.",
                "position": 231
            }
        ]
    },
    {
        "header": "GE-Act: World Action Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05635/x6.png",
                "caption": "Figure 6:Overview of the GE-Act World Action Model.GE-Act extends the GE-Base foundation model by incorporating a parallel action branch that converts visual latent representations into structured action policy trajectories. It follows the same block design and depth as GE-Base but reduces the hidden dimensions to improve efficiency. Visual latent features are integrated into the action pathway through a cross-attention mechanism, ensuring the semantic grounding of actions. Final action predictions are generated using a diffusion-based denoising flow-matching pipeline, refining noisy action predictions into coherent action trajectories.",
                "position": 263
            },
            {
                "img": "https://arxiv.org/html/2508.05635/x7.png",
                "caption": "Figure 7:Overview of the GE-Act Training Pipeline.GE-Act is derived from the GE-Base foundation model through a three-stage training process utilizing text–video–policy triplets from the AgiBot-World-Beta dataset. The first stage performs action-space pretraining, where the visual backbone is optimized to project video sequences into a latent action policy space. Subsequently, a two-stage task adaptation procedure is conducted to specialize the model for diverse downstream tasks. In this phase, the video encoder is first adapted using task-specific visual data, followed by fine-tuning of the action head using corresponding control signals.",
                "position": 311
            },
            {
                "img": "https://arxiv.org/html/2508.05635/x8.png",
                "caption": "Figure 8:Comparison of Task-Specific Real-World Robotic Manipulation Performance on the AgiBot G1 Platform.We compare GE-Act with state-of-the-art VLA baselines across multiple real-world dual-arm robotic tasks, using two evaluation metrics to assess performance.",
                "position": 343
            },
            {
                "img": "https://arxiv.org/html/2508.05635/x9.png",
                "caption": "Figure 9:Visualization of Real-World Robotic Manipulation on AgiBot G1 via GE-Act.Conditioned on natural language instructions, GE-Act generates and executes action policies on the AgiBot G1 platform. The visual samples demonstrate the model’s capability to produce consistent, reliable, and contextually appropriate manipulation behaviors, showcasing its robustness and effectiveness in real-world environments.",
                "position": 352
            },
            {
                "img": "https://arxiv.org/html/2508.05635/x10.png",
                "caption": "Figure 10:Multi-View Video Generation on the Agilex Cobot Magic Robotic Platform by GE-Base.Visualization of instruction-conditioned video generated by GE-Base for two complex folding tasks on the cross-embodiment Agilex Cobot Magic robot. Each row displays temporally sampled frames from a multi-view sequence.",
                "position": 367
            }
        ]
    },
    {
        "header": "Cross-Embodiment Generalization with Genie Envisioner",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05635/x11.png",
                "caption": "Figure 11:Performance comparison of two deformable object folding tasks on Agilex Cobot Magic.",
                "position": 408
            },
            {
                "img": "https://arxiv.org/html/2508.05635/x12.png",
                "caption": "Figure 12:Visualization of Real-World Demonstrations with GE-Act on Agilex Cobot Magic Platform.This shows GE-Act adapted to a novel Agilex Cobot Magic embodiment, performing real-world robotic manipulation tasks, including cloth-folding and box-folding.",
                "position": 414
            },
            {
                "img": "https://arxiv.org/html/2508.05635/x13.png",
                "caption": "Figure 13:Visualization of Robotic Video Generation and Real-World Manipulation on Dual Franka via GE.",
                "position": 422
            },
            {
                "img": "https://arxiv.org/html/2508.05635/x14.png",
                "caption": "Figure 14:Performance comparison on Dual Franka.",
                "position": 429
            },
            {
                "img": "https://arxiv.org/html/2508.05635/x15.png",
                "caption": "Figure 15:Overview of the GE-Sim World Simulator.(a) GE-Base is transferred into an action-conditioned video generator for simulating robotic behavior given predicted actions. Spatial pose conditions are projected into image space and fused with historical visual inputs, while temporal motion deltas are concatenated with a reference image to preserve style consistency and injected via cross-attention into the generation model.\n(b) GE-Sim enables closed-loop policy evaluation and controllable data generation by producing action-conditioned video rollouts, supporting instruction-following and consistent trajectory replay under diverse visual contexts.",
                "position": 436
            }
        ]
    },
    {
        "header": "GE-Sim: World Simulator",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05635/x16.png",
                "caption": "Figure 16:Visualization of Action-Conditioned Video Generation by GE-Sim.Given a ground-truth action policy, we generate the corresponding next-frame prediction using GE-Sim. For each sample, we visualize the head-view image by overlaying the projected action target onto the current frame, alongside the predicted next frame, to illustrate the model’s spatial alignment with the intended control signal.",
                "position": 489
            }
        ]
    },
    {
        "header": "EWMBench: Embodied World Model Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05635/x17.png",
                "caption": "Figure 17:Comprehensive Evaluation of Video World Models for Robotic Manipulation.Leveraging our EWMBench , we systematically evaluate a suite of video world models sourced from state-of-the-art general video generation and embodied world modeling approaches. All models are assessed under a unified text-and-image to video generation paradigm. Evaluation spans multiple levels, scene, motion and semantics, capturing visual fidelity, temporal coherence, and semantic grounding in diverse real-world robotic manipulation tasks.",
                "position": 579
            },
            {
                "img": "https://arxiv.org/html/2508.05635/x18.png",
                "caption": "Figure 18:Consistency and Validity Analysis of Evaluation Metrics.We compare human preference with our proposed EWMBench and the general video benchmark VBench to assess the consistency and reliability of automated evaluation metrics across different video world models.",
                "position": 633
            },
            {
                "img": "https://arxiv.org/html/2508.05635/x18.png",
                "caption": "Figure 18:Consistency and Validity Analysis of Evaluation Metrics.We compare human preference with our proposed EWMBench and the general video benchmark VBench to assess the consistency and reliability of automated evaluation metrics across different video world models.",
                "position": 636
            }
        ]
    },
    {
        "header": "Related Works",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]