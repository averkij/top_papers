[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00227/extracted/6485336/figs/fig1-v1.png",
                "caption": "Figure 1:Counterfactual Crash Generation: this diagram demonstrates the ability of our model to generate counterfactual crashes (middle-row: no crash, bottom-row: ego/car crash) while beginning from the initial frame and 3 bounding-boxes frames of the real video (top-row: the real car crash).",
                "position": 117
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Our Method: Ctrl-Crash",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00227/extracted/6485336/figs/fig2-v2.png",
                "caption": "Figure 2:Model architecture: Ctrl-Crash treats Bounding Boxes (BBs) as images. Both BBs and images frames as fed to a VAE image encoder. The crash type and BB embeddings are fed to ControlNet. The images embedding after adding noise (xtsubscriptùë•ùë°x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT), the noise level (tùë°titalic_t), and the ControlNet intermediate outputs (cùëêcitalic_c) are fed to the Stable Video Diffusion (SVD) model to obtain the predicted noiseœµŒ∏‚Å¢(xt,t,c)subscriptitalic-œµùúÉsubscriptùë•ùë°ùë°ùëê\\epsilon_{\\theta}(x_{t},t,c)italic_œµ start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t , italic_c ). Clip embeddings are computed by passing the first image for each video through a pretrained CLIP encoder. These CLIP embeddings are then fed to the ControlNet and SVD models. on The diffusion process is solved over multiple steps using Classifier-Free Guidance in the latent space and then decoded back to images using the VAE image decoder. See Section3for details.",
                "position": 235
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00227/extracted/6485336/figs/visual_fidelity.png",
                "caption": "(a)Visual preference comparison in generated videos.",
                "position": 523
            },
            {
                "img": "https://arxiv.org/html/2506.00227/extracted/6485336/figs/visual_fidelity.png",
                "caption": "(a)Visual preference comparison in generated videos.",
                "position": 526
            },
            {
                "img": "https://arxiv.org/html/2506.00227/extracted/6485336/figs/physical_realism.png",
                "caption": "(b)Realism preference comparison in physical appearance.",
                "position": 531
            },
            {
                "img": "https://arxiv.org/html/2506.00227/x1.png",
                "caption": "Figure 4:Qualitative results comparing AVD2, DrivingGen, Ctrl-V, and Ctrl-Crash. The crash generated by AVD2 is visually shaky, with scenes that often lack consistency. Driving-Gen also produces low-quality and choppy videos. While Ctrl-V achieves good visual quality, it fails to generate realistic crash events. In contrast, Ctrl-Crash outperforms all baselines in both visual fidelity and scene consistency, while accurately modeling crash dynamics. Additional video demonstrations are available on theproject page, and in AppendixA.1.",
                "position": 547
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00227/x2.png",
                "caption": "Figure 5:Other diffusion-based video models‚Äîincluding state-of-the-art approaches‚Äîconsistently fail to generate plausible crash scenarios.Top row: Nvidia Cosmos-Predict1-7B-Text2World with the prompt\"On a highway two cars collide at very fast speeds head-on\"produces a highly implausible scene, where the car on the left suddenly starts to levitate from its rear-end and a cloud of smoke resembling an explosion appears, followed by disjointed fragments of torn metal emerging from the ground that transforms in a dark vehicle rushing towards the camera.Middle row: OpenAI Sora with the prompt\"At an intersection two cars collide with each other at full speed resulting in a crash\"generates a car that spins erratically, changes shape and direction, and produces visible artifacts‚Äîculminating in a pile of twisted metal and glass.Bottom row: Ctrl-V, conditioned on a sequence of bounding boxes representing a head-on collision, renders two extremely blurry cars approaching each other. One car vanishes entirely at the moment of impact, while the other gradually fades away. Despite partially capturing the intent of the prompts, all models fail to produce a physically plausible and coherent crash, even when multiple samples are generated.",
                "position": 1163
            },
            {
                "img": "https://arxiv.org/html/2506.00227/x3.png",
                "caption": "Figure 6:Qualitative comparison of \"rear-end crashes\" between different methods. For each method, we show 5 frames from the video along with either a green check mark if there appears to have a crash in the video otherwise a red ‚ÄôX‚Äô.From top to bottom:SVD(stable-video-diffusion-img2vid)[2]prompted with the initial frame from a rear-end crash video, we see some normal driving but very inconsistent lighting and color shades with visible distorsions.AVD2[27], we see what appears to be a rear-end crash with a very distorted leading vehicle and background.DrivingGen[15], we see a rear-end crash with a leading vehicle that changes appearance every frame. Overall the video is very choppy with little temporal consistency.Cosmos(Cosmos-Predict1-7B-Video2World)[1], prompted with text suggesting a rear-end crash and 9 initial images where a car is rapidly approaching a truck, the predicted frames show the car unrealistically shrinking as it approaches the truck without any signs of a collision.Ctrl-V[30], prompted with a sequence of bounding-boxes suggesting a rear-end crash with a leading car, we see the leading car keep its distance and not crash occurs.Ctrl-Crash(ours): prompted with the same bounding box sequence as Ctrl-V and with the discrete crash type \"ego/vehicle crash\", we see a physically plausible rear-end collision with the ego vehicle visibly shaking from the impact. Visit our project page for animated video examples:https://anthonygosselin.github.io/Ctrl-Crash-ProjectPage/",
                "position": 1173
            },
            {
                "img": "https://arxiv.org/html/2506.00227/x4.png",
                "caption": "Figure 7:Qualitative comparison of \"t-bone crashes\" between different methods. For each method, we show 6 frames from the video along with either a green check mark if there appears to have a crash in the video otherwise a red ‚ÄôX‚Äô.From top to bottom:SVD(stable-video-diffusion-img2vid)[2]prompted with the initial frame from a t-bone car crash video, we see blurry vehicle a distorted motion blur as it drives in front of the ego vehicle without any collision.AVD2[27], we can make out what seems to be a t-bone crash with a heavily distorted black car. There are many artifacts and temporal inconsistencies which makes the sequence of events hard to follow.DrivingGen[15], a gray sedan drive in front of the ego vehicle progressively getting closer until it seems to collide with it. Motion is jerky and uneven between timesteps and the appearance of the gray car shapes almost every frame.Cosmos(Cosmos-Predict1-5B-Video2World)[1], prompted with creating a t-bone crash and 9 initial frames showing a car turn in front of the ego car, we see the leading car start to distort as the ego approaches it and then it shrivels and shrinks until it nearly disappears.Ctrl-V[30], prompted with a sequence of bounding-boxes suggesting a t-bone crash with a car incoming from the left, we see a car drive in from the left and then just passed the ego car without any collision.Ctrl-Crash(ours): prompted with the same bounding box sequence as Ctrl-V and with the discrete crash type \"ego/vehicle crash\", we see a physically plausible t-bone collision with the a black sedan incoming from the left. Visit our project page for animated video examples:https://anthonygosselin.github.io/Ctrl-Crash-ProjectPage/",
                "position": 1177
            },
            {
                "img": "https://arxiv.org/html/2506.00227/x5.png",
                "caption": "Figure 8:Ctrl-Crash qualitative results conditioned on an initial 9 bounding box frames (i.e., the first two frames of these sequences were conditioned on bounding box frames, but not the others). The animated videos for the examples presented above are provided in the supplementary material ZIP file and more video examples can be viewed at our project page:https://anthonygosselin.github.io/Ctrl-Crash-ProjectPage/",
                "position": 1181
            },
            {
                "img": "https://arxiv.org/html/2506.00227/x6.png",
                "caption": "Figure 9:Ctrl-Crash qualitative results conditioned on 25 (all) bounding box frames. The animated videos for the examples presented above are provided in the supplementary material ZIP file and more video examples can be viewed at our project page:https://anthonygosselin.github.io/Ctrl-Crash-ProjectPage/",
                "position": 1185
            },
            {
                "img": "https://arxiv.org/html/2506.00227/x7.png",
                "caption": "Figure 10:Counterfactual Crash Generation: this diagram demonstrates the ability of our model to generate counterfactual crashes while beginning from the identical initial frame.Top:a ground truth accident between two vehicles other than the ego-vehicle, where the red car hits the rear of the blue car and spins into the lane in front of the ego-vehicle.Bottom:the model generates an alternative accident involving the ego-vehicle. In this alternative future the red car avoids the blue car but turns into the path of the ego-vehicle leading to the crash.",
                "position": 1190
            }
        ]
    },
    {
        "header": "Appendix BFFT-Based Filtering Heuristic",
        "images": []
    },
    {
        "header": "Appendix CVideo Processing",
        "images": []
    },
    {
        "header": "Appendix DBounding Box Extraction Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00227/x8.png",
                "caption": "Figure 11:Shows sequence of frames with a car going into the ditch. SAM2‚Äôs mask predictions for the two cars in the video are shown in purple and orange. YOLO‚Äôs first prediction of the purple colored car is shown in frame 16 with a light pink bounding box. SAM2 works bidirectionally and can therefore infer the appearance of the purple-colored car many frames earlier and until the very last instant before it disappears into the ditch; this would not be possible with a tracker solely based on YOLO which relies on the object to be recognizable as one of the target classes in order to be detected.",
                "position": 1399
            }
        ]
    },
    {
        "header": "Appendix EBounding Box Conditioning",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00227/x9.png",
                "caption": "Figure 12:Left: example frame from a driving video.Right: associated bounding box frame conditioning generated from our pipeline. Road users are represented as 2D bounding boxes with unique fill colors representing their track ID and specific border colors representing their class.",
                "position": 1468
            }
        ]
    },
    {
        "header": "Appendix FFVD and JEDi Metrics Computation",
        "images": []
    },
    {
        "header": "Appendix GTraining and Implementation Details",
        "images": []
    },
    {
        "header": "Appendix HDatasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00227/extracted/6485336/figs/dada_accident_classification.jpg",
                "caption": "Figure 13:Original accident type definition used by the MM-AU dataset[10]as defined by the DADA2000 dataset[11]",
                "position": 1783
            },
            {
                "img": "https://arxiv.org/html/2506.00227/extracted/6485336/figs/gform-screen1.png",
                "caption": "Figure 14:User Survey Screenshot 1, showing 2 (out of 3) samples from question 1. What is shown as static image here was a GIF in the original Google Form.",
                "position": 1873
            },
            {
                "img": "https://arxiv.org/html/2506.00227/extracted/6485336/figs/gform-screen2.png",
                "caption": "Figure 15:User Survey Screenshot 2, showing the evaluation questions for each batch of 3 videos. Users were forced to rank all videos and to only use each rank once (i.e. it is not possible to submit the form when more than one video in each batch has the same rank).",
                "position": 1877
            }
        ]
    },
    {
        "header": "Appendix IUser Survey",
        "images": []
    }
]