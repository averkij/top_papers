[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12990/x1.png",
                "caption": "Figure 1:Left: During training, the domain-specific SAE is trained on the residuals of the general SAE (frozen weights).Right: During inference, the domain-specific and general SAEs are stitched together.\nWe show the equivalence of the training (left) and inference (right) outputs in Section3.2.",
                "position": 159
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12990/x2.png",
                "caption": "Figure 2:Similarity analysis between domain-specific UN Debate features and the base features. (left) The distribution of the maximum cosine similarity between each domain-adapted feature and any feature in the base model. (right) The cumulative distribution function of these similarities. Notably, SAE Boost features exhibit lower similarity to base features, indicating that they capture more novel domain-specific information rather than merely replicating existing representations.",
                "position": 527
            },
            {
                "img": "https://arxiv.org/html/2507.12990/x3.png",
                "caption": "Figure 3:SAE Boost is compared to other baselines on the Pareto frontier, with the optimal area marked by an arrow. SAE Boost is closer to the optimal point. See Section4.2.3for more details.",
                "position": 535
            },
            {
                "img": "https://arxiv.org/html/2507.12990/x4.png",
                "caption": "Figure 4:Evolution of explained variance during training for domain-specific and general domains. This figure tracks the training progress of the residual SAE on both UN Debates (blue line) and Fineweb datasets (orange line), measured against the baseline performance of the pretrained SAE (dashed lines). This demonstrates that sufficient training (>>>50M tokens) is critical for the residual SAE to develop complementary features that enhance domain-specific performance without degrading general capabilities. The training dynamics reveal that undertraining the residual SAE (<<<50M tokens) would result in suboptimal feature quality with potential negative impacts on general domain performance.",
                "position": 568
            },
            {
                "img": "https://arxiv.org/html/2507.12990/x5.png",
                "caption": "Figure 5:Representative domain-specific features discovered by SAE Boost, along with their corresponding top activations.",
                "position": 574
            },
            {
                "img": "https://arxiv.org/html/2507.12990/x6.png",
                "caption": "Figure 6:A t-SNE visualization of the feature embeddings from the base SAE and multiple domain-specific SAE Boost models. The figure reveals distinct clustering by domain type, with base features (blue) occupying the central region, Chemistry features (orange) forming a compact cluster to the left, and UN Debates features (turquoise) at the bottom. Notably, cross-lingual features cluster together on the right side, with related languages in close proximity (e.g., the Romance languages Portuguese and Italian, and the Germanic languages German and Dutch). This organization demonstrates that SAE Boost captures both domain-specific knowledge and inherent relationships between related domains, while maintaining clear separation from general features.",
                "position": 577
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Experimental Results",
        "images": []
    }
]