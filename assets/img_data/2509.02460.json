[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02460/x1.png",
                "caption": "",
                "position": 87
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Task Definition",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02460/x2.png",
                "caption": "Figure 2:Workflow of GenCompositor. GenCompositor takes a background video and a foreground video as input. Users can specify the trajectory and scale of the added foreground elements. We firstly convert user-given instructions to model inputs, then generate with a background preservation branch and a foreground generation mainstream, consisting of the proposed ERoPE and DiT fusion blocks. Following this, our model automatically composites input videos.",
                "position": 199
            },
            {
                "img": "https://arxiv.org/html/2509.02460/x3.png",
                "caption": "Figure 3:DiT fusion blockreceives concatenated tokens, some are to-be-generated tokens (blue) and some are unaligned conditional tokens (green). It fuses the two via pure self-attention. The final tokens with gradient color represent the mixture of generated tokens and masked conditional tokens from the BPBranch.",
                "position": 202
            },
            {
                "img": "https://arxiv.org/html/2509.02460/x4.png",
                "caption": "Figure 4:RoPE and our ERoPE. (A) 3D RoPE assigns labels to embeddings of videos. (B) For videos with inconsistent layout, our ERoPE extends RoPE by assigning unique labels to each embedding from different videos, avoiding interference.",
                "position": 255
            },
            {
                "img": "https://arxiv.org/html/2509.02460/x5.png",
                "caption": "Figure 5:ERoPE is superior in fusing layout-unaligned videos. As dynamic content in foreground video is often centered, using RoPE leads to content interference as shown in red box of “w/ RoPE”, while our ERoPE resolves this issue well.",
                "position": 258
            },
            {
                "img": "https://arxiv.org/html/2509.02460/x6.png",
                "caption": "Figure 6:Visual comparison with video harmonization. The compared methods cannot achieve satisfactory results with jagged artifacts at the edges of foreground elements, inconsistent color or lighting, while our method achieves better performance.",
                "position": 271
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02460/x7.png",
                "caption": "Figure 7:Visual comparison with trajectory-controlled video generation. All methods share the same trajectory, which is plotted as a red line in the results of Tora. Obviously, Tora and ReVideo remain with problems such as inconsistent foreground element IDs across frames and uncontrollable foreground element motions. Our method resolves these issues well.",
                "position": 343
            },
            {
                "img": "https://arxiv.org/html/2509.02460/x8.png",
                "caption": "Figure 8:Visual ablation results. The manually paste results showcase jagged artifacts. “w/o fusion block” cannot inject element faithfully. “w/o augmentation” and “w/o mask inflation” contain jagged artifacts at the edge of element. “w/o BPBranch” cannot realistically adjust foreground element. The ”fullmodel” setting performs best, which naturally fuses foreground elements with the background video.",
                "position": 401
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02460/x9.png",
                "caption": "Figure 9:Dataset construction pipeline. We construct VideoComp dataset with two stages: data curation and data filtering. The former includes three steps: collection, labeling, and segmentation. Then, we select a high-quality subset based on several rules.",
                "position": 1307
            }
        ]
    },
    {
        "header": "7Dataset Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02460/x10.png",
                "caption": "Figure 10:Generalizability. After training for video compositing, our method seamlessly enables video inpainting and the removal of target objects from videos with blank foreground condition.",
                "position": 1329
            }
        ]
    },
    {
        "header": "8Generalization Ability",
        "images": []
    },
    {
        "header": "9Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02460/x11.png",
                "caption": "Figure 11:User study comparison. Our method receives the most user preference for both video harmonization(a) and trajectory-controlled generation(b).",
                "position": 1356
            }
        ]
    },
    {
        "header": "10User Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02460/x12.png",
                "caption": "Figure 12:Loss curves of applying ERoPE. We apply ERoPE by extending along different optional dimensions,height,width, andtiming. For comparison, we showcase the loss curve of ablation setting that applies the same RoPE on both masked video and foreground video, which is marked inpink.",
                "position": 1366
            },
            {
                "img": "https://arxiv.org/html/2509.02460/x13.png",
                "caption": "Figure 13:Architectures of ablation settings. We visualize architecture details of the two ablation settings, “w/o BPBranch” (upper) and “w/o fusion block” (lower). The former deletes the control branch, and the latter uses cross-attention to inject foreground condition instead of the proposed fusion block. Their control signal conversion is the same as full model. So we omit it and mainly showcase model designs.",
                "position": 1369
            }
        ]
    },
    {
        "header": "11Loss Curve of Applying ERoPE",
        "images": []
    },
    {
        "header": "12Ablation Architectures",
        "images": []
    },
    {
        "header": "13Interactivity",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02460/x14.png",
                "caption": "Figure 14:Generating with different user-provided trajectories. We apply two different trajectories to the same foreground-background video pairs. We can see that, given the same foreground and background videos with different trajectories, GenCompositor could generate different contents that strictly follow their corresponding trajectories.",
                "position": 1400
            },
            {
                "img": "https://arxiv.org/html/2509.02460/x15.png",
                "caption": "Figure 15:Generating with different user-provided scale factors. Our method produces mask videos that follow the user-provided factors and control the size of added elements in the final results.",
                "position": 1403
            },
            {
                "img": "https://arxiv.org/html/2509.02460/x16.png",
                "caption": "Figure 16:More visual results of GenCompositor. Our method enables seamless video elements injection and user interaction. On the one hand, GenCompositor precisely injects foreground elements into the user-given position and harmonizes their color. On the other hand, the background environmental changes caused by the inserted objects (such as the shadows in the red box) are also automatically predicted by our generative model, proving the superiority of generative video compositing.",
                "position": 1406
            }
        ]
    },
    {
        "header": "14More Visual Results",
        "images": []
    }
]