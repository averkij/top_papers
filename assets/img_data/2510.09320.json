[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09320/x1.png",
                "caption": "Figure 1:CLIP and DINO exhibit complementary strengths: CLIP excels in capturing global semantic context, while DINO specializes in local spatial detail extraction. However, their fusion is hindered by inherent feature-level mismatches. Direct aggregation strategies like channel concatenation (Fig. 1(a)) result in suboptimal depth representations due to misaligned semantic and spatial features. In contrast, our approach (Fig. 1(b)) employs depth-aware language prompts as a granularity calibrator to align cross-level features into a unified depth hierarchy, ensuring semantic coherence and spatial precision.",
                "position": 96
            },
            {
                "img": "https://arxiv.org/html/2510.09320/x2.png",
                "caption": "Figure 2:The detailed pipeline of our proposed method. We first aggregate different-grained features from CLIP and DINO for coarse depth sensing under\ncontrastive language guidance (Fig. 2(b.1)), incorporating prior geometric knowledge (Fig. 2(a)).\nThese models are then optimized with the help of the auxiliary camera pose from PoseNet like existing methods to learn a fine depth estimation (Fig. 2(b.2)).\nIt also can be extended to improve the capture of continuous depth variations.\nBy equipping our method, existing self-supervised MDE methods (e.g., Monodepth2[27], ManyDepth[112], and Mono-VIFI[70]) achieve significant performance improvements (Fig. 2(c)).",
                "position": 99
            },
            {
                "img": "https://arxiv.org/html/2510.09320/x3.png",
                "caption": "Figure 3:Left: For the coarse depth sensing stage, we first aggregate the CLIP and DINO features and then design two contrastive learning strategies to endow them with coarse depth sensing capabilities by leveraging geometric priors from self-driving scenes.\nRight: During the fine depth estimation phrase, different from previous methods, Hybrid-depth  not only combines camera pose information from PoseNet but also conducts pixel-wise alignment with learnable depth prompts for hybrid-grained features to learn a fine depth estimation ability.",
                "position": 122
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Hybrid-depth",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09320/x4.png",
                "caption": "Figure 4:Patch selecting and feature concatenating.",
                "position": 263
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09320/x5.png",
                "caption": "Figure 5:Qualitative comparison with Manydepth[112], SQLDepth[109]and Monodepth2 with Hybrid-depth on the KITTI dataset. Monodepth2 with Hybrid-depth accurately predicts continuous depth in the ground region while preserving sharp object edges.",
                "position": 913
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]