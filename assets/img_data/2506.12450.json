[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.12450/x1.png",
                "caption": "Figure 1:We inspect the alignment in the middle layer representation of LLMs, allowing us to disentangle the language-specific and language-agnostic information. By exploiting this behavior, we are able to achieve Inference-Time Language Control (ITLC), alleviating the language confusion problem in LLMs.",
                "position": 235
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.12450/extracted/6522657/images/finding_1_labse_qwen_histogram.png",
                "caption": "Figure 2:Cross-lingual similarity across different layers in LaBSE and Qwen2.5-0.5B. LaBSE exhibits high cross-lingual similarity in its final layer, whereas Qwen2.5-0.5B shows this similarity in the middle layer. This difference suggests that the alignment of representations occurs at distinct positions within the two models.",
                "position": 284
            }
        ]
    },
    {
        "header": "3Understanding Representation Alignment in LLMs",
        "images": []
    },
    {
        "header": "4Inference-Time Language Control",
        "images": []
    },
    {
        "header": "5Implication ofInference-Time Language Control(ITLC)",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.12450/x2.png",
                "caption": "Figure 3:Language correctness (%) for Qwen2.5-0.5B across EN→→\\rightarrow→XX and XX→→\\rightarrow→EN Directions. The result of its instruct version is shown at AppendixD.3.",
                "position": 630
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethical Considerations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails of All Evaluation Datasets",
        "images": []
    },
    {
        "header": "Appendix BDetail Experiment for Understanding Representation Alignment in LLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.12450/extracted/6522657/images/finding_1_labse_mean_pooling_bar_chart.png",
                "caption": "(a)Mean Cosine Similarity Score on LaBSE Model",
                "position": 2177
            },
            {
                "img": "https://arxiv.org/html/2506.12450/extracted/6522657/images/finding_1_labse_mean_pooling_bar_chart.png",
                "caption": "(a)Mean Cosine Similarity Score on LaBSE Model",
                "position": 2180
            },
            {
                "img": "https://arxiv.org/html/2506.12450/extracted/6522657/images/finding_1_qwen_mean_pooling_bar_chart.png",
                "caption": "(b)Mean Cosine Similarity Score on Qwen2.5-0.5B Model",
                "position": 2185
            },
            {
                "img": "https://arxiv.org/html/2506.12450/extracted/6522657/images/finding_2_labse_bar_chart.png",
                "caption": "(a)Performance of LaBSE across downstream tasks compared to random baselines.",
                "position": 2406
            },
            {
                "img": "https://arxiv.org/html/2506.12450/extracted/6522657/images/finding_2_labse_bar_chart.png",
                "caption": "(a)Performance of LaBSE across downstream tasks compared to random baselines.",
                "position": 2409
            },
            {
                "img": "https://arxiv.org/html/2506.12450/extracted/6522657/images/finding_2_qwen_bar_chart.png",
                "caption": "(b)Performance of Qwen2.5-0.5B across downstream tasks compared to random baselines.",
                "position": 2414
            }
        ]
    },
    {
        "header": "Appendix CLID Methods and Results",
        "images": []
    },
    {
        "header": "Appendix DLanguage Control Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.12450/x3.png",
                "caption": "Figure 6:Controlling the number of language feature representations by using LDA performance accuracy (Left) and unused variance (Right) across number of components.",
                "position": 2938
            },
            {
                "img": "https://arxiv.org/html/2506.12450/x4.png",
                "caption": "",
                "position": 2946
            },
            {
                "img": "https://arxiv.org/html/2506.12450/x5.png",
                "caption": "Figure 7:Examples of generated outputs from Qwen2.5-0.5B-Instruct with injection in XX→→\\rightarrow→EN.",
                "position": 3660
            },
            {
                "img": "https://arxiv.org/html/2506.12450/x6.png",
                "caption": "Figure 8:Examples of generated outputs from Qwen2.5-0.5B-Instruct with injection in EN→→\\rightarrow→XX.",
                "position": 3663
            }
        ]
    },
    {
        "header": "Appendix ELanguage Confusion Result",
        "images": []
    },
    {
        "header": "Appendix FAnnotation Guidelines",
        "images": []
    }
]