[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.09755/x1.png",
                "caption": "Figure 1:Our learnings from scaling ViTok.We showcase our ViTok architecture (left) and key findings (right) from scaling auto-encoders for image and video reconstruction and generation. We enhance traditional CNN-based auto-encoders by integrating Vision Transformers (ViTs) with an upgraded Llama architecture into an asymmetric auto-encoder framework formingVision Transformer Tokenizeror ViTok. Visual inputs are embedded as patches or tubelets, processed by a compact Llama Encoder, and bottlenecked to create a latent code. The encoded representation is then upsampled and handled by a larger Llama Decoder to reconstruct the input. Color-coded text boxes highlight the effects of scaling the encoder, adjusting the bottleneck size, and expanding the decoder. Additionally, we discuss trade-offs in loss optimization and the modelâ€™s adaptability to video data. Our best performing ViTok variant achieves competitive performance with prior state-of-the-art tokenizers while reducing computational burden.",
                "position": 195
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Bottlenecks, Scaling, and Trade-offs in Visual Tokenization",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.09755/x2.png",
                "caption": "Figure 2:256p image reconstruction sweep over floating pointsEğ¸Eitalic_E.We evaluate ViTok S-B trained with stage 1 (Section2.3) using combinations of patch sizespâˆˆ8,16,32ğ‘81632p\\in{8,16,32}italic_p âˆˆ 8 , 16 , 32and channel widthscâˆˆ4,8,16,32,64ğ‘48163264c\\in{4,8,16,32,64}italic_c âˆˆ 4 , 8 , 16 , 32 , 64to investigate how the total floating pointsE=2562p2â‹…cğ¸â‹…superscript2562superscriptğ‘2ğ‘E=\\frac{256^{2}}{p^{2}}\\cdot citalic_E = divide start_ARG 256 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG â‹… italic_cinfluences FID, IS, SSIM, and PSNR in reconstruction tasks. Our findings reveal a strong correlation betweenlogâ¡(E)ğ¸\\log(E)roman_log ( italic_E )andlogâ¡(rFID)rFID\\log(\\text{rFID})roman_log ( rFID ),logâ¡(E)ğ¸\\log(E)roman_log ( italic_E )andrIS,logâ¡(E)ğ¸\\log(E)roman_log ( italic_E )andrSSIM, as well aslogâ¡(E)ğ¸\\log(E)roman_log ( italic_E )andrPSNR, independent of the number of FLOPs utilized by the auto-encoder. This indicates thatEğ¸Eitalic_Eis the primary bottleneck for reconstruction, irrespective of the code shape or FLOPs expended. Additionally, similar trends are observed across the ImageNet-1K and COCO datasets, indicating that these patterns are consistent regardless of the dataset used.",
                "position": 378
            },
            {
                "img": "https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/0_truth.png",
                "caption": "Figure 3:256p image reconstruction visualization over floating pointsEğ¸Eitalic_E.Example reconstructions for varying the number of floating pointsEğ¸Eitalic_Evalues on ViTok S-B/16, achieved by adjusting the channel sizec=64,32,16,8,4ğ‘64321684c={64,32,16,8,4}italic_c = 64 , 32 , 16 , 8 , 4for each image across the row. AsEğ¸Eitalic_Edecreases, high-frequency details diminish, with small colors and fine details gradually lost. WhenE<4096ğ¸4096E<4096italic_E < 4096, textures merge, and significant detail loss becomes apparent.",
                "position": 396
            },
            {
                "img": "https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/0_cw64.png",
                "caption": "",
                "position": 412
            },
            {
                "img": "https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/0_cw32.png",
                "caption": "",
                "position": 413
            },
            {
                "img": "https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/0_cw16.png",
                "caption": "",
                "position": 414
            },
            {
                "img": "https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/0_cw8.png",
                "caption": "",
                "position": 415
            },
            {
                "img": "https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/0_cw4.png",
                "caption": "",
                "position": 416
            },
            {
                "img": "https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/2_truth.png",
                "caption": "",
                "position": 419
            },
            {
                "img": "https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/2_cw64.png",
                "caption": "",
                "position": 420
            },
            {
                "img": "https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/2_cw32.png",
                "caption": "",
                "position": 421
            },
            {
                "img": "https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/2_cw16.png",
                "caption": "",
                "position": 422
            },
            {
                "img": "https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/2_cw8.png",
                "caption": "",
                "position": 423
            },
            {
                "img": "https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/2_cw4.png",
                "caption": "",
                "position": 424
            },
            {
                "img": "https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/3_truth.png",
                "caption": "",
                "position": 427
            },
            {
                "img": "https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/3_cw64.png",
                "caption": "",
                "position": 428
            },
            {
                "img": "https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/3_cw32.png",
                "caption": "",
                "position": 429
            },
            {
                "img": "https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/3_cw16.png",
                "caption": "",
                "position": 430
            },
            {
                "img": "https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/3_cw8.png",
                "caption": "",
                "position": 431
            },
            {
                "img": "https://arxiv.org/html/2501.09755/extracted/6134830/recon_processed/3_cw4.png",
                "caption": "",
                "position": 432
            },
            {
                "img": "https://arxiv.org/html/2501.09755/x3.png",
                "caption": "Figure 4:512p Image reconstruction overEğ¸Eitalic_E.We evaluate ViTok S-B trained with stage 1 (Section2.3) across all combinations of patch sizespâˆˆ8,16,32ğ‘81632p\\in{8,16,32}italic_p âˆˆ 8 , 16 , 32and a fixed channel widthc=16ğ‘16c=16italic_c = 16, analyzing how the total floating-point operations, calculated asE=5122p2â‹…cğ¸â‹…superscript5122superscriptğ‘2ğ‘E=\\frac{512^{2}}{p^{2}}\\cdot citalic_E = divide start_ARG 512 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG â‹… italic_c, influence reconstruction metrics such as FID, IS, SSIM, and PSNR.Eğ¸Eitalic_Eshows trends similar to 256p results (Figure2). However, achieving comparable rPSNR/rSSIM to 256p requires4Ã—E4ğ¸4\\times E4 Ã— italic_Efor 512p reconstruction, which indicates that compression ratio of pixels to channels should be fixed to maintain performance.",
                "position": 478
            },
            {
                "img": "https://arxiv.org/html/2501.09755/x4.png",
                "caption": "Figure 5:256p image generation overEğ¸Eitalic_E.We evaluate each tokenizer from Figure2on DiT following Section2.3. Results for CFG scales of 1.5 and 3.0 are on the left two and right two plots respectively. Our results show no strong linear correlation betweenlogâ¡(E)ğ¸\\log(E)roman_log ( italic_E )and generation performance. Instead, a second-order trend reveals an optimalEğ¸Eitalic_Efor each patch sizepğ‘pitalic_p, indicating a complex interplay betweenEğ¸Eitalic_Eandcğ‘citalic_c. This highlights the necessity of optimizing both parameters to balance reconstruction quality with generative capabilities.",
                "position": 498
            },
            {
                "img": "https://arxiv.org/html/2501.09755/x5.png",
                "caption": "Figure 6:Encoder scaling on 256p image reconstruction.We evaluate reconstruction metrics of ViTok trained with stage 1 (Section2.3) over model sizes S-S, B-S, S-B, B-B, B-L, L-L with fixedp=16,c=16,L=256,E=4096formulae-sequenceğ‘16formulae-sequenceğ‘16formulae-sequenceğ¿256ğ¸4096p=16,c=16,L=256,E=4096italic_p = 16 , italic_c = 16 , italic_L = 256 , italic_E = 4096. There is no correlation between encoder size and reconstruction performance indicating that scaling the encoder is unhelpful in improving reconstruction capabilities. This argues that visual encoding does not require much computation.",
                "position": 517
            },
            {
                "img": "https://arxiv.org/html/2501.09755/x6.png",
                "caption": "Figure 7:Decoder scaling on 256p image reconstruction.Using the results from Figure6, we plot various decoder sizes (S, B, L) over reconstruction performance. There is a strong correlation between decoder size and reconstruction performance, which indicates scaling the decoder improves reconstruction. Although, increasing the decoder size from Base to Large does not provide the same boost of performance as doublingEğ¸Eitalic_Eto8192819281928192from4096409640964096.",
                "position": 523
            },
            {
                "img": "https://arxiv.org/html/2501.09755/x7.png",
                "caption": "Figure 8:Encoder scaling on 256p image generation.We evaluate each tokenizer from Figure6on DiT following Section2.3. We plot encoder size over generation metric results for CFG scales of 1.5 and 3.0 on the left two and right two plots respectively. There is a weak negative correlation between encoder size and final performance indicating that scaling the encoder is harmful for generation results. This is coupled by the fact that increased encoder sizes make training slower due to increased computational overhead.",
                "position": 535
            },
            {
                "img": "https://arxiv.org/html/2501.09755/x8.png",
                "caption": "Figure 9:Decoder scaling on 256p image generation.Using the results from Figure6, we plot various decoder sizes (S, B, L) over generation performance. We plot decoder size over generation metric results for CFG scales of 1.5 and 3.0 on the left two and right two plots respectively. Unlike reconstruction, there is no clear correlation between decoder size and generation performance. This indicates that scaling the decoder has minimal benefits overall for auto-encoding.",
                "position": 541
            },
            {
                "img": "https://arxiv.org/html/2501.09755/x9.png",
                "caption": "Figure 10:Metric trade-offs in 256p image reconstruction.We train ViTok S-B/16 with stage 1 (Section2.3), varying the LPIPS (LP in figure) weightÎ»âˆˆ{0.0,0.5,1.0}ğœ†0.00.51.0\\lambda\\in\\{0.0,0.5,1.0\\}italic_Î» âˆˆ { 0.0 , 0.5 , 1.0 }and using either L1 or L2 MSE reconstruction loss (Equation1). Additionally, we finetune ViTok S-B/16 with stage 2 and include the result as L2+LP+GAN. The results indicate that enhancing rFID/rIS scores through increased perceptual and visual losses requires a trade-off with rSSIM/rPSNR, resulting in loss of information from the original image. This indicates the decoderâ€™s role as a generative component.",
                "position": 575
            },
            {
                "img": "https://arxiv.org/html/2501.09755/x10.png",
                "caption": "Figure 11:256p video reconstruction results overEğ¸Eitalic_E.We train ViTok S-B with stage 1 (Section2.3) on 16Ã—\\timesÃ—256Ã—\\timesÃ—256 videos at 8 fps, varying tubelet patch sizespâˆˆ{8,16,32}ğ‘81632p\\in\\{8,16,32\\}italic_p âˆˆ { 8 , 16 , 32 }and temporal stridesqâˆˆ{1,2,4,8}ğ‘1248q\\in\\{1,2,4,8\\}italic_q âˆˆ { 1 , 2 , 4 , 8 }with a channel sizec=16ğ‘16c=16italic_c = 16. Reconstruction performance is evaluated using rFID per frame, rFVD, rSSIM, and rPSNR on the Kinetics-700 validation, UCF101 training, and Shutterstock validation datasets. The results exhibit a similar trend to image reconstruction in Figure2, demonstrating a strong correlation betweenEğ¸Eitalic_Eand reconstruction performance. Expectantly, videos are more compressible than a direct scaling from images would suggest; instead of requiring 16Ã—\\timesÃ—Eğ¸Eitalic_E, achieving comparable rFID, rSSIM, and rPSNR to 256p image reconstruction only necessitates 4â€“8Ã—\\timesÃ—Eğ¸Eitalic_E.",
                "position": 614
            },
            {
                "img": "https://arxiv.org/html/2501.09755/x11.png",
                "caption": "Figure 12:56p video reconstruction results detailed overEğ¸Eitalic_E.We label patch and tubelet sizes from tokenizers trained in Figure11, we focus on just UCF-101 dataset due to its higher motion. For equivalentEğ¸Eitalic_E, lower temporal strides are slightly more effective for better results but overall there is little benefit in trading off temporal stride for patch size in ViTok for videos.Eğ¸Eitalic_Eis still the dominating factor in predicted reconstruction performance.",
                "position": 617
            },
            {
                "img": "https://arxiv.org/html/2501.09755/x12.png",
                "caption": "Figure 13:Multi-frame 256p video reconstruction.We train ViTok S-B/4x16 with stage 1 (Section2.3) on 16-, 32-, and 64-frame 256p videos and evaluate reconstruction metrics on the UCF-101 dataset. The results indicate that increasing the number of frames generally improves performance, demonstrating that ViTokÂ leverages higher redundancy in videos to achieve more efficient relative compression with same compression ratio or pixels per channelTÃ—HÃ—WÃ—3Eğ‘‡ğ»ğ‘Š3ğ¸\\frac{T\\times H\\times W\\times 3}{E}divide start_ARG italic_T Ã— italic_H Ã— italic_W Ã— 3 end_ARG start_ARG italic_E end_ARG.",
                "position": 620
            }
        ]
    },
    {
        "header": "4Experimental Comparison",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.09755/extracted/6134830/gen_viz_processed/cfg_main_256p.png",
                "caption": "Figure 14:256p image generation examples.We show randomly selected 256p image generation examples from our DiT-XL trained using the ViTok S-B/16 variant for 4 million steps at a batch size of 256. Images were sampled with 250 steps using the DDIM sampler and a CFG weight of 4.0.",
                "position": 664
            },
            {
                "img": "https://arxiv.org/html/2501.09755/extracted/6134830/gen_viz_processed/cfg_main_512p.png",
                "caption": "Figure 15:512p image generation examples.We show randomly selected 512p image generation examples from our DiT-XL trained using the ViTok S-B/16 variant for 4 million steps at a batch size of 256. Images were sampled with 250 steps using the DDIM sampler and a CFG weight of 4.0.",
                "position": 667
            },
            {
                "img": "https://arxiv.org/html/2501.09755/extracted/6134830/figs/gen_figure_videos_1024.png",
                "caption": "Figure 16:128p video generation examples.We show randomly selected 16Ã—\\timesÃ—128Ã—\\timesÃ—128 video generation examples from our DiT-L trained with ViTok S-B/4x8 variant. Videos are sampled with 250 steps and a CFG weight of 2.0.",
                "position": 670
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "8Extra Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.09755/x13.png",
                "caption": "Figure 17:256p Detailed Image Reconstruction Results with Fixed Architecture Size.We provide more details for the sweep in Figure2on the just the ImageNet-1K validation set. For1024â‰¤Eâ‰¤163841024ğ¸163841024\\leq E\\leq 163841024 â‰¤ italic_E â‰¤ 16384, where intersections ofEğ¸Eitalic_Eexist across patch sizes, we see very little variation in performance for fixedEğ¸Eitalic_E. This indicates thatEğ¸Eitalic_Eis the main bottleneck for visual auto-encoding and is not influence by increasing FLOPs.",
                "position": 2077
            },
            {
                "img": "https://arxiv.org/html/2501.09755/x14.png",
                "caption": "Figure 18:Finetuning the Decoder with a GAN.We study the effects of finetuning the decoder in ViTok S-B/16 on 256p images. We compare: (1) no GAN finetuning, (2) different discriminator learning rates, (3) an increased GAN loss weight (0.1), and (4) a full finetuning of all model parameters (including the encoder). The best results occur with a discriminator learning rate of2Ã—10âˆ’52superscript1052\\times 10^{-5}2 Ã— 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT, while higher rates cause instabilities. We also observe a clear shift toward more generative behavior: as the decoder gains better IS/FID, it sacrifices some SSIM/PSNR, reflecting its transition into a stronger generative component.",
                "position": 2090
            },
            {
                "img": "https://arxiv.org/html/2501.09755/x15.png",
                "caption": "Figure 19:256p Simple vs Latent ViTok Results.We implement a latent variant of ViTok S-B/16, withp=16ğ‘16p=16italic_p = 16andLâˆˆ{64,128,256,512,1024}ğ¿641282565121024L\\in\\{64,128,256,512,1024\\}italic_L âˆˆ { 64 , 128 , 256 , 512 , 1024 }latent tokens appended to the original patch embedding, then processed using full self-attention, and subsequently bottlenecked toc=16ğ‘16c=16italic_c = 16. Although this latent variant slightly underperforms the simpler version in rFID/rIS, it remains comparable overall and follows the same rules asEğ¸Eitalic_E. Consequently, it provides an alternative to Simple ViTok with greater control over the latent space.",
                "position": 2148
            },
            {
                "img": "https://arxiv.org/html/2501.09755/x16.png",
                "caption": "Figure 20:256p Adaptive Masking ViTok Results.We investigate variations of ViTok S-B/16 that apply token masking after encoding. We consider two approaches:Mask Simple, which masks the patch tokens following encoding, andMask Latent, which introduces latent tokens (like the architecture used for Figure19) and masks them. At stage 1 training time we randomly selected token lengths{32,64,128,256}3264128256\\{32,64,128,256\\}{ 32 , 64 , 128 , 256 }withc=16ğ‘16c=16italic_c = 16, then at inference evaluate each model on every token length and compare to the simple ViTok baseline at similarEğ¸Eitalic_E. While the masking variations underperform the simple variant, they still perform strongly.Mask Simpletends to perform better at higherEğ¸Eitalic_E, whileMask Latentachieves better results at lowerEğ¸Eitalic_E.",
                "position": 2166
            },
            {
                "img": "https://arxiv.org/html/2501.09755/extracted/6134830/figs/gen_figures_videos_512.png",
                "caption": "Figure 21:512 Token Video Generation Examples.We show randomly selected 16Ã—\\timesÃ—128Ã—\\timesÃ—128 video generation examples from our DiT-L trained at 512 tokens using the B-B/4x8 variant auto-encoder. Videos are sampled with 250 steps and a CFG weight of 2.0.",
                "position": 2195
            },
            {
                "img": "https://arxiv.org/html/2501.09755/extracted/6134830/figs/gen_figures_videos_256.png",
                "caption": "Figure 22:256 Token Video Generation Examples.We show randomly selected 16Ã—\\timesÃ—128Ã—\\timesÃ—128 video generation examples from our DiT-L trained at 256 tokens using the B-B/4x8 variant auto-encoder. Videos are sampled with 250 steps and a CFG weight of 2.0.",
                "position": 2198
            },
            {
                "img": "https://arxiv.org/html/2501.09755/extracted/6134830/gen_viz_processed/gen_8_cw4.png",
                "caption": "Figure 23:Channel size generation visualization 256p forp=8ğ‘8p=8italic_p = 8.We show example generations for various compression ratios on ViTok S-B/8 from Figure22. Herec=4ğ‘4c=4italic_c = 4has the best visuals that look close to good images, whilec=16ğ‘16c=16italic_c = 16generally looks good as well but not as good.c=64ğ‘64c=64italic_c = 64looks very poor and the images do not look realistic.",
                "position": 2208
            },
            {
                "img": "https://arxiv.org/html/2501.09755/extracted/6134830/gen_viz_processed/gen_8_cw16.png",
                "caption": "",
                "position": 2220
            },
            {
                "img": "https://arxiv.org/html/2501.09755/extracted/6134830/gen_viz_processed/gen_8_cw64.png",
                "caption": "",
                "position": 2226
            },
            {
                "img": "https://arxiv.org/html/2501.09755/extracted/6134830/gen_viz_processed/gen_16_cw4.png",
                "caption": "Figure 24:Channel size generation visualization 256p forp=16ğ‘16p=16italic_p = 16.We show example generations for various compression ratios on ViTok S-B/16 from Figure22. Herec=16ğ‘16c=16italic_c = 16has the best visuals that look close to good images, whilec=64ğ‘64c=64italic_c = 64suffers artifacts that worsen image quality.c=4ğ‘4c=4italic_c = 4suffers from poor reconstruction quality from the auto-encoder.",
                "position": 2232
            },
            {
                "img": "https://arxiv.org/html/2501.09755/extracted/6134830/gen_viz_processed/gen_16_cw16.png",
                "caption": "",
                "position": 2244
            },
            {
                "img": "https://arxiv.org/html/2501.09755/extracted/6134830/gen_viz_processed/gen_16_cw64.png",
                "caption": "",
                "position": 2250
            },
            {
                "img": "https://arxiv.org/html/2501.09755/extracted/6134830/gen_viz_processed/gen_32_cw4.png",
                "caption": "Figure 25:Channel size generation visualization 256p forp=32ğ‘32p=32italic_p = 32.We show example generations for various compression ratios on ViTok S-B/32 from Figure22. Herec=64ğ‘64c=64italic_c = 64has the best visuals overall but the high channel sizes make the image quality look poor and jumbled. Bothcâˆ’16ğ‘16c-16italic_c - 16andc=4ğ‘4c=4italic_c = 4suffers from poor reconstruction quality from the auto-encoder.",
                "position": 2256
            },
            {
                "img": "https://arxiv.org/html/2501.09755/extracted/6134830/gen_viz_processed/gen_32_cw16.png",
                "caption": "",
                "position": 2268
            },
            {
                "img": "https://arxiv.org/html/2501.09755/extracted/6134830/gen_viz_processed/gen_32_cw64.png",
                "caption": "",
                "position": 2274
            }
        ]
    },
    {
        "header": "9Visualizations",
        "images": []
    }
]