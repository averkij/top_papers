[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.07979/fig/files/viral_robot.png",
                "caption": "",
                "position": 106
            },
            {
                "img": "https://arxiv.org/html/2509.07979/x1.png",
                "caption": "Figure 1:VIsual Representation ALignment (VIRAL)preserves fine-grained visual attributes for multimodal reasoning. (a) VIRAL introduces an auxiliary regularization objective on the visual pathway to prevent MLLMs from discarding detailed attributes during training. (b) VIRAL, when trained with DINOv2(Oquab et al.,2023)as the vision foundation model (VFM), consistently produces more accurate visually grounded responses and achieves substantial improvements over standard baselines(Liu et al.,2023)across diverse vision encoders, including CLIP(Radford et al.,2021)and SigLIPv2(Tschannen et al.,2025).",
                "position": 128
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.07979/x2.png",
                "caption": "Figure 2:Re-injecting or aligning input visual features improves visual representation alignment and task performance.(a–d) Comparisons of (a) baseline visual instruction tuning(Liu et al.,2023), (b) re-injecting visual features from thepost-projection layer, (c) re-injecting from thepre-projection layer, and (d) the proposed visual representation alignment, all applied at the 16-th layer. (e) Layer-wise alignment between visual tokens in MLLMs and vision encoder features measured by CKNNA(Huh et al.,2024), with shaded regions highlighting middle layers that are especially important for visual understanding. (f) Benchmark performance corresponding to (a–d).",
                "position": 258
            },
            {
                "img": "https://arxiv.org/html/2509.07979/x3.png",
                "caption": "Figure 3:Illustration of VIRAL.Building upon our findings in visual representation alignment, we align visual pathway representation from MLLMs to strong, informative representations from VFMs to improve the vision understanding performance of MLLMs.",
                "position": 340
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.07979/x4.png",
                "caption": "Figure 4:Analysis of attention.Qualitative comparison on text-to-image attention maps (left) and quantified spatial entropy of attention across layers and heads (right). Applying visual representation alignment encourages model to attend to more contextually important content, yielding a more focused and structured attention pattern.",
                "position": 745
            },
            {
                "img": "https://arxiv.org/html/2509.07979/x5.png",
                "caption": "Figure 5:Qualitative comparison of baseline and VIRAL. The left part presents PCA visualizations of intermediate representations, demonstrating that VIRAL yields more structured, semantically meaningful visual embeddings. The right part illustrates instance counting and spatial relation tasks, highlighting scenarios where VIRAL correctly answers questions while the baseline fails.",
                "position": 936
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AAdditional implementation details",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.07979/x6.png",
                "caption": "Figure A1:Visualization of patch random permutation experiments.",
                "position": 1847
            },
            {
                "img": "https://arxiv.org/html/2509.07979/x7.png",
                "caption": "Figure A2:Layer-wise PCA visualizations of visual representations from (a) LLaVA-1.5-7B and (b) ours.",
                "position": 1887
            },
            {
                "img": "https://arxiv.org/html/2509.07979/x8.png",
                "caption": "Figure A3:PCA visualizations of 16-th layer visual representations aligned with different VFMs: CLIP, DINOv2, SAM, DAv2, and RADIO.",
                "position": 1890
            },
            {
                "img": "https://arxiv.org/html/2509.07979/x9.png",
                "caption": "Figure A4:Cross-attention map comparison for vision centric tasks.",
                "position": 1893
            }
        ]
    },
    {
        "header": "Appendix BAdditional Visualizations",
        "images": []
    }
]