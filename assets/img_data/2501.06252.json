[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.06252/x1.png",
                "caption": "Figure 1:Overview ofTransformer2superscriptTransformer2\\text{Transformer}^{2}Transformer start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT.In the training phase, we tune the scales of the singular values of the weight matrices to generate a set of ‚Äúexpert‚Äù vectors, each of which specializes in one type of tasks. In the inference phase, a two-pass process is adopted where the first applies the task-specific expert and the second generates the answer.",
                "position": 88
            }
        ]
    },
    {
        "header": "2Related works",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.06252/extracted/6116234/images/cem_code.png",
                "caption": "",
                "position": 218
            },
            {
                "img": "https://arxiv.org/html/2501.06252/x2.png",
                "caption": "Figure 2:Method overview.Left) At training time, we employ SVF and RL to learn the ‚Äúexpert‚Äù vectorszùëßzitalic_z‚Äôs that scale the singular values of the weight matrices.\nRight) At inference time, we propose three distinct methods to adaptively select/combine the learned expert vectors.",
                "position": 231
            },
            {
                "img": "https://arxiv.org/html/2501.06252/x3.png",
                "caption": "Figure 3:Prompt based adaptation.Self-adaptation prompt used byTransformer2superscriptTransformer2\\text{Transformer}^{2}Transformer start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTto classify the task prompt into pre-defined categories.",
                "position": 302
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.06252/x4.png",
                "caption": "Figure 4:SVF learning curves.The dashed lines indicate the performance ofLlama3-8B-Instructon the test split of each task. SVF effectively fine-tunes to surpass the base performance. While we use the best validation score to select our checkpoint for evaluation (marked by red dots), we present the entire training curve without early stopping to demonstrate SVF‚Äôs learning capabilities. Tasks with only hundreds of training samples like Coding and Reasoning were stopped early. In our experiments, we update the parameters at the end of each epoch.",
                "position": 343
            },
            {
                "img": "https://arxiv.org/html/2501.06252/x5.png",
                "caption": "Table 1:Fine-tuning results.LLM performance on the test splits of math, coding and reasoning. Normalized scores are in the parentheses.",
                "position": 350
            },
            {
                "img": "https://arxiv.org/html/2501.06252/x5.png",
                "caption": "Figure 5:Results for the VLM domain.",
                "position": 430
            },
            {
                "img": "https://arxiv.org/html/2501.06252/x6.png",
                "caption": "Figure 6:Confusion matrices.These matrices display the classification percentages, where rows represent the task classes (ground truth) and columns indicate the predicted categories. Some samples are misclassified as ‚ÄúOthers,‚Äù which is reflected in rows where the totals do not sum to one.",
                "position": 608
            },
            {
                "img": "https://arxiv.org/html/2501.06252/x7.png",
                "caption": "Figure 7:ùú∂ùíåsubscriptùú∂ùíå\\bm{\\alpha_{k}}bold_italic_Œ± start_POSTSUBSCRIPT bold_italic_k end_POSTSUBSCRIPTlearned weights.",
                "position": 737
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Author contributions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation details and hyper-parameters",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.06252/x8.png",
                "caption": "Figure 8:Sample problem and answer.Math data sample used for LoRA instruction fine-tuning, text in blue is the unmasked solution.",
                "position": 1352
            }
        ]
    },
    {
        "header": "Appendix BAdditional results",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.06252/x9.png",
                "caption": "Figure 9:Training LoRA with policy gradient.The dashed line shows the performance ofLlama3-8B-Instructon the test split. LoRA collapses at the beginning of the training stage and fails to recover, leading to negative effects on test performance. We swept a wide range of learning rates(2√ó10‚àí4,5√ó10‚àí4,‚Ä¶,2√ó10‚àí2,5√ó10‚àí2)2superscript1045superscript104‚Ä¶21025superscript102(2\\times 10^{-4},5\\times 10^{-4},\\dots,2\\times 10{-2},5\\times 10^{-2})( 2 √ó 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT , 5 √ó 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT , ‚Ä¶ , 2 √ó 10 - 2 , 5 √ó 10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT ), and all learning curves were similar to the one presented.",
                "position": 1665
            }
        ]
    },
    {
        "header": "Appendix CPCA on llama3 and mistral",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.06252/x10.png",
                "caption": "Figure 10:PCA ofLlama3-8B-Instruct.We show the ratio of the variance captured by the toprùëüritalic_rsingular components on the y-axis, and the layer indices on the x-axis. Except for the Query, Key and Value projection matrices, smallrùëüritalic_rvalues only capture a tiny fraction of variance in singular values in the parameter matrices.",
                "position": 1688
            },
            {
                "img": "https://arxiv.org/html/2501.06252/x11.png",
                "caption": "Figure 11:PCA ofMistral-7B-Instruct-v0.3.We show the ratio of the variance captured by the toprùëüritalic_rsingular components on the y-axis, and the layer indices on the x-axis. Except for the Query, Key and Value projection matrices, smallrùëüritalic_rvalues only capture a tiny fraction of variance in singular values in the parameter matrices.",
                "position": 1691
            }
        ]
    },
    {
        "header": "Appendix DEfficiency considerations and improvements",
        "images": []
    }
]