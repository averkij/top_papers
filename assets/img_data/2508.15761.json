[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15761/x1.png",
                "caption": "Figure 1:Left:Human evaluation win rates (GSB) ofWavercompared to Veo3, Kling2.0, and Wan2.1 on the Waver-bench 1.0 Text-to-Video (T2V) dataset across three dimensions: motion quality, visual quality, and prompt following. Waver-bench 1.0 covers a wide range of scenarios, including sports, daily activities, landscapes, animals, animations, and more.Right:Human evaluation win rates (GSB) on the Hermes Motion Testset across three dimensions: motion quality, visual quality, and prompt following. The Hermes Motion Testset encompasses complex motion scenarios such as tennis, basketball, gymnastics, and others. We can observe that, compared to general scenarios,Waverdemonstrates a more pronounced advantage in complex motion scenarios.",
                "position": 119
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x2.png",
                "caption": "Figure 2:T2V samples generated byWaver.Waveris capable of generating 1080p videos at arbitrary aspect ratios, delivering high levels of aesthetic quality, realism, and motion fidelity, while simultaneously supporting both T2V and I2V tasks.",
                "position": 177
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Model Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15761/x3.png",
                "caption": "Figure 3:Architecture of Task-Unified DiT.",
                "position": 226
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x4.png",
                "caption": "Figure 4:Comparison of 512p T2I results using different text encoders. For each case, image on theleftis only using flan-t5-xxl and that on therightis using flan-t5-xxl and Qwen2.5-32B-Instruct.",
                "position": 231
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x5.png",
                "caption": "Figure 5:Loss comparison between Hybrid Stream, Dual Stream, and Single Stream structures. Hybrid Stream’s loss converges faster.",
                "position": 259
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x6.png",
                "caption": "Figure 6:Pipeline of Cascade Refiner.",
                "position": 319
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x7.png",
                "caption": "Figure 7:Fig.(a) illustrates the Refiner’s output, where it has upscaled a 480p video from the first stage to 1080p. Besides improving the sharpness, it has also fixed the visual artifacts. Fig.(b) showcases the Refiner’s video editing ability by transforming the woman in the source video into a man. (zoom in for a better view.)",
                "position": 355
            }
        ]
    },
    {
        "header": "3Training Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15761/x8.png",
                "caption": "Figure 8:An overview of our proposed data processing pipeline. The process consists of five main stages: multi-source acquisition, pre-processing, quality filtering, captioning, and semantic balancing.",
                "position": 397
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x9.png",
                "caption": "(a)Video quality issue distribution in over 1M annotated clips used for training the quality model.",
                "position": 433
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x9.png",
                "caption": "(a)Video quality issue distribution in over 1M annotated clips used for training the quality model.",
                "position": 436
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x10.png",
                "caption": "(b)Action label distribution in the pre-training data, dominated by Social Interactions and Daily Activities.",
                "position": 441
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x11.png",
                "caption": "Figure 10:The hierarchical data filtering funnel, which progressively refines the dataset through increasingly strict quality criteria.",
                "position": 482
            }
        ]
    },
    {
        "header": "4Training / Inference Recipe",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15761/x12.png",
                "caption": "Figure 11:Qualitative comparison of generated videos with and without our representation alignment method. For each of the four cases shown, the frame on the left is from the baseline model (no alignment), while the frame on the right is from our model with the alignment constraint.",
                "position": 704
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x13.png",
                "caption": "Figure 12:Top:480p T2V results without 192p pretrain.Bottom:480p T2V results with 192p pretrain. Results for prompt “Two people are paddling two kayaks vigorously in the river.”",
                "position": 720
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x14.png",
                "caption": "Figure 13:Top:720p T2V results with logit-normal sampling.Bottom:720p T2V results with mode sampling. Results for prompt “In the center of the boxing ring, two male boxers are fighting each other.”",
                "position": 752
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x15.png",
                "caption": "Figure 14:Probability density functions for timestep sampling.While Text-to-Image uses a broad logit-normal distribution (Eq.1), Text-to-Video and Image-to-Video tasks employ a sharply peaked mode distribution (Eq.1) to generate larger motion amplitudes.",
                "position": 757
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x16.png",
                "caption": "Figure 15:Top:720p I2V results without T2V joint training.Bottom:720p I2V results with T2V joint training. Results for prompt “A man is skiing.”",
                "position": 770
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x17.png",
                "caption": "Figure 16:Raw training video clips and the foreground optical flow.Top:Foreground motion score is 11.0.Bottom:Foreground motion score is 0.75.",
                "position": 779
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x18.png",
                "caption": "Figure 17:Aesthetic comparison for a running scene.Top: Typical real-world video.Bottom: Our synthetic data shows superior visual quality, including better lighting and composition.",
                "position": 795
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x19.png",
                "caption": "(a)Manual review statistics for synthetic video samples (N=8381), where distortion is the primary failure reason",
                "position": 804
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x19.png",
                "caption": "(a)Manual review statistics for synthetic video samples (N=8381), where distortion is the primary failure reason",
                "position": 807
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x20.png",
                "caption": "(b)Human evaluation (SBS) results before and after high-aesthetic finetuning, which shows a  7% improvement in visual quality after finetuning on the curated synthetic data, with motion quality preserved.",
                "position": 814
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x21.png",
                "caption": "Figure 19:Comparison of four videos generated by models before and after high-aesthetic finetuning. For each case, the frame on theleftis before finetuning, and that on therightis after finetuning.",
                "position": 835
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x22.png",
                "caption": "Figure 20:Six different styles of videos generated byWaverT2V after prompt tagging.",
                "position": 855
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x23.png",
                "caption": "Figure 21:Top:720p T2V results with CFG=5.Bottom:720p T2V results with the optimal APG hyperparameters (CFG=8, norm threshold=27). Results for prompt “An off-road motorcyclist speeds along a dusty motocross track, with the tires kicking up dirt and gravel.”",
                "position": 864
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x24.png",
                "caption": "Figure 22:Human evaluation (SBS) results before and after model merging. Motion quality improves by 5% after merging, while visual quality and prompt following improves by 2% and 1%.",
                "position": 873
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x25.png",
                "caption": "Figure 23:Top:720p T2V results generated with the original prompt.Bottom:720p T2V results generated with the rewritten prompt.",
                "position": 891
            }
        ]
    },
    {
        "header": "5Infrastructure",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15761/x26.png",
                "caption": "Figure 24:Illustrative diagrams of Ulysses sequence parallelism, Selective Activation Checkpointing, and Activation Offloading. In the Activation Offloading diagram, “fop” denotes a forward operation, “bop” denotes a backward operation, “h2d” refers to a host-to-device memory copy, and “d2h” refers to a device-to-host memory copy.",
                "position": 928
            }
        ]
    },
    {
        "header": "6Benchmark Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15761/x27.png",
                "caption": "Figure 25:On the left and right are the official T2V and I2V ranking results from Artificial Analysis (as of 2025-08-05 12:00 (GMT+8)), withWaverranking third on both lists.",
                "position": 986
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x28.png",
                "caption": "Figure 26:Some video examples of the Waver-Bench 1.0 generated byWaver.",
                "position": 1006
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x29.png",
                "caption": "Figure 27:Some video examples of the Hermes Motion Testset generated byWaver.",
                "position": 1011
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x30.png",
                "caption": "(a)Evaluation on Waver-bench 1.0",
                "position": 1034
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x30.png",
                "caption": "(a)Evaluation on Waver-bench 1.0",
                "position": 1037
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x31.png",
                "caption": "(b)Evaluation on the Hermes Motion Testset",
                "position": 1043
            }
        ]
    },
    {
        "header": "7Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15761/x32.png",
                "caption": "Figure 29:An illustration of attention maps is presented, with the query on the vertical axis and the key on the horizontal axis. The visualization reveals three central characteristics: (1) Heterogeneity across attention heads, reflecting functional diversity among heads; (2) Layer-wise sparsity dynamics, showing how attention sparsity shifts across layers; and (3) Timestep-wise sparsity evolution, highlighting changes in attention patterns throughout the denoise process.",
                "position": 1062
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x33.png",
                "caption": "(a)An overview of our VAE and ShuffleDown/Up blocks.",
                "position": 1098
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x33.png",
                "caption": "(a)An overview of our VAE and ShuffleDown/Up blocks.",
                "position": 1101
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x34.png",
                "caption": "(b)Comparison of the text fidelity between Wan 2.1 VAE and our VAE on I2V generation task.",
                "position": 1108
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x35.png",
                "caption": "Figure 31:Comparison of generated video quality with different VAEs on the I2V task.",
                "position": 1178
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x36.png",
                "caption": "Figure 32:Comparison of the latent distributions and reconstructed images between CogVideoX VAE and its KL=0 variant.",
                "position": 1188
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x37.png",
                "caption": "Figure 33:Ablation study of video generation results using VAEs trained with different LPIPS losses.",
                "position": 1193
            },
            {
                "img": "https://arxiv.org/html/2508.15761/x38.png",
                "caption": "Figure 34:Top:The results of the T2V model trained using the Tarsier2 caption model.Bottom:The outcomes of the T2V model trained with our proposed caption model. In case (a), there is a noticeable distortion in the legs in the top row, which is absent in the bottom row. In case (b), the strongman repeatedly lifts and lowers the stone in the top row, whereas in the bottom row, the actions strictly follow the given instructions. In case (c), the character is depicted with three hands, and the knife held in one hand is incorrectly rendered as a fork in the top row. In contrast, the bottom row accurately shows the chef holding a knife in one hand and a fork in the other.",
                "position": 1214
            }
        ]
    },
    {
        "header": "8Conclusion and Limitation",
        "images": []
    },
    {
        "header": "9Contributions and Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]