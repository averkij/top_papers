[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.01342/x1.png",
                "caption": "Figure 1:Methods to augment MLLMs with fine-grained perception tasks. (a) Relying on task decoders[33,68], (b) Previous text-based methods represent boxes with location tokens[46,61]and represent masks with suboptimal polygons[66,61]or textual classes[61,34], (c) Ours: predicting open-ended text sequences while using a simple yet effective embedding retrieval approach to support masks.",
                "position": 99
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.01342/x2.png",
                "caption": "Figure 2:Overview of our approach. (a) Segmentation modeling: the mask token embedding retrieves similar image features to generate masks (shown with matching colors). (b) Upsampling masks by multiple mask tokens, retrieving more details by more tokens. We useNùëÅNitalic_N=2 to illustrate while usingNùëÅNitalic_N=4 in implementation. (c) We output open-ended text sequences with textual numbers for detection.",
                "position": 182
            }
        ]
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.01342/x3.png",
                "caption": "Figure 3:Multi-task data template examples. Red dots represent sampled grid point features, acting as local visual prompts for generating text sequences for nearby objects.",
                "position": 368
            }
        ]
    },
    {
        "header": "4Training",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.01342/x4.png",
                "caption": "Figure 4:Attention mask visualizations. (a) We apply bidirectional attention for image features. (b) For multi-prediction tasks, we mask each subsequence from seeing others.",
                "position": 2741
            }
        ]
    },
    {
        "header": "8Dataset Details",
        "images": []
    },
    {
        "header": "9Training Details",
        "images": []
    },
    {
        "header": "10Inference Speed",
        "images": []
    },
    {
        "header": "11More ablation studies",
        "images": []
    },
    {
        "header": "12Extended Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.01342/x5.png",
                "caption": "Figure 5:Visualizations of retinal vessel segmentation.",
                "position": 3386
            }
        ]
    },
    {
        "header": "13Discussions",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.01342/x6.png",
                "caption": "Figure 6:Failure case visualizations of UFO-InternVL2-8B on REC, RES and ReasonSeg respectively.",
                "position": 3501
            },
            {
                "img": "https://arxiv.org/html/2503.01342/x7.png",
                "caption": "Figure 7:Qualitative results of multi-task training. The first three rows correspond to object detection, instance segmentation, and semantic segmentation, while the last row shows results on captioning and referring expression comprehension.",
                "position": 3504
            },
            {
                "img": "https://arxiv.org/html/2503.01342/x8.png",
                "caption": "Figure 8:Qualitative results of Fine-grained Instruction Tuning. The three rows correspond to REC, RES, and reasoning segmentation in order.",
                "position": 3507
            },
            {
                "img": "https://arxiv.org/html/2503.01342/x9.png",
                "caption": "Figure 9:Visualization of multiple mask tokens. We illustrate with four mask tokens (withNùëÅNitalic_N=2). Employing multiple mask tokens allows for capturing finer details, such as the horse leg and the dog tail, resulting in more precise and refined masks.",
                "position": 3510
            },
            {
                "img": "https://arxiv.org/html/2503.01342/x10.png",
                "caption": "Figure 10:Instance segmentation results with different mask token number (N2superscriptùëÅ2N^{2}italic_N start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT) on UFO-ViT-Bsingle-tasksingle-task{}_{\\text{single-task}}start_FLOATSUBSCRIPT single-task end_FLOATSUBSCRIPT.",
                "position": 3513
            }
        ]
    },
    {
        "header": "14Visualization",
        "images": []
    }
]