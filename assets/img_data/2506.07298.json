[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07298/x1.png",
                "caption": "Figure 1:Overview of our study.We start by studying whether ICL using pre-trained LLMs can converge to theoretical optimum on HMM sequences (Q1, Section2), then study how HMMs properties affect the convergence rate/gap with theoretical conjectures (Q2, Section3), and finally we demonstrate how these findings translate to insights on real-world datasets for studying behaviors in science (Q3, Section4).",
                "position": 148
            }
        ]
    },
    {
        "header": "2Synthetic Experiments and ICL Convergence",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07298/x2.png",
                "caption": "Figure 2:Properties of HMMs.",
                "position": 200
            },
            {
                "img": "https://arxiv.org/html/2506.07298/x3.png",
                "caption": "Figure 3:(Left)We defineTaswhenLLM converges (see AppendixBfor computation metric), and𝜺𝜺\\boldsymbol{\\varepsilon}bold_italic_εas the final accuracygapat sequence length 2048.(Middle)Examples when LLM accuracy converges to Viterbi. Each curve represents a different HMM parameter setting. LLM ICL shows consistent convergence behavior.(Right)Examples of convergence in Hellinger distance (distance between two probability distributions). LLM ICL is not just “guessing” the most probable output, but converging distributionally.",
                "position": 289
            }
        ]
    },
    {
        "header": "3Impact of HMM Properties on Convergence",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07298/x4.png",
                "caption": "Figure 4:(Left)Convergence gapε𝜀\\varepsilonitalic_εincreases with higher mixing rate (slower mixing) and higher entropy. This plot is showing results averaged across all HMM configurations we tested.(Right)Slower mixing (λ2=0.5,0.75subscript𝜆20.50.75\\lambda_{2}=0.5,0.75italic_λ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.5 , 0.75) shows delayed convergence compared to(Middle)fast mixing (λ2=0.95,0.99subscript𝜆20.950.99\\lambda_{2}=0.95,0.99italic_λ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.95 , 0.99) at similar entropy levels.",
                "position": 309
            },
            {
                "img": "https://arxiv.org/html/2506.07298/x5.png",
                "caption": "Figure 5:HMM parametersM=8𝑀8M=8italic_M = 8,L=8𝐿8L=8italic_L = 8,H⁢(𝐀)=1.5𝐻𝐀1.5H({\\mathbf{A}})=1.5italic_H ( bold_A ) = 1.5,H⁢(𝐁)=1𝐻𝐁1H({\\mathbf{B}})=1italic_H ( bold_B ) = 1.(Left)The gap betweenP⁢(Ot+1|Ot)𝑃conditionalsubscript𝑂𝑡1subscript𝑂𝑡P(O_{t+1}|O_{t})italic_P ( italic_O start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT | italic_O start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )and Viterbi is small when mixing is fast.(Middle)Accuracy comparison with baselines.(Right)Hellinger distance measures distance between two probability distributions.",
                "position": 312
            }
        ]
    },
    {
        "header": "4Guidelines for Practitioners: How to (creatively) use LLMs for your data?",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07298/x6.png",
                "caption": "Figure 6:IBL dataset mice decision-making task.(Left)GLM-HMM model developed by neuroscientists.(Middle)A cartoon illustration of the task. A mouse observes a visual stimulus presented on one side of a screen, with one of six possible intensity levels. It then chooses a side, receiving a water reward if the choice matches the stimulus location.(Right)LLM ICL performance curve averaged across all animals, with 1-σ𝜎\\sigmaitalic_σerror bar. Its prediction accuracy steadily increase with longer context window, exceeding the domain-specific model performance.",
                "position": 460
            },
            {
                "img": "https://arxiv.org/html/2506.07298/x7.png",
                "caption": "Figure 7:Rat reward-learning task.(Left)Analog agent learning to HMMs.(Middle)A cartoon illustration of the more challenging task. No stimulus is presented on either side; instead, the reward probabilities for left and right choices evolve independently via random walks. As the optimal choice changes over time, the rat must learn and adapt its decisions based solely on the history of past rewards.(Right)LLM ICL performance curve averaged across all animals, with 1-σ𝜎\\sigmaitalic_σerror bar. Its performance curve improves only marginally with increasing context length.",
                "position": 472
            }
        ]
    },
    {
        "header": "5Related Works",
        "images": []
    },
    {
        "header": "6Discussion & Takeaways",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Table of Contents",
        "images": []
    },
    {
        "header": "Appendix AAdditional Background on HMMs",
        "images": []
    },
    {
        "header": "Appendix BAdditional Details of Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07298/x8.png",
                "caption": "Figure 8:The singular value decomposition of ergodic unichain Markov matrix𝐀𝐀{\\mathbf{A}}bold_A. The darker shaded region is pre-defined for our controlled experiments. The lighter shaded region is randomly initialized and calculated using a neural network.",
                "position": 1448
            }
        ]
    },
    {
        "header": "Appendix CDetails of Benchmark Models",
        "images": []
    },
    {
        "header": "Appendix DAdditional Synthetic Experiment Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07298/x9.png",
                "caption": "Figure 9:Accuracies of six methods across different𝐀𝐀{\\mathbf{A}}bold_Aentropy,𝐁𝐁{\\mathbf{B}}bold_Bentropy, number of states, and number of emissions withλ2=0.75subscript𝜆20.75\\lambda_{2}=0.75italic_λ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.75and uniform steady state distribution.",
                "position": 1867
            },
            {
                "img": "https://arxiv.org/html/2506.07298/x10.png",
                "caption": "Figure 10:Hellinger distances of six methods across different𝐀𝐀{\\mathbf{A}}bold_Aentropy,𝐁𝐁{\\mathbf{B}}bold_Bentropy, number of states, and number of emissions withλ2=0.75subscript𝜆20.75\\lambda_{2}=0.75italic_λ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.75and uniform steady state distribution.",
                "position": 1870
            },
            {
                "img": "https://arxiv.org/html/2506.07298/x11.png",
                "caption": "Figure 11:Accuracies of six methods across different mixing rates (λ2subscript𝜆2\\lambda_{2}italic_λ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT),𝐁𝐁{\\mathbf{B}}bold_Bentropy, number of states, and number of emissions with uniform steady state distribution and (1, 2, 3)𝐀𝐀{\\mathbf{A}}bold_Aentropy for (4, 8, 16) states respectively.",
                "position": 1882
            },
            {
                "img": "https://arxiv.org/html/2506.07298/x12.png",
                "caption": "Figure 12:Hellinger distances of six methods across different mixing rates (λ2subscript𝜆2\\lambda_{2}italic_λ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT),𝐁𝐁{\\mathbf{B}}bold_Bentropy, number of states, and number of emissions with uniform steady state distribution and (1, 2, 3)𝐀𝐀{\\mathbf{A}}bold_Aentropy for (4, 8, 16) states respectively.",
                "position": 1885
            },
            {
                "img": "https://arxiv.org/html/2506.07298/x13.png",
                "caption": "Figure 13:Accuracies of six methods across different steady state distributions,𝐁𝐁{\\mathbf{B}}bold_Bentropy, number of states, and number of emissions with (0, 0.5, 2)𝐀𝐀{\\mathbf{A}}bold_Aentropy for (4, 8, 16) states respectively and (0.99, 0.95, 0.75)λ2subscript𝜆2\\lambda_{2}italic_λ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTfor (4, 8, 16) states respectively.",
                "position": 1897
            },
            {
                "img": "https://arxiv.org/html/2506.07298/x14.png",
                "caption": "Figure 14:Hellinger distances of six methods across different steady state distributions,𝐁𝐁{\\mathbf{B}}bold_Bentropy, number of states, and number of emissions with (0, 0.5, 2)𝐀𝐀{\\mathbf{A}}bold_Aentropy for (4, 8, 16) states respectively and (0.99, 0.95, 0.75)λ2subscript𝜆2\\lambda_{2}italic_λ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTfor (4, 8, 16) states respectively.",
                "position": 1900
            }
        ]
    },
    {
        "header": "Appendix EAblations on LLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07298/x15.png",
                "caption": "Figure 15:Accuracies of seven models across different𝐀𝐀{\\mathbf{A}}bold_Aentropy,𝐁𝐁{\\mathbf{B}}bold_Bentropy, number of states, and number of emissions withλ2=0.75subscript𝜆20.75\\lambda_{2}=0.75italic_λ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.75and uniform steady state distribution. Lighter color represents smaller models. The two smallest models from each family have suboptimal performance.",
                "position": 1939
            },
            {
                "img": "https://arxiv.org/html/2506.07298/x16.png",
                "caption": "Figure 16:Hellinger distances of seven models across different𝐀𝐀{\\mathbf{A}}bold_Aentropy,𝐁𝐁{\\mathbf{B}}bold_Bentropy, number of states, and number of emissions withλ2=0.75subscript𝜆20.75\\lambda_{2}=0.75italic_λ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.75and uniform steady state distribution. The models converge similarly, especially when entropy is high.",
                "position": 1942
            },
            {
                "img": "https://arxiv.org/html/2506.07298/x17.png",
                "caption": "Figure 17:Accuracies of seven models across different mixing rates (λ2subscript𝜆2\\lambda_{2}italic_λ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT),𝐁𝐁{\\mathbf{B}}bold_Bentropy, number of states, and number of emissions with uniform steady state distribution and (1, 2, 3)𝐀𝐀{\\mathbf{A}}bold_Aentropy for (4, 8, 16) states respectively. The two smallest models from each family have suboptimal performance, especially when mixing is fast.",
                "position": 1945
            },
            {
                "img": "https://arxiv.org/html/2506.07298/x18.png",
                "caption": "Figure 18:Hellinger distances of seven models across different mixing rates (λ2subscript𝜆2\\lambda_{2}italic_λ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT),𝐁𝐁{\\mathbf{B}}bold_Bentropy, number of states, and number of emissions with uniform steady state distribution and (1, 2, 3)𝐀𝐀{\\mathbf{A}}bold_Aentropy for (4, 8, 16) states respectively. The models converge similarly, especially when mixing is slow.",
                "position": 1948
            },
            {
                "img": "https://arxiv.org/html/2506.07298/x19.png",
                "caption": "Figure 19:Accuracies of seven models across different steady state distributions,𝐁𝐁{\\mathbf{B}}bold_Bentropy, number of states, and number of emissions with (0, 0.5, 2)𝐀𝐀{\\mathbf{A}}bold_Aentropy for (4, 8, 16) states respectively and (0.99, 0.95, 0.75)λ2subscript𝜆2\\lambda_{2}italic_λ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTfor (4, 8, 16) states respectively. The poor performance observed in smaller models at short context length under low𝐀𝐀{\\mathbf{A}}bold_A&𝐁𝐁{\\mathbf{B}}bold_Bentropy settings may be attributed to the filtering of repeated n-grams during pretraining, as discussed in AppendixE.2.",
                "position": 1951
            },
            {
                "img": "https://arxiv.org/html/2506.07298/x20.png",
                "caption": "Figure 20:Hellinger distances of seven models across different steady state distributions,𝐁𝐁{\\mathbf{B}}bold_Bentropy, number of states, and number of emissions with (0, 0.5, 2)𝐀𝐀{\\mathbf{A}}bold_Aentropy for (4, 8, 16) states respectively and (0.99, 0.95, 0.75)λ2subscript𝜆2\\lambda_{2}italic_λ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTfor (4, 8, 16) states respectively.",
                "position": 1954
            },
            {
                "img": "https://arxiv.org/html/2506.07298/x21.png",
                "caption": "Figure 21:Accuracy of three tokenization methods across different mixing rates (λ2subscript𝜆2\\lambda_{2}italic_λ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT),𝐀𝐀{\\mathbf{A}}bold_Aentropy, and steady states with 4 states, 4 emissions, and 1 for𝐁𝐁{\\mathbf{B}}bold_Bentropy.",
                "position": 1969
            },
            {
                "img": "https://arxiv.org/html/2506.07298/x22.png",
                "caption": "Figure 22:Hellinger distance of three tokenization methods across different mixing rates (λ2subscript𝜆2\\lambda_{2}italic_λ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT),𝐀𝐀{\\mathbf{A}}bold_Aentropy, and steady states with 4 states, 4 emissions, and 1 for𝐁𝐁{\\mathbf{B}}bold_Bentropy.",
                "position": 1972
            }
        ]
    },
    {
        "header": "Appendix FSpectral Learning HMMs for Prediction Task",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07298/extracted/6523884/figure/ibl_results.png",
                "caption": "Figure 23:LLM in-context learning prediction accuracy for mice decision-making task with varying types of information in the observed sequences. Each line is averaged over 7 mice, with 1-σ𝜎\\sigmaitalic_σerror bar. The model we use is Qwen2.5-7B.",
                "position": 2994
            }
        ]
    },
    {
        "header": "Appendix GAdditional Real World Experiments",
        "images": []
    }
]