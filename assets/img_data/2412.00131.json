[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.00131/x1.png",
                "caption": "Figure 1:The model architecture of the Open-Sora Plan consists of a VAE, a Diffusion Transformer, and conditional encoders. The conditional injection encoders enable precise manipulation of individual frames (whether it‚Äôs the first frame, a subset of frames, or all frames) using designated structural signals, such as images, canny edges, depth maps, and sketches.",
                "position": 118
            }
        ]
    },
    {
        "header": "2Core Models of Open-Sora Plan",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.00131/x2.png",
                "caption": "Figure 2:Overview of WF-VAE. WF-VAE[li2024wfvaeenhancingvideovae]consists of a backbone and a main energy path, with such a path injecting the main flow of video energy into the backbone through concatenations.",
                "position": 211
            },
            {
                "img": "https://arxiv.org/html/2412.00131/x3.png",
                "caption": "",
                "position": 275
            },
            {
                "img": "https://arxiv.org/html/2412.00131/x4.png",
                "caption": "Figure 3:Overview of the Joint Image-Video Skiparse Denoiser. The model learns the denoising process in a low-dimensional latent space, which is compressed from input videos via our Wavelet-Flow VAE. Text prompts and timesteps are injected into each Cross-DiT block layer equipped with 3D RoPE. Our Skiparse attention is applied to every layer except the first and last two layers.",
                "position": 299
            },
            {
                "img": "https://arxiv.org/html/2412.00131/x5.png",
                "caption": "Figure 4:Calculation process of Skiparse Attentionwith sparse ratiok=2ùëò2k=2italic_k = 2for example. In our Skiparse Attention operation, we alternately perform the Single Skip and the Group Skip operations, reducing the sequence length to1/k1ùëò1/k1 / italic_kcompared to the original size in each operation.",
                "position": 353
            },
            {
                "img": "https://arxiv.org/html/2412.00131/x6.png",
                "caption": "Figure 5:The interacted sequence scope of different attention mechanisms.Various attention mainly differ in the number and position of selected tokens during attention computations.",
                "position": 356
            },
            {
                "img": "https://arxiv.org/html/2412.00131/x7.png",
                "caption": "Figure 6:Overview of our Image Condition Controller.Our Controller unifies multiple image conditional tasks including image-to-video, video transition, and video continuation in one framework when giving masks are changed.",
                "position": 610
            },
            {
                "img": "https://arxiv.org/html/2412.00131/x8.png",
                "caption": "Figure 7:Overview of our Structure Condition Controller.The structure Controller contains two light components including an encoder that focuses on extracting a high-level representation from the structural signals and a projector that transforms such representation into injection features.\nFinally, we directly add obtained injection features to the pre-trained model for structure control.",
                "position": 613
            },
            {
                "img": "https://arxiv.org/html/2412.00131/x9.png",
                "caption": "",
                "position": 642
            }
        ]
    },
    {
        "header": "3Assistant Strategies",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.00131/x10.png",
                "caption": "",
                "position": 877
            },
            {
                "img": "https://arxiv.org/html/2412.00131/x11.png",
                "caption": "(a)",
                "position": 885
            },
            {
                "img": "https://arxiv.org/html/2412.00131/x11.png",
                "caption": "(a)",
                "position": 888
            },
            {
                "img": "https://arxiv.org/html/2412.00131/x12.png",
                "caption": "(b)",
                "position": 894
            },
            {
                "img": "https://arxiv.org/html/2412.00131/x13.png",
                "caption": "(c)",
                "position": 900
            },
            {
                "img": "https://arxiv.org/html/2412.00131/x14.png",
                "caption": "(d)",
                "position": 906
            },
            {
                "img": "https://arxiv.org/html/2412.00131/x15.png",
                "caption": "(e)",
                "position": 912
            },
            {
                "img": "https://arxiv.org/html/2412.00131/x16.png",
                "caption": "(f)",
                "position": 918
            },
            {
                "img": "https://arxiv.org/html/2412.00131/x17.png",
                "caption": "(g)",
                "position": 924
            },
            {
                "img": "https://arxiv.org/html/2412.00131/x18.png",
                "caption": "(h)",
                "position": 930
            }
        ]
    },
    {
        "header": "4Data Curation Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.00131/x19.png",
                "caption": "(a)",
                "position": 1371
            },
            {
                "img": "https://arxiv.org/html/2412.00131/x19.png",
                "caption": "(a)",
                "position": 1374
            },
            {
                "img": "https://arxiv.org/html/2412.00131/x20.png",
                "caption": "(b)",
                "position": 1380
            }
        ]
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.00131/x21.png",
                "caption": "Figure 10:Our structure controller can generate high-quality videos conditioned by specified structural signals corresponding to arbitrary frames.",
                "position": 1689
            },
            {
                "img": "https://arxiv.org/html/2412.00131/x22.png",
                "caption": "",
                "position": 1705
            }
        ]
    },
    {
        "header": "6Limitation and Future Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Contributors and Acknowledgements",
        "images": []
    }
]