[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.13591/x1.png",
                "caption": "Figure 1:Overall performance of all models on DSAEval.",
                "position": 119
            }
        ]
    },
    {
        "header": "2Background and Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.13591/x2.png",
                "caption": "Figure 2:Overview of DSAEval.Left:In the Data Collection Pipeline, raw cases are cleaned and synthesized into Question, Reasoning, and Answer (QRA) pairs using advanced LLMs.Middle:The Data Agent Pipeline orchestrates the agent to solve tasks within a Sandbox Environment. The agent receives multimodal observations and produces a final report and a Jupyter notebook.Right:The Multi-Dimensional Evaluation module employs a Judge model to score the reasoning, code, and results against the soft ground truth, yielding a composite final score.",
                "position": 254
            },
            {
                "img": "https://arxiv.org/html/2601.13591/x3.png",
                "caption": "Figure 3:Distribution of the DSAEval benchmark. The suite covers diverse data modalities (left), problem domains (center), and task types (right), ensuring comprehensive evaluation coverage.",
                "position": 257
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.13591/x4.png",
                "caption": "Figure 4:Fine-grained Performance Analysis.Left:Performance by Domain shows robust capabilities in Data Analysis but significant weaknesses in Computer Vision and NLP.Right:Performance by Task Type highlights proficiency in Data Ingestion but bottlenecks in Statistical Inference and Model Evaluation.",
                "position": 352
            },
            {
                "img": "https://arxiv.org/html/2601.13591/x5.png",
                "caption": "Figure 5:Efficiency and Cost-Effectiveness Analysis.Left:Total Score vs. Average Tokens.Right:Total Score vs. Average Price Per Task. The closer to the top left corner, the better.",
                "position": 414
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Results",
        "images": []
    },
    {
        "header": "6Future Directions",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethical considerations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AData Sources",
        "images": []
    },
    {
        "header": "Appendix BDetail Prompt",
        "images": []
    },
    {
        "header": "Appendix CGenerated QRA Examples",
        "images": []
    },
    {
        "header": "Appendix DData Science Agent Execution Results",
        "images": []
    },
    {
        "header": "Task Request",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.13591/x6.png",
                "caption": "",
                "position": 2999
            }
        ]
    },
    {
        "header": "Appendix EData Science Agent Final Report",
        "images": []
    },
    {
        "header": "Appendix FEvaluation Process",
        "images": []
    },
    {
        "header": "Quantitative Metrics",
        "images": []
    },
    {
        "header": "Analysis",
        "images": []
    },
    {
        "header": "Quantitative Metrics",
        "images": []
    },
    {
        "header": "Analysis",
        "images": []
    },
    {
        "header": "Quantitative Metrics",
        "images": []
    },
    {
        "header": "Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.13591/x7.png",
                "caption": "Figure 6:Scores of Doubao-Seed-1.8 as judge model.",
                "position": 3336
            },
            {
                "img": "https://arxiv.org/html/2601.13591/x8.png",
                "caption": "Figure 7:Scores of Doubao-Seed-1.8 as judge model.",
                "position": 3339
            }
        ]
    },
    {
        "header": "Appendix GSelection of Judge Models",
        "images": []
    }
]