[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background and Motivation",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16894/x1.png",
                "caption": "Figure 1:The effect of initializations from different SVD segments(ui,σi,vi⊤)subscript𝑢𝑖subscript𝜎𝑖superscriptsubscript𝑣𝑖top(u_{i},\\sigma_{i},v_{i}^{\\top})( italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_σ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT )for rank 32 and 128. The performance normalized by min-max scaling.",
                "position": 311
            },
            {
                "img": "https://arxiv.org/html/2502.16894/x2.png",
                "caption": "Figure 2:SVD initializationvs.scalings𝑠sitalic_sand rankr𝑟ritalic_r",
                "position": 319
            },
            {
                "img": "https://arxiv.org/html/2502.16894/x3.png",
                "caption": "Figure 3:Illustration of Our Method.Single Low-Rank Adaptation: LoRA reduces trainable parameters by reparameterizingW𝑊Witalic_WasW=W0+s⁢B⁢A𝑊subscript𝑊0𝑠𝐵𝐴W=W_{0}+sBAitalic_W = italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + italic_s italic_B italic_A, withB𝐵Bitalic_BandA𝐴Aitalic_Aas low-rank matrices.MoE Fine-tuning: Full MoE fine-tuning, where expertsW1superscript𝑊1W^{1}italic_W start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPTandWEsuperscript𝑊𝐸W^{E}italic_W start_POSTSUPERSCRIPT italic_E end_POSTSUPERSCRIPTare selected by the router in this moment.Subfigure (I): Our method replaces the single pairB,A𝐵𝐴B,Aitalic_B , italic_Awith multiple pairs{Bi,Ai}i=1Esuperscriptsubscriptsuperscript𝐵𝑖superscript𝐴𝑖𝑖1𝐸\\{B^{i},A^{i}\\}_{i=1}^{E}{ italic_B start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , italic_A start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_E end_POSTSUPERSCRIPT, initialized from different segments of the SVD ofW0subscript𝑊0W_{0}italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTand adaptively selected by the router.Subfigure (II): We align optimization with SVD-structured MoE by separately aligning each expert.Wressubscript𝑊resW_{\\text{res}}italic_W start_POSTSUBSCRIPT res end_POSTSUBSCRIPTensures the equivalent weight equalsW0subscript𝑊0W_{0}italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTbefore optimization, and we scale each expert’s equivalent gradient to closely approximate full MoE fine-tuning.",
                "position": 402
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16894/x4.png",
                "caption": "Figure 4:Training loss curves of Different LoRA methods and Full Fine-tuning MoE on Cars. The balance loss is excluded in the MoE baselines for a fair comparison with single LoRA baselines.",
                "position": 1417
            },
            {
                "img": "https://arxiv.org/html/2502.16894/x5.png",
                "caption": "Figure 5:Performance of different methods across ranks.",
                "position": 1427
            },
            {
                "img": "https://arxiv.org/html/2502.16894/x6.png",
                "caption": "Figure 6:Performance vs. number of experts and activation ratio (total rank=32). “2 in 8” means activating 2 out of 8 experts.",
                "position": 1430
            },
            {
                "img": "https://arxiv.org/html/2502.16894/x7.png",
                "caption": "Figure 7:Expert Load Distribution across different tasks. We illustrate the fraction of tokens assigned to each expert{ei}i=18superscriptsubscriptsubscript𝑒𝑖𝑖18\\{e_{i}\\}_{i=1}^{8}{ italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 8 end_POSTSUPERSCRIPT",
                "position": 1453
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Impact Statements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProof related with PiSSA Select Segment",
        "images": []
    },
    {
        "header": "Appendix BLoad Balance Loss",
        "images": []
    },
    {
        "header": "Appendix CProof of Theoretical Results",
        "images": []
    },
    {
        "header": "Appendix DExtend Our Method to Scenarios with Proper Scaling",
        "images": []
    },
    {
        "header": "Appendix EExperiment Details",
        "images": []
    },
    {
        "header": "Appendix FParameter and FLOPs Analysis",
        "images": []
    }
]