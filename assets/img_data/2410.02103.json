[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02103/x1.png",
                "caption": "Figure 1:MVGSsupplements general improvements for novel view synthesis on top of GaussianSplatting(Kerbl et al.,2023)representations, as shown in (b) and (c). Extensive experiments are conducted to prove that our proposed method delivers consistent advantages in (d) in extremely challenging scenes with strong reflection, transparency, and fine-scale details against baseline methods.",
                "position": 82
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02103/x2.png",
                "caption": "Figure 2:Illustration of the previous single-view training paradigm and our proposedMVGS, where (a) describes NeRF cannot be optimized in a multi-view training way. (b) points out the original 3DGS following the single-view training strategy of NeRF. (c) The proposedMVGStransforms the original training protocol followed by 3DGS and its variants. (d) The proposed cross-intrinsic guidance strategy enables multi-view training in a coarse-to-fine way. The bottom of this figure illustrates the pipeline of our proposedMVGS.",
                "position": 187
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02103/x3.png",
                "caption": "Figure 3:Qualitative comparisons of 3DGS(Kerbl et al.,2023), Scaffold-GS(Lu et al.,2024)and their improved version integrating our method across various datasets.We useredclose-up patches to highlight the visual differences for clearer visibility. We can observe that our proposed method can improve the original 3DGS and Scaffold-GS for extremely challenging scenes with strongly changed lighting effects, powerful reflection, and fine details.",
                "position": 499
            },
            {
                "img": "https://arxiv.org/html/2410.02103/x4.png",
                "caption": "Figure 4:Qualitative results of 3DGS-DR(Ye et al.,2024), 4DGS(Wu et al.,2024)and their improved version by integrating our method across various challenging datasets.It can be observed that 3DGS-DR and 4DGS integrated with our method can achieve better results for extremely challenging senses with strong reflection and dynamic changes.",
                "position": 705
            },
            {
                "img": "https://arxiv.org/html/2410.02103/x5.png",
                "caption": "Figure 5:Analysis of the multi-view training settings.We improve four representative state-of-the-art Gaussian-based methods with the proposed multi-view regulated training. We report results on three representative datasets.",
                "position": 824
            },
            {
                "img": "https://arxiv.org/html/2410.02103/x6.png",
                "caption": "Figure 6:Visualization comparisons of the ablation of the proposed components.We employ 3DGS as our baseline and improve it by gradually integrating our proposed components into it. It can be observed our method gradually improves the novel view synthesis performance of the baseline.",
                "position": 829
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Ablation Studies",
        "images": []
    },
    {
        "header": "Appendix BTraining with More Iterations",
        "images": []
    },
    {
        "header": "Appendix CAdditional Results on 3D Reconstruction",
        "images": []
    },
    {
        "header": "Appendix DExtra Comparisons on Reflective Object Reconstruction",
        "images": []
    },
    {
        "header": "Appendix EAdditional Quantitative and Qualitative Results on 4D Reconstruction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02103/x7.png",
                "caption": "Figure 7:Study on the effect of additional training iterations.We leverage state-of-the-art 3DGS(Kerbl et al.,2023)as our baseline and conduct experiments on three representative datasets, such as Mip-NeRF 360(Barron et al.,2022), Shiny Blender(Verbin et al.,2022), and Tanks&Temples(Knapitsch et al.,2017).",
                "position": 1622
            },
            {
                "img": "https://arxiv.org/html/2410.02103/x8.png",
                "caption": "Figure 8:Additional qualitative comparisons of general object reconstruction. We compare 3DGS(Kerbl et al.,2023)and Scaffold-GS(Lu et al.,2024)with their improved version by integrating our method across various datasets.We employredclose-up patches to highlight the visual differences for better differentiation. It can be observed that our proposed method can improve the original 3DGS and Scaffold-GS for challenging scenes.",
                "position": 1913
            },
            {
                "img": "https://arxiv.org/html/2410.02103/x9.png",
                "caption": "Figure 9:Additional qualitative results of 3DGS-DR(Ye et al.,2024)and our enhanced version across diverse reflective object datasets.We can find that 3DGS-DR enhanced by our proposed method can be more robust for challenging scenes with reflection effects to obtain better reflective reconstruction performance.",
                "position": 2242
            },
            {
                "img": "https://arxiv.org/html/2410.02103/x10.png",
                "caption": "Figure 10:Qualitative comparisons of HDR reconstruction and normal reconstruction by 3DGS-DR(Ye et al.,2024)and our proposed method.The better performance of HDR and normal reconstruction means better reflective object reconstruction performance.",
                "position": 2245
            },
            {
                "img": "https://arxiv.org/html/2410.02103/x11.png",
                "caption": "Figure 11:Additional visualization comparisons of 4DGS(Wu et al.,2024)and its improved version integrating with our method for 4D reconstruction.It can be found that our proposed method can enhance 4DGS to reconstruct finer dynamic details and obtain better performance.",
                "position": 2479
            },
            {
                "img": "https://arxiv.org/html/2410.02103/x12.png",
                "caption": "Figure 12:Qualitative comparisons of multi-scale scene reconstruction on BungeeNeRF dataset(Xiangli et al.,2022). We compare Octree-GS(Ren et al.,2024)and its improved version by integrating our method.We utilizeredclose-up patches to stand out the visual differences for clear comparisons. We can find that our proposed method can improve the original Octree-GS for challenging multi-scale scenes.",
                "position": 2482
            }
        ]
    },
    {
        "header": "Appendix FVisualization of Multi-scale Scene Reconstruction",
        "images": []
    }
]