[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.14460/x1.png",
                "caption": "Figure 1:Comparison of workflows, agentic workflows, and autonomous agents.\nWorkflows rely on human-designed routing or planning, while agentic workflows (e.g., ReAct) introduce iterative reasoning–acting loops. Fully autonomous agents remove predefined workflows and interact with the environment proactively through an end-to-end action–feedback cycle.",
                "position": 91
            }
        ]
    },
    {
        "header": "2From Large Language Models to Agents: An MDP Perspective",
        "images": []
    },
    {
        "header": "3Agent-R1 Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.14460/x2.png",
                "caption": "Figure 2:Illustration of the Agent-R1 training trajectory.\nThe agent performs multi-turn reasoning and tool-based actions during rollout, receives environment feedback, and appends tool responses to form the next state. This trajectory—containing thinking steps, actions, and feedback—serves as the basis for reinforcement learning updates in Agent-R1.",
                "position": 368
            },
            {
                "img": "https://arxiv.org/html/2511.14460/x3.png",
                "caption": "Figure 3:Flow diagram of Single-Turn RL and Multi-Turn RL(Agent-R1) in generation stage.",
                "position": 375
            },
            {
                "img": "https://arxiv.org/html/2511.14460/x4.png",
                "caption": "Figure 4:Flow diagram of Single-Turn RL and Multi-Turn RL(Agent-R1) in learning stage.",
                "position": 378
            }
        ]
    },
    {
        "header": "4Empirical Study",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    }
]