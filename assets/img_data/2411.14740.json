[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14740/x1.png",
                "caption": "Figure 1.3D meshes with textures generated by our method.We show a gallery of 3D meshes with textures generated by our method (left) and the texture map and multi-view renderings of the bird model (right).\nOur approach models the distribution of mesh textures at high resolution, generating high-quality textures from text and image prompts, more multi-view renderings are shown infig.5.",
                "position": 88
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": []
    },
    {
        "header": "2.Related Work",
        "images": []
    },
    {
        "header": "3.Overview",
        "images": []
    },
    {
        "header": "4.Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14740/x2.png",
                "caption": "Figure 2.An illustration of (a) a mesh with its (b) UV map.Three islandsS1subscriptùëÜ1S_{1}italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT,S2subscriptùëÜ2S_{2}italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTandS3subscriptùëÜ3S_{3}italic_S start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPTare shown both on the mesh surface and its flattened UV map, where continuous islandsS1subscriptùëÜ1S_{1}italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTandS2subscriptùëÜ2S_{2}italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTare positioned far apart on the UV map while disconnected islandsS1subscriptùëÜ1S_{1}italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTandS3subscriptùëÜ3S_{3}italic_S start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPTshow closer distance on the UV map.",
                "position": 302
            },
            {
                "img": "https://arxiv.org/html/2411.14740/x3.png",
                "caption": "Figure 3.An overview of TEXGen.(a). An overview of our training pipeline. We train a diffusion model to\ngenerate high-resolution texture maps for a given meshSùëÜSitalic_Sbased on a single-view imageIùêºIitalic_Iand text descriptions by learning to denoise from a noise texture mapxtsubscriptùë•ùë°x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. The core of our denoising network is our proposed hybrid 2D-3D block. (b). The structure of a single hybrid block. (c)-(d). The detailed designs of our UV head block and point block.",
                "position": 306
            },
            {
                "img": "https://arxiv.org/html/2411.14740/x4.png",
                "caption": "Figure 4.An illustration of the feature learning procedure in 3D space.In panel (a), we start with rasterized dense point features, which we sparsify using grid-pooling to create sparse point features shown in (b). Different pools are indicated by various colors in (a). These points are then serialized to determine their order for subsequent group-based self-attention, as part of the learning process shown in (d). In (c), we visualize different groups formed based on Hilbert serialization, where each color signifies a distinct group. Finally, the processed features are scattered back to their original coordinates, providing the output dense point features.",
                "position": 310
            },
            {
                "img": "https://arxiv.org/html/2411.14740/x5.png",
                "caption": "Figure 5.Texture generation results.For given meshes, our method can synthesize highly detailed textures conditioned on guided single-view images and text prompts. We show three novel view images from our textured results and representative zoom-in regions from the textured mesh. The generated full texture maps are also shown.",
                "position": 463
            },
            {
                "img": "https://arxiv.org/html/2411.14740/x6.png",
                "caption": "Figure 6.Comparison with state-of-the-art methods.We compare our method with four representative state-of-the-art methods. Our model can synthesize more detailed and coherent textures compared to these methods which rely on test-time optimization using a 2D pretrained text-to-image diffusion model. Also, our method trained on the 3D dataset and 3D representation avoids the Janus problem that commonly occurs in other methods.",
                "position": 466
            },
            {
                "img": "https://arxiv.org/html/2411.14740/x7.png",
                "caption": "Figure 7.An indoor scene with all meshes textured by TEXGen.We generate a single view using text-conditioned ControlNet with depth control for each mesh and paint them with both the text and single view prompt with TEXGen.",
                "position": 469
            }
        ]
    },
    {
        "header": "5.Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14740/x8.png",
                "caption": "Figure 8.TEXGen as a texture inpainter.We demonstrate the potential of TEXGen as a texture inpainter. We showcase here (a) randomly masked texture maps and (b) the inpainted texture maps, with unknown regions rendered as black.",
                "position": 538
            },
            {
                "img": "https://arxiv.org/html/2411.14740/x9.png",
                "caption": "Figure 9.Texture completion from sparse views.With sparse views of objects provided (front and back views as shown in (a)), unprojected textures retain many unseen regions (b). TEXGen effectively fills these unseen regions with harmonious textures (c).",
                "position": 551
            },
            {
                "img": "https://arxiv.org/html/2411.14740/x10.png",
                "caption": "Figure 10.Qualitative ablation results on the hybrid design.Compared to the full model A (a), the model B (b) with only UV blocks can not easily capture overall semantic and 3D consistency while that the model C (c) with only point blocks struggles with producing high-frequency patterns.",
                "position": 605
            }
        ]
    },
    {
        "header": "6.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14740/x11.png",
                "caption": "Figure 11.Results on 3D avatars.Our model, trained on 3D data, adeptly avoids the Janus problem.",
                "position": 2002
            },
            {
                "img": "https://arxiv.org/html/2411.14740/x12.png",
                "caption": "Figure 12.Results on real-scan models.Our method is robust to real-scan models with non-smooth surfaces and fragmented UV maps.",
                "position": 2005
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]