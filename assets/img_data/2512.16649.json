[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16649/x1.png",
                "caption": "",
                "position": 111
            },
            {
                "img": "https://arxiv.org/html/2512.16649/figures/github-logo.png",
                "caption": "",
                "position": 112
            },
            {
                "img": "https://arxiv.org/html/2512.16649/figures/fig1_aime24_curves_added.png",
                "caption": "Figure 1:JustRL achieves substantial performance gains through simple, single-stage training. (a) The AIME24 (avg@32) performance curve for scaling from DeepSeek-R1-Distill-Qwen-1.5B into JustRL-DeepSeek-1.5B, from 28% to 58% over 4,000 steps; (b) from OpenMath-Nemotron-1.5B into our 1.5B reasoning SOTA model JustRL-Nemotron-1.5B, showing its training journey to the final 70+% score over 3,000 steps.",
                "position": 130
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3JustRL: Simplicity at Scale",
        "images": []
    },
    {
        "header": "4Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16649/figures/fig2_training_dynamics.png",
                "caption": "Figure 2:Training Dynamics of JustRL-DeepSeek-1.5B. (a) Policy entropy remains stable throughout training, oscillating naturally around 1.2-1.4 without drift or collapse. (b) Mean reward shows smooth, monotonic improvement from negative to∼\\sim0.4, indicating consistent learning without plateau-breaking interventions. (c) Response length naturally converges from initial verbosity (∼\\sim7,000 tokens) to a stable range (4,000-5,000 tokens) with 16k max context length, without explicit length penalties.",
                "position": 896
            },
            {
                "img": "https://arxiv.org/html/2512.16649/figures/fig3_training_dynamics.png",
                "caption": "Figure 3:Ablation Study Results. (a) AIME 2024 performance diverges after∼\\sim2,000 steps. Our base recipe reaches 55%, while adding overlong penalty plateaus at 50%, and adding both modifications plateaus at 45%. (b) Entropy: Both modifications show collapsed exploration (entropy∼\\sim0.5-0.6) compared to healthy oscillation in the base recipe (∼\\sim1.2-1.4).",
                "position": 950
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    }
]