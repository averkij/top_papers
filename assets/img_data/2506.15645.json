[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Are VLMs Robust to Visual Quality?",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.15645/x1.png",
                "caption": "Figure 1:Visualizaion of the relative attention of LLaVA-v1.5-7B on a image-question pair (image 1301) of the ScienceQA dataset. Lighter regions indicate higher attention weights, while darker regions represent lower attention weights.",
                "position": 629
            },
            {
                "img": "https://arxiv.org/html/2506.15645/extracted/6549962/pics/entropy_plot.png",
                "caption": "Figure 2:Changes in relative attention entropy of LLaVA-v1.5-7B on ScienceQA dataset across different levels of image degradation.",
                "position": 644
            },
            {
                "img": "https://arxiv.org/html/2506.15645/extracted/6549962/pics/img180.png",
                "caption": "Figure 3:The logit lens of LLaVA-v1.5-7B model with a image-question pair (image 180) of the ScienceQA dataset. We present the next token distribution of the 50th image token for each layer in the heatmap. Lighter regions indicate higher probability, while darker regions represent lower probability.",
                "position": 650
            },
            {
                "img": "https://arxiv.org/html/2506.15645/extracted/6549962/pics/original_image_logit_lens_heatmap.png",
                "caption": "",
                "position": 653
            },
            {
                "img": "https://arxiv.org/html/2506.15645/extracted/6549962/pics/corrupted_image_logit_lens_heatmap.png",
                "caption": "",
                "position": 654
            }
        ]
    },
    {
        "header": "4How to Modulate Optimal Visual Quality for VLMs?",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.15645/x2.png",
                "caption": "Figure 4:Illustration of VQ-TTT framework.",
                "position": 708
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails of the Utilized Restoration Models",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experiments and Results",
        "images": []
    },
    {
        "header": "Appendix CExamples of the Input Images with Degradations and Restorations",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.15645/x3.png",
                "caption": "Figure 5:Examples of the images with degradations and restorations.",
                "position": 3084
            }
        ]
    },
    {
        "header": "Appendix DBroader Impacts",
        "images": []
    },
    {
        "header": "Appendix ELimitations",
        "images": []
    }
]