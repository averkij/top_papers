[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20499/x1.png",
                "caption": "Figure 1:Weak context utilization in the multi-head attention of existing methods, e.g., Diffusion Forcing(Chen et al.,2024), Self Forcing(Huang et al.,2025). Naively pruning all KV caches of 25% heads results in only a marginal performance drop (84.0 vs. 83.78) while speedup inference from 17.6FPS to 19.6FPS.",
                "position": 95
            },
            {
                "img": "https://arxiv.org/html/2601.20499/x2.png",
                "caption": "Figure 2:The proposed Dummy Forcing can be applied to (1) efficiently generate videos, (2) overcome quadratic complexity in high-resolution video generation, and (3) enlarge context lengths without increasing computational overhead.",
                "position": 98
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20499/x3.png",
                "caption": "Figure 3:We compute the frame attention score by summing across rows and averaging across columns within the sink/neighbor/current frame group.",
                "position": 170
            },
            {
                "img": "https://arxiv.org/html/2601.20499/x4.png",
                "caption": "Figure 4:(a)-(c): We gather attention maps from all heads and useLABEL:eq:attn_calto compute the frame attention scores on the sink/neighbor/current frames. (d): The core set ratio under different conditions. For each bar, we change the corresponding condition while keeping the others fixed. We provide more implementation details of the observation experiment in theAppendix.",
                "position": 202
            },
            {
                "img": "https://arxiv.org/html/2601.20499/x5.png",
                "caption": "Figure 5:(a) We assign different contextual receptive fields to different head types. (b) A toy example of classifying different head types, with num_head=8 andNN=4 in this case. (c) We fuse different heads by context packing for more aggressive compression.",
                "position": 217
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20499/x6.png",
                "caption": "Figure 6:Quantitative comparison on30s long video generation. Our Dummy Forcing achieves1.4×1.4\\timesend-to-end acceleration without compromising output quality. More results are provided in theAppendix.",
                "position": 661
            },
            {
                "img": "https://arxiv.org/html/2601.20499/x7.png",
                "caption": "Figure 7:Quantitative comparison onlong-context video generation. We design a set of text prompts with a storytelling narrative that involves two scene transitions, where each shot is interactively generated through the corresponding prompt. See more results inAppendix.",
                "position": 773
            },
            {
                "img": "https://arxiv.org/html/2601.20499/x8.png",
                "caption": "Figure 8:Ablation experiments on Vbench scores and average runtime per AR step across different numbers of dummy heads.",
                "position": 891
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20499/x9.png",
                "caption": "Figure 9:Distribution of the number of neighbor heads and dummy heads across different layers.",
                "position": 907
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProof of Optimality for Greedy Strategy",
        "images": []
    },
    {
        "header": "Appendix BDummy Head in Other Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20499/x10.png",
                "caption": "Figure 10:Frame attention scores on the current frame in the CausVid model(Yin et al.,2025). We present the distribution across different AR steps and denoising timesteps.",
                "position": 1738
            },
            {
                "img": "https://arxiv.org/html/2601.20499/x11.png",
                "caption": "Figure 11:Frame attention scores on the current frame in the Rolling Forcing model(Liu et al.,2025). We present the distribution across different AR steps and denoising timesteps.",
                "position": 1741
            }
        ]
    },
    {
        "header": "Appendix CMore Implementation Details",
        "images": []
    },
    {
        "header": "Appendix DMore Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20499/x12.png",
                "caption": "Figure 12:Runtime profiling on a single self-attention layer under different context lengths.H​WHWdenotes the total number of visual tokens of one latent frame. The attention runtime is averaged across all layers of the model.",
                "position": 1853
            },
            {
                "img": "https://arxiv.org/html/2601.20499/x13.png",
                "caption": "Figure 13:Attention maps visualization of sink, neighbor, and dummy heads. Zoom in for better effects.",
                "position": 1925
            }
        ]
    },
    {
        "header": "Appendix EDetails of Long-context Generation",
        "images": []
    },
    {
        "header": "Appendix FLimitation and Future Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20499/x14.png",
                "caption": "Figure 15:Quantitative comparison between CausVid(Yin et al.,2025)and our Dummy Forcing. We prune 50% heads’ KV cache to serve as dummy heads in the proposed method.",
                "position": 2133
            },
            {
                "img": "https://arxiv.org/html/2601.20499/x15.png",
                "caption": "Figure 16:Quantitative comparison between Rolling Forcing(Liu et al.,2025)and our Dummy Forcing. We prune 50% heads’ KV cache to serve as dummy heads in the proposed method.",
                "position": 2136
            },
            {
                "img": "https://arxiv.org/html/2601.20499/x16.png",
                "caption": "Figure 17:More quantitative comparison results inlong-context video generationusing context probing (Part1).",
                "position": 2140
            },
            {
                "img": "https://arxiv.org/html/2601.20499/x17.png",
                "caption": "Figure 18:More quantitative comparison results inlong-context video generationusing context probing (Part2).",
                "position": 2144
            },
            {
                "img": "https://arxiv.org/html/2601.20499/x18.png",
                "caption": "Figure 19:More quantitative comparison results in5s short video generationtask (Part1).",
                "position": 2148
            },
            {
                "img": "https://arxiv.org/html/2601.20499/x19.png",
                "caption": "Figure 20:More quantitative comparison results in5s short video generationtask (Part2).",
                "position": 2152
            },
            {
                "img": "https://arxiv.org/html/2601.20499/x20.png",
                "caption": "Figure 21:More quantitative comparison results in30s long video generationtask (Part1).",
                "position": 2156
            },
            {
                "img": "https://arxiv.org/html/2601.20499/x21.png",
                "caption": "Figure 22:More quantitative comparison results in30s long video generationtask (Part2).",
                "position": 2160
            }
        ]
    },
    {
        "header": "Appendix GMore Generation Results",
        "images": []
    }
]