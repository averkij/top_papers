[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16166/figures/evovla_logo.png",
                "caption": "",
                "position": 97
            },
            {
                "img": "https://arxiv.org/html/2511.16166/x1.png",
                "caption": "",
                "position": 115
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16166/x2.png",
                "caption": "Figure 2:EvoVLA Data Engine. Aligned with Discoverse-L and the video-driven stage discovery pipeline to close the data–reward–policy loop.",
                "position": 173
            },
            {
                "img": "https://arxiv.org/html/2511.16166/x3.png",
                "caption": "Figure 3:EvoVLA overview.Built on OpenVLA-OFT backbone, EvoVLA integrates three modules: Stage-Aligned Reward (SAR) with hard negatives and temporal smoothing, Pose-Based Object Exploration (POE) via world models, and Long-Horizon Memory with context selection and gated fusion. The framework couples with Discoverse-L for training and deploys to real robots.",
                "position": 180
            }
        ]
    },
    {
        "header": "3The Proposed Method",
        "images": []
    },
    {
        "header": "4Discoverse-L Benchmark",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16166/x4.png",
                "caption": "Figure 4:Simulation and real-world rollouts.Columns depict temporal progression, rows pair the same task family across domains. Left: dual-camera Discoverse-L rollouts for Stack, Bridge, Jujube-Cup; right: AIRBOT-Play deployments for Stack, Bridge, Insert. The alignment highlights consistent gripper–object interactions and viewpoints after Sim2Real transfer.",
                "position": 435
            },
            {
                "img": "https://arxiv.org/html/2511.16166/x5.png",
                "caption": "Figure 5:Stack task qualitative comparison.Columns (1–5) cover sub-task 1 (left block onto middle) then sub-task 2 (right block on top).Top:OpenVLA-OFT (cross markers) opens before contact, dithers, misaligns, and drops the block.Bottom:EvoVLA (check marks) delays opening until contact, aligns within a few corrections, and leaves a stable stack, matching the hallucination reductions in Section5.3.",
                "position": 571
            },
            {
                "img": "https://arxiv.org/html/2511.16166/x6.png",
                "caption": "Figure 6:Real-world evaluation.Green arrows: absolute gains over OpenVLA-OFT (+9.2–+13.4 points). Blue labels: relative gains (+18.4%–+32.1%). Horizontal dashed lines mark model averages (37.7%, 43.6%, 54.6%); the vertical dashed line splits Sim2Real transfers (left) from the on-robot Insert task (right).",
                "position": 585
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Limitations and Discussion",
        "images": []
    },
    {
        "header": "8Reproducibility Statement",
        "images": []
    },
    {
        "header": "9Real-World Detailed Results",
        "images": []
    },
    {
        "header": "10Additional Rollout Visualizations",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16166/x7.png",
                "caption": "Figure 7:Supplementary Discoverse-L rollouts. Each row shows a full episode for place, bridge, and stack with dual third-person/wrist views; columns denote temporal progression.",
                "position": 1531
            },
            {
                "img": "https://arxiv.org/html/2511.16166/x8.png",
                "caption": "Figure 8:Supplementary AIRBOT-Play rollouts. Each row shows stack, bridge, and insert executions with synchronized third-person and wrist cameras in the real world.",
                "position": 1534
            }
        ]
    },
    {
        "header": "11Problem Formulation",
        "images": []
    },
    {
        "header": "12Benchmark Technical Details",
        "images": []
    },
    {
        "header": "13Hyperparameter Sensitivity Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16166/x9.png",
                "caption": "Figure 9:CLIP threshold sensitivity for EvoVLA (full model) averaged across three Discoverse-L tasks and three seeds. (a) Success Rate remains stable within±\\pm0.5 points aroundθ=0.7\\theta=0.7(68.7–69.2%). (b) Hallucination Rate decreases monotonically with higher thresholds;θ=0.7\\theta=0.7balances false-positive and false-negative rates while maintaining peak success. Results demonstrate that our choice is robust to threshold variations within [0.65, 0.75].",
                "position": 1604
            },
            {
                "img": "https://arxiv.org/html/2511.16166/x10.png",
                "caption": "Figure 10:Intrinsic weightρ\\rhosensitivity.Performance withρ∈{0.3,0.6,0.9}\\rho\\in\\{0.3,0.6,0.9\\}(averaged across 3 tasks, 3 seeds). (a) Success Rate: stable within±2.5%\\pm 2.5\\%, peaking atρ=0.6\\rho=0.6(69.2%). (b) Training efficiency:ρ=0.6\\rho=0.6achieves fastest convergence to 50% success. Results confirm robustness to intrinsic weight selection.",
                "position": 1614
            }
        ]
    },
    {
        "header": "14Complete Hyperparameter Table",
        "images": []
    },
    {
        "header": "15Video-driven Stage Discovery: Real Example",
        "images": []
    },
    {
        "header": "16Additional Implementation Details",
        "images": []
    },
    {
        "header": "17Network Architecture Details",
        "images": []
    },
    {
        "header": "18Computational Cost",
        "images": []
    }
]