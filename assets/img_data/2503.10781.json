[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10781/x1.png",
                "caption": "",
                "position": 106
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10781/x2.png",
                "caption": "Table 1:Comparison of our two datasets iGround and HowToGround1M with state-of-the-art video grounding datasets.",
                "position": 136
            },
            {
                "img": "https://arxiv.org/html/2503.10781/x2.png",
                "caption": "Figure 2:A method for automatic annotation of spatio-temporally grounded captions.In the first stage (left), we apply a still-image grounded caption generation model on individual video frames producing temporally inconsistent outputs. In the second stage (middle), the captions from individual frames are aggregated into a single video-level caption describing the most salient actions/objects in the video. Third (right), individual frame-level phrases and bounding boxes are associated over time into a temporally consistent and dense labelling of object bounding boxes over the video.",
                "position": 261
            }
        ]
    },
    {
        "header": "3Large-scale generation of grounded captions",
        "images": []
    },
    {
        "header": "4The GROVE Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10781/x3.png",
                "caption": "Figure 3:An overview of our GROVE model for grounded video caption generation.Dashed red rectangles outline the key technical contributions enabling grounded caption generation in video and include: (i) spatio-temporal adapters; (ii) the bounding box decoder and (iii) the temporal objectness head.",
                "position": 325
            }
        ]
    },
    {
        "header": "5Manually annotated iGround dataset",
        "images": []
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10781/x4.png",
                "caption": "Figure 4:Qualitative examples showing predictions of our GROVE model.Please note that GROVE is able to: (i) produce video-level natural language captions describing the main action in the video; (ii) ground multiple objects; and (iii) produce spatio-temporally consistent bounding box predictions.\nPlease note that the second row shows an example of model prediction that is partly incorrect as the blue box, while temporally consistent, does not depict a “yarn”.More qualitative results including video results are in the supp. mat.",
                "position": 617
            },
            {
                "img": "https://arxiv.org/html/2503.10781/x5.png",
                "caption": "Figure 5:Results after pre-training (PT) vs. after fine-tuning and pre-training (PT+FT) as a function of the pre-training dataset size. Results are reported on the iGround validation set.",
                "position": 625
            },
            {
                "img": "https://arxiv.org/html/2503.10781/x6.png",
                "caption": "Table 5:Ablation of spatio-temporal adapters (AD) and unfreezing the bounding box decoder and projection layers (unfreeze). We report results on the iGround validation set.",
                "position": 641
            },
            {
                "img": "https://arxiv.org/html/2503.10781/x6.png",
                "caption": "Table 6:Benefits of temporal objectness.AP50 (left) and recall (right) of our model for different temporal objectness thresholds. Results are reported on the iGround validation set.",
                "position": 720
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Supplementary material",
        "images": []
    },
    {
        "header": "Appendix AAdditional qualitative results",
        "images": []
    },
    {
        "header": "Appendix BDataset Statistics",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10781/x7.png",
                "caption": "(a)",
                "position": 1515
            },
            {
                "img": "https://arxiv.org/html/2503.10781/x7.png",
                "caption": "(a)",
                "position": 1518
            },
            {
                "img": "https://arxiv.org/html/2503.10781/x8.png",
                "caption": "(b)",
                "position": 1524
            },
            {
                "img": "https://arxiv.org/html/2503.10781/x9.png",
                "caption": "Figure 7:Qualitative results of our GROVE model on the (unseen) iGround test set.\nThe colour-coded sentence fragments are spatio-temporally localised in the video with the bounding boxes colour coded with the same colour. The results demonstrate that: (i) our model can localise even small objects such as a pen or a tooth brush; (ii) objects are consistently labelled across frames despite changes of viewpoint or scale; (iii) the model focuses on the human and the interacted objects; (iv) the model can successfully ground multiple objects in the video.Additional results are shown in the supplementary video.",
                "position": 1531
            },
            {
                "img": "https://arxiv.org/html/2503.10781/x10.png",
                "caption": "Figure 8:Additional qualitative results of our GROVE model on the (unseen) iGround test set.\nThe colour-coded sentence fragments are spatio-temporally localised in the video with the bounding boxes colour coded with the same colour. In addition to the model’s properties discussed in Fig.7, GROVE is capable of predicting whether an object is present in a certain frame via the temporal objectness head; in the second example there are no bounding box predictions for the hand in the first three frames while in the fourth example there are no predictions for the hand and the screwdriver in the second and fifth frame.Additional results are shown in the supplementary video.",
                "position": 1537
            }
        ]
    },
    {
        "header": "Appendix CDetails of the GROVE model",
        "images": []
    },
    {
        "header": "Appendix DDetails of the automatic annotation method",
        "images": []
    },
    {
        "header": "Appendix EProtocol for human annotations",
        "images": []
    },
    {
        "header": "Appendix FPrompts for automatic\ncuration of spatio-temporally grounded captions",
        "images": []
    }
]