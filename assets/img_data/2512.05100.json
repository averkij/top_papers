[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05100/x1.png",
                "caption": "Figure 1:A structured document translation example (English→\\rightarrowJapanese), with markup highlighted in color. The lower part shows the translation of XML tree structure with nodeϕ​(⋅)\\phi(\\mathord{\\cdot}),tag​(⋅)\\text{tag}(\\mathord{\\cdot}), andtext​(⋅)\\text{text}(\\mathord{\\cdot})mappings.",
                "position": 201
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05100/x2.png",
                "caption": "Figure 2:OurFormatRLpipeline consists of two stages. First, we fine-tune a pre-trained LLM (e.g., Llama-3.1-8B-Instruct) using real and synthetic structured document pairs. Second, we reinforce the format handling ability through GRPO with our proposed format reward functions.",
                "position": 261
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Evaluation Metrics: StrucAUC",
        "images": []
    },
    {
        "header": "5Experimental Settings",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05100/x3.png",
                "caption": "Figure 3:Inline markup version of the example in Fig.1.",
                "position": 558
            }
        ]
    },
    {
        "header": "6Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05100/x4.png",
                "caption": "Figure 4:Comparison with GPT-4.1-nano (2025-04-14), GPT-4o-mini (2024-07-18), and GPT-4o (2024-08-06).",
                "position": 1001
            },
            {
                "img": "https://arxiv.org/html/2512.05100/x5.png",
                "caption": "Figure 5:Comparison with parse-and-assemble baselines, in which the LLM acts as sentence-level MT model with or without document context.",
                "position": 1024
            },
            {
                "img": "https://arxiv.org/html/2512.05100/x6.png",
                "caption": "Figure 6:Improvement ofFormatRLover SFT using various single rewards, and combinations of two rewards. Points represent mean improvement and ellipses visualize the local covariance directional structure between two metrics improvements.",
                "position": 1027
            },
            {
                "img": "https://arxiv.org/html/2512.05100/x7.png",
                "caption": "Figure 7:SFT Model performance comparison for English to Chinese translation by composition of training data.syn-ALTrefers to fine-tuning using the raw ALT document pairs.syn-0-shotrefers to data synthesized in a zero-shot manner. Forsyn-1-shot, the synthesizing LLM was provided with one example. The*-tagsetups additionally guided the LLM with example XML tags from the development set. TheXreal+Ysynsetups are a mixture of real data and synthetic data generated with the syn-1-shot-tag approach.",
                "position": 1176
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Limitations",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "Appendix ASynthetic Data Generation",
        "images": []
    },
    {
        "header": "Appendix BExample: Metrics Calculation",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05100/fig/evaluation_example.png",
                "caption": "Figure 9:Toy-example of a model translation and reference with markings for purely structural errors.XML-Validity: The translation can be successfully parsed into an XML and therefore achieves a score of 1.XML-Match: The translation does not match the exact structure of the reference and therefore scores 0.XML-Bleu: Since the translation XML tree does not match the one of the reference the node contents of the reference will be paired with empty translations - e.g. (\" \", \"In-App-Help\") - for corpusBleucomputation.StrucAUC: The score is computed as a corpus level area under curve based on the respectiveNode-chrFandOptimal Node-chrF.Node-chrF: The structural errors of the translation will lead to misalignment in the parallel depth-first traversal. For instance, we will see a pairing of […,(<conbody>,<prolog>), (<prolog>, <source>), (<source>, <conbody>)…] which overall results in a low node-level chrF score of 16.89.Optimal Node-chrF: With 3.5 edit operations (note that changing the label of the <concept> is considered half an edit), the nodes of the translation can be realigned to the reference, resulting in a Optimal Node-chrF of 52.92.",
                "position": 1289
            }
        ]
    },
    {
        "header": "Appendix CFull Results of Structured Document Setting",
        "images": []
    },
    {
        "header": "Appendix DFull Results of Inline Markup Setting",
        "images": []
    },
    {
        "header": "Appendix ESynthetic Data Performance: All Translation Directions",
        "images": []
    },
    {
        "header": "Appendix FComparison with GPT",
        "images": []
    },
    {
        "header": "Appendix GAblation: Reward Choice on Content-COMET and XML-Match",
        "images": []
    },
    {
        "header": "Appendix HDetails in StrucAUC",
        "images": []
    },
    {
        "header": "Appendix IDiscussion",
        "images": []
    },
    {
        "header": "Appendix JLicense",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05100/x8.png",
                "caption": "(a) enja",
                "position": 2218
            },
            {
                "img": "https://arxiv.org/html/2512.05100/x8.png",
                "caption": "(a) enja",
                "position": 2221
            },
            {
                "img": "https://arxiv.org/html/2512.05100/x9.png",
                "caption": "(b) jaen",
                "position": 2226
            },
            {
                "img": "https://arxiv.org/html/2512.05100/x10.png",
                "caption": "(c) enzh",
                "position": 2232
            },
            {
                "img": "https://arxiv.org/html/2512.05100/x11.png",
                "caption": "(d) zhen",
                "position": 2237
            },
            {
                "img": "https://arxiv.org/html/2512.05100/x12.png",
                "caption": "Figure 11:Comparison with GPT models across four language pairs.",
                "position": 2244
            },
            {
                "img": "https://arxiv.org/html/2512.05100/x13.png",
                "caption": "",
                "position": 2249
            },
            {
                "img": "https://arxiv.org/html/2512.05100/x14.png",
                "caption": "",
                "position": 2252
            },
            {
                "img": "https://arxiv.org/html/2512.05100/x15.png",
                "caption": "",
                "position": 2253
            },
            {
                "img": "https://arxiv.org/html/2512.05100/x16.png",
                "caption": "Figure 12:Improvement ofFormatRLover SFT using various single rewards, and combinations of two rewards. Points represent mean improvement and ellipses visualize the local covariance directional structure between two metrics improvements. Estimates are constructed from RL results based on 8 SFT checkpoints each.",
                "position": 2259
            }
        ]
    },
    {
        "header": "Appendix KThe Use of AI Assistants",
        "images": []
    }
]