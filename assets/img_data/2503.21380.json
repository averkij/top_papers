[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21380/x1.png",
                "caption": "Figure 1:Performance comparisons of mainstream reasoning models between our OlymMATH (English version) and other Olympiad-level mathematical benchmarks. OlymMATH-HARD emerges as the most challenging, with significantly higher difficulty than existing evaluation benchmarks.",
                "position": 94
            }
        ]
    },
    {
        "header": "2Benchmark Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21380/x2.png",
                "caption": "Figure 5: Example of the format used in OlymMATH.",
                "position": 368
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21380/x2.png",
                "caption": "Figure 6: The reasoning length distribution of DeepSeek-R1 in AIME 2024, AIME 2025, OlymMATH-EN-EASY and OlymMATH-EN-HARD. The vertical dashed line represents the mean.",
                "position": 860
            }
        ]
    },
    {
        "header": "4Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]