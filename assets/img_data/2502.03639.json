[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.03639/x1.png",
                "caption": "",
                "position": 90
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.03639/x2.png",
                "caption": "Figure 2:Training Pipeline Overview.We sample video-point pairs, concatenate them in channel dimensions and used to train a UNet. In addition to standard condition and latent cross attention, we further add cross attention between video and point in corresponding channels for a better alignment between the two modalities. Furthermore, the 3D information from the points is utilized to regularize the RGB video generation by applying a misalignment penalty to the video diffusion process.",
                "position": 119
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.03639/x3.png",
                "caption": "Figure 3:PointVid Dataset Generation Workflow. Given an input video, we use the first frame as a reference frame and perform semantic segmentation to obtain masks for foreground objects. Next, we randomly sample pixels with a distribution favoring pixels inside foreground objects. We perform 3D point tracking on these queried pixels, and map these points to the input video frames. The resulting data point contains 3D coordinates of tracked foreground pixels while remaining pixels are zeroed out.",
                "position": 184
            },
            {
                "img": "https://arxiv.org/html/2502.03639/x4.png",
                "caption": "Figure 4:Point Regularization.The reconstructed point cloud in the diffusion output often contains noise and deformations (middle). This issue is mitigated using our point regularization (right). The synthetic point cloud above (e.g.boxandshoesfalling on the ground) is generated by Kubric[13]and trained with our pipeline.",
                "position": 278
            },
            {
                "img": "https://arxiv.org/html/2502.03639/x5.png",
                "caption": "Figure 5:Ablation on Point Augmentation and Regularization.Our point-augmented model demonstrates a degree of 3D-awareness compared to the model fine-tuned on video data alone, but it still exhibits some artifacts, which are mitigated through regularization.",
                "position": 337
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.03639/x6.png",
                "caption": "Figure 6:Comparison on General Videos.We showcase the generated videos across various categories, including static and dynamic objects, humans, and animals. Our method ensures smooth transitions in object shape and motion and eliminates morphing artifacts.",
                "position": 381
            }
        ]
    },
    {
        "header": "5Conclusion and Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APointVid Dataset Generation",
        "images": []
    },
    {
        "header": "Appendix BAdditional Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.03639/x7.png",
                "caption": "Figure 7:Comparison on Task-oriented Videos.",
                "position": 1058
            },
            {
                "img": "https://arxiv.org/html/2502.03639/x8.png",
                "caption": "Figure 8:Comparison on General Categories.",
                "position": 1062
            },
            {
                "img": "https://arxiv.org/html/2502.03639/x9.png",
                "caption": "Figure 9:Ablation Studies on Different Components.We compare the results from (i) our full generation pipeline with (ii) model trained with RGB only, (iii) model trained with point augmentation only (no regularization), (iv) model without channel cross-attention, and (v) model trained without diffusion loss.",
                "position": 1066
            }
        ]
    },
    {
        "header": "Appendix CAdditional Ablation Study",
        "images": []
    },
    {
        "header": "Appendix DUser Study",
        "images": []
    }
]