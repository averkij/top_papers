[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05318/img/mic_logo.png",
                "caption": "",
                "position": 279
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05318/x1.png",
                "caption": "Figure 1:Task overview ofBird-Interactshowing the evaluated system interacting with DB Environment and User Simulator to complete the user task with a sequence of sub-tasks.",
                "position": 306
            }
        ]
    },
    {
        "header": "2Problem Definition",
        "images": []
    },
    {
        "header": "3Benchmark Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05318/x2.png",
                "caption": "Figure 2:Knowledge chain breaking ambiguity.",
                "position": 397
            }
        ]
    },
    {
        "header": "4Evaluation Settings",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05318/x3.png",
                "caption": "Figure 3:Two evaluation settings forBird-Interact:cc-Interact, where the system engages in conversation with the user, andaa-Interact, where the system interacts flexibly. At the end of the task, the system will receive a rewardr∈[0,1]r\\in[0,1].",
                "position": 523
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05318/img/interaction_time_scaling.png",
                "caption": "Figure 4:The performance of different LLMs with different user patience onBird-Interact-Lite. The red line denotesaa-Interact mode (-a); the blue line denotescc-Interact mode (-c). And the dotted line (IdealizedPerformance) denotes the performance under ambiguity-free single-turn text-to-SQL.",
                "position": 873
            },
            {
                "img": "https://arxiv.org/html/2510.05318/x4.png",
                "caption": "Figure 5:SR of GPT-5 with memory grafting.",
                "position": 881
            }
        ]
    },
    {
        "header": "6User Simulator Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05318/x5.png",
                "caption": "Figure 6:The accuracy of different user simulators onUserSim-Guard.",
                "position": 925
            }
        ]
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Future Work",
        "images": []
    },
    {
        "header": "9Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix Contents",
        "images": []
    },
    {
        "header": "Appendix ALimitations",
        "images": []
    },
    {
        "header": "Appendix BAnnotation Group Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05318/img/BI_tutorial_1.png",
                "caption": "Figure 7:Examples of training materials by screenshots for BIRD-Interact annotators.",
                "position": 2139
            },
            {
                "img": "https://arxiv.org/html/2510.05318/img/BI_tutorial_2.png",
                "caption": "",
                "position": 2148
            }
        ]
    },
    {
        "header": "Appendix CBenchmark Design Principles",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05318/img/no.png",
                "caption": "Table 4:Data statistics of features inBird-Interactcompared to the evaluation set of related benchmarks.# Avg Turns: Number of User-System interactions by unfolding the model’s interaction trajectory.# Toks./Output: Average number of tokens in the reference output; “/” indicates benchmarks without reference output.Dynamic User: Whether the benchmark supports real-time user interaction (vs. static offline datasets).Dynamic Env State: Whether the database or environment state can be modified during interaction.Amb. Sources: Sources of ambiguity in user queries or environments. LLM + Guard means LLM as user simulator with Guard mechanism to make actions more controllable. [†]: Results taken from publicly available Spider 2.0 Lite Gold SQL.",
                "position": 2272
            },
            {
                "img": "https://arxiv.org/html/2510.05318/img/yes.png",
                "caption": "",
                "position": 2337
            },
            {
                "img": "https://arxiv.org/html/2510.05318/x6.png",
                "caption": "Table 5:Comparison of released databases across benchmarks.",
                "position": 2567
            },
            {
                "img": "https://arxiv.org/html/2510.05318/x6.png",
                "caption": "Figure 8:Distribution of advanced SQL features inBird-Interact.",
                "position": 2656
            }
        ]
    },
    {
        "header": "Appendix DComparison with Related Benchmarks",
        "images": []
    },
    {
        "header": "Appendix EEvaluation Metrics",
        "images": []
    },
    {
        "header": "Appendix FTest Scripts",
        "images": []
    },
    {
        "header": "Appendix GAmbiguity and Follow-up Annotation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05318/x7.png",
                "caption": "Figure 9:Ambiguity types distribution.",
                "position": 2981
            }
        ]
    },
    {
        "header": "Appendix HExperiment Details",
        "images": []
    },
    {
        "header": "Appendix IAction Space and Selection Patterns inaa-Interact",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05318/x8.png",
                "caption": "Figure 10:System action distribution of systems under default setting (patience=3) onLiteset. P1 and P2 indicate the success rate for the first sub-task and the second sub-task.",
                "position": 3700
            },
            {
                "img": "https://arxiv.org/html/2510.05318/x9.png",
                "caption": "Figure 11:System action distribution of systems under default setting (patience=3) onFullset.. P1 and P2 indicate the success rate for the first sub-task and the second sub-task.",
                "position": 3710
            },
            {
                "img": "https://arxiv.org/html/2510.05318/x10.png",
                "caption": "Figure 12:System action distribution of systems under default setting (patience=3) in heatmap onFullset.",
                "position": 3720
            },
            {
                "img": "https://arxiv.org/html/2510.05318/x11.png",
                "caption": "Figure 13:The interaction pattern of systems: action groups over turns under default setting (patience=3) onFullset.",
                "position": 3730
            }
        ]
    },
    {
        "header": "Appendix JPerformance on Different Ambiguity Types",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05318/x12.png",
                "caption": "Figure 14:Success Rate of LLMs on different ambiguity types overccandaa-Interact Modes.",
                "position": 3867
            },
            {
                "img": "https://arxiv.org/html/2510.05318/x13.png",
                "caption": "Figure 15:Success Rate of LLMs on linear and higher-order ambiguity overccandaa-Interact Modes.",
                "position": 3872
            }
        ]
    },
    {
        "header": "Appendix KExperiments onBird-Interact-Lite",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05318/x14.png",
                "caption": "Figure 16:The performance under our proposed two-stage user simulator and baseline user simulator compared with human users on 100 sampled tasks.",
                "position": 4146
            }
        ]
    },
    {
        "header": "Appendix LError Analysis",
        "images": []
    },
    {
        "header": "Appendix MUser Simulator Design Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05318/x15.png",
                "caption": "Figure 17:An example of an Abstract Syntax Tree (AST) for a SQL query.",
                "position": 4172
            }
        ]
    },
    {
        "header": "Appendix NEvaluating the Function-Driven User Simulator",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05318/x16.png",
                "caption": "Figure 18:Case study of effective communication undercc-Interact.",
                "position": 4327
            }
        ]
    },
    {
        "header": "Appendix OPathways to Effective Communication",
        "images": []
    },
    {
        "header": "Appendix PHuman Evaluation of Dataset Quality",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05318/x17.png",
                "caption": "Figure 19:System prompt undercc-Interact.",
                "position": 4433
            },
            {
                "img": "https://arxiv.org/html/2510.05318/x18.png",
                "caption": "Figure 20:System prompt underaa-Interact (part 1).",
                "position": 4436
            },
            {
                "img": "https://arxiv.org/html/2510.05318/x19.png",
                "caption": "Figure 21:System prompt underaa-Interact (part 2).",
                "position": 4439
            },
            {
                "img": "https://arxiv.org/html/2510.05318/x20.png",
                "caption": "Figure 22:System prompt underaa-Interact (part 3).",
                "position": 4442
            },
            {
                "img": "https://arxiv.org/html/2510.05318/x21.png",
                "caption": "Figure 23:The prompt of baseline user simulator.",
                "position": 4452
            },
            {
                "img": "https://arxiv.org/html/2510.05318/x22.png",
                "caption": "Figure 24:Our proposed two-stage function-driven User Simulator: the prompt of User Simulator stage 1: LLM as Parser.",
                "position": 4455
            },
            {
                "img": "https://arxiv.org/html/2510.05318/x23.png",
                "caption": "Figure 25:Our proposed two-stage function-driven User Simulator: the prompt of User Simulator stage 2: LLM as Generator.",
                "position": 4458
            },
            {
                "img": "https://arxiv.org/html/2510.05318/x24.png",
                "caption": "Figure 26:LLM-as-judge prompt to evaluate the performance of user simulators.",
                "position": 4461
            }
        ]
    },
    {
        "header": "Appendix QPrompts",
        "images": []
    }
]