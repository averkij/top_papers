[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21696/extracted/6315622/images/embodied-reasoner-logo.png",
                "caption": "",
                "position": 103
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21696/x1.png",
                "caption": "Figure 1:We design an embodied interactive task: searching for objects in an unknown room. Then we proposeEmbodied-Reasoner, which presentsspontaneous reasoning and interactionability. Before each action, it generates diverse thoughts, e.g., self-reflection or spatial reasoning, forming animage-textinterleaved trajectory. It shows consistent reasoning and efficient search behaviors, whereas OpenAI\no3-mini often exhibits repetitive searches and logical inconsistencies with higher failure rates.",
                "position": 146
            },
            {
                "img": "https://arxiv.org/html/2503.21696/x2.png",
                "caption": "Figure 2:Embodied-Reasonerexhibits spontaneous thinking behaviors, e.g.,analyzingenvironmental states(#1,3),reflectingon missed details(#4),reasoningbased on the latest observations(#5), andrecalling cuesfor efficient planning(#9). These thoughts remain coherent and logically consistent despite spanning multiple rounds. In contrast, general VLMs lacking thinking abilities struggle with long-horizon interactive tasks and produce unreasonable actions, e.g., forget tasks or repetitive searching.",
                "position": 150
            },
            {
                "img": "https://arxiv.org/html/2503.21696/x3.png",
                "caption": "Figure 3:Left: Data Engine for<<<Instruction, Interactive Trajectory>>>synthesis. First, we synthesize instructions from task templates, and build an affiliation graph from scene’s meta-data. It enables us to derive key actions needed for task. We add exploratory actions and insert thinking thoughts between observation and actions.Right: Three-stage training recipe. ①We fine-tune on synthesized trajectory to develop interaction skills. ②We sample multiple trajectories on novel tasks and evaluate their correctness. The successful ones are used for developing its exploring abilities. ③We continue to sample trajectories using updated model, injecting anomalous states and reflective thoughts in successful cases and correcting errors in failed ones. This self-correction training yieldsEmbodied-Reasoner.",
                "position": 184
            }
        ]
    },
    {
        "header": "2Observation-Thought-Action Corpora",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21696/x4.png",
                "caption": "Figure 4:We analyze the frequency of five types of thoughts and their flexible transition relationships in all trajectories.",
                "position": 255
            }
        ]
    },
    {
        "header": "3Training Recipe for Embodied-Reasoner",
        "images": []
    },
    {
        "header": "4Dataset Statistics",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21696/x5.png",
                "caption": "Figure 5:Relations betweentask lengthandsuccess rate, and outputtoken number. As task complexity increases, our model generates more reasoning tokens to maintain high success rates.",
                "position": 690
            },
            {
                "img": "https://arxiv.org/html/2503.21696/x6.png",
                "caption": "Figure 6:Repetitive Exploration Rate measures repetitive search issues, which are often observed in baselines. Our models reduce repetitive searches by recalling and reflecting on past trajectories.",
                "position": 693
            },
            {
                "img": "https://arxiv.org/html/2503.21696/x7.png",
                "caption": "Figure 7:Real-world experiments. Our model achieves a higher success rate (56.7%) than OpenAI o3-mini (43.4%) and o1 (50%).",
                "position": 699
            }
        ]
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelated Works",
        "images": []
    },
    {
        "header": "Appendix BExperiment Details",
        "images": []
    },
    {
        "header": "Appendix CDataset Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21696/x8.png",
                "caption": "Figure C1:The distribution of the training dataset with9,390samples, including4task types and10sub-task types.",
                "position": 1644
            },
            {
                "img": "https://arxiv.org/html/2503.21696/x9.png",
                "caption": "Figure C2:The distribution of the test set with809tasks, including4task types and11sub-task types.",
                "position": 1653
            },
            {
                "img": "https://arxiv.org/html/2503.21696/x10.png",
                "caption": "Figure C3:The distribution of the training set interactions, including8interaction types in trajectories:navigate to, pickup, open, close, put in, observe, move forward, and toggle.",
                "position": 1666
            },
            {
                "img": "https://arxiv.org/html/2503.21696/x11.png",
                "caption": "Figure C4:The distribution of the test set interactions, including6interaction types in key actions:navigate to, pickup, open, close, put in, and toggle.",
                "position": 1669
            },
            {
                "img": "https://arxiv.org/html/2503.21696/x12.png",
                "caption": "Figure C5:The quantity distribution of trajectory lengths in the training set and the corresponding task type composition, whereSearch Taskis mainly within1-9,Manipulate Taskwithin2-11,Transport Taskwithin3-14, andComposite Taskabove8, extending beyond 23.",
                "position": 1682
            },
            {
                "img": "https://arxiv.org/html/2503.21696/x13.png",
                "caption": "Figure C6:The quantity distribution of key action lengths in the test set and the corresponding task type composition, whereSearch Taskis mainly within 1-2,Manipulate Taskwithin 2, 4-5,Transport Taskwithin 4-7, andComposite Taskabove 8, extending beyond 19.",
                "position": 1685
            },
            {
                "img": "https://arxiv.org/html/2503.21696/x14.png",
                "caption": "Figure C7:The quantity distribution of the top 32 object types in the training dataset’s trajectories, withOthersrepresenting the remaining 62 categories, such asBread, Book, DeskLamp, etc.",
                "position": 1695
            },
            {
                "img": "https://arxiv.org/html/2503.21696/x15.png",
                "caption": "Figure C8:The quantity distribution of the top 32 object types in the test set’s key actions, withOthersrepresenting the remaining 44 categories, such asWatch, Pencil, Cup, etc.",
                "position": 1698
            }
        ]
    },
    {
        "header": "Appendix DDetailed Task Templates and Constraints",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21696/x16.png",
                "caption": "",
                "position": 1749
            },
            {
                "img": "https://arxiv.org/html/2503.21696/x17.png",
                "caption": "",
                "position": 1892
            },
            {
                "img": "https://arxiv.org/html/2503.21696/x18.png",
                "caption": "",
                "position": 1943
            },
            {
                "img": "https://arxiv.org/html/2503.21696/x19.png",
                "caption": "",
                "position": 1994
            },
            {
                "img": "https://arxiv.org/html/2503.21696/x20.png",
                "caption": "",
                "position": 2048
            },
            {
                "img": "https://arxiv.org/html/2503.21696/x21.png",
                "caption": "",
                "position": 2115
            },
            {
                "img": "https://arxiv.org/html/2503.21696/x22.png",
                "caption": "",
                "position": 2183
            },
            {
                "img": "https://arxiv.org/html/2503.21696/x23.png",
                "caption": "",
                "position": 2189
            },
            {
                "img": "https://arxiv.org/html/2503.21696/x24.png",
                "caption": "",
                "position": 2376
            },
            {
                "img": "https://arxiv.org/html/2503.21696/x25.png",
                "caption": "",
                "position": 2486
            }
        ]
    },
    {
        "header": "Appendix EDetailed Prompts",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21696/x26.png",
                "caption": "Figure F9:Trajectory Case for Embodied Reasoner",
                "position": 2677
            },
            {
                "img": "https://arxiv.org/html/2503.21696/x27.png",
                "caption": "Figure F10:Trajectory Case for GPT-o1",
                "position": 2693
            },
            {
                "img": "https://arxiv.org/html/2503.21696/x28.png",
                "caption": "Figure F11:Trajectory Case for Embodied Reasoner in Real World",
                "position": 2704
            }
        ]
    },
    {
        "header": "Appendix FCase Study",
        "images": []
    }
]