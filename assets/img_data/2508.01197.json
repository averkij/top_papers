[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.01197/x1.png",
                "caption": "Figure 1:Examples of 3D occupancy grounding.Conventional bounding boxes cannot accurately describe irregularly shaped vehicles in daily driving scenes, such as the excavators in (a) and (c). In contrast, 3D occupancy representations provide a more precise depiction of complex obstacles, as shown in (b) and (d).",
                "position": 107
            },
            {
                "img": "https://arxiv.org/html/2508.01197/x2.png",
                "caption": "Figure 2:Illustration for 3D occupancy grounding.(a) Our proposed 3D occupancy grounding aims to perceive the 3D occupancy of the referred object in a scene using language descriptions.\n(b) The traditional 3D visual grounding intends to estimate the location and size of the referred object.\n(c) Monocular 3D visual grounding relies only on a single image to localize the 3D extent of the referred object.\n(d) The counterpart 2D task does not capture the 3D extent of the referred object.",
                "position": 110
            }
        ]
    },
    {
        "header": "IIRelated Work",
        "images": []
    },
    {
        "header": "IIIThe Talk2Occ Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.01197/x3.png",
                "caption": "Figure 3:Overall architecture of three proposed baselines.The LiDAR-based baseline (top) employs parameterized voxelization and sparse 3D convolutions to generate LiDAR voxel features, while the camera-based baseline (bottom) utilizes a 3D voxel query mechanism to extract features from multi-view images. The multi-modal baseline combines both modalities through an adaptive fusion module to leverage complementary information.",
                "position": 357
            }
        ]
    },
    {
        "header": "IVMethodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.01197/x4.png",
                "caption": "Figure 4:The overview framework of the multi-modal GroundingOcc.GroundingOcc replaces semantic supervision with geometric supervision for more precise occupancy grounding prediction, utilizing a voxel encoder and fusion module to generate final occupancy grounding results. The architecture outputs both 3D occupancy grounding predictions and optional 3D bounding boxes that can be refined in post-processing. Here,T,T−1,…,T−kT,T-1,\\dots,T-kitalic_T , italic_T - 1 , … , italic_T - italic_kdenote the current image frameTTitalic_Tand itskkitalic_kpreceding frames.",
                "position": 428
            },
            {
                "img": "https://arxiv.org/html/2508.01197/x5.png",
                "caption": "Figure 5:Comparison between different depth ground truth generation methods.Our occupancy-based ray-casting pipeline generates more complete and geometrically accurate depth maps.",
                "position": 487
            }
        ]
    },
    {
        "header": "VExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.01197/x6.png",
                "caption": "Figure 6:Qualitative results from baseline method and our GroundingOcc.Red, blue, and purple boxes denote the ground truth occupancy grounding, prediction, and ground truth bounding box, respectively.",
                "position": 678
            }
        ]
    },
    {
        "header": "VIConclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]