[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15110/x1.png",
                "caption": "(a)DLER Training on DeepSeek-R1-7B",
                "position": 139
            },
            {
                "img": "https://arxiv.org/html/2510.15110/x1.png",
                "caption": "(a)DLER Training on DeepSeek-R1-7B",
                "position": 142
            },
            {
                "img": "https://arxiv.org/html/2510.15110/x2.png",
                "caption": "(b)DLER-R1-7B enable better test-time scaling",
                "position": 147
            }
        ]
    },
    {
        "header": "2Preliminary",
        "images": []
    },
    {
        "header": "3Re-examining the Simplest Length Penalty - Truncation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15110/x3.png",
                "caption": "Figure 2:Accuracy and average response length of DeepSeek-R1-7B on the AIME-24 test set, evaluated every 10 training steps across two RL training runs: group-wise reward normalization (GRPO) and batch-wise normalization. GRPO shows declining accuracy while batch-wise reward normalization remains stable despite reduced token counts.",
                "position": 270
            },
            {
                "img": "https://arxiv.org/html/2510.15110/x4.png",
                "caption": "(a)",
                "position": 304
            },
            {
                "img": "https://arxiv.org/html/2510.15110/x4.png",
                "caption": "(a)",
                "position": 307
            },
            {
                "img": "https://arxiv.org/html/2510.15110/x5.png",
                "caption": "(b)",
                "position": 312
            },
            {
                "img": "https://arxiv.org/html/2510.15110/x6.png",
                "caption": "Figure 4:Left: Ratio of training prompts with all 16 rollouts receiving zero reward, including those caused by exceeding the truncation length. Around half of the prompts fall into this category early in training, weakening the signal and biasing the model toward easier prompts that model already know how to solve within the target length.Right: Ratio of training prompts with all 16 rollouts receiving reward score of one steadily increases, while average response length declines and remains markedly shorter than that for prompts whose rollouts all receive a reward of zero.",
                "position": 351
            },
            {
                "img": "https://arxiv.org/html/2510.15110/x7.png",
                "caption": "Figure 5:Accuracy and average response length of DeepSeek-R1-7B on the AIME-24 test set, evaluated every 10 training steps across two RL training runs: one with batch-wise reward normalization and the other with our DLER recipe. DLER enhances the accuracyâ€“token efficiency by reducing token usage while fully recovering the accuracy loss of applying plain batch-wise reward normalization.",
                "position": 382
            },
            {
                "img": "https://arxiv.org/html/2510.15110/x8.png",
                "caption": "(a)Batch-wise Reward Normalization",
                "position": 385
            },
            {
                "img": "https://arxiv.org/html/2510.15110/x8.png",
                "caption": "(a)Batch-wise Reward Normalization",
                "position": 388
            },
            {
                "img": "https://arxiv.org/html/2510.15110/x9.png",
                "caption": "(b)Batch-wise Reward Normalization w/ Higher Clipping Threshold",
                "position": 394
            },
            {
                "img": "https://arxiv.org/html/2510.15110/x10.png",
                "caption": "(c)DLER: Batch-wise Reward Normalization w/ Higher Clipping Threshold and w/ Dynamic Sampling",
                "position": 400
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15110/x11.png",
                "caption": "(a)DLER-R1-1.5B vs Baselines",
                "position": 692
            },
            {
                "img": "https://arxiv.org/html/2510.15110/x11.png",
                "caption": "(a)DLER-R1-1.5B vs Baselines",
                "position": 695
            },
            {
                "img": "https://arxiv.org/html/2510.15110/x12.png",
                "caption": "(b)DLER-R1-7B vs Baselines",
                "position": 701
            },
            {
                "img": "https://arxiv.org/html/2510.15110/x13.png",
                "caption": "(a)MATH",
                "position": 731
            },
            {
                "img": "https://arxiv.org/html/2510.15110/x13.png",
                "caption": "(a)MATH",
                "position": 734
            },
            {
                "img": "https://arxiv.org/html/2510.15110/x14.png",
                "caption": "(b)AIME-24",
                "position": 739
            },
            {
                "img": "https://arxiv.org/html/2510.15110/x15.png",
                "caption": "(c)Olympiad",
                "position": 745
            },
            {
                "img": "https://arxiv.org/html/2510.15110/x16.png",
                "caption": "(a)DeepSeek-R1-7B",
                "position": 857
            },
            {
                "img": "https://arxiv.org/html/2510.15110/x16.png",
                "caption": "(a)DeepSeek-R1-7B",
                "position": 860
            },
            {
                "img": "https://arxiv.org/html/2510.15110/x17.png",
                "caption": "(b)Laser-DE-L4096-7B",
                "position": 865
            },
            {
                "img": "https://arxiv.org/html/2510.15110/x18.png",
                "caption": "(c)DLER-R1-7B",
                "position": 870
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADLER achieves SOTA Accuracy/Length of CoT trade-off and enable better test-time scaling",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15110/x19.png",
                "caption": "(a)DLER Training on DeepSeek-R1-1.5B",
                "position": 1446
            },
            {
                "img": "https://arxiv.org/html/2510.15110/x19.png",
                "caption": "(a)DLER Training on DeepSeek-R1-1.5B",
                "position": 1449
            },
            {
                "img": "https://arxiv.org/html/2510.15110/x20.png",
                "caption": "(b)DLER Training on DeepSeek-R1-7B",
                "position": 1454
            },
            {
                "img": "https://arxiv.org/html/2510.15110/x21.png",
                "caption": "(a)DLER-R1-1.5B vs DeepSeek-R1-1.5B",
                "position": 1461
            },
            {
                "img": "https://arxiv.org/html/2510.15110/x21.png",
                "caption": "(a)DLER-R1-1.5B vs DeepSeek-R1-1.5B",
                "position": 1464
            },
            {
                "img": "https://arxiv.org/html/2510.15110/x22.png",
                "caption": "(b)DLER-R1-7B vs DeepSeek-R1-7B",
                "position": 1469
            }
        ]
    },
    {
        "header": "Appendix BLarger reward variance results in larger bias in advantage estimation.",
        "images": []
    },
    {
        "header": "Appendix CHyperparameters Setting",
        "images": []
    },
    {
        "header": "Appendix DParallel Thinking Latency",
        "images": []
    }
]