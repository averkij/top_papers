[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.16117/x1.png",
                "caption": "Figure 1:Performance of DiffusionNFT.(a)Head-to-head comparison with FlowGRPO on the GenEval task.(b)By employing multiple reward models, DiffusionNFT significantly boosts the performance of SD3.5-Medium in every benchmark tested, while being fully CFG-free.",
                "position": 101
            },
            {
                "img": "https://arxiv.org/html/2509.16117/x2.png",
                "caption": "",
                "position": 111
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.16117/x3.png",
                "caption": "Figure 2:Comparison between Forward-Process RL (NFT) and Reverse-Process RL (GRPO). NFT allows using any solvers and does not require storing the whole sampling trajectory for optimization.",
                "position": 135
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Diffusion Reinforcement via Negative-aware Finetuning",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.16117/x4.png",
                "caption": "Figure 3:Improvement Direction.",
                "position": 315
            },
            {
                "img": "https://arxiv.org/html/2509.16117/x5.png",
                "caption": "Figure 4:DiffusionNFT jointly optimizes two dual diffusion objectives, on both positive (r=1r=1) and negative (r=0r=0) branches. Rather than training two independent modelsùíóŒ∏+{\\bm{v}}_{\\theta}^{+}andùíóŒ∏‚àí{\\bm{v}}_{\\theta}^{-}, it adopts an implicit parameterization technique that directly optimizes a single target policyùíóŒ∏{\\bm{v}}_{\\theta}.",
                "position": 326
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.16117/x6.png",
                "caption": "Figure 5:Qualitative Comparison.The prompts are taken fromGenEval,OCRandDrawBenchrespectively, where we compare the corresponding FlowGRPO model with our model.",
                "position": 675
            },
            {
                "img": "https://arxiv.org/html/2509.16117/x7.png",
                "caption": "Figure 6:Head-to-head comparison between DiffusionNFT with FlowGRPO on single rewards.",
                "position": 689
            },
            {
                "img": "https://arxiv.org/html/2509.16117/x8.png",
                "caption": "",
                "position": 699
            },
            {
                "img": "https://arxiv.org/html/2509.16117/x9.png",
                "caption": "",
                "position": 705
            },
            {
                "img": "https://arxiv.org/html/2509.16117/x10.png",
                "caption": "Figure 7:Different diffusion samplers for data collection.",
                "position": 719
            },
            {
                "img": "https://arxiv.org/html/2509.16117/x10.png",
                "caption": "Figure 7:Different diffusion samplers for data collection.",
                "position": 721
            },
            {
                "img": "https://arxiv.org/html/2509.16117/x11.png",
                "caption": "",
                "position": 732
            },
            {
                "img": "https://arxiv.org/html/2509.16117/x12.png",
                "caption": "Figure 8:Soft-update strategies.",
                "position": 741
            },
            {
                "img": "https://arxiv.org/html/2509.16117/x13.png",
                "caption": "Figure 9:Different time-dependent weighting strategies.",
                "position": 747
            },
            {
                "img": "https://arxiv.org/html/2509.16117/x13.png",
                "caption": "Figure 9:Different time-dependent weighting strategies.",
                "position": 749
            },
            {
                "img": "https://arxiv.org/html/2509.16117/x14.png",
                "caption": "",
                "position": 760
            },
            {
                "img": "https://arxiv.org/html/2509.16117/x15.png",
                "caption": "Figure 10:Choices of strengthŒ≤\\beta.",
                "position": 769
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProof of Theorems",
        "images": []
    },
    {
        "header": "Appendix BTheoretical Discussions",
        "images": []
    },
    {
        "header": "Appendix CExperiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.16117/x16.png",
                "caption": "Figure 11:Qualitative comparison between FlowGRPO and our model on GenEval prompts.",
                "position": 2165
            },
            {
                "img": "https://arxiv.org/html/2509.16117/x17.png",
                "caption": "Figure 12:Qualitative comparison between FlowGRPO and our model on OCR prompts.",
                "position": 2169
            },
            {
                "img": "https://arxiv.org/html/2509.16117/x18.png",
                "caption": "Figure 13:Qualitative comparison between FlowGRPO and our model on DrawBench prompts.",
                "position": 2173
            }
        ]
    },
    {
        "header": "Appendix DAdditional Results",
        "images": []
    }
]