[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05027/images/img7.png",
                "caption": "Figure 1:The trade-off between normalized reconstruction error (L2) and sparsity (L0) for models at layer 12.",
                "position": 510
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/frame-cluster-to-layer-percentages-bigger.png",
                "caption": "Table 1:SAE feature set coverage between models and layers. Suffix2is for SAE trained on the same activations but initialized with different random seeds;\nsuffixL+nis for coverage between different layers of the model (each layer is compared with the layer from the next column, i.e. in L4 column we show the coverage between features from layer 4 with features from layer 7); (dup) means the amount of duplicates. For Gemma model we took SAEs from layers 2, 6, 12, 18 and 24. Note that the number of features for our SAEs in 6144, and for Gemma-Scope is 16384.",
                "position": 529
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/frame-cluster-to-layer-percentages-bigger.png",
                "caption": "Figure 2:Layer-wise feature specialization ratio byspeech,sounds, and music domains for Whisper (solid line) and HuBERT (dashed) at frame (top) and audio (bottom) levels.",
                "position": 642
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/audio-cluster-to-layer-percentages-bigger.png",
                "caption": "",
                "position": 646
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/whisper_layer0_modif1.png",
                "caption": "Figure 3:Top-kkprobing and unlearning for accent classification. More results in AppendixD.",
                "position": 671
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/A_SAE_9_small.png",
                "caption": "Figure 4:Selective unlearning of letter ’A’ via iterative feature removal.\nFeature indices on x-axis ordered by discriminative importance for target vowel.",
                "position": 674
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/laugh_whisper_l6_3470.png",
                "caption": "(a)",
                "position": 699
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/laugh_whisper_l6_3470.png",
                "caption": "(a)",
                "position": 702
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/sneeze_whisper_4719_l6.png",
                "caption": "(b)",
                "position": 708
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/MM1.png",
                "caption": "Figure 6:Activation of features responsible for the beginning (3249) and the end (3081) of speech, aligned with corresponding waveform. HuBERT, layer 11.",
                "position": 723
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/trf_hubert_4012.png",
                "caption": "(a)",
                "position": 732
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/trf_hubert_4012.png",
                "caption": "(a)",
                "position": 735
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/trf_hubert_1423.png",
                "caption": "(b)",
                "position": 741
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/trf_hubert_4274.png",
                "caption": "(c)",
                "position": 747
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/img5.png",
                "caption": "Table 2:FPR (τ=0.5\\tau=0.5) for steering configurations. No SAE: Whisper inference without modification. No Steer.: Whisper with injected SAE on the last layer. S-Vec.: S-Vector, calculated on Musan. Optimal and Best SAE: SAE S-Vector, top-100 features from FSD50k dataset withα\\alphaequals to 1 and 3 respectively. LibriSpeech (LS) line represents WER. Lower is better.",
                "position": 763
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/img5.png",
                "caption": "Figure 8:Influence of the expansion rate to the number of \"alive\" features",
                "position": 1796
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/img6.png",
                "caption": "Figure 9:Connection between reconstruction quality and neurons survival rate",
                "position": 1799
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/img8.png",
                "caption": "Figure 10:Connection between sparsity and neurons survival rate",
                "position": 1812
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/audio-level-freq-whisper-6.png",
                "caption": "Table 4:SAE feature set coverage between models and layers.\nLS (LibriSpeech), FSD (FSD50K) and MTG mean datasets for coverage score calculation.\nSuffix2is for SAE trained on the same activations but initialized with different random seeds;\nsuffix 100K is for early stage of SAE training (100K iterations);\nsuffixL+nis for coverage between different layers of the model (each layer is compared with the layer from the next column, i.e. in L4 column we show the coverage between features from layer 4 with features from layer 7).",
                "position": 1818
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/audio-level-freq-whisper-6.png",
                "caption": "Table 5:Numbers of features having duplicates within same SAE. Dataset and model names are as in TableB",
                "position": 2014
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/audio-level-freq-whisper-6.png",
                "caption": "Figure 11:Frequency-based domain specialization.Audio-level (1st and 3rd rows from the top) and frame-level (2nd and 4th rows from the top) activation frequency versus activation magnitude for Whisper (1st and 2nd rows from the top) and HuBERT (3rd and 4th rows from the top) layers 6–7 (left and right columns respectively). Colors: red (speech), blue (sounds), green (music), gray (unassigned), black (dead).",
                "position": 2317
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/audio-level-freq-whisper-6.png",
                "caption": "",
                "position": 2320
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/audio-level-freq-whisper-7.png",
                "caption": "",
                "position": 2325
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/frame-level-freq-whisper-6.png",
                "caption": "",
                "position": 2330
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/frame-level-freq-whisper-7.png",
                "caption": "",
                "position": 2335
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/audio-level-freq-hubert-6.png",
                "caption": "",
                "position": 2340
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/audio-level-freq-hubert-7.png",
                "caption": "",
                "position": 2345
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/frame-level-freq-hubert-6.png",
                "caption": "",
                "position": 2350
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/frame-level-freq-hubert-7.png",
                "caption": "",
                "position": 2355
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/audio-level-venn-whisper-6-bigger.png",
                "caption": "Figure 12:Feature overlapfor Whisper (layers 6 and 7): Venn diagrams for audio and frame levels.",
                "position": 2361
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/audio-level-venn-whisper-6-bigger.png",
                "caption": "",
                "position": 2364
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/audio-level-venn-whisper-7-bigger.png",
                "caption": "",
                "position": 2369
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/frame-level-venn-whisper-6-bigger.png",
                "caption": "",
                "position": 2374
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/frame-level-venn-whisper-7-bigger.png",
                "caption": "",
                "position": 2379
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/whisper_layer2_sp-noise.png",
                "caption": "Figure 13:Top-k probing and unlearning for four classification tasks",
                "position": 2398
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/hubert_layer8.png",
                "caption": "",
                "position": 2402
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/whisper_layer2.png",
                "caption": "",
                "position": 2404
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/hubert_layer11.png",
                "caption": "",
                "position": 2406
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/hubert_layer2_modif1.png",
                "caption": "",
                "position": 2410
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/whisper_layer2_modif1.png",
                "caption": "",
                "position": 2412
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/hubert_layer2_.png",
                "caption": "",
                "position": 2414
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/laugh_hubert_l4_3704.png",
                "caption": "(a)",
                "position": 2467
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/laugh_hubert_l4_3704.png",
                "caption": "(a)",
                "position": 2470
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/repretitiva_hubert_l4_1393.png",
                "caption": "(b)",
                "position": 2476
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/Automatic_interpretation_pipeline.png",
                "caption": "Figure 15:Automatic Interpretation Pipeline",
                "position": 2486
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/whisper_6_layer_5_wordcloud.png",
                "caption": "Figure 16:Features label frequencies for Whisper and Hubert models.",
                "position": 2768
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/whisper_6_layer_5_wordcloud.png",
                "caption": "",
                "position": 2771
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/hubert_52_layer_5_wordcloud.png",
                "caption": "",
                "position": 2775
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/fsd50k-distribution-analysis.png",
                "caption": "Figure 17:Distribution of no_speech_prob on the FSD50k dataset before and after applying steering vectors. The post-steering distribution is skewed towards 1.0.",
                "position": 3279
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/musan-distribution-analysis.png",
                "caption": "Figure 18:Distribution of no_speech_prob on the Musan dataset before and after applying steering vectors. The post-steering distribution is skewed towards 1.0.",
                "position": 3282
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/wham-distribution-analysis.png",
                "caption": "Figure 19:Distribution of no_speech_prob on the WHAM dataset before and after applying steering vectors.",
                "position": 3285
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/ls-test-clean-distribution-analysis.png",
                "caption": "Figure 20:Distribution of no_speech_prob on the LibriSpeech test-clean dataset before and after applying steering vectors. The post-steering distribution is skewed towards 1.0.",
                "position": 3288
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/mel012.png",
                "caption": "Figure 21:Features 3249 and 3081 of Hubert’s SAE from layer 11.",
                "position": 3298
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/audio-level-t-SNE-decomp-0-w.png",
                "caption": "Figure 22:t-SNE decomposition of SAE encoder weights for Whisper layer 6 in the audio-level setup. Each point corresponds to a single latent feature, colored by its domain assignment (speech, sounds, music, or unassigned) obtained from activation-frequency–based specialization. Brighter dots indicate features with larger activation frequency differences between domains, highlighting the most strongly specialized units in the representation space.",
                "position": 3313
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/audio-level-t-SNE-decomp-0-w.png",
                "caption": "",
                "position": 3316
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/audio-level-t-SNE-decomp-1-w.png",
                "caption": "",
                "position": 3321
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/audio-level-t-SNE-decomp-2-w.png",
                "caption": "",
                "position": 3326
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/audio-level-t-SNE-decomp-3-w.png",
                "caption": "",
                "position": 3331
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/audio-level-t-SNE-decomp-0-h.png",
                "caption": "Figure 23:t-SNE decomposition of SAE encoder weights for HuBERT layer 6 in the audio-level setup. Points represent individual latent features colored by their domain assignments, with unassigned gray features active only in alternative domain combinations.",
                "position": 3337
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/audio-level-t-SNE-decomp-0-h.png",
                "caption": "",
                "position": 3340
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/audio-level-t-SNE-decomp-1-h.png",
                "caption": "",
                "position": 3345
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/audio-level-t-SNE-decomp-2-h.png",
                "caption": "",
                "position": 3350
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/audio-level-t-SNE-decomp-3-h.png",
                "caption": "",
                "position": 3355
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/A_SAE_11_layer.png",
                "caption": "Figure 24:Unlearning plots for letters ’A’, ’E’ and ’I’ at the last layer of HuBERT model, using standardLogisticRegressionwith standardpenalty=’l2’andmax_iter=10000",
                "position": 3361
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/A_embed_11_layer.png",
                "caption": "",
                "position": 3365
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/E_SAE_11_layer.png",
                "caption": "",
                "position": 3367
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/E_embed_11_layer.png",
                "caption": "",
                "position": 3369
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/I_SAE_11_layer.png",
                "caption": "",
                "position": 3371
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/I_embed_11_layer.png",
                "caption": "",
                "position": 3373
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/O_SAE_11_layer.png",
                "caption": "Figure 25:Unlearning plots for letters ’O’ and ’U’ at the last layer of HuBERT model, using standardLogisticRegressionwith standardpenalty=’l2’andmax_iter=10000",
                "position": 3377
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/O_embed_11_layer.png",
                "caption": "",
                "position": 3381
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/U_SAE_11_layer.png",
                "caption": "",
                "position": 3383
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/U_embed_11_layer.png",
                "caption": "",
                "position": 3385
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/A_SAE_11_layer_noreg.png",
                "caption": "Figure 26:Unlearning plots for letters ’A’, ’E’ and ’I’ at the last layer of HuBERT model, usingLogisticRegressionwithout regularization andmax_iter=10000",
                "position": 3389
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/A_embed_11_layer_noreg.png",
                "caption": "",
                "position": 3393
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/E_SAE_11_layer_noreg.png",
                "caption": "",
                "position": 3395
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/E_embed_11_layer_noreg.png",
                "caption": "",
                "position": 3397
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/I_SAE_11_layer_noreg.png",
                "caption": "",
                "position": 3399
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/I_embed_11_layer_noreg.png",
                "caption": "",
                "position": 3401
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/O_SAE_11_layer_noreg.png",
                "caption": "Figure 27:Unlearning plots for letters ’O’ and ’U’ at the last layer of HuBERT model, usingLogisticRegressionwithout regularization andmax_iter=10000",
                "position": 3405
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/O_embed_11_layer_noreg.png",
                "caption": "",
                "position": 3409
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/U_SAE_11_layer_noreg.png",
                "caption": "",
                "position": 3411
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/U_embed_11_layer_noreg.png",
                "caption": "",
                "position": 3413
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/A_SAE_9_layer.png",
                "caption": "Figure 28:K‑probe vowel classification at layer 9. The curves show classification accuracy as the most informative features are added sequentially. Only the first 49 features are displayed; beyond this point, accuracy approaches perfect accuracy for all vowels.",
                "position": 3417
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/A_embed_9_layer.png",
                "caption": "",
                "position": 3421
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/E_SAE_9_layer.png",
                "caption": "",
                "position": 3423
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/E_embed_9_layer.png",
                "caption": "",
                "position": 3425
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/I_SAE_9_layer.png",
                "caption": "",
                "position": 3427
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/I_embed_9_layer.png",
                "caption": "",
                "position": 3429
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/O_SAE_9_layer.png",
                "caption": "",
                "position": 3431
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/O_embed_9_layer.png",
                "caption": "",
                "position": 3433
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/U_SAE_9_layer.png",
                "caption": "",
                "position": 3435
            },
            {
                "img": "https://arxiv.org/html/2602.05027/images/U_embed_9_layer.png",
                "caption": "",
                "position": 3437
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]