[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.12810/figures/IntroductionChallenges.png",
                "caption": "Figure 1:Some challenging camouflage scenarios, including: multiple objects (rows 1 and 2), small objects (row 3), and tiny objects (row 4).",
                "position": 109
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.12810/figures/LR_Decoding.png",
                "caption": "Figure 2:The five decoding strategies in the literature:(a)The progressive decoding strategy,(b)The dense decoding strategy,(c)The feedback decoding strategy,(d)The separate decoding strategy, and(e)The pyramidal decoding strategy.",
                "position": 141
            },
            {
                "img": "https://arxiv.org/html/2511.12810/figures/Methodology_Diagram.png",
                "caption": "Figure 3:The the overall architecture of MSRNet consists of three scales of the original image, each of which is input into a PVT for feature extraction, generating four feature maps of different resolutions:f1f_{1},f2f_{2},f3f_{3}, andf4f_{4}. In the next stage, the feature maps of the same resolution across all scales are merged by the Attention-Based Scale Integration Unit (ABSIU). Each merged feature map is further refined inside the decoder using the Multi-Granularity Fusion Unit (MGFU). The Recursive-Feedback decoding strategy combines feedback from all lower resolutions with the current resolution being processed by the MGFU.",
                "position": 247
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.12810/figures/FeatureExtraction_Diagram.png",
                "caption": "Figure 4:Feature Extraction Approach",
                "position": 258
            },
            {
                "img": "https://arxiv.org/html/2511.12810/figures/scale_merging_network.png",
                "caption": "Figure 5:The diagram illustrates the Attention-Based Scale Integration Unit (ABSIU) for multi-scale feature integration. Features from the three scales (f1.0f^{1.0},f1.5f^{1.5},f2.0f^{2.0}) are first aligned to a common resolution and concatenated. The attention mechanism then applies a series of convolutional layers followed by a Softmax activation layer to generate three-channel attention maps (Ai1A^{1}_{i},Ai2A^{2}_{i},Ai3A^{3}_{i}), each channel corresponds to a different scale. An element-wise multiplication⊗\\otimesbetween the attention maps and their corresponding feature maps (Fi1F^{1}_{i},Fi2F^{2}_{i},Fi3F^{3}_{i}) is applied, resulting in three scale-grouped processed feature maps that are then summed to produce multi-scale feature maps. This process is repeated for each attention group, yielding four groups of multi-scale features. Lastly, a summation across groups merges features from all attention groups, producing the final outputFA​B​S​I​UF^{ABSIU}.",
                "position": 277
            },
            {
                "img": "https://arxiv.org/html/2511.12810/figures/new_decoding_stratgy.png",
                "caption": "Figure 6:This diagram illustrates the proposed novel decoding strategy. This strategy combines progressive decoding and feedback decoding. The feedback decoding strategy is an advanced recursive feedback mechanism that takes feedback from lower-resolution feature maps and applies it to all subsequent higher-resolution feature maps, enhancing the network’s contextual learning.",
                "position": 299
            },
            {
                "img": "https://arxiv.org/html/2511.12810/figures/MGFU.png",
                "caption": "Figure 7:Demonstration of all processes in the Multi-Granularity Fusion Unit (MGFU). The MGFU module enhances feature representations by analyzing and integrating features from multiple granularities. It processes features across groups with cross-channel interaction, then adaptively fuses them.",
                "position": 314
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.12810/figures/Visual_comparison.png",
                "caption": "Figure 8:Visual Comparison between our model and competing SOTA methods on sample images from all datasets.",
                "position": 1627
            },
            {
                "img": "https://arxiv.org/html/2511.12810/figures/more_comparison_results.png",
                "caption": "Figure 9:Visual Comparison illustrating the superiority of our model in detecting multiple (rows 1-3), small (rows 4 and 5), and tiny (rows 6 and 7) camouflaged objects.",
                "position": 1634
            },
            {
                "img": "https://arxiv.org/html/2511.12810/figures/FailureCases.png",
                "caption": "Figure 10:Some instances where our model does not perform detection perfectly compared to the GT. While the model occasionally (a) detects minor false regions (rows 1 and 3), (b) misses small object parts (row 4), or (c) overlooks fine details (row 2), it outperforms SOTA methods on these challenging samples.",
                "position": 1640
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]