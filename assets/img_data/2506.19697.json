[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19697/x1.png",
                "caption": "Figure 1:Comparison of\nperformance degradation patterns under 4-bit\nquantization across different training methodologies.Adamrepresents various open-source LLMs trained with the Adam optimizer. Muon (OSP) represents checkpoints from our model trained within theOutlier-Safe Pre-Training (OSP)framework.\nResults demonstrate that our framework produces fundamentally different degradation characteristics\ncompared to conventional Adam-trained models.",
                "position": 107
            },
            {
                "img": "https://arxiv.org/html/2506.19697/x2.png",
                "caption": "Figure 2:Activation distribution analysis from the 20th layer input to Multi-Head Self-Attention (MHSA) and Feed-Forward Network (FFN) modules in 1.4 billion parameter models trained on 100 billion tokens. Three optimization strategies are examined: (a) standard Adam optimizer, (b) Muon optimizer without architectural modifications, and (c) theOutlier-Safe Pre-Training (OSP)framework. The histograms reveal that substituting Adam with Muon optimizer alone provides insufficient mitigation of activation outliers.",
                "position": 116
            },
            {
                "img": "https://arxiv.org/html/2506.19697/x3.png",
                "caption": "",
                "position": 126
            },
            {
                "img": "https://arxiv.org/html/2506.19697/x4.png",
                "caption": "",
                "position": 132
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Outlier-Safe Pre-Training",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19697/x5.png",
                "caption": "Figure 3:Training dynamics comparison showing loss (left) and excess kurtosis evolution (right) across 100 billion tokens for ablation study examining various configurations of OSP components. The excess kurtosis demonstrate that only when all OSP components are simultaneously enabled does the kurtosis remain near zero throughout training, indicating complete elimination of outlier formation. Partial implementation of OSP components results in insufficient outlier suppression, as evidenced by elevated kurtosis values that persist across the training duration.",
                "position": 621
            },
            {
                "img": "https://arxiv.org/html/2506.19697/x6.png",
                "caption": "Figure 4:WikiText-2 perplexity under varying weight and activation quantization bit-widths for models trained on 100B tokens. Three configurations are compared: standard Adam, Muon, and the OSP framework.",
                "position": 626
            }
        ]
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19697/x7.png",
                "caption": "Figure 5:Activation magnitudes of query and key tokens within attention sink heads comparing Adam and OSP models. Adam models exhibit concentrated activation patterns with sparse high-magnitude channels, while OSP demonstrates broadly distributed activation patterns across multiple channels.",
                "position": 978
            },
            {
                "img": "https://arxiv.org/html/2506.19697/x8.png",
                "caption": "",
                "position": 988
            },
            {
                "img": "https://arxiv.org/html/2506.19697/x9.png",
                "caption": "Figure 6:Attention logit distributions at sink token positions comparing Adam and OSP training. Adam models exhibit skewed distributions with predominantly negative logits, while OSP models demonstrate balanced distributions between sink tokens and other positions.",
                "position": 997
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19697/x10.png",
                "caption": "Figure 7:Training dynamics over 1 trillion tokens demonstrating production-scale viability of our framework. The loss (left) and excess kurtosis evolution (right) compare Adam baseline against complete OSP implementation. Results confirm that the OSP maintains consistently low kurtosis values throughout extended training, validating the frameworkâ€™s effectiveness at production scale while achieving competitive convergence characteristics.",
                "position": 2195
            },
            {
                "img": "https://arxiv.org/html/2506.19697/x11.png",
                "caption": "Figure 8:Activation distribution visualization of models trained with Adam optimizer across 1 trillion training tokens. The histograms display input activation distributions to Multi-Head Self-Attention (MHSA) and Feed-Forward Network (FFN) layers at four transformer block depths: 1st, 7th, 15th, and 22nd layers. Results demonstrate the evolution of activation patterns across network depth and illustrate the characteristic input distribution behavior produced by standard Adam optimization in both attention and feed-forward layers.",
                "position": 2217
            },
            {
                "img": "https://arxiv.org/html/2506.19697/x12.png",
                "caption": "Figure 9:Activation distribution visualization of models trained withOSPacross 1 trillion training tokens. The histograms display input activation distributions to Multi-Head Self-Attention (MHSA) and Feed-Forward Network (FFN) layers at four transformer block depths: 1st, 7th, 15th, and 22nd layers. Results demonstrate the evolution of activation patterns across network depth and illustrate the distinctive input distribution characteristics achieved through theOSPframework in both attention and feed-forward layers.",
                "position": 2222
            },
            {
                "img": "https://arxiv.org/html/2506.19697/x13.png",
                "caption": "Figure 10:Weight distribution visualization of models trained with Adam optimizer across 1 trillion training tokens. The histograms display weight distributions within Multi-Head Self-Attention (MHSA) and Feed-Forward Network (FFN) layers at four transformer block depths: 1st, 7th, 15th, and 22nd layers. Results illustrate the evolution of weight distributions across network depth and demonstrate the characteristic patterns produced by standard Adam optimization in both attention and feed-forward layers.",
                "position": 2227
            },
            {
                "img": "https://arxiv.org/html/2506.19697/x14.png",
                "caption": "Figure 11:Weight distribution visualization of model trained withOSPacross 1 trillion training tokens. The histograms display weight distributions within Multi-Head Self-Attention (MHSA) and Feed-Forward Network (FFN) layers at four transformer block depths: 1st, 7th, 15th, and 22nd layers. Results illustrate the evolution of weight distributions across network depth and demonstrate the characteristic patterns induced by theOSPframework in both attention and feed-forward layers.",
                "position": 2232
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]