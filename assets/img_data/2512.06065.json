[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06065/x1.png",
                "caption": "",
                "position": 123
            },
            {
                "img": "https://arxiv.org/html/2512.06065/x2.png",
                "caption": "Figure 2:In-the-wild video edits produced in real time by EgoEdit’s streaming variant EgoEdit-RT on a single H100 GPU. The model demonstrates strong generalization to out-of-distribution scenarios, producing compelling real-time results suitable for immersive AR experiences. Additional results are presented in Appx.7and the website.",
                "position": 129
            },
            {
                "img": "https://arxiv.org/html/2512.06065/x3.png",
                "caption": "Figure 3:Comparison of EgoEdit and EgoEdit-RT against baselines according to VLM score on EgoEditBench and EditVerseBench[ju2025editverse]. EgoEdit and its real-time variant EgoEdit-RT achieve superior results on egocentric editing tasks and perform competitively with the strongest baselines on general editing tasks. EditVerse is excluded from EgoEditBench as source code is unavailable. Streaming models are indicated in dashed lines.",
                "position": 132
            },
            {
                "img": "https://arxiv.org/html/2512.06065/x4.png",
                "caption": "Figure 4:Architecture of EgoEdit.EgoEdit extends a video generation DiT model for video editing by performing channel-wise concatenation of the source and noisy target video inputs, avoiding the computational overheads of sequence-wise concatenation.",
                "position": 265
            },
            {
                "img": "https://arxiv.org/html/2512.06065/x5.png",
                "caption": "Figure 5:Inference of EgoEdit.EgoEdit performs inference in a streaming fashion. A camera continuously acquires video sequences which are edited by the model in a chunk-by-chunk manner so that the edited video can be served to the user in a watch-as-you-generate fashion. Each blue arrow represents a model forward pass on a single video chunk for the case of a 3 steps model.",
                "position": 327
            },
            {
                "img": "https://arxiv.org/html/2512.06065/x6.png",
                "caption": "Figure 6:Qualitative comparison of EgoEdit and its real time streaming variant EgoEdit-RT against baselines on EgoEditBench. EgoEdit and EgoEdit-RT consistently perform better than their baselines. Note that Señorita-2M uses the first frame from EgoEdit for frame propagation. Additional results are presented in Appx.7and the website.",
                "position": 330
            },
            {
                "img": "https://arxiv.org/html/2512.06065/x7.png",
                "caption": "(a)Source Word Cloud",
                "position": 771
            },
            {
                "img": "https://arxiv.org/html/2512.06065/x7.png",
                "caption": "(a)Source Word Cloud",
                "position": 774
            },
            {
                "img": "https://arxiv.org/html/2512.06065/x8.png",
                "caption": "(b)Target Word Cloud",
                "position": 779
            },
            {
                "img": "https://arxiv.org/html/2512.06065/x9.png",
                "caption": "(c)Prompt Length Distribution",
                "position": 784
            },
            {
                "img": "https://arxiv.org/html/2512.06065/x10.png",
                "caption": "Figure 8:Distribution of most frequent scenarios in EgoEditData according to the original datasets[grauman2024egoexo4d,grauman2022ego4d]categorization.",
                "position": 791
            },
            {
                "img": "https://arxiv.org/html/2512.06065/x11.png",
                "caption": "Figure 9:Exocentric video edits produced in real time by EgoEdit’s streaming variant EgoEdit-RT on a single H100 GPU.",
                "position": 1766
            },
            {
                "img": "https://arxiv.org/html/2512.06065/x12.png",
                "caption": "Figure 10:In-the-wild video edits produced in real time by EgoEdit’s streaming variant EgoEdit-RT on a single H100 GPU.",
                "position": 1769
            },
            {
                "img": "https://arxiv.org/html/2512.06065/x13.png",
                "caption": "Figure 11:Qualitative comparison of EgoEdit and its real-time streaming variant EgoEdit-RT against baselines on EgoEditBench. EgoEdit and EgoEdit-RT consistently perform better than their baselines. Note that Señorita-2M uses the first frame from EgoEdit for frame propagation.",
                "position": 1772
            },
            {
                "img": "https://arxiv.org/html/2512.06065/x14.png",
                "caption": "Figure 12:Qualitative comparison of EgoEdit at different stages of distillation. EgoEdit indicates the original 80 NFEs model, EgoEdit-DMD represents the model after the 4-step DMD distillation, EgoEdit-RT represents the final real-time streaming model obtained after Self Forcing distillation.",
                "position": 1775
            },
            {
                "img": "https://arxiv.org/html/2512.06065/x15.png",
                "caption": "Figure 13:Qualitative comparison of different variants of EgoEdit trained on a data mixture with reduced portions of EgoEditData. Percentages indicate the proportion of unique source video samples in EgoEditData retained for training.",
                "position": 1778
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]