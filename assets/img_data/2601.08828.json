[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08828/x1.png",
                "caption": "Figure 1:Motive.Top.Motion-gradient computation (ยง3.4) has three steps: (1) detect motion with AllTracker; (2) compute motion-magnitude patches; (3) apply loss-space motion masks to focus gradients on dynamic regions.Bottom.Our method (ยง3.2) is made scalable via a single-sample variant with common randomness and a projection, computed for each pair of training and query data, aggregated (ยง3.5) for a final ranking, and eventually used to select fine-tuning subsets.",
                "position": 1147
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08828/fig/fig2.png",
                "caption": "Figure 2:Motion attribution examples.Top: Query clips showing float (left) and roll (right) motions.Middle:\nTop-ranked positive training samples identified byMotivewith high influence scores.Bottom: Negative influence samples with minimal, camera-only motion, or cartoon-style content that conflict with target motions.",
                "position": 1306
            },
            {
                "img": "https://arxiv.org/html/2601.08828/x2.png",
                "caption": "Figure 3:Qualitative Comparisons.We compare four motion scenarios (compress, spin, slide, free fall) across the base model, random selection, and our method. Our approach yields more realistic motion dynamics. Supplementary videos are included.",
                "position": 1347
            },
            {
                "img": "https://arxiv.org/html/2601.08828/x3.png",
                "caption": "Figure 4:Projection dimension analysis.Spearman correlation between projected and full gradients shows rapid improvement with projection dimension, with512512providing a strong trade-off between accuracy and efficiency.",
                "position": 1494
            },
            {
                "img": "https://arxiv.org/html/2601.08828/fig/fig5.png",
                "caption": "Figure 5:Impact of Frame-Length Normalization on Motion Attribution.Comparison of top-ranked samples for floating motion query.Left: With proper frame-length normalization, top samples consistently exhibit floating motion (waves, floating objects, surfing).Right: Without normalization, rankings are biased by video length, resulting in no coherent patterns among top samples.",
                "position": 1505
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ANotation",
        "images": []
    },
    {
        "header": "Appendix BExtended Related Work",
        "images": []
    },
    {
        "header": "Appendix CAdditional Experiments",
        "images": []
    },
    {
        "header": "Appendix DAnalysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08828/x4.png",
                "caption": "(a)Motion Distribution",
                "position": 1942
            },
            {
                "img": "https://arxiv.org/html/2601.08828/x4.png",
                "caption": "(a)Motion Distribution",
                "position": 1945
            },
            {
                "img": "https://arxiv.org/html/2601.08828/x5.png",
                "caption": "(b)Distribution Across Motion Bins",
                "position": 1950
            },
            {
                "img": "https://arxiv.org/html/2601.08828/x6.png",
                "caption": "(a)4DNEX Influence Heatmap",
                "position": 1973
            },
            {
                "img": "https://arxiv.org/html/2601.08828/x6.png",
                "caption": "(a)4DNEX Influence Heatmap",
                "position": 1976
            },
            {
                "img": "https://arxiv.org/html/2601.08828/x7.png",
                "caption": "(b)VIDGEN Influence Heatmap",
                "position": 1981
            }
        ]
    },
    {
        "header": "Appendix EAdditional Method Details",
        "images": []
    },
    {
        "header": "Appendix FAdditional Experiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08828/fig/fig7.png",
                "caption": "Figure 8:Illustration of motion query set.We generate near-realistic video queries with Veo-3 across ten motion categories.\nEach category contains five query videos synthesized with controlled prompts and manually screened for clarity and physical plausibility.",
                "position": 2198
            }
        ]
    },
    {
        "header": "Appendix GDiscussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08828/fig/fig6.png",
                "caption": "Figure 9:Motion overlay visualization.Comparison of original frames and motion overlays for seven video samples across three time points (early, middle, late). The motion overlay demonstrates the spatial weighting of our motion loss: dynamic regions remain visible, while static backgrounds are attenuated to neutral gray.Takeaway:This provides heuristic intuition into what information our motion attribution focuses on: the information in grayer regions, which lack motion, is down-weighted by our method.",
                "position": 2420
            }
        ]
    },
    {
        "header": "Appendix HVisualization",
        "images": []
    }
]