[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.00599/x1.png",
                "caption": "",
                "position": 108
            },
            {
                "img": "https://arxiv.org/html/2501.00599/x2.png",
                "caption": "",
                "position": 108
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.00599/x3.png",
                "caption": "Figure 1:Comparisons with previous general and specialized MLLMs. Our VideoRefer excels in multiple fine-grained regional and temporal video understanding tasks, including basic video object referring, complex video relationship analysis, and video object retrieval.",
                "position": 140
            }
        ]
    },
    {
        "header": "2Preliminary",
        "images": []
    },
    {
        "header": "3VideoRefer Suite",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.00599/x4.png",
                "caption": "Figure 2:A multi-agent data engine for the construction of our VideoRefer-700K.",
                "position": 235
            },
            {
                "img": "https://arxiv.org/html/2501.00599/x5.png",
                "caption": "Figure 3:Model architecture of our VideoRefer for spatial-temporal video object understanding.",
                "position": 274
            },
            {
                "img": "https://arxiv.org/html/2501.00599/x6.png",
                "caption": "Figure 4:Exemplar visual illustration of VideoRefer-Bench.",
                "position": 402
            },
            {
                "img": "https://arxiv.org/html/2501.00599/x7.png",
                "caption": "Figure 5:Data characteristics of VideoRefer-Bench.",
                "position": 417
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.00599/x8.png",
                "caption": "Figure 6:Visual comparisons between our VideoRefer with general GPT-4o and regional video-level Elysium and Artemis. Here we provide detailed illustrations on VideoRefer-BenchDD{}^{\\text{D}}start_FLOATSUPERSCRIPT D end_FLOATSUPERSCRIPT.",
                "position": 741
            }
        ]
    },
    {
        "header": "5Related Works",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.00599/x9.png",
                "caption": "Figure 7:Visualizations of similarity among adjacent object-level token pairs across the temporal dimension. Here, we use cosine similarity as the measurement.",
                "position": 1784
            }
        ]
    },
    {
        "header": "Appendix AMore Qualitative Results",
        "images": []
    },
    {
        "header": "Appendix BAdditional Implemental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.00599/x10.png",
                "caption": "Figure 8:Visual illustrations of the data distribution for each training stage.",
                "position": 1805
            },
            {
                "img": "https://arxiv.org/html/2501.00599/x11.png",
                "caption": "Figure 9:Data distributions of our VideoRefer-700K dataset, encompassing five different data types.",
                "position": 1808
            },
            {
                "img": "https://arxiv.org/html/2501.00599/x12.png",
                "caption": "Figure 10:Visual illustrations of human check process. TP, TN, FP and FN are introduced for the assessment on Reviewer.",
                "position": 1811
            }
        ]
    },
    {
        "header": "Appendix CMore Details of VideoRefer-700K Dataset and Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.00599/x13.png",
                "caption": "Figure 11:A detailed illustrative example of the construction pipeline in our multi-agent data engine.",
                "position": 1833
            },
            {
                "img": "https://arxiv.org/html/2501.00599/x14.png",
                "caption": "Figure 12:Visualization results of VideoRefer across various tasks, including single-object referring, video relationship analysis, complex reasoning, future prediction, video object retrieval, as well as general video understanding and image object understanding.",
                "position": 1946
            },
            {
                "img": "https://arxiv.org/html/2501.00599/x15.png",
                "caption": "Figure 13:Visual samples from our VideoRefer-700 dataset, typical including short descriptions, detailed descriptions, and QA pairs.",
                "position": 1949
            },
            {
                "img": "https://arxiv.org/html/2501.00599/x16.png",
                "caption": "Figure 14:Visual examples of our VideoRefer-Bench, including VideoRefer-BenchDD{}^{\\text{D}}start_FLOATSUPERSCRIPT D end_FLOATSUPERSCRIPTand VideoRefer-BenchQQ{}^{\\text{Q}}start_FLOATSUPERSCRIPT Q end_FLOATSUPERSCRIPT.",
                "position": 1952
            }
        ]
    },
    {
        "header": "Appendix DLimitations",
        "images": []
    }
]