[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Measuring diversity in text-to-image models",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.10547/x1.png",
                "caption": "Figure 1:Evaluating diversity requires specifying both the concept being assessed and the factor of variation to reduce ambiguity in the annotation process.",
                "position": 131
            }
        ]
    },
    {
        "header": "2The three ingredients for diversity evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.10547/x2.png",
                "caption": "Figure 2:Each slice represents a concept, grouped and color-coded by its overall category.",
                "position": 212
            },
            {
                "img": "https://arxiv.org/html/2511.10547/x3.png",
                "caption": "Figure 3:Match with the golden set depending on different set sizes.",
                "position": 232
            },
            {
                "img": "https://arxiv.org/html/2511.10547/x4.png",
                "caption": "(a)The “diverse” golden set.",
                "position": 244
            },
            {
                "img": "https://arxiv.org/html/2511.10547/x4.png",
                "caption": "(a)The “diverse” golden set.",
                "position": 247
            },
            {
                "img": "https://arxiv.org/html/2511.10547/x5.png",
                "caption": "(b)The “non-diverse” golden set.",
                "position": 252
            }
        ]
    },
    {
        "header": "3Our framework in practice",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.10547/figures/03-human-eval/k_alpha_human_evals.png",
                "caption": "(a)Krippendorff’sα\\alpha-reliability.",
                "position": 279
            },
            {
                "img": "https://arxiv.org/html/2511.10547/figures/03-human-eval/k_alpha_human_evals.png",
                "caption": "(a)Krippendorff’sα\\alpha-reliability.",
                "position": 282
            },
            {
                "img": "https://arxiv.org/html/2511.10547/figures/autoeval/pilot.png",
                "caption": "(a)The “diverse” golden set.",
                "position": 353
            },
            {
                "img": "https://arxiv.org/html/2511.10547/figures/autoeval/pilot.png",
                "caption": "(a)The “diverse” golden set.",
                "position": 356
            },
            {
                "img": "https://arxiv.org/html/2511.10547/figures/autoeval/model.png",
                "caption": "(b)Side-by-side model comparisons.",
                "position": 361
            },
            {
                "img": "https://arxiv.org/html/2511.10547/figures/autoeval/model_counts=4.png",
                "caption": "(c)Side-by-side model comparisons with diversity gap>4>4.",
                "position": 366
            },
            {
                "img": "https://arxiv.org/html/2511.10547/figures/autoeval-qualitative/vit_diverse_1.png",
                "caption": "Figure 7:Qualitative results for different autoraters on the T2I annotated dataset, showing two very diverse and two non diverse sets as determined by the ViT-based autorater.",
                "position": 384
            },
            {
                "img": "https://arxiv.org/html/2511.10547/figures/autoeval-qualitative/vit_diverse_2.png",
                "caption": "",
                "position": 394
            },
            {
                "img": "https://arxiv.org/html/2511.10547/figures/autoeval-qualitative/vit_nondiverse_1.png",
                "caption": "",
                "position": 395
            },
            {
                "img": "https://arxiv.org/html/2511.10547/figures/autoeval-qualitative/vit_nondiverse_2.png",
                "caption": "",
                "position": 396
            }
        ]
    },
    {
        "header": "4Related work",
        "images": []
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Ethics Statement",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AHuman evaluation task details",
        "images": []
    },
    {
        "header": "Appendix BHuman evaluation template",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.10547/x6.png",
                "caption": "Figure 9:Examples of human evaluation templates used in the pilot study. In the template variantw/o aspect, only thecategoryis provided. In the variant withcount, an additional question is included for each set, prompting annotators to specify the number of distinct values observed for the target attribute within the corresponding image set. For exact examples see Figs.12-12.",
                "position": 898
            },
            {
                "img": "https://arxiv.org/html/2511.10547/figures/03-human-eval/sec4_wo_aspect.png",
                "caption": "Figure 10:A screenshot of the user interface for one annotation example for the condition \"No aspect\".",
                "position": 901
            },
            {
                "img": "https://arxiv.org/html/2511.10547/figures/03-human-eval/sec4_aspect.png",
                "caption": "",
                "position": 906
            },
            {
                "img": "https://arxiv.org/html/2511.10547/figures/03-human-eval/sec4_count.png",
                "caption": "",
                "position": 909
            }
        ]
    },
    {
        "header": "Appendix CAdditional human evaluation results",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.10547/figures/03-human-eval/counts_histogram.png",
                "caption": "Figure 13:Distribution of all counts annotated by human raters.",
                "position": 922
            }
        ]
    },
    {
        "header": "Appendix DA deep dive on our curated Prompt set generation details",
        "images": []
    },
    {
        "header": "Appendix EAdditional autoevaluation results",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.10547/figures/06-appendix-autoeval/equals_auc.png",
                "caption": "Figure 14:AUC to measure metrics ability to identify sets of equal diversity. It is clear that no metric is particularly effective at differentiating visually similar versus not sets of images.",
                "position": 1046
            },
            {
                "img": "https://arxiv.org/html/2511.10547/figures/autoeval-qualitative/clip_diverse_1.png",
                "caption": "Figure 15:Qualitative results for different models, showing two very diverse and two non diverse sets.",
                "position": 1056
            },
            {
                "img": "https://arxiv.org/html/2511.10547/figures/autoeval-qualitative/clip_diverse_2.png",
                "caption": "",
                "position": 1066
            },
            {
                "img": "https://arxiv.org/html/2511.10547/figures/autoeval-qualitative/clip_nondiverse_1.png",
                "caption": "",
                "position": 1067
            },
            {
                "img": "https://arxiv.org/html/2511.10547/figures/autoeval-qualitative/clip_nondiverse_2.png",
                "caption": "",
                "position": 1068
            },
            {
                "img": "https://arxiv.org/html/2511.10547/figures/autoeval-qualitative/pali_logits_diverse_1.png",
                "caption": "",
                "position": 1079
            },
            {
                "img": "https://arxiv.org/html/2511.10547/figures/autoeval-qualitative/pali_logits_diverse_2.png",
                "caption": "",
                "position": 1080
            },
            {
                "img": "https://arxiv.org/html/2511.10547/figures/autoeval-qualitative/pali_logits_nondiverse_1.png",
                "caption": "",
                "position": 1081
            },
            {
                "img": "https://arxiv.org/html/2511.10547/figures/autoeval-qualitative/pali_logits_nondiverse_2.png",
                "caption": "",
                "position": 1082
            },
            {
                "img": "https://arxiv.org/html/2511.10547/figures/06-appendix-autoeval/pilot_text.png",
                "caption": "(a)Results on the “diverse” golden set.",
                "position": 1135
            },
            {
                "img": "https://arxiv.org/html/2511.10547/figures/06-appendix-autoeval/pilot_text.png",
                "caption": "(a)Results on the “diverse” golden set.",
                "position": 1138
            },
            {
                "img": "https://arxiv.org/html/2511.10547/figures/06-appendix-autoeval/model_counts=4_text.png",
                "caption": "(b)Results on the annotation set, where annotators see count differences>4>4.",
                "position": 1143
            },
            {
                "img": "https://arxiv.org/html/2511.10547/figures/autoeval-ranking/image_embeddings_win_matrices_dists.png",
                "caption": "Figure 20:Model ranking using auto evaluation approaches.Win rate matrices and score distributions for Flux1.1 and Imagen3 using image models to compute embeddings.",
                "position": 1360
            },
            {
                "img": "https://arxiv.org/html/2511.10547/figures/autoeval-ranking/vlm_embeddings_win_matrices_dists_attribute.png",
                "caption": "Figure 21:Model ranking using auto evaluation approaches.Win rate matrices and score distributions for Flux1.1 and Imagen3 using text-conditioned multimodal models to compute embeddings, conditioned on attributes.",
                "position": 1363
            },
            {
                "img": "https://arxiv.org/html/2511.10547/figures/autoeval-ranking/vlm_embeddings_win_matrices_dists_object_attribute.png",
                "caption": "Figure 22:Model ranking using auto evaluation approaches.Win rate matrices and score distributions for Flux1.1 and Imagen3 using text-conditioned multimodal models to compute embeddings, conditioned on objects and attributes.",
                "position": 1366
            },
            {
                "img": "https://arxiv.org/html/2511.10547/figures/autoeval/autoraters_comparison_golden_set.png",
                "caption": "Figure 23:Accuracy of autoraters based on the Gemini model family on the task of comparing diversity of side-by-side sets of 8 images from the golden set. Most recent versions of Gemini perform better in the task, with the v2.5 Flash model surpassing the accuracy of human evaluators.",
                "position": 1389
            }
        ]
    },
    {
        "header": "Appendix FAbsence of a Diversity-Fidelity Trade-off",
        "images": []
    },
    {
        "header": "Appendix GLLM use disclosure",
        "images": []
    }
]