[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07083/x1.png",
                "caption": "",
                "position": 100
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3DataDoP Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07083/x2.png",
                "caption": "(a)Distribution of Translation and Rotation Motion Tags.",
                "position": 270
            },
            {
                "img": "https://arxiv.org/html/2504.07083/x2.png",
                "caption": "(a)Distribution of Translation and Rotation Motion Tags.",
                "position": 273
            },
            {
                "img": "https://arxiv.org/html/2504.07083/x3.png",
                "caption": "(b)Diverse Trajectories.",
                "position": 278
            },
            {
                "img": "https://arxiv.org/html/2504.07083/x4.png",
                "caption": "Figure 3:Our Auto-regressive Generation Model.Our model supports multi-modal inputs and generates trajectories based on these inputs. By treating the task as an auto-regressive next-token prediction problem, the model sequentially generates trajectories, with each new pose prediction influenced by previous camera states and input conditions.",
                "position": 352
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07083/x5.png",
                "caption": "Figure 4:Qualitative Results of Text-conditioned Trajectory Generation.We offer a comparative analysis of text-conditioned trajectory generation in the figure. Our model’s trajectories (color-coded to highlight text alignment) remain stable and closely follow the instructions, while other models exhibit significant jitter or fail to match the instructions well.",
                "position": 622
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07083/x6.png",
                "caption": "Figure 5:Qualitative Results of RGBD & Text-conditioned Generation.This figure compares the impact of incorporating RGBD input on trajectory generation under identical text conditions. While both models generate command-compliant trajectories, the RGBD & Text-conditioned model demonstrates superior scene adaptation by utilizing RGBD data to integrate geometric and contextual constraints.",
                "position": 659
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ADataDoP Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07083/x7.png",
                "caption": "(a)Shot Length.",
                "position": 804
            },
            {
                "img": "https://arxiv.org/html/2504.07083/x7.png",
                "caption": "(a)Shot Length.",
                "position": 807
            },
            {
                "img": "https://arxiv.org/html/2504.07083/x8.png",
                "caption": "(b)Trajectory Scale.",
                "position": 812
            },
            {
                "img": "https://arxiv.org/html/2504.07083/x9.png",
                "caption": "Figure R2:Semantic Filtering.Following the definitions in Sec. 3.2, shots are classified. Leveraging GPT-4o[18], we automate shot categorization intoStatic,Free-Moving, andTracking. Shots categorized asObject/Scene-Centric, common in multi-view datasets, are not considered in films.",
                "position": 880
            },
            {
                "img": "https://arxiv.org/html/2504.07083/x10.png",
                "caption": "Figure R3:Tag Distribution.The distribution of Translation and Rotation combinations is shown in the figure. Different tag modes are represented by shades of yellow, ranging from deep to light: Static, Translation only, Rotation only, and both Translation and Rotation.",
                "position": 916
            },
            {
                "img": "https://arxiv.org/html/2504.07083/x11.png",
                "caption": "Figure R4:Caption Generation.We structure the motion tags by incorporating context, instructions, constraints, and examples, and then leverage GPT-4o to generateMotioncaptions that describe the camera motion alone. Next, we extract 16 evenly spaced frames from the shots to create a4×4444\\times 44 × 4grid, prompting GPT-4o to consider both the previous caption and the image sequence. This enables GPT-4o to generateDirectorialcaptions that describe the camera movement, the interaction between the camera and scene, and the directorial intent.",
                "position": 923
            }
        ]
    },
    {
        "header": "Appendix BGenDoP Method",
        "images": []
    },
    {
        "header": "Appendix CExperiments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]