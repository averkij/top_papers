[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.21817/VITA-E/pic/vita-e-demo.png",
                "caption": "Figure 1:VITA-E is capable of handling a variety of complex interactive scenarios, including nearly real-time concurrency and interruption. Please see our demo video at thisYouTube link.",
                "position": 82
            },
            {
                "img": "https://arxiv.org/html/2510.21817/x1.png",
                "caption": "Figure 2:VITA-E’s responses and actions in various interactive scenarios and instructions.",
                "position": 85
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3VITA-E System Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.21817/x2.png",
                "caption": "Figure 3:The logical architecture and operational states of the active model in our dual-model VITA-E framework. Each of the two models can switch between “Active” and “Standby” states. When a model becomes active, the VLM part acts as a controller, processing user inputs in the “Hearing” state and generating special tokens that can trigger a transition to the “Action” state, where it collaborates with an action expert as a whole VLA model. For example, when the VLM’s output starts with the[ACT]token, the text between the[ACT]token and the[INST]token will be played as audio, and the text after the[INST]token will be sent to the VLA model as an action instruction. Otherwise, the text after the special token will only be played as audio, and the system will execute the command corresponding to the special token. For the detailed functions of each special token, see Table1.",
                "position": 163
            },
            {
                "img": "https://arxiv.org/html/2510.21817/x3.png",
                "caption": "Figure 4:Sequence diagrams illustrating the four primary interaction modes. Model I is the Active Model, and Model II is the Standby Model. V and A represent voice and action generation, respectively. The Standby Model can process new requests in parallel or preempt the Active Model to handle interruptions and task switches.",
                "position": 318
            }
        ]
    },
    {
        "header": "4Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.21817/x4.png",
                "caption": "Figure 5:Success rate comparison of VITA-E and GR00T on the Libero benchmark. Although the success rates of VITA-E on this benchmark are not as good as the baseline with the same structure, we emphasize that the goal of this work is not for VITA-E to achieve the state-of-the-art experimental performance, but to prove that the model is capable of completing embodied tasks and to provide quantitative metrics.",
                "position": 360
            },
            {
                "img": "https://arxiv.org/html/2510.21817/x5.png",
                "caption": "Figure 6:Success rate comparison of VITA-E and baseline models on two fundamental manipulation tasks: (a) Pick up can and (b) Pick and place toy. Results are reported over 30 evaluation trials.",
                "position": 369
            }
        ]
    },
    {
        "header": "5Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Appendix",
        "images": []
    }
]