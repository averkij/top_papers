[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18609/extracted/6083489/figures/teaser.png",
                "caption": "Figure 1:Model performance on MSVD-QA versus the model size of the visual component in logarithmic scale. The bubble size indicates the amount of finetuning data (in thousands). Models using the same training dataset as ours (100K samples) are shown in dark green, while those using different datasets are in blue.",
                "position": 81
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18609/extracted/6083489/figures/designs.png",
                "caption": "Figure 2:Existing video-language model architectures:From left to right: Early approaches use image encoders for both image and video inputs. The alignment module aligns the embeddings of the visual modality with the language modality. The integration of Q-Former improved this alignment. Instead of a single encoder, dual encoder approaches have separate encoders for images and videos where the alignment block consists of projection layers. The additional encoders, however, make these models very heavy where the alignment module and encoders have at least 300M and sometimes over 1B parameters. In contrast, our encoder-free design (rightmost) directly processes video inputs through a novel spatio-temporal alignment block (STAB). It eliminates the need for heavyweight pretrained encoders and requires less than 50M parameters.",
                "position": 133
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18609/extracted/6083489/figures/main_arch.png",
                "caption": "Figure 3:Detailed architecture of our Spatio-Temporal Alignment Block (STAB):The input video is first converted into patches. The Local Spatio-Temporal Encoding (LSTE) uses 3D convolutions to model spatio-temporal relations and adds a 3D convolution dynamic position encoding (DPE) to encode position with respect to the local spatio-temporal window. As a result, we obtain per-frame tokens with positional encoding. The tokens are then processed in two ways. While the Global Spatio-Temporal Relationship Aggregator (GSTRA) at the top captures video-level context, the Frame-wise Spatial Relationship Aggregator (FSRA) at the bottom captures spatial context within each frame. To reduce the cost, we perform a Local Spatial Downsampling (LSD) to reduce the spatial dimension for each token. The video-level context tokens and the frame-wise spatial tokens are then linearly combined through learnable weighted fusion (Œ±ùõº\\alphaitalic_Œ±), producing a frame-specific context token. These context tokens are then prepended to their respective frame‚Äôs flattened spatial tokens, with<row>split tokens inserted to demarcate row boundaries in the spatial layout. This combination of global context and preserved spatial structure enables effective video understanding while maintaining computational efficiency.",
                "position": 160
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18609/extracted/6083489/figures/qualitative.png",
                "caption": "Figure 4:Qualitative examples showing the impact of removing Frame-wise Spatial Relationship Aggregator (FSRA) and Global Spatio-Temporal Relationship Aggregator (GSTRA).",
                "position": 933
            }
        ]
    },
    {
        "header": "5Analysis and Ablations",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetailed Hyperparameters",
        "images": []
    },
    {
        "header": "Appendix BAdditional Ablations",
        "images": []
    },
    {
        "header": "Appendix CAdditional Dataset Details",
        "images": []
    },
    {
        "header": "Appendix DAdditional Qualitative Ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18609/extracted/6083489/figures/appendix_qualitative.png",
                "caption": "Figure 5:Comparative analysis of Video-Panda qualitative ablation studies:The figure presents eight video examples with ground truth (GT) annotations and model predictions under different training configurations. The top row demonstrates: the effect of 702K training samples in stage 1, and the impact of performing Local Spatial Downsampling (LSD) before Local Spatial-Temporal Encoding (LSTE). The second row shows results from removing LSD while using: average pooling, half-resolution and perceiver resampler (third row left). The third row right and bottom row illustrate the effects of different teacher models: using Intern-Video and CLIP, and utilizing DINOv2 for knowledge distillation. Each example includes the original model prediction (yellow) and an ablated version (purple), highlighting how architectural and training choices affect Video-Panda‚Äôs ability to interpret dynamic visual scenes and answer questions accurately.",
                "position": 1897
            }
        ]
    },
    {
        "header": "Appendix EEVE Baseline for Videos",
        "images": []
    },
    {
        "header": "Appendix FBroader Impact",
        "images": []
    }
]