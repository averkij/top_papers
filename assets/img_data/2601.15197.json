[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Motivation: The Illusion of Instruction Following",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.15197/x1.png",
                "caption": "Figure 1:Examples of the vision shortcut in RoboCasa(Nasirianyet al.,2024). Training data exhibits visual diversity but limited task diversity. As a result, the model learns to execute tasks directly based on specific visual cues rather than relying on language instructions.",
                "position": 208
            },
            {
                "img": "https://arxiv.org/html/2601.15197/x2.png",
                "caption": "Figure 2:(a) In LIBERO Goal(Liuet al.,2023), the same scene corresponds to multiple tasks, revealing the ambiguity that vision-only models fail to resolve. (b) Action loss curves on BridgeDataV2(Walkeet al.,2023)show that the vision-only model achieves comparable training loss to the full vision-language model, indicating the presence of visual shortcuts even in diverse, in-the-wild datasets.",
                "position": 227
            }
        ]
    },
    {
        "header": "3Method:BayesianVLA",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.15197/x3.png",
                "caption": "Figure 3:The framework ofBayesianVLA. The framework employs a dual-branch architecture with shared VLM weights. The Priori Branch (left) processes[v,ùí¨,‚Ñì][v,\\mathcal{Q},\\ell]with causal masking to learn the vision-only priorp‚Äã(a‚à£v)p(a\\mid v). The Posteriori Branch (right) processes[v,‚Ñì,ùí¨][v,\\ell,\\mathcal{Q}]to learn the full policyœÄ‚Äã(a‚à£v,‚Ñì)\\pi(a\\mid v,\\ell). Latent Action Queriesùí¨\\mathcal{Q}serve as a bottleneck interface, and the LLR objective (in Eq.7) encourages the model to maximize the information between actions and instructions. At inference, only the Posteriori Branch is used, incurring no additional computational overhead.",
                "position": 271
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.15197/x4.png",
                "caption": "Figure 4:Qualitative comparison of general multimodal reasoning.We present a case where the model is asked to solve a mathematical problem. The standard VLA baseline (QwenGR00T) suffers from catastrophic forgetting; while the text before the comma implies ‚Äúdifferentiating all terms together‚Äù, the subsequent output degenerates into repetitive and meaningless gibberish (bottom right). In contrast,BayesianVLA(top right) retains the VLM‚Äôs original reasoning and language generation capabilities (left), successfully solving the problem.",
                "position": 909
            },
            {
                "img": "https://arxiv.org/html/2601.15197/x5.png",
                "caption": "Figure 5:Additional qualitative comparison.Demonstrating the preservation of general VLM capabilities on another example.",
                "position": 918
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitation and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADerivation of the LLR Objective",
        "images": []
    }
]