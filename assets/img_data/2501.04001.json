[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04001/x1.png",
                "caption": "",
                "position": 75
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04001/x2.png",
                "caption": "Figure 2:Our proposed Sa2VA model.The model first encodes the input texts, visual prompts, images, and videos into token embeddings. These tokens are then processed through a large language model (LLM). The output text tokens are used to generate the [SEG] token and associated language outputs. The SAM-2 decoder receives the image and video features from the SAM-2 encoder, along with the [SEG] token, to generate corresponding image and video masks.",
                "position": 825
            },
            {
                "img": "https://arxiv.org/html/2501.04001/x3.png",
                "caption": "Figure 3:Data annotation pipeline. Our proposed automatic data annotation pipeline consists of three stages: object/part-level, scene-level, and video-level text expression annotation. We use different colors in the final expression to highlight the information derived from each stage. Best view on screen and zoom out.",
                "position": 859
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04001/x4.png",
                "caption": "Figure 4:The samples of our Ref-SAV benchmark. Our proposed benchmark features multi-granularity, complex occlusion and reappearing, and both short and long-format text expressions.",
                "position": 1779
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Appendix",
        "images": []
    },
    {
        "header": "Appendix AMore Experiment Results",
        "images": []
    },
    {
        "header": "Appendix BModel Details and Implementation Details",
        "images": []
    },
    {
        "header": "Appendix CMore Details on Ref-SAV Dataset",
        "images": []
    },
    {
        "header": "Appendix DVisualization Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04001/x5.png",
                "caption": "Figure 5:Visualization results on image referring segmentation task.",
                "position": 2410
            },
            {
                "img": "https://arxiv.org/html/2501.04001/x6.png",
                "caption": "Figure 6:Visualization results on video referring segmentation.",
                "position": 2417
            },
            {
                "img": "https://arxiv.org/html/2501.04001/x7.png",
                "caption": "Figure 7:Visualization results on visual prompt understanding task. We use the masks predicted by our model under the GCG task as visual prompts, and generated region-level descriptions for these masks. The object masks and their captions for the corresponding region are highlighted in the same color.",
                "position": 2424
            },
            {
                "img": "https://arxiv.org/html/2501.04001/x8.png",
                "caption": "Figure 8:Visualization results on GCG tasks. Top: our method. Bottom: OMG-LLaVA[99]. Note that, our method has stronger and fined-grained grounding ability and text alignment than OMG-LLaVA[99], previous strong baseline.",
                "position": 2431
            },
            {
                "img": "https://arxiv.org/html/2501.04001/x9.png",
                "caption": "Figure 9:Visualization failure cases.",
                "position": 2437
            }
        ]
    },
    {
        "header": "Appendix EFailure Cases and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]