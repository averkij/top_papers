[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.09342/x1.png",
                "caption": "Figure 1:Overview of the proposed LLM√ó\\times√óMapReduce framework. After dividing the provided long text into a series of chunks, the model processes each chunk to extract an information structure containing the essential content needed to address the query. This is referred to as the map stage in our framework. The mapped results are then compressed during the collapse stage, preparing them for the reduce stage. The structure of the collapsed results mirrors that of the mapped results. The collapse stage ensures that the input to the reducing model remains within its effective length (i.e.,LùêøLitalic_L). Based on the structured outputs from the first two stages (i.e., the map and collapse stages), the reduce model aggregates information from all chunks, resolves inter-chunk conflicts using calibrated confidence scores, and predicts the final answer.",
                "position": 167
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.09342/x2.png",
                "caption": "Figure 3:Performance of Llama3-70B-Instruct√ó\\times√óMapReduce on the NIAH test, with the maximum length of the haystack set to 1280K tokens.",
                "position": 555
            },
            {
                "img": "https://arxiv.org/html/2410.09342/x3.png",
                "caption": "Figure 4:Comparison of inference latency. ‚ÄúL3-70B-G‚Äù represents Llama3-70B-Instruct-Gradient-1048K.",
                "position": 568
            }
        ]
    },
    {
        "header": "4Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]