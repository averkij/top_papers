[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16419/x1.png",
                "caption": "Figure 1:The pipeline of developing efficient reasoning for LLMs. A reasoning model can be trained on the base model using SFT, RL, or a combination of both. While reasoning models demonstrate strong reasoning capabilities, they often suffer from the “overthinking phenomenon”, generating unnecessarily lengthy reasoning steps. To improve efficiency, various methods can be applied to reduce redundant steps while maintaining accuracy, or to fine-tune non-reasoning models to incorporate efficient reasoning capabilities. This approach enables the model to answer questions with concise and effective reasoning steps. In this paper, we explore the latest progress in efficient reasoning for LLMs, aiming to provide insights that can guide future research and the development of reasoning-driven applications across various domains.",
                "position": 172
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16419/x2.png",
                "caption": "Figure 2:Overview of efficient reasoning methods, which can be summarized as model-oriented (Left:I,II) and reasoning output-oriented (Middle:III,IV), and input prompts-oriented (Right:V, VI) methods. Specifically, (I) Reinforcement Learning with Length Reward Design (Section3.1); (II) Supervised Fine-Tuning with Variable-Length CoT Data (Section3.2); (III) Compressing Reasoning Steps into Fewer Latent Representation (Section4.1); (IV) Dynamic Reasoning Paradigm during Inference (Section4.2); (V) Prompt-guided Efficient Reasoning (Section5.1); (VI) Routing Prompts to Optimize Reasoning Efficiency (Section5.2);",
                "position": 269
            }
        ]
    },
    {
        "header": "2Background: Long CoT Reasoning Models and Overthinking Phenomenon",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16419/x3.png",
                "caption": "Figure 4:An example of the “overthinking phenomenon”: when the reasoning model is asked“Which is larger, 0.9 or 0.11?”, it takes an unnecessarily long time (i.e., 19 seconds for QwQ-32B[79]and 42 seconds for DeepSeek-R1[31]) to deliver its final answer. The example was tested in March 2025.",
                "position": 432
            }
        ]
    },
    {
        "header": "3Model-based Efficient Reasoning",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16419/extracted/6297489/figs/bulb.png",
                "caption": "",
                "position": 683
            },
            {
                "img": "https://arxiv.org/html/2503.16419/x4.png",
                "caption": "Figure 5:Illustration of the method for RL fine-tuning with length reward designs. In principle, the length reward assigns higher rewards to short, correct answers and penalizes lengthy or wrong answers to achieve efficient reasoning LLMs.",
                "position": 687
            },
            {
                "img": "https://arxiv.org/html/2503.16419/x5.png",
                "caption": "Figure 6:Illustration of methods for utilizing SFT with variable-length CoT reasoning datasets.",
                "position": 715
            }
        ]
    },
    {
        "header": "4Reasoning Output-based Efficient Reasoning",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16419/x6.png",
                "caption": "Figure 7:Comparison of methods of compressing reasoning steps into fewer latent representations.",
                "position": 1040
            },
            {
                "img": "https://arxiv.org/html/2503.16419/x7.png",
                "caption": "Figure 8:Examples of efficient Best-of-N sampling methods.(Left) Speculative Rejection[76]uses a reward model to estimate partial generation quality. It then early stops the sampled sequence with lower scores.(Right) ST-BoN[85]evaluates the latent embedding of the early generation. The latent embedding of each thinking path will be used to calculate pairwise consistency between other tokens. The sequence with the highest consistency is more likely to arrive at the correct answer.",
                "position": 1377
            }
        ]
    },
    {
        "header": "5Input Prompts-based Efficient Reasoning",
        "images": []
    },
    {
        "header": "6Reasoning Abilities via Efficient Training Data and Model Compression",
        "images": []
    },
    {
        "header": "7Evaluation and Benchmark",
        "images": []
    },
    {
        "header": "8Applications and Discussion",
        "images": []
    },
    {
        "header": "9Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]