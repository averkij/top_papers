[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Task Formulation",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.15715/x1.png",
                "caption": "Figure 1:Overview of our RebuttalAgent framework. First, we extract each comment from raw reviews and retrieves their relevant context from the paper. Next, based on our TSR pipeline, we collect a tailored strategy and response for each comment, grounded in Theory of Mind. Finally, our RebuttalAgent is trained via Supervised Fine-Tuning, followed by Reinforcement Learning with a self-reward mechanism, enabling both scalability and self-improvement.",
                "position": 182
            }
        ]
    },
    {
        "header": "3Data Preparation",
        "images": []
    },
    {
        "header": "4ToM-Strategy-Response Framework",
        "images": []
    },
    {
        "header": "5Agent Training for Strategic Persuasion",
        "images": []
    },
    {
        "header": "6Rebuttal-RM as Judge",
        "images": []
    },
    {
        "header": "7Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.15715/x2.png",
                "caption": "Figure 2:Performance of base models when augmented with the ToM analysis and Strategy generated by our model.",
                "position": 1112
            }
        ]
    },
    {
        "header": "8Related Work",
        "images": []
    },
    {
        "header": "9Conclusion",
        "images": []
    },
    {
        "header": "Ethical Consideration",
        "images": []
    },
    {
        "header": "Reproducibility statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALLM usage",
        "images": []
    },
    {
        "header": "Appendix BData Preparation",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.15715/figure/image.png",
                "caption": "Figure 3:Heatmap for retrieval effectiveness",
                "position": 2018
            }
        ]
    },
    {
        "header": "Appendix CDistribution of Reviews and Comments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.15715/x3.png",
                "caption": "Table 4:Dimensions of the Hierarchical Reviewer Profile. The complete list of categories, along with a visualization of the data distribution for reviews and comments, is provided in AppendixC.",
                "position": 2025
            },
            {
                "img": "https://arxiv.org/html/2601.15715/x3.png",
                "caption": "Figure 4:Comparative Evaluation of Model Performance on Rebuttal Quality.",
                "position": 2132
            }
        ]
    },
    {
        "header": "Appendix DInstruction for SFT with output format example",
        "images": []
    },
    {
        "header": "Appendix EInstruction for SFT scoring model with output format example",
        "images": []
    },
    {
        "header": "Appendix FPrompt for reviewer stance modeling",
        "images": []
    },
    {
        "header": "Appendix GPrompt for Rdiv",
        "images": []
    },
    {
        "header": "Appendix HPrompt for Rthink",
        "images": []
    },
    {
        "header": "Appendix IExamples for performance of base model vs base model with TSR",
        "images": []
    },
    {
        "header": "Appendix JDetails for SFT",
        "images": []
    },
    {
        "header": "Appendix KDetails for RebuttalRM SFT",
        "images": []
    },
    {
        "header": "Appendix LDetails for RL stages",
        "images": []
    },
    {
        "header": "Appendix MCase Study",
        "images": []
    }
]