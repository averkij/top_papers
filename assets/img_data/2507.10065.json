[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.10065/x1.png",
                "caption": "Figure 1:Overview.MoVieSconsists of a shared image encoder, an attention-based feature backbone (Section3.2.1), and three heads (Section3.2.2) that simultaneously predict appearance, geometry and motion. Image shortcut for splatter head and time-varying Gaussian attributes are omitted for brevity.",
                "position": 200
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.10065/x2.png",
                "caption": "Figure 2:Motion Head.\nGiventqsubscriptùë°ùëût_{q}italic_t start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPTtarget timesteps, the proposed motion head is conditioned via adaptive layer normalization (AdaLN) and predicts 3D displacements for each input pixel. After rasterization using theMùëÄMitalic_Mcorresponding query-time cameras, output images in shapeM√ó3√óH√óWùëÄ3ùêªùëäM\\times 3\\times H\\times Witalic_M √ó 3 √ó italic_H √ó italic_Ware rendered for supervision. Gaussian attribute deformationŒî‚Å¢ùêöŒîùêö\\Delta\\mathbf{a}roman_Œî bold_ais omitted for brevity.",
                "position": 288
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.10065/x3.png",
                "caption": "Figure 3:Novel View Synthesis for Dynamic Scenes. Given a monocular video, we compare synthesized views from a novel view across different methods. Regions invisible in the input are rendered as black or white, depending on the rendering implementation. More results in Figure6.",
                "position": 610
            },
            {
                "img": "https://arxiv.org/html/2507.10065/x4.png",
                "caption": "Figure 4:Motion Visualization for Ablation Studies. We investigate key factors affecting motion learning inMoVieS, such as loss design and synergy with view synthesis. XYZ values in motion maps are normalized as RGB for visualization. Red arrows on video frames indicate motion directions.",
                "position": 901
            },
            {
                "img": "https://arxiv.org/html/2507.10065/x5.png",
                "caption": "Figure 5:Zero-shot Applications. The predicted pixel-aligned motion maps from our model can be directly applied to downstream tasks, such as (a) scene flow estimation and (b) moving object segmentation, in azero-shotmanner, without any task-specific fine-tuning or supervision.",
                "position": 1144
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BDiscussion",
        "images": []
    },
    {
        "header": "Appendix CMore Visualization Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.10065/x6.png",
                "caption": "Figure 6:Qualitative results for reconstruction. ‚ÄúMotion‚àó‚Äù means the 3D points‚Äô movements of the input frame with respect to the first one.",
                "position": 2593
            },
            {
                "img": "https://arxiv.org/html/2507.10065/x7.png",
                "caption": "Figure 7:Qualitative results for 3D point tracking. ‚ÄúMotion‚àó‚Äù means the 3D points‚Äô movements of the input frame with respect to the first one.",
                "position": 2596
            },
            {
                "img": "https://arxiv.org/html/2507.10065/x8.png",
                "caption": "Figure 8:More results for scene flow estimation. ‚ÄúFlow‚àó‚Äù means the optical flow of the input pixels with respect to the first frame. Arrows on pictures roughly mark directions.",
                "position": 2599
            },
            {
                "img": "https://arxiv.org/html/2507.10065/x9.png",
                "caption": "Figure 9:More results for moving object segmentation. Masks for moving parts, which are filtered from motion maps, are highlighted across frames in light red.",
                "position": 2602
            }
        ]
    },
    {
        "header": "Appendix DLicense Information",
        "images": []
    }
]