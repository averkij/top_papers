[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.18712/extracted/6305680/Figures/llavaction1.png",
                "caption": "",
                "position": 150
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.18712/x1.png",
                "caption": "Figure 1:LLaVAction-7B.Top: Qualitative inspection of distractors. We show an example clip with labels from random choices (which empirically is easy to solve), vs. our proposed harder benchmark with action labels generated by TIM[7]. Our hard example generation strategy can\nautomatically explore challenges such as temporal order and similar objects that are emphasized in other benchmarks. Bottom: Note, while GPT-4o is very good at random choices, it suffers in the harder benchmarking regime, and our method, LLaVAction outperforms the GPT-4o models.",
                "position": 168
            },
            {
                "img": "https://arxiv.org/html/2503.18712/x2.png",
                "caption": "Figure 2:LLaVAction-7B pipeline.Our full model includes an action token and additional auxiliary visual tasks, as noted. Inputs are shown for a given video clip. The responses are from the direction prediction, GPT-4o distillation, and the adversarial MQA.",
                "position": 213
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.18712/x3.png",
                "caption": "Table 1:Methods: Comparison between different sources of distractors on EPIC-KITHCENS-100-MQA.Models were evaluated on either random, AVION-generated, or TIM-generated distractors. The values are reported as percent accuracy. Note, the Random does not include action-recognition model-generated distractors (thus is ‘easy’), while TIM produced the hardest distractors. Data are based on the validation set of EPIC-KITCHENS-100. An 8 frame sequence is uniformly sampled from annotated video clips.",
                "position": 329
            },
            {
                "img": "https://arxiv.org/html/2503.18712/x3.png",
                "caption": "Figure 3:Qualitative results.LLaVAction-7B consistently outperforms GPT-4o and LLaVA-Video-7B when tested on hard distractors. Bold option denotes ground truth, and the icons denote the selection of the models. See also Appendix12.",
                "position": 470
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Results",
        "images": []
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Methods: Licensing information",
        "images": []
    },
    {
        "header": "8Methods: GPT-4o distillation.",
        "images": []
    },
    {
        "header": "9Methods: LLaVAction task prompts",
        "images": []
    },
    {
        "header": "10Leave-one-out Ablation",
        "images": []
    },
    {
        "header": "11Additional Benchmarks Descriptions.",
        "images": []
    },
    {
        "header": "12Qualitative examples",
        "images": []
    },
    {
        "header": "13Sub-category performance comparisons on the additional benchmarks",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.18712/x4.png",
                "caption": "Figure 4:Official keys vs. narrations.Blue option denotes ground truth and the pink option denotes LLaVAction-7B’s prediction. Official keys usually reduce multiple nouns into one noun, resulting in ambiguity that could mislead a MLLM. Note that the narration also contains crucial particles with the phrasal verbs to clarify the meaning such as “put down”, “put into”.",
                "position": 3069
            },
            {
                "img": "https://arxiv.org/html/2503.18712/x5.png",
                "caption": "Figure 5:TIM’s choices are harder than AVION and randomby introducing more visually similar objects and actions. Bold option denotes ground truth.",
                "position": 3072
            },
            {
                "img": "https://arxiv.org/html/2503.18712/x6.png",
                "caption": "Figure 6:The captioning capability can provide insights into models.A comparison among LLaVAction-7B, GPT-4o and LLaVA-Video-7B. Although mainly trained with MQA data, our LLaVAction-7B still retains the video caption ability and can generate plausible descriptions of the video.",
                "position": 3075
            },
            {
                "img": "https://arxiv.org/html/2503.18712/extracted/6305680/Figures/analysis.jpg",
                "caption": "Figure 7:Breakdown of the performance of LLaVAction-7B on Verbs and Nouns.We analyzed the accuracy per verb and noun in EPIC-KITCHENS-100 for that our LLaVAction-7B (32f), evaluated on the validation set with official keys (i.e., the model reported in Table4that achieved 58.3 accuracy). There are more nouns than verbs, thus nouns are shown across four subplots for visualization but otherwise are not separated in an intentional way. The number above each bar is the total per class.",
                "position": 3078
            }
        ]
    },
    {
        "header": "14Extended Discussion",
        "images": []
    }
]