[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22281/x1.png",
                "caption": "Figure 1:We present MesaTask, a novel LLM-based framework for generating task-oriented 3D tabletop scenes directly from high-level human instructions, featuring realistic layouts, articulated objects, and complex inter-object relations like stacking and containment.\nTo support this task, we introduce a large-scale dataset of tabletop scenes, MesaTask-10K, comprising over12,00012,0003D assets,11,70811,708tabletop scenes with manually crafted layouts covering 6 common indoor table types.",
                "position": 196
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3MesaTask-10K Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22281/x2.png",
                "caption": "Figure 2:The dataset construction pipeline.First, an LLM is used to generate diverse tabletop scene descriptions, including relevant object lists and spatial relations. Conditioned on the scene description, a text-to-image model synthesizes reference images, from which coarse 3D layouts are built using depth estimation, object detection, and 3D asset retrieval. These layouts are refined through human annotations and physical simulation to ensure spatial plausibility.",
                "position": 307
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22281/x3.png",
                "caption": "Figure 3:Overview of our MesaTask Framework.1)Task-to-Scene Generation (upper-left). Given a task instruction, we extract detailed task information including environment, sub-goals, and task-relevant objects. A structured spatial reasoning chain performs object list completion, interrelation inference, and scene graph construction, which guides the generation of 3D layouts. Final scenes are obtained via 3D asset retrieval.2)Reasoning Data Construction (bottom). Based on scene graphs and descriptions of our MesaTask-10K dataset, A multimodal LLM is leveraged to produce task instructions, detailed task information, and complete object lists and interrelations.3)DPO Data Construction (upper right). To enable DPO training, we generate negative examples by randomly perturbing object positions or relations and removing key objects from normal layouts.",
                "position": 360
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22281/x4.png",
                "caption": "Figure 4:Qualitative comparisonunder the same input task descriptions.\nOur proposed method, MesaTask, outperforms all baseline approaches across multiple perspectives, specifically exhibiting enhanced realism, superior task-scene alignment, more plausible tabletop layouts, and improved modeling of complex inter-object relationships.",
                "position": 655
            },
            {
                "img": "https://arxiv.org/html/2509.22281/x5.png",
                "caption": "Figure 5:Ablation studyon DPO training. Compared to the model fine-tuned solely via SFT, the model additionally trained with DPO maintains a lower collision rate (left), higher fidelity to task-related objects (middle), and superior alignment with input task instructions (right).",
                "position": 661
            },
            {
                "img": "https://arxiv.org/html/2509.22281/x6.png",
                "caption": "Figure 6:MesaTask is capable of generating realistic tabletop scenes belonging to novel categories that are not present in the training dataset.",
                "position": 679
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails of MesaTask-10K",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22281/x7.png",
                "caption": "Figure 7:Distribution of the top 100 object categories",
                "position": 1438
            },
            {
                "img": "https://arxiv.org/html/2509.22281/x8.png",
                "caption": "Figure 8:Qualitative results of ATISS, DiffuScene, and PhyScene Trained on MesaTask-10k",
                "position": 1550
            }
        ]
    },
    {
        "header": "Appendix BDetails of MesaTask",
        "images": []
    },
    {
        "header": "Appendix CDetails of experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22281/Figure/userstudy.png",
                "caption": "Figure 9:User study interface",
                "position": 1792
            }
        ]
    },
    {
        "header": "Appendix DMore result",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22281/x9.png",
                "caption": "Figure 10:More qualitative comparisons of task-conditioned scene generation results across GPT-4o, I-Design-table, Holodeck-table, and our proposed MesaTask.",
                "position": 2052
            },
            {
                "img": "https://arxiv.org/html/2509.22281/x10.png",
                "caption": "Figure 11:Additional qualitative results generated by our proposed MesaTask.",
                "position": 2057
            },
            {
                "img": "https://arxiv.org/html/2509.22281/x11.png",
                "caption": "Figure 12:Additional qualitative results generated by our proposed MesaTask (continued).",
                "position": 2062
            },
            {
                "img": "https://arxiv.org/html/2509.22281/x12.png",
                "caption": "Figure 13:Additional qualitative results generated by our proposed MesaTask (continued).",
                "position": 2067
            }
        ]
    },
    {
        "header": "Appendix ELimitation and future work",
        "images": []
    },
    {
        "header": "Appendix FPrompt",
        "images": []
    }
]