[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22496/x1.png",
                "caption": "Figure 1:Eagleattribution which perceptual regions drive the generation (Where MLLMs Attend) and quantifies modality reliance (What They Rely On).",
                "position": 98
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22496/x2.png",
                "caption": "Figure 2:Overview of the proposedEagleframework. The input image is first sparsified into sub-regions, then attributed via greedy search with the designed objective, and finally analyzed for modality relevance between language priors and perceptual evidence.",
                "position": 167
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22496/x3.png",
                "caption": "Figure 3:Visualization of explanation results for LLaVA-1.5, Qwen2.5-VL, and InternVL3.5 on the MS COCO and MMVP datasets.",
                "position": 714
            },
            {
                "img": "https://arxiv.org/html/2509.22496/x4.png",
                "caption": "Figure 4:Visualization of word-level explanation results for LLaVA-1.5, Qwen2.5-VL, and InternVL3.5 on the MS COCO datasets.",
                "position": 963
            },
            {
                "img": "https://arxiv.org/html/2509.22496/x5.png",
                "caption": "Figure 5:Hallucination attribution on RePOPE. Our method produces sparse, focused maps that more accurately reveal regions responsible for hallucinated outputs, compared with IGOS++ and TAM.",
                "position": 1302
            }
        ]
    },
    {
        "header": "5Conclusion and Limitation",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALLM Usage",
        "images": []
    },
    {
        "header": "Appendix BEagleAlgorithm",
        "images": []
    },
    {
        "header": "Appendix CAdditional Experimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22496/x6.png",
                "caption": "Figure 6:Sentence-level explanation results forLLaVA-1.5on the MS COCO dataset. Our method consistently identifies semantically critical regions that align with highlighted tokens in the caption, while baseline methods either fail to capture relevant areas (LLaVA-CAM) or over-highlight irrelevant background regions (IGOS++).",
                "position": 2002
            },
            {
                "img": "https://arxiv.org/html/2509.22496/x7.png",
                "caption": "Figure 7:Sentence-level explanation results forLLaVA-1.5on the MMVP dataset. Compared to the baselines, our method highlights regions that are directly related to the VQA queries, resulting in explanations that are more interpretable and trustworthy.",
                "position": 2008
            },
            {
                "img": "https://arxiv.org/html/2509.22496/x8.png",
                "caption": "Figure 8:Sentence-level explanation results forQwen2.5-VLon the MS COCO dataset. Our method highlights critical objects with strong correspondence to the generated captions, reducing redundancy in comparison to IGOS++.",
                "position": 2011
            },
            {
                "img": "https://arxiv.org/html/2509.22496/x9.png",
                "caption": "Figure 9:Sentence-level explanation results forQwen2.5-VLon the MMVP dataset. Our method improves alignment between highlighted visual regions and VQA-relevant words, enhancing interpretability.",
                "position": 2017
            },
            {
                "img": "https://arxiv.org/html/2509.22496/x10.png",
                "caption": "Figure 10:Sentence-level explanation results forInternVL3.5on the MS COCO dataset. Our method captures object-centric regions more consistently than baseline methods.",
                "position": 2020
            },
            {
                "img": "https://arxiv.org/html/2509.22496/x11.png",
                "caption": "Figure 11:Sentence-level explanation results forInternVL3.5on the MMVP dataset. Our approach ensures strong consistency between highlighted evidence and the VQA queries.",
                "position": 2026
            },
            {
                "img": "https://arxiv.org/html/2509.22496/x12.png",
                "caption": "Figure 12:Object-level explanation results forLLaVA-1.5on the MS COCO dataset. Bounding box overlays show that our method provides sparse yet highly accurate localization.",
                "position": 2036
            },
            {
                "img": "https://arxiv.org/html/2509.22496/x13.png",
                "caption": "Figure 13:Object-level explanation results forQwen2.5-VLon the MS COCO dataset. Our method produces localized attribution maps with high correspondence to ground-truth bounding boxes.",
                "position": 2039
            },
            {
                "img": "https://arxiv.org/html/2509.22496/x14.png",
                "caption": "Figure 14:Object-level explanation results forInternVL3.5on the MS COCO dataset. Our method captures object-centric highlights with strong correspondence to caption tokens and bounding boxes.",
                "position": 2042
            },
            {
                "img": "https://arxiv.org/html/2509.22496/x15.png",
                "caption": "Figure 15:Hallucination attribution forLLaVA-1.5on the MS COCO dataset. Our method highlights the minimal hallucination-inducing regions across different queries, such as “snowboard,” “traffic light,” and “cup.”",
                "position": 2053
            },
            {
                "img": "https://arxiv.org/html/2509.22496/x16.png",
                "caption": "Figure 16:Hallucination attribution forQwen2.5-VLon the MS COCO dataset. Our method isolates misleading cues leading to hallucinations in queries such as “cell phone,” “bicycle,” and “truck.”",
                "position": 2065
            },
            {
                "img": "https://arxiv.org/html/2509.22496/",
                "caption": "Figure 17:Hallucination attribution forInternVL3.5on the MS COCO dataset. Our method identifies hallucination-prone regions for queries such as “spoon,” “tv,” and “dining table,” especially in cases of overlapping or occluded objects.",
                "position": 2068
            }
        ]
    },
    {
        "header": "Appendix DAdditional Qualitative Results",
        "images": []
    }
]