[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.11305/x1.png",
                "caption": "Figure 1:Needle-in-a-Haystack test performance comparison between Transformer++, Mamba, RWKV-7, and Avey, all using 1.5B parameters. The x-axis denotes the lengths of haystacks (i.e., documents with distractor texts, varying from 2k to 64k tokens) and the y-axis refers to the position of the needle (i.e., a short sentence) within any of the haystacks. A green cell means that the model was able to recall the needle and a red one entails that it could not. Transformer++, Mamba, and RWKV-7 were all trained with 2k-token contexts, while Avey was trained withonlya 512-token context and was able to extrapolate to the maximum needed modeling capacity.",
                "position": 117
            }
        ]
    },
    {
        "header": "2Avey",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.11305/x2.png",
                "caption": "Figure 2:The ranker (left) partitions each input sequence into equal-sized splits and identifies the topkùëòkitalic_kmost relevant ones (e.g., splits 1 and 3 fork=2ùëò2k=2italic_k = 2) with respect to thecurrent split(e.g., split 4), using the MaxSim operator. These top-kùëòkitalic_ksplits are then weighted by their normalized scores, where the normalized score (NS) of a split is computed as the ratio of its MaxSim value to the highest MaxSim score among thekùëòkitalic_ksplits. Finally, the weighted top-kùëòkitalic_ksplits are contextualized together with the current split by the neural processor (right).",
                "position": 183
            },
            {
                "img": "https://arxiv.org/html/2506.11305/x3.png",
                "caption": "Figure 3:The neural processor (top) with its three major components, the enricher, contextualizer (Cx), and fuser. The processor is unfolded into two copies for illustrative purposes only, to show how different embeddings, (e.g.,e1subscriptùëí1e_{1}italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTande2subscriptùëí2e_{2}italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, or more precisely, parts of their tails, i.e.,e122subscriptùëí122e_{122}italic_e start_POSTSUBSCRIPT 122 end_POSTSUBSCRIPTande222subscriptùëí222e_{222}italic_e start_POSTSUBSCRIPT 222 end_POSTSUBSCRIPT) are contextualized by Cx (i.e., in reality, all components are shared across all embeddings and many embeddings can be input to Cx simultaneously).",
                "position": 217
            },
            {
                "img": "https://arxiv.org/html/2506.11305/x4.png",
                "caption": "Figure 4:The Time to First Token (TTFT) for Avey, Transformer++, Mamba, and RWKV-7 using‚àºsimilar-to\\sim‚àº1.5B parameters, across varying sequence lengths. See Section3.1for details on the experimental methodology.",
                "position": 304
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.11305/x5.png",
                "caption": "Figure 5:Performance comparison between Transformer++, Mamba, RWKV-7, and Avey on S-NIAH-1 and S-NIAH-2. The x-axis denotes the lengths of haystacks (i.e., documents with distractor texts, varying from 2k to 64k tokens). All models use 0.5B parameters. Similar results are shown in AppendixMfor other model sizes.",
                "position": 727
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperimental Methodology",
        "images": []
    },
    {
        "header": "Appendix BActivation or No Activation in the Enricher",
        "images": []
    },
    {
        "header": "Appendix CActivation or No Activation in the Contextualizer",
        "images": []
    },
    {
        "header": "Appendix DWhat is the Best Expansion Factor?",
        "images": []
    },
    {
        "header": "Appendix EWhat is the Best Tail Size?",
        "images": []
    },
    {
        "header": "Appendix FDeeper Models and Narrower Embeddings, or Shallower Models and Wider Embeddings",
        "images": []
    },
    {
        "header": "Appendix GWhat are the Best Sequence Length, Split Size, and Top-kùëòkitalic_kValues?",
        "images": []
    },
    {
        "header": "Appendix HRMSNorm or LayerNorm",
        "images": []
    },
    {
        "header": "Appendix ILearning Rate: To Decay or Not to Decay?",
        "images": []
    },
    {
        "header": "Appendix JScaling Laws",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.11305/x6.png",
                "caption": "Figure 6:Scaling law results, comparing how perplexity decreases as compute increases, assuming three model sizes of 150M, 500M, and 1.5B parameters and a proportional increase in the number of training tokens with model size, following the Chinchilla scaling laws.",
                "position": 4616
            }
        ]
    },
    {
        "header": "Appendix KAblation Study",
        "images": []
    },
    {
        "header": "Appendix LAdditional Short-Range Benchmark Results",
        "images": []
    },
    {
        "header": "Appendix MAdditional Long-Range Benchmark Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.11305/x7.png",
                "caption": "Figure 7:Performance comparison between Transformer++, Mamba, RWKV-7, and Avey on S-NIAH-1 and S-NIAH-2. The x-axis denotes the lengths of haystacks (i.e., documents with distractor texts, varying from 2k to 64k tokens). All models use‚àºsimilar-to\\sim‚àº150M parameters.",
                "position": 5580
            },
            {
                "img": "https://arxiv.org/html/2506.11305/x8.png",
                "caption": "Figure 8:Performance comparison between Transformer++, Mamba, RWKV-7, and Avey on S-NIAH-1 and S-NIAH-2. The x-axis denotes the lengths of haystacks (i.e., documents with distractor texts, varying from 2k to 64k tokens). All models use‚àºsimilar-to\\sim‚àº1.5B parameters.",
                "position": 5586
            }
        ]
    },
    {
        "header": "Appendix NComplexity Analysis",
        "images": []
    },
    {
        "header": "Appendix ORelated Work",
        "images": []
    },
    {
        "header": "Appendix PLimitations",
        "images": []
    }
]