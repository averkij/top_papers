[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16426/x1.png",
                "caption": "Figure 1:a): ViTs process all visual tokens uniformly.b): DynamicVis selectively extracts key tokens at each block to perform adaptive modeling.c): Memory consumption of different model architectures at varying input resolutions.d): The proposed DynamicVis demonstrates versatility in interpreting diverse temporal and spatial localization patterns. Comprehensive evaluations across nine downstream tasks, spanning region-, instance-, and pixel-level understanding, demonstrate its efficacy, generalizability, and scalability.",
                "position": 116
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16426/x2.png",
                "caption": "Figure 2:The overview of Dynamic Region-aware SSM Backbone, comprising four interconnected stages that generate hierarchical semantic feature maps at varying scales. Red boxes highlight regions of interest, while yellows denote regions exhibiting structural simplicity or repetitive patterns.",
                "position": 244
            },
            {
                "img": "https://arxiv.org/html/2503.16426/x3.png",
                "caption": "Figure 3:The structure of the Sparse Mixer, including three key elements: a flattening operation,NisubscriptùëÅùëñN_{i}italic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTselective token incremental modeling (STIM) units, and an un-flattening operation.",
                "position": 404
            },
            {
                "img": "https://arxiv.org/html/2503.16426/x4.png",
                "caption": "Figure 4:The detailed architecture of the Selective Token Incremental Modeling (STIM) unit.",
                "position": 440
            },
            {
                "img": "https://arxiv.org/html/2503.16426/x5.png",
                "caption": "Figure 5:The structure of dual-path SSM scanning.",
                "position": 509
            },
            {
                "img": "https://arxiv.org/html/2503.16426/x6.png",
                "caption": "Figure 6:The overview of the meta-embedding-based multiple-instance learning to integrate contextual and semantic feature representations in the latent space.",
                "position": 573
            }
        ]
    },
    {
        "header": "4Experimental Results and Analyses",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16426/x7.png",
                "caption": "Figure 7:Illustrative samples from the fMoW dataset, demonstrate diversity in geographical and temporal distributions.",
                "position": 648
            },
            {
                "img": "https://arxiv.org/html/2503.16426/x8.png",
                "caption": "Figure 8:A percentage-normalized confusion matrix of DynamicVis-L on the UC-Merced test dataset, where the horizontal axis represents predictions, and the vertical axis indicates labels.",
                "position": 1011
            },
            {
                "img": "https://arxiv.org/html/2503.16426/x9.png",
                "caption": "Figure 9:A percentage-normalized confusion matrix of DynamicVis-L on the AID test dataset, where the horizontal axis represents predictions and the vertical axis indicates annotations.",
                "position": 1022
            },
            {
                "img": "https://arxiv.org/html/2503.16426/x10.png",
                "caption": "Figure 10:Training loss (left y-axis) and evaluation accuracy (right y-axis) vs. iterations for the UC-Merced dataset.",
                "position": 1026
            },
            {
                "img": "https://arxiv.org/html/2503.16426/x11.png",
                "caption": "Figure 11:Detection performance comparison between DynamicVis-L and other methods on LEVIR-Ship test set, withredboxes showing true positives,yellowshowing false positives, andgreenshowing false negatives.",
                "position": 1267
            },
            {
                "img": "https://arxiv.org/html/2503.16426/x12.png",
                "caption": "Figure 12:DynamicVis-L detection examples on LEVIR-Ship test set showing ships under severe environmental interference. Left: ground-truth annotations; Right: the corresponding predictions.",
                "position": 1272
            },
            {
                "img": "https://arxiv.org/html/2503.16426/x13.png",
                "caption": "Figure 13:Segmentation results of DynamicVis-L on the NWPU test set. Left: ground truth annotations; Right: model predictions.",
                "position": 1685
            },
            {
                "img": "https://arxiv.org/html/2503.16426/x14.png",
                "caption": "Figure 14:Segmentation results from DynamicVis-L on the SSDD test set. Left: ground truth annotations; Right: model predictions.",
                "position": 1690
            },
            {
                "img": "https://arxiv.org/html/2503.16426/extracted/6289318/figs/vis_massachusetts.png",
                "caption": "Figure 15:Comparative analysis of road detection performance between the proposed method and existing approaches on the Massachusetts road dataset. True positive (TP), false negative (FN), and false positive (FP) detections are annotated withgreen,blue, andredmarkers, respectively.",
                "position": 2186
            },
            {
                "img": "https://arxiv.org/html/2503.16426/extracted/6289318/figs/vis_whu_building.png",
                "caption": "Figure 16:Comparative analysis of road detection performance between the proposed method and existing approaches on the WHU building dataset. True positive (TP), false negative (FN), and false positive (FP) detections are annotated withgreen,blue, andredmarkers, respectively.",
                "position": 2191
            },
            {
                "img": "https://arxiv.org/html/2503.16426/x15.png",
                "caption": "Figure 17:Detection samples from DynamicVis on the LEVIR-CD test set. Theredboxes highlight the capability of DynamicVis to accurately depict ‚Äúchanges‚Äù in complex scenes, some of which have even been overlooked by human annotators.",
                "position": 3139
            },
            {
                "img": "https://arxiv.org/html/2503.16426/x16.png",
                "caption": "Figure 18:Examples from the ForestNet-12 and BigEarthNet-43 datasets: query images (left) with their corresponding labels and retrieved images (right) using DynamicVis with 64-bit hash codes. The retrieval quality is indicated by color-coded frames:greenframes denote correct matches,redindicates incorrect, andorangerepresents partial matches (with numbers showing the count of matching labels).",
                "position": 3392
            },
            {
                "img": "https://arxiv.org/html/2503.16426/x17.png",
                "caption": "Figure 19:t-SNE visualization of the ForestNet-4 test set, with classes differentiated by color coding, comparing embeddings generated by ViT against those produced by DynamicVis.",
                "position": 3397
            },
            {
                "img": "https://arxiv.org/html/2503.16426/x18.png",
                "caption": "Figure 20:Latency (ms) of different models across various input resolutions.",
                "position": 3537
            },
            {
                "img": "https://arxiv.org/html/2503.16426/x19.png",
                "caption": "Figure 21:GPU memory usage (GB) across different models with varying input resolutions.",
                "position": 3542
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]