[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16676/x1.png",
                "caption": "",
                "position": 366
            },
            {
                "img": "https://arxiv.org/html/2512.16676/x2.png",
                "caption": "",
                "position": 367
            },
            {
                "img": "https://arxiv.org/html/2512.16676/x3.png",
                "caption": "",
                "position": 368
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background and Related Works",
        "images": []
    },
    {
        "header": "3DataFlow System Overview",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16676/x4.png",
                "caption": "Figure 1:High-level architecture ofDataFlow.\nThe system consists of a core execution engine (storage, operators, templates, and LLM serving),\nreusable pipelines, user-facing control layers (CLI and agent), and an extensible ecosystem for domain-specialized workflows.DataFlowproduces high-quality, task-aligned datasets consumed by downstream LLM applications.",
                "position": 725
            }
        ]
    },
    {
        "header": "4Framework Design and Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16676/x5.png",
                "caption": "Figure 2:The standard execution pattern of an operator’srun()method inDataFlow.\nWithinrun(), the operator interacts with the globalDataFlowStorageby retrieving inputs throughstorage.read(), applying its transformation logic, and writing updated fields back viastorage.write().\nThis read–transform–write paradigm captures how data flows from one operator to the next throughout the workflow.",
                "position": 793
            },
            {
                "img": "https://arxiv.org/html/2512.16676/x6.png",
                "caption": "Figure 3:Example of how an operator’srun()method interacts with data via key-based bindings.\nThis flexible key-binding mechanism adapts to arbitrary datasets without preprocessing and enables seamless operator composition.",
                "position": 845
            },
            {
                "img": "https://arxiv.org/html/2512.16676/x7.png",
                "caption": "Figure 4:Illustration of theDataFlowpipeline API.\nThe example shows how a pipeline declares its storage and serving backends, instantiates operators with task-specific configurations, and executes them viaforward()using input/output key bindings.\nThe interface supports compilation and stepwise resumption, enabling flexible and modular workflow construction.",
                "position": 899
            },
            {
                "img": "https://arxiv.org/html/2512.16676/x8.png",
                "caption": "Figure 5:Evolution of sample counts across operator stages inDataFlowpipelines.\nAll pipelines start with 1000 input samples.\nThe Text pipeline mainly performs pre-training data filtering, and the Code pipeline focuses on expanding code capabilities based on existing instruction data; therefore, neither of these pipelines involves any generative components.",
                "position": 960
            }
        ]
    },
    {
        "header": "5DataFlow-Agent",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16676/x9.png",
                "caption": "Figure 6:DataFlow-Agentarchitecture: a LangGraph-orchestrated multi-agent workflow that translates natural-language intent into a verified executable DAG pipeline.",
                "position": 1058
            }
        ]
    },
    {
        "header": "6Use Cases & Pipelines",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16676/x10.png",
                "caption": "Figure 7:Overall framework of Text-to-SQL pipelines inDataFlow.",
                "position": 1174
            }
        ]
    },
    {
        "header": "7Experiments",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "9Author Contributions",
        "images": []
    }
]