[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IINTRODUCTION",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03597/x1.png",
                "caption": "Figure 1:The real-is-sim framework, illustrating the information flow between its components. A policy trained in a correctable physics simulator (leveraging Embodied Gaussians) controls a simulated robot. Real-world observations continuously update the simulator, maintaining its state close to ground truth. The real robot then mirrors the simulated robot’s joint positions. This approach shifts the sim-to-real gap challenge from the policy to the adaptable physics simulator.",
                "position": 88
            },
            {
                "img": "https://arxiv.org/html/2504.03597/x2.png",
                "caption": "Figure 2:Comparison of three paradigms: Real-is-Sim, Real-Only, and Sim-to-Real, highlighting their online (real-world interaction) and offline (simulation-based) capabilities. Real-is-Sim offers a unified framework where the offline mode is a simplified version of the online, lacking only real-time correction. This ensures seamless transferability. Real-Only is confined to real-world execution. Sim-to-Real struggles with distribution mismatch and dynamics discrepancies, making successful transfer uncertain. In the Real-is-Sim paradigm, successful transfer between online and offline modes is mostly dependent on whether environment synchronization was successful during the demo collection phase.",
                "position": 110
            }
        ]
    },
    {
        "header": "IIRELATED WORK",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03597/x3.png",
                "caption": "Figure 3:The two modes of the Real-is-Sim framework: online and offline. In online mode, the real robot operates alongside the correctable simulator (Embodied Gaussians). Synchronization is maintained through fictitious visual forces derived from the rendering loss between observed and real images, as well as by the real robot mimicking the simulated robot’s state. In offline mode, parallel environments execute the learned policy and evaluate its success rate. The simulator can run up to 7x faster than real time with policy sampling in the loop.",
                "position": 132
            }
        ]
    },
    {
        "header": "IIIPRELIMINARIES",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03597/x4.png",
                "caption": "Figure 4:Comparison of different representations extracted from the Real-to-Sim framework, contrasted with a real image baseline. (a) A purely state-based representation, utilizing the object’s position and orientation (in quaternion form). (b) A virtual moving camera positioned on the robot’s end effector. (c) A virtual camera placed at the same location as the original camera that observed the scene. (d) One of the real camera images. All representations are augmented with the robot’s state. In the Real-to-Sim framework, the joint states correspond to those of the simulated robot, while in the real image representation, the joint states reflect those of the real robot.",
                "position": 230
            }
        ]
    },
    {
        "header": "IVMETHOD",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03597/x5.png",
                "caption": "Figure 5:The success rate of an imitation learning policy as a function of training steps. The plot compares policy evaluation in the offline mode of real-is-sim with the online mode of real-is-sim. The high correlation between the two curves demonstrates that the (significantly faster) offline evaluation mode is a reliable proxy for real-world testing, offering a practical alternative for policy assessment. Error bars are 95% confidence intervals.",
                "position": 272
            }
        ]
    },
    {
        "header": "VEXPERIMENTS",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03597/x6.png",
                "caption": "Figure 6:Success rates of different demonstration collection strategies. A dataset consisting of 30 real demonstrations achieves a success rate of approximately 58%. Since Real-to-Sim can also operate offline, the demonstrations can be augmented with additional simulated data. A combined dataset of 60 demonstrations (30 real and 30 simulated) achieves an 80% success rate. In comparison, a dataset of 60 real demonstrations alone achieves a slightly lower success rate of 72%. Error bars are 95% confidence intervals.",
                "position": 299
            },
            {
                "img": "https://arxiv.org/html/2504.03597/x7.png",
                "caption": "Figure 7:Success rates of policies using different representations to condition the flow matching policy. The baseline image-based policy (not using real-is-sim) uses a static camera and has a success rate of 63%. The state-based policy, which uses the simulator’s object pose achieves a higher success rate of 76%. Lastly, policies trained on the rendered virtual cameras (one statically mounted and another mounted on the gripper) achieve 73% and 82% respectively. This shows that a gripper-mounted camera is likely a better representation for the PushT task. Error bars are 95% confidence intervals.",
                "position": 304
            }
        ]
    },
    {
        "header": "VILimitations",
        "images": []
    },
    {
        "header": "VIIFuture Work",
        "images": []
    },
    {
        "header": "VIIICONCLUSIONS",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]