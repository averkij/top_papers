[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.00523/Figures/motivation-example.png",
                "caption": "Figure 1:Visual-interactive paradigms for image-to-image (I2I) with masks assuming candidate images contain only dogs or only cats across different scenes, and image-to-text (I2T) with bounding boxes. False retrievals occur when retrieved content does not match the queryâ€™s scene context.",
                "position": 111
            },
            {
                "img": "https://arxiv.org/html/2510.00523/Figures/SCaR-logo.png",
                "caption": "",
                "position": 129
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3SCaR: Segmentation-and-Scene Caption Retrieval Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.00523/Figures/SCaR-pipeline.png",
                "caption": "Figure 2:The data collection pipeline to buildSCaR. We adopt GPT-4V to generate missing elements for the ground-truth caption as well as negative candidates.\nCollected samples (left) are filtered via LLM-then-human inspection (right) to ensure quality. Each SCaR sample contains an image with a bounding box, one ground-truth caption, and nine distractors.",
                "position": 204
            }
        ]
    },
    {
        "header": "4VIRTUE",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.00523/Figures/model-architecture.png",
                "caption": "Figure 3:Overview of VIRTUE. The framework trained with contrastive loss consists of a segmentation model, a segmentation-language connector (orange), and a VLM (blue). It supports arbitrary combinations of visual and textual inputs with an optional visual prompt. If no prompt is provided, the model samplesNNpoints uniformly from the image to extract entity-level information.",
                "position": 306
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Ethics Statements",
        "images": []
    },
    {
        "header": "Reproducibility Statements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitations and Future Directions",
        "images": []
    },
    {
        "header": "Appendix BBroader Impacts",
        "images": []
    },
    {
        "header": "Appendix CImplementation Details",
        "images": []
    },
    {
        "header": "Appendix DSCaR Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.00523/Figures/scar-statistics.png",
                "caption": "Figure 6:Detailed statistics of the SCaR train and evaluation sets. (a), (d) Word clouds for the candidates. (b), (e): Dataset compositions in terms of numbers of samples. (c), (f): Dataset compositions in terms of numbers of images.",
                "position": 2163
            },
            {
                "img": "https://arxiv.org/html/2510.00523/Figures/scar_length_comparison_individual.png",
                "caption": "Figure 7:Distributions of sentence lengths for ground-truth and negative captions across each dataset.",
                "position": 2166
            },
            {
                "img": "https://arxiv.org/html/2510.00523/Figures/scar-examples/refcoco_plus.png",
                "caption": "Table 5:Examples of datasets in SCaR. The instruction across all datasets isFind the caption that best describes the segmented object, considering both local details and global context in the given image. Referring object bbox:{bbox}.Each first candidate in redis the ground-truth caption.",
                "position": 2183
            },
            {
                "img": "https://arxiv.org/html/2510.00523/Figures/scar-examples/refcocog.png",
                "caption": "",
                "position": 2233
            },
            {
                "img": "https://arxiv.org/html/2510.00523/Figures/scar-examples/visualgenome.png",
                "caption": "",
                "position": 2271
            },
            {
                "img": "https://arxiv.org/html/2510.00523/Figures/scar-examples/coco-stuff.png",
                "caption": "",
                "position": 2309
            },
            {
                "img": "https://arxiv.org/html/2510.00523/Figures/scar-examples/ADE20K.jpg",
                "caption": "",
                "position": 2347
            },
            {
                "img": "https://arxiv.org/html/2510.00523/Figures/parameter-analysis.png",
                "caption": "Figure 8:The impacts of varying LoRA ranks, batch sizes, visual-prompt design choice, and sampling pointsNNon VIRTUE-2B.",
                "position": 2589
            },
            {
                "img": "https://arxiv.org/html/2510.00523/Figures/in-the-wild-I2I.png",
                "caption": "Figure 9:In-the-wild visual-interactive (top) and naive cropping (bottom) image-to-image retrieval scenarios. VIRTUE-7B leverages visual prompts (bounding boxes in this paradigm) to guide the retrieval of regions of interest while accounting for both entity-level details and global scene context.",
                "position": 2762
            },
            {
                "img": "https://arxiv.org/html/2510.00523/Figures/instant-correction.png",
                "caption": "Figure 10:On-the-fly correction with explicitly bounding box hinting of VQA and I2T retrieval paradigms. All of them are used with VIRTUE-2B.",
                "position": 2790
            },
            {
                "img": "https://arxiv.org/html/2510.00523/Figures/SCaR-qualitative.png",
                "caption": "Figure 11:Qualitative comparison of VLM2Vec-7B, MMRet-7B, UniME-7B, and VIRTUE-7B. Ground-truth captions are shown in green and incorrect ones in red.",
                "position": 2793
            }
        ]
    },
    {
        "header": "Appendix EExtensive Experiments",
        "images": []
    }
]