[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.06209/x1.png",
                "caption": "Figure 2:Generated videos conditioned on various conditions. (1) The top row displays the input conditions, including road maps and bounding boxes, projected to the camera. (2) The second row shows the corresponding real-world video. The subsequent rows demonstrate the model’s ability to generate videos under different conditions: (3) identical conditions to the original video, (4) changing the weather from no-rain to rain, (5) changing the time of day to 00:00 (at midnight), (6) with both rain and nighttime conditions.",
                "position": 72
            },
            {
                "img": "https://arxiv.org/html/2510.06209/imgs/fig2_vid_93_baseline_log_cropped.png",
                "caption": "",
                "position": 72
            },
            {
                "img": "https://arxiv.org/html/2510.06209/imgs/fig2_vid_93_baseline_gen_cropped.png",
                "caption": "",
                "position": 72
            },
            {
                "img": "https://arxiv.org/html/2510.06209/imgs/fig2_vid_93_change_weather_rainy_gen_cropped.png",
                "caption": "",
                "position": 72
            },
            {
                "img": "https://arxiv.org/html/2510.06209/imgs/fig2_vid_93_change_time_00_gen_cropped.png",
                "caption": "",
                "position": 72
            },
            {
                "img": "https://arxiv.org/html/2510.06209/imgs/fig2_vid_93_change_weather_rainy_change_time_00_gen_cropped.png",
                "caption": "",
                "position": 72
            }
        ]
    },
    {
        "header": "IIRelated Work",
        "images": []
    },
    {
        "header": "IIIMethod",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.06209/imgs/architecture3.png",
                "caption": "Figure 3:Model architecture of our video generation model. We enable control of scene and traffic layout (bounding boxes, road map, and ego car pose) and operational conditions (time-of-day, weather), extending the latent video diffusion model W.A.L.T[5]. The conditions are encoded and interact with intermediate features in the diffusion transformer via a combination of AdaLN and cross attention mechanisms. The model is fine-tuned on a large corpus of driving videos.",
                "position": 210
            }
        ]
    },
    {
        "header": "IVExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.06209/x2.png",
                "caption": "Figure 4:Evaluation of controllable video generation with FVD, ADE@5s, and BPT on 5000 random samples. FVD doesn’t fully capture visual quality – FVD for Rain/Night (relatively rare in our dataset) are much higher (because of distribution shifts) though the photo-realism of videos are visually similar. FVD cannot measure controllability – removing the conditioning on bounding boxes greatly changes the car locations but has little effect on FVD. ADE and BPT don’t suffer from such data distribution shifts, and can capture model controllability – both metrics are notably worse when bounding boxes are removed.",
                "position": 264
            },
            {
                "img": "https://arxiv.org/html/2510.06209/x3.png",
                "caption": "",
                "position": 307
            },
            {
                "img": "https://arxiv.org/html/2510.06209/x4.png",
                "caption": "",
                "position": 312
            },
            {
                "img": "https://arxiv.org/html/2510.06209/x5.png",
                "caption": "",
                "position": 317
            },
            {
                "img": "https://arxiv.org/html/2510.06209/x6.png",
                "caption": "",
                "position": 322
            },
            {
                "img": "https://arxiv.org/html/2510.06209/x7.png",
                "caption": "Figure 5:Behavioral Permutation Test (BPT) visualizations. BPT performs a set-to-set comparison of predicted trajectories from real and generated videos.\nIn the top row, when the two sets of trajectories are similar, the distance between the two sets (red dash line) falls well within permuted distributions, resulting in a failure to reject the null hypothesis. The bottom shows a rejection of the null hypothesis, where the two sets of trajectories are significantly different from each other.",
                "position": 423
            },
            {
                "img": "https://arxiv.org/html/2510.06209/imgs/efm_gt4_crop.png",
                "caption": "Figure 6:Comparison of predicted trajectories from a planner given real and generated videos. Same scene layouts in two videos lead to highly similar trajectory predictions.",
                "position": 535
            },
            {
                "img": "https://arxiv.org/html/2510.06209/imgs/efm_gen4_crop.png",
                "caption": "",
                "position": 538
            },
            {
                "img": "https://arxiv.org/html/2510.06209/imgs/img_1326_baseline16_5000_gt_efm_before_ft.png",
                "caption": "Before FT",
                "position": 775
            },
            {
                "img": "https://arxiv.org/html/2510.06209/imgs/img_1326_baseline16_5000_gt_efm_before_ft.png",
                "caption": "Before FT",
                "position": 778
            },
            {
                "img": "https://arxiv.org/html/2510.06209/imgs/img_1326_baseline16_5000_gt_efm_before_ft.png",
                "caption": "Before FT",
                "position": 781
            },
            {
                "img": "https://arxiv.org/html/2510.06209/imgs/img_1326_baseline16_5000_gt_efm_after_ft.png",
                "caption": "After FT",
                "position": 786
            },
            {
                "img": "https://arxiv.org/html/2510.06209/imgs/img_525_baseline16_5000_gt_efm_before_ft.png",
                "caption": "Before FT",
                "position": 795
            },
            {
                "img": "https://arxiv.org/html/2510.06209/imgs/img_525_baseline16_5000_gt_efm_before_ft.png",
                "caption": "Before FT",
                "position": 798
            },
            {
                "img": "https://arxiv.org/html/2510.06209/imgs/img_525_baseline16_5000_gt_efm_after_ft.png",
                "caption": "After FT",
                "position": 803
            }
        ]
    },
    {
        "header": "VLimitations and Discussions",
        "images": []
    },
    {
        "header": "VIConclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]