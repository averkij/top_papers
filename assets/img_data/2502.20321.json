[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20321/x1.png",
                "caption": "Figure 1:(a): The unified tokenizer training paradigm. (b): Comparison with the unified tokenizer VILA-U in terms of ImageNet zero-shot accuracy and reconstruction FID.",
                "position": 97
            },
            {
                "img": "https://arxiv.org/html/2502.20321/x2.png",
                "caption": "Figure 2:An overview of UniTok.The tokenizer is trained to faithfully reconstruct the input image while aligning its discrete latent features with the text caption. For vector quantization, each visual token is split into multiple chunks, which then undergo code index lookup on corresponding sub-codebooks concurrently.",
                "position": 112
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20321/x3.png",
                "caption": "Figure 3:Roadmap to build UniTok.Theblue barsillustrate the progressive changes in VQA performance from the CLIP tokenizer to the unified tokenizer, while thepurple barsrepresent the proposed improvements in UniTok. The VQA score is measured using the average accuracy across the VQAv2, GQA, TextVQA, and POPE benchmarks. All models are trained from scratch on 512m image-text pairs from DataComp.",
                "position": 178
            },
            {
                "img": "https://arxiv.org/html/2502.20321/x4.png",
                "caption": "Figure 4:Modified attention blocks for factorization. Modules in yellow indicate a change in the number of channels.",
                "position": 214
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20321/extracted/6238271/fig/vis.png",
                "caption": "Figure 5:Images generated in a resolution of256×256256256256\\times 256256 × 256with our unified MLLM.",
                "position": 688
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImage Reconstruction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20321/extracted/6238271/fig/rec.png",
                "caption": "Figure 6:Qualitative results on image reconstruction in a resolution of256×256256256256\\times 256256 × 256.",
                "position": 1865
            }
        ]
    },
    {
        "header": "Appendix BMore Generation Results",
        "images": []
    }
]