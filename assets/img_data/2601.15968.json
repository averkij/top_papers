[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.15968/x1.png",
                "caption": "",
                "position": 90
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.15968/x2.png",
                "caption": "Figure 2:Task-specific test-time alignment of HyperAlign. Compared to the original generative model, HyperAlign adapts the model’s behavior to each combination of prompt and temporal states, producing aligned and visually appealing results",
                "position": 135
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.15968/x3.png",
                "caption": "Figure 3:The framework of HyperAlign. Given a user prompt, the hypernetwork produces step-wise modulation weightsΔ​θt\\Delta\\theta_{t}that are injected into the generative model to steer the denoising trajectory (top). During training (bottom), the hypernetwork is optimized using the reward loss and the preference-regularization loss, enabling it to produce input-specific adjustments.",
                "position": 162
            }
        ]
    },
    {
        "header": "3Problem Setup: Diffusion Model Alignment",
        "images": []
    },
    {
        "header": "4Methodology: HyperAlign",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.15968/x4.png",
                "caption": "Figure 4:The prompt-invariant temporal dynamics of one-step predicted data. Average over 1000 prompts.",
                "position": 276
            },
            {
                "img": "https://arxiv.org/html/2601.15968/x5.png",
                "caption": "Figure 5:Qualitative comparison examples based on FLUX backbones.",
                "position": 279
            },
            {
                "img": "https://arxiv.org/html/2601.15968/x6.png",
                "caption": "Figure 6:Qualitative comparison based on SD V1.5 backbones.",
                "position": 362
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.15968/x7.png",
                "caption": "Figure 7:User study results.",
                "position": 527
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7More Details of HyperAlign with Flow-Matching Models",
        "images": []
    },
    {
        "header": "8Additional Experimental Details and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.15968/x8.png",
                "caption": "Figure 8:Visualization of LoRA weight variations at different timesteps relative to the initial stateTT. The cosine similarity andℓ1\\ell_{1}difference of the LoRA generated at each step relative to the LoRA at the initial step are calculated and demonstrated. Average over 1000 prompts.",
                "position": 1937
            },
            {
                "img": "https://arxiv.org/html/2601.15968/x9.png",
                "caption": "Figure 9:Visualization of the statistics of prompt-specific LoRA weights across different steps. The top two PCA components of the LoRAs generated for different prompts (200 examples) at each step are shown.",
                "position": 1940
            },
            {
                "img": "https://arxiv.org/html/2601.15968/x10.png",
                "caption": "Figure 10:Qualitative comparison based on SD V1.5 backbones.",
                "position": 1967
            },
            {
                "img": "https://arxiv.org/html/2601.15968/x11.png",
                "caption": "Figure 11:Qualitative comparison based on FLUX backbones.",
                "position": 1970
            },
            {
                "img": "https://arxiv.org/html/2601.15968/x12.png",
                "caption": "Figure 12:Diversity comparison based on FLUX backbones.",
                "position": 1973
            },
            {
                "img": "https://arxiv.org/html/2601.15968/x13.png",
                "caption": "Figure 13:The visual results of ablation study.",
                "position": 1976
            },
            {
                "img": "https://arxiv.org/html/2601.15968/x14.png",
                "caption": "Figure 14:The comparison includes the baseline FLUX outputs, the results obtained through HPS-only optimization, and the versions further improved with joint HPS and CLIP objectives.",
                "position": 1979
            },
            {
                "img": "https://arxiv.org/html/2601.15968/x15.png",
                "caption": "Figure 15:The comparison includes the baseline FLUX outputs, the results obtained through HPS-only optimization, and the versions further improved with joint HPS and CLIP objectives.",
                "position": 1982
            },
            {
                "img": "https://arxiv.org/html/2601.15968/supplyfig/Q1.png",
                "caption": "Figure 16:The screenshot of human preference investigation: Which image do you prefer given the prompt?",
                "position": 1985
            },
            {
                "img": "https://arxiv.org/html/2601.15968/supplyfig/Q2.png",
                "caption": "Figure 17:The screenshot of human preference investigation: Which image is more visually appealing?",
                "position": 1988
            },
            {
                "img": "https://arxiv.org/html/2601.15968/supplyfig/Q3.png",
                "caption": "Figure 18:The screenshot of human preference investigation: Which image better fits the text description?",
                "position": 1991
            }
        ]
    },
    {
        "header": "9Ethical and Social Impacts",
        "images": []
    }
]