[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12143/x1.png",
                "caption": "Figure 1:Small student models (≤\\leq≤3B parameters) do not consistently benefit from long CoT reasoning or distillation from large teacher models. Instead, they perform better when fine-tuned on shorter CoT reasoning or distilled from smaller teachers, which better matches their intrinsic learning capacity. We term this phenomenon theSmall Model Learnability Gap.",
                "position": 150
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Small Model Learnability Gap",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12143/x2.png",
                "caption": "Figure 2:Long CoT Gap (ΔL⁢o⁢n⁢g=PL⁢o⁢n⁢g−PS⁢h⁢o⁢r⁢tsubscriptΔ𝐿𝑜𝑛𝑔subscript𝑃𝐿𝑜𝑛𝑔subscript𝑃𝑆ℎ𝑜𝑟𝑡\\Delta_{Long}=P_{Long}-P_{Short}roman_Δ start_POSTSUBSCRIPT italic_L italic_o italic_n italic_g end_POSTSUBSCRIPT = italic_P start_POSTSUBSCRIPT italic_L italic_o italic_n italic_g end_POSTSUBSCRIPT - italic_P start_POSTSUBSCRIPT italic_S italic_h italic_o italic_r italic_t end_POSTSUBSCRIPT) of student models with different models sizes for (a) Qwen family (b) Llama family. For teacher models,QwQ-preview-32Bis chosen to generate long CoT responses, whileQwen2.5-32B-Instructis chosen to generate short CoT responses. Negative (Positive)ΔL⁢o⁢n⁢gsubscriptΔ𝐿𝑜𝑛𝑔\\Delta_{Long}roman_Δ start_POSTSUBSCRIPT italic_L italic_o italic_n italic_g end_POSTSUBSCRIPTindicates that long CoT is worse (better) than short CoT. Our results demonstrate that short CoT is better for smaller student models (indicated byΔL⁢o⁢n⁢gsubscriptΔ𝐿𝑜𝑛𝑔\\Delta_{Long}roman_Δ start_POSTSUBSCRIPT italic_L italic_o italic_n italic_g end_POSTSUBSCRIPT< 0), while long CoT is better for larger student models (indicated byΔL⁢o⁢n⁢gsubscriptΔ𝐿𝑜𝑛𝑔\\Delta_{Long}roman_Δ start_POSTSUBSCRIPT italic_L italic_o italic_n italic_g end_POSTSUBSCRIPT> 0).",
                "position": 226
            },
            {
                "img": "https://arxiv.org/html/2502.12143/x3.png",
                "caption": "Figure 3:Large model CoT Gap (ΔL⁢a⁢r⁢g⁢e=PL⁢a⁢r⁢g⁢e−PS⁢m⁢a⁢l⁢lsubscriptΔ𝐿𝑎𝑟𝑔𝑒subscript𝑃𝐿𝑎𝑟𝑔𝑒subscript𝑃𝑆𝑚𝑎𝑙𝑙\\Delta_{Large}=P_{Large}-P_{Small}roman_Δ start_POSTSUBSCRIPT italic_L italic_a italic_r italic_g italic_e end_POSTSUBSCRIPT = italic_P start_POSTSUBSCRIPT italic_L italic_a italic_r italic_g italic_e end_POSTSUBSCRIPT - italic_P start_POSTSUBSCRIPT italic_S italic_m italic_a italic_l italic_l end_POSTSUBSCRIPT) of student models with different models sizes for (a) Qwen family (b) Llama family. For teacher models,Qwen2.5-72B-Instructis chosen as the large teacher to generate responses, whileQwen2.5-3B-Instructis chosen as the small teacher to generate responses. Negative (positive)ΔL⁢a⁢r⁢g⁢esubscriptΔ𝐿𝑎𝑟𝑔𝑒\\Delta_{Large}roman_Δ start_POSTSUBSCRIPT italic_L italic_a italic_r italic_g italic_e end_POSTSUBSCRIPTindicates that large teacher CoT is worse (better) than small teacher CoT. Our results demonstrate that small teacher CoT is better for smaller student models (indicated byΔL⁢a⁢r⁢g⁢esubscriptΔ𝐿𝑎𝑟𝑔𝑒\\Delta_{Large}roman_Δ start_POSTSUBSCRIPT italic_L italic_a italic_r italic_g italic_e end_POSTSUBSCRIPT< 0), while large model CoT is better for larger student models (indicated byΔL⁢a⁢r⁢g⁢esubscriptΔ𝐿𝑎𝑟𝑔𝑒\\Delta_{Large}roman_Δ start_POSTSUBSCRIPT italic_L italic_a italic_r italic_g italic_e end_POSTSUBSCRIPT> 0).",
                "position": 229
            },
            {
                "img": "https://arxiv.org/html/2502.12143/x4.png",
                "caption": "Figure 4:Math expert models usually have a less significant Learnability Gap than the general models.\nA positive Gap means long CoT or large teacher CoT is better while negative means worse. This indicates that the math expert model could more easily learn from long CoT data or large teacher CoT.",
                "position": 728
            },
            {
                "img": "https://arxiv.org/html/2502.12143/x5.png",
                "caption": "Figure 5:Base models generally exhibit a more significant learnability gap than Instruct models. A positive gap indicates that long CoT data or large teacher CoT enhance performance, whereas a negative gap suggests they have the opposite effect. This implies that it is more challenging for small base models to effectively learn from long CoT data or large teacher CoT.",
                "position": 745
            }
        ]
    },
    {
        "header": "4Mix Distillation: Bridge Small Model Learnability Gap",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12143/x6.png",
                "caption": "Figure 6:The average performance varies with the mix weight of long CoT or large teacher CoT data.Qwen2.5-3B-Instructis chosen as the student model. At a weight of 0.2, mix distillation achieves the highest average performance.",
                "position": 807
            },
            {
                "img": "https://arxiv.org/html/2502.12143/extracted/6211364/figs/mix_long.png",
                "caption": "Figure 7:Case Study of Mix-Long. Models fine-tuned on long CoT tended to overthink, while those trained on short CoT produced incorrect answers. In contrast, Mix-Long, incorporating branching elements (e.g., “Alternatively”), achieved a balanced reasoning process and arrived at the correct answer.",
                "position": 810
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethical Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetailed Experimental Setups",
        "images": []
    },
    {
        "header": "Appendix BMore Experiments Results",
        "images": []
    },
    {
        "header": "Appendix CExamples of Speaking Style Shift",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12143/extracted/6211364/figs/speaking_way_shift.png",
                "caption": "Figure 8:The process of calculating most shifted tokens. We decode each token generated by the fine-tuned LLM in the student model before fine-tuning. Then we calculate the rank shift in the student model for each token generated by the fine-tuned model. We annotate the tokens that exhibit the largest rank shifts as the most shifted tokens. We found that these tokens are predominantly associated with expressive and stylistic elements, such as “But” and “Let”.",
                "position": 2533
            }
        ]
    },
    {
        "header": "Appendix DExamples of Various CoT Data",
        "images": []
    }
]