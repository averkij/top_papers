[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.16153/x1.png",
                "caption": "",
                "position": 105
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.16153/x2.png",
                "caption": "Figure 2:High-level comparisons of MotiFvs.prior works.Previous TI2V generation methods mainly focused on deriving additional motion signals (motion score and/or motion mask) as inputs for the model to leverage implicitly. On the contrary, we focus on the learning objective and propose to weight the diffusion loss based on the motion intensity, that is derived from optical flow. Our method is simple, effective, and does not require additional inputs during inference. Moreover, MotiF is complementary to existing techniques.",
                "position": 233
            },
            {
                "img": "https://arxiv.org/html/2412.16153/x3.png",
                "caption": "Figure 3:Example image-text pairs in TI2V Bench. For each scenario (column), we first think of a scene that could be potentially animated to generate different types of motion. We include challenging scenarios when there are multiple objects (yellow/blue/red balloon) in the initial image for fine-grained control or the text prompt describes a new object (frisbee, bubbles) to enter the scene. Then we come up with different prompts and use the publicly availablemeta.aitool to generate diverse sets of images. Images of low quality or those not in the appropriate initial state are removed.",
                "position": 335
            }
        ]
    },
    {
        "header": "4TI2V Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.16153/x4.png",
                "caption": "Figure 4:Human evaluation results comparing MotiF to nine open-sourced models[46,7,25,50,9,53,28,12,33]on our proposed TI2V Bench. We achieved considerable improvements across the board with an average preference of72%percent7272\\%72 %. Through examining the justification choices, we found that our model mostly excel at improving text alignment and object motion, which matches very well with our motivation. Note that all the inference of prior works are done at Brown University.",
                "position": 470
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.16153/x5.png",
                "caption": "Figure 5:Qualitative comparison to prior works on TI2V Bench.Sampled frames are ordered from left to right.",
                "position": 496
            },
            {
                "img": "https://arxiv.org/html/2412.16153/x6.png",
                "caption": "Figure 6:Loss comparison.We calculate the ratio of the average loss in the high motion region to the average overall loss on a hold-out validation set with different timesteps. MotiF can effectively reduce the relative loss of the high motion regions.",
                "position": 630
            }
        ]
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AAdditional Ablation Study",
        "images": []
    },
    {
        "header": "Appendix BAdditional Quantitative Evaluation",
        "images": []
    },
    {
        "header": "Appendix CAdditional Visualization",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.16153/x7.png",
                "caption": "Figure A1:More qualitative comparison to prior works on TI2V Bench.MotiF can generate videos that align better with the text prompts. More video samples are available in the project website.",
                "position": 1711
            },
            {
                "img": "https://arxiv.org/html/2412.16153/x8.png",
                "caption": "Figure A2:Typical failure and challenging cases of MotiF on TI2V Bench.We observe two typical cases that the model fail: 1) the generated videos may have unnatural motion ((a)); 2) the generated videos do not align well with the prompts ((b), (c), (d)). For 2), there are two specific scenarios when following the text is challenging including novel object ((c)) or multiple objects ((d)). We also include more video samples in the project website.",
                "position": 1714
            },
            {
                "img": "https://arxiv.org/html/2412.16153/extracted/6086198/figures/mturk_instruction.png",
                "caption": "Figure A3:Illustration of human evaluation interface on the Amazon Mechanical Turk platform.",
                "position": 1743
            },
            {
                "img": "https://arxiv.org/html/2412.16153/extracted/6086198/figures/mturk_question.png",
                "caption": "",
                "position": 1747
            }
        ]
    },
    {
        "header": "Appendix DAdditional Implementation Details",
        "images": []
    }
]