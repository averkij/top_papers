[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.13676/x1.png",
                "caption": "Figure 1:(a) Visualize the hybrid-head module in Hymba; (b) Interpret from the memory aspect.",
                "position": 156
            },
            {
                "img": "https://arxiv.org/html/2411.13676/x2.png",
                "caption": "",
                "position": 160
            },
            {
                "img": "https://arxiv.org/html/2411.13676/x3.png",
                "caption": "Figure 2:Performance comparison of Hymba-1.5B against sub-2B models in terms of average task accuracy, cache size (MB) relative to sequence length, and throughput (tok/sec). Specifically, the tasks include 5-shot MMLU, ARC-C, ARC-E, PIQA, Hellaswag, Winogrande, and SQuAD-C, and the throughput is measured on an NVIDIA A100 with a sequence length of 8k and a batch size of 128 using PyTorch. For models encountering out-of-memory (OOM) issues during throughput measurement, we halve the batch size until the OOM is resolved. This approach is used to measure the maximal achievable throughput without OOM.",
                "position": 165
            }
        ]
    },
    {
        "header": "2Hymba: The Proposed Hybrid-Head Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.13676/x4.png",
                "caption": "Figure 3:Visualize the accuracy difference, measured using 1000 samples from Hellaswag[21], after removing the Attention or SSM heads in each layer.",
                "position": 391
            },
            {
                "img": "https://arxiv.org/html/2411.13676/x5.png",
                "caption": "Figure 4:(a) The overall architecture of our Hymba model; (b) The building block of Hymba.",
                "position": 415
            },
            {
                "img": "https://arxiv.org/html/2411.13676/extracted/6013569/Figures/meta_token_boxplot.png",
                "caption": "Figure 5:Averaged attention scores received by the meta tokens in the last layer of Hymba-1.5B model. Prompts of ‘Article’, ‘Math’ and ‘Code’ are from SQuAD[24], GSM8K[25], and GitHub-Code[26]datasets, respectively.",
                "position": 426
            },
            {
                "img": "https://arxiv.org/html/2411.13676/extracted/6013569/Figures/attention-schematics-crop.png",
                "caption": "Figure 6:Schematics of the attention map of Hymba as a combination of meta tokens, sliding window attention, and Mamba contributions.",
                "position": 452
            },
            {
                "img": "https://arxiv.org/html/2411.13676/x6.png",
                "caption": "Figure 7:Sum of attention score from different categories (i.e., ‘Meta’, ‘BOS’, ‘Self’, ‘Cross’) in Llama-3.2-3B, Jamba and Hymba-1.5B. Parallel SSM and Attention fusion in the latter disentangles attention.",
                "position": 455
            },
            {
                "img": "https://arxiv.org/html/2411.13676/x7.png",
                "caption": "Figure 8:Training pipeline adapted for Hymba family. For detailed loss curve of Hymba-Base-1.5B see Fig14.",
                "position": 458
            },
            {
                "img": "https://arxiv.org/html/2411.13676/x8.png",
                "caption": "Figure 9:Visualize the trade-off between (a) commonsense reasoning accuracy (avr. ARC-C, ARC-E, PIQA, Hellaswag, OBQA, and Winogrande using[28]) and cache size, with throughput represented by the point size of different models, and (b) commonsense reasoning accuracy and throughput, with cache size represented by the point size. The throughput is measured with a 8k sequence length and a 128 batch size on an NVIDIA A100 GPU. The cache size is measured with a 8k sequence length, assuming the FP16 format.",
                "position": 718
            }
        ]
    },
    {
        "header": "3Model Evaluations",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.13676/extracted/6013569/Figures/needle_apple2apple.png",
                "caption": "Figure 10:Needle-in-the-haystack performance comparison across different architecture under apple-to-apple setting. The white vertical line represents the finetuning sequence length(4k).",
                "position": 1173
            }
        ]
    },
    {
        "header": "4Related Works",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExtensive Benchmark for More Hymba Model Variants",
        "images": []
    },
    {
        "header": "Appendix BAblation Studies of Our Hymba Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.13676/x9.png",
                "caption": "Figure 11:Visualize the ERF and cache size trade-off.",
                "position": 3863
            },
            {
                "img": "https://arxiv.org/html/2411.13676/extracted/6013569/Figures/attn_mamba_gate.png",
                "caption": "Figure 12:Left: visualization of output magnitudes of attention and SSM heads. SSM heads consistently have higher output magnitude than attention heads due to their structure. Right: visualization of attention and SSM heads’ gate magnitudes. Through model learning, the relative magnitudes of attention and SSM gates vary across different layers.",
                "position": 4113
            },
            {
                "img": "https://arxiv.org/html/2411.13676/x10.png",
                "caption": "Figure 13:Visualize the task performance difference across three tasks after removing the Attention or SSM heads in each layer. The task performance is measured using 1000 samples from each task. Note that removing critical modules in specific layers causes a significant gap compared to others, making their bars fall outside the box. For such layers, we annotate the task performance with text.",
                "position": 4116
            }
        ]
    },
    {
        "header": "Appendix CHead Importance Analysis",
        "images": []
    },
    {
        "header": "Appendix DMeta Tokens: More Analysis and Visualization",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.13676/extracted/6013569/Figures/lmloss_lr.png",
                "caption": "Figure 14:Training curves of Hymba-1.5B.",
                "position": 4137
            },
            {
                "img": "https://arxiv.org/html/2411.13676/x11.png",
                "caption": "Figure 15:Visualize the layer-wise attention map entropy of (a) attention heads, and (b) SSM heads with and without meta tokens.",
                "position": 4140
            }
        ]
    },
    {
        "header": "Appendix EPretraining and Post-training Implementation Details",
        "images": []
    }
]