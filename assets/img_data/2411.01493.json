[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.01493/x1.png",
                "caption": "Figure 1:Win rate comparison of model responses against reference responses on the TL;DR task, judged by the preference oracle. All compared methods use the same optimization method (DPO).(Left)Performance improvements at convergence over SFT models achieved by offline (Offline DPO), passively online (Online DPO), and ouractive exploration(SEADPO) methods.(Right)The number of queries required by the passively online method (Passive) versus that by different active exploration methods to attain various levels of win rates.SEAachieves the best sample efficiency for online alignment compared to XPO and APL.",
                "position": 87
            },
            {
                "img": "https://arxiv.org/html/2411.01493/x2.png",
                "caption": "",
                "position": 96
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2LLM alignment as contextual dueling bandits",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.01493/x3.png",
                "caption": "Figure 2:Illustrative comparison between CDB and LLM alignment.",
                "position": 204
            }
        ]
    },
    {
        "header": "3How prior works (partially) solve LLM alignment as CDB",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.01493/x4.png",
                "caption": "Figure 3:Different paradigms for solving the LLM alignment problem in the CDB framework. Note that although all paradigms follow the LLM alignment interface (Figure2) with the interaction loop, some are actually offline or iteratively online (i.e., loop only once or a few times). Detailed comparisons will be made inSection3. We use colors to denotelearnable components,RL optimizer,direct optimizer, andactive exploration.rœïsubscriptùëüitalic-œïr_{\\phi}italic_r start_POSTSUBSCRIPT italic_œï end_POSTSUBSCRIPTdenotes a point estimate of human‚Äôs implicit reward, while‚ÑõŒ¶subscript‚ÑõŒ¶{\\mathcal{R}}_{\\Phi}caligraphic_R start_POSTSUBSCRIPT roman_Œ¶ end_POSTSUBSCRIPTrefers to an uncertainty-aware reward model.",
                "position": 257
            }
        ]
    },
    {
        "header": "4SEA: sample-efficient alignment for LLMs",
        "images": []
    },
    {
        "header": "5Experimental setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.01493/x5.png",
                "caption": "Figure 4:The learning system for experimenting online LLM alignment algorithms.",
                "position": 500
            }
        ]
    },
    {
        "header": "6Empirical results",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.01493/x6.png",
                "caption": "Figure 5:Win rate comparison of different algorithms against their initial SFT models across three scales and three direct optimizers.",
                "position": 524
            },
            {
                "img": "https://arxiv.org/html/2411.01493/x7.png",
                "caption": "Figure 6:Win rate comparison of different agent variants when using(Left)policy and(Right)Best-of-N sampling for inference.",
                "position": 609
            },
            {
                "img": "https://arxiv.org/html/2411.01493/x8.png",
                "caption": "Figure 7:(Left and Middle)Win rate comparison of different exploration strategies measured in E&E and BAI settings.(Right)Win rate comparison of different agents when usingGPT4o-minito simulate human feedback via LLM-as-a-judge.",
                "position": 622
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAlgorithm details",
        "images": []
    },
    {
        "header": "Appendix BOn connections with single-step RL",
        "images": []
    },
    {
        "header": "Appendix CSystem benchmarking",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.01493/x9.png",
                "caption": "Figure 8:Two example configurations ofoatused in benchmarking experiments.",
                "position": 1998
            },
            {
                "img": "https://arxiv.org/html/2411.01493/x10.png",
                "caption": "Figure 9:Averaged training latency (over 10 batches, equivalent to 1280 samples) comparingsail-sg/oatagainsthuggingface/trl.",
                "position": 2004
            }
        ]
    },
    {
        "header": "Appendix DFull experimental details",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.01493/extracted/5986906/assets/chat-gpt-online-feedback.jpg",
                "caption": "Figure 10:ChatGPT system asks for users‚Äô preference feedback to strategically explore better answers. In this case, algorithms should be designed around the objective ofminimizing cumulative regret(i.e., the E&E setting), because the quality of both responses generated by the system affects user experience.",
                "position": 2258
            }
        ]
    },
    {
        "header": "Appendix ESupplementary materials",
        "images": []
    }
]