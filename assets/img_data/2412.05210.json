[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05210/graph/unicoder_icon.png",
                "caption": "",
                "position": 107
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05210/x1.png",
                "caption": "Figure 1:A comparison between the GPT4o with better human preference and Qwen2.5-Coder-7B-Instruct. Qwen2.5-Coder-7B-Instruct solves the user question by simply replying with the code snippet without details.",
                "position": 142
            }
        ]
    },
    {
        "header": "2CodeArena",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05210/x2.png",
                "caption": "Figure 2:Task types of CodeArena.",
                "position": 272
            },
            {
                "img": "https://arxiv.org/html/2412.05210/x3.png",
                "caption": "Figure 3:Statistics of programming languages in CodeArena.",
                "position": 275
            },
            {
                "img": "https://arxiv.org/html/2412.05210/x4.png",
                "caption": "Figure 4:Number of samples of different difficulties (Easy/Medium/Hard) across categories in CodeArena.",
                "position": 278
            },
            {
                "img": "https://arxiv.org/html/2412.05210/x5.png",
                "caption": "Figure 5:Overview of the CodeArena creation benchmark. We first collect the online code Q&A and code-related raw text from the website. We cluster the code-related data and classify them into different categories using LLM. We uniformly sample the samples from different subtasks as the seed data for manual annotation.",
                "position": 443
            }
        ]
    },
    {
        "header": "3SynCode-Instruct",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05210/x6.png",
                "caption": "Figure 6:Prompt of generating large-scale self-contained synthetic instruction data.",
                "position": 487
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Results and Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05210/x7.png",
                "caption": "Figure 7:Examples of CodeArena. The LLM judger decides which response is better.",
                "position": 1723
            },
            {
                "img": "https://arxiv.org/html/2412.05210/x8.png",
                "caption": "Figure 8:Comparison between MultiPL-E and CodeArena. LLMs in the blue circle present relatively mismatched performances on two benchmarks.",
                "position": 1735
            },
            {
                "img": "https://arxiv.org/html/2412.05210/x9.png",
                "caption": "Figure 9:Results of CodeArena with different data size on MultiPL-E and CodeArena.",
                "position": 1745
            },
            {
                "img": "https://arxiv.org/html/2412.05210/x10.png",
                "caption": "Figure 10:Distribution of CodeArena and MultiPL-E of different languages.",
                "position": 1755
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]