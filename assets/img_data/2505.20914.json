[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20914/x1.png",
                "caption": "Figure 1:(a) Leverages compact semantic embeddings to enable object editability but fails to preserve appearance details.\n(b) Utilizes appearance features to retain visual fidelity, yet lacks editing capability.\nUnlike both, Our method implicitly learns the geometry-editable representation and explicitly aligns fine-grained appearance features with the geometry-edited representation, facilitating both precise geometry editing and faithful appearance preservation.",
                "position": 100
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20914/x2.png",
                "caption": "Figure 2:The training process of the proposed Disentangled Geometry-editable and Appearance-preserving Diffusion (DGAD) is as follows (the inference process is similar but involves iterative denoising): It first leverages semantic embeddings to implicitly capture the desired geometric transformations, and then employs a cross-attention retrieval mechanism to align fine-grained appearance features with the geometry-edited representation, facilitating both precise geometry editing and faithful appearance preservation in object composition.",
                "position": 140
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20914/x3.png",
                "caption": "Figure 3:Qualitative comparison with recent advanced methods. The results show that the proposed method can edit objects with desired geometric properties to align with the background scene while preserving their appearance details.For more detailed and comprehensive visualizations, please refer to the appendixA.2.",
                "position": 377
            },
            {
                "img": "https://arxiv.org/html/2505.20914/x4.png",
                "caption": "Figure 4:Left half:Visualization of the implicitly captured geometric properties of the object.Right half:Qualitative comparisons of “Ours w/o dense CA” and “Ours”, Using dense cross-attention effectively retrieves accurate appearance features, thereby promoting appearance preservation.",
                "position": 511
            }
        ]
    },
    {
        "header": "5Conclusion and Limitation",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20914/x5.png",
                "caption": "Figure 5:Qualitative comparison with recent advanced methods. The images shown are sampled from the test dataset.",
                "position": 1162
            },
            {
                "img": "https://arxiv.org/html/2505.20914/x6.png",
                "caption": "Figure 6:Qualitative comparison with recent advanced methods. The images shown are sampled from the test dataset.",
                "position": 1165
            },
            {
                "img": "https://arxiv.org/html/2505.20914/x7.png",
                "caption": "Figure 7:Qualitative comparison with recent advanced methods. The images shown are sourced from real-world scenes. We separate the foreground and background, then apply each method to perform object composition.",
                "position": 1168
            }
        ]
    },
    {
        "header": "Appendix ATechnical Appendices and Supplementary Material",
        "images": []
    }
]