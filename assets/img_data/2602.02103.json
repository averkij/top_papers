[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2CoT Planning Horizon",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02103/x1.png",
                "caption": "Figure 1:Results for the final-answer probing: average accuracy of In-Domain LLM for the first five tokens within CoT trajectories, measured across selected Transformers layers and tasks. The full figure across all tasks is presented inFigure13(see AppendixC).",
                "position": 366
            },
            {
                "img": "https://arxiv.org/html/2602.02103/x2.png",
                "caption": "(a)Parity example with In-Domain LLM.",
                "position": 369
            },
            {
                "img": "https://arxiv.org/html/2602.02103/x2.png",
                "caption": "(a)Parity example with In-Domain LLM.",
                "position": 372
            },
            {
                "img": "https://arxiv.org/html/2602.02103/x3.png",
                "caption": "(b)Cycle example with In-Domain LLM.",
                "position": 377
            },
            {
                "img": "https://arxiv.org/html/2602.02103/x4.png",
                "caption": "Figure 3:Average final-answer probing accuracy on CSQA with Off-the-Shelf LLM (Qwen3-32B) along CoT positions. The most frequent token at each position is annotated with its occurrence frequency. The notably earlier accuracy spikes are especially pronounced for Knowledge and Semantic tasks, but largely remain flat for Compositional tasks. The full results across all tasks are shown inFigure17for Off-the-Shelf LLM andFigure18for In-Domain LLM (AppendixC) .",
                "position": 430
            },
            {
                "img": "https://arxiv.org/html/2602.02103/x5.png",
                "caption": "Figure 4:Task accuracy comparison for Off-the-Shelf LLM (Qwen3-32B) under four settings: using thinking mode (w/ CoT); using non-thinking mode (w/o CoT); the best probing accuracy among initial CoT positions (Probing); the random-guess baseline (Random). The coarse signals of early final-answer planning are shown inferior to the direct prediction counterpart without CoT involved. Full results across all tasks are provided inFigure19. Similar comparisons for In-Domain LLM is provided inFigure20.",
                "position": 433
            },
            {
                "img": "https://arxiv.org/html/2602.02103/x6.png",
                "caption": "Figure 5:Top-5 accuracy for subsequent token prediction, using the last Transformers layer of In-Domain LLM. Full results across layers and tasks are presented inFigure21andFigure22.",
                "position": 506
            },
            {
                "img": "https://arxiv.org/html/2602.02103/x7.png",
                "caption": "Figure 6:Heatmap of the predicted reasoning length (y-axis) using initial CoT hidden states against the actual reasoning length (x-axis). The unreliable predictions suggest that a precise global plan does not emerge early in CoT, even for the task-aware In-Domain LLM. Full results across tasks are provided inFigure23and24.",
                "position": 509
            },
            {
                "img": "https://arxiv.org/html/2602.02103/x8.png",
                "caption": "(a)Reasoning length predictions for Parity.",
                "position": 512
            },
            {
                "img": "https://arxiv.org/html/2602.02103/x8.png",
                "caption": "(a)Reasoning length predictions for Parity.",
                "position": 515
            },
            {
                "img": "https://arxiv.org/html/2602.02103/x9.png",
                "caption": "(b)Input sequence length predictions for Parity.",
                "position": 520
            }
        ]
    },
    {
        "header": "3Leveraging CoT Dynamics",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02103/x10.png",
                "caption": "Figure 8:Spatial density distribution of selectedpivotpositions with In-Domain LLM along CoT paths. Using Tele-Lens, the selected positions tend to concentrate near CoT completion, whereas positions selected by general LM entropy typically are distributed across the entire CoT trajectory. Integrating multiple sources of latent signals may spur further improvement.",
                "position": 767
            }
        ]
    },
    {
        "header": "4Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATasks and Datasets",
        "images": []
    },
    {
        "header": "Appendix BIn-Domain LLM",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02103/x11.png",
                "caption": "Figure 9:Example of Parity Response with Off-the-Shelf LLM (Qwen3-32B). Full evaluation is discussed inSection2.5.",
                "position": 2509
            },
            {
                "img": "https://arxiv.org/html/2602.02103/x12.png",
                "caption": "Figure 10:Example of Parity Response with our In-Domain LLM trained via GRPO. The resulting reasoning trajectory is much shorter with predictable patterns, as discussed inSection2.5.2.",
                "position": 2512
            },
            {
                "img": "https://arxiv.org/html/2602.02103/x13.png",
                "caption": "Figure 11:Example of Cycle Response with our In-Domain LLM. The length of the reasoning trajectory is in proportional to the length of the path/cycle between two vertices, but not to the number of input edges. Without the latter heuristics, LLM is not able to reliably predict the total reasoning length at the initial stage of CoT, as discussed inSection2.5.3.",
                "position": 2515
            }
        ]
    },
    {
        "header": "Appendix CProbing Results",
        "images": []
    },
    {
        "header": "Appendix DLeveraging CoT Dynamics",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02103/x14.png",
                "caption": "Figure 12:Density distribution of LM entropy for next-token prediction steps per task. As depicted, most tokens exhibit low entropy, reflecting confident local transitions (Section3.1).",
                "position": 2711
            },
            {
                "img": "https://arxiv.org/html/2602.02103/x15.png",
                "caption": "Figure 13:Probing for final answers: averaged accuracy with In-Domain LLM for the first six tokens along CoT trajectories, measured across Transformers layers and tasks. Result discussions are addressed inSection2.5.1.",
                "position": 3312
            },
            {
                "img": "https://arxiv.org/html/2602.02103/x16.png",
                "caption": "Figure 14:Probing for final answers: averaged accuracy with Off-the-Shelf LLM (Qwen3-32B) for the first six tokens along CoT trajectories, measured across selected Transformers layers and tasks.",
                "position": 3315
            },
            {
                "img": "https://arxiv.org/html/2602.02103/x17.png",
                "caption": "(a)Parity example with Off-the-Shelf LLM.",
                "position": 3318
            },
            {
                "img": "https://arxiv.org/html/2602.02103/x17.png",
                "caption": "(a)Parity example with Off-the-Shelf LLM.",
                "position": 3321
            },
            {
                "img": "https://arxiv.org/html/2602.02103/x18.png",
                "caption": "(b)Cycle example with Off-the-Shelf LLM.",
                "position": 3326
            },
            {
                "img": "https://arxiv.org/html/2602.02103/x19.png",
                "caption": "(a)Trajectory example from GSM8K.",
                "position": 3333
            },
            {
                "img": "https://arxiv.org/html/2602.02103/x19.png",
                "caption": "(a)Trajectory example from GSM8K.",
                "position": 3336
            },
            {
                "img": "https://arxiv.org/html/2602.02103/x20.png",
                "caption": "(b)Trajectory example from MATH.",
                "position": 3342
            },
            {
                "img": "https://arxiv.org/html/2602.02103/x21.png",
                "caption": "(c)Trajectory example from Zebra.",
                "position": 3348
            },
            {
                "img": "https://arxiv.org/html/2602.02103/x22.png",
                "caption": "(d)Trajectory example from CSQA.",
                "position": 3354
            },
            {
                "img": "https://arxiv.org/html/2602.02103/x23.png",
                "caption": "Figure 17:Probing for final answers: averaged accuracy with Qwen3-32B along CoT positions. The most frequent token at each position is annotated with its occurrence frequency (the remaining 6 tasks are shown inFigure18).",
                "position": 3361
            },
            {
                "img": "https://arxiv.org/html/2602.02103/x24.png",
                "caption": "Figure 18:Probing for final answers: averaged accuracy by Qwen3-32B along CoT positions. The most frequent token at each position is annotated with its occurrence frequency. Result discussions are addressed nearFigure3.",
                "position": 3364
            },
            {
                "img": "https://arxiv.org/html/2602.02103/x25.png",
                "caption": "Figure 19:Task accuracy comparison for Off-the-Shelf LLM (Qwen3-32B) under four settings: using thinking mode (w/ CoT); using non-thinking mode (w/o CoT); the best probing accuracy among initial CoT positions (Probing); the random-guess baseline (Random).",
                "position": 3367
            },
            {
                "img": "https://arxiv.org/html/2602.02103/x26.png",
                "caption": "Figure 20:Task accuracy comparison for In-Domain LLM under four settings: standard inference with learned CoT (w/ CoT); direct prediction by a separately trained model via naive supervised finetuning, without CoT learned (w/o CoT); the best probing accuracy among initial CoT positions (Probing); the random-guess baseline (Random). Result discussions are addressed nearFigure4.",
                "position": 3370
            },
            {
                "img": "https://arxiv.org/html/2602.02103/x27.png",
                "caption": "Figure 21:Averaged Top-5 Accuracy of subsequent token prediction with In-Domain LLM, across Transformers layers and subsequent positions (up to the 8th following position). Result discussions are addressed nearFigure5.",
                "position": 3373
            },
            {
                "img": "https://arxiv.org/html/2602.02103/x28.png",
                "caption": "Figure 22:Averaged Top-5 accuracy for subsequent token prediction with Off-the-Shelf LLM, across selected Transformers layers and subsequent positions (up to the 8th following position).",
                "position": 3376
            },
            {
                "img": "https://arxiv.org/html/2602.02103/x29.png",
                "caption": "Figure 23:Probing for reasoning length: heatmap of the predicted length (y-axis) against the actual length (x-axis) for In-Domain LLM.",
                "position": 3379
            },
            {
                "img": "https://arxiv.org/html/2602.02103/x30.png",
                "caption": "Figure 24:Probing for reasoning length: heatmap of the predicted length (y-axis) against the actual length (x-axis) for Off-the-Shelf LLM. Result discussions are addressed nearFigure6.",
                "position": 3382
            }
        ]
    },
    {
        "header": "Appendix ERelated Works and Discussions",
        "images": []
    }
]