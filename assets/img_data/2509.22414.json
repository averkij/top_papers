[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22414/x1.png",
                "caption": "Figure 1:We present LucidFlux, a universal image restoration framework built on a large-scale diffusion transformer that delivers photorealistic restorations of real-world low-quality (LQ) images, outperforming state-of-the-art (SOTA) diffusion-based models across diverse degradations.",
                "position": 85
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22414/x2.png",
                "caption": "Figure 2:Overview of the proposed architecture for universal image restoration. Our method integrates dual condition streams (LQ and LRP) with timestep- and layer-adaptive modulation modules, and incorporates SigLIP semantic priors through a connector into a Flux-based DiT backbone to jointly enhance perceptual quality and semantic consistency.",
                "position": 146
            },
            {
                "img": "https://arxiv.org/html/2509.22414/x3.png",
                "caption": "Figure 3:Comparison of dataset attributes. Our dataset exhibits higher CLIP-IQA scores, lower flatness, and more diverse resolutions than Flickr2KAgustsson & Timofte (2017b)and DIV2KAgustsson & Timofte (2017a).",
                "position": 280
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22414/x4.png",
                "caption": "Figure 4:Qualitative comparisons on RealLQ250. Baseline methods either leave noticeable artifacts or yield over-smoothed textures, while our approach restores sharper details. See Figure8to Figure13in Appendix for more visual comparisons.",
                "position": 717
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22414/x5.png",
                "caption": "Figure 5:Impact of captions with and without degradation-related descriptions on restoration results.\nThe second to fourth columns illustrate that inconsistent captions generated by the same MLLM across different runs lead to variations in the restoration outcomes.\nThe fifth and sixth columns show that captions containing explicit degradation descriptions misguide the restoration model and result in inferior quality compared with captions focusing purely on content and style.",
                "position": 1635
            },
            {
                "img": "https://arxiv.org/html/2509.22414/x6.png",
                "caption": "Figure 6:t-SNE visualization of CLIP image–text embeddings. Our dataset covers a broader semantic range than Flickr2K and DIV2K, indicating richer image–text diversity.",
                "position": 1665
            },
            {
                "img": "https://arxiv.org/html/2509.22414/x7.png",
                "caption": "Figure 7:Qualitative comparison with commercial models on RealLQ250.",
                "position": 1734
            },
            {
                "img": "https://arxiv.org/html/2509.22414/x8.png",
                "caption": "Figure 8:More examples of visual comparison with open-source state-of-the-art methods on RealLQ250.",
                "position": 1737
            },
            {
                "img": "https://arxiv.org/html/2509.22414/x9.png",
                "caption": "Figure 9:More examples of visual comparison with open-source state-of-the-art methods on RealLQ250.",
                "position": 1740
            },
            {
                "img": "https://arxiv.org/html/2509.22414/x10.png",
                "caption": "Figure 10:More examples of visual comparison with open-source state-of-the-art methods on DRealSR.",
                "position": 1743
            },
            {
                "img": "https://arxiv.org/html/2509.22414/x11.png",
                "caption": "Figure 11:More examples of visual comparison with open-source state-of-the-art methods on RealSR.",
                "position": 1746
            },
            {
                "img": "https://arxiv.org/html/2509.22414/x12.png",
                "caption": "Figure 12:More examples of visual comparison with open-source state-of-the-art methods on Div2k-Val.",
                "position": 1749
            },
            {
                "img": "https://arxiv.org/html/2509.22414/x13.png",
                "caption": "Figure 13:More examples of visual comparison with open-source state-of-the-art methods on LSDIR-Val.",
                "position": 1752
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]