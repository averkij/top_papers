[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Domain2Vec",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10952/x1.png",
                "caption": "Figure 1:The domain vector of each sub-dataset of The Pile(Gao et al.,2021), where each row corresponds to a sub-dataset and each column corresponds to a meta-domain. The higher the proportion of data belonging to a particular meta-domain, the closer the color of the corresponding cell is toblue. We display distribution on some English meta-domains for clarity. The full picture is shown in Figure7.",
                "position": 197
            },
            {
                "img": "https://arxiv.org/html/2506.10952/extracted/6536950/figs/number_of_meta_domains.png",
                "caption": "Figure 2:The number of meta-domains vs. Inertia.",
                "position": 235
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10952/extracted/6536950/figs/pilot_study.png",
                "caption": "Figure 3:The validation loss on the EuroParl (The Pile) and Stackexchange (RedPajama) of models trained using data mixture in Table1. The validation loss on other validation sets are shown in AppendixD.",
                "position": 334
            },
            {
                "img": "https://arxiv.org/html/2506.10952/extracted/6536950/figs/lightGM.png",
                "caption": "Figure 4:Relationship between the number of trained data mixtures and the Spearman correlation.",
                "position": 474
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10952/extracted/6536950/figs/loss.png",
                "caption": "Figure 5:The validation loss on the Pile-CC subset.Domain2Vecachieves the comparable validation loss of Human (The model using original data mixture from The Pile), but uses only51.5%percent51.551.5\\%51.5 %training computational costs of Human. Using the same training cost,Domain2Veccan reduce the validation loss by approximately4.72%percent4.724.72\\%4.72 %compared to Human.",
                "position": 496
            },
            {
                "img": "https://arxiv.org/html/2506.10952/extracted/6536950/figs/tsne-3d-crop.png",
                "caption": "Figure 6:Visualization (t-SNE) of domain vectors of The Pile.",
                "position": 821
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetailed Description of the Distribution Alignment Assumption",
        "images": []
    },
    {
        "header": "Appendix BAlgorithm",
        "images": []
    },
    {
        "header": "Appendix CData Mixture of Different Methods",
        "images": []
    },
    {
        "header": "Appendix DExperimental Results of Pilot Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10952/extracted/6536950/figs/pile_en_qwen_Blues_final.png",
                "caption": "Figure 7:The Domain Vector of each sub-dataset of The Pile(Gao et al.,2021), where each row corresponds to a sub-dataset and each column corresponds to a meta-domain. The higher the proportion of data belonging to a particular meta-domain, the closer the color of the corresponding cell is toblue). Additionally, since The Pile primarily consists of English texts, we only display the distribution on English meta-domains for clarity.",
                "position": 2911
            },
            {
                "img": "https://arxiv.org/html/2506.10952/extracted/6536950/figs/pilot_study_1.png",
                "caption": "Figure 8:The validation loss on different dataset of models trained using data mixture in Table1.",
                "position": 2914
            },
            {
                "img": "https://arxiv.org/html/2506.10952/extracted/6536950/figs/pilot_study_2.png",
                "caption": "Figure 9:The validation loss on different dataset of models trained using data mixture in Table1.",
                "position": 2918
            }
        ]
    },
    {
        "header": "Appendix EComparative Study on Different Distributional Measures of DA2",
        "images": []
    }
]