[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3REINFORCE++ Enhancements",
        "images": []
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.03262/extracted/6110288/imgs/llama3.png",
                "caption": "Figure 1:General domain results show that PPO and REINFORCE++ have smaller length hacking issues compared to GRPO in general scenarios with Bradley-Terry Reward Models.",
                "position": 574
            },
            {
                "img": "https://arxiv.org/html/2501.03262/extracted/6110288/imgs/rule.jpg",
                "caption": "Figure 2:Mathematical scenario 1 shows that comparable results between REINFORCE++ and GRPO(Group Norm) under rule-based rewards.",
                "position": 577
            },
            {
                "img": "https://arxiv.org/html/2501.03262/extracted/6110288/imgs/math.jpg",
                "caption": "Figure 3:Mathematical scenario 2 results show that, under the same unit KL consumption, REINFORCE++ and RLOO achieve a greater reward increase compared to GRPO (Group Norm).",
                "position": 580
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]