[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25045/figures/detective.png",
                "caption": "",
                "position": 35
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25045/x1.png",
                "caption": "Figure 1:We first compress neural representations of the LLM‚Äôs next-token task (FF, blue). Next, we train a neural VSA encoder to map these neural embeddings into a proxy space, VSA encodings structured with input-related concepts (TT, orange).\nWe then probe the LLM‚Äôs embeddings by extracting concepts from VSA encodings using hypervector algebra (II, green).\nThis process of concept extraction ultimately enables deeper analysis of the model‚Äôs erroneous answers (red).",
                "position": 210
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Background",
        "images": []
    },
    {
        "header": "4Hyperdimensional probe",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25045/x2.png",
                "caption": "Figure 2:Our experimental setup uses textual inputs with syntactic structures unseen during training.",
                "position": 388
            }
        ]
    },
    {
        "header": "5Experiments and results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25045/x3.png",
                "caption": "Figure 4:LLM performance in completing the analogies with target words (left), and the effectiveness of our decoding method in extracting the targets from LLM latent representations (right).",
                "position": 647
            },
            {
                "img": "https://arxiv.org/html/2509.25045/x4.png",
                "caption": "Figure 5:Concepts extractedbeforeandafterthe LLM‚Äôs text generation, with respect toquestionandanswerfeatures.\nRed denotes the subset of failure instances, while green the full sampleùí¨¬Ø\\bar{\\mathcal{Q}}.",
                "position": 827
            }
        ]
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Reproducibility statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitations",
        "images": []
    },
    {
        "header": "Appendix BAlgorithm to process LLM embeddings as described inSectionÀú4.3",
        "images": []
    },
    {
        "header": "Appendix CArchitecture of ourHyperdimensional probe",
        "images": []
    },
    {
        "header": "Appendix DTraining performance of the neural VSA encoders",
        "images": []
    },
    {
        "header": "Appendix EUnbinding stage fromSectionÀú4.5",
        "images": []
    },
    {
        "header": "Appendix FExperimental results",
        "images": []
    },
    {
        "header": "Appendix GExperimental comparison",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25045/x5.png",
                "caption": "Figure 6:Comprehensive raw outputs obtained though DLA on OLMo-2 for a sampled analogy.",
                "position": 3461
            }
        ]
    },
    {
        "header": "Appendix HApplicability to other domains",
        "images": []
    },
    {
        "header": "Appendix IQuestion-answering setting fromSectionÀú5.3",
        "images": []
    },
    {
        "header": "Appendix JCosine similarities among the items of the VSA codebook",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25045/x6.png",
                "caption": "Figure 7:Distribution of pair-wise cosine similarities among the items of the codebook.",
                "position": 3635
            }
        ]
    },
    {
        "header": "Appendix KSpearman correlation for the QA-related experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25045/x7.png",
                "caption": "Figure 8:Spearman correlation coefficients computed onùí¨¬Ø\\bar{\\mathcal{Q}}.",
                "position": 3643
            },
            {
                "img": "https://arxiv.org/html/2509.25045/x8.png",
                "caption": "Figure 9:P-values of the Spearman correlation coefficients.",
                "position": 3646
            }
        ]
    },
    {
        "header": "Appendix LOverview of the experimental metrics",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25045/x9.png",
                "caption": "Figure 10:Experimental metrics of the LLM‚Äôs next-token prediction task and probing performance for Llama 4.Precision@kis displayed as a categorical variable, with its binary values portrayed as boolean. The categoryinitial tokenis associated to the special case (0.5) introduced inSectionÀú5.1. We measure VSA noise by computing the cosine similarity between the retrieved target concept and its codebook versionŒ¶\\Phi.",
                "position": 3657
            },
            {
                "img": "https://arxiv.org/html/2509.25045/x10.png",
                "caption": "Figure 11:Experimental metrics of the LLM‚Äôs next-token prediction task and probing performance for OLMo-2.Precision@kis displayed as a categorical variable, with its binary values portrayed as boolean. The categoryinitial tokenis associated to the special case (0.5) introduced inSectionÀú5.1.",
                "position": 3666
            }
        ]
    },
    {
        "header": "Appendix MSynthetic corpus",
        "images": []
    },
    {
        "header": "Appendix NDeclaration of LLM usage",
        "images": []
    },
    {
        "header": "Appendix ODimensionality reduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25045/x11.png",
                "caption": "Figure 12:Average Person correlations among the second half of model‚Äôs hidden layers for Llama3.1",
                "position": 3956
            },
            {
                "img": "https://arxiv.org/html/2509.25045/x12.png",
                "caption": "Figure 13:Silhouette scores for varying numbers of clusters, computed using a random sample of 10,000 textual inputs fromùíÆ\\mathcal{S}. The six language models have varied layer counts (seeTableÀú3), which results in different maximum possible cluster numbers.",
                "position": 4064
            },
            {
                "img": "https://arxiv.org/html/2509.25045/x13.png",
                "caption": "Figure 14:Distribution of model‚Äôs hidden layers grouped by k-means clustering within the ingestion algorithmFFfor Llama3.1-8B. It portrays the percentages of cluster assignments across all instances.",
                "position": 4072
            }
        ]
    },
    {
        "header": "Appendix PProof of concept for hyperdimensional probe in multimodal settings",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25045/x14.png",
                "caption": "Figure 15:Proof of concept for using hyperdimensional probe in multimodal settings. Figure A shows a complete probing procedure for a MNIST-based mathematical analogy. Figure B exhibits a VSA encodings describing a multimodal input using textual and image features.",
                "position": 4105
            }
        ]
    },
    {
        "header": "Appendix QComputational workload",
        "images": []
    }
]