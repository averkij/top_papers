[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.00169/x1.png",
                "caption": "Figure 1:An overview of the key sections and an illustrative end-to-end pipeline encompasses key elements like general pre-training tasks & data, foundation language models, domain-specific tasks & data, materials-oriented model adaptation, goal-driven Large Language Model (LLM) agents, and iterative open-ended materials experimentation.\nThe colored arrows show how feedback signals from open-ended experiments are routed to the agentic LLM, the materials science-tuned LLM, the pre-trained LLM, and their corresponding task modules.\nDifferent colors of arrows indicate the sources of the feedbacks.\nFor example, if general-purpose pre-training data does not contribute positively to the ultimate goal of novel and useful materials discovery, adjustments should be made by revising the pre-training tasks and corpus or fine-tuning the pre-trained LLM accordingly to better align model knowledge with the ultimate objective of discovering novel materials.\nDashed arrows denote the forward information flows.Note that humans are not only responsible for monitoring the open-ended experiments, but also co-improve with the agentic LLM.",
                "position": 401
            },
            {
                "img": "https://arxiv.org/html/2602.00169/x2.png",
                "caption": "Figure 2:Technology tree of AI4Material science research. With the emergence of LLMs and agents, research on materials science initially focused on domain specific task, primarily concentrating on seperate reactive tasks. Subsequent research has delved deeper, gradually integrating more with agentic systems for materials science.",
                "position": 668
            }
        ]
    },
    {
        "header": "2Recent Progress of AI and LLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.00169/fig/section_2.4.png",
                "caption": "Figure 4:The gradual integration of general AI into the materials science workflow. This framework illustrates the progressive evolution of broad, pre-trained foundation models as they are adapted for scientific discovery. Rather than a sudden replacement, the diagram depicts how general capabilities are iteratively refined through domain adaptation and feedback loops, allowing general-purpose AI to be gradually and effectively applied to specialized materials challenges.",
                "position": 865
            }
        ]
    },
    {
        "header": "3Reactive Tasks in Materials Science from an AI Perspective",
        "images": []
    },
    {
        "header": "4Agentic Systems for Materials Science",
        "images": []
    },
    {
        "header": "5Discussion & Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.00169/x3.png",
                "caption": "Figure 8:Search is defined as trial-and-error, generate-and-test, and variation-and-selection, whereas memory is to remember what worked best for each situation and start from there next time. In these processes, certain level of generalization is expected. The pre-trained and MatSci-tuned LLMs serve as the unified memory. The agentic LLM mostly acts as the search operator interacting with the environment, but it can access and store information during interaction into the memory. The agentic LLM can employ the search operator to interact with either the virtual world (e.g., a simulator) or the real world. Following the definition and notations in4, the directed informationùïÄ‚Äã(X‚ÜíY)\\mathbb{I}(X\\to Y)captures the influence that the past elementsXiX_{i}have on future elementsYiY_{i}.",
                "position": 1790
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]