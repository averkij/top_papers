[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.12953/x1.png",
                "caption": "Figure 1:The proposed MoDE architecture (left) uses a transformer with causal masking, where each block includes noise-conditional self-attention and a noise-conditioned router that assigns tokens to expert models based on the noise level. This design enables efficient, scalable action generation. On the right, the router’s activation of subsets of simple MLP experts with Swish-GLU activation during denoising is illustrated.",
                "position": 131
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.12953/x2.png",
                "caption": "Figure 2:After training MoDE, the router is noise-conditioned, allowing pre-computation of the experts used at each noise level before inference. This enables removing the router and retaining only the selected experts, significantly improving network efficiency.",
                "position": 365
            }
        ]
    },
    {
        "header": "4Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.12953/x3.png",
                "caption": "(a)LIBERO-90 Tasks",
                "position": 415
            },
            {
                "img": "https://arxiv.org/html/2412.12953/x3.png",
                "caption": "(a)LIBERO-90 Tasks",
                "position": 418
            },
            {
                "img": "https://arxiv.org/html/2412.12953/x4.png",
                "caption": "(b)Results for LIBERO-10 and LIBERO-90",
                "position": 423
            },
            {
                "img": "https://arxiv.org/html/2412.12953/x5.png",
                "caption": "(a)Environments",
                "position": 459
            },
            {
                "img": "https://arxiv.org/html/2412.12953/x5.png",
                "caption": "(a)Environments",
                "position": 462
            },
            {
                "img": "https://arxiv.org/html/2412.12953/x6.png",
                "caption": "(b)Example CALVIN-Rollout",
                "position": 467
            },
            {
                "img": "https://arxiv.org/html/2412.12953/x7.png",
                "caption": "Figure 5:Computational efficiency comparison betweenMoDEand a Dense-Transformer model with the same number of parameters.\nLeft: Average inference speed over 100 forward passes for various batch sizes.\nRight: FLOP count forMoDEwith router cache and without compared against a dense baseline.MoDEdemonstrates superior efficiency with lower FLOP count and faster inference thanks to its router caching and sparse expert design.",
                "position": 715
            },
            {
                "img": "https://arxiv.org/html/2412.12953/x8.png",
                "caption": "Figure 6:Visualized Expert Utilization.\nThe average usage of all experts inMoDEacross all layers is shown.\nPurple color corresponds to low usage and yellow color to high one, and each image is separately normalized. The average activation shows thatMoDElearns to utilize different experts for different noise levels.",
                "position": 845
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.12953/x9.png",
                "caption": "Figure 7:Example Scenes of the SIMPLERLi et al. (2024b)benchmark used to test generalist policies on various tasks from the Bridge and Google Fractal dataset.",
                "position": 2316
            },
            {
                "img": "https://arxiv.org/html/2412.12953/x10.png",
                "caption": "Figure 8:Scaling performance of MoDE and Dense-MoDE on CALVIN ABC and ABCD environments, showing average performance for2222to8888experts using best-performing variants for each environment.",
                "position": 2918
            },
            {
                "img": "https://arxiv.org/html/2412.12953/x11.png",
                "caption": "(a)",
                "position": 2924
            },
            {
                "img": "https://arxiv.org/html/2412.12953/x11.png",
                "caption": "(a)",
                "position": 2927
            },
            {
                "img": "https://arxiv.org/html/2412.12953/x12.png",
                "caption": "(b)",
                "position": 2933
            },
            {
                "img": "https://arxiv.org/html/2412.12953/x13.png",
                "caption": "(c)",
                "position": 2939
            },
            {
                "img": "https://arxiv.org/html/2412.12953/x14.png",
                "caption": "(d)",
                "position": 2945
            },
            {
                "img": "https://arxiv.org/html/2412.12953/x15.png",
                "caption": "(a)",
                "position": 2952
            },
            {
                "img": "https://arxiv.org/html/2412.12953/x15.png",
                "caption": "(a)",
                "position": 2955
            },
            {
                "img": "https://arxiv.org/html/2412.12953/x16.png",
                "caption": "(b)",
                "position": 2961
            },
            {
                "img": "https://arxiv.org/html/2412.12953/x17.png",
                "caption": "(c)",
                "position": 2967
            },
            {
                "img": "https://arxiv.org/html/2412.12953/x18.png",
                "caption": "(d)",
                "position": 2973
            },
            {
                "img": "https://arxiv.org/html/2412.12953/x19.png",
                "caption": "(a)",
                "position": 3008
            },
            {
                "img": "https://arxiv.org/html/2412.12953/x19.png",
                "caption": "(a)",
                "position": 3011
            },
            {
                "img": "https://arxiv.org/html/2412.12953/x20.png",
                "caption": "(b)",
                "position": 3017
            },
            {
                "img": "https://arxiv.org/html/2412.12953/x21.png",
                "caption": "Figure 12:This image shows the expert usage distribution across 12 layers in a Mixture of Experts (MoE) model. Each subplot represents a different layer (from layer 0 to layer 11), with experts labeled E0, E1, E2, and E3 on the x-axis. The y-axis indicates the log-scaled token count, displaying the frequency of tokens routed to each expert within that layer. The color gradient indicates the proportion of tokens assigned to each expert, where darker colors represent higher usage.",
                "position": 3165
            }
        ]
    },
    {
        "header": "Appendix AAppendix / supplemental material",
        "images": []
    }
]