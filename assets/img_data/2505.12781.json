[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.12781/x1.png",
                "caption": "Figure 1:LRC results that achieve higher accuracy with 1,000√ó\\times√ófewer training tokens, significantly boosting efficiency.",
                "position": 151
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Low-Rank Clone",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.12781/x2.png",
                "caption": "Figure 2:The overall procedure of LRC. To ensure clarity, attention and normalization modules are omitted.\nLRC involves two main steps:\n(1) Low-Rank Projection: applying low-rank projection matrices to compress the teacher‚Äôs weights into a lower-dimensional space, which are then assigned to the student;\n(2) Activation Clone, executing standard forward passes in both models to collect intermediate activations, which are aligned using Mean Squared Error (MSE) loss.",
                "position": 333
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.12781/x3.png",
                "caption": "Figure 3:Effect of LRC component ablations on LM loss convergence over training time.",
                "position": 977
            },
            {
                "img": "https://arxiv.org/html/2505.12781/x3.png",
                "caption": "Figure 3:Effect of LRC component ablations on LM loss convergence over training time.",
                "position": 980
            },
            {
                "img": "https://arxiv.org/html/2505.12781/x4.png",
                "caption": "Figure 4:The trend of MMLU scores with increasing training tokens.",
                "position": 985
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProof of Lemma 1",
        "images": []
    },
    {
        "header": "Appendix BThe Pseudo-code ofForwardFunction",
        "images": []
    },
    {
        "header": "Appendix CExperiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.12781/x5.png",
                "caption": "Figure 5:The trend of ARC-C scores with increasing training tokens.",
                "position": 3321
            },
            {
                "img": "https://arxiv.org/html/2505.12781/x6.png",
                "caption": "Figure 6:The impact ofŒ±ùõº\\alphaitalic_Œ±on model average performance.",
                "position": 3461
            },
            {
                "img": "https://arxiv.org/html/2505.12781/x7.png",
                "caption": "Figure 7:The impact of removing the clone loss from certain layers on the convergence of LM loss.",
                "position": 3471
            }
        ]
    },
    {
        "header": "Appendix DAdditional Experiments",
        "images": []
    }
]