[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21493/x1.jpg",
                "caption": "Figure 1.An exploded view, generated and enhanced by our frameworkBANG, of a futuristic mechanical humanoid where the fusion of organic form and mechanical precision is laid bare.\nEach component of the humanoid is generated by recursively exploding its parent component using Generative Exploded Dynamics (Sec.3) and enhanced through Per-part Geometric Details Enhancement (Sec.5.1). This process is conducted iteratively to create the final exploded view, which is rendered using Blender(Blender Foundation,oing).",
                "position": 207
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": []
    },
    {
        "header": "2.Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21493/figs/overview.jpg",
                "caption": "Figure 2.The overview illustrates the proposed framework forGenerative Exploded Dynamics. The pipeline consists of four stages:\nExploded Data Synthesis generates the training data (Section.3.2).\nExploded Dynamics Generator produces the exploded dynamics based on the input geometry (Section.3.1).\nTrajectory Optimization refines the trajectories of the exploded parts, ensuring smooth reassembly of the components (Section.3.3).\nFinally, Controllable Generation allows users to interactively control and refine the explosion by conditions (Section.4).",
                "position": 268
            }
        ]
    },
    {
        "header": "3.Generative Exploded Dynamics",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21493/figs/generator.jpg",
                "caption": "Figure 3.The architecture of the base generative model and adaptation modules in ourGenerative Exploded Dynamicsframework. The gray blocks represent the pretrained base model, which is a transformer-based latent diffusion model, and remains frozen after pretraining. The red blocks include the exploded view adapter and temporal attention module, which are learnable during the exploded dynamics training phase. During inference, input geometry, along with a target time sequence{t}\\{t\\}{ italic_t }, is fed into the exploded view adapter. Temporal attention ensures that the entire diffusion model outputs a continuous, smoothed geometry sequence in one pass.",
                "position": 331
            },
            {
                "img": "https://arxiv.org/html/2507.21493/x2.png",
                "caption": "Figure 4.Example data from our dataset illustrating synthetic exploded dynamics. The images show a cannon (top), spaceship (middle), and valve (bottom) transitioning fromt=0t=0italic_t = 0(left) tot=1t=1italic_t = 1(right), highlighting the decomposition of each object over time.",
                "position": 404
            },
            {
                "img": "https://arxiv.org/html/2507.21493/x3.png",
                "caption": "Figure 5.Histograms illustrating the distribution of key geometric characteristics within the data used to train exploded dynamics. The plots show the distribution of: (top left) the number of parts comprising each 3D asset; (top right) the asset bounding box expansion ratio after explosion optimization, which is calculated as the maximum dimension of the bounding box after explosion divided by the maximum dimension of the bounding box at the assembled state; (bottom left) the ratio between the minimum and maximum volumes of the parts, in logarithmic scale; and (bottom right) the initial overlapping volume between parts in the assembled state, in logarithmic scale.",
                "position": 437
            },
            {
                "img": "https://arxiv.org/html/2507.21493/figs/trackvisall.png",
                "caption": "Figure 6.Visualization of the SDF-based part trajectory tracking process. The figure illustrates this process for two example generated objects, with the cross-section SDF att=0t=0italic_t = 0shown as the background. Each row, from left to right, represents: (1) the assembled and exploded geometry; (2) the parts att=1t=1italic_t = 1, indicating the fully exploded state; (3) an intermediate state att=0.5t=0.5italic_t = 0.5, showing the parts moving along their trajectories; and (4) the final assembled result att=0t=0italic_t = 0. Red arrows denote the optimized trajectories, and part centers are highlighted with red circles. Parts far from the cross-section plane are omitted for clarity.",
                "position": 480
            }
        ]
    },
    {
        "header": "4.Controllable Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21493/figs/prompt.jpg",
                "caption": "Figure 7.The architecture of the extended exploded view adapter for processing spatial conditions. The conditions, provided as either a 3D bounding box or a selected surface region on the input geometry, is first encoded into tokens. These tokens are then fed into the Prompt Transformer Blocks, which exchange information with the Transformer Blocks of the exploded view adapter. This enables the system to dynamically adjust the exploded view generation process based on the user-defined constraints.",
                "position": 523
            },
            {
                "img": "https://arxiv.org/html/2507.21493/figs/promptvis.jpg",
                "caption": "Figure 8.Effect of spatial prompt control for exploded view generation. The figure illustrates two distinct examples: a sofa (left) and a wooden horse toy (right). The first row displays the generated exploded views without any spatial prompt input, serving as the baseline. The middle and bottom rows show the effects of varying spatial prompt settings, including different surface regions and varying bounding boxes. These prompts enable users to selectively control which parts to exploded, demonstrating the controllability and flexibility of our method.",
                "position": 528
            },
            {
                "img": "https://arxiv.org/html/2507.21493/figs/3dencoder.jpg",
                "caption": "Figure 9.2D-3D semantic correspondence is established by aligning features from a 2D image (top left) and its 3D geometry (top right) using geometric feature extractor and DINOv2. The bottom row demonstrates how selected regions of interest (ROIs) in the image corresponds to regions on the 3D geometry.",
                "position": 560
            },
            {
                "img": "https://arxiv.org/html/2507.21493/x4.jpg",
                "caption": "Figure 10.Enhanced geometry quality through per-part enhancement. We start with an input image of a robotic dinosaur from TRELLIS(Xiang et al.,2024)(bottom left). We generate a 3D geometry (center left) and explode it into parts (top left). Each part is then regenerated based on its coarse geometry, enhancing its detail (top right). Finally, the enhanced parts are reassembled, resulting in a more detailed and accurate 3D geometry (center right) that closely matches the input image.",
                "position": 596
            }
        ]
    },
    {
        "header": "5.Applications",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21493/figs/chat.jpg",
                "caption": "Figure 11.Interactive exploded views via chatbot integration: Our framework combines object understanding and generative explosion through interactive dialogue with a Chatbot. We showcase two interaction paradigms: “Exploded then Understanding” (left), where an automatic explosion generates functional descriptions, and “Understanding then Explosion” (right), where user queries guide the decomposition of specific parts.",
                "position": 621
            },
            {
                "img": "https://arxiv.org/html/2507.21493/figs/3dprint.jpg",
                "caption": "Figure 12.Expedite physical prototyping of combinable structures. Each part of a robot, generated from a cute robot design (featured in TRELLIS(Xiang et al.,2024)), is 3D printed using the X1 Carbon(Bambu Lab,2022)and then assembled (right column). The interlocking structures between parts is programmatically generated, allowing parts to be seamlessly connected and assembled after printing. This demonstrates our approach’s ability to preserve structural integrity while enabling easy post-printing assembly.",
                "position": 626
            },
            {
                "img": "https://arxiv.org/html/2507.21493/figs/ga.jpg",
                "caption": "Figure 13.A fictional journey from Earth’s surface to the far reaches of space, celebrating humanity’s boundless ingenuity and spirit of discovery. Each object is generated from a concept image and illustrated in four assembly states, using parts generated from ourGenerative Exploded Dynamics.",
                "position": 631
            },
            {
                "img": "https://arxiv.org/html/2507.21493/figs/ga1.jpg",
                "caption": "Figure 14.A steampunk workshop, where blueprints transform into tangible reality, powered by ourBANGframework. Each asset begins as a concept image generated by FLUX(Black Forest Labs,2023), is then transformed into an integral 3D mesh via our base generative model, and subsequently exploded into parts and are meticulously enhanced part-wise for maximum visual fidelity. The generated exploded structures are displayed against the backdrop, showcasing the enhanced details achieved through our exploded-enhance pipeline.",
                "position": 636
            }
        ]
    },
    {
        "header": "6.Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21493/figs/compare_complex.jpg",
                "caption": "Figure 16.Comparison on complex generated 3D assets. While segmentation methods exhibit fragmented patches, inconsistent part groupings, and jagged segmentation boundaries, especially under structural complexity, our method produces clean, volumetric part decompositions with consistent semantics and clear structural logic.",
                "position": 747
            },
            {
                "img": "https://arxiv.org/html/2507.21493/figs/gradient_sdf.png",
                "caption": "Figure 18.Visualization of the gradient in overlapping regions for a 2D toy case. By masking out the gradients from sampling points in the overlapping regions (indicated by the small red arrows within the rectangle), the yellow circle follows the correct optimization direction (orange arrows), aligning with the target geometry.",
                "position": 798
            },
            {
                "img": "https://arxiv.org/html/2507.21493/figs/framenumberGT.png",
                "caption": "Figure 19.Quantitative analysis of the impact of frame number on part trajectory tracking.\nTop: For ground truth sequences, both weighted IoU (red) and SDF objective (blue) stabilize after 3 frames, indicating that more frames improve tracking accuracy.\nBottom: For generated exploded dynamics, performance metrics continue to improve up to 5 frames, while computational time cost (purple bars) increases with more frames. Although our model was trained with a maximum of 5 frames due to GPU memory limitations, this result suggests that 5 frames offer a reasonable trade-off between tracking accuracy and computational efficiency, balancing performance with processing time.",
                "position": 851
            },
            {
                "img": "https://arxiv.org/html/2507.21493/figs/framenumber.png",
                "caption": "",
                "position": 855
            },
            {
                "img": "https://arxiv.org/html/2507.21493/figs/partscount.jpg",
                "caption": "Figure 20.Evaluation of the effect of part number embedding. The figure demonstrates the ability to control the number of parts through part number embedding. While achieving precise control can be challenging, the approach enables coarse control, where increasing the number of exploded parts in the embedding leads to the generation of more parts.",
                "position": 872
            }
        ]
    },
    {
        "header": "7.Discussions and Conclusions",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21493/figs/failure_cases.jpg",
                "caption": "Figure 21.Failure cases on highly detail geometry. Top: input meshes with complex structures. Bottom: generated exploded views. While BANG captures the overall structure, local detail is lost, and the exploded geometry drifts from the original. This is due to the lack of per-part supervision and limited token length in the current latent representation.",
                "position": 893
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]