[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.01427/x1.png",
                "caption": "",
                "position": 124
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.01427/x2.png",
                "caption": "Figure 2:The pipelines of our VideoAnydoor. First, we input the concatenation of the original video, object masks, and masked video into the 3D U-Net. Meanwhile, the background-removed reference image is fed into the ID extractor, and the obtained features are injected into the 3D U-Net. In our pixel warper, the reference image marked with key points and the trajectories are utilized as inputs for the content and motion encoders. Then, the extracted embeddings are input into cross-attentions for further fusion. The fused results serve as the input of a ControlNet, which extracts multi-scale features for fine-grained injection of motion and identity. The framework is trained with reweight reconstruction losses. We use a blend of real videos and image-simulated videos for training to compensate for the data scarcity.",
                "position": 200
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.01427/x3.png",
                "caption": "Figure 3:Pipeline of trajectory generation for training data. We first perform NMS to filter out densely-distributed points and then select points with larger motion. The retained ones can be sparsely distributed in each part of the target and contain more motion information, thus inducing more precise control.",
                "position": 238
            },
            {
                "img": "https://arxiv.org/html/2501.01427/x4.png",
                "caption": "Figure 4:Comparison resultsbetween VideoAnydoor and existing state-of-the-art video editing works. Our VideoAnydoor can achieve superior performance on precise control of both motion and content.",
                "position": 433
            },
            {
                "img": "https://arxiv.org/html/2501.01427/x5.png",
                "caption": "Figure 5:Demonstrations for precise motion control. VideoAnydoor can achieve precise alignment with the given trajectories and objects when using a pair of reference images marked with key-points and corresponding trajectory maps as input.",
                "position": 447
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.01427/x6.png",
                "caption": "Figure 6:More visual examplesof VideoAnydoor. It preserves fine-grained details (e.g., logos on the car) and achieves smooth motion control (e.g., the tail of the cat) with our pixel warper.",
                "position": 476
            },
            {
                "img": "https://arxiv.org/html/2501.01427/x7.png",
                "caption": "Figure 7:Qualitative ablation studieson the core components of VideoAnydoor. When removing the pixel warper, it suffers from poor motion consistency due to the undesired posture. And it can be observed that all the components contribute to the best performance.",
                "position": 554
            },
            {
                "img": "https://arxiv.org/html/2501.01427/x8.png",
                "caption": "Figure 8:More applications of VideoAnydoor. Our framework seamlessly supports various tasks like video virtual try-on, talk head generation, multi-region editing,etc.The results show that VideoAnydoor could effectively preserve the structure and identity and impose precise control on movements of multiple objects in diverse scenarios.",
                "position": 692
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]