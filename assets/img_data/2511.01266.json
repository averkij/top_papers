[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.01266/x1.png",
                "caption": "Figure 1:Prior motion-controlled video diffusion models typically operate offline to generate fixed-length sequences in parallel (top left). In contrast, ourMotionStreamenables streaming long-video generation from a single image with track control at interactive speed (bottom left).MotionStreamcan be applied to a variety of online downstream applications, such as real-time motion transfer, user drag operations, and 3D camera control (right).",
                "position": 76
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3MotionStream: Streaming Generation Meets Motion Controls",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.01266/x2.png",
                "caption": "Figure 2:Model architecture and training pipeline.To build a teacher motion-controlled video model, we extract and randomly sample 2D tracks from the input video and encode them using a lightweight track head. The resulting track embeddings are combined with the input image, noisy video latents, and text embeddings as input to the diffusion transformer with bidirectional attention, which is then trained with a flow matching loss (top).\nWe then distill a few-step causal diffusion model from the teacher through Self Forcing-style DMD distillation, integrating joint text-motion guidance into the objective, where autoregressive rollout with rolling KV cache and attention sink is applied during both training and inference (bottom).",
                "position": 156
            },
            {
                "img": "https://arxiv.org/html/2511.01266/x3.png",
                "caption": "Figure 3:Visualization of self attention probability map.We visualize attention probability maps for bidirectional, full causal,\nand causal sliding window attentions. Several attention heads focus on\nthe tokens corresponding to the initial frame throughout denoising generation.",
                "position": 211
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.01266/x4.png",
                "caption": "Figure 4:Quantitative ablation on guidance.We use Sora subset to ablate guidance strategies. Higher text guidance reduces overall metrics while motion guidance improves trajectory accuracy at the cost of visual quality (LPIPS).",
                "position": 575
            },
            {
                "img": "https://arxiv.org/html/2511.01266/x4.png",
                "caption": "Figure 4:Quantitative ablation on guidance.We use Sora subset to ablate guidance strategies. Higher text guidance reduces overall metrics while motion guidance improves trajectory accuracy at the cost of visual quality (LPIPS).",
                "position": 578
            },
            {
                "img": "https://arxiv.org/html/2511.01266/x5.png",
                "caption": "Figure 5:Qualitative ablation on guidance.Pure motion guidance produces rigid movements while text guidance enables natural motion and shape preservation even with imperfect tracks. OurHybridjoint guidance balances these two.",
                "position": 583
            },
            {
                "img": "https://arxiv.org/html/2511.01266/x6.png",
                "caption": "Figure 6:Impact of Sparse Attention Patterns.Using longer clips (up to 241 frames) from the Sora subset, we ablate attention sink size and local window size in extrapolation scenarios. Having at least a single sink chunk is crucial, but more provides marginal benefit, while larger window sizes degrade performance as attending to long-past history allows errors to accumulate in context tokens.",
                "position": 594
            },
            {
                "img": "https://arxiv.org/html/2511.01266/x7.png",
                "caption": "Figure 7:Qualitative Results.MotionStreamcan perform diverse downstream applications, including long video motion transfer (from offline or online sources), drag-based controls, and precise camera control with depth estimation. We showcase a few examples here.",
                "position": 597
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix ATraining Efficient Tiny VAE",
        "images": []
    },
    {
        "header": "Appendix BVBench results and User study",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.01266/x8.png",
                "caption": "Figure A1:User Study Results.We evaluate video quality through pairwise comparisons on 20 Sora samples. In terms of pure video quality, our models outperform all baselines except ATI, which uses a 10×\\timeslarger backbone (Wan 2.1-14B), producing visually favorable videos.",
                "position": 1006
            }
        ]
    },
    {
        "header": "Appendix CAdditional Ablation Experiments and Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.01266/x9.png",
                "caption": "Figure A2:Speed and Quality Tradeoffs with Chunk Sizes and Sampling Steps.We visualize the latency-throughput relationship across varying chunk sizes and sampling steps (left), and image quality (LPIPS) across different sampling steps, using our default setup ofc3s1w1(right).",
                "position": 1019
            },
            {
                "img": "https://arxiv.org/html/2511.01266/x10.png",
                "caption": "Figure A3:Long video extrapolation with and without attention sink.Models without attention sink (top two rows) exhibit cumulative drift over time, while our approach with attention sink (bottom) maintains stable quality throughout extrapolation.",
                "position": 1026
            },
            {
                "img": "https://arxiv.org/html/2511.01266/x11.png",
                "caption": "Figure A4:Qualitative Comparison.Generated videos from the Sora subset. As seen on the left, our model successfully reconstructs the flower blooming motion, while GWTF captures the motion but suffers quality degradation. ATI produces high-quality videos but with reduced trajectory adherence in motion transfer scenarios.",
                "position": 1177
            },
            {
                "img": "https://arxiv.org/html/2511.01266/x12.png",
                "caption": "Figure A5:Streaming Demo.We show some examples of our streaming demo. Green grids indicate the points that are being dragged (online), red grids indicate a set of points that should remain static, and blue grids indicate user’s pre-drawn trajectories for moving multiple points simultaneously (see chameleon example). More examples can be found on our supplementary website.",
                "position": 1183
            }
        ]
    },
    {
        "header": "Appendix DTraining Details",
        "images": []
    },
    {
        "header": "Appendix EEvalaution Protocols",
        "images": []
    },
    {
        "header": "Appendix FLimitation and Future Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.01266/x13.png",
                "caption": "Figure A6:Failure Cases.In the cat and turtle examples, the intention was to bring the cat out of the box and make the turtle hatch from the egg. However, due to limitations in hand-drawn trajectories for expressing complex motions and the backbone model’s generalization capacity, the outputs are not physically plausible, and objects are either deformed or simply translated along the tracks. In complex scenes with multiple human identities (second example), the model often loses identity information and produces artifacts. The last row shows our model’s output on the world exploration task. Since track representation struggles to capture complete scene transitions and the attention sink prioritizes preserving source features, our pipeline faces difficulty in scenarios where new objects appear or scenes continuously change. More videos can be found on our supplementary website.",
                "position": 1273
            }
        ]
    },
    {
        "header": "Appendix GEthics Statement",
        "images": []
    }
]