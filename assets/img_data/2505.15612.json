[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15612/x1.png",
                "caption": "Figure 1:Left: Accuracy and response length on AIME2024. For the figure of more benchmarks, please refer to AppendixA. Each point represents a single training run with different hyper-parameters. Given the high computational cost of obtaining this figure, the base model used isDeepSeek-R1-Distill-Qwen-1.5B. Results on 7B and 32B models are in §6.3. Our methods,Laser,Laser-D, andLaser-DEachieve a Pareto-optimal trade-off compared to all other methods. Notably, they yield a+6.16.1+\\bf{6.1}+ bold_6.1improvement in accuracy while reducing tokens usage by𝟔𝟑%percent63\\bf{63}\\%bold_63 %compared to the original model.Right: Example of a reasoning process afterLaser-Dtraining. While the original model produces meaningless “self-reflection” repeatedly for trivial questions like “1+1=?”, LRMs afterLaser-Dtraining efficiently recognize such questions during thinking and provide the answer directly.",
                "position": 247
            },
            {
                "img": "https://arxiv.org/html/2505.15612/x1.png",
                "caption": "",
                "position": 250
            },
            {
                "img": "https://arxiv.org/html/2505.15612/x2.png",
                "caption": "",
                "position": 254
            }
        ]
    },
    {
        "header": "2Preliminary",
        "images": []
    },
    {
        "header": "3Truncation: A Simple Yet Effective Baseline",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15612/x3.png",
                "caption": "Table 1:Results of baseline truncation method with different context window.TksubscriptT𝑘\\operatorname{T}_{k}roman_T start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPTdenotes the models after RL training with context windowk𝑘kitalic_k. Accuracy (%) with average token usage for each dataset. “Original” denotes the originalDeepSeek-R1-Distill-Qwen-1.5B.",
                "position": 322
            }
        ]
    },
    {
        "header": "4A Unified View on Efficient Reasoning with RL",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15612/x3.png",
                "caption": "Table 2:Formulation of different approaches based on Eq.2.C⁢(y)𝐶𝑦C(y){}italic_C ( italic_y )is mainly for correctness,S⁢(y)𝑆𝑦S(y)italic_S ( italic_y )is the length reward, andλ⁢(y)𝜆𝑦\\lambda(y)italic_λ ( italic_y )is a control variable to control how length reward is applied.𝕀⁢(R)𝕀𝑅\\mathbb{I}(R)blackboard_I ( italic_R )stands for𝕀⁢(R⁢(x,y)=1)𝕀𝑅𝑥𝑦1\\mathbb{I}(R(x,y)=1)blackboard_I ( italic_R ( italic_x , italic_y ) = 1 )and𝕀⁢(⋅)𝕀⋅\\mathbb{I}(\\cdot)blackboard_I ( ⋅ )is an indicator function.ρ𝜌\\rhoitalic_ρis the negative reward given for incorrect responses.L⁢(y)𝐿𝑦L(y)italic_L ( italic_y )is the length of the generated response.α𝛼\\alphaitalic_αis the coefficient that controls the magnitude of the length reward. The shapes of different rewards are shown in the visualization, where x axis is the length of the response.Bluerepresents the curve for correct responses, whileRedrepresents the curve for incorrect responses. For approaches, ThinkPrune,Laser-DandLaser-DE, there are different lines with similar colors indicate that the reward is dynamic which is realized by differentLAsubscript𝐿𝐴L_{A}italic_L start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPTvalues. The details of visualization are available in the AppendixJ.",
                "position": 422
            },
            {
                "img": "https://arxiv.org/html/2505.15612/x4.png",
                "caption": "",
                "position": 485
            },
            {
                "img": "https://arxiv.org/html/2505.15612/x5.png",
                "caption": "",
                "position": 508
            },
            {
                "img": "https://arxiv.org/html/2505.15612/x6.png",
                "caption": "",
                "position": 534
            },
            {
                "img": "https://arxiv.org/html/2505.15612/x7.png",
                "caption": "",
                "position": 555
            },
            {
                "img": "https://arxiv.org/html/2505.15612/x8.png",
                "caption": "",
                "position": 575
            },
            {
                "img": "https://arxiv.org/html/2505.15612/x9.png",
                "caption": "",
                "position": 595
            },
            {
                "img": "https://arxiv.org/html/2505.15612/x10.png",
                "caption": "",
                "position": 615
            },
            {
                "img": "https://arxiv.org/html/2505.15612/x11.png",
                "caption": "",
                "position": 637
            }
        ]
    },
    {
        "header": "5Adaptive Length-based Step Reward Shaping",
        "images": []
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15612/x12.png",
                "caption": "Table 3:Accuracy (%) with average token usage for each dataset and different methods. Most important results in this table are visualized in Figure1and Figure5in AppendixA. The base model isDeepSeek-R1-Distill-Qwen-1.5B. \"Original\" denotes the original model.Tksubscript𝑇𝑘T_{k}italic_T start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPTis the truncation method with context windowk𝑘kitalic_k. “Group” denotes the Efficient Reasoning[2]with differentα𝛼\\alphaitalic_α. Due to the space limit, we only show three most representative results here. For the full results, please refer to TabelHin AppendixH.",
                "position": 739
            },
            {
                "img": "https://arxiv.org/html/2505.15612/x12.png",
                "caption": "Table 4:Accuracy (%) with average token usage for each dataset and different methods using 7B and 32B models. \"Original\" denotes the original model.Tksubscript𝑇𝑘T_{k}italic_T start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPTis the truncation method with context windowk𝑘kitalic_k.",
                "position": 858
            },
            {
                "img": "https://arxiv.org/html/2505.15612/x12.png",
                "caption": "Figure 2:Performance on out-of-domain benchmarks: GPQA and average performance across all three benchmarks (GPQA, MMLU, LSAT).",
                "position": 934
            },
            {
                "img": "https://arxiv.org/html/2505.15612/x12.png",
                "caption": "",
                "position": 937
            },
            {
                "img": "https://arxiv.org/html/2505.15612/x13.png",
                "caption": "",
                "position": 941
            }
        ]
    },
    {
        "header": "7Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15612/x14.png",
                "caption": "Figure 3:Average keyword amount and response length over RL training on AIME24. The truncation method uses a 8192 token context window.Laser,Laser-D, andLaser-DEemploy a target length ofLT=2048subscript𝐿𝑇2048L_{T}=2048italic_L start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT = 2048.",
                "position": 981
            },
            {
                "img": "https://arxiv.org/html/2505.15612/x15.png",
                "caption": "Figure 4:Changes in reasoning behaviors ratio and response length over RL training iterations on AIME2024. The figure shows howLaser-DE’s thinking patterns change during training with a target length ofLT=2048subscript𝐿𝑇2048L_{T}=2048italic_L start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT = 2048.",
                "position": 984
            }
        ]
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APareto-Optimality",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15612/x16.png",
                "caption": "(a)",
                "position": 1396
            },
            {
                "img": "https://arxiv.org/html/2505.15612/x16.png",
                "caption": "(a)",
                "position": 1399
            },
            {
                "img": "https://arxiv.org/html/2505.15612/x1.png",
                "caption": "(b)",
                "position": 1404
            }
        ]
    },
    {
        "header": "Appendix BRatio of Truncated Responses During Training with Truncation",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15612/x17.png",
                "caption": "Figure 6:The ratio of truncated responses in training data with 8192 tokens limit.",
                "position": 1418
            }
        ]
    },
    {
        "header": "Appendix CDynamics of Accuracy and Rewards Throughout Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15612/x18.png",
                "caption": "(a)",
                "position": 1428
            },
            {
                "img": "https://arxiv.org/html/2505.15612/x18.png",
                "caption": "(a)",
                "position": 1431
            },
            {
                "img": "https://arxiv.org/html/2505.15612/x19.png",
                "caption": "(b)",
                "position": 1436
            }
        ]
    },
    {
        "header": "Appendix DSupplementary Details: Length-based Reward Shaping Formulations",
        "images": []
    },
    {
        "header": "Appendix ETraining Configurations",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15612/x20.png",
                "caption": "Figure 8:Training prompt for our training.",
                "position": 1671
            }
        ]
    },
    {
        "header": "Appendix FBudget-Forcing Inference",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15612/x21.png",
                "caption": "(a)",
                "position": 1739
            },
            {
                "img": "https://arxiv.org/html/2505.15612/x21.png",
                "caption": "(a)",
                "position": 1742
            },
            {
                "img": "https://arxiv.org/html/2505.15612/x22.png",
                "caption": "(b)",
                "position": 1747
            }
        ]
    },
    {
        "header": "Appendix GDynamics of Adaptive Target Lengths",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15612/x23.png",
                "caption": "Figure 10:Dynamics of adaptive target lengths during the training process ofLaser-DandLaser-DE. The figure shows how the adaptive target lengthLAsubscript𝐿𝐴L_{A}italic_L start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPTchanges over training iterations for problems of different difficulty levels (easy, medium, hard). For easy problems, the model selects short target lengths; for medium problems, it gradually decreases from higher initial values; and for hard problems, it maintains consistently high target lengths near the context window limit. This demonstrates the methods’ ability to adaptively allocate computational resources based on problem complexity, unlike fixed-length approaches.",
                "position": 1764
            }
        ]
    },
    {
        "header": "Appendix HFull Main Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15612/x24.png",
                "caption": "Table 6:Full results of accuracy (%) with average token usage for each dataset and different methods. The base model isDeepSeek-R1-Distill-Qwen-1.5B. \"Original\" denotes the original model.Tksubscript𝑇𝑘T_{k}italic_T start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPTis the truncation method with context windowk𝑘kitalic_k. “Group” denotes the Efficient Reasoning[2]with differentα𝛼\\alphaitalic_α. Due to the space limit, we only show three most representative results of truncation method here.",
                "position": 1774
            }
        ]
    },
    {
        "header": "Appendix IFull Experimental Results on Out-of-Domain Benchmarks",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15612/x24.png",
                "caption": "(a)GPQA",
                "position": 1872
            },
            {
                "img": "https://arxiv.org/html/2505.15612/x24.png",
                "caption": "(a)GPQA",
                "position": 1875
            },
            {
                "img": "https://arxiv.org/html/2505.15612/x25.png",
                "caption": "(b)LSAT",
                "position": 1880
            },
            {
                "img": "https://arxiv.org/html/2505.15612/x26.png",
                "caption": "(c)MMLU",
                "position": 1886
            },
            {
                "img": "https://arxiv.org/html/2505.15612/x27.png",
                "caption": "(d)Average",
                "position": 1891
            }
        ]
    },
    {
        "header": "Appendix JVisualization Details",
        "images": []
    },
    {
        "header": "Appendix KAnalysis of Reasoning Behaviros",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15612/x28.png",
                "caption": "Figure 12:Prompt used to identify and analyze reasoning behaviors withgpt-4.1-mini",
                "position": 2286
            }
        ]
    },
    {
        "header": "Appendix LQualitative Analysis on Efficient Reasoning",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15612/x29.png",
                "caption": "Figure 13:The full example of Figure1",
                "position": 2295
            },
            {
                "img": "https://arxiv.org/html/2505.15612/x30.png",
                "caption": "Figure 14:Additional case study demonstrating the evolution of reasoning efficiency. In this example, the original model required over 17K tokens to solve a question from the MATH500 dataset, while our trained model accomplished the same task using only 1K+ tokens.",
                "position": 2298
            },
            {
                "img": "https://arxiv.org/html/2505.15612/x31.png",
                "caption": "Figure 15:Further example demonstrating improvements in reasoning approach",
                "position": 2301
            }
        ]
    },
    {
        "header": "Appendix MLimitations",
        "images": []
    }
]