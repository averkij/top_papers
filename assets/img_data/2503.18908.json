[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.18908/extracted/6306213/assets/nvlogo2.png",
                "caption": "",
                "position": 84
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.18908/x1.png",
                "caption": "Figure 1:An overview of our FFN Fusion approach. Step1111: We apply Puzzle to partially remove FFN layers and remove entire attention layers. Step2222: We fuse consecutive FFN layers into a single wide FFN layer.",
                "position": 287
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3FFN Fusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.18908/x2.png",
                "caption": "Figure 2:Block-wise dependency heatmap for Ultra-PreFusion (log-scale).\nEach coordinate(i,j)𝑖𝑗(i,j)( italic_i , italic_j )encodes how much blockj𝑗jitalic_jdepends on blocki𝑖iitalic_i, measured by the cosine distance betweenhj⁢(𝑿)superscriptℎ𝑗𝑿h^{j}({\\bm{X}})italic_h start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT ( bold_italic_X )andh~ij⁢(𝑿)subscriptsuperscript~ℎ𝑗𝑖𝑿\\tilde{h}^{j}_{i}({\\bm{X}})over~ start_ARG italic_h end_ARG start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_italic_X ).Darker blue huesindicate weaker dependencies.\nThe attention-removed region (dotted box) shows consistently lower values, suggesting greater potential for parallelization.Darker red huesindicate stronger dependencies. Further analysis of this Figure can be found in AppendixE.",
                "position": 480
            }
        ]
    },
    {
        "header": "4Producing Large-Scale Models with FFN Fusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.18908/x3.png",
                "caption": "Figure 3:Comparison of Ultra-253B-Base before and after applying an additional longer continual pretraining.",
                "position": 515
            },
            {
                "img": "https://arxiv.org/html/2503.18908/x4.png",
                "caption": "Figure 4:Accuracy vs. latency performance of Ultra-253B-Base. Latency is measured on a single NVIDIA H100 node with tensor parallel (TP) 8, running in FP8. Theredline represents the efficient frontier, highlighting models with the best accuracy-to-throughput tradeoff. Accuracy =(MT-Bench×10+MMLU+MMLU-Pro+Arena Hard+HumanEval)/5MT-Bench10MMLUMMLU-ProArena HardHumanEval5(\\text{MT-Bench}\\times 10+\\text{MMLU}+\\text{MMLU-Pro}+\\text{Arena Hard}+\\text{%\nHumanEval})/5( MT-Bench × 10 + MMLU + MMLU-Pro + Arena Hard + HumanEval ) / 5.",
                "position": 590
            }
        ]
    },
    {
        "header": "5Additional Empirical Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.18908/x5.png",
                "caption": "Figure 5:Accuracy vs. Latency for FFN Removal vs. Fusion.",
                "position": 756
            },
            {
                "img": "https://arxiv.org/html/2503.18908/x6.png",
                "caption": "Figure 6:Per-layer metrics. Upper row is the cosine distance betweenf⁢(𝑿)𝑓𝑿f({\\bm{X}})italic_f ( bold_italic_X )and𝑿𝑿{\\bm{X}}bold_italic_Xfor the (a) The 49B model and (b) Ultra-253B-Base model. Bottom row represents the ratio betweenh⁢(𝑿)ℎ𝑿h({\\bm{X}})italic_h ( bold_italic_X )and𝑿𝑿{\\bm{X}}bold_italic_Xfor the (c) The 49B model and (d) Ultra-253B-Base model.",
                "position": 896
            }
        ]
    },
    {
        "header": "6Block Parallelization",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.18908/x7.png",
                "caption": "(a)",
                "position": 1012
            },
            {
                "img": "https://arxiv.org/html/2503.18908/x7.png",
                "caption": "(a)",
                "position": 1015
            },
            {
                "img": "https://arxiv.org/html/2503.18908/x8.png",
                "caption": "(b)",
                "position": 1020
            }
        ]
    },
    {
        "header": "7Concluding Remarks",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProof of Theorem3.1",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.18908/x9.png",
                "caption": "Figure 8:A visualization of FFN Fusion applied to SwiGLU. Two FFNs (left) are fused into a single FFN (right).",
                "position": 1823
            }
        ]
    },
    {
        "header": "Appendix BDatasets",
        "images": []
    },
    {
        "header": "Appendix CUltra-253B-Base and Puzzle-49B Architecture Overview",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.18908/x10.png",
                "caption": "(a)Ultra-PreFusion Attention Layout",
                "position": 1843
            },
            {
                "img": "https://arxiv.org/html/2503.18908/x10.png",
                "caption": "(a)Ultra-PreFusion Attention Layout",
                "position": 1846
            },
            {
                "img": "https://arxiv.org/html/2503.18908/x11.png",
                "caption": "(b)Ultra-PreFusion FFN Layout",
                "position": 1851
            },
            {
                "img": "https://arxiv.org/html/2503.18908/x12.png",
                "caption": "(c)Puzzle-49B Attention Layout",
                "position": 1857
            },
            {
                "img": "https://arxiv.org/html/2503.18908/x13.png",
                "caption": "(d)Puzzle-49B FFN Layout",
                "position": 1862
            }
        ]
    },
    {
        "header": "Appendix DMoE Inference Performance",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.18908/x14.png",
                "caption": "Figure 10:FFN Fusion helps reduce latency by increasing GPU utilization and by reducing syncs",
                "position": 1905
            },
            {
                "img": "https://arxiv.org/html/2503.18908/x15.png",
                "caption": "Figure 11:Replication of the block-wise dependency heatmap for Ultra-PreFusion, shown here for convenience.\nEach coordinate(i,j)𝑖𝑗(i,j)( italic_i , italic_j )represents the cosine distance betweenhj⁢(𝑿)superscriptℎ𝑗𝑿h^{j}({\\bm{X}})italic_h start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT ( bold_italic_X )andh~ij⁢(𝑿)subscriptsuperscript~ℎ𝑗𝑖𝑿\\tilde{h}^{j}_{i}({\\bm{X}})over~ start_ARG italic_h end_ARG start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_italic_X ), quantifying how much blockj𝑗jitalic_jdepends on blocki𝑖iitalic_i.Darker blueindicates weaker dependencies, whiledarker redindicates stronger dependencies.\nThe dotted box marks an attention-removed region with generally low dependency values, suggesting high parallelization potential.",
                "position": 1926
            }
        ]
    },
    {
        "header": "Appendix EFurther Pairwise Block Dependency Analysis",
        "images": []
    }
]