[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16257/x1.png",
                "caption": "(a)Layer 5 Key",
                "position": 414
            },
            {
                "img": "https://arxiv.org/html/2503.16257/x1.png",
                "caption": "(a)Layer 5 Key",
                "position": 417
            },
            {
                "img": "https://arxiv.org/html/2503.16257/x2.png",
                "caption": "(b)Layer 5 Value",
                "position": 422
            },
            {
                "img": "https://arxiv.org/html/2503.16257/x3.png",
                "caption": "(c)Layer 24 Key",
                "position": 427
            },
            {
                "img": "https://arxiv.org/html/2503.16257/x4.png",
                "caption": "(d)Layer 24 Value",
                "position": 432
            },
            {
                "img": "https://arxiv.org/html/2503.16257/x5.png",
                "caption": "(f)Layer 5 Value",
                "position": 434
            },
            {
                "img": "https://arxiv.org/html/2503.16257/x6.png",
                "caption": "(g)Layer 22 Key",
                "position": 437
            },
            {
                "img": "https://arxiv.org/html/2503.16257/x7.png",
                "caption": "(h)Layer 22 Value",
                "position": 440
            },
            {
                "img": "https://arxiv.org/html/2503.16257/x8.png",
                "caption": "(e)Layer 5 Key",
                "position": 457
            },
            {
                "img": "https://arxiv.org/html/2503.16257/x9.png",
                "caption": "Figure 2:Overview of our proposed method VidKV. We implement1.x-bitmixed-precision quantization for the key cache and 1.58-bit quantization for the value cache. In addition, as shown in the figure, to balance precision and model performance, we protect important visual tokens in the value cache. It is noteworthy that we perform mixed-precision quantization on the key cache along the channel dimension, whereas on the value cache, we apply mixed-precision quantization along the token dimension. All key-value caches are quantized in aper-channelfashion, different from prior KV cache quantization methods for LLMs such as KIVI[25].",
                "position": 493
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16257/x10.png",
                "caption": "Figure 3:Analysis of the normal channel in the key cache and its distribution following FFT transformation reveals that the frequency-domain distribution is smoother after using FFT transformation, resulting in reduced quantization error.",
                "position": 718
            },
            {
                "img": "https://arxiv.org/html/2503.16257/x11.png",
                "caption": "",
                "position": 733
            },
            {
                "img": "https://arxiv.org/html/2503.16257/x12.png",
                "caption": "Figure 4:Illustration of our 1.58-bit quantization for the value cache during the decoding stage.",
                "position": 1099
            }
        ]
    },
    {
        "header": "5Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16257/x13.png",
                "caption": "Figure 5:Additional results on KV cache1.x-bitquantization of VidKV. (a) Results indicate that the key cache undergoes a sharp performance drop when quantized to 1.1-bit. Notably, FFT-based 1-bit quantization is applied only when the key cache is quantized to 1.5-bit or 1.75-bit, due to the computational constraints of FFT. (b) For value cache, aspùëùpitalic_pincreases from 0 to 1, the average number of bits increases from 1.58 to 2.",
                "position": 1329
            },
            {
                "img": "https://arxiv.org/html/2503.16257/x14.png",
                "caption": "",
                "position": 1344
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "ADetailed Implementations",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16257/x15.png",
                "caption": "Figure 6:Showcases of our VidKV with LLaVA-OV-7B on long video understanding. Theredmark indicates the reasoning results after KV cache quantization remain consistent with the 16-bit, showing that there are no abnormal errors in the output after KV cache quantization.",
                "position": 2671
            }
        ]
    },
    {
        "header": "BAblation Study about the weightŒ≥ùõæ\\gammaitalic_Œ≥",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16257/extracted/6296208/ICCV2025-Author-Kit/figures/line_plot.png",
                "caption": "Figure 7:Ablation study ofŒ≥ùõæ\\gammaitalic_Œ≥.CI stands for correctness of information, DO stands for detail orientation, CU stands for contextual understanding, TU stands for temporal understanding, and CO stands for consistency.",
                "position": 2720
            }
        ]
    },
    {
        "header": "CMore Observations and Future Work",
        "images": []
    }
]