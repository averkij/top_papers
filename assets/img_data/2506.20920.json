[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.20920/x1.png",
                "caption": "",
                "position": 151
            },
            {
                "img": "https://arxiv.org/html/2506.20920/x2.png",
                "caption": "",
                "position": 151
            },
            {
                "img": "https://arxiv.org/html/2506.20920/x3.png",
                "caption": "",
                "position": 151
            },
            {
                "img": "https://arxiv.org/html/2506.20920/x4.png",
                "caption": "",
                "position": 151
            },
            {
                "img": "https://arxiv.org/html/2506.20920/x5.png",
                "caption": "",
                "position": 152
            },
            {
                "img": "https://arxiv.org/html/2506.20920/x6.png",
                "caption": "",
                "position": 152
            },
            {
                "img": "https://arxiv.org/html/2506.20920/x7.png",
                "caption": "",
                "position": 152
            },
            {
                "img": "https://arxiv.org/html/2506.20920/x8.png",
                "caption": "",
                "position": 152
            },
            {
                "img": "https://arxiv.org/html/2506.20920/x9.png",
                "caption": "",
                "position": 153
            },
            {
                "img": "https://arxiv.org/html/2506.20920/x10.png",
                "caption": "",
                "position": 153
            },
            {
                "img": "https://arxiv.org/html/2506.20920/x11.png",
                "caption": "",
                "position": 154
            },
            {
                "img": "https://arxiv.org/html/2506.20920/x12.png",
                "caption": "",
                "position": 154
            },
            {
                "img": "https://arxiv.org/html/2506.20920/x13.png",
                "caption": "",
                "position": 157
            },
            {
                "img": "https://arxiv.org/html/2506.20920/x14.png",
                "caption": "",
                "position": 158
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.20920/x15.png",
                "caption": "Figure 1:The FineWeb2 pipeline:Evaluation results of models trained on 350 billion tokens show that each pipeline step – Language Identification (LID), Deduplication (Dedup),Filtering, and Dedup-informed upsampling (Rehydration) – improves performance.",
                "position": 168
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Experimental setup",
        "images": []
    },
    {
        "header": "4The FineWeb2 pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.20920/x16.png",
                "caption": "Figure 2:Filtering rates by MinHash cluster sizefor French documents. The global filtering rate represents the overall percentage of documents removed during the full filtering process. Individual filtering rates are shown for each cluster size, providing a proxy for cluster quality—higher removal rates may indicate lower-quality clusters. We assignupsampling weightsto each cluster size based on the filtering rates.",
                "position": 566
            }
        ]
    },
    {
        "header": "5Validating and Applying the FineWeb2 Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.20920/x17.png",
                "caption": "Figure 3:High-level performance comparison of FineWeb2 to other multilingual and language-specific datasets. We evaluate performance both on the canary languages used to design the FineWeb2 pipeline as well as unseen languages. For brevity, for each language we plot the performance of only the best-performing single-language dataset. The best-performing dataset for each language is marked with★★\\bigstar★. Expanded results are provided inSectionA.9andSectionA.10.2.",
                "position": 620
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.20920/x18.png",
                "caption": "Figure 5:FT176 vs GlotLIDwithout any threshold filtering applied to either classifier. While GlotLID seems to outperform in higher resource languages, FT176 performs slightly better on lower resource languages. However, GlotLID supports a considerably larger number of (lower-resource) languages.",
                "position": 4467
            },
            {
                "img": "https://arxiv.org/html/2506.20920/x19.png",
                "caption": "Figure 6:Contamination scores for 1,900 languages via wordlist filtering.The plot indicates that the majority of the languages have their data in-language (non-contaminated).",
                "position": 5318
            },
            {
                "img": "https://arxiv.org/html/2506.20920/x20.png",
                "caption": "Figure 7:Per language comparison of FineWeb2to other multilingual and language-specific datasets. All models were trained for 30 billion tokens. The plots have sliding window smoothing of size 3.",
                "position": 7072
            },
            {
                "img": "https://arxiv.org/html/2506.20920/x21.png",
                "caption": "Figure 8:Language composition of FineWeb2Distribution of languages in the final FineWeb2 dataset. Percentages refer to total utf-8 bytes of each language or language family.",
                "position": 8623
            },
            {
                "img": "https://arxiv.org/html/2506.20920/x22.png",
                "caption": "Figure 9:Ratio of Wikipedia and Bible content per languageMost languages have a small fraction of their content originating from Wikipedia (with some exceptions). Bible content, on the other hand, is a big part of the corpora of many lower-resource languages.",
                "position": 10540
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]