[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.14470/x1.png",
                "caption": "Figure 1:Training methods determine what layers become critical.We measure the criticality of fifty different ResNet-50-based models that all utilize the same exact network architecture and training data (ImageNet-1k) but differ in their training methods. Darker spots denote layers that arecritical,i.e., in significantly different predictions and decreased performance after reset. Brighter spots areauxiliary,i.e., resetting these layers does not significantly affect the model. We denote the average (mean±plus-or-minus\\pm±std) layer criticality for both, a model across layers on the right, for a layer across model on the bottom.",
                "position": 62
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Methodology",
        "images": []
    },
    {
        "header": "3Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.14470/x2.png",
                "caption": "Figure 2:Adversarial training increases the average criticality proportional to the training attack budgetϵitalic-ϵ\\epsilonitalic_ϵ.We ablateℓ∞subscriptℓ\\ell_{\\infty}roman_ℓ start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPTfromℓ2subscriptℓ2\\ell_{2}roman_ℓ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT-norm training but do not observe any significant differences in their trends. The marker size in the plot indicates the validation accuracy on ImageNet-1k (larger is better).",
                "position": 153
            },
            {
                "img": "https://arxiv.org/html/2410.14470/x3.png",
                "caption": "Figure 3:Correlation between average network criticality and performance on ImageNet-1k.",
                "position": 176
            }
        ]
    },
    {
        "header": "4Conclusion, Limitations, and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.14470/x4.png",
                "caption": "Figure 4:Criticality difference to the baseline.In addition to the plot inFigure1, we here show the difference to the baseline model(He et al.,2015). Positive numbers indicate increases in criticality and negative numbers decrease.",
                "position": 1042
            },
            {
                "img": "https://arxiv.org/html/2410.14470/x5.png",
                "caption": "Figure 5:Criticality standard error.In addition to the plot inFigure1, we here show the standard error in criticality measurements over 3 runs with different seeds for the randomization.",
                "position": 1045
            }
        ]
    },
    {
        "header": "Appendix BModel Training Strategy Details",
        "images": []
    }
]