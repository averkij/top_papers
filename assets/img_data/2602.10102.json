[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10102/fig/fig1_seed_v5.png",
                "caption": "Figure 1:(left) VideoWorld 2 explores how to learn transferable knowledge from unlabeled real-world videos. We construct a handicraft benchmark to evaluate the learned knowledge. (right) Comparison of different frameworks in success rate for long-horizon paper folding tasks in Video-CraftBench. We split the task into seven key steps and evaluate sequential success rates (detailed in Sec.4). VDM (e.g., Wan2.2 14B[59]) produces high visual fidelity but fails to learn task-relevant dynamics or long-horizon policies.\nVideoWorld[51]improves policy learning but suffers from poor visual quality in real-world scenarios. The bottom-right figure presents the failure cases of these baseline methods.\nVideoWorld 2 learns more robust latent dynamics while also achieving significantly better visual quality, enabling generalizable long-horizon knowledge learning from videos.",
                "position": 122
            },
            {
                "img": "https://arxiv.org/html/2602.10102/x1.png",
                "caption": "Figure 2:Qualitative Results.VideoWorld 2 learns transferable knowledge and generates long-horizon videos in unseen environments. This figure shows the output on long-horizon handicraft tasks.",
                "position": 140
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10102/x2.png",
                "caption": "Figure 3:Overview of the VideoWorld 2 model architecture.(Left) First, the dLDM compresses future visual changes into compact and generalizable latent codes. These codes are then modeled by an autoregressive transformer.\n(Right) In inference, the transformer predicts latent codes for a new, unseen environment from the input image, which are subsequently decoded into task execution videos.",
                "position": 209
            },
            {
                "img": "https://arxiv.org/html/2602.10102/x3.png",
                "caption": "Figure 4:The proposed dynamics-enhanced latent dynamics model (dLDM).(Left) Latent dynamic model in VideoWorld[51]. Visual changes between the first and subsequent frames are compressed into a set of latent codes. (right) The dLDM proposed in VideoWorld 2. It employs a pre-trained VDM as an appearance prior, yielding better latent codes and facilitating high-fidelity video output.",
                "position": 234
            },
            {
                "img": "https://arxiv.org/html/2602.10102/x4.png",
                "caption": "Figure 5:Video clips with similar latent dynamic features.The text below represents the dynamic type.",
                "position": 238
            }
        ]
    },
    {
        "header": "4Video-CraftBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10102/x5.png",
                "caption": "Figure 6:Overview of Video-CraftBenchand keys steps of paper folding tasks. Best viewed in color.",
                "position": 278
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10102/fig/UMAP_seed_v2.png",
                "caption": "Figure 7:UMAP of latent codes.With a pretrained VDM, latent codes for the same action align more tightly than those in VideoWorld[51]across environments, indicating more consistent and transferable dynamics. (Left) In distinct environments like Bridge[58]and CALVIN[45], latent representations of the same action (e.g., the robotic arm moving right) are highly similar. (Right) Conversely, in VideoWorld, latent codes for the same action exhibit significant divergence across environments and fail to cluster effectively in the UMAP visualization.",
                "position": 1229
            },
            {
                "img": "https://arxiv.org/html/2602.10102/x6.png",
                "caption": "Figure 8:Visualization of failure casesin other baselines on the Video-CraftBench. Best viewed in color.",
                "position": 1234
            }
        ]
    },
    {
        "header": "6Further Discussion with Other Works",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "8Implementation Details",
        "images": []
    },
    {
        "header": "9Details in Video-CraftBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10102/fig/data_ana_time_1.png",
                "caption": "Figure 9:Video duration distribution.",
                "position": 2518
            },
            {
                "img": "https://arxiv.org/html/2602.10102/fig/data_ana_time_1.png",
                "caption": "Figure 9:Video duration distribution.",
                "position": 2521
            },
            {
                "img": "https://arxiv.org/html/2602.10102/fig/data_ana_task_1.png",
                "caption": "Figure 10:Task type distribution.",
                "position": 2526
            },
            {
                "img": "https://arxiv.org/html/2602.10102/x7.png",
                "caption": "Figure 11:More visualization of VideoWorld 2 on Video-CraftBench.",
                "position": 2723
            }
        ]
    },
    {
        "header": "10Visualizations",
        "images": []
    }
]