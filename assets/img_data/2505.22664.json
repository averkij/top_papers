[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22664/x1.png",
                "caption": "Figure 1:Zero-shot vision encoder graftingvia a small language surrogate (srgt) model to trigger the target LLM to perform visual understanding task without any additional training.",
                "position": 101
            },
            {
                "img": "https://arxiv.org/html/2505.22664/x2.png",
                "caption": "Figure 2:Reducing full decoder training costwith our surrogate-trained encoder for Llama-70B in VLMs.\nHollow‚ö™indicates the average score of the surrogate-trained encoder on the left.",
                "position": 106
            }
        ]
    },
    {
        "header": "1Building Surrogate Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22664/x3.png",
                "caption": "Figure 3:The trajectory of predictionacross different layers of Llama-3B, 8B, and 70B, and Gemma-2B from a different model family.\nThe arrow marks the transition point where the trajectories of 300 random samples converge.",
                "position": 170
            },
            {
                "img": "https://arxiv.org/html/2505.22664/x4.png",
                "caption": "Figure 4:Replacing layers with a translator.\nDespite the relative size in the illustration, our translator is simply an identical transformer layer inherited from the target LLM.\nThe translator bypasses many network layers, and is initialized from the shallowest original layer that it replaced.",
                "position": 247
            },
            {
                "img": "https://arxiv.org/html/2505.22664/x5.png",
                "caption": "Figure 5:Qualitative results on zero-shot grafting capabilityof encoders trained with small models for Llama-3B.\nFor comparison, we also include responses from the encoder trained with Llama-3B and the fine-tuned Llama-3B.\nThe encoder trained onùíØ‚Å¢(16,26)ùíØ1626\\mathcal{T}(16,26)caligraphic_T ( 16 , 26 )achieves strong zero-shot transfer to Llama-3B.\nResponse is sampled with greedy decoding.\nA ‚Üí¬†B denotes plugging A into B.",
                "position": 254
            },
            {
                "img": "https://arxiv.org/html/2505.22664/x6.png",
                "caption": "Figure 6:Qualitative results on zero-shot grafting capabilityof encoders trained with surrogate models for Llama-8B.\nFor comparison, we also include responses from the encoder trained with Llama-8B and the fine-tuned Llama-8B.\nMore early-phase layers preserved lead to stronger zero-shot grafting capability.\nResponses are sampled with greedy decoding.\nA ‚Üí¬†B denotes plugging A into B.",
                "position": 951
            },
            {
                "img": "https://arxiv.org/html/2505.22664/x7.png",
                "caption": "Figure 7:Qualitative results on strong zero-shot grafting abilityof surrogate-trained encoder for Llama-70B, which produces fine-grained image features to trigger Llama-70B to perform complex visual understanding tasks.\nResponse is sampled with greedy decoding.",
                "position": 1600
            }
        ]
    },
    {
        "header": "2Generalizing to Giant Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22664/x8.png",
                "caption": "Figure 8:Qualitative OCR results on strong zero-shot grafting abilityof surrogate-trained encoder for Llama-70B.\nThe input image size is 3362.",
                "position": 1661
            },
            {
                "img": "https://arxiv.org/html/2505.22664/x8.png",
                "caption": "Figure 8:Qualitative OCR results on strong zero-shot grafting abilityof surrogate-trained encoder for Llama-70B.\nThe input image size is 3362.",
                "position": 1664
            }
        ]
    },
    {
        "header": "3Related Work Overview",
        "images": []
    },
    {
        "header": "4Conclusion",
        "images": []
    },
    {
        "header": "A. Appendix",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22664/x9.png",
                "caption": "Figure A.1:The trajectory of predictionacross different layers of 1B, 4B, 12B, and 27B instruct models from Gemma-3.\nThe arrow marks the transition point where the trajectories of 300 random samples converge.",
                "position": 4808
            },
            {
                "img": "https://arxiv.org/html/2505.22664/x10.png",
                "caption": "Figure A.2:The trajectory of predictionacross different layers of 7B, 14B, 32B, and 72B instruct models from Qwen2.5.\nThe arrow marks the transition point where the trajectories of 300 random samples converge.",
                "position": 5245
            },
            {
                "img": "https://arxiv.org/html/2505.22664/x11.png",
                "caption": "Figure A.3:The trajectory of predictionacross different layers of 4B, 8B, 14B, and 32B instruct models from Qwen3.\nThe arrow marks the transition point where the trajectories of 300 random samples converge.",
                "position": 5251
            },
            {
                "img": "https://arxiv.org/html/2505.22664/x12.png",
                "caption": "",
                "position": 5318
            },
            {
                "img": "https://arxiv.org/html/2505.22664/x13.png",
                "caption": "Figure A.4:Training hyper-parameters, recipes and settings.",
                "position": 5328
            },
            {
                "img": "https://arxiv.org/html/2505.22664/x13.png",
                "caption": "Figure A.5:Dynamic loss weightsfor balancing loss contributions from different response lengths in a global batch.\nMultiple curves represent different maximum response lengths in a batch, gradually increasing from 30 to 300.",
                "position": 5603
            },
            {
                "img": "https://arxiv.org/html/2505.22664/x14.png",
                "caption": "Figure A.6:Training cost comparisonamong our surrogate training approach, baseline method, and training with distill models as surrogates for Llama-3B and 8B.",
                "position": 5841
            }
        ]
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "Disclaimer",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]