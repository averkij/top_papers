[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.14253/x1.png",
                "caption": "",
                "position": 84
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Motion 3-to-4",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.14253/x2.png",
                "caption": "Figure 2:An overview of our Motion 3-to-4 framework for 4D synthesis.At the core of the framework is a motionâ€“latent learning module consisting of a geometry encoder and a video encoder, which jointly process the input video and sampled points. The resulting latent tokens are decoded into a frame-wise 3D motion flow relative to the first video frame, producing temporally consistent 4D assets.",
                "position": 178
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.14253/x3.png",
                "caption": "Figure 3:Geometric comparisonon the Consistent4D benchmark[29]. Through spatially consistent motion reconstruction, we obtain plausible and high-quality 3D geometry.",
                "position": 652
            },
            {
                "img": "https://arxiv.org/html/2601.14253/x4.png",
                "caption": "Figure 4:Qualitative Comparisons.We compare our method with strong baselines including GVFD[100], L4GM[60], and V2M4[7]on our proposed Motion-80 benchmark. For fair evaluation, we render the generated 4D assets from all methods into an orthogonal novel view. Our approach produces more temporally coherent and structurally consistent motion. We invite reviewers to consult the supplemental material for animation visualization.",
                "position": 655
            },
            {
                "img": "https://arxiv.org/html/2601.14253/x5.png",
                "caption": "Figure 5:In-the-Wild Video-to-4D Synthesis.Our method generalizes to diverse in-the-wild inputs, including real-world videos (top row) and generated animations (bottom row). By formulating motion reconstruction as surface-to-pixel alignment, we achieve robust local correspondence reasoning across varied shapes and motion patterns.",
                "position": 665
            },
            {
                "img": "https://arxiv.org/html/2601.14253/x6.png",
                "caption": "Figure 6:Motion Transfer Example.By disentangling 4D synthesis into 3D mesh generation and motion reconstruction, our framework can animate static articulated objects with motion retargeted from videos of different sources.",
                "position": 686
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.14253/x7.png",
                "caption": "Figure 7:Failure cases.(A) Vertex sticking in challenging cases. (B) Initial mesh topology not able to adapt to later motion.",
                "position": 700
            }
        ]
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.14253/x8.png",
                "caption": "Figure 8:Additional visual results from synthesis video.",
                "position": 2309
            },
            {
                "img": "https://arxiv.org/html/2601.14253/x9.png",
                "caption": "Figure 9:Additional visual results from real-world video.",
                "position": 2319
            },
            {
                "img": "https://arxiv.org/html/2601.14253/x10.png",
                "caption": "Figure 10:Results of existing 3D model condition on generated video.",
                "position": 2329
            },
            {
                "img": "https://arxiv.org/html/2601.14253/x11.png",
                "caption": "Figure 11:More results from the held-out objaverse dataset.",
                "position": 2332
            },
            {
                "img": "https://arxiv.org/html/2601.14253/x12.png",
                "caption": "Figure 12:Visual comparison with SOTA methods on Consist4D.",
                "position": 2349
            }
        ]
    },
    {
        "header": "Appendix BMore Results",
        "images": []
    }
]