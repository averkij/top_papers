[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06942/x1.png",
                "caption": "",
                "position": 75
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06942/x2.png",
                "caption": "Figure 2:Method Overview.The SRPO contains two key elements: Direct-Align, and a single reward model that derives both rewards and penalties from positive and negative prompts. The pipeline of Direct-Align consists of four stages: (0) generate/load an image for training; (1) inject noise into image; (2) perform one-step denoise/inversion; (3) recover image.",
                "position": 87
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06942/x3.png",
                "caption": "Figure 3:Comparison on one-step prediction at early timestepThe values 0.075 and 0.025 denote the weight of the model prediction term used for method, respectively. The earliest 5% represent state with 95% noise from an unshifted timestep. By constructing a Gaussian prior, our one-step sampling method achieves high-quality results at early timesteps, even when the input image is highly noised.",
                "position": 196
            },
            {
                "img": "https://arxiv.org/html/2509.06942/x4.png",
                "caption": "Figure 4:Comparison of human evaluation results for Vanilla FLUX, ReFL, DRaFT_LV, DanceGRPO, Direct-Align, and SRPOon the criteria of Realism, Aesthetics, and Overall Preference. SRPO demonstrates significant improvements in Aesthetics and achieves a substantial reduction in AIGC artifacts.",
                "position": 433
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06942/x5.png",
                "caption": "Figure 5:Qualitative Comparison on FLUX, DanceGRPO and SRPO with same seed.Our approach demonstrates superior performance in realism and detail complexity.",
                "position": 441
            },
            {
                "img": "https://arxiv.org/html/2509.06942/x6.png",
                "caption": "Figure 6:Cross-reward results of SRPO.",
                "position": 467
            },
            {
                "img": "https://arxiv.org/html/2509.06942/x7.png",
                "caption": "Figure 7:Comparison of Optimization Effects of Different timestpe Intervals & Comparison of Reward-System and SRPO on Direct-Align.(1) Hacking Rate: Annotators compare three outputs and select the one that is least detailed or most over-processed, labeling it ashacking(2) The prompt isA young girl riding a gray wolf in a dark forest.Reward-System can only adjusts scale of rewards, resulting in trade-offs between two rewards effect. In contrast, SRPO penalizes out irrelevant directions from the reward, effectively preventing reward hacking and enhancing image texture.",
                "position": 490
            },
            {
                "img": "https://arxiv.org/html/2509.06942/x8.png",
                "caption": "Figure 8:Visualization of SRPO-controlled results for different style words",
                "position": 493
            },
            {
                "img": "https://arxiv.org/html/2509.06942/x9.png",
                "caption": "Figure 9:Overview of experimental results demonstrating the key properties of our SRPO method on the HPDv2 dataset:A: Comparison between FLUX.1.Krea and FLUX.1.dev following the application of our SRPO method. B: Comparison between our main model and vanilla FLUX.1.dev using realism-related recaptioning. C: Illustration of enhanced style control achieved through the incorporation of style-word conditioning. D: Ablation study on the main components of SRPO.",
                "position": 503
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "S1Extension to Aesthetic Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06942/x10.png",
                "caption": "Figure S1:Extension to the Aesthetic Model.The first row is trained with Direct-Align using the original Aesthetic Predictor 2.5, while the second row is trained using SRPO with Aesthetic Predictor 2.5.",
                "position": 1034
            }
        ]
    },
    {
        "header": "S2Comparison to GRPO",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06942/x11.png",
                "caption": "Figure S2:Comparison on GRPO and SRPO.",
                "position": 1044
            },
            {
                "img": "https://arxiv.org/html/2509.06942/x12.png",
                "caption": "Figure S3:High-frequency Word StatisticsÂ (part) in HPDv2 Training Set.",
                "position": 1047
            }
        ]
    },
    {
        "header": "S3High-frequency Word Statistics in HPDv2 Training Set",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06942/x13.png",
                "caption": "Figure S4:Qualitative Comparison on several methods with same seed.Our approach demonstrates superior performance in realism and detail complexity.",
                "position": 1061
            },
            {
                "img": "https://arxiv.org/html/2509.06942/x14.png",
                "caption": "Figure S5:Qualitative Comparison on offline-SRPO and online-SRPO",
                "position": 1275
            }
        ]
    },
    {
        "header": "S4Visualization Comparsion",
        "images": []
    }
]