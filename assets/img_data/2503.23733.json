[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.23733/x1.png",
                "caption": "Figure 1:(a) Illustration of three steps in AdaMMS: Step-1, mapping MLLMs with different model architecture; Step-2, merging MLLMs with linear interpolation; Step-3, searching for optimal merging hyper-parameter by approximate task performance through generation consistency without labeled data. (b) The gain performance of AdaMMS on a broad range of multimodal tasks in comparison with existing merging approaches. Gain refers to the improvement obtained by subtracting the average result from the result of the fused model on a certain task. The result here is the average of the gains from the two MLLM pairs merging.",
                "position": 104
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiment",
        "images": []
    },
    {
        "header": "5Results",
        "images": []
    },
    {
        "header": "6Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.23733/x2.png",
                "caption": "Figure 2:Results on merging LLaVA-v1.5-7B into Qwen2-VL-7B. TheŒ±ùõº\\alphaitalic_Œ±with the best perfo, bb=0 0 461 346rmance are the same as theŒ±ùõº\\alphaitalic_Œ±with the fewest response differences.",
                "position": 1116
            },
            {
                "img": "https://arxiv.org/html/2503.23733/x3.png",
                "caption": "Figure 3:Model responses with the change ofŒ±ùõº\\alphaitalic_Œ±in linear interpolation. Similar colors indicate similar responses.",
                "position": 1133
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AResults on Additional Model Pairs",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details of AdaMMS",
        "images": []
    },
    {
        "header": "Appendix CEvaluation Details",
        "images": []
    },
    {
        "header": "Appendix DComparing Supervised and Unsupervised",
        "images": []
    },
    {
        "header": "Appendix EIntermediate Results in Searching",
        "images": []
    },
    {
        "header": "Appendix FSupplementary Proof",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.23733/x4.png",
                "caption": "Figure 4:Results on linear interpolation at different granularities ofŒ±ùõº\\alphaitalic_Œ±when merging LLaVA-OneVison-7B into Qwen2-VL-7B-7B. (Left: MME, Right: OCRBench)",
                "position": 1775
            },
            {
                "img": "https://arxiv.org/html/2503.23733/x4.png",
                "caption": "",
                "position": 1778
            },
            {
                "img": "https://arxiv.org/html/2503.23733/x5.png",
                "caption": "",
                "position": 1782
            },
            {
                "img": "https://arxiv.org/html/2503.23733/x6.png",
                "caption": "Figure 5:Generation consistency and model performance (score) for MME, MMMU, OCRBench and SeedBench when merging LLaVA-OneVision-7B into Qwen2-VL-7B. Generation consistency is calculated as the reciprocal of the sum of different responses from models with adjacentŒ±ùõº\\alphaitalic_Œ±candidates. The horizontal axis is theŒ±ùõº\\alphaitalic_Œ±of the linear interpolation.",
                "position": 1788
            }
        ]
    },
    {
        "header": "Appendix GExperimental Results in Granularity forŒ±ùõº\\alphaitalic_Œ±",
        "images": []
    }
]