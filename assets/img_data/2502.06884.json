[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": []
    },
    {
        "header": "IIBackground",
        "images": []
    },
    {
        "header": "IIILearning Conformal Abstention Policies",
        "images": []
    },
    {
        "header": "IVExperiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06884/x1.png",
                "caption": "Figure 1:Accuracy vs. Expected Calibration Error (ECE) comparison of CAP, APS, and LAC across various VLMs and five datasets: MMBench, ScienceQA, OODCV, SEEDBench, and AI2D. An ideal model has high accuracy and low ECE (upper-left). ATCP shows significant ECE improvement over baselines. Please refer toFigure 4in AppendixC-D3for complete list of figures.",
                "position": 1447
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x2.png",
                "caption": "",
                "position": 1453
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x3.png",
                "caption": "",
                "position": 1454
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x4.png",
                "caption": "",
                "position": 1455
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x5.png",
                "caption": "Figure 2:Accuracy versus Expected Calibration Error (ECE) comparison between ATCP, APS and LAC methods across different LLMs and five datasets i.e. CosmosQA, HaluDial, HaluSum, HellaSwag, MMLU. The ideal model should have high accuracy and low ECE, indicating accurate predictions with well calibrated uncertainty quantification (upper-left of the plot). The ECE of ATCP shows significant improvement compared to baseline methods. Please refer toFigure 6in AppendixC-D3for the complete list of figures.",
                "position": 1461
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x6.png",
                "caption": "",
                "position": 1467
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x7.png",
                "caption": "",
                "position": 1468
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x8.png",
                "caption": "",
                "position": 1469
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x9.png",
                "caption": "Figure 3:Performance comparison of CAP (Ours), APS, and LAC on Llava-v1.6-34B (VLM) and Yi-34B (LLM) across four metrics: (i) accuracy, (ii) set size, (iii) AUROC, and (iv) AUARC. Each figure shows model performance across ten benchmark datasets, illustrating the impact of conformal method on uncertainty metrics.",
                "position": 1484
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x10.png",
                "caption": "",
                "position": 1490
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x11.png",
                "caption": "",
                "position": 1491
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x12.png",
                "caption": "",
                "position": 1492
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x13.png",
                "caption": "",
                "position": 1493
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x14.png",
                "caption": "",
                "position": 1501
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x15.png",
                "caption": "",
                "position": 1502
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x16.png",
                "caption": "",
                "position": 1503
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x17.png",
                "caption": "",
                "position": 1504
            }
        ]
    },
    {
        "header": "VConclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AFormal Proof of Conformal Coverage Guarantee",
        "images": []
    },
    {
        "header": "Appendix BTraining via Reinforcement Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06884/x18.png",
                "caption": "Figure 4:Accuracy vs. Expected Calibration Error (ECE) comparison of CAP, APS, and LAC across various VLMs and five datasets: MMBench, ScienceQA, OODCV, SEEDBench, and AI2D. An ideal model has high accuracy and low ECE (upper-left). ATCP shows significant ECE improvement over baselines.",
                "position": 3816
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x19.png",
                "caption": "",
                "position": 3832
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x20.png",
                "caption": "",
                "position": 3833
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x21.png",
                "caption": "Figure 5:Accuracy versus Expected Calibration Error (ECE) comparison between CAP, APS and LAC methods across different VLMs and five datasets i.e. MMBench, ScienceQA, OODCV, SEEDBench, AI2D. The ideal model should have high accuracy and low ECE, indicating accurate predictions with well calibrated uncertainty quantification (upper-left of the plot). The ECE of ATCP shows significant improvement compared to baseline methods.",
                "position": 3839
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x22.png",
                "caption": "",
                "position": 3845
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x23.png",
                "caption": "",
                "position": 3846
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x24.png",
                "caption": "",
                "position": 3847
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x25.png",
                "caption": "",
                "position": 3855
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x26.png",
                "caption": "",
                "position": 3856
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x27.png",
                "caption": "Figure 6:Accuracy versus Expected Calibration Error (ECE) comparison between ATCP, APS and LAC methods across different LLMs and five datasets i.e. CosmosQA, HaluDial, HaluSum, HellaSwag, MMLU. The ideal model should have high accuracy and low ECE, indicating accurate predictions with well calibrated uncertainty quantification (upper-left of the plot). The ECE of ATCP shows significant improvement compared to baseline methods.",
                "position": 3862
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x28.png",
                "caption": "",
                "position": 3878
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x29.png",
                "caption": "",
                "position": 3879
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x30.png",
                "caption": "Figure 7:Accuracy versus Expected Calibration Error (ECE) comparison between ATCP, APS and LAC methods across different LLMs and five datasets i.e. CosmosQA, HaluDial, HaluSum, HellaSwag, MMLU. The ideal model should have high accuracy and low ECE, indicating accurate predictions with well calibrated uncertainty quantification (upper-left of the plot). The ECE of ATCP shows significant improvement compared to baseline methods.",
                "position": 3885
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x31.png",
                "caption": "",
                "position": 3891
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x32.png",
                "caption": "",
                "position": 3892
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x33.png",
                "caption": "",
                "position": 3893
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x34.png",
                "caption": "",
                "position": 3901
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x35.png",
                "caption": "",
                "position": 3902
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x36.png",
                "caption": "Figure 8:Performance comparison of VLMs with different model sizes (2.7B to 34B) across various metrics. Figures from left to right represents the performance of four models on one of the four metrics i) accuracy, ii) set size, iii) AUROC, and iv) AUARC respectively. In each figure, we have drawn the performance of models across five datasets in VLM benchmark. Each figure represents the effect of model scale (number of parameters) in its performance across different uncertainty metrics.",
                "position": 3918
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x37.png",
                "caption": "",
                "position": 3924
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x38.png",
                "caption": "",
                "position": 3925
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x39.png",
                "caption": "",
                "position": 3926
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x40.png",
                "caption": "",
                "position": 3927
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x41.png",
                "caption": "Figure 9:Performance comparison of additional VLMs with different model sizes (6B to 7B) across various metrics. Figures from left to right represents the performance of four models on one of the four metrics i) accuracy, ii) set size, iii) AUROC, and iv) AUARC respectively. In each figure, we have drawn the performance of models across five datasets in VLM benchmark. Each figure represents the effect of model scale (number of parameters) in its performance across different uncertainty metrics.",
                "position": 3933
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x42.png",
                "caption": "",
                "position": 3939
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x43.png",
                "caption": "",
                "position": 3940
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x44.png",
                "caption": "",
                "position": 3941
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x45.png",
                "caption": "",
                "position": 3942
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x46.png",
                "caption": "Figure 10:Performance comparison of LLMs with different model sizes (7B to 34B) across various metrics. Figures from left to right represents the performance of four models on one of the four metrics i) accuracy, ii) set size, iii) AUROC, and iv) AUARC respectively. In each figure, we have drawn the performance of models across five datasets in LLM benchmark. Each figure represents the effect of model scale (number of parameters) in its performance across different uncertainty metrics.",
                "position": 3948
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x47.png",
                "caption": "",
                "position": 3954
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x48.png",
                "caption": "",
                "position": 3955
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x49.png",
                "caption": "",
                "position": 3956
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x50.png",
                "caption": "",
                "position": 3957
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x51.png",
                "caption": "Figure 11:Performance comparison of Llama-2 series LLMs with different model sizes (7B and 13B) across various metrics. Figures from left to right represents the performance of two models on four metrics i) accuracy, ii) set size, iii) AUROC, and iv) AUARC respectively. In each figure, we show the performance of models across five datasets in LLM benchmark. Each figure represents the effect of model scale (number of parameters) in its performance across different uncertainty metrics.",
                "position": 3963
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x52.png",
                "caption": "",
                "position": 3969
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x53.png",
                "caption": "",
                "position": 3970
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x54.png",
                "caption": "",
                "position": 3971
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x55.png",
                "caption": "",
                "position": 3972
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x56.png",
                "caption": "Figure 12:Performance comparison of Llama-2 series LLMs with different model sizes (7B and 13B) across various metrics. Figures from left to right represents the performance of two models on four metrics i) accuracy, ii) set size, iii) AUROC, and iv) AUARC respectively. In each figure, we show the performance of models across five datasets in LLM benchmark. Each figure represents the effect of model scale (number of parameters) in its performance across different uncertainty metrics.",
                "position": 3978
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x57.png",
                "caption": "",
                "position": 3984
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x58.png",
                "caption": "",
                "position": 3985
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x59.png",
                "caption": "Figure 13:Distribution of CAP’s prediction types for LLaVA-1.6-34B (VLM) and Yi-34B (LLM). The model’s responses are categorized into single predictions (confident single answers), set predictions (multiple possible answers), and abstentions (declining to answer).",
                "position": 3991
            },
            {
                "img": "https://arxiv.org/html/2502.06884/x60.png",
                "caption": "",
                "position": 3997
            }
        ]
    },
    {
        "header": "Appendix CExperimental Details",
        "images": []
    }
]