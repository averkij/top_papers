[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.07096/extracted/6440300/images/introduction.png",
                "caption": "Figure 1:Overview ofX-Sim:We introduceX-Sim, a real-to-sim-to-real framework that bridges the human-robot embodiment gap by learning robot policies.Real-to-Sim. We generate photorealistic simulation using object-centric rewards generated from human videos.Training X-Sim. We first train RL policies with privileged state using GPU-parallelized environment. Then, we collect a diverse image-action dataset use it to distill behaviors into an image-conditioned policy.Sim-to-Real.Image-based policy is deployed in the real-world. Its observation encoder automatically calibrates itself by aligning real and sim image observations at test-time.",
                "position": 129
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.07096/extracted/6440300/images/approach.png",
                "caption": "Figure 2:Real-to-Sim:X-Simreconstructs a photorealistic environment with accurate geometry from multi-view images. It tracks object motion across time from an RGBD human video to define a dense object-centric reward function to train RL policies in simulation.",
                "position": 190
            },
            {
                "img": "https://arxiv.org/html/2505.07096/extracted/6440300/images/approach_sim.png",
                "caption": "Figure 3:Sim-to-Real:(Left)X-Simdistills privileged-state policies into image-conditioned policies by generating and a synthetic image-action dataset using multiple environment randomizations.(Right)During deployment, real policy rollouts are replayed in simulation to generate paired images across real and sim. Their discrepancy is utilized to minimize and calibrate the sim-to-real visual gap.",
                "position": 193
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.07096/extracted/6440300/images/barplots.png",
                "caption": "Figure 4:Performance on Real-World Tasks:We reportAvg. Task Progresson 5 tasks across two environments, and find thatX-Simboth with and without calibration consistently outperforms hand-tracking baselines that attempt to retarget human hand motion. A rough sketch of each task is visualized on top.",
                "position": 261
            },
            {
                "img": "https://arxiv.org/html/2505.07096/extracted/6440300/images/hand_retargeting_failures.png",
                "caption": "Figure 5:Hand Re-targeting Failure Modes:Hand Maskfails due to a significant visual domain gap between human and robots, even when the motions are physically feasible for the robot.Object-Aware IKfails under execution mismatch, as certain human hand motions are kinematically or dynamically infeasible.",
                "position": 301
            },
            {
                "img": "https://arxiv.org/html/2505.07096/extracted/6440300/images/calibration.png",
                "caption": "Figure 7:Sim-to-Real Calibration:We compareX-Simimage embeddings using t-SNE before and after calibration for one rollout.",
                "position": 351
            },
            {
                "img": "https://arxiv.org/html/2505.07096/extracted/6440300/images/scaling.png",
                "caption": "Figure 8:Data Efficiency:X-Simscales more efficiently with data collection time than behavior cloning from robot teleoperation, achieving comparable success onMustard Placewith 10×\\times×less time.",
                "position": 366
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "7Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.07096/extracted/6440300/images/task-imgs.jpeg",
                "caption": "Figure 10:Visualization of tasks that we report results for in Fig.4",
                "position": 1383
            },
            {
                "img": "https://arxiv.org/html/2505.07096/extracted/6440300/images/mustard-data-viz.png",
                "caption": "Figure 11:Visualization of training states for results in Fig.8",
                "position": 1741
            },
            {
                "img": "https://arxiv.org/html/2505.07096/extracted/6440300/images/viewpoints-viz.png",
                "caption": "Figure 12:Visualization of viewpoints for results in Fig.9",
                "position": 1751
            }
        ]
    },
    {
        "header": "8Appendix",
        "images": []
    }
]