[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.20279/x1.png",
                "caption": "Figure 1:Comparison of idealized vs. real-world dense prediction.Left: Idealized tasks under controlled conditions. Right: Real-world tasks with complexity and noise, posing greater challenges.",
                "position": 109
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.20279/x2.png",
                "caption": "Figure 2:Overview of the DenseWorld benchmark.Upper left: the construction pipeline. Center left: examples of representative tasks across five real-world categories. Lower left: unified evaluation. Right: full taxonomy of 25 dense prediction tasks, each aligned with a practical application scenario.",
                "position": 184
            }
        ]
    },
    {
        "header": "3DenseWorld Benchmark",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.20279/x3.png",
                "caption": "Figure 3:Overview of the DenseDiT architecture for data-efficient real-world dense prediction.",
                "position": 408
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.20279/x4.png",
                "caption": "Figure 4:Qualitative comparisons on pixel-level regression tasks.In the first and second column, DenseDiT successfully predicts occluded structures in fog or shadow, highlighting its capability for scene-level reasoning. The third column showcases DenseDiTâ€™s ability to capture fine-grained details such as distant lampposts and layered foliage. The forth column emphasizes its sensitivity to abrupt depth transitions, producing sharper and more consistent boundaries than competing models.",
                "position": 813
            },
            {
                "img": "https://arxiv.org/html/2506.20279/x5.png",
                "caption": "Figure 5:Qualitative comparisons on pixel-level classification tasks.The first row shows strong resilience to cluttered backgrounds in complex scenes. In the second row, DenseDiT perceptively captures abstract concepts such as fire regions with minimal ambiguity. The third and fourth rows demonstrate its ability to localize fine structures, including spinal contours in medical images and dense urban building layouts in satellite views.",
                "position": 816
            },
            {
                "img": "https://arxiv.org/html/2506.20279/x6.png",
                "caption": "Table 4:Effectiveness of the demonstration branch.",
                "position": 841
            },
            {
                "img": "https://arxiv.org/html/2506.20279/x6.png",
                "caption": "Figure 6:Effect of loss functions.",
                "position": 937
            },
            {
                "img": "https://arxiv.org/html/2506.20279/x7.png",
                "caption": "Figure 7:Effect of inference steps.",
                "position": 941
            },
            {
                "img": "https://arxiv.org/html/2506.20279/x8.png",
                "caption": "Figure 8:Effect of the prompt branch.",
                "position": 949
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails of DenseWorld",
        "images": []
    },
    {
        "header": "Appendix BMore detailed and visual results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.20279/x9.png",
                "caption": "Figure 9:Adverse Env. Perception",
                "position": 4097
            },
            {
                "img": "https://arxiv.org/html/2506.20279/x10.png",
                "caption": "Figure 10:Adverse Env. Perception",
                "position": 4100
            },
            {
                "img": "https://arxiv.org/html/2506.20279/x11.png",
                "caption": "Figure 11:Adverse Env. Perception",
                "position": 4103
            },
            {
                "img": "https://arxiv.org/html/2506.20279/x12.png",
                "caption": "Figure 12:Adverse Env. Perception",
                "position": 4106
            },
            {
                "img": "https://arxiv.org/html/2506.20279/x13.png",
                "caption": "Figure 13:Adverse Env. Perception",
                "position": 4109
            }
        ]
    },
    {
        "header": "Appendix CAdditional Evaluation Results",
        "images": []
    },
    {
        "header": "Appendix DBroader Impacts",
        "images": []
    },
    {
        "header": "Appendix ELimitations",
        "images": []
    }
]