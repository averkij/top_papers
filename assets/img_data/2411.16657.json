[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16657/x1.png",
                "caption": "",
                "position": 85
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16657/x2.png",
                "caption": "Figure 2:Implementation details for region-based diffusion.We extend the vanilla self-attention mechanism tospatial-temporal-region-based 3D attention(see upperorangepart), which is capable of aligning different regions with their respective text descriptions via region-specific masks. The region-based character and motion LoRAs (see loweryellowandblueparts) are then injected interleavingly to the attention and FFN layers in each transformer block (see the right part). Note that although we resize the visual tokens into sequential 2D latent frames for better visualization, they are flattened and concatenated with all conditions when performing region-based attention.",
                "position": 120
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16657/x3.png",
                "caption": "Figure 3:Qualitative comparison ofDreamRunneron multi-scene story generation with multiple objects.DreamRunnergenerates videos with significantly better character consistency compared to other strong baseline methods, while other methods either fail to maintain consistency for the same object across scenes (e.g., VLogger), fail to generate objects that match the reference images (e.g., VideoDirectorGPT), or fail to generate multiple objects correctly (e.g., CogVideoX w/ Character LoRAs).",
                "position": 365
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAblations of RAG pipeline",
        "images": []
    },
    {
        "header": "Appendix BEvaluation Metrics",
        "images": []
    },
    {
        "header": "Appendix CCompositional T2V Examples",
        "images": []
    },
    {
        "header": "Appendix DCharacters.",
        "images": []
    },
    {
        "header": "Appendix ESingle-Character Examples",
        "images": []
    },
    {
        "header": "Appendix FMulti-Character Examples",
        "images": []
    },
    {
        "header": "Appendix GRegion-Based 3D Attention Masks",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16657/x4.png",
                "caption": "Figure 4:Qualitative results ofDreamRunnergenerated with prompts characterizingaction binding.",
                "position": 1751
            },
            {
                "img": "https://arxiv.org/html/2411.16657/x5.png",
                "caption": "Figure 5:Qualitative results ofDreamRunnergenerated with prompts characterizingconsistent attribute binding.",
                "position": 1756
            },
            {
                "img": "https://arxiv.org/html/2411.16657/x6.png",
                "caption": "Figure 6:Qualitative results ofDreamRunnergenerated with prompts characterizingdynamic attribute binding.",
                "position": 1761
            },
            {
                "img": "https://arxiv.org/html/2411.16657/x7.png",
                "caption": "Figure 7:Qualitative results ofDreamRunnergenerated with prompts characterizingmotion binding.",
                "position": 1765
            },
            {
                "img": "https://arxiv.org/html/2411.16657/x8.png",
                "caption": "Figure 8:Qualitative results ofDreamRunnergenerated with prompts characterizingobject interactions.",
                "position": 1770
            },
            {
                "img": "https://arxiv.org/html/2411.16657/x9.png",
                "caption": "Figure 9:Qualitative results ofDreamRunnergenerated with prompts characterizingspatial relationships.",
                "position": 1775
            },
            {
                "img": "https://arxiv.org/html/2411.16657/x10.png",
                "caption": "Figure 10:Qualitative results ofDreamRunnergenerated witha single character (mermaid).",
                "position": 1780
            },
            {
                "img": "https://arxiv.org/html/2411.16657/x11.png",
                "caption": "Figure 11:Qualitative results ofDreamRunnergenerated witha single character (astronaut).",
                "position": 1785
            },
            {
                "img": "https://arxiv.org/html/2411.16657/x12.png",
                "caption": "Figure 12:Qualitative results ofDreamRunnergenerated witha single character (mermaid).",
                "position": 1790
            },
            {
                "img": "https://arxiv.org/html/2411.16657/x13.png",
                "caption": "Figure 13:Qualitative results ofDreamRunnergenerated withmultiple characters (witch and cat 1).",
                "position": 1795
            },
            {
                "img": "https://arxiv.org/html/2411.16657/x14.png",
                "caption": "Figure 14:Qualitative results ofDreamRunnergenerated withmultiple characters (warrior and dog 2).",
                "position": 1800
            },
            {
                "img": "https://arxiv.org/html/2411.16657/x15.png",
                "caption": "Figure 15:Visualization of spatial-temporal region-based 3D attention masks. Different text colors represent different conditions, while the white region indicates the masked areas. For simplicity, we reduce each condition to two words, each frame to three segments, and display only three conditions and two frames in the figure. In practice, conditions can be longer and more numerous, frames can have more segments, and there are 12 latent frames in total.",
                "position": 1805
            }
        ]
    },
    {
        "header": "Appendix HLLM Prompts",
        "images": []
    }
]