[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.11630/x1.png",
                "caption": "Figure 1:Benchmark performance of Thyme.The comprehensive set of image manipulation capabilities enables Thyme to achieve significant improvements over the baseline in perception tasks. By leveraging its ability to convert complex mathematical reasoning into executable code, it consistently outperforms baselines in mathematical reasoning benchmarks. Furthermore, the observed gains across a wide range of general benchmarks further validate the effectiveness of our training approach.",
                "position": 134
            }
        ]
    },
    {
        "header": "Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.11630/x2.png",
                "caption": "Figure 2:Overall pipeline of the Thyme, illustrating the interaction between the model and the sandbox for iterative reasoning and code execution. Key processes such as reasoning, code generation, sandbox execution, and result feedback are highlighted.",
                "position": 213
            }
        ]
    },
    {
        "header": "Preliminary and Overall Pipeline",
        "images": []
    },
    {
        "header": "Thyme-SFT Cold Start",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.11630/x3.png",
                "caption": "Figure 3:SFT Data Construction Pipeline.First, samples are taken from an existing dataset and prompts are constructed based on the target functions (such as cropping, rotating, etc.). The model generates a thinking process and corresponding code based on the prompt. The code is then executed in a sandbox environment to filter out samples that fail to run properly. The remaining samples are reviewed by an additional MLLM to verify whether the code execution results align with the thinking process and effectively answer the question, eliminating invalid code samples. Finally, manual review is conducted to remove low-quality samples, ensuring the quality of the cold-start dataset.",
                "position": 368
            },
            {
                "img": "https://arxiv.org/html/2508.11630/x4.png",
                "caption": "Figure 4:Visualization of SFT Data instances. The left side presents a sample of data related to image processing operations, while the right side showcases a sample of data related to complex computations. During the training phase, the model autonomously generates code based on the analysis process, enabling the execution of desired image processing or computational tasks. This capability enhances the quality of perception and reasoning processes.",
                "position": 371
            }
        ]
    },
    {
        "header": "Thyme-RL",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.11630/x5.png",
                "caption": "Figure 5:Visualization of RL Data Instances. Thyme RL data focuses on complex scenarios and high-resolution image interpretation. Human annotators identify challenging objects within the images, design corresponding questions, and provide answers, along with the appropriate bounding boxes.",
                "position": 606
            },
            {
                "img": "https://arxiv.org/html/2508.11630/x6.png",
                "caption": "Figure 6:GRPO-ATS Sampling Pieline.The model sets the temperatureτ\\tauto 0 when generating code to reduce diversity and ensure accuracy; when generating the reasoning process, the temperature is set to 1 to encourage exploration. Sandbox outputs do not participate in training or advantage estimation.",
                "position": 723
            }
        ]
    },
    {
        "header": "Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.11630/x7.png",
                "caption": "Figure 7:Key Metrics During the RL Training Phase.The plots illustrate the dynamic changes in average response length, accuracy reward, and consistency reward over the training steps.",
                "position": 1602
            },
            {
                "img": "https://arxiv.org/html/2508.11630/arXiv/cases/case_images/179.png",
                "caption": "Figure 8:Cropping & Zooming Case 1.Thyme first evaluates the size and distance of the sign, determining that cropping and zooming in on the corresponding area would improve visibility. It then proceeds to write code to crop and enlarge the region containing the sign. Thyme accurately locates the sign’s position, successfully crops and zooms in on the area, and correctly answers the question.",
                "position": 1620
            },
            {
                "img": "https://arxiv.org/html/2508.11630/arXiv/cases/case_images/224.png",
                "caption": "Figure 9:Cropping & Zooming Case 2.Thyme accurately locates the phone number’s position, successfully crops and zooms in on the area, and correctly answers the question.",
                "position": 1677
            },
            {
                "img": "https://arxiv.org/html/2508.11630/arXiv/cases/case_images/10993.png",
                "caption": "Figure 10:Cropping & Zooming Case 3.Thyme first analyzes the size of the awning-tricycles and the image, concluding that enlarging the area most likely to contain awning-tricycles could help determine the answer to the question. It then crops and zooms in on the lower-left corner of the image for closer inspection, ultimately concluding that there are no awning-tricycles in the picture.",
                "position": 1732
            },
            {
                "img": "https://arxiv.org/html/2508.11630/arXiv/cases/case_images/rotate.png",
                "caption": "Figure 11:Rotation Case.Thyme realizes that the input image is not properly oriented, so it performs a rotation operation with Python code to adjust the angle of the input image. Then, it correctly identifies the expression in the image and represents the result with LaTeX code.",
                "position": 1791
            },
            {
                "img": "https://arxiv.org/html/2508.11630/arXiv/cases/case_images/150.png",
                "caption": "Figure 12:Contrast Enhancement Case.Thyme accurately locates the phone number’s position, successfully crops and zooms in on the area, and correctly answers the question.",
                "position": 1844
            },
            {
                "img": "https://arxiv.org/html/2508.11630/arXiv/cases/case_images/10454.jpg",
                "caption": "Figure 14:Failure Case 1.In this case, the image resolution is high, but Thyme does not perform the necessary crop to focus on the most relevant area and instead answers directly.",
                "position": 1995
            },
            {
                "img": "https://arxiv.org/html/2508.11630/arXiv/cases/case_images/1882.jpg",
                "caption": "Figure 15:Failure Case 2.The case requires only trivial calculations, yet Thyme opts to write code, adding unnecessary overhead. Furthermore, Thyme confuses variableaawith variablebb, mistaking the value ofbbfor the value ofaa, which leads to an incorrect result.",
                "position": 2022
            },
            {
                "img": "https://arxiv.org/html/2508.11630/arXiv/cases/case_images/243.png",
                "caption": "Figure 16:Failure Case 3.In this case, although Thyme eventually arrives at the correct answer, the region it crops during thinking process is irrelevant to the problem.",
                "position": 2056
            }
        ]
    },
    {
        "header": "Conclusion and Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAnnotation Requirements",
        "images": []
    },
    {
        "header": "Appendix BRelated Work",
        "images": []
    }
]