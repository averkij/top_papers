[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17434/x1.png",
                "caption": "Figure 1:Effectiveness of our LongVU over commonly-used uniform sampling and dense sampling. Uniform sampling overlooks critical frames due to its sparse nature. Dense sampling may surpass the maximum context length, leading to truncation of tokens from targeted frames. In contrast, our method can adaptively conduct spatiotemporal compression, accommodating long video sequences while preserving more visual details.",
                "position": 148
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17434/x2.png",
                "caption": "Figure 2:Architecture of LongVU. Given a densely sampled video frames, we first utilize DINOv2(Oquab et al.,2023)prior to remove redundant frames, and fuse the remaining frame features from both SigLIP(Zhai et al.,2023)and DINOv2(Oquab et al.,2023), described in Section3.1. Then we selectively reduce visual tokens via cross-modal query, detailed in Section3.2. Finally, as demonstrated in Section3.3, we conduct spatial token compression based on temporal dependencies to further meet the context length of LLMs.",
                "position": 198
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17434/x3.png",
                "caption": "Figure 3:Examples for various video understanding capabilities of LongVU model. We showcase that our LongVU is able to complete different types of video understanding tasks.",
                "position": 536
            },
            {
                "img": "https://arxiv.org/html/2410.17434/x4.png",
                "caption": "Figure 4:We randomly sample hundreds of videos to demonstrate the frames/tokens level reduction rate. (a) The number of frames before/after temporal reduction based on DINOv2 features (Section3.1). (b) The number of tokens before/after spatial token compression (Section3.3).",
                "position": 723
            },
            {
                "img": "https://arxiv.org/html/2410.17434/x5.png",
                "caption": "Figure 5:Needle-in-a-Haystack results. Our adaptive token compression scheme improves the score for locating the needle frame within an hour-long video from 0.80 to 0.88 on average.",
                "position": 726
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Training Datasets",
        "images": []
    },
    {
        "header": "7Frame-level Position Encoding",
        "images": []
    },
    {
        "header": "8DINOv2 v.s. SigLIP",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17434/x6.png",
                "caption": "Figure 6:Similarity comparison between SigLIP(Zhai et al.,2023)and DINOv2(Oquab et al.,2023)features. The similarity is calculated between the first frame and the remainings. DINO concentrating on vision centric task effectively capture subtle frame differences compared with SigLIP(Zhai et al.,2023)which is aligned on semantic space.",
                "position": 2002
            }
        ]
    },
    {
        "header": "9Needle-In-A-Video-Haystack",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17434/x7.png",
                "caption": "Figure 7:Needle-In-A-Video-Haystack results. Our spatiotemporal adaptive token compression scheme improves the score for locating the needle frame.",
                "position": 2012
            }
        ]
    },
    {
        "header": "10Limitation",
        "images": []
    }
]