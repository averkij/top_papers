[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01637/x1.png",
                "caption": "Figure 1:(Top)Perplexity (lower is better) on the OLMo(Groeneveld et¬†al.,2024)evaluation mixture. Inference-time FLOPS refer to the forward pass computation cost for four model sizes (0.7B, 1B, 1.3B, and 1.9B). With 10M f-grams, the 1.3B model matches the 1.9B baseline, while with 1B f-grams, the 1B model surpasses it.(Bottom)End-to-end token generation speed on a single A100 using vLLM(Kwon et¬†al.,2023). Storing f-gram embeddings in main memory introduces negligible latency, while NVMe storage slows generation slightly but does not create a bottleneck.",
                "position": 130
            },
            {
                "img": "https://arxiv.org/html/2502.01637/x2.png",
                "caption": "Figure 2:Illustration ofScone(with a maximumnùëõnitalic_n-gram length of3333). The termf-gramsrefers to the set of frequentnùëõnitalic_n-grams (Section3).",
                "position": 133
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3SconeArchitecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01637/x3.png",
                "caption": "Figure 3:Number of unique2222- to6666-grams that appear at least five times. We vary the size of the corpus by uniformly sampling sequences from the OLMo tokenized training corpus(Soldaini et¬†al.,2024).",
                "position": 318
            },
            {
                "img": "https://arxiv.org/html/2502.01637/x4.png",
                "caption": "Figure 4:Amortized per-token query latency (ms), averaged over 100,000 batches. The latency spike from batch size 1 to 2 when reading from system memory is due to batch operator overhead, which is less pronounced for solid-state drives.",
                "position": 444
            },
            {
                "img": "https://arxiv.org/html/2502.01637/x5.png",
                "caption": "Figure 5:Effect of the maximum f-gram lengthnùëõnitalic_ninVf‚Å¢-‚Å¢gramsubscriptùëâf-gramV_{\\mathrm{f\\text{-}gram}}italic_V start_POSTSUBSCRIPT roman_f - roman_gram end_POSTSUBSCRIPT, on perplexity and matched length. The leftyùë¶yitalic_y-axis shows perplexity (averaged over three seeds), where the leftmost star indicates baseline performance. The rightyùë¶yitalic_y-axis shows the average length of matched f-grams. The perplexity decreases as we increase the maximum length from 2 to 4, but then plateaus with some fluctuation. Similarly, the average matched length initially rises but stabilizes after size 4.",
                "position": 451
            }
        ]
    },
    {
        "header": "4Experimental Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01637/x6.png",
                "caption": "Figure 6:Evaluation perplexity as a function of|Vf‚Å¢-‚Å¢gram|subscriptùëâf-gram|V_{\\mathrm{f\\text{-}gram}}|| italic_V start_POSTSUBSCRIPT roman_f - roman_gram end_POSTSUBSCRIPT |. Model sizes in the legend correspond to the main model sizes, including the token embedding layer. The dashed lines and leftmost stars indicate baseline performance. Perplexity decreases overall with increasing sizes ofVf‚Å¢-‚Å¢gramsubscriptùëâf-gramV_{\\mathrm{f\\text{-}gram}}italic_V start_POSTSUBSCRIPT roman_f - roman_gram end_POSTSUBSCRIPT.",
                "position": 462
            },
            {
                "img": "https://arxiv.org/html/2502.01637/x7.png",
                "caption": "Figure 7:Evaluation perplexity on Wikitext-103 as a function of the size ofùíúf‚Å¢-‚Å¢gramsubscriptùíúf-gram\\mathcal{A}_{\\mathrm{f\\text{-}gram}}caligraphic_A start_POSTSUBSCRIPT roman_f - roman_gram end_POSTSUBSCRIPT. Model sizes in the legend correspond to the main model sizes, including the token embedding layer. Dashed lines and stars on the left represent baseline performance. The perplexity improves as the size ofùíúf‚Å¢-‚Å¢gramsubscriptùíúf-gram\\mathcal{A}_{\\mathrm{f\\text{-}gram}}caligraphic_A start_POSTSUBSCRIPT roman_f - roman_gram end_POSTSUBSCRIPTgrows.",
                "position": 499
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Related Work",
        "images": []
    },
    {
        "header": "Appendix BAdditional Algorithms",
        "images": []
    },
    {
        "header": "Appendix CChallenges of Scaling Vocabulary Size in Embedding Layers",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01637/x8.png",
                "caption": "Figure 8:BPC of three model sizes on the validation set (lower is better). For all three model sizes, BPC initially improves as vocabulary size increases but eventually deteriorates.",
                "position": 1728
            },
            {
                "img": "https://arxiv.org/html/2502.01637/x9.png",
                "caption": "Figure 9:Percentages of tokens (y-axis) that receive more than a given number of updates (x-axis), measured over 100 million training tokens. As the vocabulary size increases, tokens receive increasingly sparse updates.",
                "position": 1734
            },
            {
                "img": "https://arxiv.org/html/2502.01637/x10.png",
                "caption": "Figure 10:Number of embedding layer parameters on the GPU and corresponding GPU memory usage. Computational costs increase linearly with vocabulary size.",
                "position": 1740
            }
        ]
    },
    {
        "header": "Appendix DAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01637/x11.png",
                "caption": "Figure 11:Effect of the maximum f-gram length inVf‚Å¢-‚Å¢gramsubscriptùëâf-gramV_{\\mathrm{f\\text{-}gram}}italic_V start_POSTSUBSCRIPT roman_f - roman_gram end_POSTSUBSCRIPT, evaluated on the WebText validation split.",
                "position": 1753
            },
            {
                "img": "https://arxiv.org/html/2502.01637/x11.png",
                "caption": "Figure 11:Effect of the maximum f-gram length inVf‚Å¢-‚Å¢gramsubscriptùëâf-gramV_{\\mathrm{f\\text{-}gram}}italic_V start_POSTSUBSCRIPT roman_f - roman_gram end_POSTSUBSCRIPT, evaluated on the WebText validation split.",
                "position": 1755
            },
            {
                "img": "https://arxiv.org/html/2502.01637/x12.png",
                "caption": "Figure 12:Evaluation perplexity on WebText as a function of the size ofùíúf‚Å¢-‚Å¢gramsubscriptùíúf-gram\\mathcal{A}_{\\mathrm{f\\text{-}gram}}caligraphic_A start_POSTSUBSCRIPT roman_f - roman_gram end_POSTSUBSCRIPT.",
                "position": 1759
            },
            {
                "img": "https://arxiv.org/html/2502.01637/x13.png",
                "caption": "Figure 13:Average perplexity on the OLMo evaluation mixture throughout training. Models withSconeenabled converge later, indicating stronger capacity, and achieve better perplexity.",
                "position": 1781
            }
        ]
    },
    {
        "header": "Appendix EImplementation Details",
        "images": []
    }
]