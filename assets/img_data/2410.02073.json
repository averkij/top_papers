[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/dis5k/4892828841_7f1bc05682_o.jpg",
                "caption": "Figure 1:Results on images from the AM-2k(Li et¬†al.,2022a)(1st & 3rd column) and DIS-5k(Qin et¬†al.,2022)(2nd column) datasets. Input image on top, estimated depth maps from Depth Pro, Marigold(Ke et¬†al.,2024), Depth Anything v2(Yang et¬†al.,2024b), and Metric3D v2(Hu et¬†al.,2024)below. Depth Pro produces zero-shot metric depth maps with absolute scale at 2.25-megapixel native resolution in 0.3 seconds on a V100 GPU.",
                "position": 88
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/am2k/m_75b2ab26.jpg",
                "caption": "",
                "position": 111
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/dis5k/4892828841_7f1bc05682_o_depthpro_pv5i9yzjef.jpg",
                "caption": "",
                "position": 132
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/am2k/m_75b2ab26_depthpro_pv5i9yzjef.jpg",
                "caption": "",
                "position": 138
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/dis5k/4892828841_7f1bc05682_o_marigold.jpg",
                "caption": "",
                "position": 157
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/am2k/m_75b2ab26_marigold.jpg",
                "caption": "",
                "position": 163
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/dis5k/4892828841_7f1bc05682_o_depth_anything_v2_relative.jpg",
                "caption": "",
                "position": 183
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/am2k/m_75b2ab26_depth_anything_v2_relative.jpg",
                "caption": "",
                "position": 189
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/dis5k/4892828841_7f1bc05682_o_metric3d_v2g.jpg",
                "caption": "",
                "position": 209
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/am2k/m_75b2ab26_metric3d_v2g.jpg",
                "caption": "",
                "position": 215
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x1.png",
                "caption": "Figure 2:Boundary recall versus runtime. Depth Pro outperforms prior work by a multiplicative factor in boundary accuracy while being orders of magnitude faster than works focusing on fine-grained predictions (e.g.,¬†Marigold, PatchFusion).",
                "position": 239
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x2.png",
                "caption": "",
                "position": 244
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02073/x3.jpeg",
                "caption": "Figure 3:Overview of the network architecture. An image is downsampled at several scales. At each scale, it is split into patches, which are processed by a ViT-based patch encoder, with weights shared across scales. Patches are merged into feature maps, upsampled, and fused via a DPT decoder. Predictions are anchored by a separate image encoder that provides global context.",
                "position": 325
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02073/x4.png",
                "caption": "Table 2:Zero-shot boundary accuracy.We report the F1 score for dataset with ground-truth depth, and boundary recall (RùëÖRitalic_R) for matting and segmentation datasets. Qualitative results are shown on a sample from the AM-2k dataset(Li et¬†al.,2022a). Higher is better for all metrics.",
                "position": 633
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x5.png",
                "caption": "",
                "position": 775
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x6.png",
                "caption": "",
                "position": 776
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x7.png",
                "caption": "",
                "position": 777
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x8.png",
                "caption": "",
                "position": 778
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x9.png",
                "caption": "",
                "position": 779
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/tmpi/024/024_depthpro_frame0038.jpg",
                "caption": "Figure 4:Impact on novel view synthesis.We plug depth maps produced by Depth Pro, Marigold(Ke et¬†al.,2024), Depth Anything v2(Yang et¬†al.,2024b), and Metric3D v2(Hu et¬†al.,2024)into a recent publicly available novel view synthesis system(Khan et¬†al.,2023). We demonstrate results on images from AM-2k(Li et¬†al.,2022a)(1st & 3rd column) and DIS-5k(Qin et¬†al.,2022)(2nd column). Depth Pro produces sharper and more accurate depth maps, yielding cleaner synthesized views. Depth Anything v2 and Metric3D v2 suffer from misalignment between the input images and estimated depth maps, resulting in foreground pixels bleeding into the background. Marigold is considerably slower than Depth Pro and produces less accurate boundaries, yielding artifacts in synthesized images. Zoom in for detail.",
                "position": 815
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/tmpi/m_0c7da85d/m_0c7da85d.jpg",
                "caption": "",
                "position": 838
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/tmpi/m_0c7da85d/m_0c7da85d_depthpro_frame0000.jpg",
                "caption": "",
                "position": 865
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/tmpi/024/024_depth_anything_v2_relative_frame0038.jpg",
                "caption": "",
                "position": 885
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/tmpi/m_0c7da85d/m_0c7da85d_depth_anything_v2_relative_frame0000.jpg",
                "caption": "",
                "position": 891
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/tmpi/024/024_marigold_frame0038.jpg",
                "caption": "",
                "position": 911
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/tmpi/m_0c7da85d/m_0c7da85d_marigold_frame0000.jpg",
                "caption": "",
                "position": 917
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/tmpi/024/024_metric3d_frame0038.jpg",
                "caption": "",
                "position": 938
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/tmpi/m_0c7da85d/m_0c7da85d_metric3d_frame0000.jpg",
                "caption": "",
                "position": 944
            }
        ]
    },
    {
        "header": "5Conclusion & limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Supplemental material",
        "images": []
    },
    {
        "header": "Appendix AAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02073/x10.png",
                "caption": "Figure 8:Evaluation metrics for sharp boundaries.We propose novel metrics to evaluate the sharpness of occlusion boundaries. The metrics can be computed on ground-truth depth maps (first two rows), and binary maps that can be derived from matting or segmentation datasets (subsequent rows). Each row shows a sample image, the ground truth for deriving occlusion boundaries, our prediction, ground-truth occluding contours, and occluding contours from the prediction. For these visualizations we sett=15ùë°15t=15italic_t = 15.",
                "position": 4388
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x11.png",
                "caption": "",
                "position": 4406
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x12.png",
                "caption": "",
                "position": 4407
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x13.png",
                "caption": "",
                "position": 4408
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x14.png",
                "caption": "",
                "position": 4409
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x15.png",
                "caption": "",
                "position": 4417
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x16.png",
                "caption": "",
                "position": 4418
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x17.png",
                "caption": "",
                "position": 4419
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x18.png",
                "caption": "",
                "position": 4420
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x19.png",
                "caption": "",
                "position": 4421
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x20.png",
                "caption": "",
                "position": 4429
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x21.png",
                "caption": "",
                "position": 4430
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x22.png",
                "caption": "",
                "position": 4431
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x23.png",
                "caption": "",
                "position": 4432
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x24.png",
                "caption": "",
                "position": 4433
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x25.png",
                "caption": "",
                "position": 4441
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x26.png",
                "caption": "",
                "position": 4442
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x27.png",
                "caption": "",
                "position": 4443
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x28.png",
                "caption": "",
                "position": 4444
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x29.png",
                "caption": "",
                "position": 4445
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x30.png",
                "caption": "",
                "position": 4453
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x31.png",
                "caption": "",
                "position": 4454
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x32.png",
                "caption": "",
                "position": 4455
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x33.png",
                "caption": "",
                "position": 4456
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x34.png",
                "caption": "",
                "position": 4457
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x35.png",
                "caption": "Figure 9:Boundary evaluation metrics empirical study.We demonstrate how various types of image perturbations impact our proposed edge metrics. We report quantitative and qualitative results for multiple ground-truth perturbations, such as simple image shifts, downsampling followed by upsamplings, and Gaussian blurring. We report both ground-truth and perturbed occluding contours, used to derive ourFùêπFitalic_F1 scores. Our results empirically demonstrate the correlation between erroneous depth edge predictions and low precision and recall values.",
                "position": 4463
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x36.png",
                "caption": "",
                "position": 4481
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x37.png",
                "caption": "",
                "position": 4482
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x38.png",
                "caption": "",
                "position": 4483
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x39.png",
                "caption": "",
                "position": 4484
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x40.png",
                "caption": "",
                "position": 4490
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x41.png",
                "caption": "",
                "position": 4491
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x42.png",
                "caption": "",
                "position": 4492
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x43.png",
                "caption": "",
                "position": 4493
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x44.png",
                "caption": "",
                "position": 4502
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x45.png",
                "caption": "",
                "position": 4503
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x46.png",
                "caption": "",
                "position": 4504
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x47.png",
                "caption": "",
                "position": 4505
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x48.png",
                "caption": "",
                "position": 4514
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x49.png",
                "caption": "",
                "position": 4515
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x50.png",
                "caption": "",
                "position": 4516
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x51.png",
                "caption": "",
                "position": 4517
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x52.png",
                "caption": "",
                "position": 4526
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x53.png",
                "caption": "",
                "position": 4527
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x54.png",
                "caption": "",
                "position": 4528
            },
            {
                "img": "https://arxiv.org/html/2410.02073/x55.png",
                "caption": "",
                "position": 4529
            }
        ]
    },
    {
        "header": "Appendix BControlled Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02073/x56.png",
                "caption": "Figure 10:Evaluation of focal length estimation.Each plot compares a number of methods on a given dataset. Thexùë•xitalic_xaxis represents the AbsRel error and theyùë¶yitalic_yaxis represents the percentage of samples whose error is below that magnitude.",
                "position": 5354
            }
        ]
    },
    {
        "header": "Appendix CImplementation, Training and Evaluation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/controlnet/_DqbrkesR2M/_DqbrkesR2M_reference.jpg",
                "caption": "Figure 11:Comparison on conditional image synthesis.We use ControlNet(Zhang et¬†al.,2023a)to synthesize a stylized image given a prompt (top row, right) and a depth map. The depth map is predicted from the input image(Li et¬†al.,2022a)(top row, left) via Depth Pro, and baselines. The left column shows depth maps, the right column the synthesized image. For the baselines, missing cables (Depth Anything v2 & Matric3D v2) or a spurious gradient in the sky (Marigold) alter the scene structure of the synthesized image.",
                "position": 6171
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/controlnet/_DqbrkesR2M/_DqbrkesR2M_depth_depthpro.jpg",
                "caption": "",
                "position": 6195
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/controlnet/_DqbrkesR2M/_DqbrkesR2M_depthpro_72_out.jpg",
                "caption": "",
                "position": 6196
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/controlnet/_DqbrkesR2M/_DqbrkesR2M.jpg_depth_anything_out.jpg",
                "caption": "",
                "position": 6203
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/controlnet/_DqbrkesR2M/_DqbrkesR2M.jpg_depth_anything_out_123.jpg",
                "caption": "",
                "position": 6204
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/controlnet/_DqbrkesR2M/_DqbrkesR2M.jpg_marigold_out.jpg",
                "caption": "",
                "position": 6211
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/controlnet/_DqbrkesR2M/_DqbrkesR2M.jpg_marigold_out_123.jpg",
                "caption": "",
                "position": 6212
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/controlnet/_DqbrkesR2M/_DqbrkesR2M.jpg_metric3d_out.jpg",
                "caption": "",
                "position": 6219
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/controlnet/_DqbrkesR2M/_DqbrkesR2M.jpg_metric3d_out_123.jpg",
                "caption": "",
                "position": 6220
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/out-of-focus/366_out/366_reference.jpg",
                "caption": "Depth Pro (Ours)",
                "position": 6230
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/out-of-focus/366_out/366_reference.jpg",
                "caption": "Depth Pro (Ours)",
                "position": 6244
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/out-of-focus/366_out/366_disparity_depthpro.jpg",
                "caption": "",
                "position": 6251
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/out-of-focus/366_out/366_disparity_marigold.jpg",
                "caption": "",
                "position": 6254
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/out-of-focus/366_out/366_disparity_da.jpg",
                "caption": "",
                "position": 6257
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/out-of-focus/366_out/366_depthpro.jpg",
                "caption": "Depth Pro (Ours)",
                "position": 6263
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/out-of-focus/366_out/366_marigold.jpg",
                "caption": "Marigold",
                "position": 6268
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/out-of-focus/366_out/366_da.jpg",
                "caption": "Depth Anything v2",
                "position": 6274
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/out-of-focus/415_out/415_reference.jpg",
                "caption": "Depth Pro (Ours)",
                "position": 6283
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/out-of-focus/415_out/415_disparity_depthpro.jpg",
                "caption": "",
                "position": 6290
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/out-of-focus/415_out/415_disparity_marigold.jpg",
                "caption": "",
                "position": 6293
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/out-of-focus/415_out/415_disparity_da.jpg",
                "caption": "",
                "position": 6296
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/out-of-focus/415_out/415_depthpro.jpg",
                "caption": "Depth Pro (Ours)",
                "position": 6302
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/out-of-focus/415_out/415_marigold.jpg",
                "caption": "Marigold",
                "position": 6307
            },
            {
                "img": "https://arxiv.org/html/2410.02073/extracted/5897170/fig/out-of-focus/415_out/415_da.jpg",
                "caption": "Depth Anything v2",
                "position": 6313
            }
        ]
    },
    {
        "header": "Appendix DApplications",
        "images": []
    }
]