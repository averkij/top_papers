[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.17745/x1.png",
                "caption": "Figure 1:Image-to-3D Generation Results ofUltra3D.Ultra3Ddelivers high-quality 3D meshes with fine-grained geometric details while maintaining efficient generation. Please zoom in to view detailed geometry.",
                "position": 87
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.17745/x2.png",
                "caption": "Figure 2:Expeiments on different attention mechanisms.Each color denotes an attention group, within which attention is computed independently. All other settings remain unchanged, with only the attention mechanism being replaced. 3D Window Attention partitions the object space into 8 fixed regions by splitting at the center along each axis. This fixed partitioning often misaligns with semantic boundaries, leading to degraded quality and style inconsistencies.",
                "position": 113
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.17745/x3.png",
                "caption": "Figure 3:Pipeline Overview.We introduceUltra3D, an efficient and high-quality 3D generation framework that first generates sparse voxel layout via VecSet and then refines it by generating per-voxel latent. The core ofUltra3Dis Part Attention, an efficient localized attention mechanism that performs attention computation independently within each part group. Besides, when the input condition is an image, each part group performs cross attention only with the image tokens onto which its voxel tokens are projected.",
                "position": 193
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.17745/x4.png",
                "caption": "Figure 4:Impact of Resolution on Generation Quality.We compare results under different configurations, where “512_64” denotes a mesh resolution of 512 and a sparse voxel resolution of 64. In previous works, to reduce computational cost in the second stage, the sparse voxels are typically downsampled by half before attention computation in the DiT, then upsampled afterward—annotated as “Downsample” in the figure. As shown, both the mesh resolution and the sparse voxel resolution used during attention computation significantly impact the final quality. However, due to efficiency constraints, prior methods were limited to lower resolutions. In contrast, our efficient framework supports higher sparse voxel resolutions, making high-quality generation feasible.",
                "position": 252
            },
            {
                "img": "https://arxiv.org/html/2507.17745/x5.png",
                "caption": "Figure 5:Robustness of Part Annotation. Although our method is trained using data with exactly 8 part groups, we find it to be robust to variations in part annotation. Varying the number of part groups has little impact on generation quality, suggesting that increasing the number of annotated part groups can further accelerate computation without compromising performance.",
                "position": 297
            },
            {
                "img": "https://arxiv.org/html/2507.17745/x6.png",
                "caption": "Figure 6:Percentile for Filtering Metrics of Part Annotation.We apply two metrics to filter poorly segmented samples: (1) the sum of squared voxel ratios, which identifies imbalanced part distributions, and (2) neighborhood inconsistency, which measures the proportion of voxels whose neighbors have different part labels. Lower values on both metrics indicate better segmentation quality. As shown in the plot, most samples exhibit stable and low values across both metrics, indicating high annotation quality.",
                "position": 317
            },
            {
                "img": "https://arxiv.org/html/2507.17745/x7.png",
                "caption": "Figure 7:Comparison with Prior Methods.Our method produces higher fidelity and richer surface details. As highlighted in the red boxes, our results align more closely with the input image compared to other methods.",
                "position": 342
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]