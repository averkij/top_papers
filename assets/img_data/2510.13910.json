[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Problem Formulation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13910/x1.png",
                "caption": "Figure 1:Left: Proportion of samples in each MCQ type. Right: Average number of options in each MCQ type.",
                "position": 202
            }
        ]
    },
    {
        "header": "3RAGCap-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13910/x2.png",
                "caption": "Figure 2:General agentic RAG pipeline and data construction processes for RAGCap-Bench. We take queries from open-source question-answering (QA) datasets. We then run different deep research systems and collect the intermediate outputs. Two main strategies, Vanilla Generation and Error-Guided Generation are deployed to generate the MCQs based on queries and the intermediate outputs. The generated MCQs are filtered to ensure the quality and experts are recruited to provide the answers.",
                "position": 220
            }
        ]
    },
    {
        "header": "4Evaluations",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13910/x3.png",
                "caption": "Figure 3:RAGCap-Bench overall EM scores for different fast-thinking (left) and slow-thinking models (right), with informative (orange) and bare (blue) prompts.",
                "position": 704
            },
            {
                "img": "https://arxiv.org/html/2510.13910/x4.png",
                "caption": "Figure 4:Correlation of performance on RAGCap-Bench with performance on InfoDeepSeek and BrowseComp-Zh, for Qwen3-8B, Qwen3-32B and Qwen3-235B.",
                "position": 717
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADatasets",
        "images": []
    },
    {
        "header": "Appendix BConvergent and Divergent Capabilities",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13910/x5.png",
                "caption": "Figure 5:Convergent (top) and divergent (bottom) planning capabilities required for different user queries.",
                "position": 1530
            }
        ]
    },
    {
        "header": "Appendix CExamples of Errors",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13910/x6.png",
                "caption": "Figure 6:Error examples from agentic RAG pipelines.",
                "position": 1540
            }
        ]
    },
    {
        "header": "Appendix DGeneration of MCQs",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13910/x7.png",
                "caption": "Figure 7:Example prompt for generating planning questions using Error-Guided Generation.",
                "position": 1554
            },
            {
                "img": "https://arxiv.org/html/2510.13910/x8.png",
                "caption": "Figure 8:Example prompt for generating grounded reasoning questions using Error-Guided Generation.",
                "position": 1557
            },
            {
                "img": "https://arxiv.org/html/2510.13910/x9.png",
                "caption": "Figure 9:Example prompt for generating grounded reasoning questions using Vanilla Generation.",
                "position": 1560
            },
            {
                "img": "https://arxiv.org/html/2510.13910/x10.png",
                "caption": "Figure 10:Vanilla Generation for evidence extraction.",
                "position": 1563
            },
            {
                "img": "https://arxiv.org/html/2510.13910/x11.png",
                "caption": "Table 6:Performance of different fast- and slow-thinking LLMs using bare prompts. EMcand F1cdenote the scores for converging ability. EMddenotes the EM score for diverging ability. EMadenotes the EM score for noise-abstain. EMrand F1rdenote the scores for noise-reliability. The scores are presented as percentages for clarity.",
                "position": 1566
            },
            {
                "img": "https://arxiv.org/html/2510.13910/x11.png",
                "caption": "Figure 11:An example from RAGCap-Bench.",
                "position": 1800
            },
            {
                "img": "https://arxiv.org/html/2510.13910/x12.png",
                "caption": "Figure 12:An example from RAGCap-Bench.",
                "position": 1811
            }
        ]
    },
    {
        "header": "Appendix EExamples of MCQs",
        "images": []
    },
    {
        "header": "Appendix FResults with Bare Prompt",
        "images": []
    },
    {
        "header": "Appendix GG. Evaluation Prompts",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13910/x13.png",
                "caption": "Figure 13:Prompt for evaluation on the intermediate outputs from WebThinker.",
                "position": 1835
            },
            {
                "img": "https://arxiv.org/html/2510.13910/x14.png",
                "caption": "Figure 14:Bare prompt for grounded reasoning.",
                "position": 1845
            },
            {
                "img": "https://arxiv.org/html/2510.13910/x15.png",
                "caption": "Figure 15:Bare prompt for noise robustness (reliability).",
                "position": 1848
            },
            {
                "img": "https://arxiv.org/html/2510.13910/x16.png",
                "caption": "Figure 16:Informative prompt for grounded reasoning.",
                "position": 1851
            },
            {
                "img": "https://arxiv.org/html/2510.13910/x17.png",
                "caption": "Figure 17:Informative prompt for noise robustness (reliability).",
                "position": 1854
            }
        ]
    },
    {
        "header": "Appendix HBare and Informative Prompts",
        "images": []
    }
]