[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.06172/x1.png",
                "caption": "Figure 1:Illustration of multimodal situational safety. The model must judge the safety of the user’s query or instruction based on the visual context and adjust their answer accordingly. Given an unsafe visual context, the model should remind the user of the potential risk instead of directly answering the user’s query. However, current MLLMs struggle to achieve this in most unsafe situations.",
                "position": 140
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Multimodal Situational Safety",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.06172/x2.png",
                "caption": "Figure 2:Presentation of MSSBench across four domains and ten secondary categories in Chat and Embodied tasks.",
                "position": 241
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x2.png",
                "caption": "Figure 2:Presentation of MSSBench across four domains and ten secondary categories in Chat and Embodied tasks.",
                "position": 244
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x3.png",
                "caption": "Figure 3:The overall structure of the chat data collection pipeline (left) and examples of two multimodal assistant scenarios (right). The pipeline includes four parts: (1) Generating Intented Activity and Unsafe Textual Situations. (2) Iterative Filtering with LLM. (3) Constructing a Multimodal Situational Safety Dataset via Image Retrieval. (4) Human Verification & Query Generation.",
                "position": 388
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x4.png",
                "caption": "",
                "position": 397
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.06172/x5.png",
                "caption": "(a)Individual performance comparison.",
                "position": 593
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x5.png",
                "caption": "(a)Individual performance comparison.",
                "position": 596
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x6.png",
                "caption": "",
                "position": 601
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x7.png",
                "caption": "(c)Settings illustration.",
                "position": 607
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x8.png",
                "caption": "Figure 5:MLLMs’ different errors when judging the safety of answering a user’s query. The full prompt informing the MLLMs of the current situation is not shown due to the space limit.",
                "position": 625
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x9.png",
                "caption": "Figure 6:Workflow of our Multi-Agent framework for enhancing situational safety in user queries, incorporating Intent Reasoning, Safety Judgment, QA and Visual Understanding agents.",
                "position": 650
            }
        ]
    },
    {
        "header": "5Multi-Agent System for Better Safety Reasoning",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.06172/x10.png",
                "caption": "Figure 7:MLLM’s performance on our benchmark with three reasoning settings. Base setting: without explicit safety reasoning. 1 step CoT: MLLMs reasoning the safety of user query and generating response at one step. Multi-agent: our designed multi-agent pipeline. The results show that the multi-agent pipeline improves performance in most cases.",
                "position": 680
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x11.png",
                "caption": "",
                "position": 689
            }
        ]
    },
    {
        "header": "6Conclusion and Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.06172/x12.png",
                "caption": "Figure 8:Binary Safety Classification of Open-Source MLLMs Based on User Intent in Safe and Unsafe Chat Scenarios (as in Chat Task, Setting II, Table6).",
                "position": 1270
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x13.png",
                "caption": "Figure 9:Binary Safety Classification of Close-Source MLLMs Based on User Intent in Safe and Unsafe Chat Scenarios (as in Chat Task, Setting II, Table6).",
                "position": 1279
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x14.png",
                "caption": "Figure 10:Binary Safety Classification of Open and Closed-Source MLLMs Based on User Intent in Safe and Unsafe Situations for Embodied Tasks.(as in Embodied Task, Setting II, Table6)",
                "position": 1298
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x15.png",
                "caption": "",
                "position": 1302
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x16.png",
                "caption": "A. DeepSeek",
                "position": 1551
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x16.png",
                "caption": "A. DeepSeek",
                "position": 1554
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x17.png",
                "caption": "B. Qwen-VL",
                "position": 1559
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x18.png",
                "caption": "C. Mplug-Owl2",
                "position": 1565
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x19.png",
                "caption": "D. MiniGPT-V2",
                "position": 1570
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x20.png",
                "caption": "E. Claude",
                "position": 1576
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x21.png",
                "caption": "F. Gemini-1.5",
                "position": 1581
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x22.png",
                "caption": "(a)Chat task safe situations",
                "position": 1879
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x22.png",
                "caption": "(a)Chat task safe situations",
                "position": 1882
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x23.png",
                "caption": "(b)Chat task unsafe situations",
                "position": 1887
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x24.png",
                "caption": "(c)Chat task average",
                "position": 1892
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x25.png",
                "caption": "(d)Embodied task safe situations",
                "position": 1898
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x26.png",
                "caption": "(e)Embodied task unsafe situations",
                "position": 1903
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x27.png",
                "caption": "(f)Embodied task average",
                "position": 1908
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x28.png",
                "caption": "Figure 13:Workflow of our Multi-Agent Framework for enhancing situational safety in user instructions, incorporating the Key Object Locator Agent and Safety Reasoning Agent.",
                "position": 2221
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x29.png",
                "caption": "Figure 14:An Example of Unsafe Scenario for Instruct Following in Chat Tasks.",
                "position": 2233
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x30.png",
                "caption": "Figure 15:An Example of Safe Scenario for Binary Safety Classification in Chat Tasks.",
                "position": 2240
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x31.png",
                "caption": "Figure 16:An Example of a Multimodal Unsafe Scenario in a Embodied Task.",
                "position": 2247
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x32.png",
                "caption": "Figure 17:An Example of a Multimodal Safe Scenario in a Embodied Task.",
                "position": 2250
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x33.png",
                "caption": "Figure 18:Examples of Physical Harm: (a) and (b) are Other-harm, while (c) and (d) are Self-harm.",
                "position": 2351
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x34.png",
                "caption": "",
                "position": 2355
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x35.png",
                "caption": "Figure 19:Examples of Property Damage: (a) and (b) are classified as public damage, while (c) and (d) are classified as personal damage.",
                "position": 2359
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x36.png",
                "caption": "",
                "position": 2363
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x37.png",
                "caption": "Figure 20:Examples of Offensive Behavior: (a) and (b) are classified as Disruptive behaviors, (c) and (d) as Religious belief infringements, and (e) and (f) as Cultural belief violations.",
                "position": 2367
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x38.png",
                "caption": "",
                "position": 2371
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x39.png",
                "caption": "",
                "position": 2373
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x40.png",
                "caption": "Figure 21:Examples of Illegal Activities: (a) and (b) are classified as Property-restricting activities, (b) and (c) as Organism-restricting activities, and (d) and (e) as Human-restricting activities",
                "position": 2377
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x41.png",
                "caption": "",
                "position": 2381
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x42.png",
                "caption": "",
                "position": 2383
            },
            {
                "img": "https://arxiv.org/html/2410.06172/x43.png",
                "caption": "Figure 22:Examples of Embodied Task: the left image shows Self-harm, and the right image shows Personal property damage.",
                "position": 2387
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]