[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.16577/x1.png",
                "caption": "Figure 1:We introduce a retrieval-augmented diffusion framework for text-to-multiview generation. Given a text prompt, our method retrieves real-world images and adaptively leverages them together with the text, enabling faithful generation of out-of-distribution and newly emerging objects.",
                "position": 85
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.16577/x2.png",
                "caption": "Figure 2:Overview of our pipeline.Given a text prompt, we retrievekkrelevant images from an in-the-wild 2D image corpus. Local features are extracted from each image, projected through a Resampler and integrated into retrieval-attention modules to guide the multi-view generation process.",
                "position": 142
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.16577/x3.png",
                "caption": "Figure 3:Overview of our training scheme.Our model adopts a hybrid training strategy that alternates between two modes.3D mode (left):A 3D object is rendered to produce ground-truth multi-view images. Additional views are generated and subjected to heavy augmentations to serve as retrievals. These augmented views, along with the target camera parameters and the associated prompt, are provided as input to the model.2D mode (right):We retrieveK+1K+1images from a 2D training corpus, whereKKimages are used as retrievals and one held-out image serves as the target view. In this mode, the model performs 2D self-attention rather than 3D attention, and no target camera parameters are provided.",
                "position": 159
            },
            {
                "img": "https://arxiv.org/html/2508.16577/x4.png",
                "caption": "Figure 4:Illustration of Prior-Guided Attention.The base model’s activations are leveraged proportionally to its prior knowledge of the object, controlled by the adaptive parameterα\\alpha. For an object where the base model has limited prior knowledge, a lower adaptiveα\\alphareduces reliance on the base model. For an object well-represented by the base model, insufficient weighting (e.g., fixedα=0.3\\alpha=0.3) degrades results.",
                "position": 232
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.16577/x5.png",
                "caption": "Figure 5:Hybrid training ablations.For the text prompts\"Ratonero Bodeguero Andaluz dog\"(top) and\"Markhor goat\"(bottom), we consider the output of our model when trained without our 2D/3D schemes in comparison to our full approach (shown on the RHS).",
                "position": 471
            },
            {
                "img": "https://arxiv.org/html/2508.16577/x6.png",
                "caption": "Figure 6:Effect of the number of retrieved images on alignment and fidelity.",
                "position": 654
            },
            {
                "img": "https://arxiv.org/html/2508.16577/x7.png",
                "caption": "Figure 7:Qualitative evaluation.Text-to-3D models fail to generate unseen (OOD) concepts, while image-to-3D models fail to reconstruct their correct 3D structure from a single view. Existing personalization methods cannot effectively leverage the diversity of retrieved images.",
                "position": 657
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    },
    {
        "header": "Appendix BAdditional Qualitative Evaluation",
        "images": []
    },
    {
        "header": "Appendix CTraining, Inference and Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.16577/x8.png",
                "caption": "Figure 8:Limitations of CLIP text-image similarity for evaluating OOD objects.Each row shows an example from our OOD-Eval benchmark: the ground truth (GT) image, the output from MVDream, and the output from our model. Below each image is its CLIP similarity score. MVDream receives a higher score than both our model and the GT image, despite producing less faithful generations.",
                "position": 1604
            },
            {
                "img": "https://arxiv.org/html/2508.16577/x9.png",
                "caption": "Figure 9:(a).Utility.Our approach learns to utilize all relevant information in retrieved views. On the LHS, we show retrieved views. The middle columns are zoom ins, for aspects used in generation, and the RHS shows back view (top), side view (middle), and front view (bottom). (b).Diversity.Unlike image-prompted methods, our method enables diverse outputs, by using different seeds for a given input text and a fixed set of retrieved images. For the prompt \"Peugeot 202\", the LHS shows retrieved views, and the RHS shows a single view output (using the same pose) for 4 different seeds.",
                "position": 1612
            },
            {
                "img": "https://arxiv.org/html/2508.16577/x10.png",
                "caption": "",
                "position": 1617
            },
            {
                "img": "https://arxiv.org/html/2508.16577/x11.png",
                "caption": "Figure 10:Evaluation Benchmarks Overview.OOD-Eval:Our out-of-distribution benchmark includes 2D images of both rare and well-known objects, featuring a diverse set of categories such as animals, vehicles, insects, foods, and everyday items.IND-Eval:The in-domain benchmark focuses on common, everyday objects that are representative of standard training distributions.",
                "position": 1637
            }
        ]
    },
    {
        "header": "Appendix DUser Study",
        "images": []
    },
    {
        "header": "Appendix EEvaluation Dataset Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.16577/x12.png",
                "caption": "Figure 11:Limitations.(a) Visually biased retrieved images (e.g., repetitive white flowers) introduce artifacts in the generated multiviews.\n(b) The model struggles to reproduce fine-grained textures, such as the hoverfly’s dorsal pattern.\n(c) When the base model (MVDream) is assigned a high attention weight (α\\alpha), 3D structural inaccuracies from the base model (e.g., a floating tail) are inherited.",
                "position": 1765
            }
        ]
    },
    {
        "header": "Appendix FLimitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.16577/x13.png",
                "caption": "Figure 12:Additional Results.",
                "position": 1794
            },
            {
                "img": "https://arxiv.org/html/2508.16577/x14.png",
                "caption": "Figure 13:Additional Results.",
                "position": 1798
            },
            {
                "img": "https://arxiv.org/html/2508.16577/x15.png",
                "caption": "Figure 14:Additional qualitative evaluation.Additional examples to those shown in Fig.7.",
                "position": 1802
            },
            {
                "img": "https://arxiv.org/html/2508.16577/x16.png",
                "caption": "Figure 15:Additional qualitative evaluation.Additional examples to those shown in Fig.7.",
                "position": 1806
            }
        ]
    },
    {
        "header": "Appendix GBroader Impact",
        "images": []
    }
]