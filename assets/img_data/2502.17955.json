[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17955/x1.png",
                "caption": "Figure 1:Illustratation of the cross-lingual factual knowledge transferability issue across linguistic knowledge clouds in LMs. The model correctly recalls thatRashed Al Shashai is from Saudi Arabiawhen queried in Arabic, but fails to retrieve this fact in English and Swahili, highlighting that factual knowledge is often stored in language-specific silos.",
                "position": 155
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17955/x2.png",
                "caption": "Figure 2:Examples from our multilingual dataset illustrating three tasks. Factual Recall: LMs recall country-specific facts better in native languages, as seen with Dharanâ€™s correct identification in Nepali but incorrect in English. Incontext Recall: Models struggle with contextual reasoning, showing regional bias when associating names with countries. Counter-Factual Context Adherence: When given counterfactual prompts about well-known figures, models rely on prior knowledge, affecting their ability to adhere to provided context.",
                "position": 226
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17955/extracted/6231246/figures/Factual_Recall_Mean.png",
                "caption": "Figure 3:Error rates for each model on the Factual Recall task. A clear pattern emerges, showing a decline in performance as we move from larger to smaller models (top to bottom) and from high-resource to low-resource languages (left to right).",
                "position": 471
            },
            {
                "img": "https://arxiv.org/html/2502.17955/extracted/6231246/figures/xfact.png",
                "caption": "Figure 4:This figure illustrates the model-wise comparison of X-FAKT scores grouped by language families. A clear trend emerges, showing that as the model size increases within a family, the X-FAKT score tends to increase.",
                "position": 655
            },
            {
                "img": "https://arxiv.org/html/2502.17955/extracted/6231246/figures/Counterfactual_Context_Adherence.png",
                "caption": "Figure 5:Error rate for each model onCounter-Factual Context Adherencetask. Models show high error rates in high resource languages such as English and French where they have high factual recall.",
                "position": 720
            },
            {
                "img": "https://arxiv.org/html/2502.17955/extracted/6231246/figures/liwei.png",
                "caption": "Figure 6:Mistral-7B-v0.2output when prompted with the given context in English. This model generation shows how spurious correlation leads to in-context recall failures",
                "position": 736
            },
            {
                "img": "https://arxiv.org/html/2502.17955/extracted/6231246/figures/george_washi.png",
                "caption": "Figure 7:Llama-3-70Boutput when prompted with a counter-factual context adherence query in English. This shows LMs favour internal knowledge over contextual understanding.",
                "position": 748
            },
            {
                "img": "https://arxiv.org/html/2502.17955/extracted/6231246/figures/french_qual.png",
                "caption": "Figure 8:Llama-3-70Boutput when prompted with a factual recall query in English",
                "position": 760
            },
            {
                "img": "https://arxiv.org/html/2502.17955/extracted/6231246/figures/hindi_qual.png",
                "caption": "Figure 9:Llama-3-70Boutput when prompted with a factual recall query in Hindi. In Hindi, it misinterprets understanding of a French word.",
                "position": 763
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "7Ethics Statement",
        "images": []
    },
    {
        "header": "8Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17955/extracted/6231246/figures/model_comparison_plot.png",
                "caption": "Figure 10:Comparision of models (in the increasing order of size with respect to the parameters) using Factual Recall Score, Knowledge Transferability Score, and Cross-Lingual Factual Knowledge Transferability Score.",
                "position": 1406
            },
            {
                "img": "https://arxiv.org/html/2502.17955/x3.png",
                "caption": "Figure 11:Prompt used for evaluation of Factual Recall and In-Context Recall tasks.",
                "position": 1718
            },
            {
                "img": "https://arxiv.org/html/2502.17955/x4.png",
                "caption": "Figure 12:Prompt used for evaluation ofCounter-Factual Context Adherencetask.",
                "position": 1721
            },
            {
                "img": "https://arxiv.org/html/2502.17955/extracted/6231246/figures/In-context_Recall_Mean.png",
                "caption": "Figure 13:Error rate for each model onIn-context Recalltask. Clearly, few models such asDeepSeek-7B,Phi-3-4B, etc. performs poorly on this simple task.",
                "position": 1847
            },
            {
                "img": "https://arxiv.org/html/2502.17955/x5.png",
                "caption": "Figure 14:English Fall Back Rate across models (The English Fall Back Rate measures the frequency with which a model defaults to English in its output).",
                "position": 1850
            },
            {
                "img": "https://arxiv.org/html/2502.17955/x6.png",
                "caption": "Figure 15:Country-Specific Factual Error Rates in each language forLlama-3-70B",
                "position": 1853
            },
            {
                "img": "https://arxiv.org/html/2502.17955/x7.png",
                "caption": "Figure 16:Country-Specific Factual Error Rates in each language forGemma-2-27B",
                "position": 1856
            },
            {
                "img": "https://arxiv.org/html/2502.17955/x8.png",
                "caption": "Figure 17:Country-Specific Factual Error Rates in each language forPhi-4-14B",
                "position": 1859
            },
            {
                "img": "https://arxiv.org/html/2502.17955/x9.png",
                "caption": "Figure 18:Country-Specific Factual Error Rates in each language forPhi-3-14B",
                "position": 1862
            },
            {
                "img": "https://arxiv.org/html/2502.17955/x10.png",
                "caption": "Figure 19:Country-Specific Factual Error Rates in each language forGemma-2-9B",
                "position": 1865
            },
            {
                "img": "https://arxiv.org/html/2502.17955/x11.png",
                "caption": "Figure 20:Country-Specific Factual Error Rates in each language forLlama-3-8B",
                "position": 1868
            },
            {
                "img": "https://arxiv.org/html/2502.17955/x12.png",
                "caption": "Figure 21:Country-Specific Factual Error Rates in each language forOrca-2-7B",
                "position": 1871
            },
            {
                "img": "https://arxiv.org/html/2502.17955/x13.png",
                "caption": "Figure 22:Country-Specific Factual Error Rates in each language forDeepSeek-7B",
                "position": 1874
            },
            {
                "img": "https://arxiv.org/html/2502.17955/x14.png",
                "caption": "Figure 23:Country-Specific Factual Error Rates in each language forMistral-7B-v0.2",
                "position": 1877
            },
            {
                "img": "https://arxiv.org/html/2502.17955/x15.png",
                "caption": "Figure 24:Country-Specific Factual Error Rates in each language forPhi-3.5-4B",
                "position": 1880
            },
            {
                "img": "https://arxiv.org/html/2502.17955/x16.png",
                "caption": "Figure 25:Country-Specific Factual Error Rates in each language forPhi-3-4B",
                "position": 1883
            },
            {
                "img": "https://arxiv.org/html/2502.17955/x17.png",
                "caption": "Figure 26:Country-Specific Factual Error Rates in each language forLlama-3.2-3B",
                "position": 1886
            },
            {
                "img": "https://arxiv.org/html/2502.17955/x18.png",
                "caption": "Figure 27:Country-Specific Factual Error Rates in each language forGemma-2-2B",
                "position": 1889
            },
            {
                "img": "https://arxiv.org/html/2502.17955/x19.png",
                "caption": "Figure 28:Country-Specific Factual Error Rates in each language forLlama-3.2-1B",
                "position": 1892
            }
        ]
    },
    {
        "header": "Appendix AAPPENDIX",
        "images": []
    }
]