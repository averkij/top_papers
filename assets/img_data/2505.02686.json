[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02686/extracted/6412844/img/icon.png",
                "caption": "",
                "position": 147
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02686/x1.png",
                "caption": "Figure 1:Illustration of the scaling phases of LLMs.\nThe learning-from-rewards paradigm plays a pivotal role in the post-training and test-time scaling.",
                "position": 222
            },
            {
                "img": "https://arxiv.org/html/2505.02686/x2.png",
                "caption": "Figure 2:A unified framework of learning from rewards.\nThe language model generates outputs conditioned on the inputs;\nthe reward model evaluates the outputs and provides reward signals based on diverse feedback sources and design choices;\nthe learning strategy leverages the rewards to either fine-tune the language model or refine the outputs.\nThis learning-from-rewards paradigm aims to fulfill preference alignment and task-specific goals.\nThe learning strategy can occur at the training, inference, or post-inference stages.",
                "position": 240
            }
        ]
    },
    {
        "header": "2A Taxonomy of Learning from Rewards for LLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02686/x3.png",
                "caption": "Figure 3:Reward Model (RM) design dimensions:\n(a) Model Architecture (Model-based and Model-free);\n(b) Reward Format (Scalar, Critique, and Implicit);\n(c) Scoring Pattern (Pointwise and Pairwise);\n(d) Reward Granularity (Outcome and Process).",
                "position": 311
            },
            {
                "img": "https://arxiv.org/html/2505.02686/x4.png",
                "caption": "Figure 4:Illustration ofTraining with Rewards.\nBased on the reward model design,\nwe mainly focus on scalar rewards, critique rewards, implicit rewards, rule-based rewards, and process rewards.\nThese reward signals are used to fine-tune the language model through reinforcement learning algorithms or supervised fine-tuning.",
                "position": 522
            }
        ]
    },
    {
        "header": "3Training with Rewards",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02686/x5.png",
                "caption": "Figure 6:Illustrations of strategies forInference with Rewards.\n(a,b): Generate-then-rank with outcome and process rewards.\n(c): Reward-guided decoding at the token and step level with search algorithms.",
                "position": 1071
            }
        ]
    },
    {
        "header": "4Inference with Rewards",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02686/x6.png",
                "caption": "Figure 8:Illustration ofPost-Inference with Rewards.\n(a): Self-Correction, using the language model itself.\n(b): Correction with External Feedback, such as trained model, external knowledge, and external tools.",
                "position": 1338
            }
        ]
    },
    {
        "header": "5Post-Inference with Rewards",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02686/x7.png",
                "caption": "Figure 10:Illustration ofBenchmarking Reward Models.\nAnnotations come from human annotators or AI annotators with human verification.\nThe annotations are mainly pointwise (e.g.,a scalar score for each sample) or pairwise (e.g.,chosen and rejected responses).",
                "position": 1545
            }
        ]
    },
    {
        "header": "6Benchmarking Reward Models",
        "images": []
    },
    {
        "header": "7Applications",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02686/x8.png",
                "caption": "Figure 12:Illustration of challenges and future directions.",
                "position": 1886
            }
        ]
    },
    {
        "header": "8Challenges and Future Directions",
        "images": []
    },
    {
        "header": "9Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]