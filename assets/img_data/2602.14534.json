[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.14534/figs/morl_logo.png",
                "caption": "",
                "position": 78
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.14534/x1.png",
                "caption": "Figure 1:Visualization comparisons with MotionLLM. In the backflip example, MotionLLM fails to maintain a coherent takeoff-rotation-landing trajectory, resulting in unstable body orientation, while MoRL completes a physically plausible flip. In the Wack-style dance, MotionLLM shows inconsistent rotation direction and fragmented poses, whereas MoRL preserves continuous left-to-right rotation and stylistic coherence.",
                "position": 128
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.14534/x2.png",
                "caption": "Figure 2:Motion CoT data engine. Build based on MotionHubV2 dataset(Linget al.,2024), one branch (MoUnd-CoT) uses motion sequences and captions with Gemini to construct reasoning chains for understanding, while the other (MoGen-CoT) builds reasoning chains for generation.",
                "position": 180
            }
        ]
    },
    {
        "header": "3Data Synthesis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.14534/x3.png",
                "caption": "Figure 3:Overview of MoRL. Our framework unifies motion understanding and generation under a reinforcement learning paradigm. Motion and text inputs are tokenized into a shared representation space. A hierarchical post-training pipeline first applies SFT on large-scale synthetic CoT datasets to align motion sequences with reasoning traces and concise descriptions, then employs reinforcement learning with verifiable rewards (RLVR) to refine outputs, enhancing semantic alignment, reasoning coherence, physical plausibility, and textâ€“motion consistency. At inference, the Chain-of-Motion (CoM) decoding strategy enables step-by-step reasoning and reflection, improving both motion understanding and perceptually realistic motion generation.",
                "position": 202
            }
        ]
    },
    {
        "header": "4The Proposed Method",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALLM Use Declaration",
        "images": []
    },
    {
        "header": "Appendix BMore Implementation Details",
        "images": []
    },
    {
        "header": "Appendix CInference Latency and Throughput of CoM",
        "images": []
    },
    {
        "header": "Appendix DChoice of RL Optimizer",
        "images": []
    },
    {
        "header": "Appendix EUser Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.14534/figs/human_eval_rating_distribution_4point.png",
                "caption": "Figure 4:Results of user study.",
                "position": 2469
            },
            {
                "img": "https://arxiv.org/html/2602.14534/figs/visualization/aa.png",
                "caption": "Table 7:Qualitative comparison (Part I).",
                "position": 2472
            },
            {
                "img": "https://arxiv.org/html/2602.14534/figs/visualization/bb.png",
                "caption": "",
                "position": 2505
            },
            {
                "img": "https://arxiv.org/html/2602.14534/figs/visualization/cc.png",
                "caption": "",
                "position": 2516
            },
            {
                "img": "https://arxiv.org/html/2602.14534/figs/visualization/dd.png",
                "caption": "",
                "position": 2520
            },
            {
                "img": "https://arxiv.org/html/2602.14534/figs/visualization/com-shifting.png",
                "caption": "",
                "position": 2531
            },
            {
                "img": "https://arxiv.org/html/2602.14534/figs/visualization/MoRL-shifting.png",
                "caption": "",
                "position": 2535
            },
            {
                "img": "https://arxiv.org/html/2602.14534/figs/visualization/comsilly.png",
                "caption": "",
                "position": 2546
            },
            {
                "img": "https://arxiv.org/html/2602.14534/figs/visualization/MoRLsilly.png",
                "caption": "",
                "position": 2550
            },
            {
                "img": "https://arxiv.org/html/2602.14534/figs/visualization/com-infinity.png",
                "caption": "Table 8:Qualitative comparison (Part II).",
                "position": 2561
            },
            {
                "img": "https://arxiv.org/html/2602.14534/figs/visualization/MoRL-infinity.png",
                "caption": "",
                "position": 2594
            },
            {
                "img": "https://arxiv.org/html/2602.14534/figs/visualization/com-curve.png",
                "caption": "",
                "position": 2605
            },
            {
                "img": "https://arxiv.org/html/2602.14534/figs/visualization/MoRL-curve.png",
                "caption": "",
                "position": 2609
            },
            {
                "img": "https://arxiv.org/html/2602.14534/figs/visualization/com-flip.png",
                "caption": "",
                "position": 2620
            },
            {
                "img": "https://arxiv.org/html/2602.14534/figs/visualization/MoRL-flip.png",
                "caption": "",
                "position": 2624
            },
            {
                "img": "https://arxiv.org/html/2602.14534/figs/visualization/com-kangfu.png",
                "caption": "",
                "position": 2635
            },
            {
                "img": "https://arxiv.org/html/2602.14534/figs/visualization/MoRL-kangfu.png",
                "caption": "",
                "position": 2639
            }
        ]
    },
    {
        "header": "Appendix FMore Qualitative Results",
        "images": []
    },
    {
        "header": "Appendix GEthical considerations",
        "images": []
    }
]