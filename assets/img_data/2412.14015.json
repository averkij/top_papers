[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14015/x1.png",
                "caption": "",
                "position": 145
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14015/x2.png",
                "caption": "Figure 2:Overview ofPrompt Depth Anything.(a)Prompt Depth Anythingbuilds on a depth foundation model[70]with a ViT encoder and a DPT decoder, and adds a multi-scale prompt fusion design, using a prompt fusion block to fuse the metric information at each scale.\n(b)\nSince training requires both low-cost LiDAR and precise GT depth, we propose a scalable data pipeline that simulates LiDAR depth for synthetic data with precise GT depth, and generates pseudo GT depth for real data with LiDAR.\nAn edge-aware depth loss is proposed to merge accurate edges from pseudo GT depth with accurate depth in textureless areas from FARO annotated GT depth on real data.",
                "position": 218
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14015/x3.png",
                "caption": "Figure 3:Effects on the synthetic data lidar simulation and real data pseudo GT generation with the edge-aware depth loss.The middle and right columns are the depth prediction results of our different models. The two rows highlight the significance of sparse anchor interpolation for lidar simulation and pseudo GT generation with edge-aware depth loss, respectively.",
                "position": 292
            },
            {
                "img": "https://arxiv.org/html/2412.14015/x4.png",
                "caption": "Figure 4:Qualitative comparisons with the state-of-the-art.“Metric3D v2” and “Depth Any. v2” are scale-shift corrected with ARKit depth.The pink boxesdenote the GT depth and depth percentage error map, whereredrepresents high error, andblueindicates low error.",
                "position": 659
            },
            {
                "img": "https://arxiv.org/html/2412.14015/x5.png",
                "caption": "Figure 5:Qualitative comparisons of TSDF reconstruction.*_align denotes the scale-shift corrected depth with ARKit depth.",
                "position": 665
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14015/x6.png",
                "caption": "Figure 6:Outdoor reconstruction by taking the vehicle LiDAR as metric prompt.Please refer to the supp. for more video results.",
                "position": 1100
            },
            {
                "img": "https://arxiv.org/html/2412.14015/x7.png",
                "caption": "Figure 7:Zero-shot testing on diverse scenes.",
                "position": 1110
            },
            {
                "img": "https://arxiv.org/html/2412.14015/x8.png",
                "caption": "Figure 8:Robotic grasping setup and input signal types.Our goal is to grasp objects of various types using image/LiDAR/depth inputs. Red rectangles indicate potential object positions.",
                "position": 1133
            }
        ]
    },
    {
        "header": "5Conclusion and Discussions",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14015/x9.png",
                "caption": "Figure 9:Our accurate and high-resolution depth enables dynamic 3D reconstruction from a single moving camera.\nHere we illustrate the reconstruction results of a human walking in the library. The foreground is segmented with a SAM2[49]model.",
                "position": 2326
            }
        ]
    },
    {
        "header": "Appendix AAdditional Discussions",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14015/x10.png",
                "caption": "Figure 10:Generalizability to different resolutions.Our model can infer depth for images of different resolutions from 512p to 2160p.",
                "position": 2340
            },
            {
                "img": "https://arxiv.org/html/2412.14015/x11.png",
                "caption": "Figure 11:Effects of using real data.",
                "position": 2358
            }
        ]
    },
    {
        "header": "Appendix BAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14015/x12.png",
                "caption": "Figure 12:Visualization results of simulated LiDAR.“Interp. Simu.” is the proposed interpolation method, which is interpolated from sparse anchors depth. This method effectively simulates the noise of real LiDAR data. We also provide the naive downsampled simulated LiDAR for comparison.",
                "position": 2443
            },
            {
                "img": "https://arxiv.org/html/2412.14015/x13.png",
                "caption": "Figure 13:ZipNeRF depth of different training frames.Training with resampled frames removing blurred frames leads to a better ZipNeRF reconstruction.",
                "position": 2453
            },
            {
                "img": "https://arxiv.org/html/2412.14015/x14.png",
                "caption": "Figure 14:Illustration of different depth annotation types.Please refer toAppendixBfor more descriptions.",
                "position": 2469
            }
        ]
    },
    {
        "header": "Appendix CMore Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14015/x15.png",
                "caption": "Figure 15:Illustrations of our method and optional designs.Please refer toSec.C.2for more details.",
                "position": 2501
            },
            {
                "img": "https://arxiv.org/html/2412.14015/x16.png",
                "caption": "Figure 16:Qualitative comparison of vehicle LiDAR completion.We include more video results in the supplementary video.",
                "position": 2608
            }
        ]
    },
    {
        "header": "Appendix DPrompting with a vehicle LiDAR",
        "images": []
    },
    {
        "header": "Appendix EGeneralized Robotic Grasping Details",
        "images": []
    }
]