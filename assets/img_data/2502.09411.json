[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09411/x1.png",
                "caption": "",
                "position": 96
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/hallucinations/prompt_516_no_imageRAG.png",
                "caption": "Figure 2:Hallucinations.When models do not know the meaning of a prompt, they may ‚Äúhallucinate‚Äù and generate unrelated images (left).\nBy applying our method to retrieve and utilize relevant references (mid), the base models can generate appropriate images (right).",
                "position": 116
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/hallucinations/image_325036.jpg",
                "caption": "",
                "position": 132
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/hallucinations/prompt_516.png",
                "caption": "",
                "position": 133
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/hallucinations/prompt_494_no_imageRAG.png",
                "caption": "",
                "position": 139
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/hallucinations/image_746802.jpg",
                "caption": "",
                "position": 140
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/hallucinations/prompt_494.png",
                "caption": "",
                "position": 141
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09411/x2.png",
                "caption": "Figure 3:Top: a high-level overview of our method. Given a text prompt<‚Å¢p‚Å¢>ùëù\\mathord{<}p\\mathord{>}< italic_p >, we generate an initial image using a text-to-image (T2I) model. Then, we generate retrieval-captions<‚Å¢cj‚Å¢>subscriptùëêùëó\\mathord{<}c_{j}\\mathord{>}< italic_c start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT >, retrieve images from an external database for each caption<‚Å¢ij‚Å¢>subscriptùëñùëó\\mathord{<}i_{j}\\mathord{>}< italic_i start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT >, and use them as references to the model for better generation.Bottom: the retrieval-caption generation block.\nWe use a VLM to decide if the initial image matches the given prompt. If not, we ask it to list the missing concepts, and to create a caption that could be used to retrieve appropriate examples for each of these missing concepts.",
                "position": 236
            },
            {
                "img": "https://arxiv.org/html/2502.09411/x3.png",
                "caption": "Figure 4:Personalized generation example.ImageRAGcan work in parallel with personalization methods and enhance their capabilities. For example, although OmniGen can generate images of a subject based on an image, it struggles to generate some concepts. Using references retrieved by our method, it can generate the required result.",
                "position": 316
            }
        ]
    },
    {
        "header": "4Implementation Details",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09411/x4.png",
                "caption": "Figure 5:Retrieval dataset size vs. CLIP score on ImageNet (left) and Aircraft (right).Dashed lines represent the scores of the base models.\nEven relatively small, unspecialized retrieval sets can already improve results. More data leads to further increased scores. However, small sets may not contain relevant retrieval examples, and their use may harm results, particularly for stronger models.",
                "position": 674
            },
            {
                "img": "https://arxiv.org/html/2502.09411/x5.png",
                "caption": "Figure 6:User study results.Users preference percentage of our method compared to other methods in terms of text alignment, visual quality, and overall preference.",
                "position": 749
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/chow_ref.jpg",
                "caption": "Figure 7:Qualitative comparisons: rare concept generation.Examples from ImageNet(Deng et¬†al.,2009), CUB(Wah et¬†al.,2011)and iNaturalist(Van¬†Horn et¬†al.,2018). The left-most image column is the retrieved reference usingImageRAGfor each prompt. OmniGen and SDXL both struggle with the uncommon concepts, sometimes generating similar concepts such as a bull or a cow instead of the dog breed ‚ÄúBoston bull‚Äù, while in other times, they generate completely unrelated images, as in the case of ‚ÄúChow‚Äù, or ‚ÄúGeococcyx‚Äù. When usingImageRAGboth models generate the correct concept.",
                "position": 753
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/chow_o.jpg",
                "caption": "",
                "position": 780
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/chow_sd.jpg",
                "caption": "",
                "position": 781
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/chow_imagerag_o.jpg",
                "caption": "",
                "position": 782
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/chow_imagerag_sd.jpg",
                "caption": "",
                "position": 783
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/boston_bull_ref.jpg",
                "caption": "",
                "position": 787
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/boston_bull_o.jpg",
                "caption": "",
                "position": 788
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/boston_bull_sd.jpg",
                "caption": "",
                "position": 789
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/boston_bull_imagerag_o.jpg",
                "caption": "",
                "position": 790
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/boston_bull_imagerag_sd.jpg",
                "caption": "",
                "position": 791
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/cab_ref.jpg",
                "caption": "",
                "position": 795
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/cab_o.jpg",
                "caption": "",
                "position": 796
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/cab_sd.jpg",
                "caption": "",
                "position": 797
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/cab_imagerag_o.jpg",
                "caption": "",
                "position": 798
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/cab_imagerag_sd.jpg",
                "caption": "",
                "position": 799
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/academic_gown_ref.jpg",
                "caption": "",
                "position": 803
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/academic_gown_o.jpg",
                "caption": "",
                "position": 804
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/academic_gown_sd.jpg",
                "caption": "",
                "position": 805
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/academic_gown_imagerag_o.jpg",
                "caption": "",
                "position": 806
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/academic_gown_imagerag_sd.jpg",
                "caption": "",
                "position": 807
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/unicycle_ref.jpg",
                "caption": "",
                "position": 811
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/unicycle_o.jpg",
                "caption": "",
                "position": 812
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/unicycle_sd.jpg",
                "caption": "",
                "position": 813
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/unicycle_imagerag_o.jpg",
                "caption": "",
                "position": 814
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/unicycle_imagerag_sd.jpg",
                "caption": "",
                "position": 815
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/geo_ref.jpg",
                "caption": "",
                "position": 819
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/geo_o.jpg",
                "caption": "",
                "position": 820
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/geo_sd.jpg",
                "caption": "",
                "position": 821
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/geo_imagerag_o.jpg",
                "caption": "",
                "position": 822
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/geo_imagerag_sd.jpg",
                "caption": "",
                "position": 823
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/iNat_20_ref.jpg",
                "caption": "",
                "position": 827
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/iNat_20_o.jpg",
                "caption": "",
                "position": 828
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/iNat_20_sd.jpg",
                "caption": "",
                "position": 829
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/iNat_20_imagerag_o.jpg",
                "caption": "",
                "position": 830
            },
            {
                "img": "https://arxiv.org/html/2502.09411/extracted/6201659/Assets/comp/iNat_20_imagerag_sd.jpg",
                "caption": "",
                "position": 831
            }
        ]
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Ethical statement",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARetrieval-Caption Generation Prompts",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09411/x6.png",
                "caption": "Figure 8:Comparisons between ImageRAG and different methods using retrieval for generation.Prompts and results of all other methods are taken from their papers. The methods we compared to are RDM(Blattmann et¬†al.,2022), Re-Imagen(Chen et¬†al.,2022), and KNN-Diffusion(Sheynin et¬†al.,2022).",
                "position": 1897
            },
            {
                "img": "https://arxiv.org/html/2502.09411/x7.png",
                "caption": "Figure 9:Examples of rare concept generation using ImageRAG with SDXL.Real examples are taken from the iNaturalist(Van¬†Horn et¬†al.,2018)dataset.",
                "position": 1901
            },
            {
                "img": "https://arxiv.org/html/2502.09411/x8.png",
                "caption": "Figure 10:Examples of rare concept generation using ImageRAG with OmniGen.Real examples are taken from the iNaturalist(Van¬†Horn et¬†al.,2018)dataset.",
                "position": 1906
            },
            {
                "img": "https://arxiv.org/html/2502.09411/x9.png",
                "caption": "Figure 11:More creative generation examples.",
                "position": 1914
            }
        ]
    },
    {
        "header": "Appendix CUser Study Questions",
        "images": []
    }
]