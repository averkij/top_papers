[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13079/x1.png",
                "caption": "Figure 1:An example from ourRamDocs dataset (left) illustrating multiple sources of conflict in retrieved documents. Conflict may occur because of ambiguity in the query (in this case, different referents for Michael Jordan), but also because of misinformation from incorrect documents and noise from irrelevant ones.Madam-RAG (right) addresses this through multi-agent debate, where each agent summarizes and represents the information in one document. Agents discuss their responses across multiple rounds, with the final answers being combined via an aggregator module that summarizes the discussion.",
                "position": 140
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3RamDocs:Retrieval withAmbiguity &Misinformation inDocuments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13079/x2.png",
                "caption": "Figure 2:Madam-RAG operates by assigning each document to a separate agent.\nAgents are responsible for summarizing and representing their documents, and engage in a multi-agent, multi-round debate with each other to filter out misinformation and noise and address ambiguity.\nAn aggregator then summarizes the discussion into a single response.",
                "position": 240
            }
        ]
    },
    {
        "header": "4Madam-RAG:Multi-agentDebate forAmbiguity andMisinformation inRAG",
        "images": []
    },
    {
        "header": "5Experiments and Results",
        "images": []
    },
    {
        "header": "6Ablations and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13079/x3.png",
                "caption": "Figure 3:Ablation study on the importance of aggregator and multiple rounds of discussion.",
                "position": 478
            },
            {
                "img": "https://arxiv.org/html/2504.13079/x4.png",
                "caption": "Figure 4:Performance of different methods with Llama3.3-70B-Instruct under setting (a) imbalance in supporting documents and (b) increasing the level of misinformation.",
                "position": 506
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARamDocs: Dataset Statistics",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13079/x5.png",
                "caption": "Figure 5:Dataset statistics across eight dimensions. The first row shows document-level properties per example: total number of documents (avg: 5.53), number of documents supporting correct answers (avg: 3.84), incorrect answers (avg: 0.61), and noisy or irrelevant content (avg: 1.08). The second row presents answer-level properties: number of correct answers per example (avg: 2.20), wrong answers (avg: 0.86), and number of documents supporting each correct answer (avg: 1.77) and each wrong answer (avg: 0.73).",
                "position": 1289
            }
        ]
    },
    {
        "header": "Appendix BAstute RAG",
        "images": []
    },
    {
        "header": "Appendix CPrompts",
        "images": []
    }
]