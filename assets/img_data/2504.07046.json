[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07046/x1.png",
                "caption": "Figure 1:An example of subject-driven image editing with human-annotated low scores.\nBoth traditional metrics and GPT-4o-based VIEScore assign high scores.\nBy integrating GPT-4o with tools,CIGEval, our agentic evaluation framework, highlights the glasses object in both images, and finds their different shapes and designs, thereby reaching the correct score. “Source” and “Subject” means “source image” and “subject image”.",
                "position": 106
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3CIGEval",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07046/x2.png",
                "caption": "Table 1:Tools used in ourCIGEvalframework.",
                "position": 308
            },
            {
                "img": "https://arxiv.org/html/2504.07046/extracted/6348981/figures/grounding.png",
                "caption": "",
                "position": 337
            },
            {
                "img": "https://arxiv.org/html/2504.07046/extracted/6348981/figures/after_grounding.png",
                "caption": "",
                "position": 347
            },
            {
                "img": "https://arxiv.org/html/2504.07046/x3.png",
                "caption": "",
                "position": 358
            },
            {
                "img": "https://arxiv.org/html/2504.07046/extracted/6348981/figures/highlight.png",
                "caption": "",
                "position": 363
            },
            {
                "img": "https://arxiv.org/html/2504.07046/extracted/6348981/figures/after_highlight.png",
                "caption": "",
                "position": 373
            },
            {
                "img": "https://arxiv.org/html/2504.07046/x4.png",
                "caption": "",
                "position": 384
            },
            {
                "img": "https://arxiv.org/html/2504.07046/extracted/6348981/figures/diff1.png",
                "caption": "",
                "position": 389
            },
            {
                "img": "https://arxiv.org/html/2504.07046/extracted/6348981/figures/diff2.png",
                "caption": "",
                "position": 394
            },
            {
                "img": "https://arxiv.org/html/2504.07046/extracted/6348981/figures/combine.png",
                "caption": "",
                "position": 399
            },
            {
                "img": "https://arxiv.org/html/2504.07046/x5.png",
                "caption": "",
                "position": 410
            },
            {
                "img": "https://arxiv.org/html/2504.07046/extracted/6348981/figures/scene_graph_1.png",
                "caption": "",
                "position": 415
            },
            {
                "img": "https://arxiv.org/html/2504.07046/extracted/6348981/figures/scene_graph_2.png",
                "caption": "",
                "position": 421
            },
            {
                "img": "https://arxiv.org/html/2504.07046/x6.png",
                "caption": "Figure 2:The evaluation process ofCIGEvalregarding the example in Figure1.CIGEvalautonomously selects appropriate tools for each decomposed sub-task, and then conducts fine-grained analyses based on the observed tool outputs.",
                "position": 450
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07046/x7.png",
                "caption": "Figure 3:Case study of a text-guided image editing example with a low human annotation score.",
                "position": 1752
            },
            {
                "img": "https://arxiv.org/html/2504.07046/x8.png",
                "caption": "Figure 4:Case study of GPT-4o’s image generation. Examples are adapted from OpenAI’s official website.",
                "position": 1787
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APrompt Templates",
        "images": []
    },
    {
        "header": "Appendix BDetails of ImagenHub",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07046/x9.png",
                "caption": "Figure 5:Case study of a multi-concept image composition example. Here is the fine-grained score for concept consistency.",
                "position": 3066
            },
            {
                "img": "https://arxiv.org/html/2504.07046/x10.png",
                "caption": "Figure 6:Case study of GPT-4o’s image generation. Examples are taken from ImagenHub’s control-guided image generation task.",
                "position": 3069
            },
            {
                "img": "https://arxiv.org/html/2504.07046/x11.png",
                "caption": "Figure 7:Case study of GPT-4o’s image generation. Examples are taken from ImagenHub’s multi-concept image composition task.",
                "position": 3072
            },
            {
                "img": "https://arxiv.org/html/2504.07046/x12.png",
                "caption": "Figure 8:Case study of GPT-4o’s image generation. Examples are taken from ImagenHub’s subject-driven image editing task.",
                "position": 3075
            },
            {
                "img": "https://arxiv.org/html/2504.07046/x13.png",
                "caption": "Figure 9:Case study of GPT-4o’s image generation. Examples are taken from ImagenHub’s subject-driven image generation task.",
                "position": 3078
            },
            {
                "img": "https://arxiv.org/html/2504.07046/x14.png",
                "caption": "Figure 10:Case study of GPT-4o’s image generation. Examples are taken from ImagenHub’s text-guided image generation task.",
                "position": 3081
            },
            {
                "img": "https://arxiv.org/html/2504.07046/x15.png",
                "caption": "Figure 11:Case study of GPT-4o’s image generation. Examples are taken from ImagenHub’s text-guided image editing task.",
                "position": 3084
            }
        ]
    },
    {
        "header": "Appendix CMore Cases",
        "images": []
    }
]