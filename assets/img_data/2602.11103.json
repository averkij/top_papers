[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11103/imgs/taxonomy-examples.png",
                "caption": "Figure 1:We presentGameDevBench, a benchmark for evaluating an agentâ€™s ability to solve complex and multimodal game development tasks in a modern game engine.",
                "position": 167
            }
        ]
    },
    {
        "header": "2Benchmark Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11103/imgs/example_workflow.png",
                "caption": "Figure 2:This is an example task fromGameDevBenchthat requests for the creation of a UI minimap.\nTop is the visual GUI representation and highlighted points of interest.\nBottom is the same scenes and files represented in code.\nTasks can be solved via the editor or entirely through code although either method requires understanding multimodal assets.\nGame development tasks are complex and require editing dense files, identifying and visually understanding various assets, and navigating various nodes (game elements) and scenes (a collection of nodes).",
                "position": 233
            }
        ]
    },
    {
        "header": "3GameDevBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11103/imgs/editors.png",
                "caption": "Figure 3:Types of editors in Godot.\nTop-left is the scene editor.\nTop-right is the script editor.\nThe bottom contains various contextual editors.\nFrom left to right: tilemap, shader, animation, and audio editors.\nContextual editors surface depending on use case.\nTypically, tasks that use contextual editors require deeper multi-modal understanding.",
                "position": 462
            },
            {
                "img": "https://arxiv.org/html/2602.11103/x1.png",
                "caption": "Figure 4:GameDevBenchfeatures a diverse amount of filetypes (27 different types, left).\nThe vast majority of tasks contain either images, resources (e.g., Shaders), or multiple asset types (middle).\nEach task contains multiple scripts and scenes, both of which are context-rich and require a significant amount of tokens to process (right).",
                "position": 538
            },
            {
                "img": "https://arxiv.org/html/2602.11103/x2.png",
                "caption": "",
                "position": 547
            }
        ]
    },
    {
        "header": "4Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11103/x3.png",
                "caption": "Figure 5:In general, agents perform better on tasks that require skills focusing on gameplay functionality compared to tasks that require multimodal understanding such as 2D and 3D graphics tasks.\nPerformance on editor categories is dependent on model performance.\nStronger models (left 4 agents) tend to perform similarly across all editor types, while weaker models (right 3 agents) tend to perform worse on tasks requiring the scene and contextual editors.\nAll success rates are taken from results where the agent has access to multimodal feedback.",
                "position": 674
            },
            {
                "img": "https://arxiv.org/html/2602.11103/x4.png",
                "caption": "Figure 6:We capture the trade-off between performance and cost.\nIn general, using multimodal feedback increases cost per task while increasing performance.\nAgents above the linear-fit outperform the average cost-to-success ratio.\nWe findgemini-3-flash-previewto be the most cost-effective model.\nSincegemini-clidoes not return the full agent trajectory, we use theOpenHandscost which is likely anoverestimationasOpenHandsis typically more costly.",
                "position": 916
            }
        ]
    },
    {
        "header": "5Related Works",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATask Construction Prompt",
        "images": []
    },
    {
        "header": "Appendix BTask Refinement Prompt",
        "images": []
    },
    {
        "header": "Appendix CHuman Annotation Instructions",
        "images": []
    },
    {
        "header": "Appendix DPrompt Templates",
        "images": []
    },
    {
        "header": "Appendix ETask Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11103/x5.png",
                "caption": "Figure 7:An example task fromGameDevBench. In this example, the goal is to add physical collision and animation to the character.\nThis can be achieved through either taking actions directly in the editor or editing code files.\nEach action in the editor is equivalent to specific modifications within the code files.\nMatching steps are denoted with the same numbers in our figure.",
                "position": 2606
            },
            {
                "img": "https://arxiv.org/html/2602.11103/x6.png",
                "caption": "Figure 8:An example task fromGameDevBench. In this example, the goal is to populate an empty 3D scene with a water depth visualization, including environment lighting, shader-driven water plane, background spheres, and a camera.\nThis can be achieved through either taking actions directly in the\neditor or editing the scene file (main.tscn). Each action in the editor\nis equivalent to specific modifications within the scene file. Matching\nsteps are denoted with the same numbers in our figure",
                "position": 2622
            },
            {
                "img": "https://arxiv.org/html/2602.11103/x7.png",
                "caption": "Figure 9:An example task fromGameDevBench. In this example, the goal is to build a complete three-screen menu system (Launch, Pause, and Restart) with styled buttons, title labels, a shader-driven transition overlay, and signal connections to the menu handler script. This can be achieved through either taking actions directly in the editor or editing the scene file (menus.tscn). Each action in the editor is equivalent to specific modifications within the scene file. Matching steps are denoted with the same numbers in our figure.",
                "position": 2639
            },
            {
                "img": "https://arxiv.org/html/2602.11103/x8.png",
                "caption": "Figure 10:An example task fromGameDevBench. In this example, the goal is to build a reusable RTS unit with a sprite, collision shapes, a detection area for neighbor avoidance, and an aura shader that highlights the unit when selected. Unlike purely scene-based tasks, this task requires both editing the scene file (player.tscn) and implementing gameplay logic in a script file (unit.gd). Each action in the editor is equivalent to specific modifications within the code files. Matching steps are denoted with the same numbers in our figure.",
                "position": 2654
            }
        ]
    },
    {
        "header": "Appendix FTask Statistics",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11103/imgs/3114_case.png",
                "caption": "Figure 11:Example of Godot common game development task. GPT-Codex-5.1-Max placessub_emitterinside theParticleProcessMaterialsub-resource (left, red) instead of on theGPUParticles2Dnode (right, green). The property is belonged toGPUParticles2D.",
                "position": 2831
            }
        ]
    },
    {
        "header": "Appendix GCase Study of Model Failure",
        "images": []
    }
]