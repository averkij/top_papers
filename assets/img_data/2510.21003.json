[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.21003/tex/fig/target.png",
                "caption": "Figure 1:Our goal is to distill a multi-step AR model in to a one-step generator while keeping its distribution.",
                "position": 136
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.21003/x1.png",
                "caption": "Figure 2:Comparison of DD2 models, DD1 models, pre-trained models, and other acceleration methods for pre-trained models. DD2 achieves a significant speedup compared to pre-trained models while outperforming DD1 by a large margin. Other methods fail to achieve one-step sampling. For DD2, DD1, and the pre-trained model, each point corresponds to a different model size, whereas for the skip-last method, each point corresponds to a different number of skipped final steps.",
                "position": 167
            }
        ]
    },
    {
        "header": "2Preliminary",
        "images": []
    },
    {
        "header": "3Distilled Decoding 2",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.21003/tex/fig/generator.png",
                "caption": "(a)Training loss of the generator.",
                "position": 222
            },
            {
                "img": "https://arxiv.org/html/2510.21003/tex/fig/generator.png",
                "caption": "(a)Training loss of the generator.",
                "position": 225
            },
            {
                "img": "https://arxiv.org/html/2510.21003/tex/fig/guidance.png",
                "caption": "(b)Training loss of the guidance network.",
                "position": 230
            },
            {
                "img": "https://arxiv.org/html/2510.21003/tex/fig/ardm.png",
                "caption": "Figure 4:Training loss for initialization.",
                "position": 330
            },
            {
                "img": "https://arxiv.org/html/2510.21003/tex/fig/ardm.png",
                "caption": "Figure 4:Training loss for initialization.",
                "position": 333
            },
            {
                "img": "https://arxiv.org/html/2510.21003/tex/fig/pipeline.png",
                "caption": "Figure 5:Overall pipeline. Performance alignment is an optional technique in the CSD training stage, which is introduced inSec.˜B.4.",
                "position": 338
            }
        ]
    },
    {
        "header": "4Related Works",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Discussions",
        "images": []
    },
    {
        "header": "7Future Works and Limitations",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "NeurIPS Paper Checklist",
        "images": []
    },
    {
        "header": "Appendix ADiscussion ofProp.˜1",
        "images": []
    },
    {
        "header": "Appendix BMore Experimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.21003/tex/fig/fid_plot.png",
                "caption": "Figure 6:Training results for different initialization strategies.defaultindicates that both the generator and the guidance components are initialized using the teacher model[43],random_init_gen_last_layerrefers to the setting where only the last layer of the generator is randomly initialized, while the rest of the generator and the guidance are initialized from the teacher model.random_init_genmeans the entire generator is randomly initialized, whereas the guidance is initialized from the teacher model.random_init_guidenotes the case where only the guidance is randomly initialized, with the generator initialized from the teacher model.",
                "position": 3134
            }
        ]
    },
    {
        "header": "Appendix CImplementation Techniques",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.21003/tex/fig/Arch.png",
                "caption": "Figure 7:Demonstration of the model architectures and the corresponding inputs/outputs.",
                "position": 3426
            }
        ]
    },
    {
        "header": "Appendix DAlgorithms of Multi-step Sampling Method",
        "images": []
    },
    {
        "header": "Appendix EDetailed Experimental Settings",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.21003/fig/dd_d16_add.png",
                "caption": "Figure 8:Generated by DD2-VAR-d16 model.",
                "position": 3729
            },
            {
                "img": "https://arxiv.org/html/2510.21003/fig/dd_d20_add.png",
                "caption": "Figure 9:Generated by DD2-VAR-d20 model.",
                "position": 3732
            },
            {
                "img": "https://arxiv.org/html/2510.21003/fig/dd_d24_add.png",
                "caption": "Figure 10:Generated by DD2-VAR-d24 model.",
                "position": 3735
            },
            {
                "img": "https://arxiv.org/html/2510.21003/fig/dd_llamagen_add.png",
                "caption": "Figure 11:Generated by DD2-LlamaGen-L model.",
                "position": 3738
            }
        ]
    },
    {
        "header": "Appendix FVisualizations",
        "images": []
    }
]