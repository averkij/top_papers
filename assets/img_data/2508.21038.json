[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21038/x1.png",
                "caption": "Figure 1:A depiction of the LIMIT dataset creation process, based on theoretical limitations. We testall combinationsof relevance forNNdocuments (i.e. in the figure, all combinations of relevance for three documents with two relevant documents per query) and instantiate it using a simple mapping.Despite this simplicity, SoTA MTEB models perform poorly, scoring less than 20 recall@100.",
                "position": 623
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Representational Capacity of Vector Embeddings",
        "images": []
    },
    {
        "header": "4Empirical Connection: Best Case Optimization",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21038/x2.png",
                "caption": "Figure 2:The critical-n value where the dimensionality is too small to successfully represent all the top-2 combinations. We plot the trend line as a polynomial function.",
                "position": 895
            }
        ]
    },
    {
        "header": "5Empirical Connection: Real-World Datasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21038/x3.png",
                "caption": "Figure 3:Scores on the LIMIT task. Despite the simplicity of the task we see that SOTA models struggle. We also see that the dimensionality of the model is a limiting factor and that as the dimension increases, so does performance. Even multi-vector models struggle. Lexical models like BM25 do very well due to their higher dimensionality. Stars indicate models trained with MRL.",
                "position": 943
            },
            {
                "img": "https://arxiv.org/html/2508.21038/x4.png",
                "caption": "Figure 4:Scores on the LIMIT small task (N=46) over embedding dimensions. Despite having just 46 documents, model struggle even with recall@10 and cannot solve the task even with recall@20.",
                "position": 946
            },
            {
                "img": "https://arxiv.org/html/2508.21038/x5.png",
                "caption": "Figure 5:Training on LIMIT train does not significantly help, indicating the issue is not domain shift. But models can solve it if they overfit to the test set.",
                "position": 975
            },
            {
                "img": "https://arxiv.org/html/2508.21038/x6.png",
                "caption": "Figure 6:Model results from LIMIT datasets created with different qrel patterns. The dense qrel pattern that uses the maximum number of combinations is significantly harder than the other patterns. Note that the“dense”version is the main LIMIT shown in Figure3.",
                "position": 1009
            },
            {
                "img": "https://arxiv.org/html/2508.21038/x7.png",
                "caption": "Figure 7:No obvious correlation between BEIR vs LIMIT.",
                "position": 1017
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Using the Triangle Inequality to Provide Theoretical Limits",
        "images": []
    },
    {
        "header": "8Relationship to Order-K Voronoi Regions",
        "images": []
    },
    {
        "header": "9Hyperparameter and Compute Details",
        "images": []
    },
    {
        "header": "10Metrics Measuring Qrel Graph Density",
        "images": []
    },
    {
        "header": "11Table Forms of Figures",
        "images": []
    }
]