[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.23377/x23.png",
                "caption": "",
                "position": 24
            },
            {
                "img": "https://arxiv.org/html/2503.23377/x1.png",
                "caption": "",
                "position": 111
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.23377/x2.png",
                "caption": "Figure 1:Given the input text prompt, a JAVG system generates a spatial-temporally synchronized sounding video.\nThe sounds align perfectly with the temporal progression of the actions.",
                "position": 137
            },
            {
                "img": "https://arxiv.org/html/2503.23377/x3.png",
                "caption": "Figure 2:On the left, we present the overall DiT-based sounding video generation architecture of ourJavisDiTSystem, consisting of a video generation branch, audio generation branch, and theMM-BiCrossAttnmodule.\nOn the right, we illustrate the detailed structural design of Spatio-temporal Self-attention (ST Self-Attn), Fine-grained Spatio-temporal Cross-attention (Fine-grained ST-CrossAttn), and Multi-Modality Bidirectional Cross-attention (MM-BiCrossAttn).",
                "position": 153
            },
            {
                "img": "https://arxiv.org/html/2503.23377/x5.png",
                "caption": "",
                "position": 155
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3The ProposedJavisDiTSystem",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.23377/x6.png",
                "caption": "",
                "position": 275
            },
            {
                "img": "https://arxiv.org/html/2503.23377/x7.png",
                "caption": "Figure 3:The framework ofSpatio-temporal Prior Estimatorwith a 4-layer transformer encoder-decoder (referring to the purple region). Contrastive learning is utilized to optimize the estimator.",
                "position": 318
            }
        ]
    },
    {
        "header": "4A ChallengingJavisBenchBenchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.23377/x8.png",
                "caption": "Figure 4:Category distribution of our benchmark.",
                "position": 475
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.23377/x9.png",
                "caption": "Figure 5:Video-audio synchrony with JavisBench’s taxonomy.Current SOTA models still suffer from challenging scenarios",
                "position": 695
            },
            {
                "img": "https://arxiv.org/html/2503.23377/x10.png",
                "caption": "Figure 6:JavisDiT precisely captures the visual and auditory clues from text inputs to generate faithful sounding-videos with high-quality spatio-temporal alignments.\nColored texts are spatial-temporal objects (underlined) and actions.\nMore cases are shown inSec.E.4.",
                "position": 808
            },
            {
                "img": "https://arxiv.org/html/2503.23377/x11.png",
                "caption": "Figure 7:Visualization of cross-attention maps by spatio-temporal priors.Spatial priors successfully capture the sounding subjects (bubbles in this case), and temporal priors accurately cover the whole timeline for the continuous sounding event.",
                "position": 984
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AEthical Statement",
        "images": []
    },
    {
        "header": "Appendix BPotential Limitation and Future Work",
        "images": []
    },
    {
        "header": "Appendix CImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.23377/x12.png",
                "caption": "Figure A1:Stage1: audio pretraining. Parameters are initialized from the video branch.",
                "position": 1985
            },
            {
                "img": "https://arxiv.org/html/2503.23377/x13.png",
                "caption": "Figure A2:Architecture of VA-Fuser to encode spatio-temporal embeddings.",
                "position": 2100
            },
            {
                "img": "https://arxiv.org/html/2503.23377/x14.png",
                "caption": "Figure A3:Video augmentation for spatial and temporal negative samples.",
                "position": 2267
            },
            {
                "img": "https://arxiv.org/html/2503.23377/x15.png",
                "caption": "Figure A4:Audio augmentation for spatial and temporal negative samples.",
                "position": 2270
            }
        ]
    },
    {
        "header": "Appendix DMore Details on JavisBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.23377/x16.png",
                "caption": "Figure A5:Complex Scenario with multi-source sounds at a time.",
                "position": 2840
            },
            {
                "img": "https://arxiv.org/html/2503.23377/x17.png",
                "caption": "Figure A6:Parameter sensitivity evaluation of our JavisScore.Our method presents stable and robust video-audio synchrony estimate at various settings. We finally choose (2-second window size, 1.5-second overlap, topmin-40%) due to the relatively better performance.",
                "position": 2843
            },
            {
                "img": "https://arxiv.org/html/2503.23377/x18.png",
                "caption": "Figure A7:Metric distribution on all JavisBench’s taxonomy.",
                "position": 2870
            }
        ]
    },
    {
        "header": "Appendix EAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.23377/x19.png",
                "caption": "Figure A8:Further ablation on ST-Prior hyper-parameters,including (1) spatial-temporal token ratio, (2) token number, and (3) embedding dimension. Our default setting of “n32x32-d128” is a good trade-off between performance and training cost.",
                "position": 3009
            },
            {
                "img": "https://arxiv.org/html/2503.23377/x20.png",
                "caption": "Figure A9:Extensive JAVG cases on diverseevent scenarios,visual styles,audio types,sounding subjects, andtemporal compositions.Our JavisDiT achieves high-quality and text-consistency for single-modality generation and keeps a good video-audio synchronization.",
                "position": 3101
            },
            {
                "img": "https://arxiv.org/html/2503.23377/x21.png",
                "caption": "Figure A10:Masking strategies for X-conditional generation.The DiT architecture allows feasible conditional video-audio generation by replacing the noisy latent representation with reference videos and/or audios with specific strategies during training and inference.",
                "position": 3148
            },
            {
                "img": "https://arxiv.org/html/2503.23377/x22.png",
                "caption": "Figure A11:Zero-shot X-conditional generation cases. Our model ensures diversified audio-video generation while accurately generating based on various conditions. (1) For A2V/V2A tasks, our model’s generation differs from ground-truth video/audio but maintains spatial consistency and temporal synchronization (similar finger movements and approximate start/end timestamps). (2) For AI2V/I2AV tasks, we nearly recover the ground-truth video in AI2V while keep similar audio results between I2AV and V2A. (3) For AV-Ext, our model produces consistent continues but differs from both GT-AV and I2AV results.",
                "position": 3215
            }
        ]
    },
    {
        "header": "Appendix FExtension to X-conditional Generation",
        "images": []
    }
]