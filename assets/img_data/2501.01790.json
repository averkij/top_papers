[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.01790/extracted/6109222/precase.png",
                "caption": "Figure 1:Examples of multi-ID customized video results from our proposedIngredients.Given a reference with multiple human image set, our method can generate realistic and personalized videos while preserving specific human identity consistent.",
                "position": 109
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.01790/extracted/6109222/framework.png",
                "caption": "Figure 2:Overview ofIngredientsframework.The proposed method consists of three key modules: a facial extractor, a q-former-based projector, and an ID router.\nThe facial extractor collects versatile editable facial features with a decoupling strategy for each ID.\nThe q-former projector map multi-scale facial embedding into different layers of video diffusion transformers.\nThe ID router combines and distributes ID embeddings to their respective locations adaptively without the intervention for prompts and layouts.\nThe entire training process of the framework is curated into two stages, i.e., the facial embedding alignment stage and the router fine-tuning stage.",
                "position": 184
            },
            {
                "img": "https://arxiv.org/html/2501.01790/extracted/6109222/compare.png",
                "caption": "Figure 3:Qualitative comparison of different personalization methods on multi-ID video customization.It can been seen that compared with training-based customization, i.e., textual inversion, our method can clearly routing and attention the respect regions, benefits to ID consistency as well as strong prompt following.",
                "position": 275
            },
            {
                "img": "https://arxiv.org/html/2501.01790/extracted/6109222/badcase.png",
                "caption": "Figure 4:Additional bad examples of multi-human customization.OurIngredientsinvolves failures that generated characters appearing as though they were directly copied-pasted and out-painting, leading to an inconsistent video scenes.",
                "position": 279
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.01790/extracted/6109222/routing.png",
                "caption": "Figure 5:Visualization of routing map within each cross-attention layer of video diffusion transformers.We can see that with the routing loss, the routing network can discern different human IDs at earlier timesetps and in a more pronounced manner.",
                "position": 579
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.01790/extracted/6109222/training.png",
                "caption": "Figure 6:The curve of different training loss in router fine-tuning stage.We can see that with training steps increases, routing loss significantly decreases, the router becomes more accurate, while the diffusion loss remains almost unchanged, maintaining the original generative performance.",
                "position": 1905
            },
            {
                "img": "https://arxiv.org/html/2501.01790/extracted/6109222/sam.png",
                "caption": "Figure 7:Hyper-parameter settings for SAM segmentaion.We select -2.0 as threshold to build routing supervised labels.",
                "position": 1909
            },
            {
                "img": "https://arxiv.org/html/2501.01790/extracted/6109222/evaluate.png",
                "caption": "Figure 8:Domain distribution of evaluation images (left) and used prompt to generate text inputs (right).We consider multiple aspects for data collection to make evaluation more robust.",
                "position": 1913
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]