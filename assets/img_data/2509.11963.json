[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.11963/x1.png",
                "caption": "Figure 1:Performance of ToolRM, top reward models from RewardBench, and leading LLMs-as-judges on FC-RewardBench.Note:We abbreviate the model names for conciseness, for example, L3.1-xx corresponds to Llama-3.1-xx; SW-Rew-xx corresponds to SkyWorks-Reward-xx. Full model names can be found in the Appendix.",
                "position": 140
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.11963/x2.png",
                "caption": "Figure 2:Representative example from the FC-RewardBench dataset, where the parameterplayer_countis set to an incorrect value. The tool catalog is hidden for brevity.",
                "position": 221
            },
            {
                "img": "https://arxiv.org/html/2509.11963/x2.png",
                "caption": "Figure 2:Representative example from the FC-RewardBench dataset, where the parameterplayer_countis set to an incorrect value. The tool catalog is hidden for brevity.",
                "position": 224
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.11963/x3.png",
                "caption": "Figure 3:Correlation heatmap between performance on FC-RewardBench and downstream accuracy across generator models and benchmarks, showing consistently strong alignment (avg. correlation = 0.84).",
                "position": 501
            },
            {
                "img": "https://arxiv.org/html/2509.11963/x4.png",
                "caption": "Figure 4:Performance of the Qwen3 series (top) and xLAM-2 series (bottom) in the Best-of-nn(n=32)(n=32)setting across five function-calling benchmarks: API-Bank-1, API-Bank-2, NexusRaven, ToolAlpaca, and SealTools.",
                "position": 513
            },
            {
                "img": "https://arxiv.org/html/2509.11963/x4.png",
                "caption": "",
                "position": 516
            },
            {
                "img": "https://arxiv.org/html/2509.11963/x5.png",
                "caption": "",
                "position": 521
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.11963/x6.png",
                "caption": "Figure 5:Data samples from ToolRM training data. Each sample has a tool catalog, a conversation between the user and assistant, along with the corresponding correct and incorrect tool calls. The top sample is missing one tool call from the Incorrect version, while the Bottom sample is missing a parameter from the tool call.",
                "position": 1943
            },
            {
                "img": "https://arxiv.org/html/2509.11963/x7.png",
                "caption": "",
                "position": 1947
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]