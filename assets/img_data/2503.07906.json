[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07906/x1.png",
                "caption": "Figure 1:Overview of the proposedDCScorefor evaluating detailed image captioning. (1) Given the image and prompt, model generated responses and human written responses are decomposed into sets of primitive information units. (2) We match the primitive information units of generated responseùí´ùí´\\mathcal{P}caligraphic_Pand those of human written responseùí™ùí™\\mathcal{O}caligraphic_O. (3) Each primitive information unit inùí´ùí´\\mathcal{P}caligraphic_Pis verified individually by VLM given the content of images.",
                "position": 167
            }
        ]
    },
    {
        "header": "3DeCapBench: Image Captioning Testbed for Modern VLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07906/x2.png",
                "caption": "Figure 2:(Left) Comparison of four sources for ground-truth captions in terms of correlation betweenDCScoreand human judgments. All p-values are less than0.0010.0010.0010.001.\n(Right)DeCapBenchachieves the highest correlation with Arena Elo, with a Spearman‚Äôs correlation of 0.90 among different VLM benchmarks.",
                "position": 390
            }
        ]
    },
    {
        "header": "4Learning from Fine-grained Feedback",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07906/x3.png",
                "caption": "Figure 3:Impact of the preference dataset size in terms of downstream performance.",
                "position": 760
            },
            {
                "img": "https://arxiv.org/html/2503.07906/x4.png",
                "caption": "Figure 4:Qualitative results ofFeedQuill-7B compared with LLaVA-Onevision-7B(Li et¬†al.,2024a)in terms of image captioning.",
                "position": 1279
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07906/x5.png",
                "caption": "Figure 5:Qualitative results ofFeedQuill-7B compared with LLaVA-Onevision-7B(Li et¬†al.,2024a)in terms of image captioning.(1)",
                "position": 2371
            },
            {
                "img": "https://arxiv.org/html/2503.07906/x6.png",
                "caption": "Figure 6:Qualitative results ofFeedQuill-7B compared with LLaVA-Onevision-7B(Li et¬†al.,2024a)in terms of image captioning.(2)",
                "position": 2374
            },
            {
                "img": "https://arxiv.org/html/2503.07906/x7.png",
                "caption": "Figure 7:Qualitative results ofFeedQuill-7B compared with GPT4o(OpenAI.,2024a)in terms of image captioning.",
                "position": 2380
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]