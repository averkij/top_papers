[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20355/extracted/6469997/figures/gralora_overview.png",
                "caption": "Figure 1:Illustration of LoRA architecture and GraLoRA architecture. GraLoRA consists ofk2superscriptùëò2k^{2}italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTsmall adapter pairs, where each input and output dimension iskùëòkitalic_ktimes smaller than the original LoRA.",
                "position": 95
            }
        ]
    },
    {
        "header": "2Details and Limitations of LoRA",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20355/extracted/6469997/figures/gradient_update.png",
                "caption": "Figure 2:Gradient dynamics of FFT and LoRA in the presence of an outlier input channel. The red channel in inputXùëãXitalic_Xdenotes the outlier. While FFT localizes the gradient impact, LoRA‚Äôs entire gradient update becomes disproportionately influenced by the single outlier.",
                "position": 171
            },
            {
                "img": "https://arxiv.org/html/2505.20355/extracted/6469997/figures/outlier_and_layerwise_grads.png",
                "caption": "Figure 3:(a) Mean input channel values for the down-projection matrices across layers in LLaMA3.1‚Äì8B. A pronounced outlier exists in Layer 1, channel 198 and 2427. (b) Gradient deviation between LoRA and FFT increases with rank, showing LoRA‚Äôs susceptibility to input outliers. (c) GraLoRA gradient results at rank 128. GraLoRA noticeably reduces gradient deviation between FFT.",
                "position": 191
            },
            {
                "img": "https://arxiv.org/html/2505.20355/extracted/6469997/figures/fft_lora_grad.png",
                "caption": "Figure 4:Gradient distribution in Layer 1 down-projection matrix. LoRA gradients show poor alignment with FFT, outlier channel increases the overall gradient scale, while less emphasizing the corresponding outlier channel.",
                "position": 194
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20355/extracted/6469997/figures/gralora_a_b.png",
                "caption": "Figure 5:Regularized form of GraLoRA as multiplication of sparse two matrices,AGraLoRAsubscriptùê¥GraLoRAA_{\\text{GraLoRA}}italic_A start_POSTSUBSCRIPT GraLoRA end_POSTSUBSCRIPTandBGraLoRAsubscriptùêµGraLoRAB_{\\text{GraLoRA}}italic_B start_POSTSUBSCRIPT GraLoRA end_POSTSUBSCRIPT.",
                "position": 240
            },
            {
                "img": "https://arxiv.org/html/2505.20355/extracted/6469997/figures/gralora_grad.png",
                "caption": "Figure 6:Comparison of gradient distributions under outlier activation. In GraLoRA, only the blocks interacting with the outlier exhibit elevated gradients, mitigating global distortion and aligning with FFT behavior.",
                "position": 278
            },
            {
                "img": "https://arxiv.org/html/2505.20355/extracted/6469997/figures/hybrid_gralora.png",
                "caption": "Figure 7:Hybrid GraLoRA architecture when GraLoRAk=2ùëò2k=2italic_k = 2. LoRA parameter becomes shared across small GraLoRA adapters in the same row or same column.",
                "position": 338
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20355/extracted/6469997/figures/param_sweep.png",
                "caption": "Figure 8:(a) GraLoRAkùëòkitalic_ksweep results and (b) Hybrid GraLoRA Ratio sweep results for LLaMA3.1‚Äì8B on code generation task. Ratio 0 implies default GraLoRA and ratio 1 implies vanilla LoRA in (b).",
                "position": 731
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARank Analysis in Real-World Scenarios",
        "images": []
    },
    {
        "header": "Appendix BGradient Distribution of LoRA and GraLoRA",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20355/extracted/6469997/figures/gralora_grad_full.png",
                "caption": "Figure 9:Comparison of gradient distributions under outlier activation for rank 32, 64, and 128 in LLaMA3.1-8B Layer 1 down-projection matrix.",
                "position": 1185
            }
        ]
    },
    {
        "header": "Appendix CPrecise Analysis on Computation Overhead",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20355/extracted/6469997/figures/computation_workflow.png",
                "caption": "Figure 10:Computation workflow in GraLoRA is composed of 3 steps: two sub-block matrix multiplications and a following matrix addition.",
                "position": 1195
            }
        ]
    },
    {
        "header": "Appendix DBaselines and Hyperparameters",
        "images": []
    }
]