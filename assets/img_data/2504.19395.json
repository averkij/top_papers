[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.19395/x1.png",
                "caption": "Figure 1:An example of\\name, a cryptographic task reformulations framework where a subset of tokens are ciphered (replaced with other tokens in the lexicon) via aBijectivemapping (e.g., each instance of “school” is replaced with “apple”.)\nSince this cipher is a bijection, one can recover the original format of the ICL instance, ensuring the well-defined task upon the transformations.",
                "position": 497
            }
        ]
    },
    {
        "header": "2Defining\\name",
        "images": []
    },
    {
        "header": "3Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.19395/x2.png",
                "caption": "Figure 2:Llama 3.1 8B performance on Amazon dataset.Left:Under theBijectivecipher, accuracy decreases smoothly as the shuffle rate increases, highlighting the difficulty in interpreting the ciphered text.\nAccuracy also increases with more demonstrations, suggesting the model’s ability to solveBijectivecipher.Right:y𝑦yitalic_y-axis shows the accuracy gap betweenBijectiveandNon-Bijectiveciphers.\nFor very high shuffle rates (e.g,>0.7absent0.7>0.7> 0.7), the task become very hard to understand and solve (for the model and even humans) as it becomes ill-defined.",
                "position": 984
            },
            {
                "img": "https://arxiv.org/html/2504.19395/x3.png",
                "caption": "",
                "position": 987
            }
        ]
    },
    {
        "header": "4Empirical Findings",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.19395/x4.png",
                "caption": "Figure 3:Accuracy comparison of Llama-3.1-8B and Llama-3.1-8B-Instruct on four datasets underBijectiveandNon-bijectiveciphers with 20-shot (§4.4).Both aligned and non-aligned models achieve similar relative improvements when solving tasks encoded with aBijectivecipher, compared to those encoded withNon-Bijectiveciphers.",
                "position": 1480
            },
            {
                "img": "https://arxiv.org/html/2504.19395/x5.png",
                "caption": "Figure 4:Accuracy comparison of Llama-3.1-8B and Llama-3.1-70B on four datasets underBijectiveandNon-bijectiveciphers with 20-shot (§4.5).",
                "position": 1504
            },
            {
                "img": "https://arxiv.org/html/2504.19395/extracted/6390817/fig/ori_token_rank_-_sub_token_rank-amazon_bijective_substitution_half.png",
                "caption": "Figure 5:x𝑥xitalic_x-axis indicates thei𝑖iitalic_i-th occurrence of ciphered tokens in the Llama 3.1  context.y𝑦yitalic_y-axis indicates the rank difference\n(Eq1).\nPositive values (red) indicate the model’s preference for substituted tokens over original ones. In theBijectivecipher (left), we see a preference that favors substituted tokens. However, there is no clear preference in theNon-Bijectivecipher (right).",
                "position": 1512
            },
            {
                "img": "https://arxiv.org/html/2504.19395/extracted/6390817/fig/ori_token_rank_-_sub_token_rank-amazon_random_substitution_half.png",
                "caption": "",
                "position": 1515
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Discussion and Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Experimental Details",
        "images": []
    },
    {
        "header": "Appendix BExample Inputs/Outputs",
        "images": []
    },
    {
        "header": "Appendix CAdditional Related Work",
        "images": []
    },
    {
        "header": "Appendix DPriority vs. Non-Priority Sampling",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.19395/x6.png",
                "caption": "Figure 6:Llama 3.1 8B performance on SST-2 dataset, which shows similar trends withFigure 2.Left:Under theBijectivecipher, accuracy decreases smoothly as the shuffle rate increases, highlighting the difficulty in interpreting the ciphered text.\nAccuracy also increases with more demonstrations, suggesting the model’s ability to solveBijectivecipher.Right:y𝑦yitalic_y-axis shows the accuracy gap betweenBijectiveandNon-Bijectiveciphers.\nFor very high shuffle rates (e.g,>0.7absent0.7>0.7> 0.7), the task become very hard to understand and solve (for the model and even humans) as it becomes ill-defined.",
                "position": 2846
            },
            {
                "img": "https://arxiv.org/html/2504.19395/x7.png",
                "caption": "",
                "position": 2849
            },
            {
                "img": "https://arxiv.org/html/2504.19395/x8.png",
                "caption": "Figure 7:Peformance of Llama 3.1 8B on SST-2 dataset with non-priority sampling, comparing withFigure 6.Left:The accuracies underBijectivecipher.Right:The y-axis\ndisplays the accuracy gap betweenBijectiveandNon-Bijectiveciphers.",
                "position": 2860
            },
            {
                "img": "https://arxiv.org/html/2504.19395/x9.png",
                "caption": "",
                "position": 2863
            },
            {
                "img": "https://arxiv.org/html/2504.19395/x10.png",
                "caption": "Figure 8:Peformance of Llama 3.1 8B on Amazon dataset with non-priority sampling, comparing withFigure 2.Left:The accuracies underBijectivecipher.Right:The y-axis\ndisplays the accuracy gap betweenBijectiveandNon-Bijectiveciphers.",
                "position": 2872
            },
            {
                "img": "https://arxiv.org/html/2504.19395/x11.png",
                "caption": "",
                "position": 2875
            }
        ]
    },
    {
        "header": "Appendix EPretrained-only vs. Aligned Models",
        "images": []
    },
    {
        "header": "Appendix FSmall vs. Large Model",
        "images": []
    },
    {
        "header": "Appendix GSignificance of Results",
        "images": []
    },
    {
        "header": "Appendix HRestricting the Space of Cipher",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.19395/extracted/6390817/fig/ori_token_rank_-_sub_token_rank-amazon_bijective_substitution.png",
                "caption": "Figure 9:Complete heatmap of original token rank minus substitution token rank on Amazon forFigure 5.Left:bijectivecipherRight:non-bijectivecipher",
                "position": 4615
            },
            {
                "img": "https://arxiv.org/html/2504.19395/extracted/6390817/fig/ori_token_rank_-_sub_token_rank-amazon_random_substitution.png",
                "caption": "",
                "position": 4618
            },
            {
                "img": "https://arxiv.org/html/2504.19395/x12.png",
                "caption": "Figure 10:Average rank differences (original token rank - substitution token rank) in SST-2 (left) and Amazon (right) datasets forBijective(blue) andNon-bijective(red) cipher over 15 occurrences, divided into 5 chunks of size 3. Rank difference serves as a proxy for the model’s deciphering ability. UnderBijectivecipher, this ability improves with more exposure to substituted tokens, whileNon-Bijectivecipher shows no clear pattern.",
                "position": 4623
            },
            {
                "img": "https://arxiv.org/html/2504.19395/x13.png",
                "caption": "",
                "position": 4626
            }
        ]
    },
    {
        "header": "Appendix IFurther Results on Probing Analysis",
        "images": []
    }
]