[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08307/extracted/6270742/imgs/FLAV.png",
                "caption": "Figure 1:An overview of ourRFLAV model architecture.",
                "position": 154
            },
            {
                "img": "https://arxiv.org/html/2503.08307/x1.png",
                "caption": "Figure 2:Temporal alignment between video frames and mel spectrogram segments. Each video frame corresponds to a fixed-size section (F/T) of the mel spectrogram, allowing for a 1:1 mapping.",
                "position": 169
            },
            {
                "img": "https://arxiv.org/html/2503.08307/x2.png",
                "caption": "(a)",
                "position": 172
            },
            {
                "img": "https://arxiv.org/html/2503.08307/x2.png",
                "caption": "(a)",
                "position": 175
            },
            {
                "img": "https://arxiv.org/html/2503.08307/x3.png",
                "caption": "(b)",
                "position": 180
            },
            {
                "img": "https://arxiv.org/html/2503.08307/x4.png",
                "caption": "(c)",
                "position": 185
            },
            {
                "img": "https://arxiv.org/html/2503.08307/extracted/6270742/imgs/rolling.png",
                "caption": "(a)",
                "position": 271
            },
            {
                "img": "https://arxiv.org/html/2503.08307/extracted/6270742/imgs/rolling.png",
                "caption": "(a)",
                "position": 274
            },
            {
                "img": "https://arxiv.org/html/2503.08307/extracted/6270742/imgs/prerolling.png",
                "caption": "(b)",
                "position": 279
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08307/extracted/6270742/long_video_metrics_2.png",
                "caption": "Figure 5:AV metrics and feature drift calculated on long (i.e., 240 frames) generated videos using a sliding window of 16 frames.",
                "position": 717
            },
            {
                "img": "https://arxiv.org/html/2503.08307/extracted/6270742/imgs/long-videos.png",
                "caption": "Figure 6:Long videos generated by our model on both AIST++ and landscape. Frames are sampled every 1.6s for visualization.",
                "position": 720
            },
            {
                "img": "https://arxiv.org/html/2503.08307/extracted/6270742/imgs/comparison.png",
                "caption": "Figure 7:Frame similarity matrix of real vs.Â generated videos from the AIST++ dataset, illustrating the presence of loops in real videos and their absence in generated content.",
                "position": 723
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]