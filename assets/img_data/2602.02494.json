[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02494/x1.png",
                "caption": "",
                "position": 97
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02494/x2.png",
                "caption": "Figure 2:Overview of the MEG-XL pre-training framework.(Left) A frozen BioCodec tokenizer independently encodes each MEG channel into discrete tokens across Q=6 residual quantization levels, providing prediction targets for self-supervised learning. (Middle) Token embeddings, which are concatenated across quantization levels and projected, are combined with sensor position, orientation, and type embeddings, then processed by a criss-cross transformer. A projection head maps transformer embeddings back to Q residual tokens. (Right) In pre-training, we randomly mask contiguous 3-second blocks uniformly across all sensors, forcing the model to predict masked tokens from temporal context rather than interpolating across channels, until 40% of tokens are masked.",
                "position": 161
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02494/x3.png",
                "caption": "Figure 3:Pre-training enables generalisation with less subject data.We compare MEG-XL to the state-of-the-art supervised method(d’Ascoli et al.,2025)and a baseline trained from scratch (MEG-XL with random init.) across varying amounts of fine-tuning data. MEG-XL consistently outperforms its randomly initialised counterpart, confirming that the gains stem from learned priors rather than architecture alone. On Armeni and MEG-MASC, where per-subject data is shallower, MEG-XL outperformsd’Ascoli et al. (2025)throughout most of the data range. On LibriBrain, a deep single-subject dataset, both methods perform similarly until approximately 2.5 hours of training data, after which their supervised method pulls ahead. This suggests pre-training may substitute for subject-specific data as when recordings are scarce, learned priors help. In rare cases where recordings are abundant, learning from scratch eventually wins.",
                "position": 375
            },
            {
                "img": "https://arxiv.org/html/2602.02494/x4.png",
                "caption": "Figure 4:Linear probing shows that models pre-trained with more context generalise better to word decoding.We pre-train models with increasing context, fixed masking percentage, and constant optimisation steps, then evaluate the strength of their representations with linear probes (frozen backbone). We compare two conditions:full context, where all models see 150s of input to isolate representation quality, andmatched context, where the input is restricted to the pre-training length. The lack of divergence between the lines suggests models cannot leverage inference context that exceeds their pre-training context. We train the linear probes with 7% of the training data. We could not expand further than 150s due to GPU memory limits. Token-matched pre-training shows similar trends (AppendixD).",
                "position": 448
            },
            {
                "img": "https://arxiv.org/html/2602.02494/x5.png",
                "caption": "Figure 5:(Top) Extending neural context improves zero-shot prediction of brain activity from unseen datasets and subjects.We mask the central 3s subsegment of samples from unseen datasets and measure improvement in token prediction accuracy (relative to chance) of models pre-trained on increasing neural context. Scaling improves masked prediction, with the trend remaining through 150s. Only GPU VRAM limits prevent increasing it further. Chance accuracy is1/2561/256.(Bottom) Long-context pretraining induces selective and hierarchical attention.(Left) Models pretrained on longer context attend locally in early layers before expanding to integrate distant context; short-context models attend diffusely throughout. (Right) Attention entropy decreases with context length, indicating more selective attention patterns. We provide 150s context at inference. See AppendixFfor attention distance and entropy calculations.",
                "position": 461
            }
        ]
    },
    {
        "header": "4Discussion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails on Experimental Setup",
        "images": []
    },
    {
        "header": "Appendix BLarger Retrieval Set Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02494/x6.png",
                "caption": "Figure 6:Generalisation Across Training Data Regimes (Top 250 Words as Retrieval Set).Results in the main paper use top-50 word retrieval sets. As expected, trends with top-250 word retrieval remain the same with the larger vocabulary leading to degraded performance across all methods. See the caption in Figure3for details.",
                "position": 1475
            }
        ]
    },
    {
        "header": "Appendix CNyquist-Compliant Resampling",
        "images": []
    },
    {
        "header": "Appendix DToken-Matched Neural Context Scaling",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02494/x7.png",
                "caption": "Figure 7:(Top) Linear probing for word decoding with token-matched pre-training.We pre-train models exactly the same way as described in the caption in Figure4, except with fixed training tokens so that each pre-trained model sees exactly the same training data (token-matched). Here, we see similar trends with larger contexts systematically improving downstream performance. Diminishing returns, and potentially regression in performance, appear after 150s of pre-training context. The results are noisier as a consequence of three pre-training seeds (and linear probes) per context length instead of five due to computational constraints.(Bottom) Zero-shot prediction of masked brain activity with token-matched pre-training.The results here are collected the same way as described in the caption of Figure5(top), except with fixed training data for each pre-training run (token-matched). The trends remain very similar.",
                "position": 1629
            }
        ]
    },
    {
        "header": "Appendix ETokenizer Comparison",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02494/x8.png",
                "caption": "Figure 8:BioCodec vs BrainTokenizer (originating from BrainOmni). BioCodec reconstructs signals with lower reconstruction error (MSE of 0.41 vs 0.69). The plot shows a preprocessed 5-second sample taken from the MOUS dataset at 50Hz across three channels.",
                "position": 1639
            }
        ]
    },
    {
        "header": "Appendix FAnalysing Temporal Attention Heads",
        "images": []
    },
    {
        "header": "Appendix GWhy Non-Invasive Decoding?",
        "images": []
    }
]