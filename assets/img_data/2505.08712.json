[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.08712/x1.png",
                "caption": "Figure 1:NavDP is solely trained with simulation trajectories but can achieve zero-shot sim-to-real transfer to different types of robots. By learning from the prioritized knowledge in the simulation data, NavDP adaptively selects a safe navigation routes towards the goal without any maps.",
                "position": 79
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Data Generation",
        "images": []
    },
    {
        "header": "4Navigation Diffusion Policy",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.08712/x2.png",
                "caption": "Figure 2:NavDP processes a single RGB-D observation frame along with a navigation goal. The inputs are tokenized and processed through a unified transformer architecture to generate navigation trajectories or evaluate corresponding trajectory values. A safe trajectory is then selected based on these values for execution by the robot.",
                "position": 204
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.08712/x3.png",
                "caption": "Figure 3:Trajectory visualization of on different robots. We project the predicted trajectories back to the image space and colorize them according to the corresponding critic values. Thebluertrajectories indicate higher risk, whereas thereddertrajectories represent safer paths.",
                "position": 534
            },
            {
                "img": "https://arxiv.org/html/2505.08712/x4.png",
                "caption": "Figure 4:Ablation results for the NavDP. The left figure illustrate the entire NavDP network can benefit from critic function from test-time selection and training objectives. The middle illustrate the influence of using different tasks for training. The right illustrates the policy performance on both real-to-sim scenes and real-world scenes with respect to different data proportion.",
                "position": 537
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.08712/x5.png",
                "caption": "Figure 5:Examples of our simulation navigation dataset. Our dataset generation pipeline supports texture randomization, view randomization, light randomization and provide photorealistic rendering with BlenderProc.",
                "position": 1241
            },
            {
                "img": "https://arxiv.org/html/2505.08712/x6.png",
                "caption": "Figure 6:Visualization of the navigation evaluation benchmark. Simulation evaluation, Real-to-Sim evaluation as well as Real-world evaluation are conducted in this work.",
                "position": 1252
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]