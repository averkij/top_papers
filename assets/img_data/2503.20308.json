[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20308/x1.png",
                "caption": "",
                "position": 164
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Essential Criteria for Perceptually Accurate 3D Talking Head",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20308/x2.png",
                "caption": "Table 1:Human studies on alignment criteria.[Left] Preference scores (1-3) for 3D talking heads with varying lip movement intensities paired with different speech intensities. [Right] Human preference between (A) samples with precise timing but low expressiveness, and (B) samples with high expressiveness but 100ms asynchrony—twice the commonly accepted 50ms threshold[43].",
                "position": 324
            },
            {
                "img": "https://arxiv.org/html/2503.20308/x3.png",
                "caption": "Figure 2:Pipeline of speech-mesh synchronized representation learning.We train our speech-mesh representation space in a two-stage manner. In the first stage, we learn a rich audio-visual\nrepresentation in 2D domain to capture the synchronization between lip movement and speech.\nIn the second stage, we train the 3D mesh encoder to align the 3D mesh space with the frozen speech space. As an application of our speech-mesh representation space, we propose a plug-in perceptual loss to 3D talking head models to enhance the quality of lip movements.",
                "position": 391
            }
        ]
    },
    {
        "header": "4Speech-Mesh Synchronized Representation",
        "images": []
    },
    {
        "header": "5Evaluation Metrics",
        "images": []
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20308/x4.png",
                "caption": "Figure 3:Qualitative results of the effectiveness of our perceptual loss for lip readability.Our perceptual loss guides baselines[11,46,21]to generate perceptually accurate lip movements.",
                "position": 729
            },
            {
                "img": "https://arxiv.org/html/2503.20308/x5.png",
                "caption": "Figure 4:t-SNE plot of ablation study.We plot the t-SNE graph for each perceptual critic model.\nWe represent the features with same phoneme as same color.\nSquared and circled points denote mesh and speech features from each representation, respectively.",
                "position": 896
            },
            {
                "img": "https://arxiv.org/html/2503.20308/x6.png",
                "caption": "Figure 5:Behaviors of our representation in temporal and expressiveness sensitivity.We demonstrate the effectiveness of our representation in temporal synchronization and expressiveness using a cosine similarity graph and speech feature plots, respectively. We color the point aslow,medium, andhighintensity.",
                "position": 903
            },
            {
                "img": "https://arxiv.org/html/2503.20308/x7.png",
                "caption": "Figure 6:Qualitative results for the expressiveness.Given high and low intensity levels of speech, models trained on both MEAD-3D and VOCASET show more expressive lip movements compared to those trained on VOCASET alone, and even better with our perceptual loss.",
                "position": 908
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Contents",
        "images": []
    },
    {
        "header": "Appendix ASupplementary Video",
        "images": []
    },
    {
        "header": "Appendix BEmergent Properties of 2D Speech Representation",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20308/x8.png",
                "caption": "Figure S1:Emergent properties of 2D speech representation.We visualize a cosine similarity versus temporal offset graph and a t-SNE visualization of the 2D audio-visual speech representation.\nThe 2D speech representation already possesses desired properties we pursue, which motivates us to transfer the emergent properties to the speech-mesh representation space.",
                "position": 1851
            }
        ]
    },
    {
        "header": "Appendix CSpeech-Mesh Synchronized Representation",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20308/x9.png",
                "caption": "Figure S2:Speech and lip intensity distributions across datasets.We present speech and lip intensity distributions and corresponding standard deviation values across datasets.",
                "position": 2090
            }
        ]
    },
    {
        "header": "Appendix DDetails for Human Study on Lip Synchronization Criteria",
        "images": []
    },
    {
        "header": "Appendix EEvaluation Metrics",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20308/x10.png",
                "caption": "Figure S3:Central vertices of the lower and upper lips.We select two specific vertices that correspond to the center of the upper and lower lips to extract the lip vertex displacement sequences.",
                "position": 2167
            },
            {
                "img": "https://arxiv.org/html/2503.20308/extracted/6313747/source/Figure_DDTW2.png",
                "caption": "Figure S4:An example of DDTW matching results between ground truth and predicted lip distance sequences.We present an example of the DDTW local extrema correspondences\nof the ground truth and predicted lip vertex displacement sequences.\nWe represent matched local extrema using green lines.",
                "position": 2172
            },
            {
                "img": "https://arxiv.org/html/2503.20308/x11.png",
                "caption": "Figure S5:Physical accuracy of Mean Temporal Misalignment.We introduce temporal mismatch to the ground truth mesh sequences of VOCASET[8]by shifting the speech to lead the mesh sequences by 0 to 10 frames (where 0 represents no mismatch).\nFor each temporal offset, we calculate the average MTM and plot a graph showing the relationship between the temporal offset and the corresponding MTM values.",
                "position": 2179
            }
        ]
    },
    {
        "header": "Appendix FImplementation Details of Ablation Study",
        "images": []
    },
    {
        "header": "Appendix GAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20308/x12.png",
                "caption": "Figure S6:Perceptual loss stability.We visualize the perceptual loss between GT speech-mesh pairs on VOCASET samples. Our representation demonstrates strong generalization capability and provides a stable training signal compared to 3D SyncNet and our representation without 2D prior.",
                "position": 2520
            },
            {
                "img": "https://arxiv.org/html/2503.20308/x13.png",
                "caption": "Figure S7:Cosine similarity stability.We visualize the cosine similarity between GT speech-mesh pairs on VOCASET samples. Our representation demonstrates strong generalization capability compared to 3D SyncNet and our representation without 2D prior.",
                "position": 2524
            },
            {
                "img": "https://arxiv.org/html/2503.20308/x14.png",
                "caption": "Figure S8:Qualitative results of temporal synchronization on existing models.We plot y-t slices of rendered 3D face mesh sequences on the lip region with corresponding speech waveforms and mel-spectrogram.\nWe also indicate the time steps of lip closure and opening with vertical lines.\nThis implies that existing models already exhibit reasonable temporal sync. capability.",
                "position": 2587
            }
        ]
    },
    {
        "header": "Appendix HDiscussion",
        "images": []
    }
]