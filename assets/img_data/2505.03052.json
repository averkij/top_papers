[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.03052/x1.png",
                "caption": "Figure 1:Overview of SLUNG on Transformers. High-risk tokens are marked in red (Masked or Unlikelihood Loss) while low-risk tokens are marked in green (standard next-token prediction). Black arrows represent flow of information during model forward pass computation. Green arrows represent a possible backpropogation gradient flow. When the loss on red tokens are masked, notice thath<p⁢r⁢e⁢t⁢e⁢n⁢t⁢i⁢o⁢u⁢s>subscriptℎexpectation𝑝𝑟𝑒𝑡𝑒𝑛𝑡𝑖𝑜𝑢𝑠h_{<pretentious>}italic_h start_POSTSUBSCRIPT < italic_p italic_r italic_e italic_t italic_e italic_n italic_t italic_i italic_o italic_u italic_s > end_POSTSUBSCRIPT—the hidden state for a high-risk token—is only ever optimized by the attention mechanism to help generate low-risk tokens.",
                "position": 95
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Selective Loss to Understand but Not Generate (SLUNG)",
        "images": []
    },
    {
        "header": "4Understanding Toxicity without Generating It",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.03052/x2.png",
                "caption": "Figure 2:(a) Toxicity Generation vs Understanding tradeoff for Pretrained models. (b) Toxicity Generation vs Understanding tradeoff for Instruction-tuned Models. Error bars represent 95% confidence intervals. SLUNG methods (★) set a new Pareto frontier in both cases.",
                "position": 283
            },
            {
                "img": "https://arxiv.org/html/2505.03052/x3.png",
                "caption": "Figure 3:Effect of toxic data quantity on model understanding and generation of toxicity. Models in the upper left region exhibit the best understanding-generation tradeoff. Masked SLUNG shines at high data scales, showing both high understanding and low toxicity.",
                "position": 376
            }
        ]
    },
    {
        "header": "5Learning Entity Names without Generating Them",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AEvaluation Prompts for GPT-4o",
        "images": []
    },
    {
        "header": "Appendix BSample Outputs for TOFU",
        "images": []
    }
]