[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12276/x1.png",
                "caption": "Figure 1:Our proposed method, Spatial Forcing (SF), implicitly forces VLA models to acquire spatial-aware knowledge.(a)SF aligns intermediate visual embeddings of VLAs with geometric representations from pretrained 3D foundation models.(b)Our simple yet effective strategy yields significant improvements in training efficiency and test accuracy.(c)Depth probing proves that our SF brings spatial information into the aligned representations, further enhancing 3D perception.",
                "position": 115
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12276/x2.png",
                "caption": "Figure 2:Comparison among different paradigms for 3D VLAs.",
                "position": 226
            },
            {
                "img": "https://arxiv.org/html/2510.12276/x3.png",
                "caption": "Figure 3:Depth probingof the visual embeddings of VLAs. Embeddings learned solely from 2D images without alignment do not produce meaningful spatial structures. The aligned embeddings inherently contain rich spatial information, leading to better performance in depth probing.",
                "position": 244
            },
            {
                "img": "https://arxiv.org/html/2510.12276/x4.png",
                "caption": "Table 1:Comparisons with state-of-the-art methodson LIBERO benchmark. Please note thatmethods in gray fontincorporate extra depth or point cloud information from other sensors.Bolddenotes the best performances among the methods without extra sensor inputs.",
                "position": 288
            }
        ]
    },
    {
        "header": "3Simulation Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12276/x4.png",
                "caption": "Figure 4:Comparisons with state-of-the-art methodson RoboTwin 2.0 benchmark.",
                "position": 515
            },
            {
                "img": "https://arxiv.org/html/2510.12276/x5.png",
                "caption": "Figure 5:(a)We report the success rates vs. training iterations before and after representation alignment.(b)We report the success rate vs. training data before and after representation alignment.(c)The aligned representation exhibits almost the same distribution shape as the target.",
                "position": 891
            }
        ]
    },
    {
        "header": "4Real-world Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12276/x6.png",
                "caption": "Figure 6:Real-world Experiments.(a)A set of single-arm tasks across various visual and spatial conditions. For each task, we train a unified model to face all variations and report the success rate.(b)Dual-arm tasks to measure the spatial horizontal balance ability.(c)Top-view robot setup.",
                "position": 928
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AWeight Factor",
        "images": []
    },
    {
        "header": "Appendix BExplanations of t-SNE results",
        "images": []
    },
    {
        "header": "Appendix CReal-world Data Collection",
        "images": []
    },
    {
        "header": "Appendix DDetails of Compared Models",
        "images": []
    }
]