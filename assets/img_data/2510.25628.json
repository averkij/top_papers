[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1INTRODUCTION",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.25628/x1.png",
                "caption": "Figure 1:Overview of the EHR tasks and the proposed method.a. EHR Analysis Tasks.EHR analysis tasks are defined as consisting of two types of tasks: decision-making and risk-prediction.b. Methods Overview.Our approach addresses these challenges with a three-stage training pipeline. First, a large volume of non-reasoning data is used for continual pre-training. This is followed by an instruction-tuning phase that leverages reasoning data. Finally, reinforcement learning with Group Reward Policy Optimization (GRPO) is applied to further refine the model.c. Results.This figure compares the performance of our model against several baseline LLMs on both decision-making and risk-prediction tasks, showcasing its superior performance.",
                "position": 306
            }
        ]
    },
    {
        "header": "2Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.25628/x2.png",
                "caption": "Figure 2:Overview of reasoning data in EHR-Ins.aThe sample size of each task in the reasoning data of EHR-Ins.bExample of human evaluation on the reasoning data.cManual evaluation results on EHR reasoning data across eight decision-making tasks, each associated with a distinct type of decision-making event. We compared the quality of synthetic reasoning data with and without thinking-graph enhancement, where ‘***’ represents a significance level ofp<0.001p<0.001.",
                "position": 340
            },
            {
                "img": "https://arxiv.org/html/2510.25628/x3.png",
                "caption": "Figure 3:Overview of EHR-Ins and EHR-Bench.. The hierarchical ring chart displays the distribution of both datasets. The inner ring partitions tasks into two types: risk prediction and decision making. The middle ring shows 12 task categories (subtypes). The outer ring details all 42 specific tasks.",
                "position": 356
            },
            {
                "img": "https://arxiv.org/html/2510.25628/x4.png",
                "caption": "Figure 4:Performance comparison of EHR-R1 and nine baseline LLMs across 24 decision-making tasks on EHR-Bench.The performance is measured with F1 score.\nCross-hatched bars denote reasoning-enhanced models, highlighting the effect of explicit reasoning.\nIn each subplot, our EHR-R1 (rightmost bar) achieves a clear performance advantage on nearly all tasks.",
                "position": 490
            },
            {
                "img": "https://arxiv.org/html/2510.25628/x5.png",
                "caption": "Figure 5:Performance comparison of EHR-R1 and seven baseline LLMs on 18 risk-prediction tasks on EHR-Bench.Each subplot shows the ROC curves per task, with EHR-R1 highlighted in orange; the bottom-right corner of each plot reports EHR-R1-72B’s AUROC. The final ‘Total’ subplot summarizes aggregated performance across all 18 tasks.",
                "position": 526
            },
            {
                "img": "https://arxiv.org/html/2510.25628/x6.png",
                "caption": "Figure 6:Performance comparison of EHR-R1 and baseline LLMs in generalization evaluation.The score metric for MIMIC-IV-CDM and EHRSHOT is F1 and AUROC score, respectively.aZero-shot on MIMIC-IV-CDM:\nperformance on Main Disease and ICD-level diagnoses using the same samples.bZero-shot on EHRSHOT for 70B-parameters LLMs: aggregated AUROC across all subtasks within each of the three category groups, plus the overall average across all tasks.cFew-shot on EHRSHOT for small-scale language models (Qwen3-1.7B vs. EHR-R1-1.7B): per-task performance acrossk∈{1,2,4,8,16,32,64,128}k\\in\\{1,2,4,8,16,32,64,128\\}shots.",
                "position": 563
            },
            {
                "img": "https://arxiv.org/html/2510.25628/x7.png",
                "caption": "Figure 7:Ablation experiments results on decision making tasks of EHR-Bench.The figure show 7 categories of sub-type decision making tasks and the average performance of 5 variant of our methods, including Base Model, Base Model (with reasoning inference), EHR-R1 (w/o reasoning training), EHR-R1 (full training), and our final model EHR-R1 (full training, w/ reasoning inference), which showcase the incremental performance gains from each stage of our training pipeline.",
                "position": 602
            }
        ]
    },
    {
        "header": "3Discussion",
        "images": []
    },
    {
        "header": "4Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.25628/x8.png",
                "caption": "Figure 8:Overview of Data curation pipeline.The pipeline for our data curation process begins with the original MIMIC-IV dataset. From there, we establish EHR-Ins with reasoning enhanced EHR analysis instructions through thinking-graph–driven reasoning synthesis, along with a new comprehensive MIMIC-Bench.",
                "position": 748
            },
            {
                "img": "https://arxiv.org/html/2510.25628/x9.png",
                "caption": "Figure 9:Overview of Thinking-Graph-Driven Reasoning Synthesis.The whole pipeline comprises three parts:a Entity Relevance Identificationanalyzes electronic health records to calculate a co-exist matrix and Lift Score for medical entities.b Graph-based Medical Relation Searchfilters the entity pair with a higher lift score and conducts a bidirectional graph search on a large-scale medical knowledge graph, UMLS; The red lines indicate the search pathsc Reasoning Path Synthesisarguments GPT-4o with retrieved medical relations to generate a coherent reasoning path. The entity with the color box in the reasoning chain indicates the context entity or the target labels.",
                "position": 846
            }
        ]
    },
    {
        "header": "5Data Availability",
        "images": []
    },
    {
        "header": "6Code Availability",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Acknowledgments",
        "images": []
    },
    {
        "header": "8Author Contributions",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.25628/x10.png",
                "caption": "Supplementary Table 4:Instruction for each task in EHR-Bench.",
                "position": 3014
            },
            {
                "img": "https://arxiv.org/html/2510.25628/x11.png",
                "caption": "Supplementary Table 5:Instruction for each task in MIMIC-IV-CDM and EHRSHOT.",
                "position": 3017
            },
            {
                "img": "https://arxiv.org/html/2510.25628/x12.png",
                "caption": "Supplementary Figure 1:A case study of EHR Trajectory, Medical Relation, and Reasoning Chain. (a) EHR Trajectory for a patient, where <events>… and <item info>… represent the omission of a large amount of information for display purposes. (b) Medical Relation, showing the connections between the context medical entities and target items. (c) Reasoning Chain, detailing the process of inferring a diagnosis from the patient’s EHR data. The parts highlighted in bold are the content commonly found in the EHR Trajectory, Medical Relation, and Reasoning Chain. This indicates that the medical graph is effective in identifying valid medical entities from the trajectory and using them to enhance reasoning.",
                "position": 3053
            }
        ]
    },
    {
        "header": "9Supplementary",
        "images": []
    }
]