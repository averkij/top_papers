[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19848/x1.png",
                "caption": "Figure 1:Comparison between the captionsgenerated by our ScaleCap and those produced by other advanced VLMs. The parts of the caption that are bolded refer to the detailed descriptions of the object, while the parts that do not mention the target object are included in the ellipsis.",
                "position": 95
            },
            {
                "img": "https://arxiv.org/html/2506.19848/x2.png",
                "caption": "Figure 2:The reason for certain object detail omissions in LVLM captions is mainly due to the absence of guiding heuristic questions rather than insufficient perceptual capability.\nWe also observe that 7B and 72B LVLMs exhibit similar perceptual capabilities.",
                "position": 118
            }
        ]
    },
    {
        "header": "2ScaleCap",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19848/x3.png",
                "caption": "Figure 3:Overview of ScaleCap.ScaleCap is composed of two synergistic parts: heuristic question answering and contrastive sentence rating. The first module utilizes a general-purpose LLM to create guiding questions, and the second module addresses hallucinations by offline contrastive strategy.",
                "position": 137
            },
            {
                "img": "https://arxiv.org/html/2506.19848/x4.png",
                "caption": "Figure 4:Data processing and analysis.During the image collecting and processing stage, we primarily focus on the diversity and richness of image content. In the resulting ScaleCap-450k, the captions are significantly longer than those in other datasets.",
                "position": 242
            }
        ]
    },
    {
        "header": "3Pretraining Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19848/x5.png",
                "caption": "Figure 5:The benchmark performance under different number of pretraining data.",
                "position": 692
            }
        ]
    },
    {
        "header": "4Dive into ScaleCap",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19848/x6.png",
                "caption": "Figure 6:Human evaluation of image similarity with original image over 50 samples and 25 volunteers. Volunteers rank the images based on their similarity to the original image.",
                "position": 958
            },
            {
                "img": "https://arxiv.org/html/2506.19848/x7.png",
                "caption": "(a)",
                "position": 970
            },
            {
                "img": "https://arxiv.org/html/2506.19848/x7.png",
                "caption": "(a)",
                "position": 973
            },
            {
                "img": "https://arxiv.org/html/2506.19848/x8.png",
                "caption": "(b)",
                "position": 978
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Limitation",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APrompts Used in ScaleCap",
        "images": []
    },
    {
        "header": "Appendix BDataset processing",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19848/x9.png",
                "caption": "Figure 8:Prompts used in ScaleCap and Prism.",
                "position": 2077
            },
            {
                "img": "https://arxiv.org/html/2506.19848/x10.png",
                "caption": "Figure 9:PromptTi‚Å¢c‚Å¢tsubscriptùëáùëñùëêùë°T_{ict}italic_T start_POSTSUBSCRIPT italic_i italic_c italic_t end_POSTSUBSCRIPTused in ScaleCap to generate object instructions.",
                "position": 2082
            }
        ]
    },
    {
        "header": "Appendix CPretraining Details",
        "images": []
    },
    {
        "header": "Appendix DPotencial Societal Impacts",
        "images": []
    },
    {
        "header": "Appendix EUser Study Instructions",
        "images": []
    },
    {
        "header": "NeurIPS Paper Checklist",
        "images": []
    }
]