[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01956/x1.png",
                "caption": "",
                "position": 85
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01956/x2.png",
                "caption": "Figure 2:Pipeline of VideoScene.Given input pair views, we first generate a coarse 3D representation with a rapid feed-forward 3DGS model (i.e., MVSplat[17]), which enables accurate camera-trajectory-control rendering. The encoded rendering latent (‚Äúinput‚Äù) and encoded input pairs latent (‚Äúcondition‚Äù) are combined as input to the consistency model. Subsequently, a forward diffusion operation is performed to add noise to the video. Then, the noisedùê±n+1rsuperscriptsubscriptùê±ùëõ1ùëü\\mathbf{x}_{n+1}^{r}bold_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPTis sent to both the student and teacher model to predict videosùê±0p‚Å¢r‚Å¢e‚Å¢dsuperscriptsubscriptùê±0ùëùùëüùëíùëë\\mathbf{x}_{0}^{pred}bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p italic_r italic_e italic_d end_POSTSUPERSCRIPTof timesteptn+1subscriptùë°ùëõ1t_{n+1}italic_t start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPTandùê±^0œïsuperscriptsubscript^ùê±0italic-œï\\hat{\\mathbf{x}}_{0}^{\\phi}over^ start_ARG bold_x end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_œï end_POSTSUPERSCRIPTof timesteptnsubscriptùë°ùëõt_{n}italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT. Finally, the student model and DDPNet are updated independently through distillation loss and DDP loss.",
                "position": 153
            },
            {
                "img": "https://arxiv.org/html/2504.01956/x3.png",
                "caption": "Figure 3:Qualitative comparison.We can observe that baseline models suffer from issues such as blurriness, frame skipping, excessive motion, and shifts in the relative positioning of objects, while our VideoScene achieves higher output quality and improved 3D coherence.",
                "position": 303
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01956/x4.png",
                "caption": "Figure 4:Qualitative results in cross-dataset generalization.Models trained on the source dataset RealEstate10K are tested on ACID dataset. Fine-tuned models improve in 3D consistency but still fail with one-step.",
                "position": 552
            },
            {
                "img": "https://arxiv.org/html/2504.01956/x5.png",
                "caption": "Figure 5:Matching results comparison.Green represents high-quality matching results, while red represents discarded matching results. More green high-quality matches indicate a higher level of geometric consistency between the two views.",
                "position": 555
            },
            {
                "img": "https://arxiv.org/html/2504.01956/x6.png",
                "caption": "Figure 6:Visual results of ablation study.We ablate the design choices of 3D-aware leap flow distillation and dynamic denoising policy network (DDPNet).",
                "position": 619
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6More Discussion of Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01956/x7.png",
                "caption": "Figure 7:Visual results of the generative ability. We highlight the generated regions in the red boxes in the novel generated views.",
                "position": 2191
            }
        ]
    },
    {
        "header": "7Additional Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01956/x8.png",
                "caption": "Figure 8:Quantitative comparison across steps.We evaluate the results of CogVideo, DynamiCrafter, Stable Video Diffusion (SVD), and VideoScene across 1, 10, 20, 30, 40, and 50 steps. VideoScene not only outperforms the other methods but also demonstrates remarkable consistency, with its 1-step results closely approximating its 50-step results, whereas other methods exhibit a significant decline in performance over fewer steps.",
                "position": 2222
            },
            {
                "img": "https://arxiv.org/html/2504.01956/x9.png",
                "caption": "Figure 9:Comparisons with base renderings with severe artifacts.",
                "position": 2225
            },
            {
                "img": "https://arxiv.org/html/2504.01956/x10.png",
                "caption": "Figure 10:Comparisons with 3D-aware diffusion model ViewCrafter.",
                "position": 2228
            },
            {
                "img": "https://arxiv.org/html/2504.01956/x11.png",
                "caption": "Figure 11:Comparisons with NeRF-based methods.",
                "position": 2231
            }
        ]
    },
    {
        "header": "8Additional Experiments and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01956/x12.png",
                "caption": "Figure 12:Quantitative comparison across additional dimensions.We further evaluate the 1-step and 50-step results by incorporating additional dimensions from the VBench metrics.",
                "position": 2342
            },
            {
                "img": "https://arxiv.org/html/2504.01956/x13.png",
                "caption": "Figure 13:Qualitative comparison on Mip-Nerf 360 and Tank-and-Temples. With two sparse views as input, our method achieves much better reconstruction quality compared with baselines.",
                "position": 2421
            },
            {
                "img": "https://arxiv.org/html/2504.01956/x14.png",
                "caption": "Figure 14:Fail case of passing directly through the closed door.",
                "position": 2424
            },
            {
                "img": "https://arxiv.org/html/2504.01956/x15.png",
                "caption": "Figure 15:Visual results of VideoScene.We show visual results on NeRF-LLFF[59], Sora[11], Mip-NeRF 360[6], and Tank-and-Temples dataset[36]datasets. The first and last columns represent the input views, while the intermediate columns depict the generated views.",
                "position": 2528
            }
        ]
    },
    {
        "header": "9More Discussions",
        "images": []
    }
]