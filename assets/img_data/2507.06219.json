[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.06219/x1.png",
                "caption": "",
                "position": 123
            }
        ]
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.06219/x2.png",
                "caption": "Figure 2:Illustration of the multimodal expert behavior in task Push-T[31].The robot (blue circle) needs to move the gray T to the green target area. Expert demonstrations exhibit multimodality in both spatial and velocity dimensions: (a) Spatial multimodality arises from different trajectory choices, where the robot can approach T from either left or right sides, resulting in distinct spatial paths; (b) Velocity multimodality occurs when robots execute similar trajectory at different speeds, generating completely different demonstration profiles over time. Both spatial and velocity multimodal characteristics require models to learn these distributional properties in current action chunk-based imitation learning.",
                "position": 180
            }
        ]
    },
    {
        "header": "IIRelated Work",
        "images": []
    },
    {
        "header": "IIITask Diversity",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.06219/x3.png",
                "caption": "Figure 3:Distribution of atomic skills in two pre-training datasets.Task-based sampling (10% tasks) shows lower skill diversity but concentrates on the most commonly used skills, while episode-based sampling (10% episodes) demonstrates a more balanced distribution.",
                "position": 245
            },
            {
                "img": "https://arxiv.org/html/2507.06219/x4.png",
                "caption": "Figure 4:Real-robot evaluation of GO-1[1]on four\nchallenging tasks subsequent to pre-training on different datasets.The tasks assess fine-grained manipulation, deformable object handling, long-horizon planning, and contact-rich interactions respectively. Results show that episode-based sampling (10% Episode) outperforms task-based sampling (10% Task) by 0.1 in average score with the same data amount, and performance improves consistently with increased pre-training data while ensuring sufficient task diversity.",
                "position": 251
            },
            {
                "img": "https://arxiv.org/html/2507.06219/x5.png",
                "caption": "Figure 5:Performance scales with pre-training data size while maintaining adequate task diversity, following a predictable power-law relationship.Left: GO-1 performance scales with pre-training data size.Right: Power-law relationship between pre-training data size and model performance. The dashed line represents a power-law fit with equationy=1.24‚Å¢x‚àí0.08ùë¶1.24superscriptùë•0.08y=1.24x^{-0.08}italic_y = 1.24 italic_x start_POSTSUPERSCRIPT - 0.08 end_POSTSUPERSCRIPTand correlation coefficientr=‚àí0.99ùëü0.99r=-0.99italic_r = - 0.99, indicating a strong adherence to power-law scaling with pre-training data volume.",
                "position": 263
            }
        ]
    },
    {
        "header": "IVEmbodiment Diversity",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.06219/x6.png",
                "caption": "Figure 6:Cross-embodiment adaptation to Franka arm in ManiSkill with varying training data sizes.Left:Performance vs. number of demonstrations per task in fine-tuning data.Right:Power-law relationship between downstream performance and fine-tuning data size.",
                "position": 302
            },
            {
                "img": "https://arxiv.org/html/2507.06219/x7.png",
                "caption": "Figure 7:Cross-embodiment adaptation to Franka arm in ManiSkill with varying training steps.Left:Performance vs. training steps with 1000 fine-tuning episodes per task.Right:Performance vs. training steps with 500 fine-tuning episodes per task.",
                "position": 305
            },
            {
                "img": "https://arxiv.org/html/2507.06219/x8.png",
                "caption": "Figure 8:How the model crosses the embodiment gap to Arx in RoboTwin as data sizes increase.Left:Performance vs. number of demonstrations per task in fine-tuning data.Right:Power-law relationship between downstream performance and fine-tuning data size.",
                "position": 311
            }
        ]
    },
    {
        "header": "VExpert Diversity",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.06219/x9.png",
                "caption": "Figure 9:Illustration of distribution debiasing. Top:Two demonstrations (Demo 1 & 2) follow the same trajectory (A‚ÜíD) but have different velocities, resulting in distinct action chunks within the same time window (A‚ÜíB vs A‚ÜíD).Bottom:After velocity-based distribution debiasing, both demonstrations are normalized to similar action chunks (A‚ÜíC), reducing velocity ambiguity and facilitating model learning.",
                "position": 352
            },
            {
                "img": "https://arxiv.org/html/2507.06219/x10.png",
                "caption": "Figure 10:Two-stage distribution debiasing framework using VM. Stage 1:VM is trained to predict velocity from action chunks using MSE loss, learning the expected velocity for each input from velocity-biased training data.Stage 2:During policy training, VM first predicts the unbiased velocity for each training sample, which is then used to transform the original actions into unbiased actions. The policy is subsequently trained using these unbiased actions as supervision targets, effectively simplifying the distribution complexity.",
                "position": 394
            },
            {
                "img": "https://arxiv.org/html/2507.06219/x11.png",
                "caption": "Figure 11:GO-1-Pro consistently outperforms GO-1 on both the Wipe Table and Make Sandwich tasks.GO-1-Pro achieves comparable results using only 50% of the training data that GO-1 uses, demonstrating superior data efficiency.",
                "position": 434
            },
            {
                "img": "https://arxiv.org/html/2507.06219/x12.png",
                "caption": "Figure 12:Performance scaling with fine-tuning data usage for GO-1 and GO-1-Pro on Wipe Table and Make Sandwich tasks.Both models follow power law relationships (fitted equations shown), with GO-1-Pro demonstrating faster convergence rates",
                "position": 437
            }
        ]
    },
    {
        "header": "VIConclusion and Future Work",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.06219/x13.png",
                "caption": "Figure A-1:Deployment on AgiBot G1.",
                "position": 954
            },
            {
                "img": "https://arxiv.org/html/2507.06219/x14.png",
                "caption": "Figure A-2:Deployment on AgileX Cobot Magic with Piper.",
                "position": 957
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]