[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Is Ensembling Different LLMs Beneficial?",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.00674/x1.png",
                "caption": "Figure 1:Comparison of MoA, Self-MoA, and Self-MoA-Seq. (a) In MoA, multiple models respond to a query, followed by an aggregator synthesizing their outputs. (b) Self-MoA simplifies this by repeatedly sampling from a single model. (c) Self-MoA-Seq extends Self-MoA by applying a sliding window to combine the best output so far with candidate outputs. At each timestep, the synthesized output is repeated to bias the aggregator towards it, reducing the context length requirements and expanding the method’s applicability. Note that MoA can extend to multiple rounds of aggregation (AppendixA.1), while Self-MoA and Self-MoA-Seq can extend to more outputs, but we omit them here for clarity.",
                "position": 210
            }
        ]
    },
    {
        "header": "4The Quality-Diversity Trade-off",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.00674/x2.png",
                "caption": "Figure 2:The diversity-quality trade-off: Mixed-MoA incorporates different individual models as proposers, while Self-MoA uses the same individual model for this role. Quality is assessed based on the average performance of each proposer, and diversity is computed with the Vendi Score(Dan Friedman and Dieng,2023)of outputs generated by proposers on the same prompts.",
                "position": 525
            },
            {
                "img": "https://arxiv.org/html/2502.00674/x3.png",
                "caption": "",
                "position": 528
            },
            {
                "img": "https://arxiv.org/html/2502.00674/x4.png",
                "caption": "",
                "position": 529
            }
        ]
    },
    {
        "header": "5Scaling Inference Compute with Self-MoA",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.00674/x5.png",
                "caption": "Figure 3:The performance of Self-MoA and Self-MoA-Seq with a growing number of samples. Dashed lines indicate the performance of a single forward pass with the base model.",
                "position": 987
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASupplements",
        "images": []
    },
    {
        "header": "Appendix BAdditional Results",
        "images": []
    }
]