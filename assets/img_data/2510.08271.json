[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08271/x1.png",
                "caption": "",
                "position": 90
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08271/x2.png",
                "caption": "Figure 2:SViM3D Improvements on Common Issues.Our method introduces several new contributions which improve the reconstruction quality of our method drastically.",
                "position": 111
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x3.png",
                "caption": "",
                "position": 112
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x4.png",
                "caption": "",
                "position": 112
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x5.png",
                "caption": "",
                "position": 116
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x6.png",
                "caption": "",
                "position": 116
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x7.png",
                "caption": "",
                "position": 116
            },
            {
                "img": "https://arxiv.org/html/2510.08271/images/ablations/gtbrass_vase.png",
                "caption": "",
                "position": 120
            },
            {
                "img": "https://arxiv.org/html/2510.08271/images/ablations/v103_brass_vase.png",
                "caption": "",
                "position": 120
            },
            {
                "img": "https://arxiv.org/html/2510.08271/images/ablations/v100_brass_vase.png",
                "caption": "",
                "position": 120
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x8.png",
                "caption": "",
                "position": 124
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x9.png",
                "caption": "",
                "position": 124
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x10.png",
                "caption": "",
                "position": 124
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x11.png",
                "caption": "",
                "position": 128
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x12.png",
                "caption": "",
                "position": 128
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x13.png",
                "caption": "",
                "position": 128
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x14.png",
                "caption": "",
                "position": 136
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x15.png",
                "caption": "",
                "position": 136
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x16.png",
                "caption": "",
                "position": 136
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x17.png",
                "caption": "",
                "position": 140
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x18.png",
                "caption": "",
                "position": 140
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x19.png",
                "caption": "",
                "position": 140
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x20.png",
                "caption": "",
                "position": 148
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x21.png",
                "caption": "",
                "position": 148
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x22.png",
                "caption": "",
                "position": 148
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x23.png",
                "caption": "",
                "position": 152
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x24.png",
                "caption": "",
                "position": 152
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x25.png",
                "caption": "",
                "position": 152
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x26.png",
                "caption": "",
                "position": 160
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x27.png",
                "caption": "",
                "position": 160
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x28.png",
                "caption": "",
                "position": 160
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x29.png",
                "caption": "",
                "position": 164
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x30.png",
                "caption": "",
                "position": 164
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x31.png",
                "caption": "",
                "position": 164
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x32.png",
                "caption": "",
                "position": 172
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x33.png",
                "caption": "",
                "position": 172
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x34.png",
                "caption": "",
                "position": 172
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x35.png",
                "caption": "",
                "position": 176
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x36.png",
                "caption": "",
                "position": 176
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x37.png",
                "caption": "",
                "position": 176
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x38.png",
                "caption": "",
                "position": 184
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x39.png",
                "caption": "",
                "position": 184
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x40.png",
                "caption": "",
                "position": 184
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x41.png",
                "caption": "",
                "position": 188
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x42.png",
                "caption": "",
                "position": 188
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x43.png",
                "caption": "",
                "position": 188
            }
        ]
    },
    {
        "header": "2Related works",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08271/x44.png",
                "caption": "Figure 3:The SViM3D pipeline.We train a video diffusion model on multi-view and multi-illumination data to generate multi-view images with material parameters. During inference, given a single image, SViM3D  can generate 21 views with consistent RGB radiance, albedo, roughness, metallic, and camera space normals. We then use the synthesized novel views for 3D reconstruction that yields textured meshes with PBR materials. Starting from illumination pre-optimization, we further propose several techniques to aid the 3D reconstruction pipeline in this sparse view setting, such as visibility masking, homography correction, fast differentiable rendering.",
                "position": 294
            }
        ]
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4SViM3D: Multi-view PBR Generation",
        "images": []
    },
    {
        "header": "5Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08271/x45.png",
                "caption": "Figure 4:Multi-view consistency.We compare the generated materials from different neural diffusion priors in a multi-view setting. SV3D[99]shows multi-view consistent RGB output similar to SViM3D that also generates multi-view consistent Basecolor. Generating albedo maps on top of the SV3D views using RGB↔\\leftrightarrowX[111], StableMaterial (SM) of MaterialFusion[68]or Intrinsic Image Diffusion (IID)[59]yields inconsistent results compared to the GT.",
                "position": 591
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x46.png",
                "caption": "Figure 5:Multi-view PBR materials.Given the input image SViM3D generates multi-view consistent novel views with corresponding basecolor, roughness, metallic and normal maps. These can directly be used to generate views under novel illumination. We show 5 samples from a generated orbit and two new illumination settings as examples. The objects are sourced from our Poly Haven[39]test dataset. Please find additional results in the supplementary material.",
                "position": 594
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x47.png",
                "caption": "Figure 6:Single image PBR materials.We compare the generated materials from different neural diffusion priors for a single image from the Poly Haven[39]test set. Besides the GT rendering and SViM3D (ours) results from RGB↔\\leftrightarrowX[111], StableMaterial (SM) of MaterialFusion[68]and Intrinsic Image Diffusion (IID)[59]are presented. Note that IID uses monocular normals that are separately generated and SM does not provide any normals.",
                "position": 598
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x48.png",
                "caption": "Figure 7:Relighting comparison.We compare image-based relighting of recent diffusion-based methods IC-Light[117], Neural-Gaffer[51]and DiLightNet[109]against SViM3D and the synthetic ground truth (GT) on examples from Poly Haven[39]data.",
                "position": 832
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x49.png",
                "caption": "",
                "position": 836
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Overview",
        "images": []
    },
    {
        "header": "Appendix AAdditional Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08271/x50.png",
                "caption": "Figure 8:Multiple samples.Demonstrating the stochastic sampling process by taking three samples with the same condition image. For views that are less constrained by the conditioning diverse examples can be generated depending on the initial noise. Note, that the roughness and metallic parameters (blue and green here) are consistent with the RGB predictions, though.",
                "position": 2583
            }
        ]
    },
    {
        "header": "Appendix BOptimization",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08271/x51.png",
                "caption": "Figure 9:Multi-view material prediction.Additional examples from the Poly Haven[39]test dataset. SViM3D successfully converts a single image to a sequence of novel views with spatially-varying PBR material parameters and surface normals. These can directly be used to relight the novel views as shown in the two bottom rows.",
                "position": 2647
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x52.png",
                "caption": "(a)GT rendering",
                "position": 2650
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x52.png",
                "caption": "",
                "position": 2653
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x53.png",
                "caption": "",
                "position": 2657
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x54.png",
                "caption": "",
                "position": 2662
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x55.png",
                "caption": "(a)GT rendering",
                "position": 2666
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x56.png",
                "caption": "(b)SF3D",
                "position": 2672
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x57.png",
                "caption": "(c)SViM3D",
                "position": 2677
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x58.png",
                "caption": "Table 5:View consistency.Multi-view consistency evaluated using MEt3R[3]on the Poly Haven test data.",
                "position": 2684
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x58.png",
                "caption": "Figure 11:Multi-view error distribution.We compare the SSIM results of the Basecolor prediction across frames over the Poly Haven test set.",
                "position": 2721
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x59.png",
                "caption": "Figure 12:Multi-view PBR materials.Given the input image SViM3D generates multi-view consistent novel views with corresponding basecolor, roughness, metallic and normal maps. These can directly be used to generate views under novel illumination. We show 5 samples from a generated orbit and two new illumination settings as examples. The objects are sourced from our Poly Haven[39]test dataset. Please find additional results in the supplementary material.",
                "position": 2725
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x60.png",
                "caption": "Figure 13:More 3D reconstruction results.Objects sourced from Poly Haven[39]and GSO[29], rendered in Blender.",
                "position": 2905
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x61.png",
                "caption": "Figure 14:2.5D Relighting.Using the output of SViM3D and an environment map we can directly relight an object. We can use the same illumination representation and deferred shading as in the differentiable rendering pipeline.",
                "position": 2908
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x62.png",
                "caption": "Figure 15:3D reconstruction example.SViM3D ’s pipeline starts with a single image at the bottom left. First novel views and the corresponding material parameters and surface normals are generated. Following, an intermediate 3D representation is optimized given the multi-view material prior. Finally, a 3D mesh can be extracted and integrated into downstream applications. Here we show an example from our Poly Haven[39]test dataset.",
                "position": 2925
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x63.png",
                "caption": "Figure 16:Multi-view material examples from GSO.Two objects from the GSO[29]dataset representing common real-world houshold items. SViM3D generalizes well to this domain as long as the scene is object centric.",
                "position": 2928
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x64.png",
                "caption": "Figure 17:Multi-view material examples from generated images..Multi-view generations conditioned on generated images from text-to-image models, a wizard raccoon and a silver teapot. SViM3D is capable of estimating plausible and view consistent results. The wizard raccoon is an out-of-distribution example due to the lack of stylized character models in the training data.",
                "position": 2931
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x65.png",
                "caption": "Figure 18:Material editing.The explicit material parameters of SViM3D’s output can be edited in a physically-plausible way and the result visualized using our rendering framework. In this example the material roughness is varied between almost zero and close to one while the original value is close to the version second to left.",
                "position": 3061
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x66.png",
                "caption": "Figure 19:Relighting comparison.We compare image-based relighting results on an example object from the Poly Haven[39]dataset between the synthetic ground truth (GT), IC-Light[117], Neural-Gaffer[51], DiLightNet[109]and SViM3D (ours).",
                "position": 3072
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x67.png",
                "caption": "",
                "position": 3076
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x68.png",
                "caption": "Figure 20:Relighting.Using the output of SViM3D and an environment map (HDRI) we can directly relight any view on the camera trajectory using our 2.5D approach.",
                "position": 3080
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x69.png",
                "caption": "Figure 21:Glossiness vs. Metalness ambiguity.Examples from our generated test cases and the corresponding model predictions.",
                "position": 3090
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x70.png",
                "caption": "Figure 22:Real-world results.Example generations from casual smartphone captures of a shaker instrument and a strawberry.",
                "position": 3093
            },
            {
                "img": "https://arxiv.org/html/2510.08271/x71.png",
                "caption": "Figure 23:Comparison of multi-view material generation on Poly Haven objects.We compare generated materials of RGB↔\\leftrightarrowX[111], StableMaterial (SM) of MaterialFusion[68]and Intrinsic Image Diffusion (IID)[59]based on SV3D[98]generations and SViM3D for three views around the object against GT renders.",
                "position": 3100
            }
        ]
    },
    {
        "header": "Appendix CFurther results",
        "images": []
    }
]