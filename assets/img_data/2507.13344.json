[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.13344/x1.png",
                "caption": "",
                "position": 82
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.13344/x2.png",
                "caption": "Figure 2:Overview of Diffuman4D.\nOur model takesMùëÄMitalic_Minput-view videos as input, generatesNùëÅNitalic_Ntarget-view videos, and reconstructs a 4DGS of the human using both input and generated videos.\nSpecifically, the input-view videos are encoded into the latent space using a pretrained VAE.\nThe 3D human‚Äêskeleton sequence is projected into each view, rendered as RGB maps, and encoded into the same latent space.\nIn addition, the camera parameters are encoded as Pl√ºcker coordinates[82].\nThe skeleton latents and Pl√ºcker coordinates are then concatenated with the image latents at input views or the noise latents at target views, forming input samples or target samples, respectively.\nThe samples across all views and timestamps form a sample grid and are fed into our spatio-temporal diffusion model that denoises the samples using a sliding iterative denoising mechanism (seeSec.3.2).\nThen, the denoised image latents for the target views are decoded into target-view videos using a pretrained VAE decoder.\nFinally, the 4DGS is reconstructed using an off-the-shelf approach[73](seeSec.3.4), enabling real-time novel view rendering.",
                "position": 200
            },
            {
                "img": "https://arxiv.org/html/2507.13344/x3.png",
                "caption": "(a)Sliding iterative denoising mechanism",
                "position": 252
            },
            {
                "img": "https://arxiv.org/html/2507.13344/x3.png",
                "caption": "(a)Sliding iterative denoising mechanism",
                "position": 255
            },
            {
                "img": "https://arxiv.org/html/2507.13344/x4.png",
                "caption": "(b)Spatio-temporal denoising process",
                "position": 260
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.13344/x5.png",
                "caption": "Figure 4:Qualitative comparison of different conditioning. The skeleton-Pl√ºcker mixed conditioning serves as a robust human pose prior for diffusion models.",
                "position": 593
            },
            {
                "img": "https://arxiv.org/html/2507.13344/x6.png",
                "caption": "Figure 5:Qualitative comparison of different denoising strategies. Our sliding iterative denoising method ensures a consistent appearance throughout a long image sequence.",
                "position": 610
            },
            {
                "img": "https://arxiv.org/html/2507.13344/x7.png",
                "caption": "(a)Results on DNA-Rendering[10]test set.",
                "position": 619
            },
            {
                "img": "https://arxiv.org/html/2507.13344/x7.png",
                "caption": "(a)Results on DNA-Rendering[10]test set.",
                "position": 622
            },
            {
                "img": "https://arxiv.org/html/2507.13344/x8.png",
                "caption": "(b)Zero-shot generalization on ActorsHQ[23].",
                "position": 627
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AModel Details",
        "images": []
    },
    {
        "header": "Appendix BDatasets Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.13344/x9.png",
                "caption": "Figure 7:Our voting strategy effectively leverages the strengths of different background removal methods to produce robust foreground masks.",
                "position": 1830
            },
            {
                "img": "https://arxiv.org/html/2507.13344/x10.png",
                "caption": "Figure 8:More qualitative comparisons on DNA-Rendering[10]and ActorsHQ[23].GPS-Gaussian uses 8 input views while all others use 4 input views, and CAT4D‚Ä†is our reproduced version. Our Diffuman4D consistently outperforms baselines with higher visual quality and better spatio-temporal consistency.",
                "position": 1833
            },
            {
                "img": "https://arxiv.org/html/2507.13344/x11.png",
                "caption": "Figure 9:High-quality foreground masks and human skeletons predicted using state-of-the-art methods.",
                "position": 1844
            },
            {
                "img": "https://arxiv.org/html/2507.13344/x12.png",
                "caption": "Figure 10:Qualitative comparisons between novel views generated by our model and those rendered from the 4DGS model reconstructed using LongVolcap[73].",
                "position": 1858
            }
        ]
    },
    {
        "header": "Appendix CAdditional Comparisons",
        "images": []
    }
]