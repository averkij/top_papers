[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21880/x1.png",
                "caption": "Figure 1:Left: RL-ZVP uses an entropy-guided advantage formulation to extract learning signals from zero-variance prompts, while reverting to GRPO on other prompts.Right: RL-ZVP demonstrates significantly higher average accuracy than GRPO across six math reasoning benchmarks.",
                "position": 95
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21880/x2.png",
                "caption": "Figure 2:Rollout generation overheadas a percentage of total time of the training step.",
                "position": 104
            }
        ]
    },
    {
        "header": "2Preliminary: Group Relative Policy Optimization",
        "images": []
    },
    {
        "header": "3Reinforcement Learning with Zero-Variance Prompts",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21880/x3.png",
                "caption": "Figure 3:The percentage of zero-variance prompts.",
                "position": 180
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21880/x4.png",
                "caption": "Table 1:Detailed evaluation results on six math reasoning benchmarks.The best and second best performance across all settings areboldandunderscored, respectively. Under a fair setup of equal rollout budget, RL-ZVP achieves the best performance across all datasets, heavily outperforming GRPO. (*) Even in the unfavorable setting where Dynamic Sampling baselines use3×–5×3\\times–5\\timesmore rollouts, RL-ZVP still outperforms them on the majority of benchmarks.",
                "position": 398
            },
            {
                "img": "https://arxiv.org/html/2509.21880/x4.png",
                "caption": "(a)Acc@8 (small scale)",
                "position": 675
            },
            {
                "img": "https://arxiv.org/html/2509.21880/x4.png",
                "caption": "(a)Acc@8 (small scale)",
                "position": 678
            },
            {
                "img": "https://arxiv.org/html/2509.21880/x5.png",
                "caption": "(b)Entropy (small scale)",
                "position": 683
            },
            {
                "img": "https://arxiv.org/html/2509.21880/x6.png",
                "caption": "(c)Response Length (small scale)",
                "position": 688
            },
            {
                "img": "https://arxiv.org/html/2509.21880/x7.png",
                "caption": "(d)Acc@8 (large scale)",
                "position": 694
            },
            {
                "img": "https://arxiv.org/html/2509.21880/x8.png",
                "caption": "(e)Entropy (large scale)",
                "position": 699
            },
            {
                "img": "https://arxiv.org/html/2509.21880/x9.png",
                "caption": "(f)Response Length (large scale)",
                "position": 704
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Closing Remarks",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "APPENDIX",
        "images": []
    },
    {
        "header": "Appendix AAdditional Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21880/x10.png",
                "caption": "Table 4:Additional evaluation results on six math reasoning benchmarks when the model is trained with NuminaMath-CoT.",
                "position": 1461
            },
            {
                "img": "https://arxiv.org/html/2509.21880/x10.png",
                "caption": "(a)Acc@8 (Qwen3-1.7B-Base)",
                "position": 1609
            },
            {
                "img": "https://arxiv.org/html/2509.21880/x10.png",
                "caption": "(a)Acc@8 (Qwen3-1.7B-Base)",
                "position": 1612
            },
            {
                "img": "https://arxiv.org/html/2509.21880/x11.png",
                "caption": "(b)Entropy (Qwen3-1.7B-Base)",
                "position": 1617
            },
            {
                "img": "https://arxiv.org/html/2509.21880/x12.png",
                "caption": "(c)Response Length (Qwen3-1.7B-Base)",
                "position": 1622
            },
            {
                "img": "https://arxiv.org/html/2509.21880/x13.png",
                "caption": "(d)Acc@8 (Qwen3-8B-Base)",
                "position": 1628
            },
            {
                "img": "https://arxiv.org/html/2509.21880/x14.png",
                "caption": "(e)Entropy (Qwen3-8B-Base)",
                "position": 1633
            },
            {
                "img": "https://arxiv.org/html/2509.21880/x15.png",
                "caption": "(f)Response Length (Qwen3-8B-Base)",
                "position": 1638
            }
        ]
    },
    {
        "header": "Appendix BSupplementary Figures and Tables",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21880/x16.png",
                "caption": "(a)Accuracy",
                "position": 1652
            },
            {
                "img": "https://arxiv.org/html/2509.21880/x16.png",
                "caption": "(a)Accuracy",
                "position": 1655
            },
            {
                "img": "https://arxiv.org/html/2509.21880/x17.png",
                "caption": "(b)Pass rate",
                "position": 1660
            }
        ]
    },
    {
        "header": "Appendix CFull Implementation Details",
        "images": []
    },
    {
        "header": "Appendix DQualitivate Examples",
        "images": []
    }
]