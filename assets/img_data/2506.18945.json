[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18945/x1.png",
                "caption": "Figure 1:Comparison between Mixture-of-Experts (MoE) and Chain-of-Experts (CoE).Under the same model depth and parameters, CoE enables iterative expert communication to offer more flexible expert choice compared to MoE.",
                "position": 124
            }
        ]
    },
    {
        "header": "2Background: Mixture-of-Experts Transformers",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18945/x2.png",
                "caption": "Figure 2:Illustration of Chain-of-Experts.MoE Top-K experts operate in parallel without interaction, CoE enables the same number of experts to process sequentially via intermediate representations, allowing deeper network processing with the same per-token expert processing. Residual connections are enabled and iteration-independent routers are used for more effective training.",
                "position": 181
            }
        ]
    },
    {
        "header": "3Chain-of-Experts: Communicative Expert Processing",
        "images": []
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Experimental Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18945/extracted/6562257/images/plot.png",
                "caption": "(a)Validation loss with same expert budget.",
                "position": 413
            },
            {
                "img": "https://arxiv.org/html/2506.18945/extracted/6562257/images/plot.png",
                "caption": "(a)Validation loss with same expert budget.",
                "position": 416
            },
            {
                "img": "https://arxiv.org/html/2506.18945/extracted/6562257/images/plot_dense.png",
                "caption": "(b)Iteration effect under sparse and dense setting.",
                "position": 421
            },
            {
                "img": "https://arxiv.org/html/2506.18945/extracted/6562257/images/plot_lyr.png",
                "caption": "Figure 4:Depth scaling comparison.CoE (C=2ùê∂2C{=}2italic_C = 2) with 4 layers matches the performance of deeper MoE models (LùêøLitalic_L=8888or12121212) with significantly lower memory and parameter usage.",
                "position": 440
            },
            {
                "img": "https://arxiv.org/html/2506.18945/extracted/6562257/images/plot_scal.png",
                "caption": "Figure 5:Width scaling comparison.CoE (C=2ùê∂2C{=}2italic_C = 2) outperforms MoE variants with increased expert selection (K=16ùêæ16K{=}16italic_K = 16or24242424), while using similar resources.",
                "position": 443
            },
            {
                "img": "https://arxiv.org/html/2506.18945/extracted/6562257/images/plot_eff.png",
                "caption": "Figure 6:Matched performance with fewer experts.CoE (N=48ùëÅ48N{=}48italic_N = 48) achieves similar performance to MoE (N=64ùëÅ64N{=}64italic_N = 64) while reducing memory usage by17.6%.",
                "position": 473
            },
            {
                "img": "https://arxiv.org/html/2506.18945/extracted/6562257/images/plot_abl.png",
                "caption": "Figure 7:Ablation study.Removing either iteration-specific gating or inner residuals significantly reduces performance. Both components are critical to the effectiveness of sequential expert routing in CoE.",
                "position": 550
            },
            {
                "img": "https://arxiv.org/html/2506.18945/extracted/6562257/figure_appendix/64ept-4tpk-2itr-1SharedExp-metamathqa-len512-2k/layer_all_routing_matrix.png",
                "caption": "Figure 8:Expert co-activation between communication steps.For each selected layer, we plot a matrix counting how often each expert pair(e0,e1)subscriptùëí0subscriptùëí1(e_{0},e_{1})( italic_e start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT )is activated across the two communication steps in CoE. The non-uniform, asymmetric patterns indicate that routing decisions differ meaningfully between steps, supporting the emergence of step-wise expert specialization.",
                "position": 570
            }
        ]
    },
    {
        "header": "6Related Works",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Limitations and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExtended Related Works",
        "images": []
    },
    {
        "header": "Appendix BImpact of Shared Experts",
        "images": []
    },
    {
        "header": "Appendix CExtended Training and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18945/extracted/6562257/images/plot_math.png",
                "caption": "Figure 9:Validation loss on MetaMathQA across 10,000 steps.",
                "position": 1520
            },
            {
                "img": "https://arxiv.org/html/2506.18945/extracted/6562257/images/plot_slim.png",
                "caption": "Figure 10:Validation loss on general-domain SlimPajama across 10,000 steps.",
                "position": 1523
            },
            {
                "img": "https://arxiv.org/html/2506.18945/extracted/6562257/figure_appendix/64ept-4tpk-2itr-1SharedExp-metamathqa-len512-2k/layer_all_routing_matrix.png",
                "caption": "Figure 11:Layer-wise routing pattern on MetamathQA under the 4-experts-per-iteration, 2-iteration setup (2k training steps).",
                "position": 1564
            },
            {
                "img": "https://arxiv.org/html/2506.18945/extracted/6562257/figure_appendix/64ept-4tpk-2itr-1SharedExp-metamathqa-len512-10k/layer_all_routing_matrix.png",
                "caption": "Figure 12:Layer-wise routing pattern on MetamathQA under the 4-experts-per-iteration, 2-iteration setup (10k training steps).",
                "position": 1567
            },
            {
                "img": "https://arxiv.org/html/2506.18945/extracted/6562257/figure_appendix/64ept-4tpk-2itr-1SharedExp-slimpajama-len512-2k/layer_all_routing_matrix.png",
                "caption": "Figure 13:Layer-wise routing pattern on SlimPajama under the 4-experts-per-iteration, 2-iteration setup (2k training steps).",
                "position": 1570
            },
            {
                "img": "https://arxiv.org/html/2506.18945/extracted/6562257/figure_appendix/64ept-4tpk-2itr-1SharedExp-slimpajama-len512-10k/layer_all_routing_matrix.png",
                "caption": "Figure 14:Layer-wise routing pattern on SlimPajama under the 4-experts-per-iteration, 2-iteration setup (10k training steps).",
                "position": 1573
            },
            {
                "img": "https://arxiv.org/html/2506.18945/extracted/6562257/figure_appendix/64ept-8tpk-2itr-1SharedExp-metamathqa-len512-2k/layer_all_routing_matrix.png",
                "caption": "Figure 15:Layer-wise routing pattern on MetamathQA under the 8-experts-per-iteration, 2-iteration setup (2k training steps).",
                "position": 1576
            },
            {
                "img": "https://arxiv.org/html/2506.18945/extracted/6562257/figure_appendix/64ept-8tpk-2itr-1SharedExp-metamathqa-len512-10k/layer_all_routing_matrix.png",
                "caption": "Figure 16:Layer-wise routing pattern on MetamathQA under the 8-experts-per-iteration, 2-iteration setup (10k training steps).",
                "position": 1579
            },
            {
                "img": "https://arxiv.org/html/2506.18945/extracted/6562257/figure_appendix/64ept-8tpk-2itr-1SharedExp-slimpajama-len512-2k/layer_all_routing_matrix.png",
                "caption": "Figure 17:Layer-wise routing pattern on SlimPajama under the 8-experts-per-iteration, 2-iteration setup (2k training steps).",
                "position": 1582
            },
            {
                "img": "https://arxiv.org/html/2506.18945/extracted/6562257/figure_appendix/64ept-8tpk-2itr-1SharedExp-slimpajama-len512-10k/layer_all_routing_matrix.png",
                "caption": "Figure 18:Layer-wise routing pattern on SlimPajama under the 8-experts-per-iteration, 2-iteration setup (10k training steps).",
                "position": 1585
            },
            {
                "img": "https://arxiv.org/html/2506.18945/extracted/6562257/figure_appendix/64ept-8tpk-1itr-1SharedExp-metamathqa-len512-2k/layer_all_routing_matrix.png",
                "caption": "Figure 19:Layer-wise routing pattern on MetamathQA under the 8-experts-per-iteration, 1-iteration setup (2k training steps).",
                "position": 1588
            },
            {
                "img": "https://arxiv.org/html/2506.18945/extracted/6562257/figure_appendix/64ept-8tpk-1itr-1SharedExp-metamathqa-len512-10k/layer_all_routing_matrix.png",
                "caption": "Figure 20:Layer-wise routing pattern on MetamathQA under the 8-experts-per-iteration, 1-iteration setup (10k training steps).",
                "position": 1591
            },
            {
                "img": "https://arxiv.org/html/2506.18945/extracted/6562257/figure_appendix/64ept-8tpk-1itr-1SharedExp-slimpajama-len512-2k/layer_all_routing_matrix.png",
                "caption": "Figure 21:Layer-wise routing pattern on SlimPajama under the 8-experts-per-iteration, 1-iteration setup (2k training steps).",
                "position": 1594
            },
            {
                "img": "https://arxiv.org/html/2506.18945/extracted/6562257/figure_appendix/64ept-8tpk-1itr-1SharedExp-slimpajama-len512-10k/layer_all_routing_matrix.png",
                "caption": "Figure 22:Layer-wise routing pattern on SlimPajama under the 8-experts-per-iteration, 1-iteration setup (10k training steps).",
                "position": 1597
            }
        ]
    },
    {
        "header": "Appendix DAnalysis of Expert Co-Activation Visualizations",
        "images": []
    }
]