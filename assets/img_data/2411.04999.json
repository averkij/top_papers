[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04999/x1.png",
                "caption": "Figure 1:An illustration of how our online dynamic spatio-semantic memory DynaMem responds to open vocabulary queries in a dynamic environment. During operation and exploration, DynaMem keeps updating its semantic map in memory. DynaMem maintains a voxelized pointcloud representation of the environment, and updates with dynamic changes in the environment by adding and removing points.",
                "position": 110
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04999/x2.png",
                "caption": "Figure 2:(Left) DynaMem keeps its memory stored in a sparse voxel grid with associated information at each voxel. (Right) Updating DynaMem by adding new points to it, alongside the rules used to update the stored information.",
                "position": 226
            },
            {
                "img": "https://arxiv.org/html/2411.04999/x2.png",
                "caption": "",
                "position": 229
            },
            {
                "img": "https://arxiv.org/html/2411.04999/x3.png",
                "caption": "",
                "position": 233
            },
            {
                "img": "https://arxiv.org/html/2411.04999/x4.png",
                "caption": "Figure 3:A high-level, 2D depiction of how adding and removing voxels from the voxel map works. New voxels are included which are in the RGB-D cameras view frustum, and old voxels that should block the view frustum but does not are removed from the map.",
                "position": 263
            },
            {
                "img": "https://arxiv.org/html/2411.04999/x5.png",
                "caption": "Figure 4:Querying DynaMem with a natural language query. First, we find the voxel with the highest alighnment to the query. Next, we find the latest image of that voxel, and query with an open-vocabulary object detector to confirm the object location or abstain.",
                "position": 304
            },
            {
                "img": "https://arxiv.org/html/2411.04999/x6.png",
                "caption": "Figure 5:The prompting system for querying multimodal LLMs such as GPT-4o or Gemini-1.5 for the image index for an object query.",
                "position": 320
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04999/x7.png",
                "caption": "Figure 6:Real robot experiments in three different environments: kitchen, game room, and meeting room. In each environment, we modify the environment thrice and run 10 pick-and-drop queries.",
                "position": 412
            },
            {
                "img": "https://arxiv.org/html/2411.04999/x8.png",
                "caption": "Figure 7:Statistics of failure, broken down by failure modes, in our real robot experiments in the lab and in home environments. Statistics are collected over three environments and 30 open-vocabulary pick-and-drop queries for the lab experiments, and two environments and 17 pick-and-drop queries for the home environments, on objects whose locations change over time.",
                "position": 433
            }
        ]
    },
    {
        "header": "5Conclusions and Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]