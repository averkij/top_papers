[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22642/x1.png",
                "caption": "Figure 1:Sim-to-real reinforcement learning with FastTD3.We successfully transfer the FastTD3 policy trained in MuJoCo Playground(Zakka et al.,2025)to Booster T1.\nThis represents, to the best of our knowledge, the first successful deployment of a policy trained with off-policy RL on real humanoid hardware.\nSee ourproject pagefor videos.",
                "position": 102
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22642/x2.png",
                "caption": "Figure 2:Tasks.We consider a range of challenging humanoid and dexterous control tasks from HumanoidBench (left), MuJoCo Playground (middle), and IsaacLab (right).",
                "position": 118
            },
            {
                "img": "https://arxiv.org/html/2505.22642/x3.png",
                "caption": "Figure 3:Summary of results.FastTD3 is a simple, fast, and capable RL algorithm that significantly speeds up training for humanoid robots on tasks from popular suites such as HumanoidBench(Sferrazza et al.,2024), IsaacLab(Mittal et al.,2023), and MuJoCo Playground(Zakka et al.,2025).\nTo accelerate RL research in robotics, we provide an easy-to-use open-source implementation of FastTD3, enabling users to easily reproduce these results or build upon our work.",
                "position": 121
            },
            {
                "img": "https://arxiv.org/html/2505.22642/x4.png",
                "caption": "Figure 4:Results on a selected set of tasks.Learning curves on selected individual tasks from HumanoidBench (first two rows), IsaacLab (third row), and MuJoCo Playground (fourth row). The solid line and shaded regions represent the mean and standard deviation across three runs. The dashed lines indicate success thresholds in HumanoidBench tasks.",
                "position": 179
            }
        ]
    },
    {
        "header": "2FastTD3: Simple, Fast, Capable RL for Humanoid Control",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22642/x5.png",
                "caption": "(a)Effect of parallel environments",
                "position": 190
            },
            {
                "img": "https://arxiv.org/html/2505.22642/x5.png",
                "caption": "(a)Effect of parallel environments",
                "position": 193
            },
            {
                "img": "https://arxiv.org/html/2505.22642/x6.png",
                "caption": "(b)Effect of batch size",
                "position": 198
            },
            {
                "img": "https://arxiv.org/html/2505.22642/x7.png",
                "caption": "(c)Effect of distributional RL",
                "position": 204
            },
            {
                "img": "https://arxiv.org/html/2505.22642/x8.png",
                "caption": "(d)Effect of Clipped Double Q-learning",
                "position": 209
            },
            {
                "img": "https://arxiv.org/html/2505.22642/x9.png",
                "caption": "(a)Effect of model size",
                "position": 348
            },
            {
                "img": "https://arxiv.org/html/2505.22642/x9.png",
                "caption": "(a)Effect of model size",
                "position": 351
            },
            {
                "img": "https://arxiv.org/html/2505.22642/x10.png",
                "caption": "(b)Effect of noise scales",
                "position": 356
            },
            {
                "img": "https://arxiv.org/html/2505.22642/x11.png",
                "caption": "(c)Effect of update-to-data ratio",
                "position": 362
            },
            {
                "img": "https://arxiv.org/html/2505.22642/x12.png",
                "caption": "(d)Effect of replay buffer size",
                "position": 367
            },
            {
                "img": "https://arxiv.org/html/2505.22642/x13.png",
                "caption": "(a)Gait ofFastTD3trained withPPO-tuned reward",
                "position": 375
            },
            {
                "img": "https://arxiv.org/html/2505.22642/x13.png",
                "caption": "(a)Gait ofFastTD3trained withPPO-tuned reward",
                "position": 378
            },
            {
                "img": "https://arxiv.org/html/2505.22642/x14.png",
                "caption": "(b)Gait ofPPOtrained withPPO-tuned reward",
                "position": 388
            },
            {
                "img": "https://arxiv.org/html/2505.22642/x15.png",
                "caption": "(c)Gait ofFastTD3trained withFastTD3-tuned reward",
                "position": 398
            },
            {
                "img": "https://arxiv.org/html/2505.22642/x16.png",
                "caption": "(d)Gait ofPPOtrained withFastTD3-tuned reward",
                "position": 408
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22642/x17.png",
                "caption": "Figure 8:FastSAC.We develop FastSAC, a variant of SAC that incorporates our FastTD3 recipe. We find that FastSAC is significantly faster than vanilla SAC, though still slower than FastTD3.",
                "position": 455
            }
        ]
    },
    {
        "header": "4Discussion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22642/x18.png",
                "caption": "Figure 9:HumanoidBench results.We provide learning curves on a 39 tasks from HumanoidBench(Sferrazza et al.,2024)The solid line and shaded regions represent the mean and standard deviation across three runs.",
                "position": 1063
            },
            {
                "img": "https://arxiv.org/html/2505.22642/x19.png",
                "caption": "Figure 10:IsaacLab results.We provide learning curves on six tasks from IsaacLab(Mittal et al.,2023). The solid line and shaded regions represent the mean and standard deviation across three runs.",
                "position": 1071
            },
            {
                "img": "https://arxiv.org/html/2505.22642/x20.png",
                "caption": "Figure 11:MuJoCo Playground results.We provide learning curves on four tasks from MuJoCo Playground(Zakka et al.,2025). The solid line and shaded regions represent the mean and standard deviation across three runs.",
                "position": 1078
            }
        ]
    },
    {
        "header": "Appendix AAdditional Results",
        "images": []
    }
]