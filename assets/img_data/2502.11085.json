[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11085/extracted/6207443/images/pull_figure.png",
                "caption": "Figure 1:Pretraining on a High-Quality, Task-Relevant Dataset.Pretraining on a carefully selected high-quality dataset achieves comparable or superior mean absolute error (MAE) across tasks while reducing computational cost by a factor of 24 compared to JMP-S, which is pretrained on all upstream datasets. Lower MAE indicates better performance.",
                "position": 102
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11085/extracted/6207443/images/pipeline.png",
                "caption": "Figure 2:Pipeline Overview. Our paradigm for pretraining and finetuning consists of two new components: (1)Dataset Selection Stage, where a distance metricδ𝛿\\deltaitalic_δis employed to identify the dataset that is most similar to our downstream task dataset𝒟dsubscript𝒟𝑑\\mathcal{D}_{d}caligraphic_D start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT, in this case𝒟u(1)superscriptsubscript𝒟𝑢1\\mathcal{D}_{u}^{(1)}caligraphic_D start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT. This selected dataset is then used for pretraining the model. (2)Limited Budget Pretraining, where we impose a training budget by subsampling𝒩𝒩\\mathcal{N}caligraphic_Nrandom samples from𝒟u(1)superscriptsubscript𝒟𝑢1\\mathcal{D}_{u}^{(1)}caligraphic_D start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPTand training the model forℰℰ\\mathcal{E}caligraphic_Eepochs. This results in a computational budget of𝒞=ℰ×𝒩𝒞ℰ𝒩\\mathcal{C}=\\mathcal{E}\\times\\mathcal{N}caligraphic_C = caligraphic_E × caligraphic_N. The pretrained backboneθb(1)⁣∗superscriptsubscript𝜃𝑏1\\theta_{b}^{(1)*}italic_θ start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 1 ) ∗ end_POSTSUPERSCRIPTis subsequently finetuned on the downstream task dataset𝒟dsubscript𝒟𝑑\\mathcal{D}_{d}caligraphic_D start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPTto obtain the final model parametersθd∗superscriptsubscript𝜃𝑑\\theta_{d}^{*}italic_θ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT.",
                "position": 129
            }
        ]
    },
    {
        "header": "3Formulation and Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11085/extracted/6207443/images/CSI_bar_balanced_flat_equiformerV2_ID.png",
                "caption": "Figure 3:Alignment Between Upstream and Downstream Using CSI.We assess how well the extracted representations from each upstream dataset align with downstream tasks using our CSI metric, where lower values indicate stronger alignment. ANI-1x demonstrates the closest feature alignment with downstream tasks, whereas OC20 and OC22 show the weakest alignment.",
                "position": 278
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11085/extracted/6207443/images/highCSI.png",
                "caption": "Figure 4:Impact of Adding Less Relevant Pretraining Data.Adding1⁢M1𝑀1M1 italic_MOC22 samples to a2⁢M2𝑀2M2 italic_M-sample ANI-1x baseline worsens downstream performance despite a larger pretraining budget. This highlights the importance of dataset relevance and the CSI metric for effective pretraining.",
                "position": 648
            },
            {
                "img": "https://arxiv.org/html/2502.11085/extracted/6207443/images/CSI_bar_balanced_flat_equiformerV2_OOD.png",
                "caption": "Figure 5:CSI Between Upstream and OOD Downstream Tasks.CSI values predict that ANI-1x is the best pretraining choice for QMOF, while OC20 and OC22 are best for MatBench.",
                "position": 732
            }
        ]
    },
    {
        "header": "5Beyond In-Distribution",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Epochs or More Data?",
        "images": []
    },
    {
        "header": "Appendix BAdditional Analysis on the Metric Design",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11085/extracted/6207443/images/CSI_bar_balanced_mean_equiformerV2_ID.png",
                "caption": "Figure 6:Impact of using mean aggregation instead of flattening on CSI values.We notice that the mean pooling incorrectly reduced the score for OC22 potentially due to over-smoothing.",
                "position": 1576
            },
            {
                "img": "https://arxiv.org/html/2502.11085/extracted/6207443/images/CSI_bar_random_flat_equiformerV2_ID.png",
                "caption": "Figure 7:Impact of using random sampling strategy instead of class-balanced sampling.As highlighted in the long-tail analysis in AppendixC, random sampling can lead to class underrepresentation, potentially affecting the correlation between upstream and downstream tasks. Notably, both ANI-1x and Transition-1x exhibit different patterns compared to the class-balanced values reported in the main paper.",
                "position": 1579
            },
            {
                "img": "https://arxiv.org/html/2502.11085/extracted/6207443/images/CSI_bar_balanced_flat_JMP_ID.png",
                "caption": "Figure 8:The impact of using another backbone. We use JMP pretrained model and show that similar insights are obtained where Ani-1x is shown as the most similar.",
                "position": 1582
            }
        ]
    },
    {
        "header": "Appendix CLong-Tail Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11085/extracted/6207443/images/upstream_sampling.png",
                "caption": "Figure 9:Impact of sampling strategies on subset construction for feature extraction.We sample 10K instances for each upstream task, highlighting the differences in class coverage between random and class-balanced sampling.",
                "position": 1593
            }
        ]
    },
    {
        "header": "Appendix DImplementation Details",
        "images": []
    }
]