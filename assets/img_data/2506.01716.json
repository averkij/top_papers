[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01716/x1.png",
                "caption": "Figure 1:Overview of Self-Challenging Agent. The agent takes on two roles: task challenger, and task executor. The task challenger proposes a task along with a verification method to verify the solution to the task. The task executor generates a solution and obtains a reward from the environment based on the verification method.",
                "position": 122
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Problem Setup",
        "images": []
    },
    {
        "header": "4Self-Challenging Agents",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01716/x2.png",
                "caption": "Figure 2:An example of a syntheticCode-as-Task (CaT)generated by the task challenger, in a TauBench-based environment[46]. The task challenger interacts with the environment taking a series of actions calling different tools to gather information, before generating the synthetic task, consisting of an instruction, verification function, example solution and failure cases (shortened in the figure for brevity). Automatic filtering is applied to CaTs to keep only valid tasks where the example solution can pass the verification function and the failure cases cannot.",
                "position": 215
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01716/x3.png",
                "caption": "Figure 3:Ablation studies of different RL algorithmswith synthetic tasks generated from SCA, in the Calculation environment from M3ToolEval. Pass@1 success rates are reported. We find that online RL algorithms in general attain even better performance on out-of-distribution test sets, but they are more unstable and require more careful tuning.",
                "position": 466
            },
            {
                "img": "https://arxiv.org/html/2506.01716/x4.png",
                "caption": "Figure 4:Human annotations of synthetic task qualities.50 rollout trajectories from Llama-3.1-8B in the Retail environment from attempted synthetic tasks from each variant are manually labeled to fall into one of the four categories including False Negative (FN), False Positive (FP), True Negative (TN), and True Positive (TP). The pass rates of the task challenger generating a task passing all filters for each category are shown in parentheses. We observe that CaT can significantly reduce both FN and FP, which are invalid tasks or wrongly labeled trajectories.",
                "position": 486
            },
            {
                "img": "https://arxiv.org/html/2506.01716/x5.png",
                "caption": "Figure 5:Analysis of the distribution of task difficultybefore and after the filtering step of CaT in the Retail environment. The task difficulty is represented by the length of the example solution. The percentages of passing tasks after filtering are included in parentheses. We observe that CaT filtering can result in a less diverse task distribution for the less capable Llama-3.1-8B but preserves the original task distribution for the stronger Llama-3.1-70B model.",
                "position": 489
            },
            {
                "img": "https://arxiv.org/html/2506.01716/x6.png",
                "caption": "Figure 6:Scaling analysisfor SCA with different number of tasks in M3ToolEval Calculation. The results show that scaling the number of synthetic tasks is more effective compared to scaling the number of trajectories per task on the test data. In contrast, on the train data a smaller number of tasks but more trajectories gives a higher success rate, but this does not generalize out-of-distribution.",
                "position": 499
            }
        ]
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Environment Details",
        "images": []
    },
    {
        "header": "Appendix BAdditional Discussions",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01716/x7.png",
                "caption": "Figure 7:Comparison with oracle tasksin M3ToolEval Calculation with PPO policy optimization. Both training set contains 800 tasks. While training on synthetic tasks generated from SCA can result in significant improvements on out-of-distribution test tasks, there is still a gap compared to training on oracle tasks.",
                "position": 1339
            }
        ]
    },
    {
        "header": "Appendix CDetails for Human Annotations",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01716/extracted/6499953/figures/annotation_interface.png",
                "caption": "Figure 8:Annotation interfacethat we have build for human annotation results presented in Figure4.",
                "position": 1465
            }
        ]
    },
    {
        "header": "Appendix DBroader Impact",
        "images": []
    },
    {
        "header": "Appendix EPrompts",
        "images": []
    },
    {
        "header": "Appendix FCompute Usage",
        "images": []
    },
    {
        "header": "Appendix GHyperparameters",
        "images": []
    },
    {
        "header": "Appendix HQualitative Examples for Synthetic Tasks",
        "images": []
    },
    {
        "header": "Appendix IExample Interaction of Task Challenger",
        "images": []
    }
]