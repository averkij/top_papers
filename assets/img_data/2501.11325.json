[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.11325/x1.png",
                "caption": "",
                "position": 87
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.11325/x2.png",
                "caption": "Figure 2:Overview of the CatV2TON architecture. CatV2TON uses DiT[32]as the backbone, with the first DiT block duplicated as the Pose Encoder. The person and garment conditions are concatenated temporally as try-on conditions. The entire trainable portion consists only of the self-attention layers and Pose Encoder, accounting for less than 1/5 of the total parameters.",
                "position": 165
            },
            {
                "img": "https://arxiv.org/html/2501.11325/x3.png",
                "caption": "(a)Overlapping Clip-based Inference",
                "position": 235
            },
            {
                "img": "https://arxiv.org/html/2501.11325/x4.png",
                "caption": "(b)AdaCN",
                "position": 237
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.11325/x5.png",
                "caption": "Figure 4:Qualitative comparison on the ViViD[13]dataset for dresses. We use Stable and OOTD as the short for StableVITON[24]and OOTDiffusion[51]. Additional comparison results are provided in the supplementary materials. Please zoom in for more details.",
                "position": 280
            },
            {
                "img": "https://arxiv.org/html/2501.11325/x6.png",
                "caption": "Figure 5:Qualitative comparison on the ViViD[13]dataset for lower. We use Stable and OOTD as the short for StableVITON[24]and OOTDiffusion[51]. Additional comparison results are provided in the supplementary materials. Please zoom in for more details.",
                "position": 283
            },
            {
                "img": "https://arxiv.org/html/2501.11325/x7.png",
                "caption": "Figure 6:Qualitative comparison on the ViViD[13]dataset for upper. We use Stable and OOTD as the short for StableVITON[24]and OOTDiffusion[51]. Additional comparison results are provided in the supplementary materials. Please zoom in for more details.",
                "position": 286
            },
            {
                "img": "https://arxiv.org/html/2501.11325/x8.png",
                "caption": "Figure 7:Results of video try-on with CatV2TON. CatV2TON can perform video try-on with various types of garments, achieving high consistency in garment texture and shape. Additional comparison results are provided in the supplementary materials. Please zoom in for more details.",
                "position": 289
            },
            {
                "img": "https://arxiv.org/html/2501.11325/x9.png",
                "caption": "Figure 8:Ablation visual results about AdaCN. When AdaCN is not utilized for inference, the clothing parts in the try-on results will exhibit color difference issues, which typically intensify with the increase in video length.",
                "position": 697
            }
        ]
    },
    {
        "header": "5Limitations",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]