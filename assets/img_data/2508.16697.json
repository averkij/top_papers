[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.16697/x1.png",
                "caption": "Figure 1:QueryBandits and Its Success in Mitigating Hallucination.The original queryxtx_{t}induces a hallucinatory output: the LLM calculates 8 integers between 6 and 74/5. QueryBandits, by leveraging the feature vector, selects theExpandrewrite strategy. The rewritten queryxt′x^{\\prime}_{t}generates an accurate output of 9 integers. Noticeably, the feature vectors are different in the rewritext′x^{\\prime}_{t}-subordination (more complex clauses)is now present whilespecialization (query requiring domain-specific knowledge for understanding)is absent - signifying effects of theExpandstrategy.",
                "position": 62
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology and Evaluation Metrics",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.16697/pareto_simplex.png",
                "caption": "(a)ROC–AUC Pareto frontier on the reward-weight simplex.",
                "position": 478
            },
            {
                "img": "https://arxiv.org/html/2508.16697/pareto_simplex.png",
                "caption": "(a)ROC–AUC Pareto frontier on the reward-weight simplex.",
                "position": 481
            },
            {
                "img": "https://arxiv.org/html/2508.16697/soft_rank_heat_linear.png",
                "caption": "(b)Mean-reward ranks (1 = best) of each rewrite arm per dataset under our contextual bandit; color intensity indicates closeness to the top rank.",
                "position": 486
            }
        ]
    },
    {
        "header": "4Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.16697/cumulative_regret.png",
                "caption": "Figure 3:Cumulative rewardaveraged across all datasetsover the no-rewrite baseline for each algorithm (sorted by final performance), highlighting the superior gains achieved by contextual bandits compared to non-contextual learners and static prompt-based rewrites.",
                "position": 630
            },
            {
                "img": "https://arxiv.org/html/2508.16697/feature_var_linear.png",
                "caption": "Figure 4:Contextual Per-Feature Variance by Arm.For each arm, we compute the variance of each binary linguistic feature over all queries on which that arm was chosen. High variance means the bandit frequently switches the arm on that feature’s presence.",
                "position": 782
            },
            {
                "img": "https://arxiv.org/html/2508.16697/feature_var_linear.png",
                "caption": "Figure 4:Contextual Per-Feature Variance by Arm.For each arm, we compute the variance of each binary linguistic feature over all queries on which that arm was chosen. High variance means the bandit frequently switches the arm on that feature’s presence.",
                "position": 785
            },
            {
                "img": "https://arxiv.org/html/2508.16697/feature_contrib_raw_lin.png",
                "caption": "Figure 5:Contextual Feature Contribution Strength.These are the averagedθ\\thetaweights (direct contributions) of each feature to the expected reward under each arm. Positive weights indicate features that boost that arm’s reward; negative weights indicate features thatpenalizeit.",
                "position": 790
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Disclaimer",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.16697/fraction_linear.png",
                "caption": "(a)Arm Diversity for Contextual Bandits, as a Fraction of Trials.",
                "position": 2661
            },
            {
                "img": "https://arxiv.org/html/2508.16697/fraction_linear.png",
                "caption": "(a)Arm Diversity for Contextual Bandits, as a Fraction of Trials.",
                "position": 2664
            },
            {
                "img": "https://arxiv.org/html/2508.16697/best_arm_nonlin.png",
                "caption": "(b)Arm Diversity for Non-Contextual Bandits, as a Fraction of Trials.",
                "position": 2669
            },
            {
                "img": "https://arxiv.org/html/2508.16697/soft_rank_heat_nor.png",
                "caption": "(a)Soft Rank Heatmap for all Bandits, including armNo Rewrite.",
                "position": 2853
            },
            {
                "img": "https://arxiv.org/html/2508.16697/soft_rank_heat_nor.png",
                "caption": "(a)Soft Rank Heatmap for all Bandits, including armNo Rewrite.",
                "position": 2856
            },
            {
                "img": "https://arxiv.org/html/2508.16697/fraction_nor.png",
                "caption": "(b)Arm Diversity when includingNo Rewrite.",
                "position": 2861
            },
            {
                "img": "https://arxiv.org/html/2508.16697/feature_var_linear.png",
                "caption": "(a)Contextual Model Feature Variance.",
                "position": 2957
            },
            {
                "img": "https://arxiv.org/html/2508.16697/feature_var_linear.png",
                "caption": "(a)Contextual Model Feature Variance.",
                "position": 2960
            },
            {
                "img": "https://arxiv.org/html/2508.16697/feature_var_nonlin.png",
                "caption": "(b)Non-Contextual Model Feature Variance.",
                "position": 2965
            },
            {
                "img": "https://arxiv.org/html/2508.16697/kl_linear.png",
                "caption": "(a)Contextual Model KL Distance.",
                "position": 2972
            },
            {
                "img": "https://arxiv.org/html/2508.16697/kl_linear.png",
                "caption": "(a)Contextual Model KL Distance.",
                "position": 2975
            },
            {
                "img": "https://arxiv.org/html/2508.16697/kl_nonlin.png",
                "caption": "(b)Non-Contextual Model KL Distance.",
                "position": 2980
            },
            {
                "img": "https://arxiv.org/html/2508.16697/feature_contrib_raw_lin.png",
                "caption": "(a)Contextual Model Raw Feature Strength.",
                "position": 2987
            },
            {
                "img": "https://arxiv.org/html/2508.16697/feature_contrib_raw_lin.png",
                "caption": "(a)Contextual Model Raw Feature Strength.",
                "position": 2990
            },
            {
                "img": "https://arxiv.org/html/2508.16697/feature_contrib_raw_nonlincb.png",
                "caption": "(b)Non-Contextual Model Raw Feature Strength.",
                "position": 2995
            },
            {
                "img": "https://arxiv.org/html/2508.16697/feature_contrib_lin.png",
                "caption": "(a)Contextual Model Relative Feature Strength.",
                "position": 3215
            },
            {
                "img": "https://arxiv.org/html/2508.16697/feature_contrib_lin.png",
                "caption": "(a)Contextual Model Relative Feature Strength.",
                "position": 3218
            },
            {
                "img": "https://arxiv.org/html/2508.16697/feature_contrib_nonlincb.png",
                "caption": "(b)Non-Contextual Model Relative Feature Strength.",
                "position": 3223
            },
            {
                "img": "https://arxiv.org/html/2508.16697/x2.png",
                "caption": "(a)Contextual Model Feature Uplift.",
                "position": 3383
            },
            {
                "img": "https://arxiv.org/html/2508.16697/x2.png",
                "caption": "(a)Contextual Model Feature Uplift.",
                "position": 3386
            },
            {
                "img": "https://arxiv.org/html/2508.16697/x3.png",
                "caption": "(b)Non-Contextual Model Feature Uplift.",
                "position": 3391
            },
            {
                "img": "https://arxiv.org/html/2508.16697/feature_matchups.png",
                "caption": "Figure 13:Pairwise Normalized Coefficient Differences for Contextual Bandits.Each cell shows the min–max–normalized difference in regression weight\nfor a given linguistic feature (rows) between two rewrite arms\n(columns), e.g. “Paraphrase vs Disambiguate,” “Simplify vs Expand,” etc.\nCells labeled “Win” (blue) indicate the feature favors the first arm in\nthe matchup, while “Loss” (red) indicates it favors the second. Values\nare expressed as a percentage of the feature’s full coefficient range.",
                "position": 3400
            }
        ]
    },
    {
        "header": "Appendix AAppendix / supplemental material",
        "images": []
    }
]