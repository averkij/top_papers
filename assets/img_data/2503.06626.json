[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06626/x1.png",
                "caption": "Figure 1:CC3M Pretraining: CLIP vs. DiffCLIP Across Six Tasks.We compare standard CLIP (blue) and our DiffCLIP variant (pink) on linear probing, few-shot classification, image/text retrieval, zero-shot ImageNet, and zero-shot OOD.\nIn each case, DiffCLIP consistently outperforms CLIP, highlighting the effectiveness of differential attention with only 0.003% extra parameters.",
                "position": 121
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06626/extracted/6264640/figure/images.png",
                "caption": "Figure 2:Comparing CLIP vs. DiffCLIP Attention Maps.For two images (rows), we visualize where CLIP and DiffCLIP attend when matching each image against two different textual queries.\nWhile CLIP allocates attention to irrelevant background regions, DiffCLIP more effectively centers on query-relevant objects, highlighting how differential attention can reduce noise and improve focus.Queries:First Row:‘Mug”, Lamp”;Second Row:Flower”, Dog”.",
                "position": 173
            }
        ]
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06626/x2.png",
                "caption": "Figure 3:OOD Zero-Shot ImageNet Performance.Comparison of zero-shot accuracy (%) on ImageNet, ImageNet-V2, ImageNet-A, ImageNet-R, and ImageNet-Sketch, plus the average. Bars show performance of CLIP (blue) versus DiffCLIP (pink), trained on CC3M (left) or CC12M (right). Numerical deltas above the bars indicate the absolute improvement or drop for DiffCLIP relative to CLIP. DiffCLIP improves on average the zero-shot performance on OOD ImageNet datasets as compared to CLIP.",
                "position": 720
            },
            {
                "img": "https://arxiv.org/html/2503.06626/x3.png",
                "caption": "Figure 4:MMVP-VLM Benchmarking.Radar plot illustrating performance on different fine-grained visual categories. Both models (CLIP in blue, DiffCLIP in pink) are evaluated on properties like orientation, positional context, and color appearance. DiffCLIP (average 27.6%) consistently outperforms CLIP (average 21.9%), demonstrating more focused attention on subtle visual details.",
                "position": 811
            },
            {
                "img": "https://arxiv.org/html/2503.06626/x4.png",
                "caption": "Figure 5:Comparing Different DiffCLIP Variants.We evaluate four models on six tasks (linear probing, few-shot, image retrieval, text retrieval, ImageNet zero-shot, and zero-shot OOD), all pretrained on CC12M.\nCLIP (blue) is the baseline, DiffCLIP (pink) uses a fixed differential attention parameter,\nDiffCLIP∗(purple) employs a dynamic schedule for differential attention,\nand DiffCLIP†(yellow) applies differential attention only to the vision encoder.",
                "position": 815
            }
        ]
    },
    {
        "header": "5Future Directions & Limitations",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]