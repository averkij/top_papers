[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background and Motivation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.00269/x1.png",
                "caption": "Figure 1:Typical workflow of SpeechLM inference.",
                "position": 199
            },
            {
                "img": "https://arxiv.org/html/2602.00269/x2.png",
                "caption": "Figure 2:SpeechLMs have diversity in how to represent both text and audio data, including number of codebooks, usage of continuous feature values from audio inputs, and the existence of depth-wise LLM.",
                "position": 265
            },
            {
                "img": "https://arxiv.org/html/2602.00269/x3.png",
                "caption": "Figure 3:(Left) SpeechLM deployment is currently fragmented by bespoke, architecture-specific inference stacks, leading to suboptimal scheduling and resource management, and requires significant engineering cost to adopt a new architecture. (Right) We design a unified serving system that supports diverse SpeechLMs, which enables holistic system optimization.",
                "position": 293
            }
        ]
    },
    {
        "header": "3Design",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.00269/x4.png",
                "caption": "Figure 4:Overview ofVoxServearchitecture. The execution process has three modules: Scheduler for request orchestration, Worker for GPU management, and Model for providing a common abstraction across various SpeechLMs. Together, this design enables holistic and model-agnostic optimization of SpeechLM serving.",
                "position": 315
            },
            {
                "img": "https://arxiv.org/html/2602.00269/x5.png",
                "caption": "Figure 5:Asynchronous pipeline design.VoxServeoverlaps GPU computation with independent CPU-side tasks to reduce scheduling overhead.",
                "position": 423
            }
        ]
    },
    {
        "header": "4Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.00269/x6.png",
                "caption": "Figure 6:Serving performance ofVoxServecompared against existing systems. The x-axis shows the request rate, and the y-axis shows the TTFA latency. For each system, we show the TTFA of p90 and p99. The percentage at each point shows the fraction of audio chunks that satisfied the streaming viability requirement.",
                "position": 459
            },
            {
                "img": "https://arxiv.org/html/2602.00269/x7.png",
                "caption": "Figure 7:TTFA comparison across scheduling strategies, highlighting the benefit of optimizations for streaming modes and asynchronous pipelining.",
                "position": 475
            },
            {
                "img": "https://arxiv.org/html/2602.00269/x8.png",
                "caption": "Figure 8:Multi-GPU serving performance. Top: p90 TTFA with data parallelism across up to four H100 GPUs for CosyVoice. Bottom: p90 TTFA for disaggregated inference across two GPUs for Step-Audio.",
                "position": 491
            },
            {
                "img": "https://arxiv.org/html/2602.00269/x9.png",
                "caption": "Figure 9:Performance for throughput-oriented scenario, measured by total generated audio duration divided by execution latency, for CosyVoice model.",
                "position": 512
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.00269/x10.png",
                "caption": "Figure 10:Serving performance for additional models.",
                "position": 992
            }
        ]
    },
    {
        "header": "Appendix AEvaluation Setup Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.00269/x11.png",
                "caption": "Figure 11:Serving performance with different input data sources.",
                "position": 1049
            }
        ]
    },
    {
        "header": "Appendix BAdditional Evaluation Results",
        "images": []
    }
]