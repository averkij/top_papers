[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16518/x1.png",
                "caption": "Figure 1:Performance Comparison in Autonomous Driving and Embodied AI Benchmarks. MiMo-Embodied achieves state-of-the-art performance on both benchmarks, surpassing previous open-source, closed-source, and specialized VLMs, highlighting its superior capabilities in various autonomous driving and embodied AI tasks.",
                "position": 150
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16518/x2.png",
                "caption": "Figure 2:Overview of MiMo-Embodied Capabilities.MiMo-Embodied supports both Autonomous Driving and Embodied AI tasks, featuring 12 benchmarks in Autonomous Driving that cover Environmental Perception, Status Prediction and Driving Planning, along with 17 benchmarks in Embodied AI tasks focusing on Affordance Prediction, Task Planning, and Spatial Understanding.",
                "position": 307
            }
        ]
    },
    {
        "header": "2Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16518/x3.png",
                "caption": "Figure 3:Model architecture of MiMo-Embodied.The MiMo-Embodied model architecture is designed for embodied AI and autonomous driving tasks, effectively processing single images, multiple images, and videos, and consists of three main components: (1) a Vision Transformer for encoding visual inputs; (2) a projector that maps visual encodings to a latent space aligned with a LLM; and (3) the LLM itself for textual understanding and reasoning.",
                "position": 316
            }
        ]
    },
    {
        "header": "3Training Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16518/x4.png",
                "caption": "Figure 4:Overview of the Training Data used by MiMo-Embodied.Our model comprises three core components of training datasets: the General Dataset establishes foundational capabilities, the Embodied AI Dataset enhances capabilities in affordance, planning, and spatial perception, and the Autonomous Driving Dataset focuses on improving capabilities in perception, prediction, and planning for autonomous driving.",
                "position": 350
            }
        ]
    },
    {
        "header": "4Training Strategy",
        "images": []
    },
    {
        "header": "5Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16518/x5.png",
                "caption": "Table 2:Comparison of MiMo-Embodied with other models on affordance and planning benchmarks.We evaluate the model against various open-source, closed-source, and specialized\nembodied VLMs to show a comprehensive performance overview.\nResults marked with * are obtained using our evaluation framework.\nThe best results among the listed models areboldedand the second-best isunderlined.",
                "position": 698
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x5.png",
                "caption": "Table 3:Comparison of MiMo-Embodied with other models on spatial benchmarks.We evaluate the model against various open-source, closed-source, and specialized\nembodied VLMs to show a comprehensive performance overview.\nResults marked with * are obtained using our evaluation framework.\nThe best results among the listed models areboldedand the second-best isunderlined.",
                "position": 985
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x5.png",
                "caption": "Table 4:Comparison of MiMo-Embodied with other models on four single-view image benchmarks and two multi-view video benchmarks in autonomous driving.We evaluate the model against various open-source, closed-source, and specialized autonomous driving VLMs to show a comprehensive performance overview.\nResults marked with * are obtained using our evaluation framework.†Specialist models\ncorrespond to the performance of different models[66,37,53,14].\nThe best results among the listed models areboldedand the second-best isunderlined. Here PER. denotes perception, PRE. denotes prediction, PLA. denotes planning.",
                "position": 1261
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x5.png",
                "caption": "Table 5:Comparison of MiMo-Embodied with other models on three multi-view image benchmarks and three single-view video benchmarks in autonomous driving.We evaluate the model against various open-source, closed-source, and specialized autonomous driving VLMs to show a comprehensive performance overview.\nResults marked with * are obtained using our evaluation framework.†Specialist models\ncorrespond to the performance of different models[65,7,41,38,29].\nThe best results among the listed models areboldedand the second-best isunderlined. Here PER. denotes perception, PRE. denotes prediction, PLA. denotes planning.",
                "position": 1466
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x5.png",
                "caption": "Figure 5:Results of deploying MiMo-Embodied to downstream embodied navigation tasks.The target positions are indicated by cyan points.",
                "position": 1663
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x6.png",
                "caption": "Figure 6:Visualization of different models for target object localization in embodied navigation tasks.The target positions are indicated by cyan points.",
                "position": 1674
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x7.png",
                "caption": "Figure 7:Results of deploying MiMo-Embodied to downstream embodied manipulation tasks.The target positions are indicated by cyan points.",
                "position": 1712
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x8.png",
                "caption": "Figure 8:Visualization of different models for affordance prediction in embodied manipulation tasks.The target positions are indicated by cyan points.",
                "position": 1718
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x9.png",
                "caption": "Figure 9:Qualitative results of trajectory planning by MiMo-Embodied on the NAVSIM benchmark.",
                "position": 1851
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x10.png",
                "caption": "Figure 10:Relative performance improvement of MiMo-Embodied 7B over baseline method Qwen2.5-VL 7B[5]of trajectory planning on the proprietary dataset. Improvement is quantified as the percentage reduction in error metric, with higher bars indicating greater error reduction and better performance.",
                "position": 1864
            }
        ]
    },
    {
        "header": "6Conclusion and Next Steps",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Contributions and Acknowledgments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16518/x11.png",
                "caption": "Table 8:Comparison of MiMo-Embodied with other models on general visual understanding benchmarks.Results for Qwen2.5-VL[5], InternVL3[70], GPT-4o[25], and Claude 3.7 Sonnet[1]are sourced from the MiMo-VL[59]technical report for a consistent comparison. The best results among the listed models areboldedand the second-best isunderlined.",
                "position": 3195
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x11.png",
                "caption": "Figure 11:Embodied spatial understanding example 1.",
                "position": 3342
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x12.png",
                "caption": "Figure 12:Embodied spatial understanding example 2.",
                "position": 3352
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x13.png",
                "caption": "Figure 13:Embodied spatial understanding example 3.",
                "position": 3355
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x14.png",
                "caption": "Figure 14:Embodied spatial understanding example 4.",
                "position": 3358
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x15.png",
                "caption": "Figure 15:Embodied spatial understanding example 5.",
                "position": 3361
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x16.png",
                "caption": "Figure 16:Embodied spatial understanding example 6.",
                "position": 3364
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x17.png",
                "caption": "Figure 17:Embodied spatial understanding example 7.",
                "position": 3367
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x18.png",
                "caption": "Figure 18:Embodied spatial understanding example 8.",
                "position": 3370
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x19.png",
                "caption": "Figure 19:Embodied affordance prediction example 1.",
                "position": 3373
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x20.png",
                "caption": "Figure 20:Embodied affordance prediction example 2.",
                "position": 3383
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x21.png",
                "caption": "Figure 21:Embodied affordance prediction example 3.",
                "position": 3386
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x22.png",
                "caption": "Figure 22:Embodied affordance prediction example 4.",
                "position": 3389
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x23.png",
                "caption": "Figure 23:Embodied affordance prediction example 5.",
                "position": 3392
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x24.png",
                "caption": "Figure 24:Embodied affordance prediction example 6.",
                "position": 3395
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x25.png",
                "caption": "Figure 25:Embodied affordance prediction example 7.",
                "position": 3398
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x26.png",
                "caption": "Figure 26:Embodied affordance prediction example 8.",
                "position": 3401
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x27.png",
                "caption": "Figure 27:Embodied affordance prediction example 9.",
                "position": 3404
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x28.png",
                "caption": "Figure 28:Embodied affordance prediction example 10.",
                "position": 3407
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x29.png",
                "caption": "Figure 29:Embodied planning task example 1.",
                "position": 3410
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x30.png",
                "caption": "Figure 30:Embodied planning task example 2.",
                "position": 3420
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x31.png",
                "caption": "Figure 31:Embodied planning task example 3.",
                "position": 3423
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x32.png",
                "caption": "Figure 32:Embodied planning task example 4.",
                "position": 3426
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x33.png",
                "caption": "Figure 33:Autonomous driving scene perception example 1.",
                "position": 3432
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x34.png",
                "caption": "Figure 34:Autonomous driving scene perception example 2.",
                "position": 3442
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x35.png",
                "caption": "Figure 35:Autonomous driving scene perception example 3.",
                "position": 3445
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x36.png",
                "caption": "Figure 36:Autonomous driving scene perception example 4.",
                "position": 3448
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x37.png",
                "caption": "Figure 37:Autonomous driving scene perception example 5.",
                "position": 3451
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x38.png",
                "caption": "Figure 38:Autonomous driving scene perception example 6.",
                "position": 3454
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x39.png",
                "caption": "Figure 39:Autonomous driving scene perception example 7.",
                "position": 3457
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x40.png",
                "caption": "Figure 40:Autonomous driving scene perception example 8.",
                "position": 3460
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x41.png",
                "caption": "Figure 41:Autonomous driving prediction ability example 1.",
                "position": 3463
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x42.png",
                "caption": "Figure 42:Autonomous driving prediction ability example 2.",
                "position": 3473
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x43.png",
                "caption": "Figure 43:Autonomous driving prediction ability example 3.",
                "position": 3476
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x44.png",
                "caption": "Figure 44:Autonomous driving planning task example 1.",
                "position": 3479
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x45.png",
                "caption": "Figure 45:Autonomous driving planning task example 2.",
                "position": 3489
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x46.png",
                "caption": "Figure 46:Autonomous driving planning task example 3.",
                "position": 3492
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x47.png",
                "caption": "Figure 47:Autonomous driving planning task example 4.",
                "position": 3495
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x48.png",
                "caption": "Figure 48:Autonomous driving planning task example 5.",
                "position": 3498
            },
            {
                "img": "https://arxiv.org/html/2511.16518/x49.png",
                "caption": "Figure 49:Autonomous driving planning task example 6.",
                "position": 3501
            }
        ]
    },
    {
        "header": "8Appendix",
        "images": []
    }
]