[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09501/x1.png",
                "caption": "",
                "position": 529
            },
            {
                "img": "https://arxiv.org/html/2412.09501/x2.png",
                "caption": "",
                "position": 547
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09501/x3.png",
                "caption": "Figure 2:The framework of Lyra. Lyra supports multi-modal inputs. When the data contains a speech modality, we use the latent cross-modality regularizer to assist. Data from each modality is processed through encoders and projectors before being sent into the LLM. Within the LLM, multi-modality LoRA and latent multi-modality extraction modules operate synergistically, facilitating the simultaneous generation of both speech and text outputs.",
                "position": 1114
            }
        ]
    },
    {
        "header": "3Lyra",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09501/x4.png",
                "caption": "Figure 3:Illustration of the DTW algorithm in our alignment. Our goal is to make the speech tokens as similar as possible to the corresponding translated tokens.",
                "position": 1213
            },
            {
                "img": "https://arxiv.org/html/2412.09501/x5.png",
                "caption": "Figure 4:Long speech capability integration pipeline. (Middle) Our pipeline for generating instruction-following data for long speech. (Top) The proportion of question and speech categories in our long speech SFT dataset. (Bottom) Our long speech SFT pipeline. Long speech segments will be clipped and flattened.",
                "position": 1265
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09501/x6.png",
                "caption": "Figure 5:Visualization of latent multi-modality extractor in various modalities.The upper part is the video modality, and the lower part is the audio modality. Through latent multi-modality information extraction, semantic tokens related to the instruction are retained, reducing the computational cost of the MLLM. The visualization of the image modality and different blocks can be found in theappendix.",
                "position": 3397
            },
            {
                "img": "https://arxiv.org/html/2412.09501/x7.png",
                "caption": "(a)",
                "position": 3400
            },
            {
                "img": "https://arxiv.org/html/2412.09501/x8.png",
                "caption": "",
                "position": 3409
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix ATraining Configuration and Data",
        "images": []
    },
    {
        "header": "Appendix BMore Component-Wise Details & Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09501/x9.png",
                "caption": "Figure 9:Sound capability qualitative results.",
                "position": 4430
            },
            {
                "img": "https://arxiv.org/html/2412.09501/x10.png",
                "caption": "Figure 10:Visualization of latent multi-modality extractor in the image modality.",
                "position": 4433
            }
        ]
    },
    {
        "header": "Appendix CQualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09501/x11.png",
                "caption": "Figure 11:Image-text and video-text qualitative results of Lyra.",
                "position": 4752
            },
            {
                "img": "https://arxiv.org/html/2412.09501/x12.png",
                "caption": "Figure 12:Lyra long speech capability qualitative results for handling daily news.",
                "position": 4756
            },
            {
                "img": "https://arxiv.org/html/2412.09501/x13.png",
                "caption": "Figure 13:More long speech examples results. Lyra achieves more accurate omni-cognition compared to naive VLMs like Qwen2-VL.",
                "position": 4760
            },
            {
                "img": "https://arxiv.org/html/2412.09501/x14.png",
                "caption": "Figure 14:More examples of Lyra with hour-long lectures (more than two hours).",
                "position": 4764
            },
            {
                "img": "https://arxiv.org/html/2412.09501/x15.png",
                "caption": "Figure 15:More results from long speech examples: Lyra can subjectively answer questions about complex steps.",
                "position": 4768
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]