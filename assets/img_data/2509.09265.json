[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.09265/x1.png",
                "caption": "Figure 1:Overview of the EMPG mechanism and its algorithm performance.Left:Conceptual diagram contrasting the uniform credit assignment of baseline methods with EMPG’s confidence-modulated signal.Right:Final performance comparison on key long-horizon benchmarks showing EMPG’s superiority, along with the training dynamics on Musique that highlight its ability to achieve sustained improvement and avoid the baseline’s performance plateau.",
                "position": 190
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Entropy-Modulated Policy Gradients",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.09265/x2.png",
                "caption": "Figure 2:KL Loss dynamics during training for the Qwen2.5-32B-Instruct model. The DAPO baseline (orange) suffers from late-stage instability, evidenced by the sharp, erratic spike in KL Loss. The EMPG-enhanced model (blue) remains stable throughout, showcasing its robustness.",
                "position": 988
            },
            {
                "img": "https://arxiv.org/html/2509.09265/x2.png",
                "caption": "Figure 2:KL Loss dynamics during training for the Qwen2.5-32B-Instruct model. The DAPO baseline (orange) suffers from late-stage instability, evidenced by the sharp, erratic spike in KL Loss. The EMPG-enhanced model (blue) remains stable throughout, showcasing its robustness.",
                "position": 991
            },
            {
                "img": "https://arxiv.org/html/2509.09265/figures/entropy_change_by_bin.png",
                "caption": "Figure 3:Average entropy change after RL fine-tuning within each 5% entropy percentile range. Unlike token-level findings, even low-entropy steps undergo significant changes, validating our step-level analysis.",
                "position": 996
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProof of Proposition1",
        "images": []
    },
    {
        "header": "Appendix BTheoretical Foundation of the EMPG Update Rule",
        "images": []
    },
    {
        "header": "Appendix CExperimental Settings",
        "images": []
    },
    {
        "header": "Appendix DAnalysis of Learning Dynamics",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.09265/x3.png",
                "caption": "(a)WebShop: GRPO",
                "position": 2123
            },
            {
                "img": "https://arxiv.org/html/2509.09265/x3.png",
                "caption": "(a)WebShop: GRPO",
                "position": 2126
            },
            {
                "img": "https://arxiv.org/html/2509.09265/x4.png",
                "caption": "(b)WebShop: DAPO",
                "position": 2131
            },
            {
                "img": "https://arxiv.org/html/2509.09265/x5.png",
                "caption": "(c)ALFWorld: GRPO",
                "position": 2137
            },
            {
                "img": "https://arxiv.org/html/2509.09265/x6.png",
                "caption": "(d)ALFWorld: DAPO",
                "position": 2142
            }
        ]
    },
    {
        "header": "Appendix EAlgorithm Implementation Details",
        "images": []
    }
]