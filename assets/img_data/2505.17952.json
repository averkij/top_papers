[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4AlphaMed",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17952/extracted/6471866/single-sub-dataset.png",
                "caption": "Figure 1:Performance comparison on six medical QA benchmarks.Our models are initialized withLlama3.1-8B-Instruct[45]and trained using minimalist rule-based RL on one of three balanced subsets:MedQA-Sub,MedMCQA-Sub, orPubMedQA-Sub(shown asblue,green, andorangebars, respectively). Despite using only 1,200 examples per subset, all variants of our model achieve substantial improvements over the baseLlama3.1-8B-Instructand match or surpass the strong baselineHuatuoGPT-o1-8Bacross all benchmarks.",
                "position": 289
            },
            {
                "img": "https://arxiv.org/html/2505.17952/extracted/6471866/trainlog.png",
                "caption": "Figure 2:Dataset analysis and training dynamics.Left:Ratio of effective queries over training steps; each curve corresponds to models trained on a specific subset.Middle:Training reward per step for models trained on each subset.Right:Distribution of question lengths (number of tokens) in MedQA, MedMCQA, and PubMedQA[43,44,34].",
                "position": 293
            },
            {
                "img": "https://arxiv.org/html/2505.17952/extracted/6471866/quantity.png",
                "caption": "Figure 3:Effect of data quantity.Average accuracy across six medical QA benchmarks as the number of samples per level increases from 200 to 400, resulting in the total subset size growing from 1,200 to 2,400 examples. ScalingMedQA-SubandMedMCQA-Subleads to consistent performance gains, highlighting the value of informative data. In contrast,PubMedQA-Subshows no improvement, reflecting the limitations of low-informative data sources.",
                "position": 344
            },
            {
                "img": "https://arxiv.org/html/2505.17952/extracted/6471866/diversity.png",
                "caption": "",
                "position": 362
            },
            {
                "img": "https://arxiv.org/html/2505.17952/extracted/6471866/accum-diff.png",
                "caption": "Figure 5:Performance on six benchmarks when training on subsets with increasing difficulty levels (L1 to L6). Each blue dot represents a separately trained model on a subset that includes all data up to the indicated difficulty level; new data are incorporated only through separate training runs, not incrementally during training. While performance on MedXpert[37]increases consistently, trends on other benchmarks vary. Final models trained on the full set (L1â€“L6) generally achieve comparable or superior performance to HuatuoGPT-o1-8B[46].",
                "position": 394
            },
            {
                "img": "https://arxiv.org/html/2505.17952/extracted/6471866/dual-diff.png",
                "caption": "Figure 6:Performance on six benchmarks when training with distinct difficulty groups: easy (L1+L2), medium (L3+L4), and hard (L5+L6). While harder training data improves MedXpert[37]accuracy, performance on other benchmarks declines, suggesting that relying solely on difficult samples may impair general reasoning ability.",
                "position": 397
            },
            {
                "img": "https://arxiv.org/html/2505.17952/extracted/6471866/8b.png",
                "caption": "Figure 7:Comparison of AlphaMed(8B) with prior models on MMLU-ProM[35]and MedXpert[37]. Despite its smaller scale and use of minimalist RL, AlphaMed(8B) outperforms the larger model QwQ-32B[48]and other baselines.",
                "position": 430
            },
            {
                "img": "https://arxiv.org/html/2505.17952/extracted/6471866/8b.png",
                "caption": "Figure 7:Comparison of AlphaMed(8B) with prior models on MMLU-ProM[35]and MedXpert[37]. Despite its smaller scale and use of minimalist RL, AlphaMed(8B) outperforms the larger model QwQ-32B[48]and other baselines.",
                "position": 433
            },
            {
                "img": "https://arxiv.org/html/2505.17952/extracted/6471866/70b.png",
                "caption": "Figure 8:AlphaMed(70B)achieves superior performance over Claude-3.5-Sonnet[49], GPT-4o[47], and DeepSeek-V3 (671B)[50]on MMLU-ProM[35]and MedXpert[37], showcasing its strong reasoning ability.",
                "position": 439
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Limitations and Future Work",
        "images": []
    },
    {
        "header": "Broader Impact",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17952/extracted/6471866/qwen.png",
                "caption": "Figure 9:Performance comparison across six medical QA benchmarks. AlphaMed(7B) is initialized fromQwen2.5-7B-Instruct[52]and trained using our constructed training set and minimalist rule-based RL pipeline. It achieves consistent improvements over the base model on all benchmarks.",
                "position": 1269
            },
            {
                "img": "https://arxiv.org/html/2505.17952/extracted/6471866/3b.png",
                "caption": "Figure 10:Performance comparison across six medical QA benchmarks. AlphaMed(3B) is initialized fromQwen2.5-3B-Instruct[52]and trained with our constructed dataset using a minimalist rule-based RL pipeline. It achieves consistent gains over the base model.",
                "position": 1279
            },
            {
                "img": "https://arxiv.org/html/2505.17952/extracted/6471866/q1q.png",
                "caption": "Figure 11:Question and answer pair for Case 1.Cyan texthighlights the final predicted choices.Green highlightare used to emphasize reasoning steps and key clinically key information.",
                "position": 1289
            },
            {
                "img": "https://arxiv.org/html/2505.17952/extracted/6471866/q1a.png",
                "caption": "",
                "position": 1299
            },
            {
                "img": "https://arxiv.org/html/2505.17952/extracted/6471866/q2q.png",
                "caption": "Figure 12:Question and answer pair for Case 2.Cyan texthighlights the final predicted choices.Green highlightare used to emphasize reasoning steps and key clinically key information.",
                "position": 1308
            },
            {
                "img": "https://arxiv.org/html/2505.17952/extracted/6471866/q2a.png",
                "caption": "",
                "position": 1318
            },
            {
                "img": "https://arxiv.org/html/2505.17952/extracted/6471866/q3.png",
                "caption": "Figure 13:Question and answer pair for Case 3.Cyan texthighlights the final predicted choices.Green highlightare used to emphasize reasoning steps and key clinically key information.",
                "position": 1327
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]