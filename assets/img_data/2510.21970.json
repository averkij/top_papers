[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background and Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.21970/images/overall_model_performance.png",
                "caption": "Figure 1:Overall Model Performance Comparison. The fine-tuned Llama 3.2 1B model and its high-quality quantized variants achieve accuracy scores comparable to the top commercial baselines.",
                "position": 386
            },
            {
                "img": "https://arxiv.org/html/2510.21970/images/memory_comparison.png",
                "caption": "Figure 2:GPU Memory Comparison. GPTQ 4-bit quantization significantly reduces total VRAM, peak VRAM, and the memory required for model parameters alone.",
                "position": 409
            },
            {
                "img": "https://arxiv.org/html/2510.21970/images/load_time_comparison.png",
                "caption": "(a)Load time (s).",
                "position": 415
            },
            {
                "img": "https://arxiv.org/html/2510.21970/images/load_time_comparison.png",
                "caption": "(a)Load time (s).",
                "position": 418
            },
            {
                "img": "https://arxiv.org/html/2510.21970/images/speed_comparison.png",
                "caption": "(b)Tokens per second.",
                "position": 423
            },
            {
                "img": "https://arxiv.org/html/2510.21970/images/energy_comparison.png",
                "caption": "Figure 4:Energy per token on NVIDIA T4. GPTQ consumes substantially more energy per generated token.",
                "position": 430
            },
            {
                "img": "https://arxiv.org/html/2510.21970/images/cpu_inference_performance.png",
                "caption": "Figure 5:CPU Inference Performance. All GGUF-quantized models offer a dramatic speedup over the full-precision baseline.",
                "position": 443
            },
            {
                "img": "https://arxiv.org/html/2510.21970/images/cpu_memory_comparison.png",
                "caption": "Figure 6:CPU Memory Usage Comparison. GGUF quantization reduces RAM consumption by over 90%, from 14.39 GB to less than 1.6 GB.",
                "position": 449
            },
            {
                "img": "https://arxiv.org/html/2510.21970/images/cpu_load_time_comparison.png",
                "caption": "Figure 7:CPU Load Time Comparison. Quantized GGUF models load significantly faster than the full-precision version.",
                "position": 452
            },
            {
                "img": "https://arxiv.org/html/2510.21970/images/pareto_memory.png",
                "caption": "Figure 8:Pareto Frontier: Accuracy vs. Memory Usage (CPU). The Q5_K_M model offers the highest accuracy with minimal memory usage, while the Q3_K_M model is suboptimal due to its low accuracy.",
                "position": 462
            },
            {
                "img": "https://arxiv.org/html/2510.21970/images/pareto_speed.png",
                "caption": "Figure 9:Pareto Frontier: Accuracy vs. Generation Speed (CPU). The Q4_K_M model provides the highest speed, while the Q5_K_M model offers the best accuracy, presenting a clear trade-off.",
                "position": 465
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]