[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.21845/x1.png",
                "caption": "Figure 1:Overview of experimental tasks.A subset of tasks considered in this paper, they include whipping out a Jenga block from its tower, flipping an object in a pan, assembling complex devices such as a timing belt, a dashboard, a motherboard, and an IKEA shelf.",
                "position": 232
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Human-in-the-Loop Reinforcement Learning System",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.21845/x2.png",
                "caption": "Figure 2:Overview of HIL-SERL.This figure illustrates the architecture of HIL-SERL, which comprises three primary components: the actor process, the learner process, and replay buffers. These components communicate asynchronously to facilitate efficient data flow. The actor process receives updated policy parameters from the learner process, interacts with the environment, and sends collected interaction data to the replay buffers. The environment is modular, supporting various external devices and multiple robotic arms. A human operator can intervene via teleoperation tools, such as a SpaceMouse.\nThe learner process samples data evenly from two replay buffers and updates the policy using RLPD. When gripper control is required, a grasp critic is additionally trained with DQN.",
                "position": 353
            }
        ]
    },
    {
        "header": "4Experiment Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.21845/x3.png",
                "caption": "Figure 3:Illustrations of the tasks in our experiments.(A)-(E)A sequence of motherboard assembly tasks: SSD installation, RAM insertion, USB cable grasping and insertion into a slot and a clip, and booting up the computer to ensure motherboard functionality.(F)A manipulation sequence to assemble an IKEA furniture: the robot first assembles two side panels, then installs the top panel onto the mounted side panels.(G)A manipulation sequence to assemble a car dashboard, two robot arms first grasp the workpiece then align multiple pins to the slots.(H)Two arms performing a coordinated handover task.(I)Two arms performing a timing belt installation task.(J)A manipulation sequence of Jenga whipping task, where the robot needs to extract one Jenga piece from the tower without crashing it.(K)The robot is flipping the object in the pan to the opposite side.",
                "position": 494
            },
            {
                "img": "https://arxiv.org/html/2410.21845/x4.png",
                "caption": "Figure 4:Learning curves for experimental tasks.This figure presents the success rate, cycle time, and intervention rates for both HIL-SERL and DAgger across few representative tasks, displayed as a running average over 20 episodes.\nFor HIL-SERL, the success rate increased rapidly throughout training, eventually reaching 100%, while the intervention rate and cycle time progressively decreased, with the intervention rate ultimately reaching 0%.\nFor HG-DAgger, the success rate fluctuates throughout training episodes and does not necessarily increase as training progresses. Since interventions occur frequently, leading to successful outcomes, the true policy success rate is likely lower than the curve suggests. Additionally, the intervention rate does not consistently decrease over time, indicating that the policy is not steadily improving. This is reflected in the cycle time as well, which shows no improvement, as DAgger lacks a mechanism to enhance performance beyond the provided training data. Additional plots are available in the supplementary material.",
                "position": 1116
            },
            {
                "img": "https://arxiv.org/html/2410.21845/x5.png",
                "caption": "Figure 5:Robustness evaluation for policies learned by our method.(A)RAM insertion under external perturbations, such as a moving motherboard.(B)Retrying behavior during a handover task after the grippers are forced open.(C-D)Reactive responses in the timing belt task, addressing both external disturbances and unexpected deformations during execution.(E-F)In the dashboard assembly task, the policy performs re-grasps after one or both grippers are forcibly opened.(G-H)In the USB grasp-insertion task, the policy adapts to external disturbances and poor grasps by releasing and regrasping the object.",
                "position": 1167
            }
        ]
    },
    {
        "header": "5Result Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.21845/extracted/5980976/figs/fig6.png",
                "caption": "Figure 6:Visualization of policy training dynamics.(A)State visitation heatmaps during HIL-SERL training: The policy progressively forms a â€œfunnel\" shape, concentrating more on areas around the demonstrations and corrections, showing robustification in these regions.(B)Q-value variance scatter plots throughout training: States within the funnel show increased Q-value variance, indicating that the policy is gaining greater confidence in actions that lead to successful outcomes.(C)Q-value scatter plots across training: Critical states, marked by higher Q-value variance, are also associated with high Q-values.(D)State visitation heatmaps during HG-DAgger training: The funnel shape is less pronounced, more diffused distribution of visitation density.",
                "position": 1231
            },
            {
                "img": "https://arxiv.org/html/2410.21845/x6.png",
                "caption": "Figure 7:Reactive vs Predictive Behavior.(A-D)A sequence of reactive behaviors in the dashboard assembly task: after getting stuck in contact, the policy breaks the contact by quickly lifting two arms, then re-establishing the contact when approaching the target, finally succeeding in the insertion.(E)Variance plots from trained Gaussian policies in the RAM insertion task, showing three trajectories. Initial variance is high but rapidly decreases as the target is approached.(F)Mean plots from trained Gaussian policies in the RAM insertion task, with values ranging from -1 to 1.(G)Variance plots in the Jenga whipping task, remaining consistently low (near 0), indicating stable execution and open-loop behavior.(H)Mean plots in the Jenga whipping task, with values between -1 and 1, demonstrating consistent behavior across three trajectories.",
                "position": 1273
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Supplementary Materials",
        "images": []
    },
    {
        "header": "Appendix ATask Setup and Policy Training Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.21845/extracted/5980976/figs/sup_motherboard_setup.png",
                "caption": "Figure 8:Hardware setup for the motherboard assembly task.",
                "position": 2870
            },
            {
                "img": "https://arxiv.org/html/2410.21845/extracted/5980976/figs/sup_ram_cropped_image.png",
                "caption": "Figure 9:Sample input images from cameras used as inputs to the policy.",
                "position": 2879
            },
            {
                "img": "https://arxiv.org/html/2410.21845/extracted/5980976/figs/sup_ssd_cropped_image.png",
                "caption": "Figure 10:Sample input images from cameras used as inputs to the policy.",
                "position": 2982
            },
            {
                "img": "https://arxiv.org/html/2410.21845/extracted/5980976/figs/sup_usb_insert_cropped_image.png",
                "caption": "Figure 11:Sample input images from cameras used as inputs to the policy.",
                "position": 3085
            },
            {
                "img": "https://arxiv.org/html/2410.21845/extracted/5980976/figs/sup_usb_route_cropped_image.png",
                "caption": "Figure 12:Sample input images from cameras used as inputs to the policy.",
                "position": 3192
            },
            {
                "img": "https://arxiv.org/html/2410.21845/extracted/5980976/figs/sup_ikea_setup.png",
                "caption": "Figure 13:Hardware setup for the IKEA furniture assembly task.",
                "position": 3293
            },
            {
                "img": "https://arxiv.org/html/2410.21845/extracted/5980976/figs/sup_dashboard_setup.png",
                "caption": "Figure 14:Hardware setup for the car dashboard installation task.",
                "position": 3492
            },
            {
                "img": "https://arxiv.org/html/2410.21845/extracted/5980976/figs/sup_dash_cropped_image.png",
                "caption": "Figure 15:Sample input images from cameras used as inputs to the policy.",
                "position": 3501
            },
            {
                "img": "https://arxiv.org/html/2410.21845/extracted/5980976/figs/sup_handover_setup.png",
                "caption": "Figure 16:Hardware setup for the object handover task.",
                "position": 3602
            },
            {
                "img": "https://arxiv.org/html/2410.21845/extracted/5980976/figs/sup_handover_cropped_image.png",
                "caption": "Figure 17:Sample input images from cameras used as inputs to the policy.",
                "position": 3611
            },
            {
                "img": "https://arxiv.org/html/2410.21845/extracted/5980976/figs/sup_timing_belt_setup.png",
                "caption": "Figure 18:Hardware setup for the timing belt assembly task.",
                "position": 3712
            },
            {
                "img": "https://arxiv.org/html/2410.21845/extracted/5980976/figs/sup_timing_belt_cropped_image.png",
                "caption": "Figure 19:Sample input images from cameras used as inputs to the policy.",
                "position": 3721
            },
            {
                "img": "https://arxiv.org/html/2410.21845/extracted/5980976/figs/sup_jenga_setup.png",
                "caption": "Figure 20:Hardware setup for the Jenga whipping task.",
                "position": 3818
            },
            {
                "img": "https://arxiv.org/html/2410.21845/extracted/5980976/figs/sup_jenga_cropped_image.png",
                "caption": "Figure 21:Sample input images from cameras used as inputs to the policy.",
                "position": 3827
            },
            {
                "img": "https://arxiv.org/html/2410.21845/extracted/5980976/figs/sup_object_flipping_setup.png",
                "caption": "Figure 22:Hardware setup for the object flipping task.",
                "position": 3917
            },
            {
                "img": "https://arxiv.org/html/2410.21845/extracted/5980976/figs/sup_egg_flip_cropped_image.png",
                "caption": "Figure 23:Sample input images from cameras used as inputs to the policy.",
                "position": 3926
            }
        ]
    },
    {
        "header": "Appendix BReward Classifier Training Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.21845/x7.png",
                "caption": "Figure 24:Sample images collected to train the reward classifier for the RAM insertion task.",
                "position": 4028
            }
        ]
    },
    {
        "header": "Appendix CDiffusion Policy Training Details",
        "images": []
    },
    {
        "header": "Appendix DRobot Controller and Proprioceptive Information Representation",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.21845/x8.png",
                "caption": "Figure 25:Learning curves for experimental tasks.This figure presents the success rate, cycle time, and intervention rates for both HIL-SERL across all experiment tasks, displayed as a running average over 20 episodes.\nThe success rate increased rapidly throughout training, eventually reaching 100%, while the intervention rate and cycle time progressively decreased, with the intervention rate ultimately reaching 0%.",
                "position": 4174
            }
        ]
    },
    {
        "header": "Appendix EPolicy Training Plots",
        "images": []
    }
]