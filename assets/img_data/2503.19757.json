[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19757/extracted/6301744/Dita.png",
                "caption": "",
                "position": 84
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19757/x1.png",
                "caption": "Figure 1:Illustrations of different generalist robot policy architectures. Left head: the common robot Transformer architecture with discretization actions,e.g., Robot Transformer[8,9]and OpenVLA[32]. Middle head: the Transformer architecture with diffusion action head which denoises the individual continuous action with a small network condition on each embedding from the causal Transformer,e.g., Octo[72]andœÄ0subscriptùúã0\\pi_{0}italic_œÄ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT[5]. Right head: the proposed Dita architecture that denoises actions inherently in an in-context conditioning style.",
                "position": 115
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19757/x2.png",
                "caption": "Figure 2:Our model employs a Transformer-based diffusion architecture, integrating a pretrained CLIP network to extract language instruction tokens. The DinoV2[53]model encodes image observations, followed by a Q-Former that queries features for each image. The instruction tokens, image features, timestep embeddings, and noised action are concatenated to construct a token sequence, which is then fed into the network to denoise the raw actions.",
                "position": 143
            }
        ]
    },
    {
        "header": "4Simulation Experiments",
        "images": []
    },
    {
        "header": "5Real-Robot Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19757/x3.png",
                "caption": "Figure 3:The experimental platform consists of Franka Emika Panda robot arm, Robotiq 2F-85 gripper and RealSense D435i positioned in third-person view.",
                "position": 746
            },
            {
                "img": "https://arxiv.org/html/2503.19757/x4.png",
                "caption": "Figure 4:Quantitative results in real-robot experiments. Each task is manually divided into two sequential steps, except for the last two single-step tasks. In each stacked bar, the light-colored region represents the model‚Äôs success rate in the first stage, while the dark-colored region indicates the contribution of second-stage success to the overall success rate. A larger proportion of the dark-colored region signifies a stronger capability of the model in completing long-horizon tasks. Since the open/close drawer tasks are single-step, they are excluded from the calculation of the average success rate.",
                "position": 749
            },
            {
                "img": "https://arxiv.org/html/2503.19757/x5.png",
                "caption": "Figure 5:Qualitative comparison in real-robot experiments. Failures are highlighted with red circles. For a direct comparison, we initialize the layout consistently across all methods.",
                "position": 793
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AModel and Training Scheme",
        "images": []
    },
    {
        "header": "Appendix BSimulation Benchmarks",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19757/x6.png",
                "caption": "Figure 6:Qualitative results of Dita under variances in Google Robot.",
                "position": 1947
            },
            {
                "img": "https://arxiv.org/html/2503.19757/x7.png",
                "caption": "Figure 7:Qualitative results of Dita on LIBERO benchmark.",
                "position": 1957
            },
            {
                "img": "https://arxiv.org/html/2503.19757/x8.png",
                "caption": "Figure 8:Qualitative results of Dita on CALVIN ABC‚Üí‚Üí\\rightarrow‚ÜíD benchmark.",
                "position": 2013
            },
            {
                "img": "https://arxiv.org/html/2503.19757/x9.png",
                "caption": "Figure 9:Qualitative comparison between Dita (top) and Diffusion Action Head baseline‚Ñ∞Œ∏‚àºsD‚Å¢i‚Å¢f‚Å¢fsuperscriptsubscript‚Ñ∞similar-toùúÉùë†ùê∑ùëñùëìùëì\\mathcal{E}_{\\theta\\sim s}^{Diff}caligraphic_E start_POSTSUBSCRIPT italic_Œ∏ ‚àº italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D italic_i italic_f italic_f end_POSTSUPERSCRIPT(bottom) on ManiSkill2 (PickClutterYCB).",
                "position": 2029
            }
        ]
    },
    {
        "header": "Appendix CReal-Robot Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19757/extracted/6301744/ICLR2025/convergen_analysis.jpg",
                "caption": "Figure 10:Convergence Analysis on OXE dataset[9]. The blue line is DiT Policy, and the orange line is Diffusion action head strategy with the same number of parameters.",
                "position": 2185
            }
        ]
    },
    {
        "header": "Appendix DAnalysis, Ablations, and Discussions",
        "images": []
    }
]