[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21758/x1.png",
                "caption": "Figure 1:High-quality samples from our Lumina-Image 2.0, showcasing its capabilities in ultra-realistic, text generation, artistic versatility, bilingual mastery, logical reasoning, and unified multi-image generation.",
                "position": 75
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Revisiting Lumina-Next",
        "images": []
    },
    {
        "header": "4Lumina-Image 2.0",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21758/x2.png",
                "caption": "Figure 2:Overview of Lumina-Image 2.0, which consists of Unified Captioner and Unified Next-DiT. The Unified Captioner re-captions web-crawled and synthetic images to construct hierarchical text-image pairs, which are then used to optimize Unified Next-DiT with our efficient training strategy.",
                "position": 177
            },
            {
                "img": "https://arxiv.org/html/2503.21758/x3.png",
                "caption": "Figure 3:We compare the Diffusion Transformer architectures between our Unified Next-DiT, and PixArt[15], Lumina-Next[17], Stable Diffusion 3[36], OmniGen[37]and FLUX[9].",
                "position": 180
            },
            {
                "img": "https://arxiv.org/html/2503.21758/x4.png",
                "caption": "Figure 4:The training loss curve with respect to captions with different lengths. The “Avg. Length” represents the average character number.",
                "position": 217
            },
            {
                "img": "https://arxiv.org/html/2503.21758/x5.png",
                "caption": "Figure 5:Illustration of reformulating the Image-Text Attention as an FFN generated by a hyper-network, with its weights and hidden dimensions dynamically determined by the input text token.",
                "position": 220
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21758/x6.png",
                "caption": "Table 5:Comparison of ELO scores evaluated in text-to-image arena from Artificial Analysis1(as of February 23, 2025).",
                "position": 917
            },
            {
                "img": "https://arxiv.org/html/2503.21758/x6.png",
                "caption": "Table 5:Comparison of ELO scores evaluated in text-to-image arena from Artificial Analysis1(as of February 23, 2025).",
                "position": 920
            },
            {
                "img": "https://arxiv.org/html/2503.21758/x7.png",
                "caption": "Table 6:Comparison of ELO scores evaluated in text-to-image arena from Rapidata2(as of February 23, 2025).",
                "position": 988
            },
            {
                "img": "https://arxiv.org/html/2503.21758/x8.png",
                "caption": "Figure 6:Visualization results of multilingual text-to-image generation by our Lumina-Image 2.0, covering five languages: Chinese, Japanese, English, Russian, and German.",
                "position": 1108
            },
            {
                "img": "https://arxiv.org/html/2503.21758/x9.png",
                "caption": "Figure 7:Comparison with ShareGPT4V[46]and Florence[62]in complex scenes and dense text for caption generation. Theblue underlinecorrespond to areas with more detailed and accurate descriptions, whilered underlineandred strikethroughrepresent the incorrect and insufficient descriptions respectively.",
                "position": 1115
            },
            {
                "img": "https://arxiv.org/html/2503.21758/x10.png",
                "caption": "Figure 8:Comparison with ShareGPT4V[46]and Florence[62]in visual understanding and spatial relationships. Theblue underlinecorrespond to areas with more detailed and accurate descriptions, whilered underlineandred strikethroughrepresent the incorrect and insufficient descriptions respectively.",
                "position": 1118
            },
            {
                "img": "https://arxiv.org/html/2503.21758/x11.png",
                "caption": "Figure 9:High-quality image generation examples from Lumina-Image 2.0, showcasing its precise prompt-following ability and its capability to generate highly aesthetic and realistic images across different resolutions.",
                "position": 1125
            },
            {
                "img": "https://arxiv.org/html/2503.21758/x12.png",
                "caption": "Figure 10:Loss curves for the three training stages, showing a steady performance increase in the DPG[30]and GenEval[31]benchmark.",
                "position": 1182
            },
            {
                "img": "https://arxiv.org/html/2503.21758/x13.png",
                "caption": "Figure 11:Ablation study on efficient inference strategy. The performances are measured on a single A100 GPU with batch size 1.",
                "position": 1185
            }
        ]
    },
    {
        "header": "6Limitation",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21758/x14.png",
                "caption": "Figure 12:Generation defects of Lumina-Image 2.0, categorized into overall structural errors, texture detail errors, and text errors.",
                "position": 1209
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]