[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10210/figures/framework_v3.png",
                "caption": "Figure 1.Illustration of theHybridRAG-Benchbenchmarking framework.",
                "position": 312
            }
        ]
    },
    {
        "header": "2.Related Works",
        "images": []
    },
    {
        "header": "3.Problem Definition and Preliminaries",
        "images": []
    },
    {
        "header": "4.Benchmark Construction",
        "images": []
    },
    {
        "header": "5.Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10210/figures/token_distribution.png",
                "caption": "Figure 2.Token usage during KG construction plotted against corpus length. Token cost grows approximately linearly with input size due to proportional input tokens and a fixed number of extraction calls per document. The smooth scaling pattern confirms that EvoKGâ€™s update cost remains predictable and stable across document sizes.",
                "position": 1956
            }
        ]
    },
    {
        "header": "6.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10210/figures/time_distribution.png",
                "caption": "Figure 3.Per-document KG construction latency as a function of corpus length (in characters). Longer documents incur proportionally higher extraction time, and the trend follows the expected near-linear scaling dominated by LLM token processing.",
                "position": 2478
            }
        ]
    },
    {
        "header": "Appendix AAdditional Experiment Results",
        "images": []
    }
]