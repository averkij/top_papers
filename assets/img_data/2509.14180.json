[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Dataset construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.14180/dataset_process_flow.png",
                "caption": "Figure 1:Dataset generation pipeline. Four modular chain-of-thought phases feed into final response generation. Each phase includes LLM-jury validation (not shown) to ensure quality.",
                "position": 428
            }
        ]
    },
    {
        "header": "4Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.14180/overall.png",
                "caption": "Figure 2:LLM-jury evaluation on 504 unseen subreddit queries: stacked bars show Borda-average scores for accuracy (blue), plausibility (orange), and relevance (green); taller bars indicate stronger overall preference. Our 8 B system (fourth from left) outperforms all other sub-14 B models and approaches the 27 B–32 B leaders. The y-axis represents the average Borda points a model has received.",
                "position": 704
            }
        ]
    },
    {
        "header": "5Future Works",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "Data & Code Availability",
        "images": []
    },
    {
        "header": "Ethical Considerations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APrompting Guidelines followed for the generation and evaluation stages",
        "images": []
    },
    {
        "header": "Appendix BModular RAG for Context Analysis",
        "images": []
    },
    {
        "header": "Appendix CDeeper Evaluation Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.14180/repo_results/accuracy_overall.png",
                "caption": "Figure 3:Accuracy (mean raw Borda points per query, averaged over judges). A size-driven lead is visible; the proposed 8B is mid-pack, indicating factual calibration as the primary improvement lever.",
                "position": 2424
            },
            {
                "img": "https://arxiv.org/html/2509.14180/repo_results/plausibility_overall.png",
                "caption": "Figure 4:Plausibility (mean raw Borda points). The proposed 8B clusters near the front and matches or exceeds several larger baselines, reflecting strong logical flow and coherent reasoning.",
                "position": 2427
            },
            {
                "img": "https://arxiv.org/html/2509.14180/repo_results/relevance_overall.png",
                "caption": "Figure 5:Relevance (mean raw Borda points). The proposed 8B ranks immediately behind the top three, ahead of other baselines, indicating consistent mapping from user constraints to concrete answers.",
                "position": 2430
            },
            {
                "img": "https://arxiv.org/html/2509.14180/repo_results/relevance_overall_perbparams.png",
                "caption": "Figure 6:Relevance efficiency: mean raw Borda points per billion parameters (higher is better). The proposed 8B leads, followed by Gemma3-12B-it and Llama3-8B.",
                "position": 2474
            },
            {
                "img": "https://arxiv.org/html/2509.14180/repo_results/plausibility_overall_perbparams.png",
                "caption": "Figure 7:Plausibility efficiency: mean raw Borda points per billion parameters. The proposed 8B ranks first; compact 7–8B baselines are competitive, while very large models show lower utility density.",
                "position": 2477
            },
            {
                "img": "https://arxiv.org/html/2509.14180/repo_results/accuracy_overall_perbparams.png",
                "caption": "Figure 8:Accuracy efficiency: mean raw Borda points per billion parameters. The proposed 8B tops the cohort, indicating that factual calibration gains can be achieved more cheaply than by scaling parameters alone.",
                "position": 2480
            },
            {
                "img": "https://arxiv.org/html/2509.14180/repo_results/category-wise-relevance.png",
                "caption": "Figure 9:Category-wiseRelevance. The proposed 8B model typically sits just behind the leading cluster and near the cohort mean; gaps are largest in edge-case, rule-dense areas (e.g., Auto, Housing, Credit).",
                "position": 2522
            },
            {
                "img": "https://arxiv.org/html/2509.14180/repo_results/category-wise-accuracy.png",
                "caption": "Figure 10:Category-wiseAccuracy. Larger models lead overall; the proposed 8B is mid-pack with smaller gaps in everyday planning tasks and larger gaps where year-/jurisdiction-specific rules dominate (e.g., Housing, Insurance, Taxes).",
                "position": 2554
            },
            {
                "img": "https://arxiv.org/html/2509.14180/repo_results/category-wise-plausibility.png",
                "caption": "Figure 11:Category-wisePlausibility. The proposed 8B delivers coherent reasoning near the leading cluster, with smaller margins in routine planning tasks and larger ones in regulation-dense areas (e.g., Taxes, Retirement).",
                "position": 2580
            }
        ]
    },
    {
        "header": "Appendix DTraining Details",
        "images": []
    },
    {
        "header": "Appendix ESample Model Responses",
        "images": []
    }
]