[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13427/x1.png",
                "caption": "Figure 1:Limited semantic diversity. The top panel shows 9 images generated by the specified TTI model for each given prompt. These prompts, identified byMineTheGap, introduce bias in the generated images. In contrast, the bottom panel presents 3 images retrieved through a Google Photos search, illustrating alternative interpretations of the same prompt.",
                "position": 142
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x2.png",
                "caption": "(a)t-SNE visualization",
                "position": 164
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x2.png",
                "caption": "(a)t-SNE visualization",
                "position": 167
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x3.png",
                "caption": "(b)Prompt generations",
                "position": 172
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x4.png",
                "caption": "(c)Prompt variation generations",
                "position": 177
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Automatic Mining",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13427/x5.png",
                "caption": "Figure 3:Mining pipeline.Three iterations demonstrate the optimization evaluation process. In iterationkk, we rank the population from the previous iteration (represented by the sentences in the orange and purple frames at iterationk−1k-1) and select the top-ranked (shown in the green frame). We then generate mutations for each of the top-ranked sentences and add random sentences to form the next population. This process repeats in the subsequent iterationk+1k+1.\nThroughout all iterations, sentences are retained in the top prompts bucket if their loss improves on previous top prompts.",
                "position": 244
            }
        ]
    },
    {
        "header": "4Ranking Bias",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13427/x6.png",
                "caption": "Figure 4:Measuring bias through least-aligned elements.Comparison between more (left) and less (right) biased scenarios.\nIn the more biased example, we observe gaps in alignment, where certain textual variations fail to find well-matching images. In contrast, the less biased example demonstrates a more varied distribution of similarity scores, indicating that the TTI model successfully spans the range of plausible interpretations.\nWe setα=20th\\alpha=20^{\\text{th}}percentile which results in taking the minimum over the maximum similarities.",
                "position": 277
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x7.png",
                "caption": "Figure 5:Visual example for gender bias.According to both BLS data and our metric, air conditioning installer is the most gender-biased profession.\nTeller is among the least gender-biased and we rank it as least biased.",
                "position": 339
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x7.png",
                "caption": "Figure 5:Visual example for gender bias.According to both BLS data and our metric, air conditioning installer is the most gender-biased profession.\nTeller is among the least gender-biased and we rank it as least biased.",
                "position": 342
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x8.png",
                "caption": "Figure 6:Bias scores across CFG values.Each box plot summarizes the distribution of measured losses across prompts for a given model and CFG setting. As anticipated, as CFG increases, outputs are scored as more biased.",
                "position": 349
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13427/x9.png",
                "caption": "Figure 7:Mined biased prompts for TTI models.For each model, 15 images were generated using a mined prompt.\nThe resulting images are highly similar, exhibiting repetitive semantics.\nAt the bottom of each example, we show images generated from text variations to illustrate the additional concepts the TTI model should incorporate.\nTop pane illustrates open-set mining showing that in FLUX.1 Schnell, the dog consistently plays on a field of grass, and SD 2.1 consistently zooms in on violets.\nPrompts in bottom pane were mined specifically on gender, race, and age (left) or style (right). FLUX.1 Schnell generated only male scientists, while associated variations exhibit greater sociodemographic diversity, including female scientists of different races and age.\nSimilarly, SD 1.4 generates only photorealistic images, while variations include alternative artistic styles.",
                "position": 426
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x10.png",
                "caption": "Figure 8:Illustrative segment of the mining process.Points represent prompts, with colors distinguishing random prompts from those generated by mutation.\nGray lines connect the top five selected prompts in each iteration to their mutations in the next iteration.\nDashed line marks the overall best loss up to each iteration.",
                "position": 442
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x10.png",
                "caption": "Figure 8:Illustrative segment of the mining process.Points represent prompts, with colors distinguishing random prompts from those generated by mutation.\nGray lines connect the top five selected prompts in each iteration to their mutations in the next iteration.\nDashed line marks the overall best loss up to each iteration.",
                "position": 445
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x11.png",
                "caption": "Figure 9:Cross-model bias evaluation.Each model is evaluated for bias on prompts mined for all other models, by generating corresponding images.E.g. second boxplot from the left depicts the bias scores for SD 1.4 on the biased prompts mined for SD 2.1.\nEach model achieves its lowest score on prompts mined specifically for it.",
                "position": 452
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x12.png",
                "caption": "Figure 10:Human perceived bias of mined prompt.\nFor each model, we report the percentage of users perceiving more bias in images generated for mined prompts versus random prompts. Error bars indicate95%95\\%Wilson confidence intervals. Across all models, mined prompts were perceived as more biased.",
                "position": 462
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x12.png",
                "caption": "Figure 10:Human perceived bias of mined prompt.\nFor each model, we report the percentage of users perceiving more bias in images generated for mined prompts versus random prompts. Error bars indicate95%95\\%Wilson confidence intervals. Across all models, mined prompts were perceived as more biased.",
                "position": 465
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x13.png",
                "caption": "Figure 11:Correlation between score difference and human perception.Scatter plot showing, for each question, the difference in our bias scores and the fraction of participants selecting the corresponding prompt as less biased. The bias score correlates strongly with human preferences (Pearsonr=0.71r=0.71).",
                "position": 472
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x14.png",
                "caption": "Figure 12:Frequency of bias categories.Histograms show how often each of the1515global bias categories was assigned to the mined prompts based on their missed visual concepts. Setting-related categories appear most frequently across models.",
                "position": 513
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x15.png",
                "caption": "Figure 13:Comparison with OpenBias.\nPerson-related queries (gender, activity, age) cannot be answered when no person is present, and food type is already specified in the prompt. In contrast, our concepts (identity, location) are more diverse and reveal the underlying bias.",
                "position": 517
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AAnalysis of the Optimization Process",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13427/x16.png",
                "caption": "Figure S1:Convergence of the mining procedure on a synthetic task.For each iteration, the best (lowest loss) image is shown. The optimization minimizes the MSE between a generated image and a solid color image (blue, red, or green). Within four iterations, the method reliably finds prompts that produce images dominated by the target color.",
                "position": 1011
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x17.png",
                "caption": "Figure S2:Losses for random vs. mined prompts.Box plots show the distribution of bias scores for captions from COCO (blue), for prompts randomly sampled at initialization (yellow), and for prompts mined through our optimization method (green). Across all models, mined prompts consistently yield lower scores, indicating stronger model biases as measured by our metric.",
                "position": 1022
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x17.png",
                "caption": "Figure S2:Losses for random vs. mined prompts.Box plots show the distribution of bias scores for captions from COCO (blue), for prompts randomly sampled at initialization (yellow), and for prompts mined through our optimization method (green). Across all models, mined prompts consistently yield lower scores, indicating stronger model biases as measured by our metric.",
                "position": 1025
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x18.png",
                "caption": "Figure S3:Loss trajectories for mining versus random prompt generation.At each of 25 iterations, the random baseline generates 15 new prompts using an LLM, while mining follows the proposed mining process. Curves show the mean of the best loss across 10 runs per model.\nMining consistently achieves lower losses than random generation, indicating its effectiveness in identifying biased prompts.",
                "position": 1030
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x19.png",
                "caption": "Figure S4:Loss trajectories by population sizebb.Curves show the mean of the best loss across 10 runs per population size with SD 2, with shaded regions showing the SEM.\nLarger populations evaluate more candidates per iteration and therefore identify lower-loss (more biased) prompts earlier in the optimization, at the cost of increased NFEs.",
                "position": 1215
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x20.png",
                "caption": "Figure S5:Correlation of the bias score with human judgment as a function ofα\\alpha.Pearson correlation between score differences and user-study bias judgments for values ofα\\alpha. The correlation remains positive across values, with a peak at the25th25^{\\text{th}}percentile used throughout the paper.",
                "position": 1219
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x21.png",
                "caption": "(a)Instructions presented to the user.",
                "position": 1236
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x21.png",
                "caption": "(a)Instructions presented to the user.",
                "position": 1239
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x22.png",
                "caption": "(b)Example for a single question.",
                "position": 1245
            }
        ]
    },
    {
        "header": "Appendix BSupplementary Results for Bias Score Validation",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13427/x23.png",
                "caption": "Figure S7:Validating the bias loss with CADS sampling.Box plots of bias scores for images generated by Stable Diffusion 1.4 and 2.1 for random prompts, with and without CADS.\nCADS sampling, which is known to increase diversity, is seen to increases the scores (i.e., lowers measured bias), supporting that our metric captures bias.",
                "position": 1660
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x24.png",
                "caption": "Figure S8:Visual impact of CADS on TTI output diversity.For each prompt, the top row shows images generated with the standard sampling method, while the bottom row shows results after applying CADS sampler. Consistent with the findings inFig.S7, CADS has a stronger effect on SD 1.4 (top) by increasing visual diversity, whereas its impact on SD 2.1 (bottom) is more limited.",
                "position": 1670
            }
        ]
    },
    {
        "header": "Appendix CExperimental details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13427/x25.png",
                "caption": "Figure S9:Comparing sets of images generated with the prompt “A photo of a doctor”.FLUX.1 Schnell fails to depict the concept of a female doctor, and indeed the11th11^{\\text{th}}variation is flagged as a missed visual concept, together with variations 7, 8, and 10.\nFor the remaining models (from top to bottom), the variations identified as missed visual concepts are {1,6,8,15}, {6,7,8,15}, and {1,6,7,10}.",
                "position": 2138
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x26.png",
                "caption": "Figure S10:Visualization of the missed visual concepts in prompts mined for Qwen-Image.",
                "position": 2225
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x27.png",
                "caption": "Figure S11:Visualization of the missed visual concepts in prompts mined for FLUX.1 Schnell.",
                "position": 2228
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x28.png",
                "caption": "Figure S12:Visualization of the missed visual concepts in prompts mined for SD 3.",
                "position": 2231
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x29.png",
                "caption": "Figure S13:Visualization of the missed visual concepts in prompts mined for SD 1.4.",
                "position": 2234
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x30.png",
                "caption": "Figure S14:Visualization of the missed visual concepts in prompts mined for SD 2.1.",
                "position": 2237
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x31.png",
                "caption": "Figure S15:Recurring terms in mined prompts.Word clouds generated from the5050most biased prompts mined for each model. The recurrence of words that are related to nature such as field, forest and green, suggests that these terms often result in limited visual diversity.",
                "position": 2240
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x32.png",
                "caption": "Figure S16:Visualization of the missed visual concepts in prompts mined when relaxing the constraint on prompt length.Prompts mined for FLUX.1 Schnell.",
                "position": 2243
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x33.png",
                "caption": "Figure S17:Images generated with the prompt “a photo of a nurse”.Legend indicates TTI model used to generate the images.",
                "position": 2246
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x34.png",
                "caption": "Figure S18:Images generated with variations of the prompt “a photo of a nurse”.All images were generated with SD 3.",
                "position": 2249
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x35.png",
                "caption": "Figure S19:Nearest-neighbor similarity of prompt sets.Mean cosine similarity to nearest-neighbor between prompts mined with Llama, LLaDa and Qwen, and captions from COCO. Prompts mined with different LLMs are more similar to each other than to COCO captions.",
                "position": 2265
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x36.png",
                "caption": "Figure S20:t-SNE visualization of prompts.2D projection of CLIP embeddings for mined prompts (Llama, LLaDA, Qwen) and COCO captions. Mined prompts form overlapping distributions with some clustering, while COCO captions occupy subregions of the space.",
                "position": 2268
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x37.png",
                "caption": "Figure S21:Sociodemographic bias.By controlling the meta prompts used for Llama 3.1-8B-Instruct, we manage to capture specific biases such as gender and race. On the left pane, all images show a male firefighter although the prompt is gender-free. On the right pane, all images show a Caucasian toddler. The variations, on the other hand, are much more diverse including a female firefighter, a toddler of Asian decent, and a toddler with curly brown hair.",
                "position": 2285
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x38.png",
                "caption": "Figure S22:Mining specific biases.When aiming to reveal specific biases, controlling the meta prompts used for Llama 3.1-8B-Instruct enables capturing such targets.\nThe figure shows three settings.\nTop row depicts mining prompts that exhibit bias when referring to objects (left) and food (right), while the bottom row show examples for clothing.\nThe images generated to the variations which constitute the missed visual concepts reveal the source of the bias. For example, all the croissant images depict a single object from a similar angle, and all images of the woman exhibit the same style of coat.",
                "position": 2295
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x39.png",
                "caption": "Figure S23:Comparison with OpenBias on captions from COCO.Visual examples of captions taken from the COCO dataset that are marked as biased by our bias score however overlooked by OpenBias.",
                "position": 2438
            },
            {
                "img": "https://arxiv.org/html/2512.13427/x40.png",
                "caption": "Figure S24:Comparison with OpenBias on mined prompts.Visual examples of prompts mined byMineTheGaphowever overlooked by OpenBias.",
                "position": 2441
            }
        ]
    },
    {
        "header": "Appendix DAdditional Results",
        "images": []
    }
]