[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.02335/x1.png",
                "caption": "Figure 1:A typical case of activation sparsity (with a sparsity ratio of 60%) in a gated feed-forward network of LLMs, where considerable elements weakly contribute to the outputs within the activation scores.",
                "position": 112
            },
            {
                "img": "https://arxiv.org/html/2411.02335/x1.png",
                "caption": "Figure 1:A typical case of activation sparsity (with a sparsity ratio of 60%) in a gated feed-forward network of LLMs, where considerable elements weakly contribute to the outputs within the activation scores.",
                "position": 115
            },
            {
                "img": "https://arxiv.org/html/2411.02335/x2.png",
                "caption": "Figure 2:The PPL-activation Pareto curve of the 0.1B MoE with different expert numbers versus the 0.1B vanilla decoder-only Transformer.",
                "position": 120
            }
        ]
    },
    {
        "header": "2Preliminaries and Related Works",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.02335/x3.png",
                "caption": "Figure 3:The PPL-activation Pareto curve of our PPL-p%percentùëùp\\%italic_p %sparsity versus two baselines within models of different scales. ‚ÄúStraightforward ReLU‚Äù is only applicable to ReLU-activated models.",
                "position": 353
            },
            {
                "img": "https://arxiv.org/html/2411.02335/x4.png",
                "caption": "Figure 4:The trend of activation ratios (hereinafter using PPL-1%percent11\\%1 %sparsity) of models with different scales and activation functions during the pre-training stage. The fitted curves are plotted in brown. The number of training tokens is no less than 190 times the scale of non-embedding parameters.",
                "position": 506
            },
            {
                "img": "https://arxiv.org/html/2411.02335/x5.png",
                "caption": "Figure 5:The limit activation ratios on 0.1B ReLU-activated models.",
                "position": 550
            },
            {
                "img": "https://arxiv.org/html/2411.02335/x5.png",
                "caption": "Figure 5:The limit activation ratios on 0.1B ReLU-activated models.",
                "position": 553
            },
            {
                "img": "https://arxiv.org/html/2411.02335/x6.png",
                "caption": "Figure 6:The limit training loss on 0.1B ReLU-activated models.",
                "position": 558
            },
            {
                "img": "https://arxiv.org/html/2411.02335/x7.png",
                "caption": "Figure 7:The limit activation ratio for pre-trained models with different scales and activation functions.",
                "position": 564
            },
            {
                "img": "https://arxiv.org/html/2411.02335/x7.png",
                "caption": "Figure 7:The limit activation ratio for pre-trained models with different scales and activation functions.",
                "position": 567
            },
            {
                "img": "https://arxiv.org/html/2411.02335/x8.png",
                "caption": "Figure 8:The derivative trends of the sparsity-data curve with the increase of data-scale ratio, within ReLU/SiLU models of distinct scales.",
                "position": 572
            },
            {
                "img": "https://arxiv.org/html/2411.02335/x9.png",
                "caption": "Figure 9:The distribution of the neuron activation frequencies within models of distinct scales. Four datasets from the pre-training data are involved.",
                "position": 591
            },
            {
                "img": "https://arxiv.org/html/2411.02335/extracted/5975868/figures/token-dist.jpg",
                "caption": "Figure 10:The activation ratio (%) distributions of 71,549 tokens sampled from the vocabulary. We conduct a pair-wise comparison of the average activation ratio of each token within models of different scales. Note that the red line is they=xùë¶ùë•y=xitalic_y = italic_xcurve.",
                "position": 594
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining Loss Dynamics",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.02335/x10.png",
                "caption": "Figure 11:The trend of pre-training loss for models with different scales and activations.",
                "position": 1622
            },
            {
                "img": "https://arxiv.org/html/2411.02335/x10.png",
                "caption": "Figure 11:The trend of pre-training loss for models with different scales and activations.",
                "position": 1625
            },
            {
                "img": "https://arxiv.org/html/2411.02335/x11.png",
                "caption": "Figure 12:The limits of the training loss with the amount of training data approaches infinity.",
                "position": 1630
            }
        ]
    },
    {
        "header": "Appendix BSparsity Stabilizing Strategy",
        "images": []
    },
    {
        "header": "Appendix CBinary Search Algorithm for CETT",
        "images": []
    },
    {
        "header": "Appendix DFitting Algorithm and Results",
        "images": []
    },
    {
        "header": "Appendix EDatasets and Benchmarks",
        "images": []
    },
    {
        "header": "Appendix FDetailed Training Settings",
        "images": []
    },
    {
        "header": "Appendix GDataset-wise Activation Pattern",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.02335/x12.png",
                "caption": "Figure 13:The distributions of average activation frequencies across three individual layers at different positions within models of distinct scales, including four datasets from the pre-training data.",
                "position": 1888
            }
        ]
    },
    {
        "header": "Appendix HPerformance on Independent Benchmarks",
        "images": []
    }
]