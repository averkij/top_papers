[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Preliminaries: Unified Multimodal Models",
        "images": []
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02351/x1.png",
                "caption": "Figure 1:Overview of the proposed slimness- and sparsity-oriented techniques for efficient unified multimodal models.We introduce two complementary strategies:(1) Training-free compression, exemplified by neuron partitioning, which groups hidden neurons into subsets and prunes those less important for the target task; and(2) Training-aware MoE Adaptation, which dynamically activates neurons through a Mixture-of-Experts design, where neurons are organized into shared experts (solid lines) and routed experts (dotted lines) controlled by a router.",
                "position": 265
            },
            {
                "img": "https://arxiv.org/html/2512.02351/x2.png",
                "caption": "Figure 2:Statistical analysis of high-importance neurons, quantifying those predominantly activated in understanding tasks, in generation tasks, and jointly across both.",
                "position": 321
            },
            {
                "img": "https://arxiv.org/html/2512.02351/x3.png",
                "caption": "Figure 3:Visualization of the proportion of inactive and saturated neurons across layers.Active neurons are defined as those consistently ranked within the top 50% by activation scores, while inactive neurons never enter the top 50%.",
                "position": 350
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02351/x4.png",
                "caption": "Figure 4:Comparison of the overall performance of depth reduction on the GenEval, where the compression ratio is set as 50%.",
                "position": 454
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/examples/baseline/broccoli.jpg",
                "caption": "Figure 5:Impact of calibration data selection on neuron partition within the understanding component for generation tasks.Each triplet presents outputs from theunmodified model (left), the model after neuron partition usingimage-generation calibration (middle), and usingunderstanding calibration (right).",
                "position": 654
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/examples/gen/broccoli.jpg",
                "caption": "",
                "position": 667
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/examples/und/broccoli.jpg",
                "caption": "",
                "position": 668
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/examples/baseline/scissors.jpg",
                "caption": "",
                "position": 669
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/examples/gen/scissors.jpg",
                "caption": "",
                "position": 670
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/examples/und/scissors.jpg",
                "caption": "",
                "position": 671
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/GPT-Image/Baseline/4.jpg",
                "caption": "",
                "position": 678
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/GPT-Image/Gen/4.jpg",
                "caption": "",
                "position": 679
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/GPT-Image/Und/4.jpg",
                "caption": "",
                "position": 680
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/GPT-Image/Baseline/5.jpg",
                "caption": "",
                "position": 681
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/GPT-Image/Gen/5.jpg",
                "caption": "",
                "position": 682
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/GPT-Image/Und/5.jpg",
                "caption": "",
                "position": 683
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/examples/baseline1.jpg",
                "caption": "Figure 6:Qualitative comparison between the baseline (uncompressed) model and the model with 50% width reduction in the generation component.Thebaseline model (left)is evaluated without compression, while thecompressed model (right)reduces the generator width by 50%.\nResults are shown for the prompt “The wordSTART.” Compression leads to noticeable degradation in fine details and semantic consistency.",
                "position": 700
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/examples/gen_comp1.jpg",
                "caption": "",
                "position": 704
            },
            {
                "img": "https://arxiv.org/html/2512.02351/x5.png",
                "caption": "Figure 7:Training curves of expert-frozen tuning with only a few training steps, where MoE layers are configured with different numbers of experts (i.e., 16, 32 and 64).",
                "position": 879
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/wise/Baseline/175.jpg",
                "caption": "Figure 8:Visual comparison across different stages of MoE Adaptation.Shown are outputs from theBaselinemodel (without modification),Zeroshotsettings after expert partition with or without shared experts (Zeroshot w/o SandZeroshot w/ S), the model afterExpert-Frozen Tuning (E.F. Tuning), and the model further optimized viaMoE Adaptation (MoE Adapt.).\nTest prompts are sampled from WISE[36]and the 4o-Image Generator prompt set.",
                "position": 887
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/wise/zero_ns/32-0-16_1.jpg",
                "caption": "",
                "position": 933
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/wise/zero_s/120-8-56_1.jpg",
                "caption": "",
                "position": 937
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/wise/EFT/175.jpg",
                "caption": "",
                "position": 941
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/wise/MA/175.jpg",
                "caption": "",
                "position": 945
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/wise/Baseline/3.jpg",
                "caption": "",
                "position": 956
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/wise/zero_ns/32-0-16_0.jpg",
                "caption": "",
                "position": 960
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/wise/zero_s/120-8-56_0.jpg",
                "caption": "",
                "position": 964
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/wise/EFT/3.jpg",
                "caption": "",
                "position": 968
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/wise/MA/3.jpg",
                "caption": "",
                "position": 972
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/GPT-Image/Baseline/1.jpg",
                "caption": "",
                "position": 983
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/GPT-Image/zero_ns/1.jpg",
                "caption": "",
                "position": 987
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/GPT-Image/zero_s/1.jpg",
                "caption": "",
                "position": 991
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/GPT-Image/EFT/1.jpg",
                "caption": "",
                "position": 995
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/GPT-Image/MA/1.jpg",
                "caption": "",
                "position": 999
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/GPT-Image/Baseline/3.jpg",
                "caption": "",
                "position": 1010
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/GPT-Image/zero_ns/3.jpg",
                "caption": "",
                "position": 1014
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/GPT-Image/zero_s/3.jpg",
                "caption": "",
                "position": 1018
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/GPT-Image/EFT/3.jpg",
                "caption": "",
                "position": 1022
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/GPT-Image/MA/3.jpg",
                "caption": "",
                "position": 1026
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AGeneration Component Compression",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02351/figs/depth_reduction_gen.jpg",
                "caption": "Figure 9:Depth reduction applied to MLP layers in the generation component.Figures are shown with decreasing numbers of removed layers: 14 (50%), 7 (25%), 4 (14%), and 0.",
                "position": 1780
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/attn_dr_gen.jpg",
                "caption": "Figure 10:Compression of generation components through pruning of attention layers and heads.",
                "position": 1784
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/attn_wr_gen.jpg",
                "caption": "",
                "position": 1796
            }
        ]
    },
    {
        "header": "Appendix BAttention Head Partition",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02351/figs/meme.jpg",
                "caption": "Meme: “My Handwriting In Exams”",
                "position": 1813
            },
            {
                "img": "https://arxiv.org/html/2512.02351/figs/meme.jpg",
                "caption": "Meme: “My Handwriting In Exams”",
                "position": 1816
            }
        ]
    },
    {
        "header": "Appendix CDepth Reduction\non Understanding Tasks",
        "images": []
    },
    {
        "header": "Appendix DComparison with Compression Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02351/x6.png",
                "caption": "Figure 12:Ablation on the choice of calibration datasets for understanding tasks.",
                "position": 2044
            },
            {
                "img": "https://arxiv.org/html/2512.02351/x7.png",
                "caption": "Figure 13:Ablation of calibration datasets on generation tasks.",
                "position": 2047
            }
        ]
    },
    {
        "header": "Appendix ECalibration Data for Neuron Partitioning",
        "images": []
    }
]