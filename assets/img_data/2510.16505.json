[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16505/files/GitHub-Mark.png",
                "caption": "",
                "position": 91
            },
            {
                "img": "https://arxiv.org/html/2510.16505/files/hugging_face_logo.png",
                "caption": "",
                "position": 92
            },
            {
                "img": "https://arxiv.org/html/2510.16505/x1.png",
                "caption": "Figure 1:We collect reviewer-flagged inconsistencies in scientific papers and transform them into QA tasks that probe detection, correction, and reasoning over multimodal inconsistencies.",
                "position": 107
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16505/x2.png",
                "caption": "Figure 2:Pipeline of PRISMM-Bench.The top row illustrates the six main steps: (1) review sourcing, (2) LLM-based review filtering, and (3) manual annotation of metadata for reviewer-flagged inconsistencies (Sec.3.1), (4) LMM-based task generation, (5) manual verification to construct benchmark tasks (Sec.3.2), and (6) LLM-based debiasing to mitigate language biases (Sec.3.3). The bottom row shows representative outputs at each stage: filtered reviews after step 2, inconsistency annotation after step 3, an example multiple-choice question in natural language after step 4, and its debiased JSON-format counterpart after step 6.",
                "position": 199
            }
        ]
    },
    {
        "header": "3PRISMM-Bench",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "6Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AQualitative Examples and Dataset Viewer",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16505/x3.png",
                "caption": "Figure 3:A qualitative example of a textâ€“table inconsistency and its corresponding evaluation tasks ofIdent,RemedyandMatch.",
                "position": 1932
            },
            {
                "img": "https://arxiv.org/html/2510.16505/x4.png",
                "caption": "Figure 4:A qualitative example of a figure-equation inconsistency and its corresponding evaluation tasks ofIdent,RemedyandMatch.",
                "position": 1936
            }
        ]
    },
    {
        "header": "Appendix BList of Assets",
        "images": []
    },
    {
        "header": "Appendix CAblations",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16505/x5.png",
                "caption": "Figure 5:Inconsistency example for case study. Right: Visual context. Left: Question and answer options forIdenttask. Natural language options used for ease-of-comprehension, LMM was tasked using JSON.",
                "position": 2143
            },
            {
                "img": "https://arxiv.org/html/2510.16505/x6.png",
                "caption": "Figure 6:Raw reasoning output by InternVL3.5 38B on 3MDmM0rMPQ.",
                "position": 2155
            }
        ]
    },
    {
        "header": "Appendix DDataset Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16505/x7.png",
                "caption": "Figure 7:Example output after the LLM-assisted filtering.",
                "position": 2263
            },
            {
                "img": "https://arxiv.org/html/2510.16505/x8.png",
                "caption": "Figure 8:Example annotation output in JSON format.",
                "position": 2404
            },
            {
                "img": "https://arxiv.org/html/2510.16505/x9.png",
                "caption": "Figure 9:Distribution of inconsistency types. We identified 13 categories of inconsistencies based on the elements involved. The most common cases are figure-text mismatches and intra-figure (figure-only) inconsistencies",
                "position": 2417
            },
            {
                "img": "https://arxiv.org/html/2510.16505/x10.png",
                "caption": "Figure 10:Prompt for preparing multiple-choice questions in the inconsistency identification task.",
                "position": 2452
            },
            {
                "img": "https://arxiv.org/html/2510.16505/x11.png",
                "caption": "Figure 11:Example of generated multiple-choice question for the inconsistency identification task.",
                "position": 2481
            },
            {
                "img": "https://arxiv.org/html/2510.16505/x12.png",
                "caption": "Figure 12:Example of debiased output in evidence-claim JSON format for the inconsistency identification task.",
                "position": 2590
            },
            {
                "img": "https://arxiv.org/html/2510.16505/x13.png",
                "caption": "Figure 13:Example of output in target-action JSON format for the inconsistency remedy task, directly converted from the natural language MCQs from the inconsistency identification task.",
                "position": 2626
            },
            {
                "img": "https://arxiv.org/html/2510.16505/x14.png",
                "caption": "Figure 14:Example output for the inconsistency pair-match task.",
                "position": 2670
            }
        ]
    },
    {
        "header": "Appendix EUser Study Implementation & Statistics",
        "images": []
    },
    {
        "header": "Appendix FLLM Prompts",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16505/x15.png",
                "caption": "Figure 15:Prompt for LLM-based review filtering.",
                "position": 2701
            },
            {
                "img": "https://arxiv.org/html/2510.16505/x16.png",
                "caption": "Figure 16:Prompt for LLM-assisted conversion of natural language answers of the inconsistency identification task into evidence-claim JSON format. The evidence-claim JSON format is used as answer options in the inconsistency identification task.",
                "position": 2712
            },
            {
                "img": "https://arxiv.org/html/2510.16505/x17.png",
                "caption": "Figure 17:Prompt for LLM-assisted conversion of natural language answers of the inconsistency identification task into target-action JSON format. The target-action JSON is used as answer options in the inconsistency remedy task.",
                "position": 2723
            }
        ]
    },
    {
        "header": "Appendix GScreenshots of the Annotation App",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16505/x18.png",
                "caption": "Figure 18:First part of annotation app showing an overview over the annotation progress and embedded original PDF file.",
                "position": 2734
            },
            {
                "img": "https://arxiv.org/html/2510.16505/x19.png",
                "caption": "Figure 19:Second part of annotation app for drawing bounding boxes, entering text and further details about the inconsistency.",
                "position": 2737
            },
            {
                "img": "https://arxiv.org/html/2510.16505/x20.png",
                "caption": "Figure 20:First part of survey interface showing a question with no context provided.",
                "position": 2747
            },
            {
                "img": "https://arxiv.org/html/2510.16505/x21.png",
                "caption": "Figure 21:Second part of survey interface showing question withFocused Context.",
                "position": 2750
            },
            {
                "img": "https://arxiv.org/html/2510.16505/x22.png",
                "caption": "Figure 22:Third part of survey interface showing question withFull Document Context.",
                "position": 2753
            }
        ]
    },
    {
        "header": "Appendix HScreenshots of the Survey App",
        "images": []
    }
]