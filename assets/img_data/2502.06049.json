[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06049/extracted/6175910/figs/lm2_wf.png",
                "caption": "Figure 1:Illustration of LM2¬†overall architecture. It consists of a separate memory bank, which updates the main information flow through cross attention, and is updated using the input (‚Ñê‚Ñê\\mathcal{I}caligraphic_I), output (ùí™ùí™\\mathcal{O}caligraphic_O), and forget (‚Ñ±‚Ñ±\\mathcal{F}caligraphic_F) gates. For the information flow from one block to another, thegraycurve shows the normal attention flow and thepinkcurve shows the extra memory flow.",
                "position": 141
            }
        ]
    },
    {
        "header": "2Large Memory Model¬†(LM2)",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06049/extracted/6175910/figs/gate.png",
                "caption": "Figure 2:Illustration of how memory module works inside of each decoding block, whereblue,green, andredbox corresponds to forget, input, and output phase.",
                "position": 268
            }
        ]
    },
    {
        "header": "3Pre-training LM2",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06049/x1.png",
                "caption": "Figure 3:Performance on BABILong benchmark with different capabilities.",
                "position": 758
            },
            {
                "img": "https://arxiv.org/html/2502.06049/extracted/6175910/figs/example_.png",
                "caption": "Figure 4:We sample a question from MMLU to test the LM2¬†in a few-shot fashion. To study how the memory module focuses on relevant information, we place useful information inside one of the few-shot examples.",
                "position": 931
            },
            {
                "img": "https://arxiv.org/html/2502.06049/x2.png",
                "caption": "Figure 5:We evaluate variations of integrating memory within the decoder blocks. The number indicates how many of the initial decoder blocks include the memory module, as we found that the order of implementing memory modules does not affect performance.",
                "position": 938
            },
            {
                "img": "https://arxiv.org/html/2502.06049/x3.png",
                "caption": "(a)Cross-attention heatmaps before memory update.",
                "position": 1018
            },
            {
                "img": "https://arxiv.org/html/2502.06049/x3.png",
                "caption": "(a)Cross-attention heatmaps before memory update.",
                "position": 1021
            },
            {
                "img": "https://arxiv.org/html/2502.06049/x4.png",
                "caption": "(b)Cross-attention heatmaps after memory update.",
                "position": 1026
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ABABILong Dataset",
        "images": []
    },
    {
        "header": "Appendix BBABILong Benchmark Results",
        "images": []
    }
]