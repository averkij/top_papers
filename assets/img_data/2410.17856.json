[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17856/extracted/5999657/figures/teaser.png",
                "caption": "",
                "position": 201
            },
            {
                "img": "https://arxiv.org/html/2410.17856/extracted/5999657/figures/comparison.png",
                "caption": "Figure 2:Different pipelines in solving embodied decision-making tasks.(a)End-to-end pipeline modeling token sequences of language, observations, and actions.(b)Language prompting: VLMs decompose instructions for language-conditioned policy execution.(c)Latent prompting: maps discrete behavior tokens to low-level actions.(d)Future-image prompting: fine-tunes VLMs and diffusion models for image-conditioned control.(e)Visual-temporal prompting: VLMs generate segmentations and interaction cues to guideROCKET-1.",
                "position": 210
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17856/x1.png",
                "caption": "Figure 3:ROCKET-1architecture.ROCKET-1processes observations (oùëúoitalic_o), object segmentations (mùëömitalic_m), and interaction types (cùëêcitalic_c) to predict actions (aùëéaitalic_a) using a causal transformer. Observations and segmentations are concatenated and passed through a visual backbone for deep fusion. Interaction types and segmentations are randomly dropped with a pre-defiened probability during training.",
                "position": 262
            }
        ]
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17856/extracted/5999657/figures/datapipe.png",
                "caption": "Figure 4:Trajectory relabeling pipeline in Minecraft.A bounding box and point selection are applied to the image center in the frame preceding the interaction event to identify the interacting object. SAM-2 is then run in reverse temporal order for a specified duration.",
                "position": 283
            },
            {
                "img": "https://arxiv.org/html/2410.17856/extracted/5999657/figures/combination.png",
                "caption": "Figure 5:A hierarchical agent structure based on our proposed visual-temporal context prompting.A GPT-4o model decomposes complex tasks into steps based on the current observation, while the Molmo model identifies interactive objects by outputting points. SAM-2 segments these objects based on the point prompts, andROCKET-1uses the object masks and interaction types to make decisions. GPT-4o and Molmo run at low frequencies, while SAM-2 andROCKET-1operate at the same frequency as the environment.",
                "position": 347
            }
        ]
    },
    {
        "header": "4Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17856/extracted/5999657/figures/tasks.png",
                "caption": "Figure 6:A benchmark for evaluating open-world interaction capabilities of agents.The benchmark contains six interaction types in Minecraft, totaling 12 tasks. Unlike previous benchmarks, these tasks emphasize interacting with objects at specific spatial locations. For example, in‚Äúhunt the sheep in the right fence,‚Äùthe task fails if the agent kills the sheep on the left side. Some tasks, such as‚Äúplace the oak door on the diamond block,‚Äùnever appear in the training set. It is also designed to evaluate zero-shot generalization capabilities.",
                "position": 406
            },
            {
                "img": "https://arxiv.org/html/2410.17856/extracted/5999657/figures/icons/hunt_sheep.png",
                "caption": "Table 2:Results on the Minecraft Interaction benchmark.Each task is tested 32 times, and the average success rate (in%percent\\%%) is reported as the final result. ‚ÄúHuman‚Äù indicates instructions provided by a human.",
                "position": 411
            },
            {
                "img": "https://arxiv.org/html/2410.17856/extracted/5999657/figures/icons/hunt_cow.png",
                "caption": "",
                "position": 430
            },
            {
                "img": "https://arxiv.org/html/2410.17856/extracted/5999657/figures/icons/mine_emerald.png",
                "caption": "",
                "position": 431
            },
            {
                "img": "https://arxiv.org/html/2410.17856/extracted/5999657/figures/icons/mine_coal.png",
                "caption": "",
                "position": 432
            },
            {
                "img": "https://arxiv.org/html/2410.17856/extracted/5999657/figures/icons/interact_chest.png",
                "caption": "",
                "position": 433
            },
            {
                "img": "https://arxiv.org/html/2410.17856/extracted/5999657/figures/icons/interact_house.png",
                "caption": "",
                "position": 434
            },
            {
                "img": "https://arxiv.org/html/2410.17856/extracted/5999657/figures/icons/navigate_house.png",
                "caption": "",
                "position": 435
            },
            {
                "img": "https://arxiv.org/html/2410.17856/extracted/5999657/figures/icons/navigate_water.png",
                "caption": "",
                "position": 436
            },
            {
                "img": "https://arxiv.org/html/2410.17856/extracted/5999657/figures/icons/tool_fire.png",
                "caption": "",
                "position": 437
            },
            {
                "img": "https://arxiv.org/html/2410.17856/extracted/5999657/figures/icons/tool_lava.png",
                "caption": "",
                "position": 438
            },
            {
                "img": "https://arxiv.org/html/2410.17856/extracted/5999657/figures/icons/place_minecart.png",
                "caption": "",
                "position": 439
            },
            {
                "img": "https://arxiv.org/html/2410.17856/extracted/5999657/figures/icons/place_door.png",
                "caption": "",
                "position": 440
            },
            {
                "img": "https://arxiv.org/html/2410.17856/extracted/5999657/figures/demo.png",
                "caption": "Figure 7:Screenshots of our hierarchical agent when completing long-horizon tasks.",
                "position": 546
            },
            {
                "img": "https://arxiv.org/html/2410.17856/extracted/5999657/figures/icons/wooden_pickaxe.png",
                "caption": "Table 3:Comparison of hierarchical architectures with different communication protocols.All seven tasks require complex reasoning capabilities. The diamond task was run 100 times, while other tasks were run 20 times, with average success rates reported.",
                "position": 551
            },
            {
                "img": "https://arxiv.org/html/2410.17856/extracted/5999657/figures/icons/furnace.png",
                "caption": "",
                "position": 563
            },
            {
                "img": "https://arxiv.org/html/2410.17856/extracted/5999657/figures/icons/shears.png",
                "caption": "",
                "position": 564
            },
            {
                "img": "https://arxiv.org/html/2410.17856/extracted/5999657/figures/icons/diamond.png",
                "caption": "",
                "position": 565
            },
            {
                "img": "https://arxiv.org/html/2410.17856/extracted/5999657/figures/icons/steak.png",
                "caption": "",
                "position": 566
            },
            {
                "img": "https://arxiv.org/html/2410.17856/extracted/5999657/figures/icons/obsidian.png",
                "caption": "",
                "position": 567
            },
            {
                "img": "https://arxiv.org/html/2410.17856/extracted/5999657/figures/icons/pink_wool.png",
                "caption": "",
                "position": 568
            },
            {
                "img": "https://arxiv.org/html/2410.17856/extracted/5999657/figures/icons/hunt_sheep.png",
                "caption": "Table 4:Comparison of different condition fusion methods.",
                "position": 645
            },
            {
                "img": "https://arxiv.org/html/2410.17856/extracted/5999657/figures/icons/hunt_sheep.png",
                "caption": "Table 5:Comparison between different SAM-2 variants.We studied the impact of SAM-2 models of different sizes on the agent‚Äôs object-tracking capability (metric: success rate) and inference speed (metric: frames per second, FPS). ‚Äú#Pmt‚Äù indicates the number of frames between prompts generated by Molmo.",
                "position": 667
            }
        ]
    },
    {
        "header": "5Related Works",
        "images": []
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]