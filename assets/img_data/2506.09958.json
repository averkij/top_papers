[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Background & Summary",
        "images": []
    },
    {
        "header": "2Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09958/extracted/6533619/figures/kvasir_vqa_image.jpg",
                "caption": "Table 2:An example image and its associated questionâ€“answer pairs, stratified by complexity. Three samples are shown from each complexity category.",
                "position": 564
            }
        ]
    },
    {
        "header": "3Data Records",
        "images": []
    },
    {
        "header": "4Technical Validation",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09958/extracted/6533619/figures/heatmap.png",
                "caption": "Figure 1:Rank-normalized heatmap illustrating comparative performance rankings (1 = best, 5 = worst) of the models across Kvasir-VQA categories. Qwen2.5-VL-7B-FT consistently ranks first across most categories.",
                "position": 1658
            },
            {
                "img": "https://arxiv.org/html/2506.09958/extracted/6533619/figures/radar_combined.png",
                "caption": "Figure 2:Radar plot showing absolute performance scores of five models (Gemma3-4B, MedGemma, MedGemma-FT, Qwen2.5-VL-7B, and Qwen2.5-VL-7B-FT) across various question categories. Higher values indicate better performance.",
                "position": 1661
            }
        ]
    },
    {
        "header": "5Usage Notes",
        "images": []
    },
    {
        "header": "6Code Availability",
        "images": []
    },
    {
        "header": "7Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]