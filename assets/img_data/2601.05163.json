[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05163/x1.png",
                "caption": "Figure 1:The overall ofDocDancerfor document-grounded information seeking, wheresearchandreadtools for effective document retrieval and comprehension over processed documents.",
                "position": 136
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05163/x2.png",
                "caption": "Figure 2:Overall of theExploration-then-Synthesisframework.\n(i)Explorationstage iteratively interacts with the source document through Action(uu)–Observation(yy)–Intent(ii) steps.\n(ii)Synthesisstage aggregates the collected evidence to generate the final question and answer.\nWe present a concrete case illustrating the whole generation process in AppendixA.",
                "position": 239
            },
            {
                "img": "https://arxiv.org/html/2601.05163/x3.png",
                "caption": "Figure 3:Distribution of document used to synthesise.",
                "position": 291
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05163/x4.png",
                "caption": "Figure 4:Ablation studyon document parsing and tools.",
                "position": 699
            },
            {
                "img": "https://arxiv.org/html/2601.05163/x5.png",
                "caption": "Figure 5:Performance comparison between models trained onour synthesized QA dataandopen-source QA data.",
                "position": 718
            },
            {
                "img": "https://arxiv.org/html/2601.05163/x6.png",
                "caption": "Figure 6:Detailed domain-wise performancecomparison on MMLongBench-Doc between DocDancer and the model trained on OS-QA.",
                "position": 721
            },
            {
                "img": "https://arxiv.org/html/2601.05163/x7.png",
                "caption": "Figure 7:A case studydemonstrating that our proposed DocDancer successfully performs multi-round information gathering to reach the correct answer, as illustrated in TableLABEL:app:detailin detail, whereas OS-QA produces an incorrect result.",
                "position": 732
            },
            {
                "img": "https://arxiv.org/html/2601.05163/x8.png",
                "caption": "Figure 8:Results on DocBench across various domainsusing different models used byReadtool. We report the generalized accuracy of five types of document domains, including Academia (Aca.), Finance (Fin.), Government (Gov), Law, and News.",
                "position": 739
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethical Considerations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ACase Study of Synthetic Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05163/x9.png",
                "caption": "Figure 9:A case studyof the Exploration-then-Synthesis framework generating a multi-hop, cross-document, cross-modal numerical reasoning QA pair.",
                "position": 1407
            }
        ]
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    },
    {
        "header": "Appendix CBaselines",
        "images": []
    }
]