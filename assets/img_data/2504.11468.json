[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.11468/extracted/6351911/figs/logo_browser.png",
                "caption": "",
                "position": 122
            },
            {
                "img": "https://arxiv.org/html/2504.11468/extracted/6351911/figs/logo_hf.png",
                "caption": "",
                "position": 123
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.11468/x1.png",
                "caption": "Figure 1:Examples from LVLMs trained with different strategies for reasoningLeft: response from a model trained with SFT, showingpseudo reasoning tracesand a number ofpseudo self-reflective cues(i.e., aha-moments) imitated from R1.Right: response from a model trained with RL, showingnative reasoning abilityandauthentic aha-momentsemerged from RL training.Wrong reasoning stepsare colored red andaha-momentsare highlighted.",
                "position": 147
            },
            {
                "img": "https://arxiv.org/html/2504.11468/x2.png",
                "caption": "Figure 2:Data generation pipeline.We first generate initial reasoning traces by feeding detailed captions and visual questions into DeepSeek-R1.These outputs are then rewritten for improved fluency and verified for correctness using a GPT-based verifier. the resulting data is split intoVLAA-Thinking-SFTandVLAA-Thinking-RL.",
                "position": 170
            }
        ]
    },
    {
        "header": "2TheVLAA-ThinkingDataset",
        "images": []
    },
    {
        "header": "3Investigating The Role of SFT for Multimodal Reasoning",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.11468/x3.png",
                "caption": "Figure 3:Delta percentage performance changeof different models trained with supervised fine-tuning (SFT) only.",
                "position": 448
            }
        ]
    },
    {
        "header": "4Improving Multimodal Reasoning with Mixed Rewards",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.11468/x4.png",
                "caption": "Figure 4:The proposedMixed Reward Modulefor GRPO training, comprising 2 reward formats (rule-based and open-ended) and 5 types of verifiable rewards (digit, MCQ, math, IoU and general reasoning).",
                "position": 494
            },
            {
                "img": "https://arxiv.org/html/2504.11468/x5.png",
                "caption": "Figure 5:Impact of SFT with 5K and 10K samples before GRPO.Smaller-sized SFT datasets still jeopardizes GRPO performance.",
                "position": 611
            },
            {
                "img": "https://arxiv.org/html/2504.11468/x6.png",
                "caption": "Figure 6:Response length (left) and reward (right) during training.Training with only GRPO yields the lowest response length and yet the highest final reward and best benchmark performance, indicating that response length, reward, and model performance are NOT necessarily related.",
                "position": 629
            },
            {
                "img": "https://arxiv.org/html/2504.11468/x7.png",
                "caption": "Figure 7:Heatmap of different “aha” expressionsgenerated by VLAA-Thinker models during training.",
                "position": 850
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AData Generation",
        "images": []
    },
    {
        "header": "Appendix BDetails of SFT Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.11468/x8.png",
                "caption": "Figure 12:System Prompt used for training and evaluation.",
                "position": 1875
            },
            {
                "img": "https://arxiv.org/html/2504.11468/x9.png",
                "caption": "Figure 13:A case from MathVerse testmini (sample index 20). Markdowns are rendered for illustration purpose. Wrong reasoning paths are colored red.",
                "position": 1961
            },
            {
                "img": "https://arxiv.org/html/2504.11468/x10.png",
                "caption": "Figure 14:A VL-Thinking sample from GeoQA170K.",
                "position": 1964
            },
            {
                "img": "https://arxiv.org/html/2504.11468/x11.png",
                "caption": "Figure 15:A VL-Thinking sample from Math PUMA (subset Synthesis).",
                "position": 1967
            },
            {
                "img": "https://arxiv.org/html/2504.11468/x12.png",
                "caption": "Figure 16:A VL-Thinking sample from CLEVR-Math.",
                "position": 1970
            },
            {
                "img": "https://arxiv.org/html/2504.11468/x13.png",
                "caption": "Figure 17:A VL-Thinking sample from ArxivQA.",
                "position": 1973
            },
            {
                "img": "https://arxiv.org/html/2504.11468/x14.png",
                "caption": "Figure 18:A VL-Thinking sample from ALLaVA-LAION.",
                "position": 1976
            }
        ]
    },
    {
        "header": "Appendix CDetails of GRPO Experiments",
        "images": []
    }
]