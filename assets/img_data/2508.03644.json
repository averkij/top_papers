[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03644/figure/robot_checklist_transparent.png",
                "caption": "",
                "position": 76
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03644/x1.png",
                "caption": "Figure 1:Existing document RAG benchmarks suffer from ambiguous queries that are insufficiently specified to retrieve relevant results, failing to authentically evaluate current document retrieval models and systems.",
                "position": 105
            },
            {
                "img": "https://arxiv.org/html/2508.03644/x2.png",
                "caption": "Figure 2:Our pilot study reveals critical limitations in existing document RAG system benchmarks, which make them unable to reliably assess current system in a fine-grained and realistic manner.",
                "position": 138
            }
        ]
    },
    {
        "header": "2Limitations of Existing Document RAG System Evaluation: A Pilot Study",
        "images": []
    },
    {
        "header": "3Double-Bench: The Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03644/x3.png",
                "caption": "Figure 3:Overview of theDouble-Benchconstruction pipeline. The preprocessing stage filters the collected corpora and splits the content by modality. To alleviate identifed problems explained in Figure2, an iterative clarity-oriented refinement pipeline is introduced for single-hop query generation, while knowledge graphs are additionally constructed to assist multi-hop query generation. All document pages are thoroughly checked by annotators to produce list-of-evidence and set-of-evidence labels.",
                "position": 495
            },
            {
                "img": "https://arxiv.org/html/2508.03644/x4.png",
                "caption": "Figure 4:Statistics of theDouble-Benchdataset. See Appendix for more details.",
                "position": 637
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03644/x5.png",
                "caption": "Figure 5:Breakdown of retrieval and response model performance of frameworks under single (s) and multi-hop (m) queries. Our analysis reveals that performance drops on multi-hop questions are mainly due to retrieval failures that cause models to abstain from answering.",
                "position": 1276
            },
            {
                "img": "https://arxiv.org/html/2508.03644/x6.png",
                "caption": "Figure 6:Time efficiency of each document RAG system.",
                "position": 1292
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetailed Benchmark Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03644/x7.png",
                "caption": "Figure 7:KDE distribution of document lengths by document type.",
                "position": 2108
            }
        ]
    },
    {
        "header": "Appendix BHuman Annotation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03644/figure/human.jpg",
                "caption": "Figure 8:Human Annotation UI.",
                "position": 2191
            }
        ]
    },
    {
        "header": "Appendix CDetailed Experiment Settings",
        "images": []
    },
    {
        "header": "Appendix DPrompt Templates",
        "images": []
    },
    {
        "header": "Appendix ECase Study",
        "images": []
    }
]