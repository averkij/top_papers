[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24014/x1.png",
                "caption": "Figure 1:Attention score across denoising steps using LLaDA-1.5 (l=78l=78,T=32T=32,b​l​o​c​k​_​l​e​n​g​t​h=32block\\_length=32). Rows correspond to different attention heads. Red lines divide key tokens in prefill and generation tokens. The result shows pronounced similarity across denoising steps. More visualized attention patterns from different DLMs are provided in the AppendixA.1.",
                "position": 269
            },
            {
                "img": "https://arxiv.org/html/2509.24014/x2.png",
                "caption": "(a)32 steps",
                "position": 278
            },
            {
                "img": "https://arxiv.org/html/2509.24014/x2.png",
                "caption": "(a)32 steps",
                "position": 281
            },
            {
                "img": "https://arxiv.org/html/2509.24014/x3.png",
                "caption": "(b)128 steps",
                "position": 286
            },
            {
                "img": "https://arxiv.org/html/2509.24014/x4.png",
                "caption": "Figure 3:Overview of SparseD. SparseD first applies full attention during the early diffusion steps. It then pre-computes attention scores and selects the important scores using a block-wise scheme, while performing isolated selection for prefill and generation tokens. The resulting sparse patterns are reused in the subsequent steps.",
                "position": 361
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24014/x5.png",
                "caption": "Figure 4:Latency comparison (TT=128) for Dream-7B-Instruct and LLaDA-1.5, evaluated on a single sample from the RULER dataset with varying sequence lengths.",
                "position": 617
            },
            {
                "img": "https://arxiv.org/html/2509.24014/x5.png",
                "caption": "",
                "position": 620
            },
            {
                "img": "https://arxiv.org/html/2509.24014/x6.png",
                "caption": "",
                "position": 624
            },
            {
                "img": "https://arxiv.org/html/2509.24014/x7.png",
                "caption": "Figure 5:Latency comparison of SparseD on Dream-7B-Instruct and LLaDA-1.5 across varying diffusion steps, evaluated on a single RULER sample with a 64k context length.",
                "position": 637
            },
            {
                "img": "https://arxiv.org/html/2509.24014/x7.png",
                "caption": "",
                "position": 640
            },
            {
                "img": "https://arxiv.org/html/2509.24014/x8.png",
                "caption": "",
                "position": 644
            },
            {
                "img": "https://arxiv.org/html/2509.24014/x9.png",
                "caption": "(a)Analysis ofs​k​i​p%skip\\%.",
                "position": 712
            },
            {
                "img": "https://arxiv.org/html/2509.24014/x9.png",
                "caption": "(a)Analysis ofs​k​i​p%skip\\%.",
                "position": 715
            },
            {
                "img": "https://arxiv.org/html/2509.24014/x10.png",
                "caption": "(b)Analysis ofρ%\\rho\\%on Dream.",
                "position": 720
            },
            {
                "img": "https://arxiv.org/html/2509.24014/x11.png",
                "caption": "(c)Analysis ofρ%\\rho\\%on LLaDA.",
                "position": 725
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24014/x12.png",
                "caption": "Figure 7:Attention score across denoising steps using LLaDA-8B-Base (l=102l=102,T=32T=32,b​l​o​c​k​_​l​e​n​g​t​h=32block\\_length=32). Rows correspond to different attention heads. Red lines divide key tokens in prefill and generation tokens. The result shows pronounced similarity across denoising steps.",
                "position": 1075
            },
            {
                "img": "https://arxiv.org/html/2509.24014/x13.png",
                "caption": "Figure 8:Attention score across denoising steps using Dream-7B-Instruct (l=83l=83,T=32T=32). Rows correspond to different attention heads. Red lines divide key tokens in prefill and generation tokens. The result shows pronounced similarity across denoising steps.",
                "position": 1078
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]