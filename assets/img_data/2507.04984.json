[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.04984/x1.png",
                "caption": "",
                "position": 81
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.04984/x2.png",
                "caption": "Figure 2:(a) Model Pipeline.The Image Encoder is shared across all frames, and temporal blocks extract temporal information in the latent space.(b) Multi-level Feature Sharing.The Image Encoder and Decoder consist of several levels of resolution due to downsampling/upsampling latent features. At theit‚Å¢hsuperscriptùëñùë°‚Ñéi^{th}italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPTlevel of the encoder and the decoder, features fromI0subscriptùêº0I_{0}italic_I start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTandI1subscriptùêº1I_{1}italic_I start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTin the encoder are warped and concatenated with the original copy (when the downsampling rate is larger than 8, warped features are excluded). The concatenated features are used as keys and values in cross attention[42]where the decoder feature at the same level is the query.(c) Encoder/Decoder Temporal Block.Each temporal block consists of two sets of 3D convolution + attention. In the decoder, the second attention is cross-attention between the intermediate frame (query) and all frames (key and value) to aggregate the video feature into one feature map.(d) 3D-wavelet Feature Gating.Wavelet information is extracted from the input video clip and encoded by CNNs. A sigmoid activation is applied, and the result is element-wise multiplied by the output of the Image Encoder with a skip connection.",
                "position": 166
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.04984/x3.png",
                "caption": "Figure 3:Qualitative Comparison between our method and recent SOTAs.The leftmost image is the overlaid image ofI0subscriptùêº0I_{0}italic_I start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTandI1subscriptùêº1I_{1}italic_I start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT(blended image). Areas with drastic motion changes are cropped withblueboxes to better visualize the results.Redcircles and boxes indicate the area where we perform significantly better. Our method achieves better visual quality than recent SOTAs. Additional qualitative results are included in theSupplementary Material Sec.8.2.",
                "position": 585
            },
            {
                "img": "https://arxiv.org/html/2507.04984/x4.png",
                "caption": "Figure 4:Inconsistency between PSNR/SSIM and Visual Quality.Redcircles and arrows indicate where the results from EMAVFI are distorted.",
                "position": 591
            },
            {
                "img": "https://arxiv.org/html/2507.04984/x5.png",
                "caption": "Figure 5:Visualization of 3D Wavelet Transform.The 3D wavelet transform can extract frequency information along different directions (time, height, width). We visualize results by applying a high pass filter in the time dimension with a combination of (low, low), (low, high), and (high, low) filters in spatial dimensions.",
                "position": 691
            },
            {
                "img": "https://arxiv.org/html/2507.04984/x6.png",
                "caption": "Figure 6:M‚Å¢A‚Å¢P‚Å¢E‚Å¢(‚Ñ∞‚Å¢(In),‚Ñ∞‚Å¢(I0))ùëÄùê¥ùëÉùê∏‚Ñ∞subscriptùêºùëõ‚Ñ∞subscriptùêº0MAPE(\\mathcal{E}(I_{n}),\\mathcal{E}(I_{0}))italic_M italic_A italic_P italic_E ( caligraphic_E ( italic_I start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) , caligraphic_E ( italic_I start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) )in the setup of Consec. BB[27]andM‚Å¢A‚Å¢P‚Å¢E‚Å¢(‚Ñ∞‚Å¢(V),‚Ñ∞‚Å¢(V~))ùëÄùê¥ùëÉùê∏‚Ñ∞ùëâ‚Ñ∞~ùëâMAPE(\\mathcal{E}(V),\\mathcal{E}(\\tilde{V}))italic_M italic_A italic_P italic_E ( caligraphic_E ( italic_V ) , caligraphic_E ( over~ start_ARG italic_V end_ARG ) )in our method.V~~ùëâ\\tilde{V}over~ start_ARG italic_V end_ARGis the video clip with the intermediate frame replaced with 0s. The MAPE in Consec. BB is less than 1%, resulting in a rough identity transformation. In our method, MAPE is 40-50%, and therefore the Brownian Bridge learns to reduce this gap.",
                "position": 758
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.04984/x7.png",
                "caption": "Figure 7:(a) Given four neighboring framesI0,I1,I3,I4subscriptùêº0subscriptùêº1subscriptùêº3subscriptùêº4I_{0},I_{1},I_{3},I_{4}italic_I start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_I start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_I start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , italic_I start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT, we can predict the intermediate frameI2subscriptùêº2I_{2}italic_I start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. (b) Given a sequence of framesI0,I2,I4subscriptùêº0subscriptùêº2subscriptùêº4I_{0},I_{2},I_{4}italic_I start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_I start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_I start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT, we can predict the intermediate frame between each adjacent pairI1,I3subscriptùêº1subscriptùêº3I_{1},I_{3}italic_I start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_I start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT.",
                "position": 1459
            }
        ]
    },
    {
        "header": "6Overview",
        "images": []
    },
    {
        "header": "7Optical Flow Basics",
        "images": []
    },
    {
        "header": "8Additional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.04984/x8.png",
                "caption": "Figure 8:Visualization of optical flows.",
                "position": 1679
            },
            {
                "img": "https://arxiv.org/html/2507.04984/x9.png",
                "caption": "Figure 9:Additional qualitative comparison between our method and recent SOTAs.The leftmost image is the overlaid image ofI0subscriptùêº0I_{0}italic_I start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTandI1subscriptùêº1I_{1}italic_I start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT(blended image). Images insideblueboxes contain drastic motion changes and are cropped out to show details of interpolation results.Redcircles, boxes, and arrows indicate the area where we significantly perform better. Our method achieves better visual quality than recent SOTAs.",
                "position": 1682
            },
            {
                "img": "https://arxiv.org/html/2507.04984/x10.png",
                "caption": "Figure 10:Additional qualitative comparison between our method and recent SOTAs.The leftmost image is the overlaid image ofI0subscriptùêº0I_{0}italic_I start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTandI1subscriptùêº1I_{1}italic_I start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT(blended image). Images insideblueboxes contain drastic motion changes and are cropped out to show details of interpolation results.Redcircles, boxes, and arrows indicate the area where we significantly perform better. Our method achieves better visual quality than recent SOTAs.",
                "position": 1685
            },
            {
                "img": "https://arxiv.org/html/2507.04984/x11.png",
                "caption": "Figure 11:Visual comparison of 8x√ó\\times√óinterpolation results.We include a visual comparison of 8√ó\\times√óinterpolation between our method and PerVFI.Redarrows indicate where our method is visually better. Additional comparisons (in video form) are provided in ourProject Page.",
                "position": 1688
            }
        ]
    },
    {
        "header": "9Additional Details",
        "images": []
    }
]