[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07120/x1.png",
                "caption": "(a)Example generation withAnchored Decoding.",
                "position": 256
            },
            {
                "img": "https://arxiv.org/html/2602.07120/x1.png",
                "caption": "(a)Example generation withAnchored Decoding.",
                "position": 259
            },
            {
                "img": "https://arxiv.org/html/2602.07120/x2.png",
                "caption": "(b)Token-level risk-utility tradeoffs.",
                "position": 265
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Anchored Decoding",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07120/x3.png",
                "caption": "Figure 2:AnchoredByteDecoding(inpurple) achieves the best risk-utility tradeoff at the byte level across five model pairs.We report the average of three seeds; error bars show standard deviation.\nThe shaded threshold denotes thehigh-protection operating point, where the Normalized Copyright Reduction (NCR)≥75%\\geq 75\\%. NCR and fluency are evaluated onBooks, and factuality onBios.",
                "position": 631
            }
        ]
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07120/x4.png",
                "caption": "Figure 3:Risk-utility tradeoffs forAnchored Decodingablations.We ablate three axes: (i) optimization objective, (ii) prefix debt, and (iii) budgeting strategy. For brevity, our methods are labeled asAnc. Dec.",
                "position": 831
            },
            {
                "img": "https://arxiv.org/html/2602.07120/x5.png",
                "caption": "Figure 4:Top:Per-stepKL​(pr∥ps)\\mathrm{KL}(p_{r}\\|p_{s})histogram when sampling fromprp_{r}, conditioned on prefixes different domains. TheCopyrightdomain is more right-shifted than theCreativeandFactualdomains.Bottom:Unconditional CCDF of per-stepKL​(pr|ps)\\mathrm{KL}(p_{r}|p_{s}), shown forx≥q90x\\geq q_{90}.q90q_{90}is computed from per-step KL valuespooled across domains(shared cutoff per panel). TheCopyrightdomain has a heavier extreme tail than others.",
                "position": 960
            },
            {
                "img": "https://arxiv.org/html/2602.07120/x6.png",
                "caption": "Figure 5:High-copying regions are front-loaded under both byte-level and token-level decoding.We plot histograms (bin width of 5) of the start position of copying metrics (LCS and ACS) onCopyrightgenerations. Copying tends to cluster at early positions.",
                "position": 965
            },
            {
                "img": "https://arxiv.org/html/2602.07120/x7.png",
                "caption": "Figure 6:Left:KDEs of per-step prefix log-likelihood ratios (LLR) at the token and byte levels. A positive LLR means thatprp_{r}assigns higher probability than thepsp_{s}to the realized next step; large positive LLR events occur most often inCopyright.Right:KDEs ofδinit​(x)\\delta_{\\mathrm{init}}(x), the mean of the top-5 positive prefix LLRs.Copyrightprefixes are markedly right-shifted relative toCreativeandFactual.",
                "position": 977
            }
        ]
    },
    {
        "header": "6Analysis",
        "images": []
    },
    {
        "header": "7Discussion",
        "images": []
    },
    {
        "header": "8Impact Statement",
        "images": []
    },
    {
        "header": "9Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AGeneral Information",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07120/x8.png",
                "caption": "",
                "position": 2377
            },
            {
                "img": "https://arxiv.org/html/2602.07120/x9.png",
                "caption": "",
                "position": 2382
            }
        ]
    },
    {
        "header": "Appendix BAnchored DecodingDetails",
        "images": []
    },
    {
        "header": "Appendix CExperiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07120/x10.png",
                "caption": "Figure 7:Benchmarking LMs on natural language tasks using the OLMES evaluation suite(Gu et al.,2025). Our TinyComma 1.8B (marked with a gold star★\\bigstar) achieves the best downstream task performance among open models of its size range, trailing only behind the Comma 7B and risky models.",
                "position": 3239
            },
            {
                "img": "https://arxiv.org/html/2602.07120/x11.png",
                "caption": "Figure 8:Debt window trade-off curves forAnchored Decodingprefix debt calculation, usingn=1,3,5,7n=1,3,5,7. We use the token-level model pair{TinyComma 1.8B, Llama 3.1 70B}\\{\\textrm{TinyComma 1.8B{}, Llama 3.1 70B}\\}. The optimal trade-off region is the upper-right corner.",
                "position": 3263
            },
            {
                "img": "https://arxiv.org/html/2602.07120/x12.png",
                "caption": "Figure 9:Prefix debt correlates with copying metrics.Anchored Decodingprefix debt deciles with token-level model pair{TinyComma 1.8B, Llama 3.1 70B}\\{\\textrm{TinyComma 1.8B{}, Llama 3.1 70B}\\}.",
                "position": 3273
            }
        ]
    },
    {
        "header": "Appendix DEvaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07120/x13.png",
                "caption": "(a)Token-level trajectory visualization.",
                "position": 4257
            },
            {
                "img": "https://arxiv.org/html/2602.07120/x13.png",
                "caption": "(a)Token-level trajectory visualization.",
                "position": 4260
            },
            {
                "img": "https://arxiv.org/html/2602.07120/x14.png",
                "caption": "(b)Byte-level trajectory visualization.",
                "position": 4266
            },
            {
                "img": "https://arxiv.org/html/2602.07120/x15.png",
                "caption": "Figure 11:Factual precision onBiosstratified by entity rarity as a function of the budget parameterkk(log scale).As k increases, precision rises across buckets, with slower improvement for long-tail entities than for head and torso entities.",
                "position": 4290
            },
            {
                "img": "https://arxiv.org/html/2602.07120/x16.png",
                "caption": "Figure 12:Risk-utility tradeoff plots on non-literal copying.Anchored Decodingremains Pareto-optimal.We evaluate the fluency and extent of character overlap>3{>}3in open-ended story generation.",
                "position": 4310
            }
        ]
    },
    {
        "header": "Appendix EAdditional Results",
        "images": []
    }
]