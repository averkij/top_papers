[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21689/x1.png",
                "caption": "Figure 1:ToolOrchestra shows consistently strong performance on HLE, FRAMES, andτ2\\tau^{2}-Bench with superior cost efficiency.",
                "position": 130
            },
            {
                "img": "https://arxiv.org/html/2511.21689/x2.png",
                "caption": "",
                "position": 139
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21689/x3.png",
                "caption": "Figure 2:Overview of Orchestrator. Given a task, Orchestrator alternates between reasoning and tool calling in multiple turns to solve it. Orchestrator interacts with a diverse tool set, including basic tools (web search, functions such asget_flight_status, etc.), specialized LLMs (coding models, math models, etc.) and generalist LLMs (GPT-5, Claude Opus 4.1, etc.). In training under ToolOrchestra, Orchestrator is jointly optimized by outcome, efficiency and preference rewards via reinforcement learning.",
                "position": 172
            },
            {
                "img": "https://arxiv.org/html/2511.21689/x4.png",
                "caption": "Figure 3:Tool-calling preferences exhibited by a prompted off-the-shelf or RL-trained model.\nGPT-5 tends to call GPT-5-mini most of the time, while Qwen3-8B relies heavily on GPT-5.",
                "position": 175
            }
        ]
    },
    {
        "header": "2Agentic Problem Formulation",
        "images": []
    },
    {
        "header": "3ToolOrchestra",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21689/x5.png",
                "caption": "Figure 4:Overview of ToolScale data synthesis pipeline. Starting from a domain, LLM will (1) firstly generate domain-specific database and tool APIs to simulate the environment and (2) then generate diverse user tasks together with their corresponding golden actions.",
                "position": 347
            }
        ]
    },
    {
        "header": "4Experimental Setting",
        "images": []
    },
    {
        "header": "5Experimental Results",
        "images": []
    },
    {
        "header": "6Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21689/x6.png",
                "caption": "Figure 5:The proportion of tool calls made by LLMs to solve a task (averaged across HLE, Frames andτ2\\tau^{2}-bench).\nQwen-32B refers to Qwen3-32B[27]and Coder-32B refers to Qwen2.5-Coder-32B-Instruct[24].\nCompared to other strong foundation models, Orchestrator-8B makes more balanced tool calls, and does not exhibit strong biases toward a particular tool or model.\nDetailed statistics are shown in Table15.",
                "position": 828
            },
            {
                "img": "https://arxiv.org/html/2511.21689/x7.png",
                "caption": "Figure 6:The relationship between performance and cost. Compared to strong monolithic LLM systems, Orchestrator (ours) achieves the best cost-effectiveness.",
                "position": 839
            }
        ]
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APilot Study",
        "images": []
    },
    {
        "header": "Appendix BEvaluation Benchmarks",
        "images": []
    },
    {
        "header": "Appendix CModel description for Qwen3-32B",
        "images": []
    },
    {
        "header": "Appendix DTools in training",
        "images": []
    },
    {
        "header": "Appendix EThird-party API",
        "images": []
    },
    {
        "header": "Appendix FHumane preference example",
        "images": []
    },
    {
        "header": "Appendix GUse of LLMs Disclosure",
        "images": []
    },
    {
        "header": "Appendix HGeneralization of pricing configurations",
        "images": []
    },
    {
        "header": "Appendix IData Synthesis",
        "images": []
    },
    {
        "header": "Appendix JBreakdown of ToolScale",
        "images": []
    },
    {
        "header": "Appendix KData synthesis prompts and examples",
        "images": []
    },
    {
        "header": "Appendix LCalculation of rewards for preference-aware benchmark",
        "images": []
    }
]