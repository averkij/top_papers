[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08489/x1.png",
                "caption": "Figure 1:Overview of RLTR:RLTR augments standard RLVR with a transfer reward.Top:A trainable generator model produces a full completion, whose final-answer correctness yields on answer reward for policy optimization.Bottom:We then truncate the generated reasoning to form a prefix and feed it to a frozen receiver model to produce continued completions whose final-answer correctness defines a transfer reward that measures the transferability of partial reasoning across models. The answer reward and transfer reward are combined into a unified reward signal used to update the generator policy.",
                "position": 184
            }
        ]
    },
    {
        "header": "3RLTR: Reinforcement Learning with Transferable Reward",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08489/x2.png",
                "caption": "(a)Average accuracy",
                "position": 490
            },
            {
                "img": "https://arxiv.org/html/2602.08489/x2.png",
                "caption": "(a)Average accuracy",
                "position": 493
            },
            {
                "img": "https://arxiv.org/html/2602.08489/x3.png",
                "caption": "(b)Maj@64 accuracy",
                "position": 498
            },
            {
                "img": "https://arxiv.org/html/2602.08489/x4.png",
                "caption": "(c)Transferability",
                "position": 503
            },
            {
                "img": "https://arxiv.org/html/2602.08489/x5.png",
                "caption": "Figure 3:RLTR improves transferability consistently.Transferability (%) comparison among base model, RLVR, and RLTR.\nThe truncation ratio is the fraction of tokens remained from the begging of a reasoning trace (higher means longer prefixes continuation).\nRLTR consistently achieves higher transferability than baselines.",
                "position": 891
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperimental Details",
        "images": []
    },
    {
        "header": "Appendix BAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08489/x6.png",
                "caption": "(a)Truncation Ratio (τ\\tau): 0.3",
                "position": 1237
            },
            {
                "img": "https://arxiv.org/html/2602.08489/x6.png",
                "caption": "(a)Truncation Ratio (τ\\tau): 0.3",
                "position": 1240
            },
            {
                "img": "https://arxiv.org/html/2602.08489/x7.png",
                "caption": "(b)Truncation Ratio (τ\\tau): 0.5",
                "position": 1245
            },
            {
                "img": "https://arxiv.org/html/2602.08489/x8.png",
                "caption": "(c)Truncation Ratio (τ\\tau): 0.7",
                "position": 1251
            },
            {
                "img": "https://arxiv.org/html/2602.08489/x9.png",
                "caption": "(d)Truncation Ratio (τ\\tau): 0.9",
                "position": 1256
            }
        ]
    },
    {
        "header": "Appendix CPrompt Examples",
        "images": []
    },
    {
        "header": "Appendix DFuture Work",
        "images": []
    },
    {
        "header": "Appendix EGeneration Examples",
        "images": []
    }
]