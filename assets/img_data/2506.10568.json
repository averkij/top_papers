[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10568/extracted/6531286/images/teaser.png",
                "caption": "",
                "position": 77
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10568/extracted/6531286/images/train.png",
                "caption": "Figure 2:The pipeline of DreamActor-H1 leverages a DiT architecture, starting with dataset preparation where a VLM describes product and human images, followed by pose estimation and bounding box detection on training videos. During training, human poses and product boxes integrate with video noise for motion guidance, while a VAE encodes input images for appearance guidance; human-product descriptions are fed into the model via a text encoder. The model incorporates full attention, reference attention, and object attention (with product latents as inputs), with the reference and object attention mechanisms detailed at the top of the figure.",
                "position": 151
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10568/extracted/6531286/images/inference.png",
                "caption": "Figure 3:During inference, our framework retrieves optimal motion templates from pre-defined pools and adapts object box scaling via joint analysis of reference human/product images, enabling pose-coherent animations.",
                "position": 199
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10568/extracted/6531286/images/comparison.jpg",
                "caption": "Figure 4:Comparisons with AnchorCrafter[66], Phantom[42], VACE[32]and UniAnimate-DiT※[62]. Note that we only generate 3 videos for AnchorCrafter, and UniAnimate-DiT uses our first frames and pose sequences as inputs.",
                "position": 241
            },
            {
                "img": "https://arxiv.org/html/2506.10568/extracted/6531286/images/ablation.png",
                "caption": "Figure 5:Ablation studies with “Ours baseline” (w/o object attention and text input) and “Ours w/o text”.",
                "position": 405
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10568/extracted/6531286/images/moreresults1.jpg",
                "caption": "Figure 6:Our video results generated from human images and product images.",
                "position": 435
            },
            {
                "img": "https://arxiv.org/html/2506.10568/extracted/6531286/images/moreresults2.jpg",
                "caption": "Figure 7:Our results generated from more human images and product images.",
                "position": 438
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]