[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.03405/figures/huggingface.png",
                "caption": "",
                "position": 183
            },
            {
                "img": "https://arxiv.org/html/2509.03405/x1.png",
                "caption": "",
                "position": 184
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.03405/x2.png",
                "caption": "Figure 1:TheLMEntsuite is composed of three components (left) fine-grained entity mentions for every document in the pretraining corpus, (middle) an index that retrieves by the entity QID and outperforms string-based retrieval methods, (right) 12 models trained on 1, 2, 4, and 6 epochs where each step can be mapped to the entities it mentions, and each entity can be traced to the steps that introduce it.",
                "position": 194
            },
            {
                "img": "https://arxiv.org/html/2509.03405/x3.png",
                "caption": "Figure 2:The document for the entity “Josh Allen” is split into two chunks, which are processed independently during pretraining. In the first chunk, the “Buffalo Bills” (Q221626) is mentioned explicitly—identified through hyperlinks and entity linking—and implicitly, through coreference resolution. Although bothQ221626(Buffalo Bills) andQ40435(the city of Buffalo) share the surface form “Buffalo”,LMEntdisambiguates them as Mention (2) is linked toQ221626with 1.0 confidence by coreference resolution, while Mention (4) is linked toQ40435with 0.98 confidence using entity linking, and the two mentions are placed in separate coreference clusters. Mention (3) “the team” is linked toQ221626since it is in the same coreference cluster as “Buffalo Bills”.",
                "position": 218
            }
        ]
    },
    {
        "header": "2Labeling the English Wikipedia with Fine-Grained Entity Mentions",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.03405/emojis/pushpin.png",
                "caption": "",
                "position": 304
            },
            {
                "img": "https://arxiv.org/html/2509.03405/emojis/link.png",
                "caption": "",
                "position": 310
            },
            {
                "img": "https://arxiv.org/html/2509.03405/emojis/left-arrow.png",
                "caption": "",
                "position": 316
            }
        ]
    },
    {
        "header": "3Entity-based Retrieval from Pretraining Data",
        "images": []
    },
    {
        "header": "4TheLMEntSuite",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.03405/x4.png",
                "caption": "Figure 3:Accuracy on popular PopQA entities as a function of compute budget.LMEntmodels achieve comparable performance to other models with better compute efficiency.",
                "position": 401
            },
            {
                "img": "https://arxiv.org/html/2509.03405/x5.png",
                "caption": "Figure 4:Mean accuracy on PopQA questions binned according to the number of chunks that the subject and answer entities co-occur in. Increasing model size helps learning associations between entities that appear more frequently in the same chunk.",
                "position": 411
            },
            {
                "img": "https://arxiv.org/html/2509.03405/x6.png",
                "caption": "Figure 5:Pairwise wins rates forLMEntwith multiple string-based methods and ablatedLMEntvariations.LMEntoutperforms string-based methods by 66.7%–80.4%. Ablations (bottom three rows) show that hyperlinks and entity linking are the most crucial components ofLMEnt.",
                "position": 522
            },
            {
                "img": "https://arxiv.org/html/2509.03405/x7.png",
                "caption": "Figure 6:Distribution of the number of chunks retrieved byLMEnt, and bothCanonicalandExpandedvariants ofCI-SSandCS-SS. Retrieval was performed for 1K PopQA entities, selected via stratified sampling based on hyperlink counts.LMEntretrieves more chunks than theCanonicalvariants for torso and tail entities, which together account for 99.7% of all entities in Wikipedia. The higher number of chunks returned for head entities byExpandedvariants is likely bloated by noisy retrieval (Fig.˜7).",
                "position": 531
            },
            {
                "img": "https://arxiv.org/html/2509.03405/x8.png",
                "caption": "Figure 7:Precision at various retrieval depths (kk), the top-kkchunks retrieved per entity. As the depth increases,LMEntmaintains above 97% precision, whileCS-SS CanonicalandCI-SS Expandedconsistently decrease to 84% and 27%.",
                "position": 549
            },
            {
                "img": "https://arxiv.org/html/2509.03405/x9.png",
                "caption": "Figure 8:Accuracy forLMEnt-1B-6E model on PopQA. Subj+Answer Chunks counts chunks that mention both the subject and answer entities of a question. Answer Chunks and Subject Chunks count chunks that mention the answer and subject entities individually. Subj Pop and Answer Pop are pageview popularities fromMallen et al. (2023). Subj+Answer Chunks correlates best with model behavior.",
                "position": 566
            }
        ]
    },
    {
        "header": "6Knowledge Acquisition in Pretraining",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.03405/x10.png",
                "caption": "Figure 9:% of facts learned (top) and forgotten (bottom) by fact frequency between intermediate checkpoints ofLMEnt-1B-6E. While fact frequency correlates with gains in knowledge, it is unclear why rates of both learning and forgetting increase with frequency.",
                "position": 590
            }
        ]
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Conclusion and Discussion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix: Supplementary Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.03405/x11.png",
                "caption": "Figure 10:Accuracy on PopQA as a function of compute budget: (left) all entities and (right) questions for which the subject and answer entities appear together in 100-1K chunks.",
                "position": 1708
            },
            {
                "img": "https://arxiv.org/html/2509.03405/x12.png",
                "caption": "",
                "position": 1711
            },
            {
                "img": "https://arxiv.org/html/2509.03405/x13.png",
                "caption": "Figure 11:Accuracy on PAQ as a function of compute budget: (left) all entities and (right) questions for which the subject and answer entities appear together in 1K+ chunks.",
                "position": 1715
            },
            {
                "img": "https://arxiv.org/html/2509.03405/x14.png",
                "caption": "",
                "position": 1718
            },
            {
                "img": "https://arxiv.org/html/2509.03405/x15.png",
                "caption": "Figure 12:Accuracy of OLMo-2-1B(OLMo et al.,2025), Pythia-1.4B(Biderman et al.,2023), and SmolLM-2-1.7B(Allal et al.,2025)on PopQA, sliced by various indicators of popularity and fact frequency on PopQA.",
                "position": 1729
            },
            {
                "img": "https://arxiv.org/html/2509.03405/x16.png",
                "caption": "",
                "position": 1732
            },
            {
                "img": "https://arxiv.org/html/2509.03405/x17.png",
                "caption": "",
                "position": 1734
            },
            {
                "img": "https://arxiv.org/html/2509.03405/x18.png",
                "caption": "Figure 13:Pairwise win rates betweenCS-SS Canonicaland other string-based methods (left). Pairwise win rates betweenCI-SS CanonicalandCI-SS Expanded(right).",
                "position": 2299
            },
            {
                "img": "https://arxiv.org/html/2509.03405/x19.png",
                "caption": "",
                "position": 2302
            },
            {
                "img": "https://arxiv.org/html/2509.03405/x20.png",
                "caption": "Figure 14:Comparison betweenLMEntretrieval and the best performing string-based approach (CS-SS Canonical). Results are grouped by popularity; tail entities (right, under 10 hyperlinks), torso entities (center, between 10 and 1K chunks) center, and head entities (left, at least 1K chunks).LMEntoutperforms string-based retrieval on all popularity levels, and often wins by a large amount of better chunks for popular entities.",
                "position": 2313
            },
            {
                "img": "https://arxiv.org/html/2509.03405/x21.png",
                "caption": "Figure 15:Percentage of net gains in facts learned between intermediate checkpoints ofLMEnt-1B-6E. We analyze these results in detail in §6.",
                "position": 2323
            },
            {
                "img": "https://arxiv.org/html/2509.03405/x22.png",
                "caption": "Figure 16:Error analysis ofLMEntentity mentions.",
                "position": 2434
            },
            {
                "img": "https://arxiv.org/html/2509.03405/x23.png",
                "caption": "Figure 17:Prompt given to Gemini to automatically judge whether a chunk mentions an entity directly.",
                "position": 2460
            },
            {
                "img": "https://arxiv.org/html/2509.03405/figures/Human_Instructions.png",
                "caption": "Figure 18:Instructions given to annotators for evaluating LLM-as-a-judge.",
                "position": 2463
            },
            {
                "img": "https://arxiv.org/html/2509.03405/x24.png",
                "caption": "Figure 19:Prompt given to Gemini 2.5 Flash Preview 6-17 to convert PAQ questions to cloze-style prompts.",
                "position": 2474
            }
        ]
    },
    {
        "header": "Appendix BAppendix: Additional Implementation Details",
        "images": []
    }
]