[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.18998/x1.png",
                "caption": "(a)Performace comparsion.",
                "position": 190
            },
            {
                "img": "https://arxiv.org/html/2602.18998/x1.png",
                "caption": "(a)Performace comparsion.",
                "position": 193
            },
            {
                "img": "https://arxiv.org/html/2602.18998/x2.png",
                "caption": "(b)Sequential test-time scaling.",
                "position": 198
            },
            {
                "img": "https://arxiv.org/html/2602.18998/x3.png",
                "caption": "(c)Parallel test-time scaling.",
                "position": 204
            }
        ]
    },
    {
        "header": "2General AgentBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.18998/x4.png",
                "caption": "Figure 2:Illustration of how General AgentBench covers a wide range of task categories while providing a unified interface to simulate real-world user interactions.The green region indicates the specific task currently being handled by the agent (e.g., a search task). Orange boxes denote other clients and servers that remain active and responsive but are not directly involved in the current interaction. Red indicates that other domain-specific data are excluded.",
                "position": 342
            },
            {
                "img": "https://arxiv.org/html/2602.18998/x5.png",
                "caption": "Figure 3:Relative performance change across domains from the Baseline (BB) specialized agent setting to the general agent (GG) setting with unified context and tools. Negative values indicate performance degradation under the General AgentBench.",
                "position": 533
            }
        ]
    },
    {
        "header": "3Main Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.18998/x6.png",
                "caption": "Figure 4:Performance comparison between specialized-agent and general-agent settings.Top: Absolute performance .Bottom: Relative performance degradation under the general-agent setting.",
                "position": 558
            },
            {
                "img": "https://arxiv.org/html/2602.18998/x7.png",
                "caption": "Figure 5:Test-time scaling behaviors of general LLM agents.Results are reported for five models across four domains on General AgentBench.Top: Parallel scaling expands the solution space through increased sampling.Bottom: Sequential scaling allocates additional computation via longer interaction histories, yet exhibiting unstable or diminishing returns.",
                "position": 561
            }
        ]
    },
    {
        "header": "4Test-Time Scaling Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.18998/x8.png",
                "caption": "Figure 6:Instance-level correctness dynamics under sequential scaling.We randomly sample 10 instances from the reasoning domain and track their correctness across increasing context lengths. Red indicates incorrect predictions and blue indicates correct ones. Most instances exhibit stagnant or oscillatory behavior, repeating prior successes while failing on unresolved cases, with some fluctuating between correct and incorrect across steps.",
                "position": 668
            },
            {
                "img": "https://arxiv.org/html/2602.18998/x9.png",
                "caption": "Figure 7:Sequential scaling behavior of Gemini 2.5-Flash and Qwen3-235B across domains, with inherent context lengths indicated by the dashed line. Performance scales positively as interaction history approaches and slightly exceeds the inherent context; however, it saturates or degrades once the context extends significantly beyond this threshold. This limit represents the “context ceiling” of sequential scaling, beyond which further history yields diminishing returns.",
                "position": 678
            },
            {
                "img": "https://arxiv.org/html/2602.18998/x10.png",
                "caption": "Figure 8:Verification gap between generation and self-choice.Across four domains, we observe a consistent gap between solution generation and verification: as the number of samples increases, correct solutions appear more frequently in the sampled set, yet models often fail to identify and select them. The dashed and dotted curves represent two self-choice strategies, while the diamond denotes a stronger evaluator, GPT-5.",
                "position": 681
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.18998/x11.png",
                "caption": "Figure 9:Pairwise correlation between static long-context benchmarks and agentic domains.“All” denotes the average performance across the search, code, reasoning, and tool-use domains.",
                "position": 2489
            },
            {
                "img": "https://arxiv.org/html/2602.18998/x12.png",
                "caption": "Figure 10:Pairwise correlation between models.",
                "position": 2492
            }
        ]
    },
    {
        "header": "Appendix BAttention Behavior Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.18998/x13.png",
                "caption": "Figure 11:Comparison of attention behaviors under the General AgentBench setting. Top: Qwen3-235B (full attention). Bottom: Qwen3-Next (hybrid linear attention).",
                "position": 2742
            },
            {
                "img": "https://arxiv.org/html/2602.18998/x13.png",
                "caption": "",
                "position": 2745
            },
            {
                "img": "https://arxiv.org/html/2602.18998/x14.png",
                "caption": "",
                "position": 2750
            }
        ]
    },
    {
        "header": "Appendix CEstimated Cost",
        "images": []
    },
    {
        "header": "Appendix DGeneral AgentBench Implementation Details",
        "images": []
    },
    {
        "header": "Appendix EDetailed Prompts",
        "images": []
    }
]