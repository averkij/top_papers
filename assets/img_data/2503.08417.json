[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08417/extracted/6269815/fig/overview_alpha.png",
                "caption": "Figure 2:Overview of AnyMoLe: First, the video diffusion model is fine-tuned without using any external data (Sec.3.1) while the scene-specific joint estimator is trained (Sec.3.3.1). Next, the fine-tuned video generation model produces an in-between video (Sec.3.2), which is then refined through motion video mimicking to generate the final in-between motion (Sec.3.3).",
                "position": 216
            }
        ]
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08417/extracted/6269815/fig/ICAdapt3.png",
                "caption": "Figure 3:Overview of the ICAdapt training process. The spatial module and image injection module are trained, while the others are frozen.",
                "position": 265
            },
            {
                "img": "https://arxiv.org/html/2503.08417/extracted/6269815/fig/inpaint2.png",
                "caption": "Figure 4:Context frames guided video generation process.",
                "position": 294
            },
            {
                "img": "https://arxiv.org/html/2503.08417/extracted/6269815/fig/two_stage.png",
                "caption": "Figure 5:Two stage inference ofDa‚Å¢d‚Å¢psubscriptùê∑ùëéùëëùëùD_{adp}italic_D start_POSTSUBSCRIPT italic_a italic_d italic_p end_POSTSUBSCRIPT. First, at coarse stage, low frame-rate video is generated in auto regressive manner. Next, high frame-rate video is generated from low frame-rate video.",
                "position": 300
            },
            {
                "img": "https://arxiv.org/html/2503.08417/extracted/6269815/fig/joint_estimator.png",
                "caption": "Figure 6:Overview of joint estimator training process.",
                "position": 332
            },
            {
                "img": "https://arxiv.org/html/2503.08417/extracted/6269815/fig/anymole_comparisonrm.png",
                "caption": "Figure 7:Results of baseline comparison. Our method generated in-between frames similar to the ground truth, while SinMDM and SinMDM* generated out-of-context motion (blue box). TST generated partially out-of-context footsteps (blue box) and out-of-style motion (red box). ERD-QV generated motion that has a different style, with a stiff back and sharp hand positions (red box).",
                "position": 369
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08417/extracted/6269815/fig/ablation_add4.png",
                "caption": "Figure 8:Ablation results on video generation. Without applying ICAdapt, each frame of the video exhibited inconsistencies, such as generating noticeable style shifts (blue box). Omitting the fine-stage process resulted in a low frame rate, making identical or significant jumps between frames.",
                "position": 533
            }
        ]
    },
    {
        "header": "5Applications",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08417/extracted/6269815/fig/app.png",
                "caption": "Figure 9:From understanding of the context, AnyMoLe can naturally generate in-between frames in a multi-object scenario.",
                "position": 744
            }
        ]
    },
    {
        "header": "6Discussion and Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08417/extracted/6269815/fig/anymole_limiation.png",
                "caption": "Figure 10:Visualization of a generated frame from a fast turn-around, showing ambiguity that can hinder joint estimation.",
                "position": 754
            }
        ]
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]