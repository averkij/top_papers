[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13313/x1.png",
                "caption": "Figure 1:KlingAvatar 2.0generates vivid, identity-preserving digital humans with accurate camera control, expressive emotions, high-quality motion, and precise facial–lip and audio synchronization. It achieves coherent alignment across audio, image, and text instructions, generalizes to diverse open-domain styles, and supports multi-character synthesis with identity-specific audio control. These capabilities are enabled by our multimodal instruction-following, omni-directed spatial–temporal cascade framework for high-resolution, long-duration video generation.",
                "position": 75
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13313/x2.png",
                "caption": "Figure 2:Overview of the KlingAvatar 2.0 framework. Given multimodal instructions, the Co-Reasoning Director reasons and plans hierarchical, fine-grained positive and negative storylines in a multi-turn dialogue manner, and the spatio-temporal cascade pipeline generates coherent, long-form, high-resolution avatar videos in parallel.",
                "position": 131
            },
            {
                "img": "https://arxiv.org/html/2512.13313/x3.png",
                "caption": "Figure 3:(a) Multi-character video generation pipeline with identity-specific audio control. A mask-prediction head is attached to deep DiT features, and the predicted masks gate ID-specific audio injection into corresponding regions. (b) Automated multi-character video annotation pipeline.",
                "position": 158
            },
            {
                "img": "https://arxiv.org/html/2512.13313/x4.png",
                "caption": "Figure 4:Visualization of GSB benchmark results comparing KlingAvatar 2.0 with HeyGen, Kling-Avatar, and OmniHuman-1.5 across various evaluation criteria.",
                "position": 222
            },
            {
                "img": "https://arxiv.org/html/2512.13313/x5.png",
                "caption": "Figure 5:Qualitative comparison between KlingAvatar 2.0 and baseline methods.Left: Our method produces more natural hair dynamics and vivid facial expressions.Middle: Our results adhere more closely to the specified bottom-to-top camera motion.Right: Our generated video aligns better with the prompt “…turned to the front and folded her hands in front of her chest”.",
                "position": 225
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13313/figures/qualitative.jpg",
                "caption": "Figure 6:Representative qualitative results generated by our spatial–temporal cascade framework with the multimodal co-reasoning director.",
                "position": 280
            },
            {
                "img": "https://arxiv.org/html/2512.13313/x6.png",
                "caption": "Figure 7:Ablation study of the negative director on blueprint keyframes. The negative director enhances facial expressiveness, improves temporal stability and emotional controllability, and reduces lighting and exposure artifacts.",
                "position": 284
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Contributors",
        "images": []
    }
]