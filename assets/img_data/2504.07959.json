[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07959/x1.png",
                "caption": "",
                "position": 130
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07959/x2.png",
                "caption": "Figure 2:Example of CCM calibration (A) and application (B). CCMs are calibrated to transform between CIE XYZ chromaticity and camera-specific raw-RGB values under standard illuminants with predefined color temperatures (e.g., 2856K, 6504K). For other illuminants, the calibrated CCMs are interpolated. As a result, CCMs reflect the camera‚Äôs unique color characteristics, capturing how the cameraperceivesilluminants along the color temperature trajectory.",
                "position": 195
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07959/x3.png",
                "caption": "Figure 3:Overview of the CCMNet architecture. (A) Based on CCC[12]and C5[6], CCMNet includes a networkfùëìfitalic_fthat generates filters and bias from theu‚Å¢vùë¢ùë£uvitalic_u italic_v-histograms of the input image. To process query images from diverse camera domains with varying spectral sensitivities, CCMNet uses a camera fingerprint embedding (CFE) as guidance. (B) The CFE for three example cameras (A, B, V)‚Äîtwo real (A, B) and one imaginary (V)‚Äîis constructed by mapping predefined illuminants (2500K‚Äì7500K along the Planckian locus) from the CIE XYZ space to each camera‚Äôs native raw RGB space using calibrated CCMs. These values are converted into a64√ó64646464\\times 6464 √ó 64histogram and encoded into a 1D vector via a lightweight encoder.",
                "position": 203
            },
            {
                "img": "https://arxiv.org/html/2504.07959/x4.png",
                "caption": "Figure 4:Visualization of our imaginary camera augmentation process. An image from the Sony A57 is white-balanced using the ground-truth illuminant, converted to CIE XYZ space, and mapped to the target camera‚Äôs raw space. We illustrate two cases: mapping to the raw space of a real camera (Fujifilm XM1) and an imaginary camera. Brightness is adjusted for clarity.",
                "position": 429
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07959/x5.png",
                "caption": "Figure 5:Visual comparison of the results from C5[6]with different additional image sets (second, third column) and CCMNet (fourth column). While C5 relies on additional images, CCMNet is optimized for fixed CFE guidance, ensuring consistent performance.",
                "position": 523
            }
        ]
    },
    {
        "header": "5Conclusion and Discussion",
        "images": []
    },
    {
        "header": "Appendix ACCMs & CCTs Extraction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07959/x6.png",
                "caption": "Figure 6:A schematic diagram illustrating the use ofColorMatrixandForwardMatrix. TheForwardMatrix(FM) transforms white-balanced raw data into the CIE XYZ color space, while theColorMatrix(CM) converts CIE XYZ values of a standard light source into the camera‚Äôs native raw color space. FM1 and CM1 are calibrated for standard illuminant A (2856K), and FM2 and CM2 are calibrated for the D65 illuminant (6504K).",
                "position": 1133
            }
        ]
    },
    {
        "header": "Appendix BDetails of the CFE Encoding Process",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07959/x7.png",
                "caption": "Figure 7:Detailed visualization of CFE encoding process. As mentioned in the main paper, the camera‚Äôsfingerprintis derived by converting the reference CIE XYZ colors (locus) along the correlated color temperature (CCT) range of 2500K‚Äì7500K into the corresponding RGB locus asobservedby each device, followed by an encoding process. Due to this characteristic, the CFE feature inherently reflects the color characteristics induced by each camera‚Äôs spectral sensitivity.",
                "position": 1140
            }
        ]
    },
    {
        "header": "Appendix CCamera-to-Camera Mapping",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07959/x8.png",
                "caption": "Figure 8:Overall process of camera-to-camera mapping. In (A), subsets of images taken by different cameras from multiple datasets are white-balanced using the corresponding ground-truth illuminants, and theForwardMatrixis used to convert them to the CIE XYZ space, creating the XYZ image pool. In (B), a reference image is sampled from the pool, and an illumination color is sampled from the augmented illumination pool of the source camera (Camera A) that originally captured the image. The sampled illumination is then mapped to the native RGB space of a randomly selected target camera (Camera B) using theColorMatrix. Finally, in (C), the XYZ image is transformed into the white-balanced color space of Cameras A and B using the inverse of their respectiveForwardMatrices, and illumination casting is applied by multiplying the images with the illumination RGB values of each camera space.",
                "position": 1174
            }
        ]
    },
    {
        "header": "Appendix DImaginary Camera Augmentation Visualizations",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07959/x9.png",
                "caption": "Figure 9:Results of our imaginary camera augmentation. In each row, the leftmost and rightmost images represent the source and target camera images generated using the method described in Sec.C, while the three middle images represent those produced by theimaginarycamera, generated by interpolating between the two devices at ratios of 0.25, 0.5, and 0.75, respectively. As explained in Sec.3.4of the main paper, the CCMs of the imaginary cameras are interpolated using the same alpha values applied during image interpolation, and the resulting CFE embeddings are generated for training. Brightness is adjusted for visibility.",
                "position": 1298
            }
        ]
    },
    {
        "header": "Appendix EExperimental Setup",
        "images": []
    },
    {
        "header": "Appendix FAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07959/x10.png",
                "caption": "Figure 10:Additional results for Canon EOS 1Ds Mark III. CCMNet demonstrates superior performance on various scenes captured by unseen camera. Notably, CCMNet has never been exposed to any images or the CCM of the Canon 1Ds Mark III during training.",
                "position": 1321
            },
            {
                "img": "https://arxiv.org/html/2504.07959/x11.png",
                "caption": "Figure 11:Additional results for various cameras show that CCMNet exhibits robust performance across a range of unseen cameras. Importantly, it has not been exposed to any images or CCMs from the cameras shown in the figure during training.",
                "position": 1324
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]