[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.21220/x1.png",
                "caption": "Figure 1:Vision Search Assistant acquires unknown visual knowledge through web search. An intuitive comparison of answering the user‚Äôs question with an unseen image. The proposed Vision Search Assistant is developed based on LLaVA-1.6-7B, and its ability to answer the question on unseen images outperforms the state-of-the-art models including LLava-1.6-34B[29], Qwen2-VL-72B[5], and InternVL2-76B[11].",
                "position": 63
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.21220/x2.png",
                "caption": "Figure 2:Comparsion with Closed-Source Models including GPT-4o[34], Gemini[37], Claude 3.5 Sonnet[3]with Vision Search Assistantshows that Vision Search Assistant satisfies users‚Äô needs better even if the image is novel.",
                "position": 98
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.21220/x3.png",
                "caption": "Figure 3:Overview of Vision Search Assistant. We first identify the critical objects and generate their descriptions considering their correlations, named Correlated Formulation, using the Vision Language Model (VLM). We then use the LLM to generate sub-questions that leads to the final answer, which is referred to as the Planning Agent. The web pages returned from the search engine are analyzed, selected, and summarized by the same LLM, which is referred to as the Searching Agent. We use the original image, the user‚Äôs prompt, the Correlated Formulation together with the obtained web knowledge to generate the final answer. Vision Search Assistant produces reliable answers, even for novel images, by leveraging the collaboration between VLM and web agents to gather visual information from the web effectively.",
                "position": 135
            }
        ]
    },
    {
        "header": "3Vision Search Assistant",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.21220/x4.png",
                "caption": "Figure 4:The Chain of Search algorithm (¬ß3.2). We deduce the update of the directed graph whenk=1,2,‚ãØùëò12‚ãØk=1,2,\\cdotsitalic_k = 1 , 2 , ‚ãØ, and the web knowledge is progressively extracted from each update.",
                "position": 201
            },
            {
                "img": "https://arxiv.org/html/2410.21220/x5.png",
                "caption": "Figure 5:Open-Set Evaluation: We conduct a human expert evaluation on open-set QA tasks. Vision Search Assistant significantly outperformed Perplexity.ai Pro and GPT-4o-Web across three key objectives: factuality, relevance, and supportiveness.",
                "position": 216
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.21220/x6.png",
                "caption": "Figure 6:Comparisons among Qwen2-VL-72B, InternVL2-76B, and Vision Search Assistant.We compare the open-set QA results on both novel events (the first two rows) and images (the last two rows). Vision Search Assistant excels in generating accurate and detailed results.",
                "position": 353
            },
            {
                "img": "https://arxiv.org/html/2410.21220/x7.png",
                "caption": "Figure 7:Ablation Study on What to Search. We use the object description to avoid the visual redundancy of the image.",
                "position": 359
            },
            {
                "img": "https://arxiv.org/html/2410.21220/x8.png",
                "caption": "Figure 8:Ablation Study on How to search. We propose theChain of Search(¬ß3.2) to obtain related web knowledge for VLMs progressively.",
                "position": 362
            },
            {
                "img": "https://arxiv.org/html/2410.21220/x9.png",
                "caption": "Figure 9:Ablation Study on Complex Scenarios. We use visual correlation to improve the ability in multiple-object scenarios.",
                "position": 365
            },
            {
                "img": "https://arxiv.org/html/2410.21220/x10.png",
                "caption": "Figure 10:A series of demos of Vision Search Assistant on novel images, events, and in-the-wild scenarios.Vision Search Assistant delivers promising potential as a powerful multimodal engine.",
                "position": 368
            }
        ]
    },
    {
        "header": "5Conclusion and Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]