[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Pivotal Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18849/figs/model_performance_win_rate_0925.png",
                "caption": "Figure 1:Performance curve of SFT, DPO and RL with increasing data size.",
                "position": 200
            },
            {
                "img": "https://arxiv.org/html/2510.18849/x1.png",
                "caption": "Figure 2:An illustrative “reward hacking” case from RL training with a BT reward model. The model learns to exploit a shortcut by explicitly mention persona traits to get higher reward scores.",
                "position": 216
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18849/x2.png",
                "caption": "Figure 3:Overview of the Critique-Post-Edit framework. (1) The policy modelπθ\\pi_{\\theta}generates an original responseyoy_{o}. The GRM evaluatesyoy_{o}and provides critique, guiding the model to create an edited responseyey_{e}. (2) The GRM then provides rewards for bothyoy_{o}andyey_{e}, denoted asRoR_{o}andReR_{e}, forming a candidate pool from which a sampling strategy selects edited samples to combine with the original ones into a training batch for the policy update. Sampling strategies include: Random (selecting a random subset), Reward Rank (selecting the highest-reward samples), and Conditional (selecting if the reward exceeds the original reward).",
                "position": 238
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18849/figs/length_vs_winrate_comparison.png",
                "caption": "Figure 4:Comparison between BT reward model and GRM in PPO: length of rollout (during training) and length-controlled winrate of checkpoints, Lines: Response Length (left axis), Bars: Win Rate (right axis)",
                "position": 681
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Implementation Details",
        "images": []
    },
    {
        "header": "8Empirical Results of GRMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18849/figs/model_performance_length_controlled_winrate_0818.png",
                "caption": "(a)Length-controlled win rate across different GRM model scales during RL training. Results are smoothed using a simple moving average with a window size of 3.",
                "position": 1497
            },
            {
                "img": "https://arxiv.org/html/2510.18849/figs/model_performance_length_controlled_winrate_0818.png",
                "caption": "(a)Length-controlled win rate across different GRM model scales during RL training. Results are smoothed using a simple moving average with a window size of 3.",
                "position": 1500
            },
            {
                "img": "https://arxiv.org/html/2510.18849/figs/x_intercept_comparison_binned_smooth_100.png",
                "caption": "(b)Relationship between original scores and post-edit improvements across different model sizes. Curves represent binned smoothing fits with triangular markers indicating x-intercepts.",
                "position": 1505
            }
        ]
    },
    {
        "header": "9Correlation between Human and Different Models",
        "images": []
    },
    {
        "header": "10Length-Controlled Evaluation: Detailed Formulation",
        "images": []
    },
    {
        "header": "11Batch-level Reward Rank Sampling Imbalance",
        "images": []
    },
    {
        "header": "12Selection of the Teacher Model for GRM Distillation",
        "images": []
    },
    {
        "header": "13Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18849/x3.png",
                "caption": "Figure 6:Original Response vs. Edited Response (Based on Feedback)",
                "position": 1730
            }
        ]
    },
    {
        "header": "14Prompt of GRMs",
        "images": []
    }
]