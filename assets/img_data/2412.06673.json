[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06673/extracted/6056443/figures/logo_v1.png",
                "caption": "",
                "position": 91
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06673/x1.png",
                "caption": "Figure 1:Performance on various visual understanding (blue for General and green for Document-oriented), generation (red), and editing (gray) benchmarks. ILLUME achieves competitive results with state-of-the-art works.",
                "position": 116
            },
            {
                "img": "https://arxiv.org/html/2412.06673/x2.png",
                "caption": "Figure 2:ILLUME can handle various multimodal tasks, including understanding for images and charts; text-to-image generation; and mixed-modal generation task such as object modification and style transfer.",
                "position": 119
            },
            {
                "img": "https://arxiv.org/html/2412.06673/x3.png",
                "caption": "Figure 3:Overall architecture of ILLUME. (a) We enhance LLMs with the capability to “see” images by employing a vision adapter that maps features from a vision encoder into LLM’s input spaces. To expand the model’s abilities to generate images, the LLM is extended with an additional vision vocabulary to produce discrete vision tokens. (b) In the vision tokenizer, we utilize a pretrained vision encoder to extract semantic features and supervise quantization process through feature reconstruction loss. The reconstructed features are then processed by a Stable Diffusion model to recover the original images.",
                "position": 204
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06673/x4.png",
                "caption": "Figure 4:Overview of the three-stage training procedure and its corresponding data composition of different stages in MLLM training.",
                "position": 248
            }
        ]
    },
    {
        "header": "3ILLUME",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06673/x5.png",
                "caption": "Figure 5:Procedure of self-enhancing multimodal alignment scheme, which contains three steps: corpus self-generation, assessment generation and SFT for multimodal alignment. This scheme supervises the MLLM to self-assess the consistency between text descriptions and self-generated images, enabling the model to more accurately interpret images and avoid potential mistakes in image generation.",
                "position": 284
            }
        ]
    },
    {
        "header": "4Self-Enhancing Multimodal Alignment",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06673/x6.png",
                "caption": "Figure 6:Comparison of different tokenizers for MLLM training.We compare two types of tokenizers: 1) Reconstruction tokenizer: supervised by image reconstruction loss. 2) Semantic tokenizer: supervised by feature reconstruction loss. The results manifest that vision tokenizer with semantics significantly accelerates the convergence of MLLM pretraining.",
                "position": 1143
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details of Vision Tokenizer",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06673/x7.png",
                "caption": "Figure A:Comparison of different hyper-parameters in inference.",
                "position": 2113
            },
            {
                "img": "https://arxiv.org/html/2412.06673/x8.png",
                "caption": "Figure B:More qualitative results on understanding tasks. Regions that related to the QAs are marked with red ellipses.",
                "position": 2116
            },
            {
                "img": "https://arxiv.org/html/2412.06673/x9.png",
                "caption": "Figure C:More qualitative results on text-to-image generation tasks.",
                "position": 2120
            },
            {
                "img": "https://arxiv.org/html/2412.06673/x10.png",
                "caption": "Figure D:More qualitative results on mixed-modal generation tasks.",
                "position": 2258
            },
            {
                "img": "https://arxiv.org/html/2412.06673/x11.png",
                "caption": "Figure E:Data example of assessment data for self-enhancing multimodal alignment.",
                "position": 2262
            }
        ]
    },
    {
        "header": "Appendix BMore Results of ILLUME",
        "images": []
    }
]