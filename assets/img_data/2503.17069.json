[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.17069/x1.png",
                "caption": "Figure 1:Examples of PVChat‚Äôs ability with one-shot learning (e.g., <Nz>and <Ab>). PVChat can answer questions about the personalized information correctly while other models[5,50]fail.",
                "position": 126
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3PVChat",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.17069/x2.png",
                "caption": "Figure 2:The systematic data collection pipeline. For positive data collection, the original videos are processed by DeepFaceLab[36]for high-quality face and InterVideo2[50]for demographic characteristics, which boost identity preservation. ConsisID[56]and LivePortrait[9]with PhotoMaker[21]utilize the identity information to generate videos of various background or different motion/expression, respectively. For model‚Äôs robust perception, hard negative samples are selected from either similar face retrieval to generate negative videos, or sampled from the CelebV-HQ dataset[61]. These negative samples guarantee the model‚Äôs accurate recognition of both identity and content.",
                "position": 213
            },
            {
                "img": "https://arxiv.org/html/2503.17069/extracted/6299133/QAgen.png",
                "caption": "Figure 3:We illustrate the process of automatically generating question-answer pairs using InternVideo2[50]and ChatGPT[1]. A positive and a negative sample are shown at the bottom.",
                "position": 314
            },
            {
                "img": "https://arxiv.org/html/2503.17069/extracted/6299133/training.png",
                "caption": "Figure 4:(a) The training pipeline of our method. (b) The proposed ReMoH technique for better specialized characteristics learning.",
                "position": 376
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.17069/x3.png",
                "caption": "Figure 5:Examples of PVChat‚Äôs ability with a learned video (e.g., a man named <Sh>and another man named <Ho>). PVChat can recognize and answer questions about the personalized concept in various scenarios, such as medical scenarios (left) and TV series (right).",
                "position": 471
            },
            {
                "img": "https://arxiv.org/html/2503.17069/x4.png",
                "caption": "Figure 6:The hierarchical structure of our prompt library, which is carefully divided into four levels, such as gender, age, and scenarios, and provides different descriptions according to the specific subject.",
                "position": 555
            },
            {
                "img": "https://arxiv.org/html/2503.17069/x5.png",
                "caption": "Figure 7:The comparison of expert heads activation between MoH[12]and ReMoH in different layers, whereHisubscriptHùëñ\\text{H}_{i}H start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTrepresents theit‚Å¢hsuperscriptùëñùë°‚Ñéi^{th}italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPThead. Orange refers to the video without the target individual, while blue represents the video having the character.",
                "position": 562
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "S1Ablation Study About the Token Number",
        "images": []
    },
    {
        "header": "S2Details of the Evaluation Metrics",
        "images": []
    },
    {
        "header": "S3More Examples of Our PVChat Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.17069/extracted/6299133/supp_example_top.png",
                "caption": "Figure 1:Example of PVChat.",
                "position": 1694
            },
            {
                "img": "https://arxiv.org/html/2503.17069/extracted/6299133/supp_example1.png",
                "caption": "Figure 2:Example of PVChat.",
                "position": 1697
            },
            {
                "img": "https://arxiv.org/html/2503.17069/extracted/6299133/supp_example2.png",
                "caption": "Figure 3:Example of PVChat.",
                "position": 1700
            },
            {
                "img": "https://arxiv.org/html/2503.17069/extracted/6299133/supp_example3.png",
                "caption": "Figure 4:Example of PVChat.",
                "position": 1703
            },
            {
                "img": "https://arxiv.org/html/2503.17069/extracted/6299133/supp_example4.png",
                "caption": "Figure 5:Example of PVChat.",
                "position": 1706
            }
        ]
    },
    {
        "header": "S4More experiment detail about Multi-Character training",
        "images": []
    },
    {
        "header": "S5Example of 4 Different Questions",
        "images": []
    },
    {
        "header": "S6Example of queries prompt for GPT or Internvideo",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.17069/extracted/6299133/photo1.jpg",
                "caption": "Figure 6:Prompt for Internvideo and GPT query",
                "position": 3611
            },
            {
                "img": "https://arxiv.org/html/2503.17069/extracted/6299133/photo2.jpg",
                "caption": "",
                "position": 3619
            },
            {
                "img": "https://arxiv.org/html/2503.17069/x6.png",
                "caption": "Figure 8:Display of all dataset.",
                "position": 3624
            }
        ]
    },
    {
        "header": "S7Presentation of 25 characters",
        "images": []
    }
]