[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24078/x1.png",
                "caption": "Figure 1:Overview ofBOB.We extract background and pose attributes from training images using a captioning model (Step 1), applycontext preservationby fine-tuning the T2I model with enriched captions containing class names and context attributes (Step 2), and then performcontext marginalizationby generating synthetic data through randomly sampling background-pose pairs across the entire dataset (Step 3-4). This preserves class-relevant features while reducing spurious class-context associations.",
                "position": 134
            },
            {
                "img": "https://arxiv.org/html/2510.24078/x2.png",
                "caption": "Figure 2:Causal graph of generative process.",
                "position": 165
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24078/x3.png",
                "caption": "Figure 3:Visualizations.left.737-400 images from real data and synthetic data generated by Diff-II, DataDream, andBOB(ours). Diff-II generates images with aircrafts with high contrast in simple backgrounds. DataDream generates more realistic aircrafts that are only on the ground. Our methodBOBgenerate realistic aircrafts in very diverse settings such as taking off, flying, or on the ground with mountainous background, resulting in images that are visually similar to real images.",
                "position": 983
            },
            {
                "img": "https://arxiv.org/html/2510.24078/x4.png",
                "caption": "Figure 4:Density plot of FID of synthetic data against the real data for each class.",
                "position": 991
            },
            {
                "img": "https://arxiv.org/html/2510.24078/x5.png",
                "caption": "Figure 5:Classification accuracy of caption model vs. downstream classifier trained on synthetic data from BOB.",
                "position": 998
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "acknowledgements",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional implementation details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24078/x6.png",
                "caption": "Figure 6:Histogram of the FID difference from DataDream vs.BOBfor each class.",
                "position": 2068
            }
        ]
    },
    {
        "header": "Appendix BAdditional analysis",
        "images": []
    }
]