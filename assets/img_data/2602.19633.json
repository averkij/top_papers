[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.19633/x1.png",
                "caption": "Figure 1:Overview.We illustrate our work using Sokoban, where the goal is to push all boxes onto target locations.(a) Sources of Irrecoverable Failure in the ReAct Framework.A planning error occurs when the internal reasoning suggests a non-viable action (e.g., pushing a box against a wall); this makes the goal unachievable as the agent cannot pull the box from the wall, while a sampling error arises when LM stochasticity leads to an action deviating from the plan.(b) Conceptual Toy Analysis.We model simplified agents by injecting planning and sampling errors into a feasible policy for Sokoban. We measure success rates as the task stepTTincreases, observing that existing frameworks degrade rapidly asTTgrows. SeeAppendix Afor details.(c) Our Framework.TAPE generates and aggregates multiple plans into a graph and uses a solver to select a feasible path, thereby reducing planning errors. Then, it enforces constrained execution to suppress sampling errors.",
                "position": 190
            }
        ]
    },
    {
        "header": "2Problem Formulation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.19633/x2.png",
                "caption": "Figure 2:Overview of TAPE.The proposed framework consists of four steps.\n(a)Plan Graph Construction: The LM samples multiple trajectories, which are aggregated into a plan graph with predicted costs and scores.\n(b)Plan Path Selection: An external solver (e.g., ILP) identifies the optimal path (blue arrows) subject to constraints.\n(c)Constrained Execution: The agent executes the selected actions using constrained decoding to eliminate sampling errors.\n(d)Mismatch Check: If a mismatch occurs between the predicted and realized state, the agent re-performs plan graph construction and path selection; otherwise, the agent executes the next planned action.",
                "position": 421
            }
        ]
    },
    {
        "header": "3Our Approach: TAPE",
        "images": []
    },
    {
        "header": "4Theoretical Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.19633/x3.png",
                "caption": "Figure 3:Success rates across four agentic tasks.We evaluate our framework against ReAct and Plan-and-Act on Sokoban, ALFWorld, Musique, and GSM-Hard. We usegpt-4.1-minifor LM backbone. We find that TAPE consistently demonstrates superior performance over existing frameworks in both easy and hard settings.",
                "position": 654
            }
        ]
    },
    {
        "header": "5Empirical Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.19633/x4.png",
                "caption": "(a)Success Rate across LMs",
                "position": 743
            },
            {
                "img": "https://arxiv.org/html/2602.19633/x4.png",
                "caption": "(a)Success Rate across LMs",
                "position": 751
            },
            {
                "img": "https://arxiv.org/html/2602.19633/x5.png",
                "caption": "(b)Sensitivity toMM",
                "position": 756
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AConceptual Toy Analysis Details",
        "images": []
    },
    {
        "header": "Appendix BRelated Work",
        "images": []
    },
    {
        "header": "Appendix CILP Formulation for Budget Constrained Setting",
        "images": []
    },
    {
        "header": "Appendix DProofs of Theoretical Analysis",
        "images": []
    },
    {
        "header": "Appendix EExperiment Details",
        "images": []
    },
    {
        "header": "Appendix FImplementation Details of TAPE",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.19633/x6.png",
                "caption": "(a)Success under Larger Step Budgets",
                "position": 2626
            },
            {
                "img": "https://arxiv.org/html/2602.19633/x6.png",
                "caption": "(a)Success under Larger Step Budgets",
                "position": 2634
            },
            {
                "img": "https://arxiv.org/html/2602.19633/x7.png",
                "caption": "(b)Success–Cost Trade-off",
                "position": 2639
            },
            {
                "img": "https://arxiv.org/html/2602.19633/x8.png",
                "caption": "Figure 6:Graph construction and plan selection example in Sokoban. Graph is constructed by multiple paths from LLM lookahead plans. Then, ablue pathis selected by a formal solver (ILP). When performing constrained execution, TAPE can accurately performs its plan, achieving the goal.",
                "position": 2664
            },
            {
                "img": "https://arxiv.org/html/2602.19633/x9.png",
                "caption": "(a)ReAct",
                "position": 2671
            },
            {
                "img": "https://arxiv.org/html/2602.19633/x9.png",
                "caption": "(a)ReAct",
                "position": 2674
            },
            {
                "img": "https://arxiv.org/html/2602.19633/x10.png",
                "caption": "(b)Our Framework",
                "position": 2680
            }
        ]
    },
    {
        "header": "Appendix GAdditional Experiment",
        "images": []
    }
]