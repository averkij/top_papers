[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.08600/x1.png",
                "caption": "Figure 1:Demonstration of our work. Previous work on NL2SQL primarily relies on supervised fine-tuning to enable the model to learn how to generate SQL. However, in the case of complex database schema or ambiguous semantics, the fine-tuned model may struggle to produce SQL that does not align with the userâ€™s intentions, as it depends on a fixed generation strategy and previous data. By introducing reinforcement learning algorithms, the model can receive intuitive feedback from the database during the training process. This feedback encourages the model to independently explore various SQL generation reasoning approaches, ultimately enhancing the accuracy of its output.",
                "position": 130
            }
        ]
    },
    {
        "header": "2SQL-R1",
        "images": []
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.08600/extracted/6354434/Figures/visualization_modelsize.png",
                "caption": "Figure 2:Performance and model scale on the BIRD-Dev dataset.",
                "position": 753
            }
        ]
    },
    {
        "header": "4Limitations",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ADetailed Case Study",
        "images": []
    },
    {
        "header": "Appendix BPrompt Templates for Training",
        "images": []
    }
]