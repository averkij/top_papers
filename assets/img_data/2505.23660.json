[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23660/x1.png",
                "caption": "",
                "position": 123
            },
            {
                "img": "https://arxiv.org/html/2505.23660/x2.png",
                "caption": "Figure 2:Uncurated generated samplesfrom D-AR-XL with256√ó256256256256\\times 256256 √ó 256resolutions (CFG=4.0).",
                "position": 130
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23660/x3.png",
                "caption": "Figure 3:Different paradigms incorporating diffusion and autoregressive modelsfor vision generation. (a) uses patch-level diffusion during every single autoregressive step to tackle continuous outputsmar;causalfusion;diffusionforcing; (b) DARTdartdenoises a full image per every autoregressive step by AR transformer, together inputted with history denoised images; (c) use a single set of continuous outputs to different diffusion steps and require diffusion gradients to train AR modelsdreamllm;seedx;nextgpt;metaquery; (d) ours uses vanilla AR models, which can train with discrete inputs/outputs by simply cross entropy, and sequentially decode output tokens with our diffusion tokenizer.",
                "position": 161
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23660/x4.png",
                "caption": "Figure 4:Sequential diffusion tokenizer structure.When training the tokenizer, the pixel diffusion transformer in the tokenizer decoder calculates the velocity loss with the selected group of tokens,ùêú‚Å¢(t)ùêúùë°\\mathbf{c}(t)bold_c ( italic_t ), as conditioning tokens.",
                "position": 219
            }
        ]
    },
    {
        "header": "4Implementations",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23660/x5.png",
                "caption": "Figure 7:Consistent previews as generation trajectoriesfor every increment of 32 tokens (a group). Note that these previews can be generated in a streaming manner with AR tokens partially generated.",
                "position": 874
            },
            {
                "img": "https://arxiv.org/html/2505.23660/x6.png",
                "caption": "Figure 8:Zero-shot layout-controlled synthesiswith different prefix tokens and varying labels.",
                "position": 886
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23660/x7.png",
                "caption": "Figure 9:Vanilla AR comparison for ImageNet generation.",
                "position": 895
            }
        ]
    },
    {
        "header": "I. Detailed Architecture of Sequential Diffusion Tokenizers",
        "images": []
    },
    {
        "header": "II. Detailed Evaluation of D-AR Models",
        "images": []
    },
    {
        "header": "III. Tokenizer Ablations",
        "images": []
    },
    {
        "header": "IV. More Visualizations",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23660/extracted/6494002/fig/sample_c2i_1.png",
                "caption": "Figure 10:Uncurated generated samplesby D-AR-XL with random labels and CFG=4.0.",
                "position": 1165
            },
            {
                "img": "https://arxiv.org/html/2505.23660/extracted/6494002/fig/sample_c2i_2.png",
                "caption": "Figure 11:Uncurated generated samplesby D-AR-XL with random labels and CFG=4.0 (cont‚Äôd).",
                "position": 1168
            },
            {
                "img": "https://arxiv.org/html/2505.23660/extracted/6494002/fig/sample_c2i_3.png",
                "caption": "",
                "position": 1172
            },
            {
                "img": "https://arxiv.org/html/2505.23660/extracted/6494002/fig/sample_c2i_4.png",
                "caption": "",
                "position": 1174
            },
            {
                "img": "https://arxiv.org/html/2505.23660/extracted/6494002/fig/vq1.png",
                "caption": "Figure 12:Reconstruction resultswith samples from the ImageNet validation set. Each pair of rows shows: first row ‚Äî input; second row ‚Äî reconstruction.",
                "position": 1178
            },
            {
                "img": "https://arxiv.org/html/2505.23660/extracted/6494002/fig/vq2.png",
                "caption": "",
                "position": 1182
            },
            {
                "img": "https://arxiv.org/html/2505.23660/extracted/6494002/fig/vq3.png",
                "caption": "",
                "position": 1184
            },
            {
                "img": "https://arxiv.org/html/2505.23660/x8.png",
                "caption": "Figure 13:Generation trajectory and previewsat each diffusion sampling step by D-AR-L.",
                "position": 1194
            },
            {
                "img": "https://arxiv.org/html/2505.23660/x9.png",
                "caption": "",
                "position": 1198
            },
            {
                "img": "https://arxiv.org/html/2505.23660/x10.png",
                "caption": "Figure 14:Zero-shot layout-controlled synthesis.",
                "position": 1208
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]