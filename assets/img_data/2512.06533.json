[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06533/x1.png",
                "caption": "Figure 1:Illustration of decoding-based regression.\nThe inputùê±\\mathbf{x}passes through an encoder to produce the representationœï‚Äã(ùê±)\\phi(\\mathbf{x}), which is then processed by a decoder. The model performs multiple sampling trials to generate several discrete token sequences (e.g., the binary representation<1><1><0>). These sequences are individually detokenized into corresponding scalar values (shown in the stacked layers asy^1=6,y^2=5,y^3=7\\hat{y}_{1}=6,\\hat{y}_{2}=5,\\hat{y}_{3}=7). Finally, these scalar values are combined via an aggregation strategy (e.g., median) to produce the final predictiony^=6\\hat{y}=6.",
                "position": 162
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06533/x2.png",
                "caption": "Figure 2:Comparison between local token-level training and global sequence-level update.Left(existing methods): The model is trained at each token[t1,‚Ä¶,tn][t_{1},\\dots,t_{n}]with a local loss (e.g., CE) that focuses solely on individual tokens.Right(ours): The model generates a full sequence and detokenizes it into a predictiony^\\hat{y}. A global reward (i.e., negative MSE) against the ground truthyyis then backpropagated to update the model parameters.",
                "position": 261
            },
            {
                "img": "https://arxiv.org/html/2512.06533/x3.png",
                "caption": "Figure 3:Training dynamics of GenRe2.Top row: Normalized reward dynamics for GenRe2combined with ReMax (left) and GRPO (right) on 100 TALENT regression tasks, where the reward is normalized to[0,1][0,1]with respect to each task.Bottom row: Visualization of regression performance dynamics onKaggle_bike_sharing_demand_challange(kaggle), comparing GenRe2with NTL-WAS(ntl)and DIST2(DIST2)on test R2score (left, higher is better) and test Wasserstein-1 distance (right, lower is better).",
                "position": 414
            },
            {
                "img": "https://arxiv.org/html/2512.06533/x4.png",
                "caption": "",
                "position": 417
            },
            {
                "img": "https://arxiv.org/html/2512.06533/x5.png",
                "caption": "",
                "position": 419
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06533/x6.png",
                "caption": "Figure 4:Average R2over 100 TALENT regression tasks of different methods under varying normalized tokenization digit bases.",
                "position": 620
            },
            {
                "img": "https://arxiv.org/html/2512.06533/x7.png",
                "caption": "Figure 5:Metric dynamics across 100 TALENT regression tasks. The left sub-figure displays the average best R2@kk, while the right one shows the average mean (dashed) and median (solid) R2@kk.",
                "position": 918
            },
            {
                "img": "https://arxiv.org/html/2512.06533/x8.png",
                "caption": "Figure 6:Impact of GenRe2finetuning on output distribution.(a)GenRe2significantly reduces entropy during training, transforming the initial high-entropy distribution into a sharper, low-entropy distribution that is more accurate (visualized on theKaggle_bike_sharing_demand_challange(kaggle)task).(b)Visualization of the test Wasserstein-1 distance (lower is better) across 100 regression datasets of TALENT benchmark, where GenRe2-ReMax (red) consistently achieves lower distances compared to the base model (blue), demonstrating better approximation towards the ground truth target.",
                "position": 921
            },
            {
                "img": "https://arxiv.org/html/2512.06533/x9.png",
                "caption": "",
                "position": 925
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "Appendix AAdditional Backgrounds",
        "images": []
    },
    {
        "header": "Appendix BDescription of Tokenizations",
        "images": []
    },
    {
        "header": "Appendix CPolicy Gradient Methods",
        "images": []
    },
    {
        "header": "Appendix DExperimental Settings",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06533/x10.png",
                "caption": "Figure 7:The proportion of models achieving the best R2. The length of each bar represents the proportion of the 100 datasets (in which a given method achieved the highest R2) on the TALENT benchmark. Note that all models utilized a normalized tokenizer with base=2.",
                "position": 1652
            },
            {
                "img": "https://arxiv.org/html/2512.06533/x11.png",
                "caption": "Figure 8:The proportion of models achieving the best R2. The length of each bar represents the proportion of the 100 datasets (in which a given method achieved the highest R2) on the TALENT benchmark. Note that all models utilized a normalized tokenizer with base=4.",
                "position": 1655
            },
            {
                "img": "https://arxiv.org/html/2512.06533/x12.png",
                "caption": "Figure 9:The proportion of models achieving the best R2. The length of each bar represents the proportion of the 100 datasets (in which a given method achieved the highest R2) on the TALENT benchmark. Note that all models utilized a normalized tokenizer with base=6.",
                "position": 1658
            },
            {
                "img": "https://arxiv.org/html/2512.06533/x13.png",
                "caption": "Figure 10:The proportion of models achieving the best R2. The length of each bar represents the proportion of the 100 datasets (in which a given method achieved the highest R2) on the TALENT benchmark. Note that all models utilized a normalized tokenizer with base=8.",
                "position": 1661
            },
            {
                "img": "https://arxiv.org/html/2512.06533/x14.png",
                "caption": "Figure 11:The proportion of models achieving the best R2. The length of each bar represents the proportion of the 100 datasets (in which a given method achieved the highest R2) on the TALENT benchmark. Note that all models utilized a normalized tokenizer with base=10.",
                "position": 1664
            },
            {
                "img": "https://arxiv.org/html/2512.06533/x15.png",
                "caption": "Figure 12:Comparison of target value distributions on the APPS Leetcode Dataset across training and validation sets.Top row: Z-score standardization results in distributions with heavy tails and extreme outliers in both the training (left) and validation (right) splits.Bottom row: In contrast, quantile normalization effectively transforms the target values into a well-formed standard normal distribution consistently across both subsets.",
                "position": 1675
            },
            {
                "img": "https://arxiv.org/html/2512.06533/x16.png",
                "caption": "",
                "position": 1679
            },
            {
                "img": "https://arxiv.org/html/2512.06533/x17.png",
                "caption": "Figure 13:Comparison of target value distributions on the Triton Kernel Latency Dataset across training and validation sets.Top row: Z-score standardization results in distributions with heavy tails and extreme outliers in both the training (left) and validation (right) splits.Bottom row: In contrast, quantile normalization effectively transforms the target values into a well-formed standard normal distribution consistently across both subsets.",
                "position": 1683
            },
            {
                "img": "https://arxiv.org/html/2512.06533/x18.png",
                "caption": "",
                "position": 1687
            }
        ]
    },
    {
        "header": "Appendix EAdditional Experiment",
        "images": []
    }
]