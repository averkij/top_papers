[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04440/x1.png",
                "caption": "",
                "position": 60
            },
            {
                "img": "https://arxiv.org/html/2412.04440/x2.png",
                "caption": "",
                "position": 83
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04440/x3.png",
                "caption": "Figure 2:FrameworkofGenMAC.Collaborative workflowincludes three stages with an iterative loop:Design,Generation, andRedesign(Section3.1).Task decompositiondecomposes the redesign stage into four sub-tasks, handled by four agents: verification agent, suggestion agent, correction agent, and output structuring agent (Section3.2).Self-routingmechanism allows for adaptive selection of suitable correction agent to address the diverse requirements for compositional text-to-video generation (Section3.3).",
                "position": 128
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04440/x4.png",
                "caption": "Figure 3:Illustrationof Task Decomposition for theRedesignstage (Section3.2). The diagram illustrates the allocation of roles: verification agent, suggestion agent, correction agent, and output structuring agent within a sequential task breakdown, highlighting the clear responsibilities of each agent.",
                "position": 157
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04440/x5.png",
                "caption": "Figure 4:Qualitative Comparison.Our proposedGenMACgenerates videos that accurately adhere to complex compositional scenarios, demonstrating a clear advantage in handling such requirements in comparision with\nSOTA text-to-video models.",
                "position": 470
            },
            {
                "img": "https://arxiv.org/html/2412.04440/x6.png",
                "caption": "Figure 5:Qualitative Results.Our proposedGenMACgenerates videos that highly aligned with complex compositional prompts, including attribute binding, multiple objects, quantity, and dynamic motion binding.",
                "position": 692
            },
            {
                "img": "https://arxiv.org/html/2412.04440/x7.png",
                "caption": "Figure 6:Visualizationof the iterative refinement process in our multi-agent framework, demonstrating iterations enhance scene accuracy by progressively aligning video content with compositional prompts.",
                "position": 714
            },
            {
                "img": "https://arxiv.org/html/2412.04440/x8.png",
                "caption": "Figure 7:Cumulative Corrected Ratio. For each subset in T2V-CompBench, we calculate the ratio of prompts that have completed the refinement and exited theGenMACloop to the total size of the subset in each iteration. Dynamic attribute binding remains challenging, while generative numeracy, spatial relationships, and motion binding show substantial improvements from iteration 1 to 9.",
                "position": 727
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AFramework Details",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04440/x9.png",
                "caption": "(a)Visualizationof multi-agent collaboration. Initial generation lacks “tug” motion between the rope and boat;Redesignagents adjust spatial alignment and visual tension, leading to a final video that aligns with the prompt’s interaction requirements.",
                "position": 1770
            },
            {
                "img": "https://arxiv.org/html/2412.04440/x9.png",
                "caption": "(a)Visualizationof multi-agent collaboration. Initial generation lacks “tug” motion between the rope and boat;Redesignagents adjust spatial alignment and visual tension, leading to a final video that aligns with the prompt’s interaction requirements.",
                "position": 1773
            },
            {
                "img": "https://arxiv.org/html/2412.04440/x10.png",
                "caption": "(b)Visualizationof the iterative refinement in correcting object quantity and motion direction. TheRedesignagents adjust guidance scale and alignment over successive iterations, progressively enhancing adherence to the prompt.",
                "position": 1778
            },
            {
                "img": "https://arxiv.org/html/2412.04440/x11.png",
                "caption": "(a)The number of corrections.",
                "position": 1785
            },
            {
                "img": "https://arxiv.org/html/2412.04440/x11.png",
                "caption": "(a)The number of corrections.",
                "position": 1788
            },
            {
                "img": "https://arxiv.org/html/2412.04440/x12.png",
                "caption": "(b)The contribution (%) of different guidance types to the video scores withDesignandRedesignstages.",
                "position": 1793
            },
            {
                "img": "https://arxiv.org/html/2412.04440/x13.png",
                "caption": "(c)The contribution (%) of different guidance types to the video scores with only theRedesignstage.",
                "position": 1799
            },
            {
                "img": "https://arxiv.org/html/2412.04440/x14.png",
                "caption": "Figure A10:More qualitative comparisons.",
                "position": 1810
            },
            {
                "img": "https://arxiv.org/html/2412.04440/x15.png",
                "caption": "Figure A11:More qualitative comparisons.",
                "position": 1815
            },
            {
                "img": "https://arxiv.org/html/2412.04440/x16.png",
                "caption": "Figure A12:Qualitative resultsofGenMAC.GenMACshows ability to adhere to complex compositional prompts, including attribute binding for multiple objects, temporal dynamics for object movement, and interactions.",
                "position": 1853
            },
            {
                "img": "https://arxiv.org/html/2412.04440/x17.png",
                "caption": "Figure A13:Qualitative resultsofGenMAC.GenMACshows exhibit superior performances in controllability of generative numeracy, multiple objects with different attributes in compositionality.",
                "position": 1857
            }
        ]
    },
    {
        "header": "Appendix CLimitation and Potential Negative Social Impacts",
        "images": []
    }
]