[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.18983/extracted/6389494/images/arch.png",
                "caption": "Figure 1:Architecture of MediAug: We enhance medical representation learning via advanced visual augmentation.",
                "position": 142
            },
            {
                "img": "https://arxiv.org/html/2504.18983/extracted/6389494/images/6_methods.png",
                "caption": "Figure 2:Augmentation in Eye diseases classification dataset[24]: (a) An image of a cataract. (b)MixUp[99]: Original image and contrast-enhanced image is mixed with a mixing parameterλ=0.42𝜆0.42\\lambda=0.42italic_λ = 0.42. (c)YOCO[34]: splits, flips, and recombines the image for augmentation. (d)CropMix[35]: combines three 25% cropped views using MixUp[99]for augmentation. (e)CutMix[97]: augments by relocating a 1/4 cropped region within the image. (f)AugMix[38]: AugMix blends blurring, sharpening, and color adjustments with the original. (g)SnapMix[42]: SnapMix blends interpolated regions, weighted by saliency maps, with the original.",
                "position": 145
            },
            {
                "img": "https://arxiv.org/html/2504.18983/extracted/6389494/images/brain.png",
                "caption": "Figure 3:Augmentation in Brain Tumor Classification (MRI) dataset[7]: (a) An image of a no_tumor in Testing. (b)MixUp[99]: Contrast-adjusted and mixed (λ=0.30𝜆0.30\\lambda=0.30italic_λ = 0.30), in grayscale and pseudo-color. (c)YOCO[34]: The image was resized, contrast-enhanced, flipped, and concatenated. (d)CropMix[35]: It from a contrast-enhanced, randomly cropped image using CutMix. (e)CutMix[97]: Original image flipped and mixed using random CutMix enhancement (λ=0.70𝜆0.70\\lambda=0.70italic_λ = 0.70). (f)AugMix[38]: It enhances the image with flipping and brightness adjustment. (g)SnapMix[42]: It enhances by blending resized cropped regions with the original image.",
                "position": 183
            },
            {
                "img": "https://arxiv.org/html/2504.18983/extracted/6389494/images/eye-disease-pie-chart.png",
                "caption": "Figure 4:Eye disease classification dataset[24]with four categories (23.9%-26.0%).",
                "position": 229
            },
            {
                "img": "https://arxiv.org/html/2504.18983/extracted/6389494/images/eye-disease-pie-chart.png",
                "caption": "Figure 4:Eye disease classification dataset[24]with four categories (23.9%-26.0%).",
                "position": 232
            },
            {
                "img": "https://arxiv.org/html/2504.18983/extracted/6389494/images/tsne_eye_diseases.png",
                "caption": "Figure 5:t-SNE visualization of eye diseases[24], showing clustering patterns across cataract, diabetic retinopathy, glaucoma, and normal cases.",
                "position": 237
            },
            {
                "img": "https://arxiv.org/html/2504.18983/extracted/6389494/images/brain_pie.png",
                "caption": "Figure 6:Brain MRI dataset[7]showing Meningioma tumor 28.7%, Glioma 28.4%, Pituitary 27.6%, No tumor 15.3%.",
                "position": 277
            },
            {
                "img": "https://arxiv.org/html/2504.18983/extracted/6389494/images/brain_pie.png",
                "caption": "Figure 6:Brain MRI dataset[7]showing Meningioma tumor 28.7%, Glioma 28.4%, Pituitary 27.6%, No tumor 15.3%.",
                "position": 280
            },
            {
                "img": "https://arxiv.org/html/2504.18983/extracted/6389494/images/tsne_Brain_Tumor_Classification.png",
                "caption": "Figure 7:t-SNE visualization of brain tumor categories[7]showing clustering of glioma, meningioma, pituitary, and non-tumor cases.",
                "position": 285
            },
            {
                "img": "https://arxiv.org/html/2504.18983/extracted/6389494/images/BRAIN_VIT-B.png",
                "caption": "Figure 8:ROC curves with ViT-B onbrain tumor dataset[7].",
                "position": 874
            },
            {
                "img": "https://arxiv.org/html/2504.18983/extracted/6389494/images/BRAIN_VIT-B.png",
                "caption": "Figure 8:ROC curves with ViT-B onbrain tumor dataset[7].",
                "position": 877
            },
            {
                "img": "https://arxiv.org/html/2504.18983/extracted/6389494/images/BRAIN_resnet.png",
                "caption": "Figure 9:ROC curves with ResNet-50 onbrain tumor dataset[7].",
                "position": 882
            },
            {
                "img": "https://arxiv.org/html/2504.18983/extracted/6389494/images/EYE_VIT-B.png",
                "caption": "Figure 10:ROC curves with ViT-B oneye diseases dataset[24].",
                "position": 942
            },
            {
                "img": "https://arxiv.org/html/2504.18983/extracted/6389494/images/EYE_VIT-B.png",
                "caption": "Figure 10:ROC curves with ViT-B oneye diseases dataset[24].",
                "position": 945
            },
            {
                "img": "https://arxiv.org/html/2504.18983/extracted/6389494/images/EYE_resnet.png",
                "caption": "Figure 11:ROC curves with ResNet-50 oneye diseases dataset[24].",
                "position": 950
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.18983/x1.png",
                "caption": "Figure 1:Flippingis a data augmentation technique that generates a mirror image by inverting pixel positions across a specified axis, typically horizontally, to expand training datasets and improve model generalization.",
                "position": 1977
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x2.png",
                "caption": "Figure 2:Rotationbased image augmentation transforms the original image by pivoting it around its centre across an axis within 1-359 degrees while preserving pixel relationships.",
                "position": 1984
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x3.png",
                "caption": "Figure 3:Scaling Ratiois a data augmentation technique that transforms images by applying different scaling factors to the x and y axes, creating zoom-in, zoom-out, or non-uniform distortion effects.",
                "position": 1991
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x4.png",
                "caption": "Figure 4:Noise injectionmethod combines transform integration and controlled noise addition to accurately estimate variance in natural images, enabling improved denoising performance.",
                "position": 1998
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x5.png",
                "caption": "Figure 5:Colour spaceaugmentation transforms images by manipulating RGB channels, applying colour shifts, modifying HSV components, isolating individual channels, and converting between different colour spaces while preserving spatial properties.",
                "position": 2005
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x6.png",
                "caption": "Figure 6:Contrastis calculated as a point-wise measure using frequency decomposition to analyze spatial vision and image processing applications.",
                "position": 2012
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x7.png",
                "caption": "Figure 7:Sharpeningwith Adaptive Bilateral Filter increases edge slopes without halos while removing noise, unlike traditional Unsharp Mask techniques.",
                "position": 2019
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x8.png",
                "caption": "Figure 8:Translationis a data augmentation method that shifts an image along an axis while preserving pixel relationships, helping machine learning models develop spatial invariance and prevent location-specific biases.",
                "position": 2026
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x9.png",
                "caption": "Figure 9:Croppinga novel method that extrapolates image content outward to achieve optimal aesthetic composition.",
                "position": 2033
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x10.png",
                "caption": "Figure 10:Shearingtransforms an image by sliding one edge along a vertical or horizontal axis to create a parallelogram effect based on a specified angle.",
                "position": 2040
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x11.png",
                "caption": "Figure 11:Jitterdata augmentation: A technique that generates synthetic power trace variations with time shifts to train CNN models for robust side-channel attacks against protected cryptographic implementations.",
                "position": 2047
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x12.png",
                "caption": "Figure 12:Local Augment: a feature enhancement method that incorporates both visual and geometric contextual information to improve keypoint matching across diverse scenes.",
                "position": 2054
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x13.png",
                "caption": "Figure 13:Kernel Filtersa scale-space transformation method that convolves signals with Gaussian kernels of varying bandwidths to create hierarchical representations through zero-crossing analysis.",
                "position": 2061
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x14.png",
                "caption": "Figure 14:FixMatcha semi-supervised learning method that generates pseudo-labels from weakly-augmented unlabeled images and trains the model to predict these labels from strongly-augmented versions of the same images.",
                "position": 2068
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x15.png",
                "caption": "Figure 15:Augmentation using intensitiestransforms images by modifying pixel values through brightness/contrast adjustments, blurring, normalization, histogram equalization, sharpening, and the addition of various noise patterns.",
                "position": 2075
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x16.png",
                "caption": "Figure 16:cutoutdata augmentation method randomly masks square regions of input images during training to improve CNN robustness and prevent overfitting.",
                "position": 2082
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x17.png",
                "caption": "Figure 17:Hide-and-Seek (HaS)randomly masks image patches during training to force neural networks to learn diverse features beyond the most discriminative regions.",
                "position": 2089
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x18.png",
                "caption": "Figure 18:Random Erasingaugments training data by randomly erasing rectangular regions of images with random pixel values to improve CNN robustness to occlusion.",
                "position": 2096
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x19.png",
                "caption": "Figure 19:GridMasksystematically removes information in a grid pattern to enhance model robustness by forcing neural networks to learn from partially obscured images.",
                "position": 2103
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x20.png",
                "caption": "Figure 20:FenceMaska grid-based occlusion data augmentation method that balances information retention with object occlusion for improved performance in computer vision tasks.",
                "position": 2110
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x21.png",
                "caption": "Figure 21:Pairing Samplesdata augmentation method creates new training samples by pixel-wise averaging of randomly selected image pairs, significantly improving classification accuracy across datasets.",
                "position": 2117
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x22.png",
                "caption": "Figure 22:Mixupaugments training data by linearly interpolating pairs of images and their labels with a mixing parameterλ𝜆\\lambdaitalic_λto improve model generalization and robustness.",
                "position": 2124
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x23.png",
                "caption": "Figure 23:CutMixaugmentation creates new training examples by cutting regions from one image and pasting them onto another while proportionally mixing their class labels.",
                "position": 2131
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x24.png",
                "caption": "Figure 24:CropMixaugments training data by mixing multiple crops at different scales from the same image to capture richer features and improve model performance.",
                "position": 2138
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x25.png",
                "caption": "Figure 25:YOCOThis schematic representation illustrates the You Only Cut Once (YOCO) data augmentation technique, which segments input images for independent transformations, thereby enhancing sample diversity and enabling neural networks to recognize objects from partial information with demonstrated performance improvements across multiple architectures.",
                "position": 2145
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x26.png",
                "caption": "Figure 26:FMixgenerates augmented training samples by combining image pairs using organically-shaped binary masks derived from low-frequency Fourier space sampling, offering advantages over conventional MixUp and CutMix approaches.",
                "position": 2152
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x27.png",
                "caption": "Figure 27:AugMixenhances model robustness through a combination of diverse augmentation chains, stochastic operations, and consistency regularization via Jensen-Shannon divergence loss between original and augmented predictions.",
                "position": 2159
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x28.png",
                "caption": "Figure 28:ManifoldMixinterpolates hidden layer representations to create smoother decision boundaries and improve model robustness against adversarial examples.",
                "position": 2166
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x29.png",
                "caption": "Figure 29:Self-Augmentationcombines regional dropout (Self-Mix) with knowledge sharing across network branches (Self-Distillation) to improve few-shot learning generalization.",
                "position": 2173
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x30.png",
                "caption": "Figure 30:SalfMixThe SalfMix technique uses saliency maps to create augmented training data by intelligently combining original and transformed image regions.",
                "position": 2180
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x31.png",
                "caption": "Figure 31:Cut-Thumbnaildata augmentation Method diagram showing how images are processed into reduced thumbnails and reinserted to enhance shape bias in machine learning models.",
                "position": 2187
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x32.png",
                "caption": "Figure 32:SaliencyMixaugmentation identifies salient regions in a source image and transfers them to a target image. The resulting mixed image is paired with a weighted combination of both labels, proportional to the area of the transferred patch.",
                "position": 2194
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x33.png",
                "caption": "Figure 33:Puzzle Mixintelligently combines images by preserving salient regions through alternating optimization of mixing masks and optimal transport to enhance model generalization and adversarial robustness.",
                "position": 2201
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x34.png",
                "caption": "Figure 34:SnapMixa data augmentation technique that uses Class Activation Maps to create semantically proportional mixed images with corresponding mixed labels for improved fine-grained recognition.",
                "position": 2208
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x35.png",
                "caption": "Figure 35:MixMoframework for neural networks, showing how it improves on previous approaches by using feature mixing (CutMix) between multiple inputs to create stronger, more diverse subnetworks with better performance.",
                "position": 2215
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x36.png",
                "caption": "Figure 36:StyleMixdata augmentation method separately extracts and recombines content and style features from image pairs based on class distance to create robust training samples.",
                "position": 2222
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x37.png",
                "caption": "Figure 37:RandomMixgenerates diverse image augmentations by applying pixel-specific transformations via Gaussian random fields that generalize traditional affine and colour transformations.",
                "position": 2229
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x38.png",
                "caption": "Figure 38:MixMatcha unified semi-supervised learning approach that combines data augmentation, low-entropy pseudo-labelling, and MixUp to achieve state-of-the-art results with limited labelled data.",
                "position": 2236
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x39.png",
                "caption": "Figure 39:ReMixMatchan advanced semi-supervised learning approach that improves upon MixMatch by incorporating distribution alignment, augmentation anchoring, and learned strong augmentations to achieve superior performance with significantly less labelled data.",
                "position": 2243
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x40.png",
                "caption": "Figure 40:Copy-Pastea simple yet effective data augmentation technique that randomly places segmented objects onto images, significantly improving instance segmentation performance, especially for rare categories.",
                "position": 2250
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x41.png",
                "caption": "Figure 41:Mixed-Exampledata augmentation extends beyond traditional label-preserving transformations and linear combinations to explore a broader space of non-linear example mixing techniques for improved neural network training.",
                "position": 2257
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x42.png",
                "caption": "Figure 42:RICAPcombines random crops from four different images into a single composite training sample while mixing their class labels to improve CNN generalization performance.",
                "position": 2264
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x43.png",
                "caption": "Figure 43:CutBluraugments super-resolution training by strategically exchanging patches between low and high-resolution image pairs to teach models where and how much enhancement to apply.",
                "position": 2271
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x44.png",
                "caption": "Figure 44:ResizeMixaugments data by resizing a source image and pasting it onto a target image while preserving complete object information and proportionally mixing their labels.",
                "position": 2278
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x45.png",
                "caption": "Figure 45:Classmixcreates data-augmented images by using network predictions to generate masks that preserve object boundaries when mixing unlabeled images for semi-supervised semantic segmentation.",
                "position": 2285
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x46.png",
                "caption": "Figure 46:CDAenhances knowledge distillation by using intermediate layer representations and contrastive learning to generate high-quality augmented training samples.",
                "position": 2292
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x47.png",
                "caption": "Figure 47:ObjectAugdecouples objects from the background, individually augments them, and reassembles the image to improve semantic segmentation performance.",
                "position": 2299
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x48.png",
                "caption": "Figure 48:AutoAugmentautomatically searches for optimal data augmentation policies by selecting sub-policies with paired operations, each having learned probability and magnitude parameters.",
                "position": 2306
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x49.png",
                "caption": "Figure 49:Fast AutoAugmentaccelerates data augmentation policy search through efficient density matching between hold-out and augmented data distributions, reducing computational requirements while maintaining performance.",
                "position": 2313
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x50.png",
                "caption": "Figure 50:Population-Based Augmentation(PBA)evolves nonstationary policy schedules throughout training using evolutionary algorithms, achieving state-of-the-art performance with orders of magnitude less computation than previous methods.",
                "position": 2320
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x51.png",
                "caption": "Figure 51:RandAugmentsimplifies data augmentation by randomly applying transformations with two tunable parameters, N (number of transformations) and M (magnitude), across different tasks and model sizes.",
                "position": 2327
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x52.png",
                "caption": "Figure 52:KeepAugmentpreserves important image regions during data augmentation by using saliency maps to generate more faithful and informative training examples.",
                "position": 2334
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x53.png",
                "caption": "Figure 53:OHL-Auto-Augenables online, joint optimization of augmentation policies with network training, dramatically reducing computational search costs while maintaining model performance.",
                "position": 2341
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x54.png",
                "caption": "Figure 54:Augmentation-wise Weight Sharingmethod enables efficient augmentation policy search by using a shared neural network to evaluate multiple data augmentation policies simultaneously, dramatically reducing search time while maintaining evaluation reliability.",
                "position": 2348
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x55.png",
                "caption": "Figure 55:RAD(Reinforcement Learning with Augmented Data)enhances RL algorithms by applying diverse data augmentations to visual observations, improving data efficiency and generalization without modifying the core algorithm.",
                "position": 2355
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x56.png",
                "caption": "Figure 56:MARL(Multi-Agent Reinforcement Learning)selectively identifies beneficial data augmentations for multi-agent reinforcement learning, enhancing sample efficiency and training stability in multi-robot systems through adaptive augmentation selection guided by theoretical insights.",
                "position": 2362
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x57.png",
                "caption": "Figure 57:Scale-Aware Automatic Augmentationaddresses object detection’s scale variation challenge by automatically discovering optimal augmentation policies through a specialized search space and Pareto Scale Balance metric that maintains scale invariance across both image and box-level transformations.",
                "position": 2369
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x58.png",
                "caption": "Figure 58:ADA(Adaptive Data Augmentation)employs a dual-model reinforcement learning architecture that dynamically adjusts data augmentation magnitudes based on real-time feedback from the target network, eliminating manual tuning and improving model generalization.",
                "position": 2376
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x59.png",
                "caption": "Figure 59:RADA(Robust Adversarial Data Augmentation)is a robust adversarial data augmentation method that improves camera localization by identifying and perturbing vulnerable pixels rather than applying general image transformations.",
                "position": 2383
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x60.png",
                "caption": "Figure 60:PTDA(Perspective Transformation Data Augmentation)is a data augmentation framework that generates new training images by applying perspective transformations to simulate different camera viewpoints without requiring additional manual annotations.",
                "position": 2390
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x61.png",
                "caption": "Figure 61:DADA(Differentiable Automatic Data Augmentation)is a differentiable data augmentation framework that dramatically accelerates automatic policy search by using Gumbel-Softmax relaxation and the RELAX gradient estimator for one-pass optimization.",
                "position": 2397
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x62.png",
                "caption": "Figure 62:FeatMatchis a semi-supervised learning approach that enhances data augmentation by performing complex transformations in feature space using within-class and across-class prototypical representations.",
                "position": 2404
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x63.png",
                "caption": "Figure 63:Moment Exchangeis a novel data augmentation technique that improves recognition models by swapping feature statistics (mean and standard deviation) between images while interpolating their labels.",
                "position": 2411
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x64.png",
                "caption": "Figure 64:Dataset Augmentation in Feature Spaceis a domain-agnostic approach that performs simple transformations like noise addition, interpolation, and extrapolation in learned feature representations rather than input data.",
                "position": 2418
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x65.png",
                "caption": "Figure 65:Feature Space Augmentation for Long-Tailed Datadecomposes features into class-generic and class-specific components, then generates synthetic samples by fusing class-generic features from data-rich classes with class-specific features from under-represented classes.",
                "position": 2425
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x66.png",
                "caption": "Figure 66:Adversarial Feature Augmentation for Unsupervised Domain Adaptationachieves unsupervised domain adaptation by combining a domain-invariant feature extractor with GAN-based feature space augmentation to bridge the gap between labelled source and unlabeled target domains.",
                "position": 2432
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x67.png",
                "caption": "Figure 67:LDAS(Latent Data Augmentation Strategy)uses a conditional variational autoencoder to generate synthetic power cepstral coefficients representing various structural damage conditions, enabling robust damage classification despite limited real-world training data.",
                "position": 2439
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x68.png",
                "caption": "Figure 68:STaDA(Style Transfer as Data Augmentation)improves image classification accuracy by applying neural style transfer techniques to expand training datasets while preserving semantic content.",
                "position": 2446
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x69.png",
                "caption": "Figure 69:NSTDA(Non-Statistical Targeted Data Augmentation)improves machine learning models by employing targeted data augmentation and synthesis techniques to overcome limited training data challenges in computer vision applications.",
                "position": 2453
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x70.png",
                "caption": "Figure 70:SAS(Self-Augmentation Strategy)improves language model pre-training by using a single network that evolves from MLM-only to joint MLM-RTD training, eliminating the separate generator network needed in previous approaches like ELECTRA.",
                "position": 2460
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x71.png",
                "caption": "Figure 71:Generative adversarial networks(GANs)are a machine learning framework where two neural networks—a generator and a discriminator—compete to create increasingly realistic synthetic data.",
                "position": 2467
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x72.png",
                "caption": "Figure 72:Pix2Pixis a conditional adversarial network framework for image-to-image translation that simultaneously learns both the mapping function and loss function, enabling diverse applications from edge maps to photos, label maps to scenes, and black/white to colourized images.",
                "position": 2474
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x73.png",
                "caption": "Figure 73:CycleGANenables unpaired image-to-image translation through dual generators and cycle consistency, allowing applications like style transfer without requiring matched image pairs.",
                "position": 2481
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x74.png",
                "caption": "Figure 74:StarGANachieves multi-domain image translation with a single unified model through domain labels and cycle consistency, enabling flexible facial attribute manipulation using one generator-discriminator network.",
                "position": 2488
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x75.png",
                "caption": "Figure 75:StarGAN v2advances image-to-image translation by combining a style-based generator with a unified multi-domain framework to produce diverse, high-quality outputs across domains using a single model.",
                "position": 2495
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x76.png",
                "caption": "Figure 76:GAN basedaugmentation uses generative adversarial networks to create synthetic training images through various specialized architectures like conditional, deep convolutional, cycle, and Wasserstein GANs that address quality and stability challenges.",
                "position": 2502
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x77.png",
                "caption": "Figure 77:Channel-wise gamma correctionis independent random gamma transformations applied to each RGB channel to improve retinal vessel segmentation robustness.",
                "position": 2509
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x78.png",
                "caption": "Figure 78:Zooming and CLAHEis a data augmentation approach combining random zoom transformations with contrast enhancement to achieve 98% accuracy in diabetic retinopathy classification.",
                "position": 2516
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x79.png",
                "caption": "Figure 79:Blurring and shiftingdata augmentation techniques for diabetic retinopathy classification showing differential performance in binary versus multiclass 3D-CNN models.",
                "position": 2523
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x80.png",
                "caption": "Figure 80:Heuristic augmentation with NV-like structuresis a data augmentation approach that synthetically generates neovascularization patterns on retinal images to address the underrepresentation of PDR cases in training datasets.",
                "position": 2530
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x81.png",
                "caption": "Figure 81:Deep convolutional GANis a generative adversarial network architecture that synthesizes diverse proliferative diabetic retinopathy images to address class imbalance in training datasets, enabling improved classification performance.",
                "position": 2537
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x82.png",
                "caption": "Figure 82:Conditional GANis a generative model that synthesizes high-resolution diabetic retinopathy images with controllable severity grades by incorporating structural masks, lesion patterns, and grading vectors as input conditions.",
                "position": 2544
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x83.png",
                "caption": "Figure 83:Style based GANis a generative approach that leverages style modulation and dynamic input sampling to create realistic diabetic retinopathy images, addressing class imbalance by synthesizing rare disease cases for improved classifier training.",
                "position": 2551
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x84.png",
                "caption": "Figure 84:StyPathenhances AMR classification in kidney transplant histology through lightweight style transfer augmentation that reduces stain variation bias, improving model accuracy and generalization by simulating diverse staining conditions.",
                "position": 2558
            },
            {
                "img": "https://arxiv.org/html/2504.18983/x85.png",
                "caption": "Figure 85:Deep CNN Ensembleenhances object detection performance by combining multiple complementary CNN architectures with an augmented training set that integrates selected Microsoft COCO images with PASCAL VOC data.",
                "position": 2565
            }
        ]
    },
    {
        "header": "Appendix 0.AData Augmentation Methods",
        "images": []
    }
]