[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03050/x1.png",
                "caption": "Figure 1:Single Speaker Generationv.s.Mulit-human Interactive Talking GenerationandAutomatic Data Collection Pipeline.\nThe pipeline of existing tasks are shown inblue, Co-speech Gesture GenerationGinosar et al. (2019); Liu et al. (2024a), and Talking or Listening Head GenerationCui et al. (2024); Wang et al. (2023).\nIn contrast, Multi-person Interactive Talking Generation enables dynamic speaker interactions by incorporating identity, interactive pose and audio control, as shown inred.\nAnd the automatic data collection is shown consisting of raw data collection, valid video clip extraction and annotation.",
                "position": 92
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03050/x2.png",
                "caption": "Figure 2:Multi-human Interactive Talking Dataset.SapiensKhirodkar et al. (2024)and WhisperVHu et al. (2023)are used to annotate multi-human gesture and interactive speech respectively.\nMIT dataset captures rich conversation interaction pattens of multi-human, such as talking-listening, tune-talking, over-talking and other complex pattens.",
                "position": 147
            }
        ]
    },
    {
        "header": "3Multi-Human Interactive Talking Datset",
        "images": []
    },
    {
        "header": "4Baseline: CovOG",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03050/x3.png",
                "caption": "Figure 3:Overview of proposed method CovOG.(a) The overall architecure of CovOG.\n(b) Implement of Multi-human Pose Encoder used in Pose Adaptor and Pose Guider.\n(c) Implement of Interactive Audio Driver to capture the dynamic facial interaction between multiple speakers.",
                "position": 326
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03050/x4.png",
                "caption": "Figure 4:Qualitative ComparisonandInteraction Visualization.\nLeft: Theredbox indicates the speaker, and thebluebox indicates the listener.\nCompared to AnimateAnyone, CovOG achieves superior lip synchronization for speakers and generates more natural, context-aware responses for listeners.\nRight: Visualization of the alignment with speaking scores, audio (i.e., subtitles), and pose.",
                "position": 660
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]