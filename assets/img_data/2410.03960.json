[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03960/x1.png",
                "caption": "Figure 1:Illustration of SwiftKV with 50% SingleInputKV and 2-way AcrossKV. After distillation, the KV cache of layers 5–8 can all be populated using the hidden state outputs of layer 4. For prefill tokens, layers 5–8 may be skipped altogether, while decode tokens complete all layers. Existing models may be efficiently adapted for SwiftKV by distilling from the original unmodified model using a small dataset. Model knowledge is preserved by keeping the trainable parameters limited to the Q, K, and V projections of the layers affected by SingleInputKV.",
                "position": 124
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3SwiftKV: Design and Implementation",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03960/x2.png",
                "caption": "Figure 2:LEFT: the input similarity of several smaller scale models (Fig.A.1in the Appendix shows a similar observation for larger models). RIGHT: The time per forward pass of Llama-3.1-8B. SingleInputKV effectively reduces the forward pass processing time across a range of batch sizes.",
                "position": 169
            },
            {
                "img": "https://arxiv.org/html/2410.03960/x3.png",
                "caption": "",
                "position": 172
            }
        ]
    },
    {
        "header": "4Main Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03960/x4.png",
                "caption": "Figure 3:Combined input and output throughput for Llama-3.1-8B (left) and Llama-3.1-70B (right) across a range of input lengths (bottom). For each experiment, a number of requests are sent to vLLM at once such that the total number of tokens is roughly 15M. Each request generates 256 output tokens.",
                "position": 571
            },
            {
                "img": "https://arxiv.org/html/2410.03960/x5.png",
                "caption": "Figure 4:Time to first token (TTFT, top) and time per output token (TPOT, bottom) for input lengths 2000 (left), 8000 (middle), and 32000 (right) for Llama-3.1-70B. For each experiment, a range of different request arrival rates is simulated. Each request generates 256 output tokens.",
                "position": 580
            }
        ]
    },
    {
        "header": "5Ablation and Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03960/x6.png",
                "caption": "Figure 5:Density of early exit probabilities and alignment of early exit vs final logits.",
                "position": 901
            },
            {
                "img": "https://arxiv.org/html/2410.03960/x6.png",
                "caption": "Figure 5:Density of early exit probabilities and alignment of early exit vs final logits.",
                "position": 904
            }
        ]
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "Limitations and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Motivation",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03960/x7.png",
                "caption": "Figure A.1:The input similarity of small scale models (left) and large scale models (right).",
                "position": 1779
            },
            {
                "img": "https://arxiv.org/html/2410.03960/x8.png",
                "caption": "",
                "position": 1782
            }
        ]
    },
    {
        "header": "Appendix BCalculating Computation Reduction",
        "images": []
    },
    {
        "header": "Appendix CExperimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03960/x9.png",
                "caption": "Figure C.1:Time to first token (TTFT, top) and time per output token (TPOT, bottom) for input lengths 2000 (left), 8000 (middle), and 32000 (right) for Llama-3.1-8B. For each experiment, a range of different request arrival rates is simulated. Each request generates 256 output tokens.",
                "position": 1860
            }
        ]
    },
    {
        "header": "Appendix DInter-layer AcrossKV vs Intra-Layer KV cache Reduction",
        "images": []
    },
    {
        "header": "Appendix EEarly Exit Details",
        "images": []
    }
]