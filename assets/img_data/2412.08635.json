[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08635/x1.png",
                "caption": "Figure 1:Latent Language Modeling (LatentLM) seamlessly handles continuous (e.g., image, audio, video) and discrete (e.g., text and code) data using causal Transformers. We introduce next-token diffusion to autoregressively generate the latent vectors one by one.\nThe proposed method provides a general-purpose interface that unifies multimodal generation and understanding.",
                "position": 147
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Latent Language Modeling",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08635/x2.png",
                "caption": "Figure 2:LatentLM unifies the modeling of continuous and discrete data. We introduceœÉùúé\\sigmaitalic_œÉ-VAE (Section2.3) to represent continuous data as latent vectors. We perform next-token diffusion (Section2.1) to autoregressively predict the latent vectors one by one. The diffusion head generates vectors by conditioning on the output states of Transformer. The predicted vectors can be decoded to produce the final outputs.",
                "position": 203
            },
            {
                "img": "https://arxiv.org/html/2412.08635/x3.png",
                "caption": "Figure 3:Compared to variational autoencoder (VAE),œÉùúé\\sigmaitalic_œÉ-VAE uses a fixed variance for the latent space. It avoids variance collapse and makes LatentLM more robust to exposure bias during autoregressive generation. In our method,œÉùúé\\sigmaitalic_œÉis a scalar that is sampled fromùí©‚Å¢(0,CœÉ)ùí©0subscriptùê∂ùúé\\mathcal{N}(0,C_{\\sigma})caligraphic_N ( 0 , italic_C start_POSTSUBSCRIPT italic_œÉ end_POSTSUBSCRIPT )for each example.",
                "position": 360
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/scaling_curve.png",
                "caption": "Figure 4:Scaling curves of DiT and LatentLM. FID[27]consistently becomes better with larger model size.",
                "position": 650
            },
            {
                "img": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/imagenet/0.png",
                "caption": "Figure 5:Samples of LatentLM trained on ImageNet. The resolution is 384√ó\\times√ó384. The image is generated by models described in Section3.1.2.",
                "position": 672
            },
            {
                "img": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/imagenet/1.png",
                "caption": "",
                "position": 681
            },
            {
                "img": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/imagenet/2.png",
                "caption": "",
                "position": 686
            },
            {
                "img": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/imagenet/3.png",
                "caption": "",
                "position": 691
            },
            {
                "img": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/imagenet/4.png",
                "caption": "",
                "position": 697
            },
            {
                "img": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/imagenet/5.png",
                "caption": "",
                "position": 702
            },
            {
                "img": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/imagenet/6.png",
                "caption": "",
                "position": 707
            },
            {
                "img": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/imagenet/7.png",
                "caption": "",
                "position": 712
            },
            {
                "img": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/fid_combined.png",
                "caption": "Figure 6:Image generation results of Diffusion Transformer (DiT)[51]and LatentLM on ImageNet. We report FID[27]scores (lower is better) in the settings of different tokenizer variance and CFG[28]scale.\nThe ‚Äústars‚Äù represent the tokenizers tuned for previous image-level diffusion models[53], which are ineffective for LatentLM. The results indicate that LatentLM favors tokenizers with larger variances.",
                "position": 772
            },
            {
                "img": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/inference_bsz128.png",
                "caption": "(a)Throughput with increasing model sizes.",
                "position": 801
            },
            {
                "img": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/inference_bsz128.png",
                "caption": "(a)Throughput with increasing model sizes.",
                "position": 804
            },
            {
                "img": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/inference_XL.png",
                "caption": "(b)Throughput with increasing batch sizes.",
                "position": 809
            },
            {
                "img": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/mllm_fid.png",
                "caption": "(a)Text-to-image FID[27].",
                "position": 953
            },
            {
                "img": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/mllm_fid.png",
                "caption": "(a)Text-to-image FID[27].",
                "position": 956
            },
            {
                "img": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/mllm_ppl.png",
                "caption": "(b)Image-to-text validation perplexity.",
                "position": 961
            },
            {
                "img": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/text-to-image/mountain.png",
                "caption": "(c)A majestic mountain range covered in snow.",
                "position": 968
            },
            {
                "img": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/text-to-image/mountain.png",
                "caption": "(c)A majestic mountain range covered in snow.",
                "position": 971
            },
            {
                "img": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/text-to-image/city.png",
                "caption": "(d)A city street illuminated by lights.",
                "position": 976
            },
            {
                "img": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/text-to-image/lake.png",
                "caption": "(e)A crystal lake surrounded by autumn trees.",
                "position": 981
            },
            {
                "img": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/text-to-image/house.png",
                "caption": "(f)A small house in a wooden at sunset.",
                "position": 986
            },
            {
                "img": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/tts_cfg_scale.png",
                "caption": "(a)Results using different CFG scales.",
                "position": 1532
            },
            {
                "img": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/tts_cfg_scale.png",
                "caption": "(a)Results using different CFG scales.",
                "position": 1535
            },
            {
                "img": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/tts_sampling_steps.png",
                "caption": "(b)Results using different sampling steps.",
                "position": 1540
            }
        ]
    },
    {
        "header": "4Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AHyperparameters for Image Generation Scaling",
        "images": []
    },
    {
        "header": "Appendix BHyperparameters for Tokenizer Analysis",
        "images": []
    },
    {
        "header": "Appendix CInference Efficiency with Different Model Sizes",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/inference_Large.png",
                "caption": "(a)Model Size: 1.03B",
                "position": 2770
            },
            {
                "img": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/inference_Large.png",
                "caption": "(a)Model Size: 1.03B",
                "position": 2773
            },
            {
                "img": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/inference_3B.png",
                "caption": "(b)Model Size: 3.68B",
                "position": 2778
            },
            {
                "img": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/inference_7B.png",
                "caption": "(c)Model Size: 9.35B",
                "position": 2783
            },
            {
                "img": "https://arxiv.org/html/2412.08635/extracted/6062822/figure/inference_13B.png",
                "caption": "(d)Model Size: 17.96B",
                "position": 2788
            }
        ]
    },
    {
        "header": "Appendix DHyperparameters for Multimodal Large Language Model",
        "images": []
    },
    {
        "header": "Appendix EHyperparameters for Text-to-Speech Synthesis",
        "images": []
    }
]