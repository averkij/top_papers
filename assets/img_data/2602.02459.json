[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02459/x1.png",
                "caption": "Figure 1:TIC-VLA enables real-time, language-conditioned navigation by decoupling slow vision-language reasoning from fast reactive control via a delayed semantic-control interface. A latency-consistent training strategy improves robustness under variable reasoning delays. Performance is demonstrated in the DynaNav simulation and real-world indoor and outdoor navigation tasks.",
                "position": 89
            },
            {
                "img": "https://arxiv.org/html/2602.02459/x2.png",
                "caption": "Figure 2:Overview ofTIC-VLA. The architecture adopts a decoupled dual-system design with a fast action expert and a slow reasoning VLM. A shared vision encoder provides real-time observations to the policy and time-lagged observations to the VLM, where the delay arises naturally from slow inference. The delayed semantic-control interface (including delayed VLM KV cache features and latency metadata) is explicitly recorded. The Transformer-based action expert takes as input the current observation, robot state, and delayed semantic-control interface data to generate actions from learnable action queries via cross-attention. Multi-stage training combines imitation learning with delayed inference and reinforcement learning to ensure robustness to real-world, time-sensitive conditions.",
                "position": 128
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02459/x3.png",
                "caption": "Figure 3:Details of TIC-VLA action policy structure, training, and asynchronous execution.\n(a) Latency-aware action policy that predicts action chunks from multimodal inputs.\n(b) Value network used during online reinforcement learning.\n(c) Three-stage latency-consistent training pipeline combining VLM supervision, imitation learning, and reinforcement learning.\n(d) Asynchronous inference and control with explicit latency modeling.",
                "position": 244
            },
            {
                "img": "https://arxiv.org/html/2602.02459/x4.png",
                "caption": "Figure 4:Qualitative results of TIC-VLA closed-loop performance in DynaNav hospital (top) and office (bottom) environments.",
                "position": 295
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02459/x5.png",
                "caption": "Figure 5:The effect of VLM asynchronous reasoning inference latency in TIC-VLA on task performance.",
                "position": 430
            },
            {
                "img": "https://arxiv.org/html/2602.02459/x6.png",
                "caption": "Figure 6:Real-world evaluation of TIC-VLA. (a) Hardware configuration, including the robot platform and computation setup. (b) Designed indoor and outdoor vision-language navigation tasks. (c) Qualitative results from an indoor hallway navigation task, showing the robot following natural language instructions while avoiding obstacles and humans and reaching the goal.",
                "position": 435
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AModel Details",
        "images": []
    },
    {
        "header": "Appendix BTraining Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02459/x7.png",
                "caption": "Figure 7:Overview of the annotation pipeline for VLM SFT. Representative frames and future trajectory information are used to generate long-horizon navigation instructions and concise CoT reasoning annotations.",
                "position": 1392
            },
            {
                "img": "https://arxiv.org/html/2602.02459/x8.png",
                "caption": "Figure 8:Examples of navigation instruction and CoT reasoning annotations from the GND and SCAND datasets.",
                "position": 1395
            }
        ]
    },
    {
        "header": "Appendix CBenchmark Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02459/x9.png",
                "caption": "Figure 9:Overview of the DynaNav benchmark. Task instructions are provided, as well as corresponding navigation scenarios across the Hospital, Office, Outdoor, and Warehouse environments, highlighting variations in scene layout, landmarks, and human density.",
                "position": 1556
            }
        ]
    },
    {
        "header": "Appendix DExperiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02459/x10.png",
                "caption": "Figure 10:Online reinforcement learning tasks used to train TIC-VLA across three environments and tasks.",
                "position": 1571
            },
            {
                "img": "https://arxiv.org/html/2602.02459/x11.png",
                "caption": "Figure 11:Additional qualitative simulation results of TIC-VLA on the DynaNav benchmark. Rows correspond to different environment types:top-outdoor,middle-hospital, andbottom-warehouse.",
                "position": 1898
            },
            {
                "img": "https://arxiv.org/html/2602.02459/x12.png",
                "caption": "Figure 12:Real-world navigation examples demonstratingTIC-VLAexecuting language-conditioned navigation under real-time control.",
                "position": 1950
            }
        ]
    },
    {
        "header": "Appendix EAdditional Results",
        "images": []
    }
]