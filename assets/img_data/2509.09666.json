[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.09666/figures/figure1_new.jpg",
                "caption": "Figure 1:Illustration of the key insight of our UAE, an Auto-Encoder inspired design, for unified multimodal understanding and generation. The encoder converts an input image into a detailed caption (I2T), and the decoder reconstructs the image conditioned on that caption (T2I).\nWe treat reconstruction similarity as the unified score—quantified by Unified-Bench and further optimized by Unified-GRPO, which uses the semantic similarity as the RL reward to reinforce coherent and bidirectional information flow.\nWe observe that, as the unified score increases, the intermediate text output by the understanding model becomes progressively longer, reflecting multimodal “aha moments” and a more unified understanding and generation system.",
                "position": 332
            },
            {
                "img": "https://arxiv.org/html/2509.09666/figures/figure3_new.jpg",
                "caption": "Figure 2:The overall workflow of our Unified-GRPO, consisting of three stages: cold-start reconstruction, generation for understanding, and understanding for generation.",
                "position": 344
            },
            {
                "img": "https://arxiv.org/html/2509.09666/figures/omniae_gen_vis.png",
                "caption": "Figure 3:Visualization results of UAE at 1024×\\times1024 resolution.",
                "position": 350
            },
            {
                "img": "https://arxiv.org/html/2509.09666/figures/figure4_new.jpg",
                "caption": "Figure 4:The detailed illustration of our framework design. Our framework employs an autoregressive LVLM to process the input image embedding derived from the original image. The model generates a text caption, which is then fed into the autoregressive LLM. From this, we extract the final hidden state and project it through a connector into the decoder’s feature space, where it serves as the condition for image generation.",
                "position": 382
            }
        ]
    },
    {
        "header": "2UAE Methodology",
        "images": []
    },
    {
        "header": "3Unified-Bench: A Benchmark tailored for Evaluating the Unified Models",
        "images": []
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.09666/figures/figure5_vis.png",
                "caption": "Figure 5:Qualitative analysis of the results from GenEval++, where our UAE demonstrates visually consistent results aligned with the input captions, and performs reasonable composition for each element.",
                "position": 1453
            },
            {
                "img": "https://arxiv.org/html/2509.09666/figures/figure2_new.jpg",
                "caption": "Figure 6:Illustration of the generated results via our unified-GRPO, with the RL steps increasing, the understanding model (encoder) achieves better caption capability to produce a longer, detailed, yet accurate caption to reconstruct the original image comprehensively; while the generation model (decoder) can take the detailed caption as input for better generation.",
                "position": 1456
            },
            {
                "img": "https://arxiv.org/html/2509.09666/figures/figure6_new.jpg",
                "caption": "Figure 7:Case study of the results from the proposed Unified-Bench, we see that our UAE enables to produce a more detailed, accurate, comprehensive description based on the input image, and reconstructs a similar result to the original image, showcasing the improved understanding and generation capabilities, and the better unification of the system.",
                "position": 1459
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.09666/figures/examples_longcontext.png",
                "caption": "Figure 8:Visual example of the proposed 700K long-context text-to-image dataset.",
                "position": 2134
            },
            {
                "img": "https://arxiv.org/html/2509.09666/figures/examples_longcontext_1.png",
                "caption": "Figure 9:Visual example of the proposed 700K long-context text-to-image dataset.",
                "position": 2137
            },
            {
                "img": "https://arxiv.org/html/2509.09666/figures/prompt_caption.png",
                "caption": "Figure 10:Original Prompt for official LLMs to judge the two captions outputted by the understanding models.",
                "position": 2140
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]