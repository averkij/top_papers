[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15597/x1.png",
                "caption": "Figure 1:Overview ofBeing-H0.Thetext tokenizerandvisual encoderare shared by both pretraining and post-training.\nFor pretraining and hand motion/translation tasks,Being-H0generates outputs in an autoregressive manner. For post-training and downstream manipulation tasks,Being-H0incorporates a set of learnable queries as the action chunk for prediction.",
                "position": 304
            }
        ]
    },
    {
        "header": "3Overview of Being-H0",
        "images": []
    },
    {
        "header": "4Physical Instruction Tuning",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15597/x2.png",
                "caption": "Figure 2:Physical Instruction Tuning.Our training paradigm bridges human video datasets and robotic manipulation through a novel unified physical instruction tuning.Left: Part-level motion tokenization converts continuous hand motions into discrete tokens. Physical space alignment unifies heterogeneous data sources ‚Äî from human hand demonstrations in the videos (dataset) to real-robot data ‚Äî through coordinate system alignment and MANO parameterization, creating consistent representations for both pretraining and post-training supervision.Mid: During pretraining, we extend vision-text parametersŒòv,tsubscriptŒòùë£ùë°\\Theta_{v,t}roman_Œò start_POSTSUBSCRIPT italic_v , italic_t end_POSTSUBSCRIPTto include motion parametersŒòmsubscriptŒòùëö\\Theta_{m}roman_Œò start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT, enabling multi-head attention across vision, text, and motion tokens within a unified sequence. We useblueto denote visual and text attention,redfor motion attention, andyellowfor cross-modal attention.Right: The extension phase shows how attention mechanisms adapt to pretrained cross-modal dependencies (Attnv,t|msubscriptAttnùë£conditionalùë°ùëö\\text{Attn}_{v,t|m}Attn start_POSTSUBSCRIPT italic_v , italic_t | italic_m end_POSTSUBSCRIPT), followed by post-training where action parametersŒòasubscriptŒòùëé\\Theta_{a}roman_Œò start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPTare incorporated to produce the final VLA with parametersŒòa,v,t|msubscriptŒòùëéùë£conditionalùë°ùëö\\Theta_{a,v,t|m}roman_Œò start_POSTSUBSCRIPT italic_a , italic_v , italic_t | italic_m end_POSTSUBSCRIPTfor downstream robotic tasks. Thegreenpart represents action attention.",
                "position": 435
            },
            {
                "img": "https://arxiv.org/html/2507.15597/x3.png",
                "caption": "Figure 3:Architecture of part-level hand motion tokenization based on GRQ.",
                "position": 511
            },
            {
                "img": "https://arxiv.org/html/2507.15597/x4.png",
                "caption": "Figure 4:The overview of ourUniHand-2.5M.Left: The scenes and tasks from different data source types.Mid: The distribution of different data sources, data types, and durations.Right: Samples from different data types.",
                "position": 907
            }
        ]
    },
    {
        "header": "5UniHand: Scaling up Hand Motion Instructional Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15597/x5.png",
                "caption": "Figure 5:Comparison of the proportion of instruction samples derived from original data vs. view-balanced data inUniHand-2.5M.",
                "position": 1151
            }
        ]
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15597/extracted/6640315/fig/06_hardware.png",
                "caption": "Figure 6:The hardware system.",
                "position": 1292
            },
            {
                "img": "https://arxiv.org/html/2507.15597/x6.png",
                "caption": "Figure 7:Ablation of view-invariant motion distribution balancing (‚ÄúBalance‚Äù) on motion reconstruction.",
                "position": 3171
            },
            {
                "img": "https://arxiv.org/html/2507.15597/x7.png",
                "caption": "Figure 8:The performance of\\ModelName-8B alongside the increasing training data scale for visual-grounded hand motion generation. All metrics are normalized to make the last checkpoint (2.5M training samples) represent 100%, with higher values indicating better performance. The metrics include MPJPE, MWTE, PA-MPJPE, M2T R@3, and FID and are averaged on the core split and the tail split.",
                "position": 3188
            },
            {
                "img": "https://arxiv.org/html/2507.15597/x8.png",
                "caption": "Figure 9:Hand Motion Generation Samples from\\ModelName-8B across various tasks, scenes, and viewpoints. A simplified task instruction is given for each block. The three frames illustrate the generated hand motion over time, rendered in the first-frame camera coordinate system and overlaid on the RGB image. Black padding around each image is introduced to enforce consistent weak-perspective projection.",
                "position": 3191
            },
            {
                "img": "https://arxiv.org/html/2507.15597/x9.png",
                "caption": "Figure 10:Being-H0performing Pick-Place-Toy on seen objects, unseen objects, and in cluttered scenes.",
                "position": 3194
            },
            {
                "img": "https://arxiv.org/html/2507.15597/x10.png",
                "caption": "Figure 11:Qualitative comparisons ofBeing-H0and the baseline (InternVL3).",
                "position": 3197
            },
            {
                "img": "https://arxiv.org/html/2507.15597/x11.png",
                "caption": "Figure 12:Comparison of data efficiency betweenBeing-H0and the baseline without human hand pre-training (InternVL3). For each task, the horizontal axis shows the percentage of teleoperation data used for finetuning, and the vertical axis reports the task success rate of the learned policy.",
                "position": 3313
            }
        ]
    },
    {
        "header": "7Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Credit Authorship",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]