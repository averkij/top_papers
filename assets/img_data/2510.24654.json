[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24654/x1.png",
                "caption": "Figure 1:Overview of our method.aillustrates the overview of our method, we establish a virtual clinical environment, DiagGym, that can simulate examination results in real time. Then, within it, we train a diagnostic agent capable of managing multi-turn diagnostic trajectories in a long-term manner, recommendations, recommending diverse examinations until sufficient evidence is gathered for a final diagnosis.bpresents the diagnostics world model we constructed based on EHRs, representing a virtual clinical environment. It receives the patient’s basic profile and performed past examinations, and the next examination query as condition input, then simulates the related results as feedback.cdepicts the DiagAgent end-to-end multi-turn RL training, where the agent interacts with the virtual environment, rolls out different possible diagnostic trajectories, self-explores suitable examination recommendation chains, and iteratively evolves its decision-making policy through end-to-end reinforcement rewards.",
                "position": 226
            }
        ]
    },
    {
        "header": "2Problem Formulation",
        "images": []
    },
    {
        "header": "3Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24654/x2.png",
                "caption": "Figure 2:Overview of simulator evaluation settings.aInstance‑wise metrics: GPT‑4o assesses the quality of generated examination results on an individual patient case level.bExamination‑wise metrics: fidelity and diversity are evaluated by comparing the statistical distributions of generated examination results against those from real cases.",
                "position": 450
            },
            {
                "img": "https://arxiv.org/html/2510.24654/x3.png",
                "caption": "Figure 3:Overview of single-turn evaluation settings and results.ashows the single-turn evaluation setting for examination recommendation measured with the hit ratio.bthe single-turn evaluation setting for final diagnosis measured with the accuracy.ccompares our DiagAgent variants against 10 leading LLMs and 2 agentic systems on examination recommendation and diagnostic decision-making in the single-turn setting.",
                "position": 754
            },
            {
                "img": "https://arxiv.org/html/2510.24654/x4.png",
                "caption": "Figure 4:Overview of end-to-end evaluation settings and results. In this setting, diagnostic agents are evaluated through end-to-end finishing the entire diagnostic trajectory by interaction with the external diagnostics world model.aillustrates the end-to-end evaluation pipeline with automatic metrics to assess examination recommendation efficacy and diagnostic accuracy.bcompares our DiagAgent with 10 LLMs and 2 more agentic systems under end-to-end evaluation settings with automatic metrics.cillustrates our end-to-end evaluation pipeline with rubric-based metrics. A judge model evaluates the full diagnostic trajectory of a diagnostic model against physician-curated rubrics, which specify criteria, clinical importance weights (Points), and whether the criterion was satisfied (Present).dcompares the aggregate weighted proportion of satisfied rubrics(%) across different LLMs in different model sizes.",
                "position": 916
            }
        ]
    },
    {
        "header": "4Discussion",
        "images": []
    },
    {
        "header": "5Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24654/x5.png",
                "caption": "Figure 5:Overview of DiagGym data construction and training pipeline.ashows the process for constructing the DiagGym training dataset.billustrates the pipeline for DiagGym training.",
                "position": 1483
            },
            {
                "img": "https://arxiv.org/html/2510.24654/x6.png",
                "caption": "Figure 6:Overview of DiaAgent data construction and training pipeline.ashows the data construction process for the DiagAgent.billustrates the training pipeline for the diagnostic agent, where the agent interacts with the virtual clinical environment, DiagGym, explores different diagnostic trajectories, and evolves its policy based on reward scores.",
                "position": 1541
            }
        ]
    },
    {
        "header": "6Data Availability",
        "images": []
    },
    {
        "header": "7Code Availability",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "8Acknowledgments",
        "images": []
    },
    {
        "header": "9Author Contributions",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24654/x7.png",
                "caption": "Supplementary Figure 1:Example Case Study from DiagGym: Comparison of Predicted and Ground Truth Examinations.This case presents a single patient profile and final diagnosis, illustrating the step-wise evaluation setting. The core table compares the predicted examination results generated by DiagGym with the ground truth results in sequential order. The rightmost column analyzes key differences and discusses their clinical relevance in the diagnostic process.",
                "position": 2837
            },
            {
                "img": "https://arxiv.org/html/2510.24654/x8.png",
                "caption": "Supplementary Figure 2:Interactive Diagnostic Case Study with DiagAgent: Model Trajectory and Reference Timeline.This figure illustrates a multi-turn interaction between the DiagAgent model and a simulator. The table details the agent’s step-wise reasoning, differential diagnosis, and subsequent actions (e.g., ordering lab tests). The bottom Referenced Multi-Turn Trajectory provides a ground-truth clinical timeline for comparison, demonstrating the established diagnostic process leading to the final diagnosis.",
                "position": 2840
            },
            {
                "img": "https://arxiv.org/html/2510.24654/x9.png",
                "caption": "Supplementary Figure 3:Illustrative Success Case of DiagAgent Evaluated by Physician-Curated Rubrics.This figure presents a case showcasing DiagAgent’s high-quality multi-turn interaction for a lower extremity infection. The top section details the agent’s step-wise dialogue. The bottom table of Procedural Evaluation Rubrics confirms that the agent successfully satisfied the majority of high-weighted, process-oriented criteria, demonstrating procedural integrity beyond merely achieving the correct final diagnosis.",
                "position": 2843
            },
            {
                "img": "https://arxiv.org/html/2510.24654/x10.png",
                "caption": "Supplementary Figure 4:Illustrative Fail Case of DiagAgent: Diagnostic Strength in Contrast to Management Deficit.This case study (ruptured ectopic pregnancy) highlights a current limitation of DiagAgent. While the agent’s Interactive Messages show robust step-wise diagnostic reasoning, successfully leading to the correct final diagnosis, the table of Procedural Evaluation Rubrics reveals critical omissions. Specifically, the agent fails to satisfy high-weighted criteria related to immediate emergency management (e.g., fluid resuscitation and surgical transfer). However, as a primary diagnostic model, the agent’s core capability of accurate differential diagnosis and sequential information gathering remains intact, which is its main contribution.",
                "position": 2846
            }
        ]
    },
    {
        "header": "10Supplementary",
        "images": []
    }
]