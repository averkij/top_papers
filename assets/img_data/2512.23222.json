[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23222/x1.png",
                "caption": "Figure 1:Showcase of UniMAGE‚Äôs multimodal directing abilities.UniMAGE unifies script drafting, extension, continuation, and keyframe image generation, thereby enabling coherent long-form storytelling with consistent characters and cinematic visual compositions. The generated scripts and keyframes can further serve as structured, high-level guidance for existing audio-video joint generation models.",
                "position": 94
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23222/x2.png",
                "caption": "Figure 2:Script structure of UniMAGE. The script structure includes three components: global descriptions (ùí¢\\mathcal{G}), content descriptions (ùíû\\mathcal{C}), and keyframe images (‚Ñ±\\mathcal{F}), together with a user prompt (œÅ\\rho). Special tokens and indicator symbols are used to denote key script elements.",
                "position": 186
            },
            {
                "img": "https://arxiv.org/html/2512.23222/x3.png",
                "caption": "Figure 3:Illustrations of Interleaved Concept Learning and Disentangled Expert Learning.To enhance visual consistency and logical coherence across long-context scripts, as well as to fully leverage both textual and image data, we first optimize all MoT parameters using interleaved text‚Äìimage data, and then disentangle the training of the understanding and generation experts‚Äîusing pure text scripts for the former and text‚Äìimage data for the latter.",
                "position": 212
            },
            {
                "img": "https://arxiv.org/html/2512.23222/x4.png",
                "caption": "Figure 4:Illustrations of In-Context ID Prompting and Pre-Context Script Splitting.The former enhances visual consistency by aligning generated images with global character and scene descriptions, while the latter enables adaptive narrative extension and continuation.",
                "position": 228
            },
            {
                "img": "https://arxiv.org/html/2512.23222/x5.png",
                "caption": "Figure 5:Comparison with the baselines for multi-character script generation.UniMAGE demonstrates a superior ability to maintain consistent character identities and visual coherence across multiple shots, whereas the baseline methods fail to preserve such consistency.",
                "position": 238
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23222/x6.png",
                "caption": "Figure 6:Comparison with the baselines for long-form script generation.UniMAGE demonstrates substantially improved visual coherence, narrative consistency, and scene diversity across long story sequences, whereas the baseline methods are limited in these aspects.",
                "position": 264
            },
            {
                "img": "https://arxiv.org/html/2512.23222/x7.png",
                "caption": "Figure 7:Comparison with SEED-Story on multimodal script generation.UniMAGE demonstrates superior logical coherence, visual quality, and cross-domain generalization, reflecting its stronger capability in unified multimodal understanding and generation.",
                "position": 267
            },
            {
                "img": "https://arxiv.org/html/2512.23222/x8.png",
                "caption": "Figure 8:Ablation experiments.In-Context ID Prompting contributes to preserving consistent multi-character identities across multi-shot and long-form sequences; the red circles mark failure cases without it. Pre-Context Script Splitting ensures coherent and flexible script extension and continuation; the red box highlights content repetition issues when it is absent.",
                "position": 279
            },
            {
                "img": "https://arxiv.org/html/2512.23222/x9.png",
                "caption": "Figure 9:User study.Comparison of average rankings, where lower values correspond to higher quality.",
                "position": 389
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "AMixture-of-Transformers Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23222/x10.png",
                "caption": "Figure 10:Architecture of Mixture-of-Transformers.MoT employs two Transformer experts to process understanding and generation information, and all tokens are processed through a shared multi-modal self-attention in each Transformer block. Two distinct encoders, i.e., ViT and VAE, are adopted separately to capture semantic content and low-level information for image understanding and generation tasks.",
                "position": 433
            }
        ]
    },
    {
        "header": "BSupplementary on Script Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23222/x11.png",
                "caption": "Figure 11:Example of the text‚Äìimage interleaved script dataset.The dataset is constructed by captioning multi-shot video clips and extracting corresponding keyframes.",
                "position": 515
            }
        ]
    },
    {
        "header": "CSupplementary on Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23222/x12.png",
                "caption": "",
                "position": 532
            },
            {
                "img": "https://arxiv.org/html/2512.23222/x13.png",
                "caption": "",
                "position": 534
            },
            {
                "img": "https://arxiv.org/html/2512.23222/x14.png",
                "caption": "",
                "position": 536
            },
            {
                "img": "https://arxiv.org/html/2512.23222/x15.png",
                "caption": "",
                "position": 538
            },
            {
                "img": "https://arxiv.org/html/2512.23222/x16.png",
                "caption": "",
                "position": 540
            },
            {
                "img": "https://arxiv.org/html/2512.23222/x17.png",
                "caption": "",
                "position": 542
            },
            {
                "img": "https://arxiv.org/html/2512.23222/x18.png",
                "caption": "",
                "position": 544
            },
            {
                "img": "https://arxiv.org/html/2512.23222/x19.png",
                "caption": "",
                "position": 546
            },
            {
                "img": "https://arxiv.org/html/2512.23222/x20.png",
                "caption": "",
                "position": 548
            },
            {
                "img": "https://arxiv.org/html/2512.23222/x21.png",
                "caption": "",
                "position": 550
            },
            {
                "img": "https://arxiv.org/html/2512.23222/x22.png",
                "caption": "",
                "position": 552
            },
            {
                "img": "https://arxiv.org/html/2512.23222/x23.png",
                "caption": "",
                "position": 554
            },
            {
                "img": "https://arxiv.org/html/2512.23222/x24.png",
                "caption": "",
                "position": 556
            },
            {
                "img": "https://arxiv.org/html/2512.23222/x25.png",
                "caption": "",
                "position": 558
            },
            {
                "img": "https://arxiv.org/html/2512.23222/x26.png",
                "caption": "",
                "position": 560
            },
            {
                "img": "https://arxiv.org/html/2512.23222/x27.png",
                "caption": "",
                "position": 562
            },
            {
                "img": "https://arxiv.org/html/2512.23222/x28.png",
                "caption": "",
                "position": 564
            },
            {
                "img": "https://arxiv.org/html/2512.23222/x29.png",
                "caption": "",
                "position": 566
            },
            {
                "img": "https://arxiv.org/html/2512.23222/x30.png",
                "caption": "",
                "position": 568
            },
            {
                "img": "https://arxiv.org/html/2512.23222/x31.png",
                "caption": "",
                "position": 570
            },
            {
                "img": "https://arxiv.org/html/2512.23222/x32.png",
                "caption": "",
                "position": 572
            },
            {
                "img": "https://arxiv.org/html/2512.23222/x33.png",
                "caption": "",
                "position": 574
            },
            {
                "img": "https://arxiv.org/html/2512.23222/x34.png",
                "caption": "",
                "position": 576
            },
            {
                "img": "https://arxiv.org/html/2512.23222/x35.png",
                "caption": "",
                "position": 578
            },
            {
                "img": "https://arxiv.org/html/2512.23222/x36.png",
                "caption": "",
                "position": 580
            },
            {
                "img": "https://arxiv.org/html/2512.23222/x37.png",
                "caption": "",
                "position": 582
            },
            {
                "img": "https://arxiv.org/html/2512.23222/x38.png",
                "caption": "",
                "position": 584
            }
        ]
    },
    {
        "header": "DSupplementary on Audio-Video Generation",
        "images": []
    }
]