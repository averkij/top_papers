[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.02304/x1.png",
                "caption": "Figure 1:For different tasks, previous works select different VFMs from general foundation models (path 1). In contrast, we develop a unified token-level foundation model,TokenOCR, specifically tailored for text-image-related tasks (path 2).\nTokenOCR is trained on a substantial self-built dataset,TokenIT, comprising 20 million images and 1.8 billion token-mask pairs. This well-learned model is capable of supplanting other VFMs in related downstream tasks.",
                "position": 119
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.02304/x2.png",
                "caption": "Figure 2:An overview of the self-constructed token-level TokenIT dataset, comprising 20 million images and 1.8 billion text-mask pairs. (a) provides a detailed description of each sample, including the raw image, a mask, and a JSON file that records BPE token information.\nWe also count (b) the data distribution, (c) the number of selected BPE tokens, and (d) a word cloud map highlighting the top 100 BPE tokens.",
                "position": 309
            },
            {
                "img": "https://arxiv.org/html/2503.02304/x3.png",
                "caption": "Figure 3:An overview of the proposed TokenOCR, where the token-level image features and token-level language features are aligned within the same semantic space. This “image-as-text” alignment seamlessly facilitates user-interactive applications, including text segmentation, retrieval, and visual question answering.",
                "position": 322
            }
        ]
    },
    {
        "header": "3TokenIT Dataset",
        "images": []
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.02304/x4.png",
                "caption": "Figure 4:The framework of LLM-guided Token Alignment Training. Existing MLLMs primarily enhance spatial-wise text perception capabilities by integrating localization prompts to predict coordinates. However, this implicit method makes it difficult for these models to have a precise understanding. In contrast, the proposed token alignment uses BPE token masks to directly and explicitly align text with corresponding pixels in the input image, enhancing the MLLM’s localization awareness.",
                "position": 492
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AVQA-based Text Parsing Tasks",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.02304/extracted/6250328/Figure/chart0.png",
                "caption": "Table 8:The illustration of VQA-based Text Parsing tasks of TokenVL.",
                "position": 3500
            }
        ]
    },
    {
        "header": "Appendix BInteractive Demo",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.02304/x5.png",
                "caption": "Figure 5:More visualization examples of the natural scene images, document images, and code images.",
                "position": 3747
            },
            {
                "img": "https://arxiv.org/html/2503.02304/x6.png",
                "caption": "Figure 6:More visualization examples of the chart, table, and GUI images.",
                "position": 3750
            },
            {
                "img": "https://arxiv.org/html/2503.02304/x7.png",
                "caption": "Figure 7:More visualization examples of the Chinese.",
                "position": 3753
            }
        ]
    },
    {
        "header": "Appendix CTokenIT Dataset",
        "images": []
    },
    {
        "header": "Appendix DTraining Details",
        "images": []
    },
    {
        "header": "Appendix EMainstream Benchmark Results",
        "images": []
    }
]