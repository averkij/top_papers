[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.17761/x1.png",
                "caption": "Figure 1:Overview of Step1X-Edit. Step1X-Edit is an open-source general editing model that achieves proprietary-level performance with comprehensive editing capabilities.",
                "position": 85
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Step1X-Edit",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.17761/x2.png",
                "caption": "Figure 2:Data Volume Comparison.",
                "position": 152
            },
            {
                "img": "https://arxiv.org/html/2504.17761/x3.png",
                "caption": "Figure 3:Data Construction Pipeline and Sub-Task Distribution.",
                "position": 223
            },
            {
                "img": "https://arxiv.org/html/2504.17761/x4.png",
                "caption": "Figure 4:Framework of Step1X-Edit. Step1X-Edit leverages the image understanding capabilities of MLLMs to parse editing instructions and generate editing tokens, which are then decoded into images using a DiT-based network.",
                "position": 231
            }
        ]
    },
    {
        "header": "4Benchmark and Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.17761/x5.png",
                "caption": "Figure 5:De-Identification Process.",
                "position": 261
            },
            {
                "img": "https://arxiv.org/html/2504.17761/x6.png",
                "caption": "(a)VIEScore for the Intersection-subset.",
                "position": 268
            },
            {
                "img": "https://arxiv.org/html/2504.17761/x6.png",
                "caption": "(a)VIEScore for the Intersection-subset.",
                "position": 271
            },
            {
                "img": "https://arxiv.org/html/2504.17761/x7.png",
                "caption": "(b)VIEScore for the Full set.",
                "position": 276
            },
            {
                "img": "https://arxiv.org/html/2504.17761/x8.png",
                "caption": "(c)VIEScore for the Intersection-subset.",
                "position": 282
            },
            {
                "img": "https://arxiv.org/html/2504.17761/x9.png",
                "caption": "(d)VIEScore for the Full set.",
                "position": 287
            },
            {
                "img": "https://arxiv.org/html/2504.17761/x10.png",
                "caption": "Figure 7:A Comparative Illustration of Open-Source Approaches and Commercial systems for English Editing Instructions.",
                "position": 667
            },
            {
                "img": "https://arxiv.org/html/2504.17761/x11.png",
                "caption": "Figure 8:A Comparative Illustration of state-of-art algorithms for Chinese Editing Instructions.",
                "position": 670
            },
            {
                "img": "https://arxiv.org/html/2504.17761/x12.png",
                "caption": "(a)User Preference score in the Intersection subset.",
                "position": 726
            },
            {
                "img": "https://arxiv.org/html/2504.17761/x12.png",
                "caption": "(a)User Preference score in the Intersection subset.",
                "position": 729
            },
            {
                "img": "https://arxiv.org/html/2504.17761/x13.png",
                "caption": "(b)User Preference score in the full subset.",
                "position": 734
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Contributors and Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]