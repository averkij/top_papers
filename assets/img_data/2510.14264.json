[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14264/figures/web.png",
                "caption": "",
                "position": 171
            },
            {
                "img": "https://arxiv.org/html/2510.14264/figures/social.png",
                "caption": "",
                "position": 175
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14264/x1.png",
                "caption": "Figure 1:The overall architecture and workflow of AlphaQuanter. The central panel shows the agent’s iterative rollout process. Starting from an initial state (S0S_{0}), the agent first forms an initial plan (P0P_{0}) before generating further reasoning traces (Ri+1R_{i+1}) with<think>tag. In each step, it decides whether to continue acquiring information by executing a tool-based action (Ai+1A_{i+1}) and receiving its environmental feedback (Ei+1E_{i+1}), or to conclude by outputting a final decision with an<answer>tag. Throughout this process, the agent can query multi-dimensional financial data sources (bottompanel, Section6.2). Once a decision is made, the entire trajectory will be evaluated to compute a reward (top-rightpanel, Section4.2), which updates the agent’s policy. The overall workflow (top-leftpanel, Section4.1) is designed to mimic a human trader’s cognitive process of reasoning and acquiring data on demand.",
                "position": 246
            }
        ]
    },
    {
        "header": "3Problem Definition",
        "images": []
    },
    {
        "header": "4AlphaQuanter",
        "images": []
    },
    {
        "header": "5Evaluation",
        "images": []
    },
    {
        "header": "6Experimental Setup",
        "images": []
    },
    {
        "header": "7Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14264/x2.png",
                "caption": "Figure 2:Comparison of training dynamics for the AlphaQuanter-3B and -7B models.",
                "position": 1740
            },
            {
                "img": "https://arxiv.org/html/2510.14264/x3.png",
                "caption": "Figure 3:Comparison of key backtesting metrics for the AlphaQuanter-3B and -7B models on the validation set.",
                "position": 1749
            },
            {
                "img": "https://arxiv.org/html/2510.14264/x4.png",
                "caption": "Figure 4:Evolution of the tool selection strategies for the AlphaQuanter-3B and -7B models during training. The heatmap color intensity shows the percentile-based reliance on each information at different training steps. The symbols [M], [S], [X], and [F] represent the four categories of data sources, respectively.",
                "position": 1758
            },
            {
                "img": "https://arxiv.org/html/2510.14264/x5.png",
                "caption": "Table 6:Impact of reward components and the thresholdθ\\thetaon the performance of the AlphaQuanter-7B model.",
                "position": 1789
            },
            {
                "img": "https://arxiv.org/html/2510.14264/x5.png",
                "caption": "Figure 5:The effect of different decision threshold (θ\\theta) values on the agent’s action distribution during training.",
                "position": 1930
            }
        ]
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetailed Information Sources",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    },
    {
        "header": "Appendix CDetailed Result Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14264/x6.png",
                "caption": "Figure 7:A comparative analysis of the training dynamics for the AlphaQuanter-3B and -7B models, illustrating the evolution of the total reward and its score components.",
                "position": 5169
            }
        ]
    },
    {
        "header": "Appendix DA Working Example",
        "images": []
    }
]