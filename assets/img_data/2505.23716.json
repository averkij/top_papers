[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23716/x1.png",
                "caption": "",
                "position": 116
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23716/x2.png",
                "caption": "Figure 2:Overview of AnySplat.Starting from a set of uncalibrated images, a transformer-based geometry encoder is followed by three decoder heads:FGsubscriptFğº\\mathrm{F}_{G}roman_F start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT,FDsubscriptFğ·\\mathrm{F}_{D}roman_F start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT, andFCsubscriptFğ¶\\mathrm{F}_{C}roman_F start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT, which respectively predict the Gaussian parameters (ğ,Ïƒ,ğ’“,ğ’”,ğ’„ğğœğ’“ğ’”ğ’„\\boldsymbol{\\mu},\\sigma,\\boldsymbol{r},\\boldsymbol{s},\\boldsymbol{c}bold_italic_Î¼ , italic_Ïƒ , bold_italic_r , bold_italic_s , bold_italic_c), the depth mapDğ·Ditalic_D, and the camera posespğ‘pitalic_p.\nThese outputs are used to construct a set of pixel-wise 3D Gaussians, which is then voxelized into pre-voxel 3D Gaussians with the proposed Differentiable Voxelization module. From the voxelized 3D Gaussians, multi-view images and depth maps are subsequently rendered. The rendered images are supervised using an RGB loss against the ground truth image, while the rendered depth maps, along with the decoded depthDğ·Ditalic_Dand camera posespğ‘pitalic_p, are used to compute geometry losses. The geometries are supervised by pseudo-geometry priors (D~,p~~ğ·~ğ‘\\tilde{D},\\tilde{p}over~ start_ARG italic_D end_ARG , over~ start_ARG italic_p end_ARG) obtained by the pretrained VGGT[38].",
                "position": 207
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23716/x3.png",
                "caption": "Figure 3:Qualitative comparisonsagainst baseline methods: for sparse-view inputs, we benchmark against the state-of-the-art FLARE[55]and NoPoSplat[49]; for dense-view inputs, we include 3DGS[17]and MipSplatting[52]as representative comparisons.\nThe slight misalignment between the rendered novel-views and the ground-truth is likely caused by pose-free reconstruction methodâ€™s estimated pose not perfectly matching the annotated groud-truth camera poses.",
                "position": 460
            },
            {
                "img": "https://arxiv.org/html/2505.23716/x4.png",
                "caption": "Figure 4:Example visualization of our AnySplat reconstruction and novel-view synthesis across a spectrum of scene complexities and input frames densities.\nFrom top to bottom, the number of input images increasesâ€”from extremely sparse to medium and dense captures, while the scene scale grows from object-centric setups (LLFF[25], DTU[13]) through mid-scale trajectories (MegaNeRF[37], LERF[18], HorizonGS[14]) to large-scale indoor and outdoor environments (VR-NeRF[45], Waymo[35]). For each setting, we display the input views, the reconstructed 3D Gaussians, the corresponding ground-truth renderings, and example novel-view renderings.",
                "position": 646
            },
            {
                "img": "https://arxiv.org/html/2505.23716/x5.png",
                "caption": "Figure 5:Improved Rendering with Post-Optimization.In our experiments using 200 input views, an optional post-optimization stage yields noticeably higher rendering fidelity, particularly in dense-view scenarios.",
                "position": 938
            },
            {
                "img": "https://arxiv.org/html/2505.23716/x6.png",
                "caption": "Figure 6:Improvements of Multiview Consistency.From the initial iteration to 10 k training steps, we observe a marked enhancement in multiview geometry consistency, clearly visible in the depth renderings, across both the modelâ€™s outputs and the 3D Gaussian Splatting renderings. This confirms the effectiveness of our geometry consistency enhancement design.",
                "position": 942
            },
            {
                "img": "https://arxiv.org/html/2505.23716/extracted/6489078/fig/voxel.png",
                "caption": "Figure 7:Growth of Gaussian Primitives and GPU Memory Usage. As the number of input views increases, the count of Gaussian primitives grows sublinearly and eventually plateaus when using the differentiable voxelization module. In contrast, without this module, the number of Gaussians increases approximately linearly. The GPU memory consumption for rendering mirrors this saturation behavior.",
                "position": 1165
            }
        ]
    },
    {
        "header": "5Conclusion and Future Works",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]