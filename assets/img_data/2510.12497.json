[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12497/x1.png",
                "caption": "Figure 1:Empirical observation ofnoise shift.Denoising generative models suffer from a training–inference misalignment, where the posterior estimation during sampling tends to lean toward larger noise levels.\nTheyellowcurves indicate the estimated probability density of the posteriorpϕ,t​(t∣𝐱^)p_{\\phi,t}(t\\mid\\hat{\\mathbf{x}})for sampled intermediate states𝐱^\\hat{\\mathbf{x}}, while theorangecurves indicate the posteriorpϕ,t​(t∣𝐱)p_{\\phi,t}(t\\mid\\mathbf{x})for intermediate states𝐱\\mathbf{x}stochastically interpolated from training data𝐱0∼pdata​(𝐱0)\\mathbf{x}_{0}\\sim p_{\\text{data}}(\\mathbf{x}_{0})on ImageNet.\nThe (a), (b), and (c) show comparisons between posterior estimates obtained at inference and training, for prior noise levelst=0.7t=0.7,0.50.5, and0.30.3, respectively.\nAll density functions are estimated via kernel density estimation with 5,000 samples.",
                "position": 133
            }
        ]
    },
    {
        "header": "2Prelimiary",
        "images": []
    },
    {
        "header": "3Noise Shift Issue in the Denoising Process",
        "images": []
    },
    {
        "header": "4Noise Awareness Guidance",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12497/x2.png",
                "caption": "Figure 2:Conceptual comparison of guidance behaviors based on class information and noise awareness.(a) A conceptual example of noise shift, where𝐱^t\\hat{\\mathbf{x}}_{t}is drifted to a larger noise level byδ\\delta.\n(b) Class-conditional guidance pushes the trajectory toward regions aligned with the class conditioncc.\n(c) Noise-aware guidance instead pushes𝐱^t\\hat{\\mathbf{x}}_{t}toward the position better aligned with the intended noise levelttfrom the pre-defined prior. NAG explicitly targets the noise shift issue.",
                "position": 364
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12497/x3.png",
                "caption": "Figure 3:FID comparison of vanilla DiTs and SiTson ImageNet256×256256\\times 256after 80 epochs of training.\nClassifier-free guidance (CFG) is not used. All metrics are computed with 10K samples.",
                "position": 520
            },
            {
                "img": "https://arxiv.org/html/2510.12497/x4.png",
                "caption": "Figure 4:Comparisons of the estimated posteriorpϕ​(t∣𝐱)p_{\\phi}(t\\mid\\mathbf{x})on ImageNet256×256256\\times 256with a converged SiT/XL-2 model.\n(a) Noise shift across the entire sampling process, computed as the difference between the estimated posteriorpϕ​(t∣𝐱^)p_{\\phi}(t\\mid\\hat{\\mathbf{x}})and the pre-defined priortt. The visualization shows that noise shiftδ\\deltabecomes increasingly severe as sampling progresses.\n(b) Noise shift measured between the estimatedpϕ​(t∣𝐱^)p_{\\phi}(t\\mid\\hat{\\mathbf{x}})andpϕ​(t∣𝐱)p_{\\phi}(t\\mid\\mathbf{x}), where𝐱\\mathbf{x}is generated from real data. This comparison reflects the training–inference misalignment while accounting for the inherent inaccuracy ofgϕg_{\\phi}.",
                "position": 702
            },
            {
                "img": "https://arxiv.org/html/2510.12497/x5.png",
                "caption": "Figure 5:Empirical observations of NAG mitigating the noise shiftδ\\delta.(a–b) Effects of NAG without interference from CFG.\n(c–d) Compatibility of NAG under CFG, showing that NAG addresses the noise shift directly, rather than relying on the indirect effects of CFG.",
                "position": 707
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADerivation of Statement3",
        "images": []
    },
    {
        "header": "Appendix BImlementations",
        "images": []
    },
    {
        "header": "Appendix CMore Visualization Results with Kernel Density Estimation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12497/x6.png",
                "caption": "Figure 6:More visualization of noise shift.Theyellowcurves indicate the estimated probability density of the posteriorpϕ,t​(t∣𝐱^)p_{\\phi,t}(t\\mid\\hat{\\mathbf{x}})for sampled intermediate states𝐱^\\hat{\\mathbf{x}}, while theorangecurves indicate the posteriorpϕ,t​(t∣𝐱)p_{\\phi,t}(t\\mid\\mathbf{x})for intermediate states𝐱\\mathbf{x}stochastically interpolated from training data𝐱0∼pdata​(𝐱0)\\mathbf{x}_{0}\\sim p_{\\text{data}}(\\mathbf{x}_{0})on ImageNet. The black indicator is the pre-definedtt.",
                "position": 1560
            },
            {
                "img": "https://arxiv.org/html/2510.12497/x7.png",
                "caption": "Figure 7:Additional visualization of how NAG mitigates noise shift.Theyellowcurves represent the estimated probability density of the posteriorpϕ,t​(t∣𝐱^)p_{\\phi,t}(t\\mid\\hat{\\mathbf{x}})for sampled intermediate states𝐱^\\hat{\\mathbf{x}}.\nThebluecurve shows the density influenced by CFG, while thepale goldcurve highlights the mitigating effect of NAG.\nTheorangecurves correspond to the posteriorpϕ,t​(t∣𝐱)p_{\\phi,t}(t\\mid\\mathbf{x})for intermediate states𝐱\\mathbf{x}stochastically interpolated from training data𝐱0∼pdata​(𝐱0)\\mathbf{x}_{0}\\sim p_{\\text{data}}(\\mathbf{x}_{0})on ImageNet.\nThe black indicator denotes the pre-definedtt.",
                "position": 1563
            },
            {
                "img": "https://arxiv.org/html/2510.12497/x8.png",
                "caption": "Figure 8:Hyperparameter sensitivity of NAG.(a) Effect ofwNAGw_{\\text{NAG}}, measured by FID-10K.\n(b) Effect of the number of sampling steps.",
                "position": 1577
            }
        ]
    },
    {
        "header": "Appendix DSensitivity to Hyperparameters of NAG",
        "images": []
    }
]