[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.13871/x1.png",
                "caption": "Figure 1:Comparison of LLaVA-UHD v2 with other MLLMs. (a) MLLMs typically align ViT features to language space using MLPs[63]or perceiver re-samplers[6,52], lacking visual granularity. (b) Combining multiple visual encoders is non-universal and computationally intensive. (c) LLaVA-UHD v2 employs the Hiwin transformer to build aninversefeature pyramid and compress it into visual tokens, providing various semantic granularity for language generation.",
                "position": 106
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.13871/x2.png",
                "caption": "Figure 2:The overall architecture of proposed LLaVA-UHD v2, consisting of a ViT, our hierarchical window transformer (Hiwin transformer), and an LLM. Hiwin transformers process sliced patches and the overview image by capturing inner multi-level representations and compressing them into spatially consistent tokens for a better vision-language alignment.",
                "position": 159
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.13871/x3.png",
                "caption": "Figure 3:Flowchart of the Joint Bilateral Upsampling (JBU) module, which leverages the image pyramid to guide feature up-sampling, integrating high-frequency information into the up-sampled feature maps.",
                "position": 222
            },
            {
                "img": "https://arxiv.org/html/2412.13871/x4.png",
                "caption": "Figure 4:The flowchart of hierarchical window attention. Feature maps from different levels of the feature pyramid are adaptively RoI-aligned into sampling features and then concatenated along the length axis to serve as the key for the learnable queries.",
                "position": 308
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.13871/x5.png",
                "caption": "Figure 5:Performance on different visual tasks with JBU module and vanilla bilinear interpolation. “OCR” denotes the optical character recognition, “Seg” the Linear probing semantic segmentation, and “Cls” the fine-grained classification on SUB-200.",
                "position": 946
            },
            {
                "img": "https://arxiv.org/html/2412.13871/x6.png",
                "caption": "Figure 6:Qualitative comparison of proposed LLaVA-UHD v2 and advanced MLLMs, including LLaVA-Next, Mini-Gemini, and GPT-4V on high-resolution complex perception tasks, which require the integration of both fine-grained visual information and high-level semantic contexts.",
                "position": 1083
            },
            {
                "img": "https://arxiv.org/html/2412.13871/x7.png",
                "caption": "Figure 7:Activation response of specific textual tokens to different visual feature levels. Red circles highlight the obvious difference between levels. (Best viewed in color and zoomed-in)",
                "position": 1208
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "1Appendix",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.13871/x8.png",
                "caption": "Figure 8:Qualitative comparison on high-resolution dense perception task which requires the capabilities of fine-grained details perception.",
                "position": 3204
            },
            {
                "img": "https://arxiv.org/html/2412.13871/x9.png",
                "caption": "Figure 9:Qualitative comparison on high-resolution fine-grained perception task which requires robust fine-grained visual texture perception capabilities.",
                "position": 3208
            },
            {
                "img": "https://arxiv.org/html/2412.13871/x10.png",
                "caption": "Figure 10:Qualitative comparison on high-resolution spatial perception which necessitates the capabilities of high-level spatial contexts.",
                "position": 3212
            },
            {
                "img": "https://arxiv.org/html/2412.13871/x11.png",
                "caption": "Figure 11:PCA visualization of the up-sampled features by JBU module on nature scene. With hierarchical supervision, the high-resolution features (8×8\\times8 ×) could clearly depict object boundary and text appearance. (Best viewed in color and zoomed in)",
                "position": 3216
            },
            {
                "img": "https://arxiv.org/html/2412.13871/x12.png",
                "caption": "Figure 12:PCA visualization of the up-sampled features by JBU module on OCR scene. With hierarchical supervision, the high-resolution features (8×8\\times8 ×) could clearly depict object boundary and text appearance. (Best viewed in color and zoomed in)",
                "position": 3219
            }
        ]
    },
    {
        "header": "Appendix BAnalysis",
        "images": []
    }
]