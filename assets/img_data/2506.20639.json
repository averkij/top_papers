[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.20639/x1.png",
                "caption": "Figure 1:(a) A real example of DiffuCoder-Instruct‚Äôs decoding process with sampling temperature1.21.21.21.2. (b) Results on coding benchmarks. (c) When decoding steps are halved, DiffuCoder-Instruct trained with coupled-GRPO experiences a smaller performance drop, compared to Instruct itself.",
                "position": 170
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries and Notations",
        "images": []
    },
    {
        "header": "3DiffuCoder",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.20639/x2.png",
                "caption": "Figure 2:Pipeline of DiffuCoder training stages and an illustration of the coupled-GRPO algorithm. We sample complementary mask matrices for the same batch, so the coupling probability matrices can be merged into one full matrix. Coupled sampling reduces probability estimation variance while maintaining full token coverage, where each token is sampled exactly the same number of times.",
                "position": 324
            }
        ]
    },
    {
        "header": "4Understanding Mask Diffusion Models based on DiffuCoder",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.20639/x3.png",
                "caption": "Figure 3:Left: Local and global AR-ness across different models and data modalities. Adapted dLLM refers to Dream for the math task and DiffuCoder (Stage 1 trained with 65B tokens) for the code task.Right: (a) Confidence score for each position in the dLLM‚Äôs first forward decoding step. (b) LocalAR-ness@k: the fraction of decoding steps where the newly unmasked token, together with thekùëòkitalic_kimmediately preceding predicted tokens, forms a strictly increasing consecutive sequence, atk=1ùëò1k=1italic_k = 1for next-token prediction. GlobalAR-ness@k: the fraction of decoding steps where the model chooses to unmask one of the earliestkùëòkitalic_kpositions among all remaining masked tokens.",
                "position": 615
            },
            {
                "img": "https://arxiv.org/html/2506.20639/x3.png",
                "caption": "",
                "position": 618
            },
            {
                "img": "https://arxiv.org/html/2506.20639/x4.png",
                "caption": "",
                "position": 622
            },
            {
                "img": "https://arxiv.org/html/2506.20639/x5.png",
                "caption": "Figure 4:AR-ness drifts on different training stages.Left: adaptation pre-training stage and mid-training stage.Right: instruction tuning and RL post-training stage.",
                "position": 628
            },
            {
                "img": "https://arxiv.org/html/2506.20639/x5.png",
                "caption": "",
                "position": 631
            },
            {
                "img": "https://arxiv.org/html/2506.20639/x6.png",
                "caption": "",
                "position": 635
            },
            {
                "img": "https://arxiv.org/html/2506.20639/x7.png",
                "caption": "Figure 5:Affects of different sampling temperatures.Left: For base model and instruct model, changing temperature affects the AR-ness on HumanEval.Right: pass@k curves are different for different temperatures, where triangles refer to score for plus version of each task.",
                "position": 722
            },
            {
                "img": "https://arxiv.org/html/2506.20639/x7.png",
                "caption": "",
                "position": 725
            },
            {
                "img": "https://arxiv.org/html/2506.20639/x8.png",
                "caption": "",
                "position": 729
            },
            {
                "img": "https://arxiv.org/html/2506.20639/x9.png",
                "caption": "Figure 6:Pass@kùëòkitalic_kscores for different models and temperatures.",
                "position": 740
            }
        ]
    },
    {
        "header": "5Coupled-GRPO",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.20639/x10.png",
                "caption": "Figure 7:Reward curves during GRPO training.Left: Comparison between coupled GRPO and d1 baselines.Middle: Decoupled GRPO uses the same number of samplings but with randomly sampled mask noise.Right: Coupled-GRPO is sensitive to the rollout temperature.",
                "position": 824
            },
            {
                "img": "https://arxiv.org/html/2506.20639/x10.png",
                "caption": "",
                "position": 827
            },
            {
                "img": "https://arxiv.org/html/2506.20639/x11.png",
                "caption": "",
                "position": 831
            },
            {
                "img": "https://arxiv.org/html/2506.20639/x12.png",
                "caption": "",
                "position": 835
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProblem Formulation",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.20639/x13.png",
                "caption": "Figure 8:Validation loss distribution of DiffuLLaMA(Gong et¬†al.,2025)for different timesteps. The weighted loss refers to‚Ñítsubscript‚Ñíùë°\\mathcal{L}_{t}caligraphic_L start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTin Eq.¬†(11), while the unweighted loss refers to the cross-entropy term in the masked diffusion loss without the1/t1ùë°1/t1 / italic_tweighting. Timesteps that are too large or too small will lead to extreme values. The sweet spot is in the span of[0.2,0.8]0.20.8[0.2,0.8][ 0.2 , 0.8 ], which is also consistent for DiffuCoder.",
                "position": 3168
            }
        ]
    },
    {
        "header": "Appendix CAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.20639/x14.png",
                "caption": "Figure 9:Visualization of the decoding entropy for random samples. The x-axis is the index of generated token, while y-axis refers to decoding steps. Here we set the diffusion timestep and generation length to be equal.",
                "position": 3267
            },
            {
                "img": "https://arxiv.org/html/2506.20639/x14.png",
                "caption": "",
                "position": 3270
            },
            {
                "img": "https://arxiv.org/html/2506.20639/x15.png",
                "caption": "",
                "position": 3274
            },
            {
                "img": "https://arxiv.org/html/2506.20639/x16.png",
                "caption": "",
                "position": 3278
            },
            {
                "img": "https://arxiv.org/html/2506.20639/extracted/6574020/fig/case1.png",
                "caption": "Figure 10:Visualization of the decoding trajectory of DiffuCoder-Instruct under different sampling temperatures.\nEach character‚Äôs background is colored from red to purple according to the recover order of the[MASK].Left: temperature is0.20.20.20.2;Right: temperature is1.21.21.21.2.",
                "position": 3292
            },
            {
                "img": "https://arxiv.org/html/2506.20639/extracted/6574020/fig/case1.png",
                "caption": "",
                "position": 3295
            },
            {
                "img": "https://arxiv.org/html/2506.20639/extracted/6574020/fig/case2.png",
                "caption": "",
                "position": 3299
            },
            {
                "img": "https://arxiv.org/html/2506.20639/extracted/6574020/fig/case3.png",
                "caption": "",
                "position": 3304
            },
            {
                "img": "https://arxiv.org/html/2506.20639/extracted/6574020/fig/case4.png",
                "caption": "",
                "position": 3308
            },
            {
                "img": "https://arxiv.org/html/2506.20639/x17.png",
                "caption": "Figure 11:Different model variants act differently when changing decoding timesteps to 1/2 of the sequence length. 1x means the default setting where decoding timesteps are equal to the sequence length while 2x means 1/2 fewer steps which will result in 2x speedup.",
                "position": 3319
            },
            {
                "img": "https://arxiv.org/html/2506.20639/x18.png",
                "caption": "Figure 12:Reward curves during GRPO training.Left: Comparison between coupled GRPO and d1 baselines (based on an early version of DiffuCoder-Instruct).Middle: Decoupled GRPO uses the same number of sampling times but with randomly sampled masks (based on an early version of DiffuCoder-Instruct).Right: Coupled-GRPO on DiffuCoder is compared with regular GRPO for the AR model Qwen2.5Coder+SFT.",
                "position": 3338
            },
            {
                "img": "https://arxiv.org/html/2506.20639/x18.png",
                "caption": "",
                "position": 3341
            },
            {
                "img": "https://arxiv.org/html/2506.20639/x19.png",
                "caption": "",
                "position": 3345
            },
            {
                "img": "https://arxiv.org/html/2506.20639/x20.png",
                "caption": "",
                "position": 3349
            }
        ]
    },
    {
        "header": "Appendix DDiscussions",
        "images": []
    }
]