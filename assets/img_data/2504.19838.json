[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.19838/extracted/6395360/figures/llm_vs_agent.drawio.png",
                "caption": "Figure 1:Comparison between conversational LLMs and phone GUI agents. While a conversational LLM can understand queries and provide informative responses (e.g., recommending coffee beans), a Phone GUI agent can go beyond text generation to perceive the device‚Äôs interface, decide on an appropriate action (like tapping an app icon), and execute it in the real environment, thus enabling tasks like ordering a latte directly on the user‚Äôs phone.",
                "position": 157
            }
        ]
    },
    {
        "header": "2Development of Phone Automation",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.19838/extracted/6395360/figures/llm_phone_agent_milestones.drawio.png",
                "caption": "Figure 3:Milestones in the development of LLM-powered phone GUI agents. This figure divides advancements into four primary parts:Prompt Engineering,Training-Based Methods,DatasetsandBenchmarks. Prompt Engineering leverages pre-trained LLMs by strategically crafting input prompts, as detailed in ¬ß4.1, to perform specific tasks without modifying model parameters. In contrast, Training-Based Methods, discussed in ¬ß4.2, involve adapting LLMs via supervised fine-tuning or reinforcement learning on GUI-specific data, thereby enhancing their ability to understand and interact with mobile UIs.",
                "position": 597
            },
            {
                "img": "https://arxiv.org/html/2504.19838/extracted/6395360/figures/basic_pipline_mdp.drawio.png",
                "caption": "Figure 4:POMDP model for ordering a latte. Each circle represents a state (e.g., Home Screen, App Homepage, Latte Details Page, Customize Order, Order Confirmation, Order Complete). The agent starts at the initial stateS0subscriptùëÜ0S_{0}italic_S start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT(Home Screen) and makes decisions at each step (e.g., tapping the Starbucks app icon, selecting the \"Latte\" button, viewing latte details). Due to partial observability, the agent receives limited information at each decision point (e.g.,O0subscriptùëÇ0O_{0}italic_O start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT: Starbucks app icon visible,O1subscriptùëÇ1O_{1}italic_O start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT: \"Latte\" button visible,O2subscriptùëÇ2O_{2}italic_O start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT: Latte product details visible). Some actions correctly advance towards the goal, while others may cause errors requiring corrections. The final goal is to confirm the order.",
                "position": 660
            }
        ]
    },
    {
        "header": "3Frameworks and Components of Phone GUI Agents",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.19838/extracted/6395360/figures/phone_agent_framework.drawio.png",
                "caption": "Figure 5:Overview of MLLM-powered phone GUI agent framework. The user‚Äôs intent, expressed through natural language, is mapped to UI operations. By perceiving UI information and phone state(¬ß3.1) , the agent leverages stored knowledge and memory to plan, reason, and reflect (¬ß3.2) . Finally, it executes actions to fulfill the user‚Äôs goals(¬ß3.3).",
                "position": 697
            },
            {
                "img": "https://arxiv.org/html/2504.19838/extracted/6395360/figures/multi_agent_frameworks.drawio.png",
                "caption": "Figure 6:Comparison of the role-coordinated and scenario-based multi-agent frameworks. The Role-Coordinated framework organizes agents based on general functional roles with a fixed workflow, while the Scenario-Based framework dynamically assigns tasks to specialized agents tailored for specific scenarios, allowing for increased flexibility and adaptability in handling diverse tasks.",
                "position": 924
            },
            {
                "img": "https://arxiv.org/html/2504.19838/extracted/6395360/figures/model_differences.drawio.png",
                "caption": "Figure 7:Differences between training-based methods and prompt engineering in phone automation. Training-based methods adapt the model‚Äôs parameters through additional training, enhancing its ability to perform specific tasks, whereas prompt engineering leverages the existing capabilities of pre-trained models by guiding them with well-designed prompts.",
                "position": 968
            }
        ]
    },
    {
        "header": "4LLMs for Phone Automation",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.19838/extracted/6395360/figures/pe_process.drawio.png",
                "caption": "Figure 8:Schematic of prompt engineering for phone automation.\nThenecessary promptis mandatory, initiating the task, e.g., searching for a Korean restaurant.\nTheoptional promptare supplementary, enhancing tasks without being mandatory.\nTheflexible promptmust include one or more elements from the UI Info, like a screenshot or OCR info, to adapt to task needs.",
                "position": 1014
            },
            {
                "img": "https://arxiv.org/html/2504.19838/extracted/6395360/figures/pe_llm_vs_mllm.drawio.png",
                "caption": "Figure 9:Comparison between text-based prompt and multimodal prompt. In Text-Based Prompt, the LLM processes textual UI information, such as UI tree structures and OCR data, to determine the action type (index). In contrast, Multimodal Prompt integrates screenshot data with supplementary UI information to facilitate decision-making by the agent. The MLLM can then pinpoint the action location using either coordinates or indices.",
                "position": 1026
            },
            {
                "img": "https://arxiv.org/html/2504.19838/extracted/6395360/figures/github.png",
                "caption": "TABLE II:Summary of prompt engineering methods for phone GUI agents",
                "position": 1044
            },
            {
                "img": "https://arxiv.org/html/2504.19838/extracted/6395360/figures/github.png",
                "caption": "TABLE III:Summary of task-specific model architectures",
                "position": 1382
            },
            {
                "img": "https://arxiv.org/html/2504.19838/x1.png",
                "caption": "Figure 10:Illustration of screen understanding tasks. (a)UI Groundinginvolves identifying UI elements corresponding to a given description; (b)UI Referringfocuses on generating descriptions for specified UI elements; (c)Screen Question Answeringrequires answering questions based on the content of the screen.",
                "position": 1852
            },
            {
                "img": "https://arxiv.org/html/2504.19838/extracted/6395360/figures/github.png",
                "caption": "TABLE IV:Summary of supervised fine-tuning methods for phone GUI agents",
                "position": 1900
            },
            {
                "img": "https://arxiv.org/html/2504.19838/extracted/6395360/figures/github.png",
                "caption": "TABLE V:Summary of reinforcement learning methods for phone GUI agents",
                "position": 2124
            }
        ]
    },
    {
        "header": "5Datasets and Benchmarks",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.19838/extracted/6395360/figures/github.png",
                "caption": "TABLE VI:Summary of datasets for phone GUI agents.\n\"Actions\" refers to the number of distinct actions available;\n\"Demos\" refers to the number of demonstration sequences;\n\"Apps\" refers to the number of applications covered;\n\"Instr.\" refers to the number of natural language instructions;\n\"Avg. Steps\" refers to the average number of steps per task.",
                "position": 2303
            },
            {
                "img": "https://arxiv.org/html/2504.19838/extracted/6395360/figures/github.png",
                "caption": "TABLE VII:Summary of benchmarks for phone GUI agents",
                "position": 2604
            }
        ]
    },
    {
        "header": "6Challenges and Future Directions",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]