[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26555/x1.png",
                "caption": "Figure 1:Stable Cinemetricsintroduces structured taxonomies grounded in the controls required for professional video generation. These taxonomies form the foundation of our prompt based benchmark that mirrors real-world shot creation, progressing from scriptwriting to on-screen visuals. Every control element in a prompt is automatically categorized back to the taxonomy, enabling the generation of isolated evaluation questions for independent investigation into each element. This supports large scale human evaluation enabling both coarse and fine-grained insights into the capabilities of current models for professional video generation. To drive scalable annotations, we develop our own VLMs that outperform existing models in alignment with human judgements.",
                "position": 116
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26555/x2.png",
                "caption": "(a)TheSetuptaxonomy outlines the visual components within the frame, including subjects, props, and environmental context.",
                "position": 141
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x2.png",
                "caption": "(a)TheSetuptaxonomy outlines the visual components within the frame, including subjects, props, and environmental context.",
                "position": 144
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x3.png",
                "caption": "(b)TheCamerataxonomy defines all controls related to camera configuration during a shot setup.",
                "position": 149
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x4.png",
                "caption": "",
                "position": 154
            }
        ]
    },
    {
        "header": "3Stable Cinemetrics",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26555/x5.png",
                "caption": "Figure 3:TheEventstaxonomy captures the narrative dimension of a shot which includes actions, emotions, and their fine grained portrayal as they evolve over time within a shot.",
                "position": 166
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x6.png",
                "caption": "Figure 4:t-SNE visualization showing substantial overlap between ground truth screenplays and prompts inSCINE Scripts, in comparison to existing prompt based benchmarks such as VBench-2.0",
                "position": 344
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x7.png",
                "caption": "Figure 5:1. Prompt Generation PipelineSCINE Scripts are created by passing seed prompts and sampledEventstaxonomy nodes to an LLM, forming the narrative component of our benchmark. SCINE Visuals are then generated through structured upsampling, where nodes from theCamera,Lighting, andSetuptaxonomies are sampled and injected into each SCINE Script to create prompts that capture visual exposition. 2.Automatic Categorization and Question GenerationGiven a SCINE prompt and taxonomy, wecategorizeeach taxonomy element present in the prompt and generate a correspondingquestionto enable isolated evaluation of each control node.",
                "position": 385
            }
        ]
    },
    {
        "header": "4Are current Video Generative Models Ready for Professional Use?",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26555/x8.png",
                "caption": "(a)Overall results onSCINE Scripts, across genres. Minimax and Wan 14B emerge as the strongest models. The strongest genre is Biography while all models consistently struggle at Comedy.",
                "position": 399
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x8.png",
                "caption": "(a)Overall results onSCINE Scripts, across genres. Minimax and Wan 14B emerge as the strongest models. The strongest genre is Biography while all models consistently struggle at Comedy.",
                "position": 402
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x9.png",
                "caption": "(b)Overall results onSCINE Visualsacross four pillars of professional control.Eventsemerges as the most challenging category across models, whileSetupyields the strongest performance.",
                "position": 407
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x10.png",
                "caption": "Figure 7:Fine-grained evaluation onEvents. Models handle environmental changes well but struggle with dialogues and shot pacing. Standalone actions outperform interactive, and implicit emotions are easier than explicit.",
                "position": 515
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x11.png",
                "caption": "Figure 8:Model performance on Events across temporal portryal of Actions. Atomic actions are handled well, whereas models struggle with causal and overlapping Events.",
                "position": 521
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x12.png",
                "caption": "Figure 9:Split results on basic vs. advanced prompts forCameraandLighting. All models show performance drops on advanced prompts, with the largest decline in Lighting Source.",
                "position": 531
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x13.png",
                "caption": "Figure 10:Model performances across Lighting Source. Strobes and Sunlight emerge strongest, whereas HMI and Fluorescent are points of weaknesses.",
                "position": 534
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x14.png",
                "caption": "Figure 11:Model performances across Camera Angles. The Dutch angle poses a common challenge to all current video generative models",
                "position": 537
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x15.png",
                "caption": "Figure 12:Model performances across Camera Shot Size. Models perform well on Master and Establishing shots and struggle at medium-wide and extreme-close-up shots.",
                "position": 540
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x16.png",
                "caption": "(a)Fine-grained results onSubjects. Models perform well on hair and accessories, but struggle with personality and makeup.",
                "position": 549
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x16.png",
                "caption": "(a)Fine-grained results onSubjects. Models perform well on hair and accessories, but struggle with personality and makeup.",
                "position": 552
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x17.png",
                "caption": "(b)Fine-grained results onSet Design. Models perform better at Backdrop in comparison to Environment, and struggle most with styling the shot appropriately.",
                "position": 557
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x18.png",
                "caption": "Figure 14:Directorresults: Joint specification of all controls, mirroring real-world shot creation leads to a performance drop on all models, compared to evaluation in a standalone manner.",
                "position": 564
            }
        ]
    },
    {
        "header": "5Scalable Evaluation of Professional Videos",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26555/x19.png",
                "caption": "Figure 15:Our trained VLM shows consistent alignment with human annotations across video generation models, outperforming baselines, most notably on WAN-14B.",
                "position": 587
            }
        ]
    },
    {
        "header": "6Conclusion and Future Work",
        "images": []
    },
    {
        "header": "7Acknowledgement",
        "images": []
    },
    {
        "header": "8Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26555/x20.png",
                "caption": "Figure 16:Model performance on Events across genres. Across 13 models and 12 genres, portrayal of Events in Biography and Adventure are the strongest, while Comedy and Horror are the weakest.",
                "position": 2816
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x21.png",
                "caption": "Figure 17:Model performance on Emotions. Among 10 models and 19 emotions, Remorse is best portrayed, while Aggressiveness is the weakest.",
                "position": 2819
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x22.png",
                "caption": "Figure 18:Model performance on Dialogues. Compared to Actions and Emotions, models struggle at Dialogues. Within Dialogues, performance drop is seen during multi-turn conversations.",
                "position": 2825
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x23.png",
                "caption": "Figure 19:Between different frame compositions, models are better at Rule of Thirds but struggle at maintaining Symmetry.",
                "position": 2835
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x24.png",
                "caption": "Figure 20:Across Time of Day setups, Sunrise shots are handled better, while Afternoon remains more challenging for models.",
                "position": 2838
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x25.png",
                "caption": "(a)Level 1 Activations",
                "position": 3157
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x25.png",
                "caption": "(a)Level 1 Activations",
                "position": 3160
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x26.png",
                "caption": "(b)Level 2 Activations",
                "position": 3165
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x27.png",
                "caption": "(c)Level 3 Activations",
                "position": 3175
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x28.png",
                "caption": "(d)Level 4 Activations",
                "position": 3181
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x29.png",
                "caption": "(a)Level 2 Activations",
                "position": 3188
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x29.png",
                "caption": "(a)Level 2 Activations",
                "position": 3191
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x30.png",
                "caption": "(b)Level 3 Activations",
                "position": 3196
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x31.png",
                "caption": "(c)Level 4 Activations",
                "position": 3206
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x32.png",
                "caption": "(d)Level 5 Activations",
                "position": 3212
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x33.png",
                "caption": "(a)Level 1 Activations",
                "position": 3219
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x33.png",
                "caption": "(a)Level 1 Activations",
                "position": 3222
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x34.png",
                "caption": "(b)Level 2 Activations",
                "position": 3227
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x35.png",
                "caption": "(c)Level 3 Activations",
                "position": 3237
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x36.png",
                "caption": "(d)Level 4 Activations",
                "position": 3243
            },
            {
                "img": "https://arxiv.org/html/2509.26555/figures/appendix/annotator_UI.png",
                "caption": "Figure 24:User Interface used by annotators to perform evaluations.",
                "position": 3257
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x37.png",
                "caption": "Figure 25:Distribution of the years of film production experience amongst human annotators in our evaluation setup.",
                "position": 3260
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x38.png",
                "caption": "(a)Pairwise t-tests across models on emotions",
                "position": 3451
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x38.png",
                "caption": "(a)Pairwise t-tests across models on emotions",
                "position": 3454
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x39.png",
                "caption": "(b)Pairwise t-tests across models on dialogues",
                "position": 3459
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x40.png",
                "caption": "(c)Pairwise t-tests across models on actions",
                "position": 3465
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x41.png",
                "caption": "(a)Pairwise t-tests across models on Color Temperature",
                "position": 3472
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x41.png",
                "caption": "(a)Pairwise t-tests across models on Color Temperature",
                "position": 3475
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x42.png",
                "caption": "(b)Pairwise t-tests across models on Lighting Effects",
                "position": 3480
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x43.png",
                "caption": "(c)Pairwise t-tests across models on Extrinsics",
                "position": 3486
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x44.png",
                "caption": "(d)Pairwise t-tests across models on Intrinsics",
                "position": 3491
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x45.png",
                "caption": "(e)Pairwise t-tests across models on Trajectory",
                "position": 3497
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x46.png",
                "caption": "(a)Pairwise t-tests across models on Subject Class",
                "position": 3504
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x46.png",
                "caption": "(a)Pairwise t-tests across models on Subject Class",
                "position": 3507
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x47.png",
                "caption": "(b)Pairwise t-tests across models on Subjet Costume",
                "position": 3512
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x48.png",
                "caption": "(c)Pairwise t-tests across models on Elements",
                "position": 3518
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x49.png",
                "caption": "(d)Pairwise t-tests across models on Time of Day",
                "position": 3523
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x50.png",
                "caption": "(e)Pairwise t-tests across models on Location",
                "position": 3529
            },
            {
                "img": "https://arxiv.org/html/2509.26555/x51.png",
                "caption": "Figure 29:Preference Accuracy of open and closed-sourced VLMs in rating videos generated for Professional Use",
                "position": 3736
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]