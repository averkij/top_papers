[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.00174/x1.png",
                "caption": "",
                "position": 113
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.00174/x2.png",
                "caption": "Figure 2:Training pipeline of SOLAMI. We train SOLAMI through a three-stage process. In the pre-training stage, we train the model with motion-text and speech-text related tasks to align the speech and motion modalities with language. During the instruction tuning stage, we train the model with social multimodal multi-round interaction data, enabling it to generate multimodal responses that align with the character settings and the context of the topic.",
                "position": 186
            }
        ]
    },
    {
        "header": "3Social Vision-Language-Action Modeling",
        "images": []
    },
    {
        "header": "4SynMSI Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.00174/x3.png",
                "caption": "Figure 3:SynMSI dataset generation.\nOur synthesizing pipeline consists of 4 steps.\nBased on numerous character-relevant topics and state-of-the-art LLMs[52], we generate text scripts for multimodal dialogues. Using a large-scale motion database[76,29,17], we retrieve the most appropriate motions and refine the speech scripts accordingly. Finally, we employ TTS/voice cloning[19]to generate character-specific speech. This approach enables us to create multimodal interaction data of various characters using only existing motion datasets.",
                "position": 334
            }
        ]
    },
    {
        "header": "5VR Interface",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.00174/x4.png",
                "caption": "Figure 4:VR interface architecture. Our VR project consists of a Quest 3 client and a server. The Quest client captures and transmits user body motion and speech to the server. The server then generates character’s speech, body motion, and face blendshape parameters based on the selected methods. The response is then sent back to the Quest client to drive the character.",
                "position": 402
            }
        ]
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.00174/x5.png",
                "caption": "Figure 5:Results of the user study with 95% confidence.",
                "position": 650
            },
            {
                "img": "https://arxiv.org/html/2412.00174/x6.png",
                "caption": "Figure 6:Qualitative results of SOLAMI and baselines, and the user workflow for VR experience.\nOur social VLA model, trained in an end-to-end strategy on SynMSI dataset, can accurately perceive the semantic information embedded within users’ speech and motion input, and subsequently generate natural and coherent responses.",
                "position": 655
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AFuture Work",
        "images": []
    },
    {
        "header": "Appendix BMore Details of Architecture Design",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.00174/extracted/6031754/figures/images/collection_methods/cc_input.png",
                "caption": "Table 5:Methods of collecting multimodal interaction data.",
                "position": 2126
            },
            {
                "img": "https://arxiv.org/html/2412.00174/extracted/6031754/figures/images/collection_methods/cc_output.png",
                "caption": "",
                "position": 2155
            },
            {
                "img": "https://arxiv.org/html/2412.00174/extracted/6031754/figures/images/collection_methods/mj_input.png",
                "caption": "",
                "position": 2162
            },
            {
                "img": "https://arxiv.org/html/2412.00174/extracted/6031754/figures/images/collection_methods/mj_output.png",
                "caption": "",
                "position": 2167
            },
            {
                "img": "https://arxiv.org/html/2412.00174/extracted/6031754/figures/images/collection_methods/xcjy_input.png",
                "caption": "",
                "position": 2182
            },
            {
                "img": "https://arxiv.org/html/2412.00174/extracted/6031754/figures/images/collection_methods/panda_input.png",
                "caption": "",
                "position": 2195
            },
            {
                "img": "https://arxiv.org/html/2412.00174/extracted/6031754/figures/images/collection_methods/vr_input.png",
                "caption": "",
                "position": 2220
            },
            {
                "img": "https://arxiv.org/html/2412.00174/extracted/6031754/figures/images/collection_methods/vr_output.png",
                "caption": "",
                "position": 2225
            },
            {
                "img": "https://arxiv.org/html/2412.00174/extracted/6031754/figures/images/collection_methods/synmsi_input.png",
                "caption": "",
                "position": 2245
            },
            {
                "img": "https://arxiv.org/html/2412.00174/extracted/6031754/figures/images/collection_methods/synmsi_output.png",
                "caption": "",
                "position": 2250
            }
        ]
    },
    {
        "header": "Appendix CMore Details of Data Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.00174/extracted/6031754/figures/images/wordclouds/keywords_plot_assistant.png",
                "caption": "(a)Samantha",
                "position": 2310
            },
            {
                "img": "https://arxiv.org/html/2412.00174/extracted/6031754/figures/images/wordclouds/keywords_plot_assistant.png",
                "caption": "(a)Samantha",
                "position": 2313
            },
            {
                "img": "https://arxiv.org/html/2412.00174/extracted/6031754/figures/images/wordclouds/keywords_plot_11-45-G.png",
                "caption": "(b)K-VRC",
                "position": 2319
            },
            {
                "img": "https://arxiv.org/html/2412.00174/extracted/6031754/figures/images/wordclouds/keywords_plot_Batman.png",
                "caption": "(c)Batman",
                "position": 2325
            },
            {
                "img": "https://arxiv.org/html/2412.00174/extracted/6031754/figures/images/wordclouds/keywords_plot_Bananya.png",
                "caption": "(d)Banaya",
                "position": 2331
            }
        ]
    },
    {
        "header": "Appendix DMore Details of Experiments",
        "images": []
    },
    {
        "header": "Appendix EAcknowledgments",
        "images": []
    }
]