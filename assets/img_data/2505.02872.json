[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Eye Tracking Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02872/x1.png",
                "caption": "Figure 1:Example of a textual item from OneStopQA: a paragraph with three questions. Two of the questions have the samecritical span: the paragraph segment essential for answering the question.",
                "position": 227
            }
        ]
    },
    {
        "header": "3Problem Formulation",
        "images": []
    },
    {
        "header": "4Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02872/x2.png",
                "caption": "Figure 2:An overview of the two types of models presented in the paper. On the left, discriminative models (Haller RNN and RoBERTEye-Fixations), that score each candidate question and are evaluated on classification accuracy. On the right, generative models (DalEye-LLaVA and DalEye-Llama), that are trained to reconstruct the question presented to the reader, and are evaluated both on classification accuracy and reconstruction quality. Further details and diagrams of the models are presented inAppendixE.",
                "position": 273
            }
        ]
    },
    {
        "header": "5Experimental Setup and Evaluation",
        "images": []
    },
    {
        "header": "6Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02872/x3.png",
                "caption": "Figure 3:Question reconstructionevaluations of the DalEye-Llama model for (1) the identity of the generated question word, (2) BLEU score (3) the UIUC semantic category of the question, (4) BERTScore, and (5) downstream QA accuracy based on the answer selection of a multiple-choice question answering model. DalEye-Llama performance is benchmarked against two types of human-composed questions (for a different critical span and for the same span) and against arbitrary questions generated with GPT-4o and Llama 3.1. Presented are means with 95% confidence intervals (bootstrapped,n=1000ùëõ1000n=1000italic_n = 1000) for three generalization levels to new participants and new readers.",
                "position": 546
            }
        ]
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Summary and Discussion",
        "images": []
    },
    {
        "header": "9Limitations",
        "images": []
    },
    {
        "header": "10Ethical Considerations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AModel Training and Hyperparameters",
        "images": []
    },
    {
        "header": "Appendix BTask Breakdown",
        "images": []
    },
    {
        "header": "Appendix CAdditional Results",
        "images": []
    },
    {
        "header": "Appendix DQuestion Annotation into UIUC Categories",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02872/x4.png",
                "caption": "Figure 5:Question Similarity to RT-Weighted Passage",
                "position": 2685
            },
            {
                "img": "https://arxiv.org/html/2505.02872/x5.png",
                "caption": "Figure 6:RT Similarity to Question-Word Similarities",
                "position": 2695
            },
            {
                "img": "https://arxiv.org/html/2505.02872/x6.png",
                "caption": "Figure 7:Haller RNN",
                "position": 2742
            },
            {
                "img": "https://arxiv.org/html/2505.02872/x7.png",
                "caption": "Figure 8:RoBERTEye-Fixations",
                "position": 2787
            },
            {
                "img": "https://arxiv.org/html/2505.02872/x8.png",
                "caption": "Figure 9:DalEye-LLaVA",
                "position": 2794
            }
        ]
    },
    {
        "header": "Appendix EModels",
        "images": []
    }
]