[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21724/x1.png",
                "caption": "Figure 1:Illustration of the new OMCRG task.(a) In offline tasks, the generation model generates the listener’s full response only after receiving the entire input sequence from the speaker.\n(b) Differently, OMCRG task requires sequentially processing the speaker’s incoming input and generating multi-modal responses for the listener on the fly.",
                "position": 126
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21724/x2.png",
                "caption": "Figure 2:Overview of the proposed OmniResponse. The model takes textual conversational history and newly coming multimodal information (e.g., facial cues) from the speaker and listener as input, and generates temporally synchronized facial and textual responses for the listener by leveraging a pre-trained LLM enhanced with our proposed Chrono-Text Markup. The generated text embeddings are converted into audio synchronized with the facial response by the proposed TempoVoice module.",
                "position": 188
            },
            {
                "img": "https://arxiv.org/html/2505.21724/x3.png",
                "caption": "Figure 3:Architecture of TempoVoice.TempoVoice transforms textual hidden-state embeddings into audio segments.",
                "position": 243
            }
        ]
    },
    {
        "header": "4Dataset Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21724/x4.png",
                "caption": "Figure 4:Statistics of ResponseNet. (a) Distribution of video clip durations. (b) Distribution of dyadic conversation topics. (c) Word cloud of spoken words in dyadic conversations.",
                "position": 457
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21724/x5.png",
                "caption": "Figure 5:Qualitative Results.Given the speaker’s audio and video streams and corresponding utterances (left), OmniResponse autoregressively generates synchronized visual, audio, and textual response streams (right). For clarity,[LASTING]tokens are removed from the generated dialogue.",
                "position": 643
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details and Hyperparameters",
        "images": []
    },
    {
        "header": "Appendix BMethodological Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21724/x6.png",
                "caption": "Figure 6:Illustration of Prompt Construction.The final prompt (messgae) is composed of a system prompt, the conversation history from the speaker (user) and listener (assistant), and dynamic speaker/listener text processed by our Chrono-Text Markup module.",
                "position": 1973
            },
            {
                "img": "https://arxiv.org/html/2505.21724/x7.png",
                "caption": "Figure 7:Example of Dynamic Text.",
                "position": 2018
            },
            {
                "img": "https://arxiv.org/html/2505.21724/x8.png",
                "caption": "Figure 8:Illustration of Multimodal Context Modeling.Each visual token attends to all preceding visual tokens and static and dynamic text tokens annotated by Chrono-Text markers at earlier timestamps. Similarly, each dynamic text token attends to all past visual and textual tokens, enabling rich cross-modal context integration.",
                "position": 2035
            }
        ]
    },
    {
        "header": "Appendix CResponseNet Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21724/x9.png",
                "caption": "Figure 9:Illustration of Dataset Construction Pipeline.",
                "position": 2138
            }
        ]
    },
    {
        "header": "Appendix DEvaluation Protocol",
        "images": []
    },
    {
        "header": "Appendix ERisks and Potential Misuse",
        "images": []
    },
    {
        "header": "Appendix FLimitations",
        "images": []
    },
    {
        "header": "Appendix GBroader Impacts",
        "images": []
    },
    {
        "header": "Appendix HResponsibility and License",
        "images": []
    }
]