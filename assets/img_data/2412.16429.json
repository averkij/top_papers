[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.16429/x1.png",
                "caption": "Figure 1:An overview of our three-stage human evaluation pipeline and our results for comparing LearnLM with other systems. (1) Learning scenarios are developed that allow raters to role-play specific learners interacting with pairs of AI tutors (2). Grounding material (e.g. an essay, homework problem, diagram, etc.) and System Instructions specific to each scenario are passed as context to each model. The resulting conversation pairs are reviewed by pedagogy experts (3) who answer a range of questions assessing each model on its own as well as their comparative performance. These comparative ratings (on a seven-point -3 to +3 Likert scale) are aggregated (4) to show overall preference for LearnLM over GPT-4o, Claude 3.5, and Gemini 1.5 Pro. See Section4for more detailed results.",
                "position": 177
            }
        ]
    },
    {
        "header": "2Modeling",
        "images": []
    },
    {
        "header": "3Human Evaluation Design",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.16429/x2.png",
                "caption": "Figure 2:Workflow to generate conversations based on educational scenarios. A participant enacts conversations with prompted models as defined by scenarios. The participant then fills out a survey capturing quality and preference between models.",
                "position": 285
            }
        ]
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.16429/extracted/6084483/figures/length_histogram.png",
                "caption": "Figure 3:(Top) The specific LLMs compared, along with aggregate statistics across all conversations collected: average number of model turns per conversation and average number of words per turn; (Bottom) Histograms of the number of words used per turn by each model.",
                "position": 353
            },
            {
                "img": "https://arxiv.org/html/2412.16429/x3.png",
                "caption": "Figure 4:Pedagogy experts’ preferences over LearnLM and other contemporaneous systems (Claude 3.5, GPT-4o, and Gemini 1.5 Pro). The scatterplots represent the underlying distribution of seven-point preference ratings. Given the large number of ratings we collected, these scatterplots proportionally downsample to 500 ratings per measure, color-coded based on the preference scale (dark purple corresponds to strong preference for LearnLM), and randomly positioned around each integer rating for readability. The red points and error bars indicate the estimated mean and its 95% credible interval for each measure. These means are also shown in Figure1.",
                "position": 398
            },
            {
                "img": "https://arxiv.org/html/2412.16429/x4.png",
                "caption": "Figure 5:Evaluation of systems on each category of our pedagogy rubric from a 7-point Likert scale (\"Strongly disagree\" to \"Strongly agree\"). Error bars reflect 95% credible intervals from the posterior distrubtion for the mean.",
                "position": 407
            },
            {
                "img": "https://arxiv.org/html/2412.16429/x5.png",
                "caption": "Figure 6:Impressions shared by the pedagogy experts role-playing as learners in our pedagogical scenarios. Error bars reflect 95% credible intervals from the posterior distribution for the mean.\nThe rating scales for impression questions (left) were 5-point extent scales (“Not at all” to “Extremely”), and 7-point Likert scales (“Strongly disagree” to “Strongly agree”) for experience questions (right).",
                "position": 410
            },
            {
                "img": "https://arxiv.org/html/2412.16429/x6.png",
                "caption": "",
                "position": 414
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Contributions and Acknowledgments",
        "images": []
    },
    {
        "header": "Appendix AAdditional results",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.16429/x7.png",
                "caption": "Figure 7:Preferences over LearnLM and other contemporary models (Claude-3.5, GPT-4o, and Gemini 1.5 Pro) according to the pedagogical experts role-playing as learners. The scatterplots represent the underlying distribution of seven-point preference ratings. Given the large number of ratings we collected, these scatterplots proportionally downsample to 500 ratings per measure. The red points and error bars indicate the estimated mean and its 95% credible interval for each measure.",
                "position": 1117
            },
            {
                "img": "https://arxiv.org/html/2412.16429/x8.png",
                "caption": "Figure 8:At the beginning of the pedagogical assessment process, we asked experts to evaluate how closely the human participants in the conversation transcripts followed the scenario instructions (i.e., how effectively they role-played the learner in the scenario) on a seven-point scale. This plot shows the responses grouped and averaged by transcript. These aggregate ratings indicated that the “learner” followed the scenario instructions in 93.4% of conversation transcripts.",
                "position": 1124
            },
            {
                "img": "https://arxiv.org/html/2412.16429/x9.png",
                "caption": "Figure 9:Evaluation of tutor models on specific subdimensions of the “Cognitive load” rubric category. Error bars reflect 95% credible intervals from the posterior distribution for the mean.",
                "position": 1131
            },
            {
                "img": "https://arxiv.org/html/2412.16429/x10.png",
                "caption": "",
                "position": 1135
            },
            {
                "img": "https://arxiv.org/html/2412.16429/x11.png",
                "caption": "Figure 10:Evaluation of tutor models on specific subdimensions of the “Active learning” rubric category. Error bars reflect 95% credible intervals from the posterior distribution for the mean.",
                "position": 1139
            },
            {
                "img": "https://arxiv.org/html/2412.16429/x12.png",
                "caption": "Figure 11:Evaluation of tutor models on specific subdimensions of the “Deepen metacognition” rubric category. Error bars reflect 95% credible intervals from the posterior distribution for the mean.",
                "position": 1142
            },
            {
                "img": "https://arxiv.org/html/2412.16429/x13.png",
                "caption": "Figure 12:Evaluation of tutor models on specific subdimensions of the “Stimulates curiosity” rubric category. Error bars reflect 95% credible intervals from the posterior distribution for the mean.",
                "position": 1145
            },
            {
                "img": "https://arxiv.org/html/2412.16429/x14.png",
                "caption": "Figure 13:Evaluation of tutor models on specific subdimensions of the “Adaptivity” rubric category. Error bars reflect 95% credible intervals from the posterior distribution for the mean.",
                "position": 1148
            }
        ]
    },
    {
        "header": "Appendix BMethods",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.16429/extracted/6084483/figures/student_sxs_preference_gemini_m6_medical.png",
                "caption": "Figure 14:Preferences for LearnLM over Gemini 1.5 Pro according to 18 medical students on a set of 290 conversations across 50 scenarios for medical education subjects. These comparative ratings (on a seven-point -3 to +3 Likert scale) are aggregated to show overall preference for LearnLM over Gemini 1.5 Pro. The bar length and error bars indicate the estimated mean and its 95% credible interval for each measure.",
                "position": 3003
            }
        ]
    },
    {
        "header": "Appendix CFeasibility Study on Medical Education Subjects",
        "images": []
    }
]