[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22946/figures/light-bagel-logo.png",
                "caption": "",
                "position": 43
            },
            {
                "img": "https://arxiv.org/html/2510.22946/figures/logo_browser.png",
                "caption": "",
                "position": 53
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22946/x1.png",
                "caption": "Figure 1:Token efficiency comparison on T2I and image editing benchmarks. OurLightBageloutperforms many leading unified models that uses significantly more tokens for training, showing great token efficiency. Note that we use our best estimate for the number of seen tokens of OmniGen2 and UniPic since their original training recipe is unclear to the public.",
                "position": 78
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22946/x2.png",
                "caption": "Figure 2:Overview of theLightBagelarchitecture. Text and ViT tokens (understanding pathway) and VAE tokens (generation pathway) are processed by pre-trained VLM and DiT blocks, respectively. At each layer, a zero-initialized multimodal self-attention module enables cross-modal interactions without altering the original model architectures.",
                "position": 146
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22946/x3.png",
                "caption": "Figure 3:Qualitative text-to-image results fromLightBagel, showcasing high-quality generations with strong fidelity to text prompts and consistent rendering across diverse aspect ratios.",
                "position": 597
            },
            {
                "img": "https://arxiv.org/html/2510.22946/x4.png",
                "caption": "Figure 4:Qualitative image editing results generated byLightBagel. The model exhibits strong instruction following and content preservation capability across a diverse range of editing tasks.",
                "position": 1070
            },
            {
                "img": "https://arxiv.org/html/2510.22946/x5.png",
                "caption": "Figure 5:Deep fusionvs. shallow fusion design choices. Regions of different colors represent different training stages. The “0% Depth” deep fusion approach in ourLightBagelconsistently outperforms other options.",
                "position": 1394
            },
            {
                "img": "https://arxiv.org/html/2510.22946/x5.png",
                "caption": "",
                "position": 1397
            },
            {
                "img": "https://arxiv.org/html/2510.22946/x6.png",
                "caption": "",
                "position": 1402
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]