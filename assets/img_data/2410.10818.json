[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10818/x1.png",
                "caption": "Figure 1:The tasks ofTemporalBench.TemporalBenchstarts from fine-grained video descriptions and supports diverse video understanding tasks including video QA, video captioning, long video understanding,etc. It differs from existing benchmarks by the average number of words per video (middle top), word density (center) and the coverage of various temporal aspects (middle bottom).",
                "position": 100
            },
            {
                "img": "https://arxiv.org/html/2410.10818/x2.png",
                "caption": "Figure 2:Overview of the annotation pipeline forTemporalBench. In step 1, we fist collect high-quality captions for the videos using qualified AMT annotators followed by refining them. In step 2, we leverage existing LLMs to generate negative captions by replacing select words and reordering the sequence of actions before filtering them ourselves.",
                "position": 144
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3TemporalBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10818/x3.png",
                "caption": "Figure 3:Comparison of negative captions generated from the original captions and our detailed captions inTemporalBench. With fine-grained details, the negatives are more difficult and temporal centric.",
                "position": 167
            },
            {
                "img": "https://arxiv.org/html/2410.10818/x4.png",
                "caption": "Figure 4:Video length distribution of (a) short video clips and (b) long videos inTemporalBench.",
                "position": 235
            },
            {
                "img": "https://arxiv.org/html/2410.10818/x5.png",
                "caption": "Figure 5:An illustration of multi-choice QA with (a) original and (b) heuristics-guided negative captions. Orange blocks indicate the altered contents from the positive option (green box).",
                "position": 270
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10818/x6.png",
                "caption": "Figure 6:Model performance onTemporalBenchwith varying frames.",
                "position": 308
            },
            {
                "img": "https://arxiv.org/html/2410.10818/x7.png",
                "caption": "Figure 7:Visualization of binary accuracy for short video QA per (a) subset and (b) negative type. Human performance is much better than GPT-4o, Qwen2-VL-72B, LLaVA-OneVision-72B, and Gemini-1.5-Pro.",
                "position": 1070
            }
        ]
    },
    {
        "header": "5In-Depth Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10818/x8.png",
                "caption": "Figure 8:The failure cases of GPT-4o inTemporalBench. GPT-4o does not understand the fine-grained details well, including motion direction, action frequency, action type, and motion direction.",
                "position": 1491
            }
        ]
    },
    {
        "header": "6Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ABroader Impact",
        "images": []
    },
    {
        "header": "Appendix BMore Visualizations of Our Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10818/x9.png",
                "caption": "Figure 9:Visualizations (I) of our fine-grained annotations of the videos with both positive and negative descriptions.",
                "position": 3190
            },
            {
                "img": "https://arxiv.org/html/2410.10818/x10.png",
                "caption": "Figure 10:Visualizations (II) of our fine-grained annotations of the videos with both positive and negative descriptions.",
                "position": 3193
            }
        ]
    },
    {
        "header": "Appendix CPer subset Results for Short and Long Video QA under Binary Accuracy (BA)",
        "images": []
    },
    {
        "header": "Appendix DMore Results with Extended Frames",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10818/extracted/5929459/figures/positive-cap-annotation.png",
                "caption": "Figure 11:Positive caption refinement platform.",
                "position": 4059
            },
            {
                "img": "https://arxiv.org/html/2410.10818/extracted/5929459/figures/negative-cap-annotation.png",
                "caption": "Figure 12:Negative caption annotation platform.",
                "position": 4062
            }
        ]
    },
    {
        "header": "Appendix EData Annotation Platform",
        "images": []
    }
]