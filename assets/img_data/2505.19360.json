[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19360/extracted/6477459/figures/university-of-maryland-logo-1.png",
                "caption": "",
                "position": 81
            },
            {
                "img": "https://arxiv.org/html/2505.19360/extracted/6477459/figures/722666.png",
                "caption": "",
                "position": 82
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19360/x1.png",
                "caption": "Figure 1:We introduce the task of visual attribution for charts (➊), which grounds textual responses to specific regions in the chart image. This promotes reliable understanding by enabling users to verify claims (➋), thus detect potentially hallucinated responses and identifying chart-response misalignments.",
                "position": 128
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Post-Hoc Visual Attribution in Charts",
        "images": []
    },
    {
        "header": "4ChartVA-Eval",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19360/x2.png",
                "caption": "Figure 2:ChartLens:➊ Chart elements, such as bars and pie sectors, are extracted through heuristic-guided methods and refined using SAM, while lines are segmented using Lineformer. ➋ The segmented elements are then marked, labeled, and used to prompt multimodal LLMs, enabling fine-grained attribution by grounding textual responses to visual regions.",
                "position": 360
            }
        ]
    },
    {
        "header": "5ChartLens",
        "images": []
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19360/x3.png",
                "caption": "Figure 3:Qualitative comparison of our ChartLens with the baselines. ChartLens is able to effectively localize relevant, complete and precise attributions in the chart images.",
                "position": 651
            }
        ]
    },
    {
        "header": "7Results",
        "images": []
    },
    {
        "header": "8Conclusion and Future Work",
        "images": []
    },
    {
        "header": "9Ethics Statement",
        "images": []
    },
    {
        "header": "10Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails on Benchmark Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19360/extracted/6477459/figures/SCR-20241216-fxid.png",
                "caption": "Figure 4:Overview of annotation guidelines provided to annotators for ensuring accurate and consistent visual attributions.",
                "position": 1478
            },
            {
                "img": "https://arxiv.org/html/2505.19360/x4.png",
                "caption": "Figure 5:The design decision option space for MATSA synthetic charts, illustrating the various configurable elements and parameters available for customizing chart generation. This visual representation highlights the flexibility in chart design, encompassing aspects such as chart type, data presentation styles, and visual encoding options.",
                "position": 1488
            }
        ]
    },
    {
        "header": "Appendix BAlgorithmic Heuristics for Point Extraction",
        "images": []
    }
]