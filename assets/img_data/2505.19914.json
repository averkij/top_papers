[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19914/x1.png",
                "caption": "Figure 1:Overview of theEnigmatadataset: 36 puzzle tasks across seven categories, designed to enhance and evaluate diverse reasoning capabilities in large language models.",
                "position": 136
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Enigmata-Data: The Puzzle Dataset",
        "images": []
    },
    {
        "header": "4Enigmata-Model: The Training Recipe",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19914/x2.png",
                "caption": "Figure 2:Impact of training data size in the second stage of Multi-stage Training on model performance across different benchmarks. The blue dashed line represents model performance after the first training stage, while the red solid line shows performance after the second stage.",
                "position": 950
            },
            {
                "img": "https://arxiv.org/html/2505.19914/x3.png",
                "caption": "Figure 3:Learning curves across training approaches for representative puzzle tasks. Each row represents a different task, and each column represents a different training approach. The curves show how the average reward changes with training steps for different difficulty levels.",
                "position": 1091
            },
            {
                "img": "https://arxiv.org/html/2505.19914/x4.png",
                "caption": "Figure 4:The response length and reward curves during Mix-Training RL and Multi-Stage RL training.",
                "position": 1133
            },
            {
                "img": "https://arxiv.org/html/2505.19914/x4.png",
                "caption": "Figure 4:The response length and reward curves during Mix-Training RL and Multi-Stage RL training.",
                "position": 1136
            },
            {
                "img": "https://arxiv.org/html/2505.19914/x5.png",
                "caption": "Figure 5:Performance analysis of Qwen2.5-32B-Enigmataon ARC-AGI 1:\n(a) distribution of reasoning token lengths for correct and incorrect responses,\n(b) and success rate by reasoning token length.",
                "position": 1141
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Contributions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "8Reward Curves Across Individual Tasks inEnigmata",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19914/x6.png",
                "caption": "Figure 6:Reward curves for Qwen2.5-32B-Enigmataacross all individual tasks during training. Each subplot represents a different puzzle task, with the x-axis showing training steps and the y-axis showing average reward. Colors indicate different difficulty levels: Easy (green), Medium (blue), and Hard (red).",
                "position": 1810
            }
        ]
    },
    {
        "header": "9Training Dataset Details",
        "images": []
    },
    {
        "header": "10Implementation Details",
        "images": []
    },
    {
        "header": "11Baselines",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "12EnigmataDetails",
        "images": []
    }
]