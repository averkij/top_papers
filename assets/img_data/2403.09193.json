[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2403.09193/x1.png",
                "caption": "Figure 1:Unlike many unimodal models, vision language models (VLMs) prefer shape over texture for object recognition, but not to the same extent as humans. Further, we find that the (visual) texture/shape bias[1]can be steered through language alone, albeit not to the extent as through vision.Here we visualize the texture/shape bias of some exemplary VLMs, and highlight the steerability of InternVL-Chat 1.1[2].",
                "position": 158
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Are VLMs Biased towards Texture or Shape?",
        "images": [
            {
                "img": "https://arxiv.org/html/2403.09193/x2.png",
                "caption": "Figure 2:Most VLMs are slightly shape-biased but some models show differences when asked to describe an image compared to VQA.We measure the shape bias on thecue-conflictdataset[1]. For reference, we also provide measurements on ResNet-50[59]from the initial shape bias study[1], zero-shot classification (CLIP ViT-L/14[11]), and a human average (over 10 subjects[1]).",
                "position": 309
            },
            {
                "img": "https://arxiv.org/html/2403.09193/x3.png",
                "caption": "(a)",
                "position": 348
            },
            {
                "img": "https://arxiv.org/html/2403.09193/x3.png",
                "caption": "(a)",
                "position": 351
            },
            {
                "img": "https://arxiv.org/html/2403.09193/x4.png",
                "caption": "(b)",
                "position": 356
            },
            {
                "img": "https://arxiv.org/html/2403.09193/x5.png",
                "caption": "(c)",
                "position": 361
            },
            {
                "img": "https://arxiv.org/html/2403.09193/x6.png",
                "caption": "Figure 4:VLMs make similar errors on the cue-conflict datasets and share similarities with their vision encoders. In terms of errors, VLMs are also more similar to humans than ImageNet-trained/finetuned models.We measure the pair-wise error consistency[71]between predictions. For this analysis, an error is any answer that does not belong to the shape class (analogous to[29]).\nShown responses belong to LLM-based VLMs (under the VQA task), other selected models including ImageNet models, (some) VLM encoders under ImageNet-finetuning and zero-shot classification, and ten human subjects.",
                "position": 409
            }
        ]
    },
    {
        "header": "5Steering Texture/Shape Bias",
        "images": [
            {
                "img": "https://arxiv.org/html/2403.09193/x7.png",
                "caption": "Figure 5:Image preprocessing can strongly steer texture/shape bias.Left: Shuffling image patches with decreasing patch size results in a strong texture bias, Right: Increasing Gaussian noise introduces a strong shape bias.",
                "position": 426
            },
            {
                "img": "https://arxiv.org/html/2403.09193/x8.png",
                "caption": "Figure 6:Prompts can steer the texture/shape bias to some extent.We test the same texture/shape-biased instructions on multiple models, showing that these can already shift some decisions (usually in favor of texture). ForInternVL 1.1andLLaVA-NeXT 7Bwe additionally test the understanding of texture/shape by using synonyms. Furthermore, we use an LLM to automatically search for specific prompts to optimize in either direction.",
                "position": 460
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AOverview of VLMs",
        "images": []
    },
    {
        "header": "Appendix BDetailed Results Table",
        "images": []
    },
    {
        "header": "Appendix CPrompting",
        "images": [
            {
                "img": "https://arxiv.org/html/2403.09193/x9.png",
                "caption": "Figure 7:Detailed shape bias measurements under synonyms for biased VQA prompts.",
                "position": 1777
            }
        ]
    },
    {
        "header": "Appendix DLLM-based Response Extraction",
        "images": []
    },
    {
        "header": "Appendix EAutomated Prompt Search",
        "images": []
    },
    {
        "header": "Appendix FAblation of Temperature Scaling",
        "images": [
            {
                "img": "https://arxiv.org/html/2403.09193/x10.png",
                "caption": "Figure 8:Temperature scaling has no significant effect on shape bias neither under VQA (left) nor Image Captioning (right) tasks but starts to decrease accuracy at higher levels.Experiments performed onLLaVA-NeXT 7Bwith 3 seeds (except Temperature = 0 and Temperature = 1 of Image Captioning where we use a single seed).",
                "position": 1945
            }
        ]
    },
    {
        "header": "Appendix GResults on CLIP Models",
        "images": []
    },
    {
        "header": "Appendix HResults on ImageNet-trained Models",
        "images": []
    },
    {
        "header": "Appendix IAdditional Thoughts on the Image Captioning Task",
        "images": []
    },
    {
        "header": "Appendix JClass-wise Texture/Shape Bias",
        "images": [
            {
                "img": "https://arxiv.org/html/2403.09193/x11.png",
                "caption": "(a)VQA",
                "position": 2519
            },
            {
                "img": "https://arxiv.org/html/2403.09193/x11.png",
                "caption": "(a)VQA",
                "position": 2522
            },
            {
                "img": "https://arxiv.org/html/2403.09193/x12.png",
                "caption": "(b)Image Captioning",
                "position": 2527
            }
        ]
    },
    {
        "header": "Appendix KResponsibility to Human Subjects",
        "images": [
            {
                "img": "https://arxiv.org/html/2403.09193/x13.png",
                "caption": "Figure 10:Steering by Vision.For one example image, we show how patch shuffling (top) increases texture bias by destroying shape information. Below we show how adding Gaussian noise increases shape bias by destroying texture information. Please note that we show more extreme values than those used in our experiments for visualization purposes.",
                "position": 2548
            }
        ]
    },
    {
        "header": "Appendix LVisualization of Vision Steering",
        "images": []
    }
]