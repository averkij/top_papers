[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11711/x1.png",
                "caption": "Figure 1:Comparison in accumulated gradients in the SFT stage vs RL stage for popular released checkpoints. SFT stage has accumulated much denser updates, while RL is mostly sparse.",
                "position": 103
            },
            {
                "img": "https://arxiv.org/html/2505.11711/x2.png",
                "caption": "Figure 2:In PRIME, 72% parameters are never updated, 8% have gradients canceling each other out, and 20% constitute the subnetwork that is consistently updated (ยง6)",
                "position": 109
            }
        ]
    },
    {
        "header": "2Related work and Background",
        "images": []
    },
    {
        "header": "3RL Induces Sparse but Full-rank Updates; SFT Induces Dense Ones",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11711/x3.png",
                "caption": "Figure 3:Layerwise and per-parameter-matrix update sparsity for DPO (left) and PRIME (right). All layers are similarly sparsely updated, with the only exception of the layer normalization layers, which receive little to no updates.",
                "position": 343
            }
        ]
    },
    {
        "header": "4Finetuning the Subnetwork Alone Can Reproduce the Full-finetuned Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11711/x4.png",
                "caption": "Figure 4:Training loss for training DPO with subnetwork finetuning and full finetuning. Training the subnetwork in isolation consistently causes train loss to be lower.",
                "position": 436
            }
        ]
    },
    {
        "header": "5Consistency of Subnetworks Across Seeds, Data, and Algorithms",
        "images": []
    },
    {
        "header": "6Why Do the Subnetworks Emerge?",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11711/x5.png",
                "caption": "Figure 5:Update sparsity of intermediate checkpoints of a training run of PRIME. We observe that with more training the sparsity slowly decays.",
                "position": 696
            },
            {
                "img": "https://arxiv.org/html/2505.11711/x6.png",
                "caption": "Figure 6:Percentage of updated weights that are outside the final subnetwork across training steps",
                "position": 718
            }
        ]
    },
    {
        "header": "7Limitations and Future Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    },
    {
        "header": "Appendix BHyperparameter choices for Gradient Masking experiments",
        "images": []
    },
    {
        "header": "Appendix CModel Checkpoints: SFT vs RL sparsity comparison",
        "images": []
    },
    {
        "header": "Appendix DTraining Dynamics",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11711/x7.png",
                "caption": "(a)",
                "position": 1897
            },
            {
                "img": "https://arxiv.org/html/2505.11711/x7.png",
                "caption": "(a)",
                "position": 1900
            },
            {
                "img": "https://arxiv.org/html/2505.11711/x8.png",
                "caption": "(b)",
                "position": 1905
            }
        ]
    },
    {
        "header": "Appendix ERandom Guessing baseline",
        "images": []
    }
]