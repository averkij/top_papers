[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15510/x1.png",
                "caption": "Figure 1:How can we condition diffusion models in robotic control?We investigate methods forconditioningtext-to-image diffusion models(Rombach et al.,2022)to perform control, aiming to address various tasks in atask-adaptivemanner. We observe that text prompts, unlike in other vision tasks(Zhao et al.,2023), are ineffective for robotic control. Therefore, we propose to learntaskprompts in control environments and further incorporate dynamic details throughvisualprompts for conditioning diffusion models.",
                "position": 104
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Motivation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15510/x2.png",
                "caption": "Figure 2:Motivation.We aim to overcome the limitations of existingtask-agnosticapproach (a) with frozen pre-trained visual representations(Parisi et al.,2022), by leveragingconditionsin diffusion models for robotic control tasks in atask-adaptiveapproach (b). In this regard, we explore text conditions(§4.1), more advanced methods(§4.2,§5) as conditions.",
                "position": 188
            },
            {
                "img": "https://arxiv.org/html/2510.15510/x3.png",
                "caption": "Figure 3:Case study.(a)We find that text conditions can be disadvantageous in some control tasks.(b)ForButton-press, the cross-attention maps (e.g., forbutton,press) are well-grounded to relevant image regions.(c)In contrast, forCheetah-run, the attention maps (e.g., forcheetah,run) are noisy, which presumably leads to a decline in performance. Nonetheless, our approach of using task and visual tokens (§5) achieves consistent gains across all tasks, with its cross-attention maps capturing diverse regions of the image relevant to the downstream task.",
                "position": 204
            }
        ]
    },
    {
        "header": "5ORCA: Conditioning diffusion models for robotic control",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15510/x4.png",
                "caption": "Figure 4:Proposed framework.We propose ORCA, a framework for learningtaskandvisualprompts to condition diffusion models in robotic control. Specifically, we utilize the features from the downsampling blocks and the bottleneck block of Stable Diffusion(Rombach et al.,2022)to extract visual representations conditioned on our input, which are then fed to the policy network for predicting the action.",
                "position": 236
            }
        ]
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15510/x5.png",
                "caption": "Figure 5:Visualization of evaluation tasks.",
                "position": 498
            },
            {
                "img": "https://arxiv.org/html/2510.15510/x6.png",
                "caption": "Figure 6:Cross-attention visualization for task/visual prompts.We visualize the cross-attention maps for task promptptp_{t}and visual promptspv1p_{v}^{1}andpv2p_{v}^{2}inRelocatetask across frames from Adroit.",
                "position": 613
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AFurther Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15510/x7.png",
                "caption": "Figure 7:Visualization of normalized attention weights and raw attention scores for<bos>and<eos>tokens.We compare the visualization of the normalized attention weights obtained after the softmax operation and the raw attention scores obtained before the softmax operation from the cross-attention layers to further analyze the properties of<bos>and<eos>tokens.",
                "position": 1684
            }
        ]
    },
    {
        "header": "Appendix BFurther Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15510/x8.png",
                "caption": "Figure 8:Visualization of agents performing downstream tasks in Adroit(Rajeswaran et al.,2018).We provide visual comparison of our method to CLIP(Cherti et al.,2023), and VC-1(Majumdar et al.,2023)for two tasks from Adroit. We additionally report whether the task has succeeded or failed for each episode.",
                "position": 1908
            },
            {
                "img": "https://arxiv.org/html/2510.15510/x9.png",
                "caption": "Figure 9:Visualization of agents performing downstream tasks in DMC(Tassa et al.,2018).We provide a visual comparison of our method to CLIP(Cherti et al.,2023), and VC-1(Majumdar et al.,2023)for five tasks in DMC. We additionally report the normalized score for each episode.",
                "position": 1911
            },
            {
                "img": "https://arxiv.org/html/2510.15510/x10.png",
                "caption": "Figure 10:Visualization of agents performing downstream tasks in MetaWorld(Yu et al.,2020).We provide visual comparison of our method to CLIP(Cherti et al.,2023), and VC-1(Majumdar et al.,2023)for five tasks in MetaWorld. We additionally report whether the task has succeeded or failed for each episode.",
                "position": 1914
            }
        ]
    },
    {
        "header": "Appendix CQualitative visualization on robotic control tasks",
        "images": []
    }
]