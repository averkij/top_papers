[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21070/x1.png",
                "caption": "Figure 1:Performance of ScaleDiff on AIME’24, AIME’25, and HMMT-Feb’25, compared to other SFT-based LRM baselines.",
                "position": 110
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21070/x2.png",
                "caption": "Figure 2:Overview of ScaleDiff pipeline.",
                "position": 180
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": []
    },
    {
        "header": "4Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21070/x3.png",
                "caption": "Figure 3:Accuracy scaling with the size of augmented data on AIME’24, AIME’25, and MATH500. The amount of augmented data is 1/2, 1, and 2 times the size of the original dataset.",
                "position": 887
            },
            {
                "img": "https://arxiv.org/html/2509.21070/x4.png",
                "caption": "Figure 4:Distribution of solution lengths across datasets and teacher models. The superscript L denotes the use of the large-sized Qwen3-235B-A22B as the teacher model, whereas S indicates the use of the small-sized Qwen3-8B.",
                "position": 968
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]