[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Background and Methods",
        "images": []
    },
    {
        "header": "4Weight decay Improves Language Model Plasticity",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11137/x1.png",
                "caption": "(a)Llama-2 at 20 TPP",
                "position": 255
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x1.png",
                "caption": "(a)Llama-2 at 20 TPP",
                "position": 258
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x2.png",
                "caption": "(b)OLMo-2 at 20 TPP",
                "position": 265
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x3.png",
                "caption": "(c)Llama-2 4B and OLMo-2 at 140 TPP",
                "position": 272
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x4.png",
                "caption": "",
                "position": 285
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x5.png",
                "caption": "Figure 2:Weight decay during pretraining improves language model plasticity and downstream performance.This figure plots the average accuracy after fine-tuning for models pretrained with varying weight decay. The results indicate that weight decay leads to better downstream model performance, suggesting it enables the pretrained model to learn better during fine-tuning and improves model plasticity. In these experiments, the optimal weight decay for downstream performance is larger than the standard default of 0.1. In addition, the optimal weight decay based on pretraining loss (Figure1(c)) and that based on fine-tuning accuracy (this figure) are different, suggesting that optimizing hyperparameters based solely on pretraining loss does not always produce models with the best downstream performance.",
                "position": 289
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x6.png",
                "caption": "Figure 3:A model’s performance after pretraining is not perfectly predictive of its performance downstream.Models with similar pretraining losses can perform differently downstream, and models with lower validation cross-entropy loss after pretraining can perform better or worse downstream (i.e., after fine-tuning) than models with higher pretraining losses.",
                "position": 306
            }
        ]
    },
    {
        "header": "5A Mechanistic Perspective on Weight Decay and Model Behavior",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11137/x7.png",
                "caption": "(a)Llama-2-0.5B-20x",
                "position": 361
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x7.png",
                "caption": "(a)Llama-2-0.5B-20x",
                "position": 364
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x8.png",
                "caption": "(b)Llama-2-4B-20x",
                "position": 371
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x9.png",
                "caption": "(c)OLMo-2-1B-20x",
                "position": 378
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x10.png",
                "caption": "(d)OLMo-2-1B-140x",
                "position": 385
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x11.png",
                "caption": "",
                "position": 398
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x12.png",
                "caption": "(a)Query-Key",
                "position": 408
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x12.png",
                "caption": "(a)Query-Key",
                "position": 411
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x13.png",
                "caption": "(b)Value-Projection",
                "position": 418
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x14.png",
                "caption": "(c)Query-Key",
                "position": 425
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x15.png",
                "caption": "(d)Value-Projection",
                "position": 432
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x16.png",
                "caption": "",
                "position": 439
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x17.png",
                "caption": "Figure 6:Weight decay reduces overfitting on training data.The figure depicts the train-val gap (Equation3) for OLMo-2-1B models trained at 20 TPP.",
                "position": 474
            }
        ]
    },
    {
        "header": "6Discussion and Concluding Remarks",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendices",
        "images": []
    },
    {
        "header": "Appendix APre-training",
        "images": []
    },
    {
        "header": "Appendix BFine-tuning",
        "images": []
    },
    {
        "header": "Appendix CEvaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11137/x18.png",
                "caption": "(a)MetaMathQA",
                "position": 1849
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x18.png",
                "caption": "(a)MetaMathQA",
                "position": 1852
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x19.png",
                "caption": "(b)MedMCQA",
                "position": 1860
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x20.png",
                "caption": "(c)PubMedQA",
                "position": 1868
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x21.png",
                "caption": "(d)MMLUProCoT",
                "position": 1876
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x22.png",
                "caption": "(e)RACE",
                "position": 1884
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x23.png",
                "caption": "(f)SimpleScaling",
                "position": 1892
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x24.png",
                "caption": "(g)Average accuracy over tasks",
                "position": 1900
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x25.png",
                "caption": "(a)MetaMathQA",
                "position": 1911
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x25.png",
                "caption": "(a)MetaMathQA",
                "position": 1914
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x26.png",
                "caption": "(b)MedMCQA",
                "position": 1922
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x27.png",
                "caption": "(c)PubMedQA",
                "position": 1930
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x28.png",
                "caption": "(d)MMLUProCoT",
                "position": 1938
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x29.png",
                "caption": "(e)RACE",
                "position": 1946
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x30.png",
                "caption": "(f)SimpleScaling",
                "position": 1954
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x31.png",
                "caption": "(g)Average accuracy over tasks",
                "position": 1962
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x32.png",
                "caption": "Figure 9:Stability analysis for Pearson correlation coefficient.Pearson correlation is computed for each leave-one-out (LOO) subset in Figure8g. The LOO correlation can change noticeably in magnitude and sign, suggesting that the correlation for the full set of data points in Figure8g is rather unstable, which further supports the finding in that pretraining validation cross-entropy loss (pretraining performance) is not perfectly predictive of fine-tuning accuracy (downstream performance).",
                "position": 1974
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x33.png",
                "caption": "(a)AG News",
                "position": 1986
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x33.png",
                "caption": "(a)AG News",
                "position": 1989
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x34.png",
                "caption": "(b)SST",
                "position": 1996
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x35.png",
                "caption": "(c)Averaged over tasks",
                "position": 2003
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x36.png",
                "caption": "(a)AG News",
                "position": 2014
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x36.png",
                "caption": "(a)AG News",
                "position": 2017
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x37.png",
                "caption": "(b)SST",
                "position": 2024
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x38.png",
                "caption": "(c)Averaged over tasks",
                "position": 2031
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x39.png",
                "caption": "(a)AG News",
                "position": 2042
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x39.png",
                "caption": "(a)AG News",
                "position": 2045
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x40.png",
                "caption": "(b)SST",
                "position": 2052
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x41.png",
                "caption": "(c)Averaged over tasks",
                "position": 2059
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x42.png",
                "caption": "(a)AG News",
                "position": 2070
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x42.png",
                "caption": "(a)AG News",
                "position": 2073
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x43.png",
                "caption": "(b)SST",
                "position": 2080
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x44.png",
                "caption": "(c)Averaged over tasks",
                "position": 2087
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x45.png",
                "caption": "(a)AG News",
                "position": 2098
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x45.png",
                "caption": "(a)AG News",
                "position": 2101
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x46.png",
                "caption": "(b)SST",
                "position": 2108
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x47.png",
                "caption": "(c)Averaged over tasks",
                "position": 2115
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x48.png",
                "caption": "Figure 15:Probing accuracy is highly predictive of downstream model performance.The x-axis is the best average probing accuracy of the model (highest probing accuracy out of all model layers). The y-axis the average accuracy of the model over all tasks after fine-tuning. Pretrained models with higher probing accuracies from the linear probing experiments tend to perform better downstream after fine-tuning.",
                "position": 2126
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x49.png",
                "caption": "(a)Query-Key",
                "position": 2159
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x49.png",
                "caption": "(a)Query-Key",
                "position": 2162
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x50.png",
                "caption": "(b)Value-Projection",
                "position": 2169
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x51.png",
                "caption": "(c)Query-Key",
                "position": 2176
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x52.png",
                "caption": "(d)Value-Projection",
                "position": 2183
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x53.png",
                "caption": "",
                "position": 2190
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x54.png",
                "caption": "(a)Query-Key",
                "position": 2196
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x54.png",
                "caption": "(a)Query-Key",
                "position": 2199
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x55.png",
                "caption": "(b)Value-Projection",
                "position": 2206
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x56.png",
                "caption": "(a)Query-Key",
                "position": 2217
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x56.png",
                "caption": "(a)Query-Key",
                "position": 2220
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x57.png",
                "caption": "(b)Value-Projection",
                "position": 2227
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x58.png",
                "caption": "",
                "position": 2240
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x59.png",
                "caption": "(a)Input Layer",
                "position": 2244
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x59.png",
                "caption": "(a)Input Layer",
                "position": 2247
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x60.png",
                "caption": "(b)Intermediate Layer",
                "position": 2254
            },
            {
                "img": "https://arxiv.org/html/2602.11137/x61.png",
                "caption": "(c)Output Layer",
                "position": 2261
            }
        ]
    },
    {
        "header": "Appendix DWeight Decay’s Mechanistic Effects on Model Behavior",
        "images": []
    }
]