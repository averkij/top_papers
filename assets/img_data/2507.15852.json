[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15852/x1.png",
                "caption": "Figure 1:Overview of our Segment Concept (SeC) framework.Left: Compared to SAM 2, our model maintains better target tracking under severe appearance changes and scene transitions by leveraging concept-level guidance.Right: Quantitative results show that SeC consistently outperforms strong baselines, especially in scenarios involving multiple scene changes.",
                "position": 100
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15852/x2.png",
                "caption": "Figure 2:ùí•&‚Ñ±ùí•‚Ñ±\\mathcal{J}\\&\\mathcal{F}caligraphic_J & caligraphic_FCurve in terms of concept guidance ratio on SeCVOS. Sparse activation (e.g., under 10%) already achieves strong performance.",
                "position": 322
            }
        ]
    },
    {
        "header": "4SeCVOS benchmark",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15852/x3.png",
                "caption": "Figure 3:Qualitative comparison between SAM 2 and SeC (ours) on the SeCVOS benchmark.",
                "position": 1303
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix ASupplementary Material Overview",
        "images": []
    },
    {
        "header": "Appendix BDetails of SeCVOS",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15852/x4.png",
                "caption": "Figure 4:Example video sequences from the SeCVOS benchmark with overlaid target masks. Each row corresponds to frames from a single video sequence, illustrating the annotated object masks.",
                "position": 1331
            }
        ]
    },
    {
        "header": "Appendix CReferring Video Object Segmentation on SeCVOS",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15852/x5.png",
                "caption": "Figure 5:Example video sequences and corresponding referring expressions from the SeCVOS benchmark.",
                "position": 1338
            }
        ]
    },
    {
        "header": "Appendix DAdditional Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15852/x6.png",
                "caption": "Figure 6:Additional qualitative comparison between SAM 2 and SeC (ours) on the SeCVOS benchmark.",
                "position": 1431
            },
            {
                "img": "https://arxiv.org/html/2507.15852/x7.png",
                "caption": "Figure 7:Failure case example from the SeCVOS benchmark.",
                "position": 1434
            }
        ]
    },
    {
        "header": "Appendix EBroader Impacts",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]