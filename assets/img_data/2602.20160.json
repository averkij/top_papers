[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.20160/x1.png",
                "caption": "Figure 2:Given a set of posed input images, tttLRM encodes them into tokens (green boxes) after patchifying. The input tokens are fed into the LaCT block (shown in the blue frame) where fast weights are updated accordingly. Another set of virtual tokens (blue boxes) are used to query the updated fast weights, and decoded into 3D representations like 3DGS for high-quality novel view synthesis.",
                "position": 154
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.20160/x2.png",
                "caption": "Figure 3:Illustration of distributed feedforward reconstruction training. First, image tokens are sharded across GPUs, and each GPU predicts Gaussians for its assigned virtual views after the fast weights are synchronized. The predicted Gaussians are then gathered to construct the full scene, after which each GPU renders a subset of novel views and computes its respective losses. Gradients are finally all reduced and backpropagated across all devices.",
                "position": 301
            },
            {
                "img": "https://arxiv.org/html/2602.20160/x3.png",
                "caption": "Figure 4:Qualitative comparison between our method and baseline approaches. Our model reconstructs the 3DGS scene with higher fidelity than both optimization-based and feedforward baselines, as also reflected in the PSNR metrics. Please zoom in for a better comparison.",
                "position": 338
            },
            {
                "img": "https://arxiv.org/html/2602.20160/x4.png",
                "caption": "Figure 5:We demonstrate that our high-resolution1024×10241024\\times 10243DGS tttLRM can be effectively used for image-to-3D generation when combined with a multi-view generator. Our model enables the reconstruction of fine-grained, photorealistic detailse.g., hair, fur, and text, from the input images. Video results are provided in the supplemental material.",
                "position": 341
            },
            {
                "img": "https://arxiv.org/html/2602.20160/x5.png",
                "caption": "Figure 6:We show that tttLRM, as a general framework, can also interpret the latent 3D memory into formats besides 3DGS. In this experiment, we use a set of triplane tokens to query the fast weights and then fine-tune the model for triplane-based NeRF reconstruction. We visualize the resulting triplanes and present the corresponding renderings and depth maps for 4 views at a resolution of512×512512\\times 512.",
                "position": 344
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.20160/x6.png",
                "caption": "Figure 7:Our 3DGS reconstruction model leverages pretraining with LVSM on novel view synthesis tasks, which significantly accelerates learning and leads to better performance, compared to training from scratch.",
                "position": 832
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AFurther Discussions",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.20160/x7.png",
                "caption": "Figure 8:Time comparison of 3 Attention layers vs 24 layers of LaCT blocks under different numbers of tokens.",
                "position": 1915
            }
        ]
    },
    {
        "header": "Appendix BExperiment Details",
        "images": []
    },
    {
        "header": "Appendix CMore results and Comparison",
        "images": []
    }
]