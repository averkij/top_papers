[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.24391/x1.png",
                "caption": "Figure 1:We presentEasi3R, a training-free, plug-and-play approach that efficiently disentangles object and camera motion, enabling the adaptation ofDUSt3Rfor 4D reconstruction.",
                "position": 70
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.24391/x2.png",
                "caption": "Figure 2:DUSt3Rwith Dynamic Video.We process videos using a sliding window and infer theDUSt3Rnetwork pairwise. Reconstruction degrades with misalignment when dynamic objects occupy a considerable portion of the frames.",
                "position": 173
            },
            {
                "img": "https://arxiv.org/html/2503.24391/extracted/6321542/figures/fig_pipe.png",
                "caption": "Figure 3:DUSt3Rand ourEasi3Radaptation.DUSt3Rencodes two imagesIa,Ibsuperscriptğ¼ğ‘superscriptğ¼ğ‘I^{a},I^{b}italic_I start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT , italic_I start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPTinto feature tokensğ…0a,ğ…0bsuperscriptsubscriptğ…0ğ‘superscriptsubscriptğ…0ğ‘\\mathbf{F}_{0}^{a},\\mathbf{F}_{0}^{b}bold_F start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT , bold_F start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT, which are then decoded into point maps in the reference view coordinate space using two decoders. OurEasi3Raggregates the cross-attention maps from the decoders, producing four semantically meaningful maps:ğ€Î¼b=src,ğ€Ïƒb=src,ğ€Î¼a=ref,ğ€Ïƒb=refsubscriptsuperscriptğ€ğ‘srcğœ‡subscriptsuperscriptğ€ğ‘srcğœsubscriptsuperscriptğ€ğ‘refğœ‡subscriptsuperscriptğ€ğ‘refğœ\\mathbf{A}^{b=\\text{src}}_{\\mu},\\mathbf{A}^{b=\\text{src}}_{\\sigma},\\mathbf{A}^%\n{a=\\text{ref}}_{\\mu},\\mathbf{A}^{b=\\text{ref}}_{\\sigma}bold_A start_POSTSUPERSCRIPT italic_b = src end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_Î¼ end_POSTSUBSCRIPT , bold_A start_POSTSUPERSCRIPT italic_b = src end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_Ïƒ end_POSTSUBSCRIPT , bold_A start_POSTSUPERSCRIPT italic_a = ref end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_Î¼ end_POSTSUBSCRIPT , bold_A start_POSTSUPERSCRIPT italic_b = ref end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_Ïƒ end_POSTSUBSCRIPT. These maps are then used for a second inference pass to enhance reconstruction quality.",
                "position": 202
            },
            {
                "img": "https://arxiv.org/html/2503.24391/x3.png",
                "caption": "Figure 4:Visualization for Cross-Attention Maps.We color thenormalizedvalues of attention maps, ranging fromonetozero. We highlight the patterns captured by each type of attention map using relatively high values. For a more detailed demonstration, we invite reviewers to visit our webpage undereasi3r.github.io.",
                "position": 248
            },
            {
                "img": "https://arxiv.org/html/2503.24391/extracted/6321542/figures/cloud.png",
                "caption": "",
                "position": 346
            },
            {
                "img": "https://arxiv.org/html/2503.24391/extracted/6321542/figures/overlap.png",
                "caption": "",
                "position": 346
            },
            {
                "img": "https://arxiv.org/html/2503.24391/extracted/6321542/figures/cam.png",
                "caption": "",
                "position": 349
            },
            {
                "img": "https://arxiv.org/html/2503.24391/extracted/6321542/figures/elephant.png",
                "caption": "",
                "position": 352
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.24391/x4.png",
                "caption": "Figure 5:Qualitative Results of Dynamic Object Segmentation.â€œOursâ€ refers to theEasi3Rmonst3rsubscriptEasi3Rmonst3r\\mbox{{Easi3R}}_{\\text{monst3r}}Easi3R start_POSTSUBSCRIPT monst3r end_POSTSUBSCRIPTsetting. Here, we present the enhanced setting, where outputs from different methods serve as prompts and are used with SAM2[46]for mask inference.",
                "position": 453
            },
            {
                "img": "https://arxiv.org/html/2503.24391/x5.png",
                "caption": "Figure 6:Qualitative Comparison.We visualize cross-frame globally aligned static scenes with dynamic point clouds at a selected timestamp. Notably, instead of using ground truth dynamic masks in previous work, we apply the estimated per-frame dynamic masks to filter out dynamic points at other timestamps for comparison.\nOur method (top two and bottom two rows asEasi3Rdust3r/monst3rdust3r/monst3r{}_{\\text{dust3r/monst3r}}start_FLOATSUBSCRIPT dust3r/monst3r end_FLOATSUBSCRIPT, respectively) achieves temporally consistent reconstruction of both static scenes and moving objects, whereas baselines suffer from static structure misalignment and unstable camera pose estimation, and ghosting artifacts due to inaccuracy estimation of dynamic segmentation.",
                "position": 676
            },
            {
                "img": "https://arxiv.org/html/2503.24391/x6.png",
                "caption": "Figure 7:Visualization of estimated camera trajectories.Our robust estimated camera trajectory (orange) deviates less from the ground truth (gray) compared to the original backbones (blue).",
                "position": 897
            },
            {
                "img": "https://arxiv.org/html/2503.24391/x7.png",
                "caption": "",
                "position": 907
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADynamic Object Segmentation",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.24391/x8.png",
                "caption": "Figure 8:Benefits of Cross-frame Feature Clustering.We visualize the dynamic attention mapğ€t=dynsuperscriptğ€ğ‘¡dyn\\mathbf{A}^{t=\\text{dyn}}bold_A start_POSTSUPERSCRIPT italic_t = dyn end_POSTSUPERSCRIPT, cluster assignmentsCtsuperscriptğ¶ğ‘¡C^{t}italic_C start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT, and cluster-fused dynamic attention mapğ€fuset=dynsubscriptsuperscriptğ€ğ‘¡dynfuse\\mathbf{A}^{t=\\text{dyn}}_{\\text{fuse}}bold_A start_POSTSUPERSCRIPT italic_t = dyn end_POSTSUPERSCRIPT start_POSTSUBSCRIPT fuse end_POSTSUBSCRIPT.\nFeatures from the DUSt3R encoder exhibit temporal consistency, as cluster assignments (Ctsuperscriptğ¶ğ‘¡C^{t}italic_C start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT) remain unchanged across frames, thereby enhancing temporal consistency in dynamic segmentation (ğ€t=dynâ¢fusesuperscriptğ€ğ‘¡dynfuse\\mathbf{A}^{t=\\text{dyn}}{\\text{fuse}}bold_A start_POSTSUPERSCRIPT italic_t = dyn end_POSTSUPERSCRIPT fuse) through clustering-guided temporal fusing. For better visual intuition, we invite readers toeasi3r.github.io.",
                "position": 2402
            }
        ]
    },
    {
        "header": "Appendix BAblation Study",
        "images": []
    },
    {
        "header": "Appendix CLimitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.24391/x9.png",
                "caption": "Figure 9:Limitation.We visualize static reconstructions from two different viewpoints in the top and bottom rows.Easi3Rimproves camera pose estimation and point cloud reconstruction (top row), enhancing alignment in structures like swing supports through attention re-weighting and segmentation-aware global alignment. However, from another viewpoint (bottom row),Easi3Rstill produces floaters near object boundaries.",
                "position": 2753
            },
            {
                "img": "https://arxiv.org/html/2503.24391/x10.png",
                "caption": "Figure 10:Qualitative Comparison.We visualize cross-frame globally aligned static scenes with dynamic point clouds at a selected timestamp. Notably, instead of using ground truth dynamic masks in previous work, we apply the estimated per-frame dynamic masks to filter out dynamic points at other timestamps for comparison. Top and bottom rows areEasi3Rdust3r/monst3rdust3r/monst3r{}_{\\text{dust3r/monst3r}}start_FLOATSUBSCRIPT dust3r/monst3r end_FLOATSUBSCRIPT, respectively.",
                "position": 2792
            },
            {
                "img": "https://arxiv.org/html/2503.24391/x11.png",
                "caption": "Figure 11:Disentanglement vs.MonST3R[73].We visualize the disentangled 4D reconstruction, static scene and dynamic objects at different frames.MonST3Rtends to predict under-segmented dynamic masks.",
                "position": 2833
            },
            {
                "img": "https://arxiv.org/html/2503.24391/x12.png",
                "caption": "Figure 12:Disentanglement vs.DAS3R[68].We visualize the disentangled 4D reconstruction, static scene and dynamic objects at different frames.DAS3Rtends to predict over-segmented dynamic masks.",
                "position": 2838
            }
        ]
    },
    {
        "header": "Appendix DAddtional Results",
        "images": []
    }
]