[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02692/extracted/6178801/Figures/Opportunities.jpg",
                "caption": "Figure 1:Opportunities for Intelligent Sensing-to-Action:Insensing-to-actionloops, significant gains can be achieved by selectively sensing critical environmental regions whilepredictingless critical areas based on training data. This frugal sensing strategy is especially beneficial for resource-intensive modalities, such as LiDAR, enhancing task accuracy without unnecessary overhead. Similarly,action-to-sensingoptimizations can adjust control variables to opportunistically reduce sensing demands based on task relevance. While these frameworks improve loop efficiency, ensuring reliability requires robust and computationally efficient monitors to continuously assess fidelity and support aggressive optimizations. Inmulti-agent sensing-action loops, agents can collaborate by sharing sensing tasks or complementing each other’s sensing capabilities. Moreover, emerging paradigms, such asneuromorphic sensing-action loops, offer unified frameworks by adapting sensing and processing rates based on event dynamics, enabling seamless sensing and control.",
                "position": 81
            }
        ]
    },
    {
        "header": "IIIntelligent Sensing-to-Action",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02692/x1.png",
                "caption": "Figure 2:An end-to-end computing pipeline comparison sensing-processing-action loop between a biological and a neuromorphic system.In a biological system, inputs are perceived as changes in intensity (events and frames) and color (frames) by the eye. In contrast, a neuromorphic system uses frame cameras to capture analog intensity at low rates and event cameras to detect motion-induced variations, generating events. The brain’s parallel and recurrent connections enable computation within memory. Neuromorphic system emulates this by combining ANNs, SNNs, and hybrid ANN-SNN models to balance accuracy and efficiency. These algorithms also benefit from hardware acceleration via in-memory (IMC) and near-memory (NMC) computing by efficiently implementing synaptic functionality and, work alongside CPU/GPU architectures to enhance efficiency and reduce latency.",
                "position": 95
            }
        ]
    },
    {
        "header": "IIIOptimizing “Sensing-to-Action” Pathways",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02692/extracted/6178801/Figures/ICASSP_GenSense.png",
                "caption": "Figure 3:Generative Sensing:Sense only what you really need:Generative sensing optimizes resource use by focusing on essential environmental features, reducing unnecessary data collection and enhancing real-time responsiveness. For LiDAR proessing, in this approach, the input point cloud is voxelized and radially masked based on voxel distance from the sensor to minimize redundant information. A 3D spatially sparse convolutional encoder extracts latent features, while a decoder reconstructs the 3D scene, enabling efficient perception that supports adaptive sensing-to-action strategies.",
                "position": 121
            }
        ]
    },
    {
        "header": "IVOptimizing “Action-to-Sensing” Pathways",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02692/x2.png",
                "caption": "Figure 4:Our approach conditions visual representations on the task policy by incorporating contrastive spectral Koopman encoding and reinforcement learning (RL)-guided control. This high-level framework unifies perception and control, enabling task-aware sensing adjustments. The RoboKoop model leverages these representations to dynamically adjust sensing parameters based on control objectives. (Adapted from RoboKoop[18])",
                "position": 288
            },
            {
                "img": "https://arxiv.org/html/2502.02692/extracted/6178801/Figures/control_op.png",
                "caption": "Figure 5:(a) Computational load of state-of-the-art dynamical models. (b) Performance under external disturbances. (Adapted from RoboKoop[18])",
                "position": 294
            },
            {
                "img": "https://arxiv.org/html/2502.02692/extracted/6178801/Figures/control_op.png",
                "caption": "",
                "position": 297
            },
            {
                "img": "https://arxiv.org/html/2502.02692/extracted/6178801/Figures/plot_action_prob.png",
                "caption": "",
                "position": 301
            },
            {
                "img": "https://arxiv.org/html/2502.02692/x3.png",
                "caption": "Figure 6:Ensuring Sensing-Action Loop Reliability:STARNet enhances the reliability of sensing-to-action loops by ingesting feature representations from primary task networks. A VAE models the typical distribution of these features, and during inference, STARNet uses gradient-free optimization to compute likelihood regret, identifying discrepancies between sensed and learned distributions to alert the system to potential inaccuracies.",
                "position": 319
            }
        ]
    },
    {
        "header": "VReliability of Sensing-to-Action Loops",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02692/x4.png",
                "caption": "Figure 7:Object Detection Accuracy for KITTI Dataset:The VAE-based approach analyzes LiDAR point clouds and object labels, producing bounding boxes for cars, pedestrians, and cyclists. The network was tested under challenging conditions, such as varying snow intensities and other corruptions.",
                "position": 333
            },
            {
                "img": "https://arxiv.org/html/2502.02692/x5.png",
                "caption": "Figure 8:Neuromorphic sensing-action loop architectures.a) Full-ANN[48], Full-SNN[49], and Hybrid SNN-ANN[50]models for optical flow estimation using event data. b) Fusion-FlowNet[51]integrates event-based and frame-based modalities for enhanced feature extraction. Outputs are aggregated at the final SNN layer. (Adapted from Fusion-FlowNet[51]).",
                "position": 342
            }
        ]
    },
    {
        "header": "VISensing-Action Loops in Neuromorphic Systems",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02692/x6.png",
                "caption": "Figure 9:Average Endpoint Error (AEE) comparison for Optical flow estimation on the MVSEC[66]dataset.The left shows the Average Endpoint Error (AEE) for baseline models, EvFlow-Net (EvF)[48], Spike-FlowNet (SpF)[50], and Fusion-FlowNet (FF)[51]). The right showcases how AEE varies with model size for Adaptive-SpikeNet and corresponding full-ANN models. (Adapted from Adaptive-SpikeNet[49]).",
                "position": 362
            }
        ]
    },
    {
        "header": "VIIFederated, Multi-Agent Sensing-Action Loops",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02692/x7.png",
                "caption": "Figure 10:Key aspects of dynamic multi-agent systems: resource heterogeneity, adaptable architectures, hardware-aware optimization, and workload management across server-client interactions.",
                "position": 379
            },
            {
                "img": "https://arxiv.org/html/2502.02692/x8.png",
                "caption": "Figure 11:Performance comparison of DC-NAS and HaLo-FL on the CIFAR-10 dataset, showing relative reductions in energy, latency, and area with adaptive model optimization.",
                "position": 385
            }
        ]
    },
    {
        "header": "VIIIConclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]