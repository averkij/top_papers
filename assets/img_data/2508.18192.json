[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.18192/x1.png",
                "caption": "Figure 1:Connecting cognitive skills, datasets, and LLMs to form a multipartite network.(a) The schematic figure illustrates the relationships between cognitive skills, datasets, and LLM weight parameters as a multipartite network, drawing an analogy to the interconnected organization of the human brain. (b) The bar plots illustrate how different cognitive functions are categorized by the frequency of their associated skills and how often they appear across multiple-choice question datasets.",
                "position": 89
            },
            {
                "img": "https://arxiv.org/html/2508.18192/x2.png",
                "caption": "Figure 2:Multipartite Network of Skills, Dataset and Modules of Llama2 model.The network depicts modules (squares), datasets (triangles), and skills (circles) as nodes. Edges are weighted by the normalized values derived from the bipartite relationships between skills and datasets and between datasets and modules, reflecting the structural importance and interactions within the multiple types of nodes. The projection network simplifies the multipartite structure by collapsing intermediary nodes(datasets) to focus on the direct interactions between skills and modules usingB‚Äãl‚Äão‚Äãc‚ÄãkBlock-based pruning strategy. This projection highlights key dependencies and structural patterns within the model, offering insights into which modules are most influential for specific skills.",
                "position": 92
            }
        ]
    },
    {
        "header": "2Emerging community structures in skill and module networks",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.18192/x3.png",
                "caption": "Figure 3:Community structure comparison between skills and modules networks.(a)The skills projection network with nodes (skills) grouped based on the Louvain community detection algorithm, and colored with the cognitive-function label taken fromTable SI1, allowing direct visual comparison between detected communities and domain ground truth.(b)The Modules projection network, where each node (module) is assigned a community label based on Louvain partitioning applied at multiple resolutions, subsequently consolidated using average-linkage hierarchical clustering.(c)Color legend of cognitive function for node color in skills network in(a).(d)Schematic figure to represent the frequency of cognitive skills within each community for the Modules network(b).(e)Adjusted Rand Score (ARS) between the Louvain communities in the Skills network and the cognitive-function labels, plotted against different sparsity ratio for pruning and three base models (Llama, Llama-Chat, and Vicuna).(f)The chi-squared T-test statistically assessed the distinctiveness of skill distributions within each community of Modules networks, with their p-value for those three different models for different sparsity ratios.",
                "position": 109
            },
            {
                "img": "https://arxiv.org/html/2508.18192/x4.png",
                "caption": "Figure 4:Spectral property and influence of modules within each community of Modules Network.(a-c) The frequency distributions of eigenvalues reveal the spectral characteristics of the network, where the presence of distinct eigenvalue gaps signifies well-defined community structures.(d-f) Participation coefficients and Z-scores quantify the roles of modules within and across communities, where higher coefficients highlight bridge modules and Z-scores identify influential or peripheral roles.",
                "position": 114
            }
        ]
    },
    {
        "header": "3Modular localization characterizes the structure and function of module network",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.18192/x5.png",
                "caption": "Figure 5:Comparison of accuracy and magnitude of weight change across fine-tuned LLMs using different cognitive skilled-based finetuning.(a-d) Schematic representation of community-specific fine-tuning to assess the influence of targeted adaptation. (a)Communitytargets community-specific modules aligned with particular cognitive skills, reflecting a focused, skill-driven approach; (b)Randomuses random module subsets matched in size to the community-specific selections, serving as a baseline to isolate the effects of structural organization versus random variation; (c)Allrepresents fine-tuning across all modules, representing a broad, undifferentiated adaptation strategy; (d)Without Finetuningdepicts models without any fine-tuning, reflecting the unaltered original state of the model.\n(e) Average L2 weight difference for models (Llama, Llama-Chat, Vicuna) across community-aligned datasets using a block-pruning strategy. Each bar represents a different fine-tuning condition.\n(f) Accuracy for the same fine-tuning configurations as (e), capturing the model‚Äôs performance by fine-tuning on community-based datasets using a block-based pruning strategy.",
                "position": 130
            }
        ]
    },
    {
        "header": "4Reveal the functional specialization through cognitive Skill-Based Fine-Tuning",
        "images": []
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "Methods",
        "images": []
    },
    {
        "header": "6Network Formulation",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Supplementary Note 1: Skills vs Dataset Network",
        "images": []
    },
    {
        "header": "Supplementary Note 2: Dataset vs Modules",
        "images": []
    },
    {
        "header": "Supplementary Note 3: Skill Weight Function",
        "images": []
    },
    {
        "header": "Supplementary Note 4: Skills Connectivity Network",
        "images": []
    },
    {
        "header": "Supplementary Note 4: Modules Connectivity Network",
        "images": []
    },
    {
        "header": "Supplementary Note 5: Influence of Modules with each community of Modules Network",
        "images": []
    },
    {
        "header": "Finetuning Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.18192/x6.png",
                "caption": "Figure 6:Heatmap of Skills vs. Datasets (Bi‚ÄãjSD\\textbf{B}_{ij}^{\\textbf{SD}}) with Hierarchical Clustering.The bipartite matrixBi‚ÄãjSD\\textbf{B}_{ij}^{\\textbf{SD}}represents the number of times skillsis_{i}is required for datasetùêÉj\\mathbf{D}_{j}. Datasets that require similar cognitive skills exhibit strong associations, as indicated by the clustered patterns in the heatmap.",
                "position": 1470
            },
            {
                "img": "https://arxiv.org/html/2508.18192/x7.png",
                "caption": "Figure 8:Average Performance of the pruned model after pruning with different sparsity ratios.Average accuracy performance of all datasets with different pruning ratios utilizing different pruning strategies, (a), block-based pruning strategy and, (b), channel-based pruning strategy.",
                "position": 1495
            },
            {
                "img": "https://arxiv.org/html/2508.18192/x8.png",
                "caption": "Figure 9:Average Performance of the pruned model after pruning with different sparsity ratios.(a) Importance of Modules quantifies the relationship between each dataset and the weight modules of LLMs (Llama2with a25%25\\%pruning ratio). The scatter plot with the line of best fit shows the relationship between the averageùêÅ:kDM\\mathbf{B}_{:k}^{\\textbf{DM}}of all LLM modules(‚Ñ≥k\\mathcal{M}_{k}) and the change in overall model performance before and after pruning. (b) The module sparsity ratio distribution ofLlama2with a25%25\\%pruning ratio is shown for two pruning strategies.(c) The variation ofùêÅDM\\mathbf{B}^{\\textbf{DM}}(edge weight between datasets and individual LLM modules) is shown for two pruning strategies.",
                "position": 1498
            },
            {
                "img": "https://arxiv.org/html/2508.18192/x9.png",
                "caption": "Figure 10:Evaluating clustering of pruned Llama models using Davies-Bouldin Score and Hotelling‚Äôs T-squared test.(a) The Davies-Bouldin Score determines the optimal number of K-Means clusters for grouping 174 PCA values derived from module sparsity values, obtained by pruning the Llama2 model using 174 different datasets. (b) A scatter plot showing the pruned Llama2 model, grouped by the optimal number of clusters identified ina, alongside randomly pruned models with the 174 datasets. (c) P-values from Hotelling‚Äôs T-squared test between different clusters, including random pruning, are all significantly small (<0.05<0.05), indicating dissimilarity in information processing between different clusters. (d) Hotelling‚Äôs T-squared statistics highlight the differences between clusters.",
                "position": 1501
            },
            {
                "img": "https://arxiv.org/html/2508.18192/x10.png",
                "caption": "Figure 11:Comparison of community alignment measures between communities in the Skills network and ground-truth cognitive-function labels, across different sparsity levels.Subplots (a‚Äìc) show results for the block-based pruning strategy, while (d‚Äìf) display the same for channel-based pruning. Each row visualizes the trends for three base models (Llama, Llama-Chat, and Vicuna), using (a,d) Adjusted Normalized Mutual Information (NMI), (b,e) Adjusted Rand Index (ARI), and (c,f) Jaccard Index as similarity metrics. The x-axis denotes the sparsity ratio applied during pruning, enabling evaluation of how sparsity for pruning the LLMs impacts community alignment within cognitive function labels.",
                "position": 1504
            },
            {
                "img": "https://arxiv.org/html/2508.18192/x11.png",
                "caption": "Figure 12:Multi-Layered Network: This diagram highlights the cognitive skills rooted in human cognition, mapped to individual datasets. The modules within LLMs are represented as distinct components, illustrating how different datasets influence these modules. The projection network of modules at the end reflects the localization of cognitive skills across the network.",
                "position": 1507
            },
            {
                "img": "https://arxiv.org/html/2508.18192/x12.png",
                "caption": "Figure 13:Heat map clustering of modules network (PM\\textbf{P}^{M}) for the llama model with block-based pruning, where leaf colors in the dendrograms represent distinct communities formed through hierarchical clustering of the co-assignment matrix, revealing structural patterns among attention modules across layers.",
                "position": 1510
            },
            {
                "img": "https://arxiv.org/html/2508.18192/x13.png",
                "caption": "Figure 14:Heat map clustering of modules network (PM\\textbf{P}^{M}) for the llama-chat model with block-based pruning, where leaf colors in the dendrograms represent distinct communities formed through hierarchical clustering of the co-assignment matrix, revealing structural patterns among attention modules across layers.",
                "position": 1513
            },
            {
                "img": "https://arxiv.org/html/2508.18192/x14.png",
                "caption": "Figure 15:Heat map clustering of modules network (PM\\textbf{P}^{M}) for the vicuna model with block-based pruning, where leaf colors in the dendrograms represent distinct communities formed through hierarchical clustering of the co-assignment matrix, revealing structural patterns among attention modules across layers.",
                "position": 1516
            },
            {
                "img": "https://arxiv.org/html/2508.18192/x15.png",
                "caption": "Figure 16:Heat map clustering of modules network (PM\\textbf{P}^{M}) for the llama model with channel-based pruning, where leaf colors in the dendrograms represent distinct communities formed through hierarchical clustering of the co-assignment matrix, revealing structural patterns among attention modules across layers.",
                "position": 1519
            },
            {
                "img": "https://arxiv.org/html/2508.18192/x16.png",
                "caption": "Figure 17:Heat map clustering of modules network (PM\\textbf{P}^{M}) for the llama-chat model with channel-based pruning, where leaf colors in the dendrograms represent distinct communities formed through hierarchical clustering of the co-assignment matrix, revealing structural patterns among attention modules across layers.",
                "position": 1522
            },
            {
                "img": "https://arxiv.org/html/2508.18192/x17.png",
                "caption": "Figure 18:Heat map clustering of modules network (PM\\textbf{P}^{M}) for the vicuna model with channel-based pruning, where leaf colors in the dendrograms represent distinct communities formed through hierarchical clustering of the co-assignment matrix, revealing structural patterns among attention modules across layers.",
                "position": 1525
            },
            {
                "img": "https://arxiv.org/html/2508.18192/x18.png",
                "caption": "Figure 19:Influence of Modules within each community of Modules Network.(a-b) The distribution of edge weight within different communities of modules network for Block and Channel based modules.(c-d) The scatter plot for different modules over two metrics, participation coefficient and degree z-score metric.",
                "position": 1528
            },
            {
                "img": "https://arxiv.org/html/2508.18192/x19.png",
                "caption": "Figure 20:Community-based fine-tuning aligned with cognitive skill relevanceThe influence of skill distributions within identified module communities is examined by selecting datasets matching the skill profiles of these communities.",
                "position": 1531
            },
            {
                "img": "https://arxiv.org/html/2508.18192/x20.png",
                "caption": "Figure 21:Performance of Targeted FinetuningAccuracy and weight difference magnitude of fine-tuned models (Llama, Llama-Chat, Vicuna) across two datasets aligned with each community that were created usingblock-based pruning strategy.",
                "position": 1534
            },
            {
                "img": "https://arxiv.org/html/2508.18192/x21.png",
                "caption": "Figure 22:Performance of Targeted FinetuningAccuracy and weight difference magnitude of fine-tuned models (Llama, Llama-Chat, Vicuna) across two datasets aligned with each community that were created usingchannel-based pruning strategy.",
                "position": 1537
            },
            {
                "img": "https://arxiv.org/html/2508.18192/x22.png",
                "caption": "Figure 23:Visualization of changes in weight modules of the Llama model after fine-tuning, highlighting task associations such as ‚Äòdisambiguation_qa‚Äô (Block) and ‚Äòidentify_odd_metaphor‚Äô (Channel).",
                "position": 1540
            },
            {
                "img": "https://arxiv.org/html/2508.18192/x23.png",
                "caption": "Figure 24:Visualization of changes in weight modules of the Llama-Chat model after fine-tuning, highlighting task associations such as ‚Äòhigh_school_psychology‚Äô (Block) and ‚Äòelectrical_engineering‚Äô (Channel).",
                "position": 1543
            },
            {
                "img": "https://arxiv.org/html/2508.18192/x24.png",
                "caption": "Figure 25:Visualization of changes in weight modules of the Vicuna model after fine-tuning, highlighting task associations such as ‚Äòimplicatures‚Äô (Block) and ‚Äòelectrical_engineering‚Äô (Channel).",
                "position": 1546
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]