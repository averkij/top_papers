[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.09611/x1.png",
                "caption": "Figure 1:Sequential vs. parallel thinking-aware image synthesis. (a) Sequential generation (Bagel, GPT4o) may suffer from vague or incorrect reasoning. (b) Parallel generation aligns text and image at each denoising step, reducing hallucination and errors. (c) Quantitative comparison shows reasoning can degrade performance in certain categories. (d) Poorer categories also exhibit weaker reasoning–image alignment, highlighting the need for stronger cross-modal alignment.",
                "position": 166
            },
            {
                "img": "https://arxiv.org/html/2511.09611/x2.png",
                "caption": "Figure 2:MMaDA-Parallelsupports parallel, thinking-aware image editing and generation. Compared with Bagel,MMaDA-Paralleldemonstrates superior reasoning quality and stronger alignment between the generated text and image outputs.",
                "position": 188
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3MMaDA-Parallel",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.09611/x3.png",
                "caption": "Figure 3:Parallel Generation Architecture: During (a) training, image and text responses are masked and predicted in parallel with a uniform mask predictor, optimized by the masked token likelihood objective. During (b) sampling, the model performs parallel decoding to generate both image and text responses jointly, enabling efficient multimodal response generation.",
                "position": 342
            },
            {
                "img": "https://arxiv.org/html/2511.09611/x4.png",
                "caption": "Figure 4:Overview of our proposed Parallel Reinforcement Learning (ParaRL). Rather than optimization only to the final denoised outputs, ParaRL introduces reward signals along the entire denoising trajectory, reinforcing semantic alignment consistently throughout the generation process.",
                "position": 433
            },
            {
                "img": "https://arxiv.org/html/2511.09611/x5.png",
                "caption": "Figure 5:Synergy of sampling. Given the prompt: “change the blue shirt to a vibrant rainbow color,” the specific color decoding in text and image emerges at the same step.",
                "position": 446
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.09611/x6.png",
                "caption": "Figure 6:Qualitative results in comparison with Bagel.",
                "position": 549
            },
            {
                "img": "https://arxiv.org/html/2511.09611/iclr2026/figs/reward_curve.png",
                "caption": "Figure 7:ParaRL reward training curve between trajectory and output level optimization.",
                "position": 854
            },
            {
                "img": "https://arxiv.org/html/2511.09611/iclr2026/figs/reward_curve.png",
                "caption": "Figure 7:ParaRL reward training curve between trajectory and output level optimization.",
                "position": 857
            },
            {
                "img": "https://arxiv.org/html/2511.09611/iclr2026/figs/reward_curve2.png",
                "caption": "Figure 8:ParaRL reward training curve across different sampling steps of the trajectory.",
                "position": 862
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix AScaling ofMMaDA-Parallel",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.09611/x7.png",
                "caption": "Figure 9:Additional qualitative results with the same training settings.",
                "position": 931
            }
        ]
    },
    {
        "header": "Appendix BAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.09611/x8.png",
                "caption": "Figure 10:Additional qualitative results on thinking-aware image editing.",
                "position": 1082
            },
            {
                "img": "https://arxiv.org/html/2511.09611/x9.png",
                "caption": "Figure 11:Additional qualitative results on thinking-aware image generation.",
                "position": 1085
            }
        ]
    },
    {
        "header": "Appendix CMore Related Work",
        "images": []
    },
    {
        "header": "Appendix DPreliminaries",
        "images": []
    },
    {
        "header": "Appendix ESampling Details on Text and Image",
        "images": []
    },
    {
        "header": "Appendix FDetails of Training Dataset Curation",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.09611/iclr2026/figs/edit.png",
                "caption": "Figure 12:Overview of our dataset for thinking-aware editing",
                "position": 1441
            },
            {
                "img": "https://arxiv.org/html/2511.09611/iclr2026/figs/gen.png",
                "caption": "Figure 13:Overview of our dataset for thinking-aware editing",
                "position": 1444
            }
        ]
    },
    {
        "header": "Appendix GDetails of ParaBench",
        "images": []
    },
    {
        "header": "Appendix HMore Implementation Details",
        "images": []
    },
    {
        "header": "Appendix IMore Ablation Studies",
        "images": []
    },
    {
        "header": "Appendix JLimitations and Future Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.09611/x10.png",
                "caption": "Figure 14:Output alignment evaluation prompt",
                "position": 1740
            },
            {
                "img": "https://arxiv.org/html/2511.09611/x11.png",
                "caption": "Figure 15:Text quality evaluation prompt",
                "position": 1744
            },
            {
                "img": "https://arxiv.org/html/2511.09611/x12.png",
                "caption": "Figure 16:Text alignment evaluation prompt",
                "position": 1748
            },
            {
                "img": "https://arxiv.org/html/2511.09611/x13.png",
                "caption": "Figure 17:Image consistency evaluation prompt",
                "position": 1752
            },
            {
                "img": "https://arxiv.org/html/2511.09611/x14.png",
                "caption": "Figure 18:Image quality evaluation prompt",
                "position": 1756
            },
            {
                "img": "https://arxiv.org/html/2511.09611/x15.png",
                "caption": "Figure 19:Image alignment evaluation prompt",
                "position": 1760
            }
        ]
    },
    {
        "header": "Appendix KPrompts for evaluation",
        "images": []
    }
]