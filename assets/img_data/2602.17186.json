[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.17186/x1.png",
                "caption": "(a)A sample that can be answered from common sense",
                "position": 103
            },
            {
                "img": "https://arxiv.org/html/2602.17186/x1.png",
                "caption": "(a)A sample that can be answered from common sense",
                "position": 106
            },
            {
                "img": "https://arxiv.org/html/2602.17186/x2.png",
                "caption": "(b)A sample that requires fine-grained visual understanding",
                "position": 112
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Visual Information Gain",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.17186/fig/tab-vig-cat.png",
                "caption": "Table 1:VIG’s sensitivity to the degree of the visual grounding.Examples from the MS-COCO[24]validation set show that VIG quantitatively captures the strength of visual support: high positive for a perfect match, moderate positive for partial grounding, and negative for a conflicting image.",
                "position": 288
            },
            {
                "img": "https://arxiv.org/html/2602.17186/fig/tab-vig-cat2.png",
                "caption": "",
                "position": 319
            },
            {
                "img": "https://arxiv.org/html/2602.17186/fig/tab-vig-dog.png",
                "caption": "",
                "position": 323
            },
            {
                "img": "https://arxiv.org/html/2602.17186/x3.png",
                "caption": "Figure 2:VIG distribution across benchmarks.Blue benchmarks (COCO, POPE) show stronger multimodal interaction, while red benchmarks (GQA, SQA) exhibit weaker visual dependency.",
                "position": 355
            },
            {
                "img": "https://arxiv.org/html/2602.17186/fig/fig-loss-diff-scatter.png",
                "caption": "Figure 3:Visualizing the token-level VIGs.Each point shows a token’s prediction loss with (xx-axis) and without (yy-axis) visual input. The color encodes the token-level loss difference (y−xy-x).",
                "position": 367
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.17186/x4.png",
                "caption": "Figure 4:Attention fraction allocated to visual tokens.Compared to LLaVA-1.5 7B, VIG training assigns significantly more attention to visual tokens across all layers.",
                "position": 1001
            },
            {
                "img": "https://arxiv.org/html/2602.17186/x5.png",
                "caption": "Figure 5:Evaluation of text reliance under textual corruption.Base: accuracy on clean inputs. Corruption: accuracy when the same image is paired with a corrupted caption containing a conflicting description. Norm: corruption accuracy normalized by the corresponding Base (Corruption/Base).",
                "position": 1005
            },
            {
                "img": "https://arxiv.org/html/2602.17186/x6.png",
                "caption": "Figure 6:Ablation study of selection ratiop%p\\%on LLaVA-1.5 7B.We report a single metric per benchmark: LLaVAW{}^{\\text{W}}score, MMBench score,CSC_{S}for CHAIR, and Hall for MMHal.p=100p=100corresponds to the vanilla model trained on the full instruction-tuning dataset (no VIG-based selection).\nAll scores are normalized with respect to thep=100p=100setting.",
                "position": 1084
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails of Benchmarks",
        "images": []
    },
    {
        "header": "Appendix BDetails of Visual Absence Simulation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.17186/fig/fig-original.jpg",
                "caption": "(a)Ground truth image.",
                "position": 1865
            },
            {
                "img": "https://arxiv.org/html/2602.17186/fig/fig-original.jpg",
                "caption": "(a)Ground truth image.",
                "position": 1868
            },
            {
                "img": "https://arxiv.org/html/2602.17186/fig/fig-blur.jpg",
                "caption": "(b)Image-absent condition.",
                "position": 1873
            }
        ]
    },
    {
        "header": "Appendix CDetails of VIG-guided Selective Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.17186/fig/tab-response-exp1.jpg",
                "caption": "Table C.3:Examples of models’ responses from LLaVAW{}^{\\text{W}}.When applying VIG training, the model provides more visually-grounded responses for writing tasks with visual inputs than base LLaVA-1.5 and ShareGPT4V.",
                "position": 1952
            },
            {
                "img": "https://arxiv.org/html/2602.17186/fig/tab-response-exp2.jpg",
                "caption": "Table C.4:Examples of models’ responses from MMVet.When applying VIG training, the model provides more accurate responses for writing tasks with visual inputs than base LLaVA-1.5 7B.",
                "position": 2059
            }
        ]
    },
    {
        "header": "Appendix DAdditional Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.17186/x7.png",
                "caption": "(a)LLaVA-1.5 13B",
                "position": 2222
            },
            {
                "img": "https://arxiv.org/html/2602.17186/x7.png",
                "caption": "(a)LLaVA-1.5 13B",
                "position": 2225
            },
            {
                "img": "https://arxiv.org/html/2602.17186/x8.png",
                "caption": "(b)ShareGPT4V 7B",
                "position": 2230
            },
            {
                "img": "https://arxiv.org/html/2602.17186/x9.png",
                "caption": "(a)LLaVA-1.5 13B",
                "position": 2246
            },
            {
                "img": "https://arxiv.org/html/2602.17186/x9.png",
                "caption": "(a)LLaVA-1.5 13B",
                "position": 2249
            },
            {
                "img": "https://arxiv.org/html/2602.17186/x10.png",
                "caption": "(b)ShareGPT4V 7B",
                "position": 2254
            }
        ]
    },
    {
        "header": "Appendix EDetails of Ablation Study",
        "images": []
    }
]