[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14847/x1.png",
                "caption": "Figure 1:The motivation of ImagerySearch.The figure illustrates two semantic dependency scenarios related to camels.Left: The distance depicts the corresponding strength of prompt tokens during the denoising process.LDT-Benchconsists of imaginative scenarios with long-distance semantics, whose semantic dependencies are typically weak.Right: Wan2.1 performs well on short-distance semantics but fails under long-distance. Test time scaling methods (e.g.e.g., Video T1(Liu et¬†al.,2025a), Evosearch(He et¬†al.,2025a)) also struggle. However,ImagerySearchgenerates coherent, context-aware motions (orange box).",
                "position": 89
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3ImagerySearch",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14847/x2.png",
                "caption": "Figure 2:Overview of our ImagerySearch. The prompt is scored by the Constrained Semantic Scorer (producingùíü¬Øsem\\bar{\\mathcal{D}}_{\\text{sem}}) and simultaneously fed to the T2V backbone (Wan2.1). At every stepttspecified by the imagery scheduler, we sample a set of candidate clips, rank them with a reward function conditioned onùíü¬Øsem\\bar{\\mathcal{D}}_{\\text{sem}}, and retain only aùíü¬Øsem\\bar{\\mathcal{D}}_{\\text{sem}}-controlled subset. The loop repeats until generation completes.",
                "position": 160
            }
        ]
    },
    {
        "header": "4LDT-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14847/x3.png",
                "caption": "Figure 3:Overview of our LDT-Bench.Upper: (a) LDT-Bench is built by first extracting meta-information from existing recognition datasets; (b) GPT-4o is then used to generate candidate prompts, which are filtered jointly by DeepSeek and humans to obtain the final prompt set; (c) Additionally, we design a set of three MLLM-based QA tasks that serve as the creativity metric.Lower: (d) Compared with other benchmarks, LDT-Bench covers a much richer variety of categories; (e) its prompts also exhibit a semantic-distance distribution that is shifted toward substantially longer ranges. Note that ‚ÄúASD‚Äù denotes the average semantic distance of prompts.",
                "position": 245
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14847/x4.png",
                "caption": "Figure 4:Visualization of examples.Upper: Results from general models.Lower: ImagerySearch versus other test-time scaling methods. Ours produces more vivid actions under long-distance semantic prompts.",
                "position": 333
            },
            {
                "img": "https://arxiv.org/html/2510.14847/x5.png",
                "caption": "Figure 5:(a) Effect of semantic distance across different models. As semantic distance increases, our method remains the most stable. (b-e) Our AIR consistently delivers superior performance. Scaling behavior of ImagerySeach and baselines as inference-time computation increases. From left to right, theyy-axes represent the score changes forMQ\\mathrm{MQ},TA\\mathrm{TA},VQ\\mathrm{VQ}, and Overall (VideoAlign(Liu et¬†al.,2025b)). (f) Effect of reward weight.",
                "position": 539
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AThe Selection of Imagery Schedule",
        "images": []
    },
    {
        "header": "Appendix BMore Details About Imagery Evaluation Metrics",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14847/x6.png",
                "caption": "Figure S1:Imagery schedule. The heatmaps visualize 13th-layer attention projected onto the first video frame at successive denoising steps. Adjacent steps show nearly identical focus regions, whereas only a few key steps exhibit pronounced changes. Concentrating analysis and search on these pivotal steps therefore captures the prompt-to-frame semantic correspondence more efficiently.",
                "position": 1916
            },
            {
                "img": "https://arxiv.org/html/2510.14847/x7.png",
                "caption": "Figure S2:LDT-Bench prompt suite analysis: (a) Action super-category distribution shown as a horizontal bar chart. (b) Object super-category distribution displayed as a treemap, with area proportional to class count. (c) Word cloud highlighting the most frequent object-action prompts. (d) Word cloud highlighting the most frequent action-action prompts.",
                "position": 1919
            },
            {
                "img": "https://arxiv.org/html/2510.14847/x8.png",
                "caption": "Figure S3:Evaluation withImageryQA.(a)We design a structured question setImageryQA, consisting ofElementQA,AlignQA, andAnomalyQA.(b)Comparison between Wan2.1 and ImagerySearch on the same prompt. Wan2.1 fails to depict a person and the actions described, resulting in low aesthetic quality (Q4) and visual anomalies (Q5). In contrast, ImagerySearch successfully captures both actions‚Äìpolishing furnitureandpacking cleaning products‚Äìscoring higher in both Q4 and Q5.",
                "position": 1922
            },
            {
                "img": "https://arxiv.org/html/2510.14847/x9.png",
                "caption": "Figure S4:Reward-Weight Analysis. The left of figure shows anaction‚Äìactionexample and the right of figure is anobject‚Äìactionone, visualizing the videos under different weight settings.M‚ÄãQMQandV‚ÄãQVQfollow almost identical trends, whereasT‚ÄãATAmoves in the opposite direction. Accordingly, we fix theM‚ÄãQMQandV‚ÄãQVQcoefficients to 1 and vary theT‚ÄãATAcoefficient with the prompt, selecting videos that better fit imaginative scenarios.",
                "position": 1925
            }
        ]
    },
    {
        "header": "Appendix CExperimental Setup‚ÄìModel details",
        "images": []
    },
    {
        "header": "Appendix DMore Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14847/x10.png",
                "caption": "Figure S5:More examples on LDT-Bench. The images below the prompt show the result of frame sampling, where 16 frames are uniformly extracted from a 33-frame video.",
                "position": 1944
            },
            {
                "img": "https://arxiv.org/html/2510.14847/x11.png",
                "caption": "Figure S6:More examples on VBench (Part I). The images below the prompt show the result of frame sampling, where 16 frames are uniformly extracted from a 33-frame video.",
                "position": 1947
            },
            {
                "img": "https://arxiv.org/html/2510.14847/x12.png",
                "caption": "Figure S7:More examples on VBench (Part II). The images below the prompt show the result of frame sampling, where 16 frames are uniformly extracted from a 33-frame video.",
                "position": 1950
            },
            {
                "img": "https://arxiv.org/html/2510.14847/x13.png",
                "caption": "Figure S8:Error analysis about VBench scores on long-distance semantic prompts. Each box shows the score distribution for one model (mean marked by a white diamond); individual data points are overlaid in matching colors. ImagerySearch (orange) attains the highest mean with the tightest spread, while the other methods exhibit lower central tendencies and larger variances.",
                "position": 1960
            }
        ]
    },
    {
        "header": "Appendix EError Analysis",
        "images": []
    }
]