[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18577/x1.png",
                "caption": "Figure 1:Concept of theself-refining video sampling. Within the same noise level, the video latentztz_{t}is refined as the predicted endpointz^1\\hat{z}_{1}is pulled toward the data manifold.",
                "position": 116
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Preliminaries: Flow Matching in Video Diffusion Models",
        "images": []
    },
    {
        "header": "4Self-Refining Video Sampling",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18577/x2.png",
                "caption": "Figure 2:Sampling comparison on a 2D synthetic dataset.(a-b)P&P generates samples closer to the data manifold than the Euler solver.(c-d)With a fixed timestep, iterative P&P pulls the predictionz^1\\hat{z}_{1}closer to the data manifold.",
                "position": 258
            },
            {
                "img": "https://arxiv.org/html/2601.18577/x3.png",
                "caption": "",
                "position": 266
            },
            {
                "img": "https://arxiv.org/html/2601.18577/x4.png",
                "caption": "",
                "position": 273
            },
            {
                "img": "https://arxiv.org/html/2601.18577/x5.png",
                "caption": "",
                "position": 276
            },
            {
                "img": "https://arxiv.org/html/2601.18577/x6.png",
                "caption": "Figure 3:Visualization of uncertainty maps, showing higher values in motion-related regions. Maps are computed att=0.1​Tt=0.1T. Bottom row overlays the corresponding binary masks (τ=0.25\\tau=0.25) on videos generated by Wan2.2-A14B T2V(Wanget al.,2025a).",
                "position": 312
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18577/x7.png",
                "caption": "Figure 4:Qualitative comparison on challenging motion generation.",
                "position": 541
            },
            {
                "img": "https://arxiv.org/html/2601.18577/x8.png",
                "caption": "",
                "position": 547
            },
            {
                "img": "https://arxiv.org/html/2601.18577/x9.png",
                "caption": "",
                "position": 551
            },
            {
                "img": "https://arxiv.org/html/2601.18577/x10.png",
                "caption": "",
                "position": 555
            },
            {
                "img": "https://arxiv.org/html/2601.18577/x11.png",
                "caption": "(a) Full real dataset",
                "position": 753
            },
            {
                "img": "https://arxiv.org/html/2601.18577/x11.png",
                "caption": "Wan2.2-I2V",
                "position": 819
            },
            {
                "img": "https://arxiv.org/html/2601.18577/x12.png",
                "caption": "+ Ours",
                "position": 824
            },
            {
                "img": "https://arxiv.org/html/2601.18577/x13.png",
                "caption": "Figure 8:Examples ofself-refinement applied to visual reasoning tasks: (Top) graph traversal and (Bottom) maze solving fromWiedemeret al.(2025). We use Wan2.2-A14B I2V as the base model. For graph traversal, self-refinement yields a dramatic improvement in the success rate from 0.1 to 0.8. For maze solving, self-refinement does not yield meaningful gain, with success remaining near zero.",
                "position": 877
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18577/x14.png",
                "caption": "Figure 9:Ablation on uncertainty-aware strategy. Multiple P&P updates without uncertainty-aware strategy cause over-saturation. Red arrow indicates motion misaligned with the prompt.",
                "position": 948
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18577/x15.png",
                "caption": "Figure 10:Full human evaluation results onDynamic-Bench, including ties.",
                "position": 1983
            },
            {
                "img": "https://arxiv.org/html/2601.18577/x16.png",
                "caption": "Figure 11:Full human evaluation results onVideoPhy2(Bansalet al.,2025)hard subset, including ties.",
                "position": 1986
            },
            {
                "img": "https://arxiv.org/html/2601.18577/x17.png",
                "caption": "(a) Comparison of SDEdit results on an image and a video while changing the prompt fromorange cattobrown dog.",
                "position": 2056
            },
            {
                "img": "https://arxiv.org/html/2601.18577/x18.png",
                "caption": "",
                "position": 2061
            },
            {
                "img": "https://arxiv.org/html/2601.18577/x19.png",
                "caption": "",
                "position": 2064
            }
        ]
    },
    {
        "header": "Appendix BMore Discussions",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18577/x20.png",
                "caption": "Figure 14:Accumulated effect of iterative P&P at an early inference step.\nWe plot the L2 distance between the intermediate refined latentz^1(k)\\hat{z}_{1}^{(k)}and the final refined latentz^1∗\\hat{z}_{1}^{*}at a fixed inference stept=0.009​Tt=0.009T. Results are obtained using Wan2.2-A14B T2V.",
                "position": 2096
            },
            {
                "img": "https://arxiv.org/html/2601.18577/x21.png",
                "caption": "",
                "position": 2105
            },
            {
                "img": "https://arxiv.org/html/2601.18577/x22.png",
                "caption": "Figure 15:Image generation with P&P using FLUX.1-dev. With only two additional NFEs (4%), our method effectively reduces text-related artifacts, resulting in clearer and more coherent text.",
                "position": 2112
            },
            {
                "img": "https://arxiv.org/html/2601.18577/x23.png",
                "caption": "Figure 16:Ablation studies on the hyperparametersKfK_{f}andτ\\tau.",
                "position": 2117
            },
            {
                "img": "https://arxiv.org/html/2601.18577/x24.png",
                "caption": "",
                "position": 2122
            },
            {
                "img": "https://arxiv.org/html/2601.18577/x25.png",
                "caption": "Figure 18:Graph traversal task inWiedemeret al.(2025). We use Wan2.2-A14B I2V with an upsampled prompt: “Starting from the blue well, blue water begins to flow slowly through the connected channel system. The water gradually fills the nearest nodes first…”. The success rate increases from 0.1 to 0.8 with P&P method.",
                "position": 2126
            },
            {
                "img": "https://arxiv.org/html/2601.18577/x26.png",
                "caption": "",
                "position": 2131
            },
            {
                "img": "https://arxiv.org/html/2601.18577/x27.png",
                "caption": "",
                "position": 2134
            },
            {
                "img": "https://arxiv.org/html/2601.18577/x28.png",
                "caption": "Figure 21:Toy experiment on a 2D Gaussian mixture. Repeated P&P iterations (i.e.,Kf=32K_{f}\\!=\\!32) yield samples concentrated in the modes.",
                "position": 2138
            },
            {
                "img": "https://arxiv.org/html/2601.18577/x29.png",
                "caption": "Figure 22:Mode-seeking behavior induced by excessive P&P iterations in image generation. We use Wan2.2-A14B T2V with a single frame and apply P&P withKf=8,τ=0K_{f}\\!=\\!8,\\tau\\!=\\!0at steps 16–20 of the 40 step flow matching inference.",
                "position": 2142
            },
            {
                "img": "https://arxiv.org/html/2601.18577/x30.png",
                "caption": "Figure 23:P&P is also applicable to diffusion-based video generation models (e.g., CogVideoX(Yanget al.,2025b)), where it corrects video artifacts, such as a truncated lightsaber and distortions around the teddy bear’s mouth. (Image credit: MuDI(Janget al.,2024))",
                "position": 2170
            }
        ]
    },
    {
        "header": "Appendix CLimitations and Future Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18577/x31.png",
                "caption": "Figure 24:Additional visual examples of complex motion generation using Wan2.2-A14B T2V.",
                "position": 2218
            },
            {
                "img": "https://arxiv.org/html/2601.18577/x32.png",
                "caption": "",
                "position": 2222
            },
            {
                "img": "https://arxiv.org/html/2601.18577/x33.png",
                "caption": "Figure 25:Additional visual examples of physics-aligned generation using Wan2.2-A14B T2V. Our method also captures realistic physical interactions and fine-grained visual details.",
                "position": 2226
            },
            {
                "img": "https://arxiv.org/html/2601.18577/x34.png",
                "caption": "Figure 26:A screenshot of the human evaluation questionnaires used for (left) motion-enhanced video generation on Dynamic-Bench and (right) physics-aligned video generation on the VideoPhy2 hard subset.",
                "position": 2229
            },
            {
                "img": "https://arxiv.org/html/2601.18577/x35.png",
                "caption": "Figure 27:Qualitative comparison with commercial closed models, Veo 3.1(Google,2025b)and Kling 2.6(Kuaishou,2025). While the commercial models produce more aesthetic visual quality, our method demonstrates competitive performance on complex motion scenarios. Prompt: “A parkour athlete runs up a vertical wall, grabs the ledge, and muscles up to stand on the roof in one fluid motion.” and “A gymnast on a pommel horse swings their legs in wide circles (flares), supporting their entire weight on alternating hands.”",
                "position": 2232
            }
        ]
    },
    {
        "header": "Appendix DDynamic Bench",
        "images": []
    }
]