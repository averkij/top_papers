[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19418/x1.png",
                "caption": "",
                "position": 109
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19418/x2.png",
                "caption": "Figure 2:Continuous visual thinking withCoVT.CoVTintroduces compact, continuous visual tokens that encode fine-grained perceptual cues, such as object localization, spatial structure, and scene semantics, directly into VLM reasoning.\nThese tokens ground multimodal reasoning in visual space, enabling the model to capture fine-grained relationships across vision-centric tasks (e.g., counting, depth ordering, and scene understanding) without relying on external tools.\nThey can also be decoded into dense predictions, offering human-interpretable visualizations of the model’s reasoning process.",
                "position": 122
            },
            {
                "img": "https://arxiv.org/html/2511.19418/x3.png",
                "caption": "Figure 3:The training pipeline ofCoVT.CoVTfirst generates the thinking process, containing visual thinking tokens, and then leverages these visual thoughts to condition next token prediction and reason the final answer.\nTo endow these tokens with perceptual meaning, we align them with lightweight vision experts (e.g., SAM, DepthAnything, PIDINet, DINO) on their respective tasks during training.\nSpecifically: SAM uses 8 visual tokens as mask prompts; DepthAnything uses 4 tokens to reconstruct depth; PIDINet uses 4 tokens to reconstruct edges; and DINO uses 4 tokens to match patch-level features.\nThe VLM is finetuned with LoRA and all the projection layers are trainable.Note: During inference, dense predictions are decoded only when interpretability is desired; otherwise, reasoning occurs entirely in the latent visual space.",
                "position": 202
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Chain-of-Visual-Thought (CoVT)",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19418/x4.png",
                "caption": "Figure 4:Four-stage data formatting forCoVT.The first stage helps the model comprehend the visual tokens, and the second stage guides it to generate them. The third stage enables the VLM to integrate visual tokens into its reasoning process, while the final stage allows the model to efficiently select and utilize visual thinking tokens within visual thought chains.",
                "position": 350
            },
            {
                "img": "https://arxiv.org/html/2511.19418/x5.png",
                "caption": "Table 2:Comparison ofCoVTwith the baseline and closed-source models.CoVTdelivers consistent improvements across all vision-centric benchmarks and further reveals that each visual token type contributes most effectively to the tasks related to its rich information.",
                "position": 424
            },
            {
                "img": "https://arxiv.org/html/2511.19418/x5.png",
                "caption": "Figure 5:Visualization ofCoVTtokens.Different visual tokens contribute complementary cues that enable the model to solve complex perceptual reasoning tasks.Left:Segmentation tokens localize point B on the face, while the depth tokens capture the relative depth relationships.Mid:Depth visual tokens provide depth map information, and the edge tokens help highlight the positions of two boxes.Right:The Segmentation tokens identify the attended region, and the edge tokens delineate the fine-grained line structures.",
                "position": 706
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19418/x6.png",
                "caption": "Figure 6:Text-only CoTvsCoVT.CoVTsubstantially enhances VLMs’ capabilities on vision-centric tasks, whereas text-only CoT can even degrade performance.",
                "position": 908
            },
            {
                "img": "https://arxiv.org/html/2511.19418/x7.png",
                "caption": "Figure 7:Beyond the gains on vision-centric benchmarks,CoVTalso achieves slight improvements onnon–vision-centric tasks",
                "position": 1093
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "6Acknowledgment",
        "images": []
    },
    {
        "header": "Table of Contents",
        "images": []
    },
    {
        "header": "Appendix AAdditional Details ofCoVT",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19418/x8.png",
                "caption": "Figure 8:Detailed frameworks for the projection layer and segmentation token alignment.",
                "position": 1143
            },
            {
                "img": "https://arxiv.org/html/2511.19418/x9.png",
                "caption": "Figure 9:Detailed framework for the depth token alignment.",
                "position": 1172
            },
            {
                "img": "https://arxiv.org/html/2511.19418/x10.png",
                "caption": "Figure 10:Detailed framework for edge token alignment.",
                "position": 1175
            },
            {
                "img": "https://arxiv.org/html/2511.19418/x11.png",
                "caption": "Figure 11:CoVTdataset utilizes some subsets of LLaVA-OneVision, and merges the filtered TallyQA dataset and ADE20K-Depth from Aurora.",
                "position": 1424
            }
        ]
    },
    {
        "header": "Appendix BAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19418/x12.png",
                "caption": "Figure 12:Equipped with 4 depth tokens and 4 DINO tokens, the model achieves its best performance when using 8 segmentation tokens inCoVT. Allocating moreCoVTtokens leads to a slight diminishing performance and increased computational cost.",
                "position": 1653
            },
            {
                "img": "https://arxiv.org/html/2511.19418/x13.png",
                "caption": "Figure 13:Example ofCoVTcompared to the baseline Qwen2.5-VL-7B.",
                "position": 1681
            },
            {
                "img": "https://arxiv.org/html/2511.19418/x14.png",
                "caption": "Figure 14:Examples ofCoVTcompared to the baseline Qwen2.5-VL-7B.",
                "position": 1684
            },
            {
                "img": "https://arxiv.org/html/2511.19418/x15.png",
                "caption": "Figure 15:More examples ofCoVT.",
                "position": 1687
            },
            {
                "img": "https://arxiv.org/html/2511.19418/x16.png",
                "caption": "Figure 16:More examples ofCoVT.",
                "position": 1690
            },
            {
                "img": "https://arxiv.org/html/2511.19418/x17.png",
                "caption": "Figure 17:More examples ofCoVT.",
                "position": 1693
            }
        ]
    },
    {
        "header": "Appendix CLimitations and Future Work",
        "images": []
    }
]