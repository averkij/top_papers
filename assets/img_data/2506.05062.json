[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05062/x1.png",
                "caption": "Figure 1:Benchmarking data (1) and task (2). We assess the judgment capabilities and behavior of LLMs by analyzing how they ratedebate speeches- long texts that argue for or against a controversial topic.",
                "position": 152
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Speech Quality Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05062/x2.png",
                "caption": "(a)Average pair agreement",
                "position": 305
            },
            {
                "img": "https://arxiv.org/html/2506.05062/x2.png",
                "caption": "(a)Average pair agreement",
                "position": 308
            },
            {
                "img": "https://arxiv.org/html/2506.05062/x3.png",
                "caption": "(b)Agreement with average human score",
                "position": 314
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05062/x4.png",
                "caption": "Figure 3:LLMs, especially stronger ones, generally align better with human annotators when using CoT prompting.",
                "position": 350
            }
        ]
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05062/x5.png",
                "caption": "Figure 4:Strong LLM judges tend to givelowerscores than human annotators (-1 signifies parsing issues). Results for all judges are shown in AppendixF.",
                "position": 384
            },
            {
                "img": "https://arxiv.org/html/2506.05062/x6.png",
                "caption": "(a)The effect of model size on rating speeches from different sources",
                "position": 396
            },
            {
                "img": "https://arxiv.org/html/2506.05062/x6.png",
                "caption": "(a)The effect of model size on rating speeches from different sources",
                "position": 399
            },
            {
                "img": "https://arxiv.org/html/2506.05062/x7.png",
                "caption": "(b)Strong judges’ rating of speeches from different sources.",
                "position": 405
            },
            {
                "img": "https://arxiv.org/html/2506.05062/x8.png",
                "caption": "Figure 6:Strong judges rate speeches generated by GPT-4.1higherthan those by human-expert debaters.",
                "position": 431
            },
            {
                "img": "https://arxiv.org/html/2506.05062/x9.png",
                "caption": "Figure 7:Distribution of the top threeproandconkey points inLlama-3.3-70B’s chain-of-thought explanations, grouped by source.Other (pro)andOther (con)denote less frequent points. The relative share of positive key points per source reflects the judge ratings discussed in Sections5.2and5.3.",
                "position": 462
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05062/x10.png",
                "caption": "Figure 8:Speech scoring prompt. We give LLM judges a speech and its topic, and instruct them to rate it. The instructions closely follow the ones given to human annotators inSlonim et al. (2021).",
                "position": 1005
            },
            {
                "img": "https://arxiv.org/html/2506.05062/x11.png",
                "caption": "Figure 9:Chain of Thought (CoT) speech scoring prompt. We request the LLM judge to give a short justification for the given score.",
                "position": 1008
            }
        ]
    },
    {
        "header": "Appendix AAdditional details on the data",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05062/x12.png",
                "caption": "Figure 10:Speech generation prompt, requesting the model to generate a 600-word speech supporting a certain topic (e.g., \"We should abandon social media\").",
                "position": 1017
            }
        ]
    },
    {
        "header": "Appendix BJudges implementation details",
        "images": []
    },
    {
        "header": "Appendix CSpeech generation implementation",
        "images": []
    },
    {
        "header": "Appendix DParsing errors",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05062/x13.png",
                "caption": "Figure 11:Number of parse errors for different prompt variations. \"No-CoT\" refers to the prompt described by Figure8and \"CoT\" to the prompt described by Figure9. We note that parsing errors mostly occur with smaller models and could largely vary with the prompt. In general, results for the CoT prompt seem to be more challenging to parse.",
                "position": 1082
            },
            {
                "img": "https://arxiv.org/html/2506.05062/x14.png",
                "caption": "(a)Weak judges ensemble (tau-c)",
                "position": 1088
            },
            {
                "img": "https://arxiv.org/html/2506.05062/x14.png",
                "caption": "(a)Weak judges ensemble (tau-c)",
                "position": 1091
            },
            {
                "img": "https://arxiv.org/html/2506.05062/x15.png",
                "caption": "(b)Strong judges ensemble (tau-c)",
                "position": 1097
            }
        ]
    },
    {
        "header": "Appendix EJudge ensembling",
        "images": []
    },
    {
        "header": "Appendix FAdditional judges scores distributions",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05062/x16.png",
                "caption": "Figure 13:Judge scores distributions. Strong LLM judges tend to give lower scores than humans (-1 signifies parsing issues)",
                "position": 1115
            },
            {
                "img": "https://arxiv.org/html/2506.05062/x17.png",
                "caption": "Figure 14:Key point preprocessing prompt: To improve key-point analysis, we convert chain-of-thought reasoning into shorter, clearer sentences that justify the speech score.",
                "position": 1170
            },
            {
                "img": "https://arxiv.org/html/2506.05062/x18.png",
                "caption": "Figure 15:Distribution of the top threeproandconkey points inGPT-4.1-mini’s chain-of-thought explanations, grouped by source.Other (pro)andOther (con)denote less frequent points. The relative share of positive key points per source reflects the judge ratings discussed in Sections5.2and5.3",
                "position": 1173
            },
            {
                "img": "https://arxiv.org/html/2506.05062/x19.png",
                "caption": "Figure 16:Distribution of the top threeproandconkey points inQwen-32B’s chain-of-thought explanations, grouped by source.Other (pro)andOther (con)denote less frequent points. The relative share of positive key points per source reflects the judge ratings discussed in Sections5.2and5.3",
                "position": 1176
            }
        ]
    },
    {
        "header": "Appendix GKPA: Additional details",
        "images": []
    },
    {
        "header": "Appendix HSpeech examples",
        "images": []
    }
]