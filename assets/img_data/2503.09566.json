[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09566/x1.png",
                "caption": "Figure 1:Overview of our method.Our method employs progressive frame rates, which utilizes full frame rate only in the final stage as shown in (a) and (b), thereby largely optimizing computational efficiency in both training and inference shown in (c).",
                "position": 106
            }
        ]
    },
    {
        "header": "2Related works",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09566/x2.png",
                "caption": "Figure 2:Methodology. a) Pipeline of temporal pyramid video diffusion model. We divide diffusion process into multiple stages with increasing frame rate. In each stage, new frames are initially temporally interpolated from existing frames. b) Our training strategy: stage-wise diffusion. In vanilla diffusion models, the noise direction along the ODE path points toward the real data distribution. In stage-wise diffusion, the noise direction is oriented to the end point of the current stage.",
                "position": 167
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09566/x3.png",
                "caption": "Figure 3:Data-Noise Alignment.For every training sample, (a) vanilla diffusion training randomly samples noises across the entire noise distribution, resulting in stochastic ODE path during training. (b) In contrast, our method samples noises in the closest range, making the ODE path approximately deterministic during training.",
                "position": 364
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09566/x4.png",
                "caption": "Figure 4:Qualitative comparison.\nIn each pair of videos, the first row presents the results of models trained using vanilla diffusion and the second row shows the results of our method. The first two video pairs are generated by MiniFlux-vid and the remaining are generated by animatediff.",
                "position": 558
            },
            {
                "img": "https://arxiv.org/html/2503.09566/x5.png",
                "caption": "Figure 5:Convergence curve of vanilla diffusion models and our method on (a) DDIM, (b) Flow Matching. We illustrate the FVD of two methods with different GPU hours consumed. Our method achieves higher training efficiency compared to vanilla approachs.",
                "position": 691
            },
            {
                "img": "https://arxiv.org/html/2503.09566/x6.png",
                "caption": "Figure 6:Ablation study of inference strategy.Our method generates smooth, high-quality videos, whereas the baseline without inference renoising exhibits significant flickers",
                "position": 694
            },
            {
                "img": "https://arxiv.org/html/2503.09566/x7.png",
                "caption": "Figure 7:Ablation study of data-noise alignment.Our method can produce clearer videos compared to the baseline.",
                "position": 697
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09566/x8.png",
                "caption": "Figure 8:Comparison between vanilla diffusion and our method after 5000 training steps.Our method can generate temporally stable videos even at very early training steps while vanilla method cannot. The prompt is ”A serene scene of a sunflower field.”",
                "position": 777
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix.",
        "images": []
    }
]