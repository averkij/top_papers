[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.07730/x1.png",
                "caption": "Figure 1:Text-to-Image (T2I) Generation Results by MaskGen.MaskGen, powered by the proposed compact text-aware 1D tokenizer TA-TiTok, is an efficient masked generative model that achieves state-of-the-art performance on multiple T2I benchmarks using only open data. The open-data, open-weight MaskGen models are designed to promote broader access and democratize T2I masked generative models.",
                "position": 74
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.07730/x2.png",
                "caption": "Figure 2:Overview of TA-TiTok (Text-Aware Transformer-based 1-Dimensional Tokenizer).(a) TA-TiTok introduces three key enhancements to TiTok[69]: First, an efficient one-stage training procedure replaces the need for a complex two-stage pipeline. Second, TA-TiTok supports 1D tokens in both discrete (VQ) and continuous (KL) formats. Third, it incorporates textual information (using CLIP’s text encoder) during de-tokenization to improve semantic alignment with text captions.\n(b) A comparison of reconstruction results shows that TA-TiTok achieves superior reconstruction quality over TiTok.",
                "position": 148
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.07730/x3.png",
                "caption": "Figure 3:Overview of MaskGen.MaskGen is a family of text-to-image masked generative models that supports both discrete (VQ variant) and continuous (KL variant) token representations. For discrete tokens, MaskGen is trained with cross-entropy loss[11], while for continuous tokens, it employs diffusion loss[39].\nThe architecture is designed by concatenating text conditions with TA-TiTok’s latent tokens (both masked and unmasked) and feeding them into Diffusion Transformer blocks[44], with separate adaptive LayerNorm (adaLN) layers for text and image modalities, following MM-DiT[23]. Additionally, aesthetic scores are incorporated as conditioning signals via adaLN. To encode captions, MaskGen uses the CLIP text encoder[47]instead of the more resource-intensive T5-XXL[48], making it more accessible to research groups with limited computational resources.",
                "position": 241
            }
        ]
    },
    {
        "header": "5Experimental Results",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "AMore Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.07730/x4.png",
                "caption": "Figure 5:Re-captioning Results.Augmented captions, generated by Molmo[16], offer richer details and improved alignment with image content.",
                "position": 1178
            }
        ]
    },
    {
        "header": "BAblation on Text Guidance Place in TA-TiTok",
        "images": []
    },
    {
        "header": "CComparisons Between MaskGen Using Discrete and Continuous Tokens",
        "images": []
    },
    {
        "header": "DAblation Studies for MaskGen",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.07730/extracted/6129647/fig/aes_varying.png",
                "caption": "Figure 6:Generated Images with Varying Aesthetic Score Conditioning.Conditioning on higher aesthetic scores produces generated images with enhanced fine-grained details.",
                "position": 1526
            },
            {
                "img": "https://arxiv.org/html/2501.07730/x5.png",
                "caption": "Figure 7:Generated Images by MaskGen with Different Tokenizer Types.For each caption, the top row displays images generated using continuous tokens (KL), while the bottom row shows images generated using discrete tokens (VQ). Long prompts are truncated for brevity.",
                "position": 1743
            }
        ]
    },
    {
        "header": "EZero-Shot Text-to-Image Generation Results on COCO",
        "images": []
    },
    {
        "header": "FKL variant of TiTok on ImageNet",
        "images": []
    },
    {
        "header": "GQualitative Examples of MaskGen",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.07730/x6.png",
                "caption": "Figure 8:Qualitative examples of Text-to-Image (T2I) Generation with MaskGen.MaskGen, equipped with the efficient and compact text-aware 1D tokenizer TA-TiTok, generates high-fidelity and diverse images.",
                "position": 1770
            },
            {
                "img": "https://arxiv.org/html/2501.07730/x7.png",
                "caption": "Figure 9:Qualitative examples of Text-to-Image (T2I) Generation with MaskGen.MaskGen, equipped with the efficient and compact text-aware 1D tokenizer TA-TiTok, generates high-fidelity and diverse images.",
                "position": 1776
            },
            {
                "img": "https://arxiv.org/html/2501.07730/x8.png",
                "caption": "Figure 10:Qualitative examples of Text-to-Image (T2I) Generation with MaskGen.MaskGen, equipped with the efficient and compact text-aware 1D tokenizer TA-TiTok, generates high-fidelity and diverse images.",
                "position": 1782
            },
            {
                "img": "https://arxiv.org/html/2501.07730/x9.png",
                "caption": "Figure 11:Qualitative examples of Text-to-Image (T2I) Generation with MaskGen.MaskGen, equipped with the efficient and compact text-aware 1D tokenizer TA-TiTok, generates high-fidelity and diverse images.",
                "position": 1788
            },
            {
                "img": "https://arxiv.org/html/2501.07730/x10.png",
                "caption": "Figure 12:Qualitative examples of Text-to-Image (T2I) Generation with MaskGen.MaskGen, equipped with the efficient and compact text-aware 1D tokenizer TA-TiTok, generates high-fidelity and diverse images.",
                "position": 1794
            },
            {
                "img": "https://arxiv.org/html/2501.07730/x11.png",
                "caption": "Figure 13:Qualitative examples of Text-to-Image (T2I) Generation with MaskGen.MaskGen, equipped with the efficient and compact text-aware 1D tokenizer TA-TiTok, generates high-fidelity and diverse images.",
                "position": 1800
            },
            {
                "img": "https://arxiv.org/html/2501.07730/x12.png",
                "caption": "Figure 14:Qualitative examples of Text-to-Image (T2I) Generation with MaskGen.MaskGen, equipped with the efficient and compact text-aware 1D tokenizer TA-TiTok, generates high-fidelity and diverse images.",
                "position": 1806
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]