[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.14254/x1.png",
                "caption": "Figure 1:Instruction tuning trains a language model on responses conditioned on instructions.\nWe find that (1) response tuning (estimating the probability of responses with no instructions), (2) single-task finetuning (e.g., code or poetry generation), and even (3) a simple rule-based adapter all yield language models with general instruction-following behavior.",
                "position": 290
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Experiment Setting",
        "images": []
    },
    {
        "header": "4Response Tuning Yields Instruction Following",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.14254/x2.png",
                "caption": "Figure 2:Responses from response tuning, instruction tuning, and the base Llama-2-7B model.",
                "position": 481
            }
        ]
    },
    {
        "header": "5Single-Task Finetuning Yields Instruction Following",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.14254/x3.png",
                "caption": "Figure 3:Examples from each of the five single-task finetuning datasets. At the left of each dataset is the input that is conditioned on, and at the right is the output that is learned.",
                "position": 614
            },
            {
                "img": "https://arxiv.org/html/2409.14254/x4.png",
                "caption": "Figure 4:Responses generated by single-task finetuned models for each of our five datasets. MBPP trains only on python snippet generation, GSM on math word problems, Poetry on poetry generation, Recipe on recipe generation, and Chess on chess game generation. Yet, except for Chess, the responses deviate from the single-task behavior towards reasonable responses.",
                "position": 688
            },
            {
                "img": "https://arxiv.org/html/2409.14254/x5.png",
                "caption": "Figure 5:For a GSM-finetuned model, the similarity between a test-time instruction to the instructions in the GSM dataset (x-axis) plotted against the similarity between the model’s generated response to GSM responses (minus the similarity of that response to LIMA broad responses). On the left, an example of an average-similarity instruction; note that the response is unlike GSM formatting, except for the telltale####1, which is how GSM formats its final answer. On the right, a very high-similarity instruction leads to GSM-like behavior.",
                "position": 709
            }
        ]
    },
    {
        "header": "6A 3-Rule Adapter for Instruction Following.",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.14254/x6.png",
                "caption": "Figure 6:Responses from our rule-based language model product.",
                "position": 850
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AFinetuning and hyperparameter details",
        "images": []
    },
    {
        "header": "Appendix BFormatting Tags Ablation",
        "images": []
    },
    {
        "header": "Appendix CInstruction Rephrasing Removal Ablation",
        "images": []
    },
    {
        "header": "Appendix DDetails on our Validation Set",
        "images": []
    },
    {
        "header": "Appendix ERule-Based Adapter Details",
        "images": []
    }
]