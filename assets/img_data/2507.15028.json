[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15028/x1.png",
                "caption": "",
                "position": 384
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15028/extracted/6638184/figures/brain.png",
                "caption": "Figure 2:Dataset Comparisons.Left:We present Video thinking Test (Video-TT) for the following features:ensure the questions are complex.addresses the issue of selecting frames from the video.provided rationale for each answer.Middle:InVideo-TT, the top-performing model reaches only half of human performance.Right:The lower performance of GPT-4o in the VideoMME-Long track may be due to the selection of sparse frames rather than a genuine gap in understanding between humans and models.",
                "position": 396
            },
            {
                "img": "https://arxiv.org/html/2507.15028/extracted/6638184/figures/film_frames.png",
                "caption": "",
                "position": 397
            },
            {
                "img": "https://arxiv.org/html/2507.15028/extracted/6638184/figures/memo.png",
                "caption": "",
                "position": 397
            },
            {
                "img": "https://arxiv.org/html/2507.15028/extracted/6638184/figures/annotator.png",
                "caption": "",
                "position": 548
            },
            {
                "img": "https://arxiv.org/html/2507.15028/extracted/6638184/figures/gpt.png",
                "caption": "",
                "position": 548
            },
            {
                "img": "https://arxiv.org/html/2507.15028/x2.png",
                "caption": "",
                "position": 559
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15028/x3.png",
                "caption": "Figure 3:Benchmark Curation Pipeline.Our annotation pipeline ensures that each question: (1) is complex enough to differentiate between human and model video understanding capabilities; (2) can be understood with a limited number of sampled frames; (3) also assesses the modelsâ€™ robustness against natural adversarial conditions.",
                "position": 606
            }
        ]
    },
    {
        "header": "3Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15028/x4.png",
                "caption": "Figure 4:Eight Complex Factors in Our Datasets.Video links of each case:Q-1,Q-2,Q-3,Q-4,Q-5,Q-6,Q-7,Q-8",
                "position": 624
            },
            {
                "img": "https://arxiv.org/html/2507.15028/x5.png",
                "caption": "Figure 5:VQA Question Prototypes.We present our five question prototypes.Manhighlights the man framed by a bounding box.",
                "position": 662
            },
            {
                "img": "https://arxiv.org/html/2507.15028/x6.png",
                "caption": "Figure 6:Benchmark Statistics.Left:We show our 18 question by category alongside the number of questions.Right:The histogram of the video length.",
                "position": 700
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5How far is Video LMM from Humans?",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15028/x7.png",
                "caption": "Figure 7:Error cases in typical question types.We markrationaleanswers with a grey background. Video links of each case :Q-1,Q-2,Q-3,Q-4.",
                "position": 1252
            },
            {
                "img": "https://arxiv.org/html/2507.15028/x8.png",
                "caption": "Figure 8:Human-conducted analysis of errors by question type.",
                "position": 1260
            },
            {
                "img": "https://arxiv.org/html/2507.15028/x9.png",
                "caption": "Figure 9:(a-b) Comparison of human and average model performance based on correctness and robustness across question types. (c) Comparison of human performance and that of two models across different numbers of frames. (d) Relative performance change (%) when adding Chain-of-Thought (CoT) reasoning and audio transcript information.",
                "position": 1264
            }
        ]
    },
    {
        "header": "6Further Analysis",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "1Annotation Detail",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15028/x10.png",
                "caption": "Figure 10:The data annoation flow of Video Turing Test. Q stands for Question; A stands for Answer; R stands for Rationale.",
                "position": 2006
            }
        ]
    },
    {
        "header": "2Mathematical Definition of the Robustness Score",
        "images": []
    },
    {
        "header": "3Prompt for Evaluating Open-ended Answer",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15028/x11.png",
                "caption": "Figure 11:Error cases in typical question types.We markrationaleanswers with a grey background. Video links of each case :Q-1Q-2,Q-3,Q-4,Q-5,Q-6 & Q-7.",
                "position": 2218
            }
        ]
    },
    {
        "header": "4Error Analysis",
        "images": []
    }
]