[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17282/x1.png",
                "caption": "Figure 1:Cultural alignment in local languages.(a) LLMs/recommenders keep cultural consistency, but T2I models falter with “noun-only” prompts. (b) Adding a “culture-style modifier + noun” restores consistency.",
                "position": 97
            },
            {
                "img": "https://arxiv.org/html/2511.17282/x2.png",
                "caption": "Figure 2:Overview of the CultureBench pipeline.First, manually collect and rigorously quality-control datasets from 15 linguistic regions; annotate “culture-style modifier noun” captions using GPT5-Nano[chatgpt5]and, through human annotation, “noun-only” captions; convert annotated content into local languages via translation tools, supplemented by manual review.",
                "position": 100
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17282/x3.png",
                "caption": "Figure 3:Data distribution of the proposed CultureBench dataset across 15 languages.The dataset is divided into train, test, and neuron-detection subsets with a ratio of 7:2:1.",
                "position": 147
            }
        ]
    },
    {
        "header": "3CultureBench Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17282/x4.png",
                "caption": "Figure 4:Verify the hypothesis.Within the CultureBench test subset, performances under “culture-style modifier + noun” and “noun-only” prompt conditions are compared. Quantitative evaluation is conducted using CultureVQA.",
                "position": 165
            }
        ]
    },
    {
        "header": "4Cultural Probing",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17282/x5.png",
                "caption": "Figure 5:Methods for Neuronal Detection.(a) By comparing attention allocation between cultural-style modifiers and nouns across text-encoder layers, the layer with the largest divergence is designated as the culturally sensitive layer. (b) At this layer, features from the “culture-style modifier + noun” and “noun-only” prompts are fed into an SAE[cunningham2023sparse]to obtain sparse features, revealing neurons with heightened sensitivity to cultural cues.",
                "position": 180
            },
            {
                "img": "https://arxiv.org/html/2511.17282/x6.png",
                "caption": "Figure 6:PEA-Diffusion cultural sensitivity.Δ\\DeltaCA peaks layer 16. AltDiffusion results are provided in the appendix.",
                "position": 211
            },
            {
                "img": "https://arxiv.org/html/2511.17282/x7.png",
                "caption": "Figure 7:Neuronal detection result. The weighted frequency scores show only a few salient peaks per culture, indicating culture-specific neurons. We define the Top-KKset as the peak neurons, withKKadapting to the number of salient peaks.",
                "position": 255
            }
        ]
    },
    {
        "header": "5Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17282/x8.png",
                "caption": "Figure 8:Qualitative comparison of generation results.Our approach generates images that are more culturally appropriate.",
                "position": 500
            }
        ]
    },
    {
        "header": "6Experiments And Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17282/x9.png",
                "caption": "Figure 9:User Study.Evaluated using MCC, SCC, and CSR metrics, where higher scores indicate greater perceived realism and user preference.",
                "position": 530
            },
            {
                "img": "https://arxiv.org/html/2511.17282/x10.png",
                "caption": "Figure 10:Hyperparameter results.The effect of cultural enrichment varies under differentλ\\lambdavalues.",
                "position": 533
            },
            {
                "img": "https://arxiv.org/html/2511.17282/x11.png",
                "caption": "Figure 11:Hyperparameter results.Performance variations of CultureVQA under differentλ\\lambdavalues.",
                "position": 548
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17282/x12.png",
                "caption": "",
                "position": 627
            }
        ]
    },
    {
        "header": "Appendix ADetails of CultureBench Dataset",
        "images": []
    },
    {
        "header": "Appendix BFurther Evidence on the Culture Gap",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17282/x13.png",
                "caption": "Figure 13:Further evidence on the culture gap.For a fixed English prompt and identical sampling settings, activating different culture-neuron sets steers both PEA-Diffusion (top) and AltDiffusion (bottom) toward distinct cultural styles.",
                "position": 662
            }
        ]
    },
    {
        "header": "Appendix CMore Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17282/x14.png",
                "caption": "Figure 14:AltDiffusion cultural sensitivity.Δ\\DeltaCA peaks layer 14. Therefore, layer 14 is culturally sensitive.",
                "position": 703
            },
            {
                "img": "https://arxiv.org/html/2511.17282/x15.png",
                "caption": "Figure 15:AltDiffusion neuronal detection result.The weighted frequency scores show only a few salient peaks per culture, indicating culture-specific neurons.",
                "position": 706
            },
            {
                "img": "https://arxiv.org/html/2511.17282/x16.png",
                "caption": "Figure 16:Selection of ThresholdKK.The red dashed vertical line indicates the chosen thresholdKK. Neurons to the left of the line correspond to the selected Top-KKculture-sensitive neurons, while the scores to the right fall below the cutoff and are discarded.",
                "position": 749
            },
            {
                "img": "https://arxiv.org/html/2511.17282/x17.png",
                "caption": "Figure 17:Cross-domain qualitative experiments.Our approach generates images that are more culturally appropriate.",
                "position": 826
            },
            {
                "img": "https://arxiv.org/html/2511.17282/x18.png",
                "caption": "Figure 18:Example interface of the user study comprising three task types.MCC (top), SCC (bottom-left), and CSR (bottom-right).",
                "position": 835
            },
            {
                "img": "https://arxiv.org/html/2511.17282/x19.png",
                "caption": "Figure 19:More Results.Further examples of results generated by AltDiffusion and PEA-Diffusion under different methods.",
                "position": 838
            }
        ]
    },
    {
        "header": "Appendix DDetail of User Study",
        "images": []
    },
    {
        "header": "Appendix EMore Results",
        "images": []
    },
    {
        "header": "Appendix FMore Discussion",
        "images": []
    },
    {
        "header": "Appendix GLimitation",
        "images": []
    },
    {
        "header": "Appendix HEthics Statement",
        "images": []
    }
]