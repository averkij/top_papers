[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02361/x1.png",
                "caption": "Figure 1:A comparison of the number of instances in real-world SWE instances. Our multilingual SWE-Universe is significantly larger than other recent multilingual efforts like MiMo-V2-Flash(Xiao et al.,2026), DeepSeek-V3.2(DeepSeek-AI,2025), and Multi-SWE-RL(Zan et al.,2025), as well as prominent Python-only datasets including SWE-rebench(Badertdinov et al.,2025), SWE-Gym(Pan et al.,2024), CWM(Copet et al.,2025), and SWE-Bench(Jimenez et al.,2024).",
                "position": 112
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Methodology: Scalability and Reliability",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02361/figures/env-build.png",
                "caption": "Figure 2:Our SWE-Universe framework for scalable and reliable environment building. The pipeline is built around abuilding agentthat proposes a verifier (evaluation.sh). Two key components ensure quality and yield: anin-loop Hacking Detectorthat preemptively rejects superficial scripts, and anIterative Validationloop where the agent self-corrects based on feedback from testing its verifier against both buggy and resolved code states.",
                "position": 160
            },
            {
                "img": "https://arxiv.org/html/2602.02361/x2.png",
                "caption": "Figure 3:Three types ofevaluation.sh. We only accept the first two types of evaluation scripts.",
                "position": 219
            },
            {
                "img": "https://arxiv.org/html/2602.02361/x3.png",
                "caption": "Figure 4:Task Quality vs. Dataset Size (Log-Scale). Task quality is measured as the fraction of high-quality samples.",
                "position": 240
            }
        ]
    },
    {
        "header": "3Efficient Building and Benchmarking",
        "images": []
    },
    {
        "header": "4Scaling Environments to Millions",
        "images": []
    },
    {
        "header": "5Evaluation: Large-scale Agentic Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02361/x4.png",
                "caption": "(a)",
                "position": 557
            },
            {
                "img": "https://arxiv.org/html/2602.02361/x4.png",
                "caption": "(a)",
                "position": 560
            },
            {
                "img": "https://arxiv.org/html/2602.02361/x5.png",
                "caption": "(b)",
                "position": 565
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]