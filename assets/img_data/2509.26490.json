[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26490/x1.png",
                "caption": "Figure 1:Overall performances on VitaBench, sorted by main results.",
                "position": 114
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26490/x2.png",
                "caption": "Figure 2:VitaBench sources tasks from real-world environments by composing interconnected tools, diverse user requests, and structured databases. Agents interact with users through multi-turn dialogue, while a rubric-based sliding-window evaluator tracks progress across the trajectory.",
                "position": 121
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3VitaBench: A Benchmark for Versatile Interactive Tasks",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26490/x3.png",
                "caption": "Figure 3:Overview of the VitaBench construction pipeline and a simplified cross-scenario example.",
                "position": 496
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Models under Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26490/x4.png",
                "caption": "Figure 4:Pass@kkvs. Passˆkkperformance.",
                "position": 1204
            },
            {
                "img": "https://arxiv.org/html/2509.26490/x4.png",
                "caption": "Figure 4:Pass@kkvs. Passˆkkperformance.",
                "position": 1207
            },
            {
                "img": "https://arxiv.org/html/2509.26490/x5.png",
                "caption": "Figure 5:Model performance vs. Turns.",
                "position": 1212
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26490/x6.png",
                "caption": "Figure 6:User simulator reliability evaluation.",
                "position": 1245
            },
            {
                "img": "https://arxiv.org/html/2509.26490/x6.png",
                "caption": "Figure 6:User simulator reliability evaluation.",
                "position": 1248
            },
            {
                "img": "https://arxiv.org/html/2509.26490/x7.png",
                "caption": "Figure 7:MSE stability across different evaluation run counts.",
                "position": 1253
            },
            {
                "img": "https://arxiv.org/html/2509.26490/x8.png",
                "caption": "Figure 8:Ablation study of user simulation configurations.",
                "position": 1423
            },
            {
                "img": "https://arxiv.org/html/2509.26490/x8.png",
                "caption": "Figure 8:Ablation study of user simulation configurations.",
                "position": 1426
            },
            {
                "img": "https://arxiv.org/html/2509.26490/x9.png",
                "caption": "Figure 9:Error distribution of VitaBench.",
                "position": 1431
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Contributions",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AComparison Traits Details",
        "images": []
    },
    {
        "header": "Appendix BPrompt Templates",
        "images": []
    },
    {
        "header": "Appendix CAn Example Trajectory",
        "images": []
    }
]