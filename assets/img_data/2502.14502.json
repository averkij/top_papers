[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14502/x1.png",
                "caption": "Figure 1:Decrease in quality with increase of new facts learned by the model: results of the fine-tunedLlama-3.1-8B-Instructon TruthfulQA (solid line corresponds to the mean score, error margin â€“ to the min/max scores of three runs with different random seeds).",
                "position": 130
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Study Design",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14502/x2.png",
                "caption": "Figure 2:Dynamics of the reliability scoreduring training on 500 (left) and 3,000 (right)Unknownitems along with paraphrases andHighlyKnownfacts. Error bar is min-max for 3 seed run.",
                "position": 344
            },
            {
                "img": "https://arxiv.org/html/2502.14502/x3.png",
                "caption": "",
                "position": 353
            }
        ]
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14502/x4.png",
                "caption": "Figure 3:MMLU:Accuracy dependent on the amount of Unknown learned. Pointed horizontal line indicates the baseline. Models trained with less additional data tend to disrupt reasoning less.",
                "position": 666
            },
            {
                "img": "https://arxiv.org/html/2502.14502/x5.png",
                "caption": "Figure 4:TruthfulQA:MC1 and MC2 accuracy metrics dependent on the amount ofUnknownlearned. Horizontal dotted lines indicate the baselines. Models trained with paraphrases tend to disrupt truthfulness less. Error bar is min-max for 3 seed run.",
                "position": 669
            },
            {
                "img": "https://arxiv.org/html/2502.14502/x6.png",
                "caption": "",
                "position": 678
            },
            {
                "img": "https://arxiv.org/html/2502.14502/x7.png",
                "caption": "Figure 5:Difference in quality drop for the two models: Llama 3.1 8B and Mistral 7B v0.3.",
                "position": 1158
            }
        ]
    },
    {
        "header": "6Additional Results for Mistral",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethical Considerations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMMLU Performance",
        "images": []
    },
    {
        "header": "Appendix BFew-shot Prompts",
        "images": []
    },
    {
        "header": "Appendix CSamples of Knowledge Categories",
        "images": []
    },
    {
        "header": "Appendix DExamples of Positive and Negative Shifts",
        "images": []
    },
    {
        "header": "Appendix EARC and LogiQA Benchmarks",
        "images": []
    }
]