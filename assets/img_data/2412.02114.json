[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.02114/x1.png",
                "caption": "",
                "position": 110
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.02114/x2.png",
                "caption": "Figure 2:OmniCreator samples for text-to-video generation.OmniCreator not only enables universal video editing but also generates high-quality videos from text prompts.",
                "position": 130
            },
            {
                "img": "https://arxiv.org/html/2412.02114/x3.png",
                "caption": "Figure 3:Overview ofOmniCreator.During training, the original video also serves as a condition of the denoising process. To enable temporal information understanding, we incorporate an adapter. Additionally, we utilize a query transformer to effectively align video and text embeddings, which aids in the denoising process. For computational efficiency, LoRAs are integrated into the denoising U-Net. During inference, OmniCreator enables universal video editing by adopting a reference video alongside an editing text prompt.",
                "position": 141
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.02114/x4.png",
                "caption": "Figure 4:Illustration of the alignment between video and text embeddings. We utilize Euclidean Distance (left, blue) and Cosine Similarity (right, red) to evaluate the impact of the Adapter (Ada.) and Query Transformer (Query) on the embedding alignment.",
                "position": 228
            },
            {
                "img": "https://arxiv.org/html/2412.02114/x5.png",
                "caption": "Figure 5:Illustration of semantic correspondence. Top: Reference video and its original caption. Middle: Results using only one condition. Bottom: Effects of full sentencevs.delta prompt.",
                "position": 257
            },
            {
                "img": "https://arxiv.org/html/2412.02114/x6.png",
                "caption": "Figure 6:Statistics of OmniBench-99.",
                "position": 306
            },
            {
                "img": "https://arxiv.org/html/2412.02114/x7.png",
                "caption": "Figure 7:Video editing comparison with baselines.We follow the baseline‚Äôs prompt setting but only show delta prompts here. Due to space constraints, we only compare editing scenarios with TokenFlow and CCEdit, for complete comparisons, please refer to App.G.2.",
                "position": 366
            }
        ]
    },
    {
        "header": "4OmniBench-99",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.02114/x8.png",
                "caption": "Figure 8:Image editing comparison with baselines.",
                "position": 384
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.02114/x9.png",
                "caption": "Figure 9:Qualitative results of OmniCreator T2V samples.",
                "position": 430
            },
            {
                "img": "https://arxiv.org/html/2412.02114/x10.png",
                "caption": "Figure 10:Qualitative results of OmniCreator T2I samples.",
                "position": 435
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AEditing Capabilities Overview",
        "images": []
    },
    {
        "header": "Appendix BAblation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.02114/x11.png",
                "caption": "Figure 11:Ablation on video condition modeling.Ada. indicates the adapter, and Query is the query transformer.",
                "position": 4176
            },
            {
                "img": "https://arxiv.org/html/2412.02114/x12.png",
                "caption": "Figure 12:Ablation on LoRA ranks.LoRA with different ranks exhibits different learning comprehension abilities",
                "position": 4188
            }
        ]
    },
    {
        "header": "Appendix CImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.02114/x13.png",
                "caption": "Figure 13:Ablation on multimodal guidance scales.wtxtsubscriptùë§txtw_{\\mathrm{txt}}italic_w start_POSTSUBSCRIPT roman_txt end_POSTSUBSCRIPTcontrols consistency with the edit instruction, whilewvidsubscriptùë§vidw_{\\mathrm{vid}}italic_w start_POSTSUBSCRIPT roman_vid end_POSTSUBSCRIPTcontrols the similarity with reference video.",
                "position": 4213
            }
        ]
    },
    {
        "header": "Appendix DOmniBench-99",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.02114/x14.png",
                "caption": "Figure 14:Text Prompt Demonstration.",
                "position": 4244
            }
        ]
    },
    {
        "header": "Appendix EAdditional Evaluation Details",
        "images": []
    },
    {
        "header": "Appendix FMore Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.02114/x15.png",
                "caption": "Figure 15:Image editing comparison.",
                "position": 4594
            },
            {
                "img": "https://arxiv.org/html/2412.02114/x16.png",
                "caption": "Figure 16:Examples of OmniBench-99 benchmark. The delta prompts are highlighted in yellow-brown.",
                "position": 4662
            },
            {
                "img": "https://arxiv.org/html/2412.02114/x17.png",
                "caption": "Figure 17:Demonstration of our user study interface. Here we demonstrate one complete sample.",
                "position": 4666
            },
            {
                "img": "https://arxiv.org/html/2412.02114/x18.png",
                "caption": "Figure 18:Gallery of OmniCreator‚Äôs text-to-image generation results.",
                "position": 4670
            },
            {
                "img": "https://arxiv.org/html/2412.02114/x19.png",
                "caption": "Figure 19:Gallery of OmniCreator‚Äôs text-to-video generation results.",
                "position": 4674
            },
            {
                "img": "https://arxiv.org/html/2412.02114/x20.png",
                "caption": "Figure 20:Gallery of OmniCreator‚Äôs text-to-video generation results.",
                "position": 4678
            },
            {
                "img": "https://arxiv.org/html/2412.02114/x21.png",
                "caption": "Figure 21:Video editing comparison:Editing-Type-Foreground.",
                "position": 4682
            },
            {
                "img": "https://arxiv.org/html/2412.02114/x22.png",
                "caption": "Figure 22:Video editing comparison:Editing-Type-Background.",
                "position": 4686
            },
            {
                "img": "https://arxiv.org/html/2412.02114/x23.png",
                "caption": "Figure 23:Video editing comparison:Editing-Type-Style.",
                "position": 4690
            },
            {
                "img": "https://arxiv.org/html/2412.02114/x24.png",
                "caption": "Figure 24:Video editing comparison:Editing-Type-Composite.",
                "position": 4694
            },
            {
                "img": "https://arxiv.org/html/2412.02114/x25.png",
                "caption": "Figure 25:Video editing comparison:Editing-Scenario-Environment-Weather.",
                "position": 4698
            },
            {
                "img": "https://arxiv.org/html/2412.02114/x26.png",
                "caption": "Figure 26:Video editing comparison:Editing-Scenario-Environment-Time.",
                "position": 4702
            },
            {
                "img": "https://arxiv.org/html/2412.02114/x27.png",
                "caption": "Figure 27:Video editing comparison:Editing-Scenario-Environment-Background.",
                "position": 4706
            },
            {
                "img": "https://arxiv.org/html/2412.02114/x28.png",
                "caption": "Figure 28:Video editing comparison:Editing-Scenario-Object-Addition.",
                "position": 4710
            },
            {
                "img": "https://arxiv.org/html/2412.02114/x29.png",
                "caption": "Figure 29:Video editing comparison:Editing-Scenario-Object-Removal.",
                "position": 4714
            },
            {
                "img": "https://arxiv.org/html/2412.02114/x30.png",
                "caption": "Figure 30:Video editing comparison:Editing-Scenario-Object-Replacement.",
                "position": 4718
            },
            {
                "img": "https://arxiv.org/html/2412.02114/x31.png",
                "caption": "Figure 31:Video editing comparison:Editing-Scenario-Human-Appearance.",
                "position": 4722
            },
            {
                "img": "https://arxiv.org/html/2412.02114/x32.png",
                "caption": "Figure 32:Video editing comparison:Editing-Scenario-Human-Motion.",
                "position": 4726
            }
        ]
    },
    {
        "header": "Appendix GMore Results",
        "images": []
    }
]