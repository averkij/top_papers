[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15454/x1.png",
                "caption": "Figure 1:In the open-world setting, ObjectGS enables 3D object awareness during reconstruction, allowing it to achieve high-quality scene reconstruction and understanding simultaneously.",
                "position": 98
            },
            {
                "img": "https://arxiv.org/html/2507.15454/x2.png",
                "caption": "Figure 2:(a) Considering semantic information during reconstruction can help better model objects.\n(b) Existing semantic modeling methods often lead to semantic ambiguity during alpha blending, whereas our classification-based semantic modeling eliminates this problem by independently accumulating the semantics of different objects through ID encoding.",
                "position": 148
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15454/x3.png",
                "caption": "Figure 3:The overall architecture of ObjectGS. We first use a 2D segmentation pipeline to assign object ID and lift it to 3D. Then we initialize the anchors and use them to generate object-aware neural Gaussians. To provide semantic guidance, we model the Gaussian semantics and construct classification-based constraints. As a result, our method enables both object-level and scene-level reconstruction.",
                "position": 187
            },
            {
                "img": "https://arxiv.org/html/2507.15454/x4.png",
                "caption": "Figure 4:Since 2D segmentation method[37]donâ€™t account for occluded object, it cannot be used to supervise the independent rendering of objects. In contrast, our ObjectGS render semantics in the scene level, which is occlusion-aware.",
                "position": 270
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15454/x5.png",
                "caption": "Figure 5:Qualitative comparison of open-vocabulary segmentation and 3D object queries. The red box highlights that our method can achieve multi-view consistent instance segmentation. In 3D object queries, our method has more accurate object segmentation boundaries.",
                "position": 372
            },
            {
                "img": "https://arxiv.org/html/2507.15454/x6.png",
                "caption": "",
                "position": 377
            },
            {
                "img": "https://arxiv.org/html/2507.15454/extracted/6639756/figs/id_encoding.png",
                "caption": "Figure 7:Qualitative comparison of different semantic modeling methods on 3D object query. Learnable Gaussian semantics leads to fuzzy positioning at the object boundary, and the constraint of object independence leads to ineffective object query under occlusion. In contrast, our proposed one-hot ID encoding overcomes both problems and achieves accurate 3D object query.",
                "position": 381
            },
            {
                "img": "https://arxiv.org/html/2507.15454/x7.png",
                "caption": "Figure 8:Ablation on different point cloud label initializations. The majority voting strategy is more robust in the foreground regions, while the probability-based and correspondence-based voting strategies show greater robustness in the background regions.",
                "position": 785
            }
        ]
    },
    {
        "header": "5Limitation",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Training Overhead",
        "images": []
    },
    {
        "header": "8Voting Algorithm",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15454/x8.png",
                "caption": "Figure 9:Qualitative comparison of open vocabulary segmentation and 3D object query on the 3DOVS dataset.",
                "position": 2052
            },
            {
                "img": "https://arxiv.org/html/2507.15454/x9.png",
                "caption": "Figure 10:Qualitative comparison of open vocabulary segmentation and 3D object query on the LERF-Mask dataset.",
                "position": 2055
            },
            {
                "img": "https://arxiv.org/html/2507.15454/x10.png",
                "caption": "Figure 11:Qualitative comparison of 3D panoptic segmentation on the Replica dataset.",
                "position": 2058
            },
            {
                "img": "https://arxiv.org/html/2507.15454/x11.png",
                "caption": "Figure 12:Qualitative comparison of 3D panoptic segmentation on the Scannet++ dataset.",
                "position": 2061
            },
            {
                "img": "https://arxiv.org/html/2507.15454/x12.png",
                "caption": "Figure 13:Qualitative comparison of 2D panoptic segmentation on the Replica dataset.",
                "position": 2064
            },
            {
                "img": "https://arxiv.org/html/2507.15454/x13.png",
                "caption": "Figure 14:Qualitative comparison of 2D panoptic segmentation on the Scannet++ dataset.",
                "position": 2067
            }
        ]
    },
    {
        "header": "9More Visualization",
        "images": []
    }
]