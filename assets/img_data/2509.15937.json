[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15937/figures/teaser.png",
                "caption": "Figure 1:Overview. Pretrained on multi-source data, VLAC provides dense progress rewards for real-world RL while also serving as a policy to output actions, integrating into real-world RL loops to enable self-improvement in manipulation.",
                "position": 162
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3VLAC model and Real-World RL Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15937/x1.png",
                "caption": "Figure 2:The VLAC model is trained on a combination of comprehensive public robotic manipulation datasets, human demonstration data, self-collected manipulation data, and various image understanding datasets. Video data is processed into pair-wise samples to learn the different task progress between two frames, supplemented with task descriptions and task completion evaluation to enable task progress understanding and action generation, as illustrated in the bottom-left corner. As shown in the diagram on the right, the model demonstrates strong generalization capabilities to new robots, scenarios, and tasks not covered in the training dataset. It can predict task progress and distinguish failure action or trajectory, providing dense reward feedback for real-world RL and offering guidance for data refinement. Additionally, the model can directly perform manipulation tasks, exhibiting zero-shot capabilities to handle different scenarios.",
                "position": 209
            },
            {
                "img": "https://arxiv.org/html/2509.15937/figures/vlac_ar.png",
                "caption": "Figure 3:VLAC forward pass generates structured action tokens, reward tokens, and a value head is attached to estimate state value for PPO updates.",
                "position": 392
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15937/figures/VOC-F1_performance_comparison_log_scaled.png",
                "caption": "Figure 4:VOC-F1 Performance Comparison Across Different Models and Datasets.",
                "position": 499
            },
            {
                "img": "https://arxiv.org/html/2509.15937/figures/case_study.png",
                "caption": "Figure 5:Example results of VLAC for task progress understanding.",
                "position": 746
            },
            {
                "img": "https://arxiv.org/html/2509.15937/x2.png",
                "caption": "Figure 6:Experimental results of real-world RL on four tasks. This figure demonstrates the success rate for all the methods during the training procedure. Each data point is obtained via running 10 trails.",
                "position": 1001
            },
            {
                "img": "https://arxiv.org/html/2509.15937/x3.png",
                "caption": "Figure 7:Training performance curves for real-world robots at different scales. The episodes required per robot to reach a high success rate decrease as the number of robots increases from 2 to 4 to 8.",
                "position": 1014
            }
        ]
    },
    {
        "header": "5Discussion and Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AEvalauation tasks",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15937/x4.png",
                "caption": "Figure 8:Illustrations of the real-world tasks in our experiments. (A) Rice Scooping and Transfer: scoop the rice from the big bowl and transfer it to the cooker. (B) Unfold Mat: unford the folded mat on the table. (C) Pick and Place Bowl: pick up the bowl from the tray and place it on the plate. (D) Desktop Sweep Disposal: sweep the trash on the table into the trash can.",
                "position": 1979
            }
        ]
    },
    {
        "header": "Appendix BDataset",
        "images": []
    },
    {
        "header": "Appendix CScene Transfer",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15937/figures/transfer_demo.png",
                "caption": "Figure 9:Illustrations of scene/lighting transfer.",
                "position": 2114
            }
        ]
    },
    {
        "header": "Appendix DMulti-Robots Scaling",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15937/x5.png",
                "caption": "(a)Training with 2 robots",
                "position": 2124
            },
            {
                "img": "https://arxiv.org/html/2509.15937/x5.png",
                "caption": "(a)Training with 2 robots",
                "position": 2127
            },
            {
                "img": "https://arxiv.org/html/2509.15937/x6.png",
                "caption": "(b)Training with 4 robots",
                "position": 2132
            },
            {
                "img": "https://arxiv.org/html/2509.15937/x7.png",
                "caption": "(c)Training with 8 robots",
                "position": 2137
            }
        ]
    },
    {
        "header": "Appendix EAuthor contributions",
        "images": []
    }
]