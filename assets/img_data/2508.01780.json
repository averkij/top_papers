[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.01780/x1.png",
                "caption": "(a)Tool-Use Benchmarks: Simple Simulator.",
                "position": 132
            },
            {
                "img": "https://arxiv.org/html/2508.01780/x1.png",
                "caption": "(a)Tool-Use Benchmarks: Simple Simulator.",
                "position": 135
            },
            {
                "img": "https://arxiv.org/html/2508.01780/x2.png",
                "caption": "(b)MCP Benchmarks: Small-scale MCP servers.",
                "position": 141
            },
            {
                "img": "https://arxiv.org/html/2508.01780/x3.png",
                "caption": "(c)LiveMCPBench: Real & Large-scale MCP servers.",
                "position": 147
            },
            {
                "img": "https://arxiv.org/html/2508.01780/x4.png",
                "caption": "Figure 2:LiveMCPBench comprises four parts: (1) Diverse Daily Task, a collection of everyday tasks; (2) LiveMCPTool, a large-scale, out-of-the-box MCP toolset; (3) MCP Copilot Agent, a ReACT-based agent system capable of retrieval and multi-turn tool invocation across the MCP toolset; (4) LiveMCPEval, an automated LLM-as-a-Judge evaluation system for accurately assessing online, time-varying tasks.",
                "position": 154
            }
        ]
    },
    {
        "header": "LiveMCPBench",
        "images": []
    },
    {
        "header": "Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.01780/x5.png",
                "caption": "Figure 3:Correlation between model evaluation performance and human agreement rates across multiple evaluators: The analysis of annotated agent trajectories from Claude-Opus-4 and Claude-Sonnet-4 (color indicates evaluator variant).",
                "position": 554
            },
            {
                "img": "https://arxiv.org/html/2508.01780/x6.png",
                "caption": "Figure 4:Comparison of human agreement rates across different models when evaluated using human-annotated versus LLM-generated key points.",
                "position": 574
            }
        ]
    },
    {
        "header": "Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.01780/x7.png",
                "caption": "Figure 5:Log-Price vs. Performance scatter plot with pareto frontier representation. Different colors represent different model families.",
                "position": 695
            },
            {
                "img": "https://arxiv.org/html/2508.01780/x8.png",
                "caption": "Figure 6:Distribution of errors for four types. We counted two agent trajectories for the advanced claude series models.",
                "position": 737
            }
        ]
    },
    {
        "header": "Related Work",
        "images": []
    },
    {
        "header": "Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitations",
        "images": []
    },
    {
        "header": "Appendix BEthical Considerations",
        "images": []
    },
    {
        "header": "Appendix CDetails of Task Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.01780/x9.png",
                "caption": "Figure 7:Task distribution in LiveMCPBench: a comprehensive benchmark comprising 95 tasks Across 6 distinct domains.",
                "position": 1203
            }
        ]
    },
    {
        "header": "Appendix DDetails of LiveMCPTool Collection",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.01780/x10.png",
                "caption": "Figure 8:Distribution of tools in LiveMCPTool, categorized into 8 distinct types (total: 527 tools).",
                "position": 1289
            },
            {
                "img": "https://arxiv.org/html/2508.01780/x11.png",
                "caption": "Figure 9:Distribution of servers in LiveMCPTool, categorized into 8 distinct types (total: 70 servers).",
                "position": 1292
            }
        ]
    },
    {
        "header": "Appendix EImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.01780/x12.png",
                "caption": "Figure 10:An illustrative case of evaluation failure in LiveMCPEval. The assessment was conducted using DeepSeek-V3 on the completion trajectory from Claude-Sonnet-4. The evaluator model erroneously concluded the task was successful based solely on the agent’s file creation action, while failing to recognize that the agent did not actually acquire the required information.",
                "position": 1353
            },
            {
                "img": "https://arxiv.org/html/2508.01780/x13.png",
                "caption": "Figure 11:Comparison of key points in DeepSeek-V3 and human annotations: Similar content despite different ordering.",
                "position": 1393
            }
        ]
    },
    {
        "header": "Appendix FEvaluation Analysis",
        "images": []
    },
    {
        "header": "Appendix GCase Study: Error Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.01780/x14.png",
                "caption": "Figure 12:An illustration ofQuery Error: Discrepancy between the agent-generated query and the task’s required competencies.",
                "position": 1553
            },
            {
                "img": "https://arxiv.org/html/2508.01780/x15.png",
                "caption": "Figure 13:An illustration ofRetrieve Error: The retrieve system incorrectly identifies and returns an inappropriate tool.",
                "position": 1556
            },
            {
                "img": "https://arxiv.org/html/2508.01780/x16.png",
                "caption": "Figure 14:An illustration ofTool Error: The LLM misapplication with improper tool parameters.",
                "position": 1559
            },
            {
                "img": "https://arxiv.org/html/2508.01780/x17.png",
                "caption": "Figure 15:An illustration ofOther Error: The agent’s inadequate response to tool timeout.",
                "position": 1562
            }
        ]
    },
    {
        "header": "Appendix HPrompts",
        "images": []
    }
]