[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22203/x1.png",
                "caption": "Figure 1:The training and evaluation curves of RL on Qwen-2.5-7B using different verifiers, with the x-axis representing training iterations in all plots.Leftillustrates the evaluation accuracy averaged over multiple benchmarks, including GSM8K, MATH500, Minerva Math, OlympiadBench, AIME24, and AMC23.Rightdepicts changes in reward values during training. The ‚Äútraining rewards‚Äù indicate the rewards provided by the corresponding reward system to the policy model, whereas the ‚Äúoracle rewards‚Äù represent rewards the model receives when judged by combining with GPT-4o. We provide detailed breakdown of evaluation results in Table2.",
                "position": 191
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Are Verifiers Trustworthy? From a Static Evaluation Perspective",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22203/x2.png",
                "caption": "Figure 2:Recall rates of various rule-based verifiers across multiple datasets, evaluated on a subset sampled from Deepseek-R1-Distill-Qwen-32B. ‚ÄúVERL‚Äù, ‚ÄúQwen,‚Äù and ‚ÄúHF‚Äù refer to the Verl Math Verifier, Qwen-Math Verifier, and Hugging Face Math Verifier, respectively.",
                "position": 251
            },
            {
                "img": "https://arxiv.org/html/2505.22203/x2.png",
                "caption": "Figure 2:Recall rates of various rule-based verifiers across multiple datasets, evaluated on a subset sampled from Deepseek-R1-Distill-Qwen-32B. ‚ÄúVERL‚Äù, ‚ÄúQwen,‚Äù and ‚ÄúHF‚Äù refer to the Verl Math Verifier, Qwen-Math Verifier, and Hugging Face Math Verifier, respectively.",
                "position": 254
            },
            {
                "img": "https://arxiv.org/html/2505.22203/x3.png",
                "caption": "Figure 3:Recall Rate of the Huggingface Math Verifier, evaluated on data sampled from various models across different RL training datasets. ‚ÄúDS‚Äù stands for Deepseek, while ‚ÄúSkywork‚Äù refers to the Skywork-OR1 dataset.",
                "position": 261
            }
        ]
    },
    {
        "header": "4The Effect of Verifiers on RL Training",
        "images": []
    },
    {
        "header": "5When Good Verifiers Go Bad: Reward Hacking in RL Training",
        "images": []
    },
    {
        "header": "6Probing Verifier Robustness with Hacking Patterns",
        "images": []
    },
    {
        "header": "7Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails of Verifier Evaluation Dataset Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22203/x4.png",
                "caption": "Figure 4:Prompt for using GPT-4o as an annotator to provide ground-truth annotations based on the model‚Äôs response and the target answer, indicating whether the model‚Äôs response aligns with the target answer.",
                "position": 1398
            }
        ]
    },
    {
        "header": "Appendix BDetailed Results of Rule-based Verifiers Across Datasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22203/x5.png",
                "caption": "Figure 5:Examples of correct model responses that are incorrectly flagged as incorrect by the rule-based verifier.upperdemonstrates that the model‚Äôs predicted answer differs from the ground truth only in terms of mathematical formatting, while thelowerhighlights cases where different representations (such asœÄ4ùúã4\\frac{\\pi}{4}divide start_ARG italic_œÄ end_ARG start_ARG 4 end_ARGand45osuperscript45ùëú45^{o}45 start_POSTSUPERSCRIPT italic_o end_POSTSUPERSCRIPT) are considered equivalent given the query context (calculating angleŒ≤ùõΩ\\betaitalic_Œ≤).",
                "position": 1450
            }
        ]
    },
    {
        "header": "Appendix CDetailed Evaluation Setting for Model-based Verifiers",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22203/x6.png",
                "caption": "Figure 6:Prompts that include and exclude the original question.",
                "position": 1464
            }
        ]
    },
    {
        "header": "Appendix DDetailed Results of Model-based Verifiers and Hybrid Verifiers",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22203/x7.png",
                "caption": "Figure 7:Example where DeepSeek-R1-Distill-Qwen-7B correctly identifies the equivalence between Ground Truth and Predicted Answer.",
                "position": 1656
            }
        ]
    },
    {
        "header": "Appendix ETraining And Evaluation Details of Reinforcement Learning",
        "images": []
    },
    {
        "header": "Appendix FDetails Results of Reinforcement Learning",
        "images": []
    },
    {
        "header": "Appendix GTraining Details for R1-Distill-Verifier-1.5B",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22203/x8.png",
                "caption": "Figure 8:The training and evaluation curves of RL using general-verifier, with the x-axis representing training iterations in all plots.Leftillustrates the evaluation accuracy averaged over multiple benchmarks, including GSM8K, MATH500, Minerva Math, OlympiadBench, AIME24, and AMC23.Rightdepicts changes in reward values during training. The ‚Äútraining rewards‚Äù indicate the rewards provided by the corresponding reward system to the policy model, whereas the ‚Äúoracle rewards‚Äù represent rewards the model receives when judged by combining with GPT-4o. We provide a detailed breakdown of evaluation results in Table2.",
                "position": 1707
            }
        ]
    },
    {
        "header": "Appendix HAnalysis of Hacking Pattern During RL Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22203/x9.png",
                "caption": "Figure 9:Example where R1-Verifier-1.5B is hacked by a single simple character (such as ‚Äú{‚Äù) and misjudge it as correct.",
                "position": 1719
            },
            {
                "img": "https://arxiv.org/html/2505.22203/x10.png",
                "caption": "Figure 10:Examples where R1-Verifier-1.5B is hacked by a long sequence of gibberish text.",
                "position": 1724
            }
        ]
    },
    {
        "header": "Appendix IDetails of Verifier Robustness Probing",
        "images": []
    }
]