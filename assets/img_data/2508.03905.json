[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03905/x1.png",
                "caption": "Figure 1:Sotopia-RLenhances social intelligence across diverse tasks.Sotopia-RL outperformsSotopia-π\\piin goal completion score on accommodation, persuasion, collaboration, and negotiation tasks, as evaluated bySotopia-eval.Sotopia-π\\pionly utilizes episode-level goal completion score as reward signals to conduct self-reinforcement, whileSotopia-RL utilizes more fine-grained reward designs for RL training.",
                "position": 128
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3SotopiaEnvironment",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03905/x2.png",
                "caption": "Figure 2:An example of a social task in theSotopiaenvironment.\nTom is agent A, and Oliver is agent B.\nEach agent has a unique goal that is hidden from the other.\n“9 / 10” indicates a single-dimensional episode-level reward provided by LLMs to describe its goal completion status.",
                "position": 215
            }
        ]
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03905/x3.png",
                "caption": "Figure 3:Overview of social reward design.\nTo better describe and model the quality of an utterance in social interactions, we expand the episode-level reward (“9/10” mentioned above) from two axes: (1) expanding from episode-level into utterance-level; (2) expanding from single-dimension to multi-dimensions, expanding from goal completion (Goal) to relationship maintaining (Rel) and knowledge seeking (Kno). It allows us to have denser reward signals for RL training.",
                "position": 263
            },
            {
                "img": "https://arxiv.org/html/2508.03905/x3.png",
                "caption": "Figure 3:Overview of social reward design.\nTo better describe and model the quality of an utterance in social interactions, we expand the episode-level reward (“9/10” mentioned above) from two axes: (1) expanding from episode-level into utterance-level; (2) expanding from single-dimension to multi-dimensions, expanding from goal completion (Goal) to relationship maintaining (Rel) and knowledge seeking (Kno). It allows us to have denser reward signals for RL training.",
                "position": 266
            },
            {
                "img": "https://arxiv.org/html/2508.03905/x4.png",
                "caption": "Figure 4:Overview of social agent training. Our pipeline has three stages:\n(1) Data preparation: Generate GPT self‑play rollouts and annotate rewards withoffline inference, where an LLM evaluates and attributes rewards after the conversation ends.\n(2) SFT training: Train the policy and an utterance‑level reward model via fine‑tuning.\n(3) RL training: Continue RL self‑play with rewards fromonline inference, where the RM sees only the dialogue history up to the current turn for prediction.",
                "position": 272
            }
        ]
    },
    {
        "header": "5Experimental Settings",
        "images": []
    },
    {
        "header": "6Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03905/x5.png",
                "caption": "Figure 5:Evaluation results with different LLM-based evaluators.The consistent improvement on evaluators indicatesno reward hacking. Full results in Appendix §E.2.",
                "position": 667
            },
            {
                "img": "https://arxiv.org/html/2508.03905/x5.png",
                "caption": "Figure 5:Evaluation results with different LLM-based evaluators.The consistent improvement on evaluators indicatesno reward hacking. Full results in Appendix §E.2.",
                "position": 670
            },
            {
                "img": "https://arxiv.org/html/2508.03905/x6.png",
                "caption": "Figure 6:Evaluation results with different partner models.The consistent improvement with multiple partners indicatesno reward hacking. Full results in Appendix §E.2.",
                "position": 675
            },
            {
                "img": "https://arxiv.org/html/2508.03905/x7.png",
                "caption": "Figure 7:Goalscore curve during the training process.Incorporating additional rewards into training delays convergence compared to using theGoalreward alone.",
                "position": 680
            }
        ]
    },
    {
        "header": "7Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03905/x8.png",
                "caption": "Figure 8:Pairwise reward label correlation. Reward labels with various LLMs are highly correlated.",
                "position": 804
            }
        ]
    },
    {
        "header": "8Qualitative Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03905/x9.png",
                "caption": "Figure 9:Case study. The agent trained withSotopia-RL can produce utterances that integrate empathy, informativeness, and goal pursuit within a single utterance.",
                "position": 818
            }
        ]
    },
    {
        "header": "9Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AArtifact Details",
        "images": []
    },
    {
        "header": "Appendix BExperimental Details",
        "images": []
    },
    {
        "header": "Appendix CHuman Evaluation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03905/figure/evaluation_instruction1.png",
                "caption": "Figure 10:An example of the explanation of the believability dimension of social annotation in the evaluation instruction page.Each annotator is asked to read similar definitions of the social intelligence dimension and their corresponding annotation standards at the evaluation instruction page.",
                "position": 1967
            },
            {
                "img": "https://arxiv.org/html/2508.03905/figure/eval_instruction_example.png",
                "caption": "Figure 11:An annotation example of social interaction evaluation.Each dimension is annotated with one sentence and one score.",
                "position": 1970
            },
            {
                "img": "https://arxiv.org/html/2508.03905/figure/eval_explanation.png",
                "caption": "Figure 12:The evaluation metric range explanation.The prompt before the official annotation stage is to remind annotators about the rules of reasoning, writing, and social dimension scoring.",
                "position": 1976
            }
        ]
    },
    {
        "header": "Appendix DAI Assistant Details",
        "images": []
    },
    {
        "header": "Appendix EAdditional Experimental Results",
        "images": []
    },
    {
        "header": "Appendix FAdditional Analysis of Utterance-level Reward",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03905/x10.png",
                "caption": "Figure 13:Distribution ofGoal,Rel,Kno, and combined reward values in our training data.Rewards are normalized into a range of [0,1]. We observe that the combined reward is closer to a normal distribution and is more regularized than the distribution of a single reward.",
                "position": 2706
            },
            {
                "img": "https://arxiv.org/html/2508.03905/x11.png",
                "caption": "Figure 14:Reward attribution and reward combination examples. On the left, it explains 4 types of attribution methods (uniform, scaled, singular, and direct). On the right, it explains 4 types of different reward combination methods (Rel-RL,Goal-RL,Kno-RL,Sotopia-RL) where all of them are based on direct reward attribution andSotopia-RL is the combined one.\nMore details are in Appendix §Gand §H.",
                "position": 2709
            }
        ]
    },
    {
        "header": "Appendix GAdditional Details about Reward Attribution",
        "images": []
    },
    {
        "header": "Appendix HAdditional Details about Reward Dimensions",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03905/x12.png",
                "caption": "Figure 15:Case study onSotopia-RL as the first agent.Sotopia-RL skillfully proposes a win-win outcome by proposing some exchange.",
                "position": 2996
            },
            {
                "img": "https://arxiv.org/html/2508.03905/x13.png",
                "caption": "Figure 16:Case study onSotopia-RL as the first agent.Sotopia-RL reaches a collaborative solution with practical reasoning.",
                "position": 2999
            },
            {
                "img": "https://arxiv.org/html/2508.03905/x14.png",
                "caption": "Figure 17:Case study onSotopia-RL as the second agent.Sotopia-RL offers a solution-oriented perspective while acknowledging the goals of both sides.",
                "position": 3002
            },
            {
                "img": "https://arxiv.org/html/2508.03905/x15.png",
                "caption": "Figure 18:Case study onSotopia-RL as the second agent.Sotopia-RL proposes a multi-turn strategy to provide guidance to the other side.",
                "position": 3005
            }
        ]
    },
    {
        "header": "Appendix IAdditional Case Study",
        "images": []
    }
]