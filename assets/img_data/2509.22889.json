[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22889/figures/comparison.png",
                "caption": "Figure 1:Deep Sets (DS) and Set Transformer (ST) pipelines compared to the CST architecture (ours).In DS/ST pipelines, a shared CNN is applied independently to each image in the input set, yielding a set ofuncontextualized activation volumeswhose spatial dimensions are subsequently collapsed via Global Average Pooling, resulting in a set ofuncontextualized latent vectors. These vectors are then fed to the DS/ST module, which performs contextualization and outputs a set ofcontextualized latent vectorsfor downstream processing. In contrast, CST (ours) processes the set of images through a stack of SetConv2D blocks, simultaneously performing feature extraction and context modeling, and yieldingcontextualized activation volumesthat retain spatial dimensions(H′×W′×C′)(H^{\\prime}\\times W^{\\prime}\\times C^{\\prime})and can be directly used in downstream tasks.",
                "position": 113
            },
            {
                "img": "https://arxiv.org/html/2509.22889/figures/setconv2d.png",
                "caption": "Figure 2:SetConv2D layer. SetConv2D takes a set of volumes as input and processes each of these by a shared convolutional layer. A Global Average Pooling (GAP) is applied to each volume to yield a set of latent vectors that is fed to a Multi-Head Self-Attention (MHSA) unit. The output of MHSA is used as context-aware bias vectors that are summed to the volumes fed to the GAP, via a residual connection. After applying these biases, the volumes are passed through a non-linear activation function.",
                "position": 119
            },
            {
                "img": "https://arxiv.org/html/2509.22889/figures/gradcam_anomaly.png",
                "caption": "Figure 3:Unlike Set Transformer (ST), CST is transparent and explainable. The Set Anomaly Detection task aims to identify anomalous images within a set. The figure shows two image sets derived from the CelebA dataset[7]. In each set, a majority of normal images share two attributes (wearing hatandsmilingin the first,no beardandattractivein the second), while a minority lack these attributes and are thus anomalous. After training a CST and an ST on CelebA for Set Anomaly Detection, we evaluate the explainability of their predictions by overlaying Grad-CAMs on anomalous images. CST explanations correctly highlight the anomalous regions, whereas ST explanations fail to provide meaningful insights. For further details, see Section6.2.",
                "position": 125
            }
        ]
    },
    {
        "header": "2Problem Formulation",
        "images": []
    },
    {
        "header": "3Related Work",
        "images": []
    },
    {
        "header": "4Convolutional Set Transformer (CST)",
        "images": []
    },
    {
        "header": "5CST Pre-training and Transfer Learning",
        "images": []
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22889/figures/fig_ct_ablation.png",
                "caption": "Figure 4:Combinatorial Training ablation. Relative increase in Top-1 Test Accuracy for CIC models trained on the ImageNet64x64 dataset when employing CT compared to conventional training.",
                "position": 667
            },
            {
                "img": "https://arxiv.org/html/2509.22889/figures/latent_space.png",
                "caption": "Figure 5:2D latent representations of CIFAR-10 test images with varying context size. Each color corresponds to a class.",
                "position": 1150
            },
            {
                "img": "https://arxiv.org/html/2509.22889/figures/pec_tl.png",
                "caption": "Figure 6:CST-15 and VGG-19 architectures. SetConv2D blocks and Conv2D layers specified with (nf​i​l​t​e​r​sn_{filters}, kernel size). All convolutions are implemented withsamepadding. In SetConv2D blocks, attention heads have dimension 64. Maxpooling is implemented with pool size 2. SetConv2D blocks and Conv2D layers are followed by ReLU6 activation for CST-15 and ReLU for VGG-19.",
                "position": 1296
            },
            {
                "img": "https://arxiv.org/html/2509.22889/figures/pec_tl.png",
                "caption": "Figure 7:CST-15 and VGG-19 performance on the ImageNet validation set. Single-crop Classification Accuracy (%) as a function of the input set size.",
                "position": 1384
            },
            {
                "img": "https://arxiv.org/html/2509.22889/figures/PEC.png",
                "caption": "Figure 9:Four image sets drawn from the PEC dataset.Each row corresponds to an image set, with all images in a set drawn from the same personal album. From top to bottom, the event categories are Christmas, Cruise, Children’s Birthday, and Wedding. Notably, some images are irrelevant to the respective event category, introducing noise. In the figure, faces are masked to comply with the dataset’s usage policies, although the experiment was conducted on the original, unmasked images.",
                "position": 1452
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAblation Study on Combinatorial Training",
        "images": []
    },
    {
        "header": "Appendix BAdditional Details About the Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22889/figures_supplementary/grad_cam_comparision.png",
                "caption": "Figure C1:CST-15 Grad-CAMs as opposed to ConvNeXt-XL, ResNet50, and VGG-19 Grad-CAMs. For CST-15, we compute Grad-CAMs at the penultimate SetConv2D block. For the other models, we compute Grad-CAMs at the last layer before GAP or Flattening.",
                "position": 2774
            },
            {
                "img": "https://arxiv.org/html/2509.22889/figures_supplementary/layer_grad_cams.png",
                "caption": "Figure C2:Grad-CAMs for an image set provided as input to CST-15, computed with respect to the ground truth class at various layers of the CST-15 architecture.",
                "position": 2777
            },
            {
                "img": "https://arxiv.org/html/2509.22889/figures_supplementary/grad_cams1.png",
                "caption": "Figure C3:Grad-CAMs for image sets provided as input to CST-15 with respect to the ground truth class. Each row corresponds to a set of images. Grad-CAMs are computed at the penultimate SetConv2D block.",
                "position": 2780
            },
            {
                "img": "https://arxiv.org/html/2509.22889/figures_supplementary/grad_cams2.png",
                "caption": "Figure C4:Grad-CAMs for image sets provided as input to CST-15 with respect to the ground truth class. Each row corresponds to a set of images. Grad-CAMs are computed at the penultimate SetConv2D block.",
                "position": 2783
            },
            {
                "img": "https://arxiv.org/html/2509.22889/figures_supplementary/grad_cams3.png",
                "caption": "Figure C5:Grad-CAMs for image sets provided as input to CST-15 with respect to the ground truth class. Each row corresponds to a set of images. Grad-CAMs are computed at the penultimate SetConv2D block.",
                "position": 2786
            }
        ]
    },
    {
        "header": "Appendix CExplainability of CSTs",
        "images": []
    }
]