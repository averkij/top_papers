[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.08033/x1.png",
                "caption": "Figure 1:Our method generateshigh-qualityandeditablesurfel Gaussians through a cascaded 3D diffusion pipeline, given single-view images or texts as the conditions.",
                "position": 79
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4GaussianAnything",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.08033/x2.png",
                "caption": "Figure 2:Pipeline of the 3D VAE ofGaussianAnything.In the 3D latent space learning stage, our proposed 3D VAEℰϕsubscriptℰbold-italic-ϕ\\mathcal{E}_{\\bm{\\phi}}caligraphic_E start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPTencodesV−limit-from𝑉V-italic_V -views of posed RGB-D(epth)-N(ormal) renderingsℛℛ\\mathcal{R}caligraphic_Rinto a point-cloud structured latent space. This is achieved by first processing the multi-view inputs into the un-structuredset latent, which is further projected onto the 3D manifold through a cross attention block, yielding the point-cloud structured latent code𝐳𝐳{\\mathbf{z}}bold_z.\nThe structured 3D latent is further decoded by a 3D-aware DiT transformer, giving the coarse Gaussian prediction.\nFor high-quality rendering, the base Gaussian is further up-sampled by a series of cascaded upsampler𝒟Uksuperscriptsubscript𝒟𝑈𝑘\\mathcal{D}_{U}^{k}caligraphic_D start_POSTSUBSCRIPT italic_U end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPTtowards a dense Gaussian for high-resolution rasterization.\nThe 3D VAE training objective is detailed in Eq. (9).",
                "position": 251
            },
            {
                "img": "https://arxiv.org/html/2411.08033/x3.png",
                "caption": "Figure 3:Diffusion training ofGaussianAnything.Based on the point-cloud structure 3D VAE, we perform cascaded 3D diffusion learning given text (a) and image (b) conditions. We adopt DiT architecture with AdaLN-single(Chen et al.,2023)and QK-Norm(Dehghani et al.,2023; Esser et al.,2021). For both condition modality, we send in the conditional feature with cross attention block, but at different positions. The 3D generation is achieved in two stages (c), where a point cloud diffusion model first generates the 3D layout𝐳x,0subscript𝐳𝑥0{\\mathbf{z}}_{x,0}bold_z start_POSTSUBSCRIPT italic_x , 0 end_POSTSUBSCRIPT, and a texture diffusion model further generates the corresponding point-cloud features𝐳h,0subscript𝐳ℎ0{\\mathbf{z}}_{h,0}bold_z start_POSTSUBSCRIPT italic_h , 0 end_POSTSUBSCRIPT. The generated latent code𝐳0subscript𝐳0{\\mathbf{z}}_{0}bold_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTis decoded into the final 3D object with the pre-trained VAE decoder.",
                "position": 396
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.08033/x4.png",
                "caption": "Figure 4:Qualitative Comparison of Image-to-3D. We showcase the novel view 3D reconstruction of all methods given a single image from unseen GSO dataset. Our proposed method achieves consistently stable performance across all cases.\nNote that though feed-forward 3D reconstruction methods achieve sharper texture reconstruction, these method fail to yield intact 3D predictions under challenging cases (e.g., the rhino in row 2).\nIn contrast, our proposed native 3D diffusion model achieve consistently better performance.\nBetter zoom in.",
                "position": 476
            },
            {
                "img": "https://arxiv.org/html/2411.08033/x5.png",
                "caption": "Figure 5:Qualitative Comparison of Text-to-3D. We present text-conditioned 3D objects generated byGaussianAnything, displaying two views of each sample.\nThe top section compares our results with baseline methods, while the bottom shows additional samples from our method along with their geometry maps.\nOur approach consistently yields better quality in terms of geometry, texture, and text-3D alignment.",
                "position": 629
            },
            {
                "img": "https://arxiv.org/html/2411.08033/x6.png",
                "caption": "Figure 6:3D editing.Given two text prompts, we generate the corresponding point cloud𝐳0,xsubscript𝐳0𝑥{\\mathbf{z}}_{0,x}bold_z start_POSTSUBSCRIPT 0 , italic_x end_POSTSUBSCRIPTwith stage-1 diffusion model withϵΘxsuperscriptsubscriptbold-italic-ϵΘ𝑥\\bm{\\epsilon}_{\\Theta}^{x}bold_italic_ϵ start_POSTSUBSCRIPT roman_Θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_x end_POSTSUPERSCRIPT, and the corresponding point cloud features𝐳0,hsubscript𝐳0ℎ{\\mathbf{z}}_{0,h}bold_z start_POSTSUBSCRIPT 0 , italic_h end_POSTSUBSCRIPTcan be further generated withϵΘhsuperscriptsubscriptbold-italic-ϵΘℎ\\bm{\\epsilon}_{\\Theta}^{h}bold_italic_ϵ start_POSTSUBSCRIPT roman_Θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_h end_POSTSUPERSCRIPT. As can be seen, the samples from stage-2 are consistent in overall 3D structures but with diverse textures. Thanks to the proposed Point Cloud-structured Latent space, our method supports interactive 3D structure editing. This is achieved by first modifying the stage-1 point cloud𝐳0,x→𝐳0,x′→subscript𝐳0𝑥superscriptsubscript𝐳0𝑥′{\\mathbf{z}}_{0,x}\\rightarrow{{\\mathbf{z}}_{0,x}^{\\prime}}bold_z start_POSTSUBSCRIPT 0 , italic_x end_POSTSUBSCRIPT → bold_z start_POSTSUBSCRIPT 0 , italic_x end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT, and then regenerate the 3D object with the same Gaussian noise.",
                "position": 682
            },
            {
                "img": "https://arxiv.org/html/2411.08033/x7.png",
                "caption": "Figure 7:Qualitative ablation of Cascaded diffusion and latent space editing.We first show the effectiveness of our two-stage cascaded diffusion framework in (a). Compared to Fig.5, the single-stage 3D diffusion yields worse texture details and 3D structure intactness. In (b), we validate the latent point cloud editing yields less 3D artifacts compared to direct 3D editing on the 3D Gaussians.",
                "position": 768
            }
        ]
    },
    {
        "header": "6Conclusion and Discussions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AImplementation details",
        "images": []
    },
    {
        "header": "Appendix BMore visual results and videos",
        "images": []
    }
]