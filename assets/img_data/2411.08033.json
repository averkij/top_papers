[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.08033/x1.png",
                "caption": "Figure 1:Our method generateshigh-qualityandeditablesurfel Gaussians through a cascaded 3D diffusion pipeline, given single-view images or texts as the conditions.",
                "position": 79
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4GaussianAnything",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.08033/x2.png",
                "caption": "Figure 2:Pipeline of the 3D VAE ofGaussianAnything.In the 3D latent space learning stage, our proposed 3D VAEâ„°Ï•subscriptâ„°bold-italic-Ï•\\mathcal{E}_{\\bm{\\phi}}caligraphic_E start_POSTSUBSCRIPT bold_italic_Ï• end_POSTSUBSCRIPTencodesVâˆ’limit-fromğ‘‰V-italic_V -views of posed RGB-D(epth)-N(ormal) renderingsâ„›â„›\\mathcal{R}caligraphic_Rinto a point-cloud structured latent space. This is achieved by first processing the multi-view inputs into the un-structuredset latent, which is further projected onto the 3D manifold through a cross attention block, yielding the point-cloud structured latent codeğ³ğ³{\\mathbf{z}}bold_z.\nThe structured 3D latent is further decoded by a 3D-aware DiT transformer, giving the coarse Gaussian prediction.\nFor high-quality rendering, the base Gaussian is further up-sampled by a series of cascaded upsamplerğ’ŸUksuperscriptsubscriptğ’Ÿğ‘ˆğ‘˜\\mathcal{D}_{U}^{k}caligraphic_D start_POSTSUBSCRIPT italic_U end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPTtowards a dense Gaussian for high-resolution rasterization.\nThe 3D VAE training objective is detailed in Eq.Â (9).",
                "position": 251
            },
            {
                "img": "https://arxiv.org/html/2411.08033/x3.png",
                "caption": "Figure 3:Diffusion training ofGaussianAnything.Based on the point-cloud structure 3D VAE, we perform cascaded 3D diffusion learning given text (a) and image (b) conditions. We adopt DiT architecture with AdaLN-single(Chen etÂ al.,2023)and QK-Norm(Dehghani etÂ al.,2023; Esser etÂ al.,2021). For both condition modality, we send in the conditional feature with cross attention block, but at different positions. The 3D generation is achieved in two stages (c), where a point cloud diffusion model first generates the 3D layoutğ³x,0subscriptğ³ğ‘¥0{\\mathbf{z}}_{x,0}bold_z start_POSTSUBSCRIPT italic_x , 0 end_POSTSUBSCRIPT, and a texture diffusion model further generates the corresponding point-cloud featuresğ³h,0subscriptğ³â„0{\\mathbf{z}}_{h,0}bold_z start_POSTSUBSCRIPT italic_h , 0 end_POSTSUBSCRIPT. The generated latent codeğ³0subscriptğ³0{\\mathbf{z}}_{0}bold_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTis decoded into the final 3D object with the pre-trained VAE decoder.",
                "position": 396
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.08033/x4.png",
                "caption": "Figure 4:Qualitative Comparison of Image-to-3D. We showcase the novel view 3D reconstruction of all methods given a single image from unseen GSO dataset. Our proposed method achieves consistently stable performance across all cases.\nNote that though feed-forward 3D reconstruction methods achieve sharper texture reconstruction, these method fail to yield intact 3D predictions under challenging cases (e.g., the rhino in row 2).\nIn contrast, our proposed native 3D diffusion model achieve consistently better performance.\nBetter zoom in.",
                "position": 476
            },
            {
                "img": "https://arxiv.org/html/2411.08033/x5.png",
                "caption": "Figure 5:Qualitative Comparison of Text-to-3D. We present text-conditioned 3D objects generated byGaussianAnything, displaying two views of each sample.\nThe top section compares our results with baseline methods, while the bottom shows additional samples from our method along with their geometry maps.\nOur approach consistently yields better quality in terms of geometry, texture, and text-3D alignment.",
                "position": 629
            },
            {
                "img": "https://arxiv.org/html/2411.08033/x6.png",
                "caption": "Figure 6:3D editing.Given two text prompts, we generate the corresponding point cloudğ³0,xsubscriptğ³0ğ‘¥{\\mathbf{z}}_{0,x}bold_z start_POSTSUBSCRIPT 0 , italic_x end_POSTSUBSCRIPTwith stage-1 diffusion model withÏµÎ˜xsuperscriptsubscriptbold-italic-ÏµÎ˜ğ‘¥\\bm{\\epsilon}_{\\Theta}^{x}bold_italic_Ïµ start_POSTSUBSCRIPT roman_Î˜ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_x end_POSTSUPERSCRIPT, and the corresponding point cloud featuresğ³0,hsubscriptğ³0â„{\\mathbf{z}}_{0,h}bold_z start_POSTSUBSCRIPT 0 , italic_h end_POSTSUBSCRIPTcan be further generated withÏµÎ˜hsuperscriptsubscriptbold-italic-ÏµÎ˜â„\\bm{\\epsilon}_{\\Theta}^{h}bold_italic_Ïµ start_POSTSUBSCRIPT roman_Î˜ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_h end_POSTSUPERSCRIPT. As can be seen, the samples from stage-2 are consistent in overall 3D structures but with diverse textures. Thanks to the proposed Point Cloud-structured Latent space, our method supports interactive 3D structure editing. This is achieved by first modifying the stage-1 point cloudğ³0,xâ†’ğ³0,xâ€²â†’subscriptğ³0ğ‘¥superscriptsubscriptğ³0ğ‘¥â€²{\\mathbf{z}}_{0,x}\\rightarrow{{\\mathbf{z}}_{0,x}^{\\prime}}bold_z start_POSTSUBSCRIPT 0 , italic_x end_POSTSUBSCRIPT â†’ bold_z start_POSTSUBSCRIPT 0 , italic_x end_POSTSUBSCRIPT start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT, and then regenerate the 3D object with the same Gaussian noise.",
                "position": 682
            },
            {
                "img": "https://arxiv.org/html/2411.08033/x7.png",
                "caption": "Figure 7:Qualitative ablation of Cascaded diffusion and latent space editing.We first show the effectiveness of our two-stage cascaded diffusion framework in (a). Compared to Fig.5, the single-stage 3D diffusion yields worse texture details and 3D structure intactness. In (b), we validate the latent point cloud editing yields less 3D artifacts compared to direct 3D editing on the 3D Gaussians.",
                "position": 768
            }
        ]
    },
    {
        "header": "6Conclusion and Discussions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AImplementation details",
        "images": []
    },
    {
        "header": "Appendix BMore visual results and videos",
        "images": []
    }
]