[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.10708/x1.png",
                "caption": "Figure 1:The four-stageSearchInstructpipeline: seed generation, query expansion, document retrieval, and response construction.",
                "position": 173
            }
        ]
    },
    {
        "header": "4Applications ofSearchInstruct",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.10708/x2.png",
                "caption": "(a)Tourism domain",
                "position": 522
            },
            {
                "img": "https://arxiv.org/html/2509.10708/x2.png",
                "caption": "(a)Tourism domain",
                "position": 525
            },
            {
                "img": "https://arxiv.org/html/2509.10708/x3.png",
                "caption": "(b)Culinary domain",
                "position": 530
            },
            {
                "img": "https://arxiv.org/html/2509.10708/x4.png",
                "caption": "Figure 3:Iterative refinement loop enabled by theSearchInstructframework. After initial fine-tuning, specific model weaknesses are identified through targeted evaluation. New instruction–response data is then generated to directly address these shortcomings, creating a feedback loop that leads to focused and incremental improvements in model performance.",
                "position": 576
            },
            {
                "img": "https://arxiv.org/html/2509.10708/x5.png",
                "caption": "Figure 4:Pipeline for constructing update-specific instruction data used in model editing. Starting from user-provided queries, relevant documents are retrieved and used to construct grounded instruction–answer pairs.",
                "position": 620
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASystem Prompt Designs",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.10708/x6.png",
                "caption": "Figure 5:System prompt design for query expansion.",
                "position": 1047
            },
            {
                "img": "https://arxiv.org/html/2509.10708/x7.png",
                "caption": "Figure 6:System prompt design for search-oriented query rewriting.",
                "position": 1056
            },
            {
                "img": "https://arxiv.org/html/2509.10708/x8.png",
                "caption": "Figure 7:System prompt design for evidence-grounded response construction.",
                "position": 1065
            },
            {
                "img": "https://arxiv.org/html/2509.10708/x9.png",
                "caption": "Figure 8:System prompt design for answer refinement and updates.",
                "position": 1074
            }
        ]
    },
    {
        "header": "Appendix BFailure Cases of Current LLM Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.10708/x10.png",
                "caption": "Figure 9:Examples of partial outputs from GPT-4o and SearchInstruct (GPT4o mini):This demonstrates that in certain instructions, the SearchInstruct method is capable of producing better responses than larger models, as it benefits from access to highly relevant contextual information, even when using a smaller model",
                "position": 1088
            }
        ]
    },
    {
        "header": "Appendix CSeeds Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.10708/x11.png",
                "caption": "Figure 10:Representative examples of minimally revised responses with factual updates. Each row includes a user instruction, a rejected response containing outdated or incorrect information, and a chosen response with concise, targeted edits. The examples cover domains such as politics, international affairs, and sports, demonstrating factual correction with minimal changes to phrasing and tone.",
                "position": 1373
            }
        ]
    },
    {
        "header": "Appendix E – Training Hyperparameters",
        "images": []
    },
    {
        "header": "Appendix F: Future Work",
        "images": []
    }
]