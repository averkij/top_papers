[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.08825/x1.png",
                "caption": "Figure 1:We quantify LLM hacking risk through systematic replication of 37 diverse computational social science annotation tasks.\nFor these tasks, we use a combined set of 2,361 realistic hypotheses that researchers might test using these annotations.\nThen, we collect 13 million LLM annotations across plausible LLM configurations.\nThese annotations feed into 1.4 million regressions testing the hypotheses.\nFor a hypothesis with no true effect (ground truthp>0.05p>0.05), different LLM configurations yield conflicting conclusions.\nCheckmarks indicate correct statistical conclusions matching ground truth; crosses indicate LLM hacking â€“ incorrect conclusions due to annotation errors.\nAcross all experiments, LLM hacking occurs in 31-50% of cases even with highly capable models.\nSince minor configuration changes can flip scientific conclusions, from correct to incorrect, LLM hacking can be exploited to present anything as statistically significant.",
                "position": 98
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Large Language Model (LLM) Hacking",
        "images": []
    },
    {
        "header": "3Literature review: Using LLMs for automated data annotation",
        "images": []
    },
    {
        "header": "4Experimental setup",
        "images": []
    }
]