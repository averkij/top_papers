[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.11842/x1.png",
                "caption": "Figure 1:A high-level comparison between the multi-head attention and our proposed mixture-of-head attention.Subfigure (a) illustrates a standard multi-head attention layer withhℎhitalic_hattention heads, while subfigure (b) demonstrates the Mixture-of-Head attention (MoH) architecture. It is important to note that MoH does not increase the number of attention heads, ensuring that the total parameter for MoH is comparable to that of the multi-head attention.",
                "position": 132
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.11842/x2.png",
                "caption": "Figure 2:Performance evolution during continue-tuning.The MoH model quickly recovers to over 95% of the performance of the original model within a training budget of 10B tokens. Then, the performance gradually improves with the increase of the training tokens.",
                "position": 1035
            },
            {
                "img": "https://arxiv.org/html/2410.11842/x3.png",
                "caption": "Figure 3:Visualization of the head load distribution in the final MoH layer.For ViT and DiT, we present the head load distributions for the categories “Desk”, “Goldfish”, and “Ice cream”. For LLM, we display the head distributions for the tasks “LogiQA”, “PIQA”, and “WinoGrande”. MoH-ViT-B, MoH-DiT-XL/2, and MoH-LLM-B activate 75%, 90%, and 75% of the attention heads, respectively.",
                "position": 1167
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    },
    {
        "header": "Appendix AAdditional Discussions",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    },
    {
        "header": "Appendix CAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.11842/x4.png",
                "caption": "Figure A:Additional visualization of the head load distribution in the final MoH layer.For MoH-ViT-B and MoH-DiT-XL/2, we present the head load distributions for the categories “Basketball”, “Bookshop”, “Cart”, “Husky”, and “Jean”. MoH-ViT-B activates 75% of the attention heads. MoH-DiT-XL/2 activates 90% of the attention heads.",
                "position": 2800
            }
        ]
    },
    {
        "header": "Appendix DAdditional Qualitative Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.11842/x5.png",
                "caption": "Figure B:Additional visualization of the head load distribution in MoH-LLaMA3-8B.",
                "position": 2826
            },
            {
                "img": "https://arxiv.org/html/2410.11842/x6.png",
                "caption": "Figure C:Images generated from the proposed MoH-DiT-XL/2 model.We show samples generated from our class-conditional MoH-DiT-XL/2 model trained on ImageNet at 256×\\times×256 resolution. MoH-DiT-XL/2 activates 90% of the attention heads.",
                "position": 2841
            }
        ]
    },
    {
        "header": "Appendix EDetails of Quantitative Evaluations for LLMs",
        "images": []
    }
]