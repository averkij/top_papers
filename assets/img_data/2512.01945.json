[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Instruction-Policy Co-Evolution (Inspo)",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01945/x1.png",
                "caption": "Figure 1:Illustration ofInspo: Inphase 1,Inspomaintains a dynamic population of instruction candidates. For each sampled question, the instruction is sampled based on a selection probability weighted by the importance of each instruction. The reward signals not only update the policy model but also update the importance of instructions. In addition,Inspoinvolves a replay buffer that prioritizes failure trajectories or trajectories that come with low rewards (marked inred) for experience-driven self-reflection. Inphase 2, the history of experience then provides a correction signal to an LLM-based instruction-proposer module, which analyzes the failure cases and evolves new instructions via self-reflection. New instructions are then passed for verification, where the top-performing candidates are merged into the active population of instructions.",
                "position": 158
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01945/x2.png",
                "caption": "Figure 2:(a)Inspovs. Search-R1:Insposhows a better reward at convergence compared to the Search-R1 baseline. (b) Number of tool calls:Inspodiscovers instructions that lead agents to leverage a larger number of tool usages for solving the problem, whereas the baseline converges to a single-turn tool-use. (c) Prompt length: Periodically,Inspoevolves longer and more effective instructions along the RL training process, whereas the baseline sticks to a static instruction. (d) Response length: With a larger number of tool calls byInspo, the converged response comes with more tokens, which contains richer information from the search engine.",
                "position": 629
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01945/x3.png",
                "caption": "Figure 3:A demonstration of the instruction co-evolution process with the policy model for using the search tool. The policy model is first prompted with the instruction-question pair, generating trajectories with environmental feedback and rewards, which are passed to an LLM-based optimizer for the experience-driven reflection process. The optimizer generates critiques on the failures and proposes new instruction candidates, forming an online optimization loop for the instruction.",
                "position": 771
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AFuture Work and Limitations",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    },
    {
        "header": "Appendix CPrompt Templates",
        "images": []
    },
    {
        "header": "Appendix DDemonstration",
        "images": []
    }
]