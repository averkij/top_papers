[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.15748/x1.png",
                "caption": "Figure 1:The overview of our synthesis pipeline. At the theorem level, we find invocable theorems that can be used to rewrite or apply to the assumptions or assertion of the candidate statement, such as theiffand implication rules about theCoprime. Then, we construct the new statements by replacing the specific component with its equivalent form or antecedent. At the proof tree level, our method merges two existing proof trees.",
                "position": 207
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.15748/x2.png",
                "caption": "Figure 2:Prompt template",
                "position": 424
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.15748/x3.png",
                "caption": "Figure 3:Distribution of mathematical subjects. For each employed tactic, we mix the generated variants with the original theorems.\na) The distribution of Mathlib. b) The distribution of Mathlib +rw. c) The distribution of Mathlib +apply.",
                "position": 507
            },
            {
                "img": "https://arxiv.org/html/2410.15748/x4.png",
                "caption": "Table 4:Effectiveness of continual pre-training. We grouped the dataset for CPT and SFT by the tactic employed in the additional state-tactic pairs.",
                "position": 637
            },
            {
                "img": "https://arxiv.org/html/2410.15748/x4.png",
                "caption": "Figure 4:Influence of the quantity of synthesized data points.",
                "position": 749
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ABackground on Lean",
        "images": []
    },
    {
        "header": "Appendix BLimitations",
        "images": []
    },
    {
        "header": "Appendix CDetailed Information of Synthesizing Algorithms",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.15748/x5.png",
                "caption": "Figure 5:Examples of invocable theorems forapply",
                "position": 1770
            }
        ]
    },
    {
        "header": "Appendix DDeeper Analysis of Synthetic Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.15748/x6.png",
                "caption": "Figure 6:The distribution of the number of variants (only 99% of the data are visualized).",
                "position": 2325
            },
            {
                "img": "https://arxiv.org/html/2410.15748/x7.png",
                "caption": "Figure 7:The top20 theorems forrwandapply.",
                "position": 2331
            },
            {
                "img": "https://arxiv.org/html/2410.15748/x8.png",
                "caption": "Figure 8:Examples of synthesized theorems forrw",
                "position": 2341
            },
            {
                "img": "https://arxiv.org/html/2410.15748/x9.png",
                "caption": "Figure 9:Examples of synthesized theorems forapply",
                "position": 2347
            },
            {
                "img": "https://arxiv.org/html/2410.15748/x10.png",
                "caption": "Figure 10:Examples of data for pretraining",
                "position": 2363
            },
            {
                "img": "https://arxiv.org/html/2410.15748/x11.png",
                "caption": "Figure 11:Examples ofrwandapplydata points for finetuning",
                "position": 2366
            },
            {
                "img": "https://arxiv.org/html/2410.15748/x12.png",
                "caption": "Figure 12:Examples ofhavedata points for finetuning",
                "position": 2369
            },
            {
                "img": "https://arxiv.org/html/2410.15748/x13.png",
                "caption": "Figure 13:The performance of models fine-tuned on different SFT datasets on novel_premises split. a) Mathlib-train; b) Mathlib-train +rw; c) Mathlib-train +apply; d) Mathlib-train + rw + apply.",
                "position": 2566
            },
            {
                "img": "https://arxiv.org/html/2410.15748/x14.png",
                "caption": "Figure 14:a) The distribution of theorems proved by different LLMs; b) The distribution of tactics used in the proved theorems.",
                "position": 2573
            },
            {
                "img": "https://arxiv.org/html/2410.15748/x15.png",
                "caption": "Figure 15:The distribution of used tactics for Llama-3-8b fine-tuned on different SFT datasets to prove miniF2F. a) Mathlib-train; b) Mathlib-train +rw; c) Mathlib-train +apply; d) Mathlib-train +rw+apply.",
                "position": 2634
            }
        ]
    },
    {
        "header": "Appendix EAdditional Experiments",
        "images": []
    }
]