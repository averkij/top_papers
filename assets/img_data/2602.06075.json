[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06075/images/memgui-overview/memgui-overall.drawio.png",
                "caption": "Figure 1:An overview of MemGUI-Bench, first comprehensive benchmark for GUI agent memory evaluation.",
                "position": 273
            }
        ]
    },
    {
        "header": "2Memory in Mobile GUI Agents",
        "images": []
    },
    {
        "header": "3Memory-Centric Benchmarking Environment",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06075/images/task-distributions/task-distributions.drawio.png",
                "caption": "Figure 2:Statistical overview of the MemGUI-Bench task suite.",
                "position": 361
            },
            {
                "img": "https://arxiv.org/html/2602.06075/images/unified-architecture/unified-architecture.drawio.png",
                "caption": "Figure 3:The unified architecture of MemGUI-Bench’s snapshot-based plug-and-play framework.",
                "position": 392
            }
        ]
    },
    {
        "header": "4An Automated Evaluation Pipeline with Memory-Specific Metrics",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06075/images/memgui-eval-pipline/memgui-eval-pipline.drawio.png",
                "caption": "Figure 4:MemGUI-Eval’s three-stage progressive scrutiny pipeline.",
                "position": 454
            }
        ]
    },
    {
        "header": "5Benchmarking GUI Agent Baselines",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06075/x1.png",
                "caption": "Figure 5:Performance comparison betweenMemGUI-Bench(89.8% memory-intensive) andAndroidWorld(5.2% memory-intensive). Red annotations show performance drops on memory-intensive tasks.",
                "position": 822
            },
            {
                "img": "https://arxiv.org/html/2602.06075/x2.png",
                "caption": "Figure 6:Performance comparison between single-turn and multi-turn conversation modes.M3A (Single-turn)uses standard context,M3A (Multi-turn)leverages Gemini 2.5 Pro’s extended context window, andUI-TARS-1.5-7Buses sliding window (last 5 turns). Green annotations show performance gains.",
                "position": 1921
            }
        ]
    },
    {
        "header": "6Failure Pattern Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06075/x3.png",
                "caption": "Figure 7:Comprehensive failure pattern heatmap across all evaluated agents for non-timeout failures.",
                "position": 3005
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06075/images/evaluator-comparison/evaluator_comparison_diagram.drawio.png",
                "caption": "Figure 8:Limitations of existing evaluation approaches for memory-intensive GUI tasks.",
                "position": 4123
            },
            {
                "img": "https://arxiv.org/html/2602.06075/images/badcases/rebuttal/paper/01-Execution-Timeout.drawio.jpg",
                "caption": "Figure 9:Execution Timeout Example (UI-TARS-1.5-7B).The task required recording audio and saving it as “MyTestAudio”. After successful recording, the agent attempted to delete the default filename “Record1” character by character through inefficient individual click actions (steps 12-17), exhausting the 17-step limit before completing the renaming operation. This demonstrates how poor action efficiency can cause timeouts even on simple tasks.",
                "position": 10841
            },
            {
                "img": "https://arxiv.org/html/2602.06075/images/badcases/rebuttal/paper/02-Partial-Memory-Hallucination.drawio.jpg",
                "caption": "Figure 10:Partial Memory Hallucination Example (UI-TARS-1.5-7B).The task required finding stock prices for NVIDIA and Apple, calculating the value of 50 and 75 shares respectively. The agent correctly retained NVIDIA’s price (169.92 USD) but hallucinated Apple’s price as 143.92 USD instead of the correct 226.91 USD observed in step 9, leading to an incorrect final calculation.",
                "position": 10847
            },
            {
                "img": "https://arxiv.org/html/2602.06075/images/badcases/rebuttal/paper/03-Process-Memory-Hallucination.drawio.jpg",
                "caption": "Figure 11:Process Memory Hallucination Example (UI-TARS-1.5-7B).The task required finding Q3 2021 smartphone market share data, identifying the top three brands with percentages, and recording them in Joplin. After successfully finding the chart (step 5), the agent prematurely concluded the task was complete, forgetting the remaining critical steps of data extraction and note creation, revealing a failure to retain the full procedural workflow.",
                "position": 10853
            },
            {
                "img": "https://arxiv.org/html/2602.06075/images/badcases/rebuttal/paper/04-Output-Memory-Hallucination.drawio.jpg",
                "caption": "Figure 12:Output Memory Hallucination Example (M3A).The task required transcribing two complete app permission lists. The agent correctly navigated to both ‘Wi-Fi Control’ (step 7, 9 apps) and ‘Picture-in-picture’ (step 9, 9 apps) permission screens but produced an incomplete transcription in the final note (step 15), missing several apps from both lists despite having observed them.",
                "position": 10859
            },
            {
                "img": "https://arxiv.org/html/2602.06075/images/badcases/rebuttal/paper/05-Knowledge-Deficiency.drawio.jpg",
                "caption": "Figure 13:Knowledge Deficiency Example (UI-TARS-1.5-7B).The agent successfully found and retained the required dates (leap day: February 29, Halloween: October 31) but failed due to misidentifying the Google Calendar app as the target “N calendar app” in step 8, demonstrating a knowledge gap in app recognition unrelated to memory capabilities.",
                "position": 10865
            },
            {
                "img": "https://arxiv.org/html/2602.06075/images/badcases/rebuttal/paper/06-Intent-Misunderstanding.drawio.jpg",
                "caption": "Figure 14:Intent Misunderstanding Example (UI-TARS-1.5-7B).The task required comparing English and German Wikipedia article counts and staying on the page with more articles. The agent correctly identified that English Wikipedia has more articles but ended on the German Wikipedia page, misunderstanding the instruction to navigate to and remain on the edition with more articles.",
                "position": 10871
            },
            {
                "img": "https://arxiv.org/html/2602.06075/images/badcases/rebuttal/paper/07-Other.drawio.jpg",
                "caption": "Figure 15:Other Failure Example (SeeAct).The task required finding products with the best value in the Meesho app. After opening the app (step 2), the agent correctly identified that the app was loading and that waiting was necessary (step 3). However, due to action space limitations (no explicit “wait” action), it issued a “TERMINATE” command, prematurely ending the task without performing any required operations.",
                "position": 10877
            },
            {
                "img": "https://arxiv.org/html/2602.06075/x4.png",
                "caption": "Figure 16:Failure type distributions for each GUI agent among non-timeout failures.",
                "position": 10884
            },
            {
                "img": "https://arxiv.org/html/2602.06075/x5.png",
                "caption": "Figure 17:Learning potential across multiple attempts for different GUI agents.",
                "position": 11356
            },
            {
                "img": "https://arxiv.org/html/2602.06075/x6.png",
                "caption": "Figure 18:MemGUI-Eval Stage 1 Success Case: Cost-effective triage successfully identifies task completion with minimal evidence.",
                "position": 11509
            },
            {
                "img": "https://arxiv.org/html/2602.06075/x7.png",
                "caption": "Figure 19:MemGUI-Eval Stage 2 Success Case: Semantic analysis with enriched textual descriptions enables accurate judgment.",
                "position": 11524
            },
            {
                "img": "https://arxiv.org/html/2602.06075/x8.png",
                "caption": "Figure 20:MemGUI-Eval Stage 2 Failed Case: Semantic analysis determines task failure and computes Information Retention Rate (IRR).",
                "position": 11533
            },
            {
                "img": "https://arxiv.org/html/2602.06075/x9.png",
                "caption": "Figure 21:MemGUI-Eval Stage 3 Success Case: Targeted visual verification with requested historical screenshots confirms task completion.",
                "position": 11549
            },
            {
                "img": "https://arxiv.org/html/2602.06075/x10.png",
                "caption": "Figure 22:MemGUI-Eval Stage 3 Failed Case: Visual verification with targeted historical evidence determines task failure with precise IRR calculation.",
                "position": 11558
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]