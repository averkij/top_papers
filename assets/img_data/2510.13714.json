[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13714/x1.png",
                "caption": "Figure 1:Overview of the Dedelayed real-time inference setup.\nThe lightweight local model and a powerful remote model augment each other’s strengths to produce accurate and timely outputs.",
                "position": 89
            },
            {
                "img": "https://arxiv.org/html/2510.13714/x2.png",
                "caption": "Figure 2:To demonstrate the effect of temporally predictive training, we train a 3D transformer to predict the next frame with an MSE loss on pixels.\n(a) shows the original video frame.\n(b) shows the difference between (a) and a future frame, with objects such as the traffic sign and road markings in different locations.\n(c) shows the pixel predictions of the 3D transformer.\n(d) shows the difference from the true future frame.\nWhile the predictive model cannot predict high-frequency details, it is able to accurately model the motion of objects, signs, and road markings.",
                "position": 157
            },
            {
                "img": "https://arxiv.org/html/2510.13714/x3.png",
                "caption": "Figure 3:Example of activation maps from local and remote model components.\nThe remote server uses the higher level of video detail to accurately distinguish and classify objects.\nThe local model provides exact position adjustments based on the current frame.\nWhen making predictions from the combined activation map, small details that would be impossible to make out at low resolution (e.g., the distant pedestrians, labeled red) are accurately classified and localized.",
                "position": 167
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Related work",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13714/x4.png",
                "caption": "Figure 4:Time progresses left to right.\nThe client-side camera produces video frames, which are sent across a communication network to the server.\nThe server runs a heavyweight model using the latest video framext−τx_{t-\\tau}that it receives, in addition to a context of previously received video framesx<t−τx_{<t-\\tau}, as well as the measured delayτ\\tau.\nThis produces an outputzt−τz_{t-\\tau}that the server sends to the client.\nThe client pairs the latest received responsezt−τz_{t-\\tau}with a freshly produced video framextx_{t}, and runs these inputs through a lightweight model.\nThis finally produces a timely resulty^t\\hat{y}_{t}that can be used in real-time delay-sensitive applications.",
                "position": 218
            },
            {
                "img": "https://arxiv.org/html/2510.13714/x5.png",
                "caption": "Figure 5:Overview of the remote video predictive model, trained to predict the label at indexnnfrom aKK-frame context ending atn−Dn-D.",
                "position": 397
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13714/x6.png",
                "caption": "Figure 6:Segmentation accuracy (mIoU) versus round-trip latency (milliseconds or frames).",
                "position": 720
            },
            {
                "img": "https://arxiv.org/html/2510.13714/x7.png",
                "caption": "Figure 7:Segmentation accuracy (mIoU) versus total latency (milliseconds or frames) for selected local model delays.\nPoints are faded as round-trip latency increases.\n4 and 8 ms were interpolated.",
                "position": 725
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13714/x8.png",
                "caption": "Figure 8:Segmentation accuracy (mIoU) versus round-trip latency (milliseconds or frames).\nHere, the remote model’s delay input has been force-fed a specific value.",
                "position": 1345
            },
            {
                "img": "https://arxiv.org/html/2510.13714/x9.png",
                "caption": "Figure 9:Segmentation accuracy (mIoU) over observed delay and model delay input.",
                "position": 1365
            },
            {
                "img": "https://arxiv.org/html/2510.13714/x10.png",
                "caption": "Figure 10:Segmentation accuracy (mIoU) versus round-trip latency (milliseconds or frames).\nLatency jitter is modeled as a normal distribution.",
                "position": 1370
            },
            {
                "img": "https://arxiv.org/html/2510.13714/x11.png",
                "caption": "Figure 11:Segmentation accuracy (mIoU) versus round-trip latency (milliseconds or frames).\nFurther fine-tuned and evaluated on various local input resolutions.",
                "position": 1384
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]