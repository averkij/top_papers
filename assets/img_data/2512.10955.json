[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.10955/x1.png",
                "caption": "",
                "position": 81
            },
            {
                "img": "https://arxiv.org/html/2512.10955/x2.png",
                "caption": "Figure 2:Training data annotation.Our training data consist of semantically linked image pairs annotated with positive and negative attributes that define their relationships through the shared and differing characteristics. The word cloud on the right highlights the richness and diversity of our attribute annotations, facilitating the training of an open-vocabulary attribute encoder.",
                "position": 95
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Omni-Attribute",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.10955/x3.png",
                "caption": "Figure 3:Dual-objective representation learning.Our attribute-level representation learning is guided by two complementary objectives: agenerative loss(top) tomaximizeembedding information and encourage the capture of fine-grained, high-fidelity details, and acontrastive loss(bottom) that extracts attribute-specific information whilesuppressingirrelevant content.",
                "position": 194
            },
            {
                "img": "https://arxiv.org/html/2512.10955/x4.png",
                "caption": "Figure 4:Model architecture.Our attribute encoder is a LoRA-tuned MLLM followed by a trainable lightweight connector to preserve strong vision-language prior while capable of adapting to our attribute disentanglement task. The image decoder is a frozen generator with trainable IP-Adapter[ip_adapter]modules for personalization.",
                "position": 199
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.10955/figures/pdf/qualitative_comparison.png",
                "caption": "Figure 5:Qualitative comparisons of open-vocabulary attribute personalization.Each row, from top to bottom, shows(i)(i)the reference image-attribute pair and the prompt,(i​i)(ii)results generated using CLIP[clip], DINOv2[dinov2], and Qwen-VL[qwenvl2]embeddings,(i​i​i)(iii)results from editing models, including OmniGen2[omnigen], FLUX-Kontext[flux_kontext], and Qwen-Image-Edit[qwen_image_edit], and(i​v)(iv)results byOmni-Attribute. As shown,Omni-Attributeachieves the best balance between faithfully encoding the target attribute and coherently synthesizing it into new contexts aligned with the prompt, while minimizing undesired “copy-and-paste” artifacts. Full prompts are provided inSec.B.1.",
                "position": 316
            },
            {
                "img": "https://arxiv.org/html/2512.10955/x5.png",
                "caption": "Figure 6:Quantitative comparisons of open-vocabulary attribute personalization.We compareOmni-Attributewith baseline methods on the personalization of two types of attributes: (a)concrete objectsand (b)abstract concepts. We perform the evaluation across two metrics, image naturalness (higher is better) and conditioning fidelity (higher is better), using both MLLM[gpt_4o]and human evaluations.Omni-Attributeconsistently outperforms existing methods, especially for abstract concepts. Full numerical results are inSec.B.1.",
                "position": 321
            },
            {
                "img": "https://arxiv.org/html/2512.10955/x6.png",
                "caption": "Figure 7:Composability of attribute embeddings.From top to bottom, each row shows the input conditions, the effect of a single image-attribute pair, and the compositional results of multiple attributes, showing the composability of our attribute embeddings. The prompt is “A vase is standing against a plain background.”",
                "position": 357
            },
            {
                "img": "https://arxiv.org/html/2512.10955/x7.png",
                "caption": "Figure 8:T-SNE visualizations of attribute embedding spaces.We visualize the embedding spaces of the same 60 animal images across three different attributes and show that this same set of images is distributed differently and meaningfully across varying attributes.",
                "position": 533
            },
            {
                "img": "https://arxiv.org/html/2512.10955/x8.png",
                "caption": "Figure 9:Qualitative results of attribute-oriented image retrieval on CelebA[celeba].Our embeddings enable image retrieval based on a specified attribute.Omni-Attributesurpasses the performance of text-guided retrieval by GPT-4o[gpt_4o]and CLIP[clip].",
                "position": 538
            }
        ]
    },
    {
        "header": "5Limitations",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.10955/x9.png",
                "caption": "Figure 10:Training datasets.Our training image pairs are constructed from two sources:(i)(i)image collection datasets(a–b), with image pairs captured during the same photo session, exhibiting varying positive and negative attributes; and(i​i)(ii)attribute-specific datasets(c–i), with image pairs synthesized via generative or editing models that differ mainly in a single target positive attribute.",
                "position": 810
            },
            {
                "img": "https://arxiv.org/html/2512.10955/x10.png",
                "caption": "Figure 11:Instruction prompt for the first stage of attribute annotation.",
                "position": 819
            }
        ]
    },
    {
        "header": "Appendix BEvaluation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.10955/x11.png",
                "caption": "Figure 12:Instruction prompt for MLLM evaluation.",
                "position": 1355
            },
            {
                "img": "https://arxiv.org/html/2512.10955/x12.png",
                "caption": "Figure 13:Interface of the user study.Given the input conditions (top and right) and the generated image (center), participants are asked to rate three aspects:image naturalness,text fidelity, andattribute fidelityon a 1 (poor) to 5 (excellent) scale using the sliders (left).",
                "position": 1358
            },
            {
                "img": "https://arxiv.org/html/2512.10955/x13.png",
                "caption": "Figure 14:Additional results of attribute disentanglement.Each row shows three generated images (right), which are conditioned on the same reference image (left) and the same textual prompt, but with different attribute inputs (colored boxes). As seen, given the same reference image,Omni-Attributeeffectively extracts attribute-specific representations, enabling the coherent synthesis of the user-specified attribute in new contexts while reducing the leakage of irrelevant visual information from the reference image.",
                "position": 1385
            },
            {
                "img": "https://arxiv.org/html/2512.10955/x14.png",
                "caption": "Figure 15:Practical and creative applications ofOmni-Attribute.From top to bottom, each row demonstrates the practical utility ofOmni-Attributeacross four real-world applications:(i)(i)advertisement image synthesis,(i​i)(ii)hairstyle customization,(i​i​i)(iii)storytelling visualization, and(i​v)(iv)creative content generation.",
                "position": 1390
            }
        ]
    },
    {
        "header": "Appendix CAdditional Results",
        "images": []
    }
]