[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16776/x1.png",
                "caption": "Figure 1:Overview of Kling-Omni, a generalist framework that introduces multimodal visual language as the interaction mechanism, supporting diverse tasks including video generation, editing, and intelligent reasoning.",
                "position": 143
            },
            {
                "img": "https://arxiv.org/html/2512.16776/x2.png",
                "caption": "Figure 2:Attention maps in Multimodal Super-Resolution. The left panel illustrates the map for even-numbered layers, while the right panel shows the map for odd-numbered layers. Skipping the computation for the shaded regions leads to a substantial reduction in computational load and supports accelerated inference with a KV cache.",
                "position": 221
            },
            {
                "img": "https://arxiv.org/html/2512.16776/x3.png",
                "caption": "Figure 3:Online training data pipeline. Raw data is distributed across DP/PP groups using an inference scheduler. After inference, a training scheduler reorders data for balanced workload.",
                "position": 249
            },
            {
                "img": "https://arxiv.org/html/2512.16776/x4.png",
                "caption": "Figure 4:The pipeline schedule in Kling-Omni. The inference pass of VAE/TE are distributed across both data- and pipeline-parallelism, following an interleaved 1F1B pipeline schedule. Pipeine-aware offloading and onloading are introduced to reduce GPU memory consumption without blocking forward or backward pass, and an online load balance scheduler is running on CPU to determine the ulysses parallel size and the workload for each microbatch.",
                "position": 272
            }
        ]
    },
    {
        "header": "3Data System",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16776/figures/Data_Section/Data_Dataset_Distribution.png",
                "caption": "Figure 5:Cross-modal and cross-task data distribution in our constructed data system.",
                "position": 317
            },
            {
                "img": "https://arxiv.org/html/2512.16776/figures/Data_Section/Data_Filter_Pipeline_V4.png",
                "caption": "Figure 6:Data filtering pipeline for video and image samples, illustrating the stages of quality control, temporal consistency, and multimodal alignment.",
                "position": 356
            }
        ]
    },
    {
        "header": "4Model Performance",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16776/x5.png",
                "caption": "Figure 7:Quantitative comparison of Kling-Omni against SOTA methods onreference-based video generation and video editing tasks. Overall GSB is computed over all evaluation metrics.",
                "position": 376
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16776/x6.png",
                "caption": "Figure 8:Examples of image-reference-based video generation.",
                "position": 965
            },
            {
                "img": "https://arxiv.org/html/2512.16776/x7.png",
                "caption": "Figure 9:Examples of element library reference. Kling-Omni supports multi-expression references for the same subject.",
                "position": 968
            },
            {
                "img": "https://arxiv.org/html/2512.16776/x8.png",
                "caption": "Figure 10:Examples of image reference together with element library reference.",
                "position": 971
            },
            {
                "img": "https://arxiv.org/html/2512.16776/",
                "caption": "Figure 11:Examples of new camera angle generation and motion transfer in video reference.",
                "position": 974
            },
            {
                "img": "https://arxiv.org/html/2512.16776/x10.png",
                "caption": "Figure 12:Examples of camera motion transfer in video reference.",
                "position": 977
            },
            {
                "img": "https://arxiv.org/html/2512.16776/x11.png",
                "caption": "Figure 13:Examples of next shot generation and previous shot generation in video reference.",
                "position": 980
            },
            {
                "img": "https://arxiv.org/html/2512.16776/x12.png",
                "caption": "Figure 14:Examples of flexible image and video reference, e.g, sketch reference. The top example shows video generation controlled by the sketch drawing in the reference image, while the bottom example illustrates video stylization that integrates color references into the sequential sketch reference of a video.",
                "position": 983
            },
            {
                "img": "https://arxiv.org/html/2512.16776/x13.png",
                "caption": "Figure 15:Examples of temporal narrative in image reference. The input is a multi-grid image.",
                "position": 986
            },
            {
                "img": "https://arxiv.org/html/2512.16776/x14.png",
                "caption": "Figure 16:Examples of temporal narrative in image reference. The input is a multi-grid image.",
                "position": 989
            },
            {
                "img": "https://arxiv.org/html/2512.16776/x15.png",
                "caption": "Figure 17:Examples of addition, removal, and replacement in video editing.",
                "position": 992
            },
            {
                "img": "https://arxiv.org/html/2512.16776/x16.png",
                "caption": "Figure 18:Examples of reference-imageâ€“guided addition and replacement in video editing.",
                "position": 995
            },
            {
                "img": "https://arxiv.org/html/2512.16776/x17.png",
                "caption": "Figure 19:Examples of background replacement in video editing.",
                "position": 998
            },
            {
                "img": "https://arxiv.org/html/2512.16776/x18.png",
                "caption": "Figure 20:Examples of video stylization in video editing.",
                "position": 1001
            },
            {
                "img": "https://arxiv.org/html/2512.16776/x19.png",
                "caption": "Figure 21:Examples of attribute manipulation in video editing.",
                "position": 1004
            },
            {
                "img": "https://arxiv.org/html/2512.16776/x20.png",
                "caption": "Figure 22:Examples of special effects in video editing.",
                "position": 1007
            },
            {
                "img": "https://arxiv.org/html/2512.16776/x21.png",
                "caption": "Figure 23:Examples of weather change in video editing.",
                "position": 1010
            },
            {
                "img": "https://arxiv.org/html/2512.16776/x22.png",
                "caption": "Figure 24:Example of task composition: Kling-Omni combines the element library of a girl, reference images, and an video stylization prompt to generate a consistent stylized video.",
                "position": 1013
            },
            {
                "img": "https://arxiv.org/html/2512.16776/x23.png",
                "caption": "Figure 25:Two examples of task composition: (top) generating a new camera angle while adding a referenced headband; (bottom) replacing the background, adding a train platform element, and converting the video to a claymation style.",
                "position": 1016
            },
            {
                "img": "https://arxiv.org/html/2512.16776/x24.png",
                "caption": "Figure 26:Examples of visual-signal-guided video generation, which supports intelligent interpretation of user intent from images containing visual signals.",
                "position": 1019
            },
            {
                "img": "https://arxiv.org/html/2512.16776/x25.png",
                "caption": "Figure 27:Examples of reasoning-enhanced generation leveraging world knowledge. The top one demonstrates geospatial reasoning by synthesizing a subject into the specific location defined by GPS coordinates (the Eiffel Tower). The bottom one showcases temporal reasoning, where the model accurately adjusts environmental lighting and shadows on a mountain landscape based on the instruction \"6 hours later.\"",
                "position": 1022
            },
            {
                "img": "https://arxiv.org/html/2512.16776/x26.png",
                "caption": "Figure 28:Examples of reasoning-enhanced generation for logical tasks. The top one demonstrates sorting geometric shapes (tetrahedron, cube, octahedron) in ascending order of face count. The bottom one shows solving a linguistic puzzle by selecting and placing the correct character block to complete two intersecting Chinese idioms.",
                "position": 1025
            }
        ]
    },
    {
        "header": "6Contributors",
        "images": []
    }
]