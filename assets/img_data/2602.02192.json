[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Design for Cost-Efficient Distributed RL",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02192/x1.png",
                "caption": "Figure 1:Asynchronous RL execution inECHO-2with maximum bounded stalenessS=3S=3and publication periodŒ∫=2\\kappa=2. The rollout, generation, and learner update proceed concurrently. Rollout workers uses the latest policy snapshot to generate trajectories into the replay buffer. The learner consumes trajectories from replay buffer and broadcasts a new version of policy to rollout workers in eachŒ∫\\kappatraining steps.ECHO-2generates rollouts at a higher rate than it consumes during training.",
                "position": 363
            }
        ]
    },
    {
        "header": "4System Architecture and Implementation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02192/fig/architecture.png",
                "caption": "Figure 2:System Architecture ofECHO-2. The system adopts a three-plane decomposition for cost-efficient distributed RL. The centralized Learning Plane performs policy optimization using data sampled with a bounded staleness budget. The Data Plane provides a unified interface for task adaptation and manages versioned trajectory storage. The distributed Rollout Plane executes asynchronous generation across workers using pipelined broadcast.",
                "position": 555
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02192/x2.png",
                "caption": "(a)Cost‚Äìquality on AIME24.",
                "position": 628
            },
            {
                "img": "https://arxiv.org/html/2602.02192/x2.png",
                "caption": "(a)Cost‚Äìquality on AIME24.",
                "position": 631
            },
            {
                "img": "https://arxiv.org/html/2602.02192/x3.png",
                "caption": "(b)Effect of bounded stalenessSS.",
                "position": 636
            },
            {
                "img": "https://arxiv.org/html/2602.02192/x4.png",
                "caption": "(c)Bubble ratio vs rollout workers.",
                "position": 641
            },
            {
                "img": "https://arxiv.org/html/2602.02192/x5.png",
                "caption": "Figure 4:Policy broadcast latencyTbcastT_{\\text{bcast}}vs. rollout fleet size.Comparison of three dissemination strategies across different numbers of nodesNN.Star-Limited(with learner uplinkB0‚àà[300,800]‚ÄãMbpsB_{0}\\in[300,800]\\text{Mbps}) suffers from linear latency growth as the learner becomes a bandwidth bottleneck.Tree-Pipelineddissemination, by utilizing chunked peer forwarding, maintains a near-constant broadcast time that scales efficiently with the fleet size, closely matching the idealizedStar-Unlimitedbaseline.",
                "position": 736
            }
        ]
    },
    {
        "header": "6Limitations and Future Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AWorst-Case Staleness Bound under Overlap",
        "images": []
    },
    {
        "header": "Appendix BECHO-2execution",
        "images": []
    },
    {
        "header": "Appendix CSupplementary Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02192/x6.png",
                "caption": "Figure 5:Cost‚Äìquality on AIME for Qwen3-4B.",
                "position": 1180
            },
            {
                "img": "https://arxiv.org/html/2602.02192/x6.png",
                "caption": "Figure 5:Cost‚Äìquality on AIME for Qwen3-4B.",
                "position": 1183
            },
            {
                "img": "https://arxiv.org/html/2602.02192/x7.png",
                "caption": "Figure 6:Effect of bounded stalenessSSon RL quality inECHO-2for Qwen3-4B.",
                "position": 1188
            },
            {
                "img": "https://arxiv.org/html/2602.02192/fig/sandbox.png",
                "caption": "Figure 7:Overview of the Echo-2 Poker Game Alignment system. The Orchestrator (Parallax) interfaces with the Sandbox (‚Ñ∞\\mathcal{E}) to generate Trajectory Logs (‚Ñíi\\mathcal{L}_{i}). The Log-to-Rollout Converter (ùíû\\mathcal{C}) processes these logs into Training Rollouts (ùíü\\mathcal{D}), which are then used by the Trainer (ùíØ\\mathcal{T}) to update the policy parameters (Œ∏\\theta), closing the iterative training loop.",
                "position": 1283
            }
        ]
    },
    {
        "header": "Appendix DBeyond Math: Poker Game Alignment via Sandbox Integration",
        "images": []
    }
]