[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15987/x1.png",
                "caption": "Figure 1:Monthly number of fine-tuned models after a base model’s release, with colors denoting the time when it was created.",
                "position": 74
            }
        ]
    },
    {
        "header": "2Framework for Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15987/x2.png",
                "caption": "Figure 2:(a) Distribution of values forλ𝜆\\lambdaitalic_λ,μ𝜇\\muitalic_μ, andσ𝜎\\sigmaitalic_σ. (b) Pairwise relationships among immediacy (μisubscript𝜇𝑖\\mu_{i}italic_μ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT), longevity (σisubscript𝜎𝑖\\sigma_{i}italic_σ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT), and relative fitness (λisubscript𝜆𝑖\\lambda_{i}italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT) on log-scale axes.",
                "position": 124
            }
        ]
    },
    {
        "header": "3Organization-Specific analysis on model’s importance relative to other models",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15987/x3.png",
                "caption": "Figure 3:Density plots illustrating the cumulative number of fine-tuned models for relative fitness of (1≤λi≤101subscript𝜆𝑖101\\leq\\lambda_{i}\\leq 101 ≤ italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ≤ 10) at the 2-month, 6-month, and 12-month marks, segmented by companies.",
                "position": 151
            }
        ]
    },
    {
        "header": "4Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AData Collection",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15987/x4.png",
                "caption": "Figure 4:Monthly cumulative number of fine-tuned models following the release of the base model, with colors indicating the base models’ creation years, illustrating trends in fine-tuning patterns over time.",
                "position": 306
            }
        ]
    },
    {
        "header": "Appendix BParameterm𝑚mitalic_mandt𝑡titalic_t",
        "images": []
    },
    {
        "header": "Appendix CFitting Equation1on empirical data",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15987/x5.png",
                "caption": "Figure 5:Each subplot represents models, where the x-axis denotes the time, t(month), after release, and the y-axis represents the cumulative count (citsuperscriptsubscript𝑐𝑖𝑡c_{i}^{t}italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT) on a logarithmic scale. Red dots indicate empirical data points, while blue curves correspond to the fitted function using the extracted parameters (λisubscript𝜆𝑖\\lambda_{i}italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT,μisubscript𝜇𝑖\\mu_{i}italic_μ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT,σisubscript𝜎𝑖\\sigma_{i}italic_σ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT).",
                "position": 324
            }
        ]
    },
    {
        "header": "Appendix DCumulative Trajectory of Finetuned Models By Organization",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15987/x6.png",
                "caption": "Figure 6:The cumulative number of fine-tuned models (ctsubscript𝑐𝑡c_{t}italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT) over time (months) for Allen AI, Amazon, Apple, Beijing Academy of Artificial Intelligence(BAAI), CohereAI and DeepSeek.",
                "position": 663
            },
            {
                "img": "https://arxiv.org/html/2502.15987/x7.png",
                "caption": "Figure 7:The cumulative number of fine-tuned models (ctsubscript𝑐𝑡c_{t}italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT) over time (months) for Meta, Google, HuggingFace, IBM, Microsoft, and MistralAI.",
                "position": 666
            },
            {
                "img": "https://arxiv.org/html/2502.15987/x8.png",
                "caption": "Figure 8:The cumulative number of fine-tuned models (ctsubscript𝑐𝑡c_{t}italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT) over time (months) for Nvidia, OpenAI, Qwen, Salesforce, and StabilityAI.",
                "position": 669
            },
            {
                "img": "https://arxiv.org/html/2502.15987/x9.png",
                "caption": "Figure 9:The line plot of the cumulative number of downloads over time (day) for individual models ordered based on the most cumulative downloads. The blue plot is the predictive trajectory using the citation model.",
                "position": 676
            },
            {
                "img": "https://arxiv.org/html/2502.15987/x10.png",
                "caption": "Figure 10:Predicting number of downloads of recently popular DeepSeek models. The black line plot predicts the cumulative number of downloads of DeepSeek models up to 75 days after its release.",
                "position": 688
            }
        ]
    },
    {
        "header": "Appendix EAnalyzing the Cumulative Number of Downloads",
        "images": []
    }
]