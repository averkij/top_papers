[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02580/x1.png",
                "caption": "Figure 1:Comparison of RL and CAPO. (a) RL mixes positive and negative signals. (b) CAPO uses staged curriculum: positive imitation builds stability, negative discrimination improves generalization.",
                "position": 125
            },
            {
                "img": "https://arxiv.org/html/2512.02580/x2.png",
                "caption": "",
                "position": 135
            },
            {
                "img": "https://arxiv.org/html/2512.02580/x3.png",
                "caption": "Figure 2:Illustration of the CAPO scheduling mechanism.\nEach query is processed by the policy model to generate samples,\nwith advantages computed under different optimization algorithms.\nIn Phase 1, only positive-advantage samples are used to ensure stability;\nafter the switch point, Phase 2 incorporates both positive and negative advantages\nto balance stability and generalization.",
                "position": 192
            }
        ]
    },
    {
        "header": "Method",
        "images": []
    },
    {
        "header": "Experimental Setups",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02580/x4.png",
                "caption": "(a) Entropy dynamics",
                "position": 903
            },
            {
                "img": "https://arxiv.org/html/2512.02580/x4.png",
                "caption": "(a) Entropy dynamics",
                "position": 906
            },
            {
                "img": "https://arxiv.org/html/2512.02580/x5.png",
                "caption": "(b) Reward dynamics",
                "position": 911
            }
        ]
    },
    {
        "header": "Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02580/x6.png",
                "caption": "Figure 4:Switch Point Sensitivity: Results on AIME24 and AMC23. While we evaluate CAPO across 8 benchmarks in total, here we present the two most representative results. The curves show that introducing the switch point in the early phase (around 0.2–0.3) yields the best performance, aligning with the theoretical expectation that early positive-only training stabilizes learning before timely inclusion of negative signals enhances generalization.",
                "position": 1056
            },
            {
                "img": "https://arxiv.org/html/2512.02580/x7.png",
                "caption": "Figure 5:Results on two representative out-of-distribution benchmarks\n(Qwen2.5-Math-7B-Base). CAPO achieves an average accuracy of 52.8,\noutperforming GRPO by +6.5%, demonstrating improved robustness under distributional shifts.",
                "position": 1059
            }
        ]
    },
    {
        "header": "Related Work",
        "images": []
    },
    {
        "header": "Conclusion",
        "images": []
    }
]