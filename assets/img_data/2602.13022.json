[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Materials and methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13022/figure_1adataset-ortho.png",
                "caption": "Figure 1:RGB visualisation of the orthophoto spanning the whole dataset region on the left, with the manually annotated test region delineated in light green and shown separately on the right. The manual test annotations are delineated in white.",
                "position": 172
            },
            {
                "img": "https://arxiv.org/html/2602.13022/figure_1bdataset-test-manual-segments.png",
                "caption": "",
                "position": 175
            },
            {
                "img": "https://arxiv.org/html/2602.13022/x1.png",
                "caption": "Figure 2:Visualisation of the different labels on the test set.",
                "position": 218
            },
            {
                "img": "https://arxiv.org/html/2602.13022/x2.png",
                "caption": "Figure 3:Overview of the proposed data fusion method for the pseudo-supervised model training.",
                "position": 228
            }
        ]
    },
    {
        "header": "4Experiments and results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13022/x3.png",
                "caption": "Figure 4:Visualisation of the outputs of selected models. The Detectree2 results are from the best performingFlexicheckpoint.",
                "position": 386
            },
            {
                "img": "https://arxiv.org/html/2602.13022/x4.png",
                "caption": "Figure 5:Standard quartile box plot showing the prediction quality (mIoU) between different input modalities and coarse vs. pseudo-supervision.",
                "position": 552
            },
            {
                "img": "https://arxiv.org/html/2602.13022/x5.png",
                "caption": "Figure 6:Visualisation of the outputs from models using different input modalities and training labels. C. is short for coarse and P. for pseudo-labels, respectively.",
                "position": 555
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "Funding sources",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13022/x6.png",
                "caption": "Figure A.1:The NDVI distribution of initial tree crown segments with the cut-off threshold indicated with the black dashed vertical line.",
                "position": 1354
            },
            {
                "img": "https://arxiv.org/html/2602.13022/x7.png",
                "caption": "Figure A.2:Visualisation of the outputs from the Grounded SAM (denoted G. SAM) and DeepForest models.",
                "position": 1431
            },
            {
                "img": "https://arxiv.org/html/2602.13022/x8.png",
                "caption": "Figure A.3:Visualisation of the outputs from models using different input modalities and training labels. C. is short for coarse, and P. for pseudo-labels, respectively.Allrefers to models using all three input modalities (RGB, RE, and NIR).",
                "position": 1435
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]