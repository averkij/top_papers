[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.01522/x1.png",
                "caption": "Figure 1:Multi-MAV lifting system performing full-pose control of a cable-suspended load. Left: simulation environment used to train the decentralized outer-loop control policy. Right: policy transferred to the real system.",
                "position": 103
            }
        ]
    },
    {
        "header": "2Related works",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.01522/x2.png",
                "caption": "Figure 2:Overview of our method. Dotted lines indicate components only for training; dashed lines indicate those only for real-system deployment; solid lines for both. The training process involves the centralized critic (which observes the privileged global state), direct access to MAV states, and the actuator model that maps rotor speeds to thrust forces. Shared actors make decisions based on local observations, without access to other agentsâ€™ states. The output actions, namely acceleration and body rates, are tracked by a robust model-based low-level controller based on INDI.",
                "position": 183
            }
        ]
    },
    {
        "header": "4Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.01522/x3.png",
                "caption": "Figure 3:Time series of pose tracking results comparing our method and a centralized NMPC method[6]. Our method also includes a setup with 4 MAVs.",
                "position": 268
            },
            {
                "img": "https://arxiv.org/html/2508.01522/x4.png",
                "caption": "Figure 4:Real-world experiments. (A) Snapshot of the test with heterogeneous agents in which one MAV is manually controlled (hacked) to pull out and push in, and the other two MAVs counteract the interference of the hacked MAV. (B) Snapshot of the test where additional load is added to the original load, and the pose error with and without such model mismatch. (C) Snapshot of the case where one MAV fails in flight and the remaining two MAVs manage to control the load.",
                "position": 279
            },
            {
                "img": "https://arxiv.org/html/2508.01522/x5.png",
                "caption": "Figure 5:Positional and attitude errors comparing different action spaces at test time in the Gazebo environment.",
                "position": 304
            },
            {
                "img": "https://arxiv.org/html/2508.01522/x6.png",
                "caption": "Figure 7:Training curves of fully observable, partial augmented, and partially observable observation spaces.",
                "position": 360
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.01522/x7.png",
                "caption": "Figure 8:Time series of the load pose in the heterogeneous agents scenario, comparing the performance of the partially observable policy and the fully observable policy. The time points at which control commands are issued to push the load inward by 0.3 m relative to the desired policy position, or to pull it outward by 0.7 m, are indicated in green (push-in) and orange (pull-out), respectively.",
                "position": 1075
            },
            {
                "img": "https://arxiv.org/html/2508.01522/x8.png",
                "caption": "Figure 9:Time series of load pose in the in-flight failure of one MAV case without sending any commands, comparing a partially observable policy vs a fully observable policy. The thick purple line indicates the moment the MAV fails.",
                "position": 1085
            },
            {
                "img": "https://arxiv.org/html/2508.01522/x9.png",
                "caption": "Figure 10:Time series of load pose in the in-flight failure of one MAV case, comparing a partially observable policy vs a fully observable policy. An attitude command is sent after 10 seconds and a positional command after 20 seconds. The thick purple line indicates the moment the MAV fails.",
                "position": 1088
            },
            {
                "img": "https://arxiv.org/html/2508.01522/x10.png",
                "caption": "Figure 11:Comparison of our method, which is not trained for trajectory tracking, against the centralized NMPC in[6].Left: top view of the flight path of the center of mass of the load while tracking a figure-eight trajectory with a maximum velocity of 1 m/s and maximum acceleration of 0.5 m/s2.Right: position (top) and attitude (bottom) tracking errors time series.",
                "position": 1101
            },
            {
                "img": "https://arxiv.org/html/2508.01522/x11.png",
                "caption": "Figure 12:Training curves using a centralized critic vs using a local critic.",
                "position": 1108
            },
            {
                "img": "https://arxiv.org/html/2508.01522/x12.png",
                "caption": "Figure 13:Training curves comparing different history lengths for the partially observable policy.",
                "position": 1121
            },
            {
                "img": "https://arxiv.org/html/2508.01522/x13.png",
                "caption": "",
                "position": 1131
            }
        ]
    },
    {
        "header": "Appendix ASupplementary Materials",
        "images": []
    }
]