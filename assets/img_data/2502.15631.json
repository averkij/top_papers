[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15631/extracted/6223584/panda2.png",
                "caption": "",
                "position": 33
            }
        ]
    },
    {
        "header": "Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15631/extracted/6223584/Comparison_openAI_final.png",
                "caption": "Fig. 1:Accuracy comparison of OpenAI models gpt-4o, o1-mini, o3-mini (m) and o3-mini (h) on the Omni-MATH benchmark.This figure displays the accuracy of gpt-4o, o1-mini, o3-mini (m) and o3-mini (h) on the Omni-MATH benchmark across disciplines and difficulty tiers. The gpt-4o model fails to attain50%percent5050\\%50 %in any category and consistently lags behind the reasoning models. o1-mini significantly improves accuracy, reaching accuracies of40404040-60%percent6060\\%60 %across all domains, while the o3-models surpass50%percent5050\\%50 %accuracy in all categories. In general, accuracy declines as difficulty increases, with the exception of gpt-4o, which shows accuracy vs. difficulty level imbalance for Tiers2222,3333, and4444.",
                "position": 67
            },
            {
                "img": "https://arxiv.org/html/2502.15631/extracted/6223584/Heatmap_ppt.png",
                "caption": "Fig. 2:Granular performance and reasoning token usage evaluation across domains and difficulty tiers of gpt-4o, o1-mini, o3-mini (m) and o3-mini (h) on the Omni-MATH benchmark.The heatmaps visualize cross-sectional performance scores on a00-100%percent100100\\%100 %scale, represented by the color of the progress bar. The length of the progress bar in each cell represents relative token usage for the test-time scaled models. The extra column is computed by averaging over the rows. The extra row and ‚Äúaverage‚Äù cell are computed independently to give equal weight to multi-domain questions (seeMethods). This figure shows that models allocate more computational resources to problems that require complex combinatorial reasoning (Geometry, Discrete Mathematics and Number Theory), whereas foundational arithmetic and algebra problems demand relatively fewer resources. On average, token usage scales with difficulty level.",
                "position": 76
            }
        ]
    },
    {
        "header": "Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15631/extracted/6223584/Distribution_tokens.png",
                "caption": "Fig. 3:Analysis of the reasoning token distribution, evolution of token region accuracy, and consistency between difficulty tiers and token usage for o1-mini, o3-mini (m) and o3-mini (h).The main panels of the figure display the distribution of the reasoning tokens as a stacked histogram, illustrating the proportion of correctly and incorrectly answered questions in the Omni-MATH dataset by o1-mini, o3-mini (m) and o3-mini (h). The secondaryyùë¶yitalic_y-axis depicts the probability that the model answers incorrectly given that the token count has surpassed the bin threshold (seeMethods). The panels below the histogram contain a filled histogram where the color opacity represents the difficulty level of the math questions (cfr.Fig.A4).\nThe figure shows that o1-mini and o3-mini (m) have a similar reasoning token distribution, with o3-mini (m) giving more correct answers for high-token regions. o3-mini (h) has a good ratio of correct vs. incorrect answers, even for very high token counts. The probability of giving an incorrect answer increases with token count for all models. Finally, the relative proportion of tier levels in each bin reveal a clear transition from a region where the majority of the questions come from the lowest tiers to a region where the majority of the questions come from the highest tiers (for bins with a sufficient amount of data points).",
                "position": 104
            },
            {
                "img": "https://arxiv.org/html/2502.15631/extracted/6223584/Scaling_law.png",
                "caption": "Fig. 4:o3 (mini) thinks harder, not longer.This figure shows that o3-mini (m) does not require longer reasoning chains than o1-mini to achieve better accuracy and that, in general, more proficient models exhibit less accuracy decay as reasoning tokens increase.a,Accuracy per reasoning token, computed by dividing the number of correctly answered questions by the total number of questions in each bin of the histograms inFig.3. Accuracy declines as reasoning token usage increases. Furthermore, we observe that the slope of the lines becomes flatter for higher performing models. These effects are further quantified in the regression analysis (seeMethods).b,The boxplots show the distribution of the reasoning tokens forcorrectlyanswered questions. Further investigation in the left panel ofFig.A6confirms that o1-mini and o3-mini (m) have a very similar token distribution. The token distribution of o3-mini (h) is stretched linearly with respect to the one of o3-mini (m) (Fig.A6right).c,Stratifying plotaby difficulty level shows that, within difficulty tiers, accuracy also decreases with higher reasoning token usage. This suggests that the number of reasoning tokens, rather that difficulty level alone, can be used as a signal for the correctness of the model‚Äôs answer.\nInFig.A7, we show this also holds when stratifying across domains.",
                "position": 108
            }
        ]
    },
    {
        "header": "Discussion",
        "images": []
    },
    {
        "header": "Data and code availability",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "Author contributions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMethods",
        "images": []
    },
    {
        "header": "Appendix BFigures",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15631/extracted/6223584/classification_domains.png",
                "caption": "Fig. A3:Domain distribution of the Omni-MATH dataset.This figure displays the distribution of the primary domains of the Omni-MATH dataset. Math problems that belong to multiple domains are counted for each domain, so the total number of question is higher than inFig.A4.",
                "position": 849
            },
            {
                "img": "https://arxiv.org/html/2502.15631/extracted/6223584/classification_difficulty_tiers.png",
                "caption": "Fig. A4:Classification of difficulty levels in balanced difficulty tiers.This figure shows the difficulty distribution of the Omni-MATH dataset. The difficulty levels are classified in difficulty tiers based on the quartiles of the distribution (without separating difficulty levels).",
                "position": 860
            },
            {
                "img": "https://arxiv.org/html/2502.15631/extracted/6223584/Appendix_fig1.png",
                "caption": "Fig. A5:Analysis of the completion token distribution, and the relationship between completion token usage and accuracy.This figure shows that gpt-4o uses predominantly between200200200200and1000100010001000completion tokens for answering the Omni-MATH problems. We also observe that shorter answers are more likely to lead to a correct final answer. Finally, the relative proportion of tier levels in each bin reveals a clear transition from a region where the majority of the questions come from the lowest tiers to a region where the majority of the questions come from the highest tiers (for bins with a sufficient amount of data points).a,The main panel of this plot displays a stacked histogram of the reasoning tokens used for correctly and incorrectly answered questions in the Omni-MATH dataset. The secondaryyùë¶yitalic_y-axis depicts the probability that the model answers incorrectly given that the token count has surpassed the bin threshold (seeMethods). The subplot contains a filled histogram where the color opacity represents the difficulty level of the math questions (cfr.Fig.A4).b,Accuracy per reasoning token, computed by dividing the number of correctly answered questions by the total number of questions in each bin of the histogram depicted ina. Accuracy declines as completion token usage increases.",
                "position": 864
            },
            {
                "img": "https://arxiv.org/html/2502.15631/extracted/6223584/qqplot.png",
                "caption": "Fig. A6:Comparison of the distribution of reasoning tokens between o1-mini and o3-mini (m), and between o3-mini (m) and o3-mini (h) forcorrectlyanswered problems in the Omni-MATH dataset.This figure compares the token distribution of three OpenAI reasoning models by means of a QQ-plot. We observe that o1-mini and o3-mini (m) have an almost identical reasoning token distribution when we only consider the correctly answered questions. The token distribution of o3-mini (h) is a linearly scaled version of the distribution of o3-mini (m) with a factor slightly larger than2222.",
                "position": 870
            },
            {
                "img": "https://arxiv.org/html/2502.15631/extracted/6223584/Appendix_fig2.png",
                "caption": "Fig. A7:Stratification ofFig.4a for the mathematical domains of the Omni-MATH dataset.This figure displays a stratification ofFig.4a by mathematical domain.\nIn general, accuracy decreases within the domains as the use of reasoning tokens increases.",
                "position": 874
            }
        ]
    },
    {
        "header": "Appendix CTables",
        "images": []
    }
]