[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.09048/x1.png",
                "caption": "",
                "position": 87
            }
        ]
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.09048/x2.png",
                "caption": "Figure 2:Existing challenges in large-scale scene novel view synthesis task under divide-and-conquer paradigm. a) Imbalanced reconstruction complexity across blocks: The intensity of content in different scene regions exhibits significant differences. Areas with dense content require finer subdivision granularity to ensure reconstruction fidelity, while sparser-content regions benefit from coarser partitioning to enhance computational efficiency. b) Supervision mismatch in block-wise optimization: The content of a training view may be divided into multiple blocks after scene partitioning. Due to visibility constraints, the entire training view image does not match the ideal supervision when optimizing the individual block. c) Quality degradation in fusion results: Floater in airspace is an important reason for the quality degradation of fusion results. Since each block is optimized individually, these floaters fit well in the training perspective but degrade the quality of the synthesized novel views, especially in the boundary region.",
                "position": 109
            }
        ]
    },
    {
        "header": "IIRelated Work",
        "images": []
    },
    {
        "header": "IIIPreliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.09048/x3.png",
                "caption": "Figure 3:Overview of our proposed method. We first divide the entire scene and allocates viewpoints with Content-Aware Scene Partition, which jointly considering the complexity of scene content and the computational load distribution across blocks. Subsequently, we optimize each block independently, which is executable either sequentially on a single GPU or in parallel across multiple GPUs. During block optimization, we introduce auxiliary point clouds (aux pts) to address supervision mismatch issues. Pseudo-View Geometry Constraint is conducted to supervise airspace regions and mitigate floater artifacts. Finally, the optimized results from all blocks are integrated to construct a comprehensive Gaussian Representation of the entire scene, enabling interactive novel view synthesis.",
                "position": 214
            }
        ]
    },
    {
        "header": "IVMethod",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.09048/x4.png",
                "caption": "Figure 4:Illustration of the Pseudo-View Geometry Constraint. Typically, artifacts in the airspace can fit RGB images well with inaccurate depth. To address this, we impose constraints on depth to suppress floaters generated in the airspace. For each training view, we generate a pseudo-view by applying slight perturbations to the camera pose. Then we warp the pseudo-view rendered imageIpsersuperscriptsubscriptùêºpserI_{\\text{pse}}^{\\text{r}}italic_I start_POSTSUBSCRIPT pse end_POSTSUBSCRIPT start_POSTSUPERSCRIPT r end_POSTSUPERSCRIPTutilizing rendered depth mapDpsersuperscriptsubscriptùê∑pserD_{\\text{pse}}^{\\text{r}}italic_D start_POSTSUBSCRIPT pse end_POSTSUBSCRIPT start_POSTSUPERSCRIPT r end_POSTSUPERSCRIPTto train-viewIwarprsuperscriptsubscriptùêºwarprI_{\\text{warp}}^{\\text{r}}italic_I start_POSTSUBSCRIPT warp end_POSTSUBSCRIPT start_POSTSUPERSCRIPT r end_POSTSUPERSCRIPT. The loss calculated betweenIwarprsuperscriptsubscriptùêºwarprI_{\\text{warp}}^{\\text{r}}italic_I start_POSTSUBSCRIPT warp end_POSTSUBSCRIPT start_POSTSUPERSCRIPT r end_POSTSUPERSCRIPTand train-view ground-truthIrefgtsuperscriptsubscriptùêºrefgtI_{\\text{ref}}^{\\text{gt}}italic_I start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT start_POSTSUPERSCRIPT gt end_POSTSUPERSCRIPTprovides depth supervision.",
                "position": 407
            }
        ]
    },
    {
        "header": "VExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.09048/x5.png",
                "caption": "Figure 5:Qualitative Results on Mill19 and UrbanScene3D Datasets.",
                "position": 997
            },
            {
                "img": "https://arxiv.org/html/2504.09048/x6.png",
                "caption": "Figure 6:Qualitative Results on MatrixCity Dataset.",
                "position": 1000
            },
            {
                "img": "https://arxiv.org/html/2504.09048/x7.png",
                "caption": "Figure 7:Qualitative Results on MatrixCity Street Scene.",
                "position": 1003
            },
            {
                "img": "https://arxiv.org/html/2504.09048/x8.png",
                "caption": "Figure 8:The visualization of scene partition result. The scene is divided into blocks of different sizes according to the density distribution of the sparse point cloud. And the computational load is balanced among multiple blocks.",
                "position": 1023
            },
            {
                "img": "https://arxiv.org/html/2504.09048/x9.png",
                "caption": "Figure 9:The visualization of Pseudo-View Geometry Constraint ablation experiment. Artifacts are marked within red circles.",
                "position": 1101
            },
            {
                "img": "https://arxiv.org/html/2504.09048/x10.png",
                "caption": "Figure 10:Visualization of block optimization results. Top: local scene partition result. Middle: rendered by points in block. Bottom: rendered by block and auxiliary points, ground-truth image.",
                "position": 1252
            }
        ]
    },
    {
        "header": "VIDiscussion",
        "images": []
    },
    {
        "header": "VIIConclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]