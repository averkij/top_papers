[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14462/x1.png",
                "caption": "",
                "position": 106
            },
            {
                "img": "https://arxiv.org/html/2505.14462/x2.png",
                "caption": "",
                "position": 111
            },
            {
                "img": "https://arxiv.org/html/2505.14462/x3.png",
                "caption": "",
                "position": 116
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14462/x4.png",
                "caption": "Figure 1:Effectiveness of culture-aware RAG.Given a culturally grounded visual question, VLMs enhanced with culture-aware RAG—retrieving relevant Wikipedia documents—generate more accurate answers than their non-RAG counterparts. (see Section5.2).",
                "position": 130
            },
            {
                "img": "https://arxiv.org/html/2505.14462/x5.png",
                "caption": "Figure 2:Ravenea: A MultimodalRetrieval-AugmentedVisual culturEuNdErstAnding dataset.Left: Examples of cVQA and cIC tasks.Middle: Geographic and categorical distribution of cultural references.Right: Performance comparison of 14 VLMs, evaluated with and without integration of our culture-aware retriever. Here, CaCLIP=\"culture-aware CLIP-L/14@224px \".",
                "position": 168
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3RaveneaDataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14462/x6.png",
                "caption": "Figure 3:Raveneaconstruction pipeline.Left: A two-stage retrieval process to match each image with relevant documents.Middle: Decomposition of cultural relevance into three interpretable dimensions to improve human annotation.Right: Postprocessing methods for quality control.",
                "position": 202
            },
            {
                "img": "https://arxiv.org/html/2505.14462/x7.png",
                "caption": "Figure 4:Examples demonstrating the impact of CaCLIP Wikipedia retrieval integration on cVQA and cIC tasks using DeepseekVL2-Tiny.When augmented with culture-aware retrieval, the model exhibits enhanced sensitivity to cultural context.",
                "position": 261
            }
        ]
    },
    {
        "header": "4Culture-aware Multimodal Retriever",
        "images": []
    },
    {
        "header": "5Multimodal Retrieval-augmented Visual Culture Understanding",
        "images": []
    },
    {
        "header": "6Analysis and Further Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14462/x8.png",
                "caption": "Figure 5:Performance improvements for smallest and largest models per family with multimodal retrievers.Scaling models yields marginal gains with various retrievers, even negative effects in both cVQA and cIC tasks. \"ACC.\" denotes accuracy; \"R.S.\" refers to the RegionScore; \"ΔΔ\\Deltaroman_Δ\" represents the change incorporated with RAG compared to the non-RAG baseline.",
                "position": 878
            },
            {
                "img": "https://arxiv.org/html/2505.14462/x9.png",
                "caption": "Figure 6:Performance of 14 VLMs with CaCLIP across different countries.Despite the integration of CaCLIP, disparities in model performance persist across countries.",
                "position": 892
            },
            {
                "img": "https://arxiv.org/html/2505.14462/x10.png",
                "caption": "Figure 7:Ablation for different annotation questions.Combining all three culture-relevant annotation questions yields the best performance.",
                "position": 902
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitations",
        "images": []
    },
    {
        "header": "Appendix BFuture Directions",
        "images": []
    },
    {
        "header": "Appendix CEthics Statement",
        "images": []
    },
    {
        "header": "Appendix DExperimental Details for Multi-modal Retrievers",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14462/x11.png",
                "caption": "Figure 8:Data distributions across eight countries. Restrict the dataset to the images paired with at least one document annotated by human raters as culturally relevant.",
                "position": 1689
            }
        ]
    },
    {
        "header": "Appendix EData Statistics",
        "images": []
    },
    {
        "header": "Appendix FEvaluation of VLMs",
        "images": []
    },
    {
        "header": "Appendix GAnnotation Details",
        "images": []
    },
    {
        "header": "Appendix HA New Metric:RegionScore",
        "images": []
    },
    {
        "header": "Appendix IDetails of Human Evaluation in the cIC Task",
        "images": []
    },
    {
        "header": "Appendix JAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14462/x12.png",
                "caption": "Figure 9:Performance of the 14 VLMs equipped with CaCLIP in cVQA task.",
                "position": 2004
            },
            {
                "img": "https://arxiv.org/html/2505.14462/x13.png",
                "caption": "Figure 10:Performance of the 14 VLMs equipped with CaCLIP in cIC task.",
                "position": 2010
            },
            {
                "img": "https://arxiv.org/html/2505.14462/x14.png",
                "caption": "Figure 11:Average improvement across 14 VLMs in different countries with 4 retrievers for cVQA task.",
                "position": 2778
            },
            {
                "img": "https://arxiv.org/html/2505.14462/x15.png",
                "caption": "Figure 12:Average improvement across 14 VLMs in different countries with 4 retrievers in cIC task.",
                "position": 2784
            }
        ]
    },
    {
        "header": "Appendix KHuman Annotation & Evaluation Interface",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14462/x16.png",
                "caption": "Figure 13:Human evaluation interface for cIC task.",
                "position": 2794
            },
            {
                "img": "https://arxiv.org/html/2505.14462/x17.png",
                "caption": "Figure 14:Human annotation instructions.",
                "position": 2797
            },
            {
                "img": "https://arxiv.org/html/2505.14462/x18.png",
                "caption": "(a)Annotation interface (1).",
                "position": 2800
            },
            {
                "img": "https://arxiv.org/html/2505.14462/x18.png",
                "caption": "(a)Annotation interface (1).",
                "position": 2803
            },
            {
                "img": "https://arxiv.org/html/2505.14462/x19.png",
                "caption": "(b)Annotation interface (2).",
                "position": 2809
            }
        ]
    },
    {
        "header": "Appendix LCorrelation between Automatic Metrics and Human Judgments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14462/extracted/6454016/figures/dataset_examples/ccub_example_1.jpeg",
                "caption": "Table 10:Prompt example for generating captions used to retrieve the Wiki documents.",
                "position": 2830
            },
            {
                "img": "https://arxiv.org/html/2505.14462/extracted/6454016/figures/dataset_examples/cvqa_example_2.jpeg",
                "caption": "Table 11:Prompt exampleswithoutRAG.Multimodal prompt samples with interleaved image are shown for both CVQA and CCUB tasks.",
                "position": 2846
            },
            {
                "img": "https://arxiv.org/html/2505.14462/extracted/6454016/figures/dataset_examples/ccub_example_3.jpg",
                "caption": "",
                "position": 2887
            },
            {
                "img": "https://arxiv.org/html/2505.14462/extracted/6454016/figures/dataset_examples/cvqa_example_4.jpg",
                "caption": "Table 12:Prompt exampleswithRAG.Multimodal prompt samples with interleaved image are shown for both CVQA and CCUB tasks.",
                "position": 2897
            },
            {
                "img": "https://arxiv.org/html/2505.14462/extracted/6454016/figures/dataset_examples/ccub_example_4.jpg",
                "caption": "",
                "position": 2942
            }
        ]
    },
    {
        "header": "Appendix MPrompts",
        "images": []
    }
]