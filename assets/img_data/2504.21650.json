[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.21650/x1.png",
                "caption": "Figure 1.HoloTime accepts either a user-provided or model-generated panoramic image as input, and transforms it into an immersive 360-degree 4D scene, enabling a virtual roaming experience.",
                "position": 146
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": []
    },
    {
        "header": "2.Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.21650/x2.png",
                "caption": "Figure 2.Overview of HoloTime. Beginning with a user-provided or model-generated panoramic image as input, we first use the Panoramic Animator to generate panoramic videos in two stages. The guidance model generates a coarse video in the first stage, which is then refined by the refinement model in the second stage, creating the final panoramic video for 4D reconstruction. Subsequently, we perform Panoramic Space-Time reconstruction to lift the panoramic video to a 4D scene. We employ optical flow for space-time depth estimation to achieve spatial and temporal alignment, thus obtaining a 4D initialized point cloud. Finally, we employ a 4D-GS method for the final scene reconstruction representation.",
                "position": 213
            }
        ]
    },
    {
        "header": "3.Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.21650/x3.png",
                "caption": "Figure 3.Qualitative comparison of text-driven panoramic video generation. Our Panoramic Animator effectively achieves more coherent motion and avoids the occurrence of artifacts.",
                "position": 450
            }
        ]
    },
    {
        "header": "4.Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.21650/x4.png",
                "caption": "Figure 4.Qualitative comparison of image-driven 4D scene generation. Our method can generate more complex and diverse motions in the scene while maintaining spatial and temporal consistency in the dynamic scene.",
                "position": 555
            },
            {
                "img": "https://arxiv.org/html/2504.21650/x5.png",
                "caption": "Figure 5.Ablation study of hybrid data fine-tuning (HDF) and motion guided generation (MGG) for panoramic video generation.",
                "position": 794
            },
            {
                "img": "https://arxiv.org/html/2504.21650/x6.png",
                "caption": "Figure 6.Ablation study of panoramic circular techniques (PCT). We concatenate the left and right ends of the generated frames to check for continuity. PCT effectively prevents the occurrence of discontinuous seam.",
                "position": 807
            },
            {
                "img": "https://arxiv.org/html/2504.21650/x7.png",
                "caption": "Figure 7.Ablation study of temporal loss terms for Space-Time Depth Estimation. Based on the same first frame depth, our temporal loss terms contribute to higher temporal consistency of the middle frameâ€™s depth.",
                "position": 810
            }
        ]
    },
    {
        "header": "5.Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix A360World Dataset Processing",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.21650/x8.png",
                "caption": "Figure 8.Samples in the 360World dataset.",
                "position": 1790
            }
        ]
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    },
    {
        "header": "Appendix CExperiments Details",
        "images": []
    },
    {
        "header": "Appendix DVisual Results",
        "images": []
    }
]