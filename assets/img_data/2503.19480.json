[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19480/x1.png",
                "caption": "",
                "position": 156
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19480/x2.png",
                "caption": "Figure 2:Comparison with prior method[46]. (a) We only need a lightweight denoiser, but (b) achieve stronger performance than DIVA[46], which relies on pre-trained heavy generative models.",
                "position": 168
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Preliminaries of Generative Models",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19480/x3.png",
                "caption": "Figure 3:The two-stage post-training framework for visual enhancements. (a) Overall training pipeline. (b) Continuous generative model as the denoiser. We employ a lightweightFLUX-like DiT[22](but with fewer blocks) and employ a regression loss of flow matching. (c) Discrete generative model as the denoiser. We choose a lightweight Perceiver[17]and employ cross-entropy loss to predict masked tokens.",
                "position": 305
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19480/x4.png",
                "caption": "Figure 4:Qualitative results. Although DIVA achieves better reconstructions of input images, it fails to perceive fine-grained visual details between ‚Äòtongue out‚Äô and ‚Äòwithout tongue out‚Äô.",
                "position": 793
            },
            {
                "img": "https://arxiv.org/html/2503.19480/x5.png",
                "caption": "Figure 5:Performance of CLIP across various conditional visual tokens on MMVP-VLM,i.e.,[CLS]+n%percentùëõn\\%italic_n %[LOCAL].",
                "position": 1021
            },
            {
                "img": "https://arxiv.org/html/2503.19480/x6.png",
                "caption": "Figure 6:Comparison of CLIP with end-to-end and the proposed two-stage training on MMVP-VLM. Here, Cont. and Disc. denote continuous and discrete denoisers. O: OpenAICLIP. S: SigLIP.",
                "position": 1041
            }
        ]
    },
    {
        "header": "6Conclusive Remarks",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Overview",
        "images": []
    },
    {
        "header": "Appendix ARelationship with Prior Works",
        "images": []
    },
    {
        "header": "Appendix BMore Training Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19480/x7.png",
                "caption": "Figure 7:Probability density function of different distributions.",
                "position": 2061
            }
        ]
    },
    {
        "header": "Appendix CDiagrams of Timestamp Sampling",
        "images": []
    },
    {
        "header": "Appendix DMore Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19480/x8.png",
                "caption": "Figure 8:The effect of LoRA on several CLIP backbones.",
                "position": 2086
            },
            {
                "img": "https://arxiv.org/html/2503.19480/x9.png",
                "caption": "Figure 9:The performance of whether to update the denoiser and the projector in Stage-2.",
                "position": 2100
            },
            {
                "img": "https://arxiv.org/html/2503.19480/x10.png",
                "caption": "Figure 10:Qualitative results of CLIP on MMVP-VLM benchmark. The enhanced CLIP overcomes original visual shortcomings in fine-grained details.",
                "position": 2138
            }
        ]
    },
    {
        "header": "Appendix EQualitative Results of CLIPs",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19480/x11.png",
                "caption": "Figure 11:Qualitative results of MLLMs on MMVP-MLLM benchmark. When equipped with our enhanced CLIP, MLLMs produce better vision-centric performance.",
                "position": 2149
            }
        ]
    },
    {
        "header": "Appendix FQualitative Results of MLLMs",
        "images": []
    },
    {
        "header": "Appendix GAlgorithms",
        "images": []
    }
]