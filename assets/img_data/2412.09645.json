[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09645/x1.png",
                "caption": "Figure 1:An Example of Evaluation Agent.Existing evaluation methods typically assess visual generative models by extensively sampling from a fixed benchmark. In contrast, our Evaluation Agent framework requires only a small number of sampled images or videos, tailored to the user’s specific evaluation request. Additionally, it goes beyond providing a simple numerical score by offering detailed explanations to the evaluation conclusions.",
                "position": 151
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09645/x2.png",
                "caption": "Figure 2:Overview of Evaluation Agent Framework.This framework leverages LLM-powered agents for efficient and flexible visual model assessments. As shown, it consists of two stages: (a) the Proposal Stage, where user queries are decomposed into sub-aspects, and prompts are generated, and (b) the Execution Stage, where visual content is generated and evaluated using an Evaluation Toolkit. The two stages interact iteratively to dynamically assess models based on user queries.",
                "position": 390
            },
            {
                "img": "https://arxiv.org/html/2412.09645/extracted/6071683/figures/fig_prompts_abl.png",
                "caption": "Table 2:Evaluation Results Comparison with VBenchHuang et al. (2024a). We evaluated 15 specific ability dimensions in VBench using our Evaluation Agent and compared its results against VBench in terms of conclusion accuracy. The numerical results show the percentages of the Evaluation Agent’s correct predictions falling either within the exact range (left) or within an error margin of one range (right) across ten trials.",
                "position": 428
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09645/extracted/6071683/figures/fig_prompts_abl.png",
                "caption": "Figure 3:Validation on VBench Percentage Dimensions.We conducted additional validation experiments on VBench by increasing the number of prompts in each evaluation. For each model and dimension, lighter bars represent results with the original settings, darker bars with increased sample size. Hatched portions indicate predictions within the exact range, and solid portions within an error margin of one range. Specific numerical results are provided in Table6",
                "position": 1358
            },
            {
                "img": "https://arxiv.org/html/2412.09645/x3.png",
                "caption": "Figure 4:A Case of Open-Ended User Query Evaluation.For open-ended user queries, the Evaluation Agent systematically explores the model’s capabilities in specific areas, starting from basic aspects and gradually delving deeper, culminating in a detailed analysis and summary. Please refer to the AppendixE.2for the complete results.",
                "position": 1446
            }
        ]
    },
    {
        "header": "5Further Discussions",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "8Ethical Considerations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Supplementary",
        "images": []
    },
    {
        "header": "Appendix ADetailed Explanation of Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09645/extracted/6071683/figures/open_dataset_stats.png",
                "caption": "Figure 5:Data Distribution of Open-Ended User Query Dataset.We analyze the constructed open-ended user query dataset from three aspects: General/Specific, Ability, and Specific Domain. The results indicate that our dataset exhibits a relatively balanced distribution across these dimensions.",
                "position": 2181
            }
        ]
    },
    {
        "header": "Appendix BExperiment Implementation Details",
        "images": []
    },
    {
        "header": "Appendix COpen-Ended User Query Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09645/extracted/6071683/figures/fig_performance_ind.png",
                "caption": "Table 7:Evaluation Results Comparison with VBenchHuang et al. (2024a)using Claude as Base Model. We adhere to the same experimental settings and parameters as in the main experiments, but we replace the planning and reasoning agents’ backbones withclaude-3-5-sonnet-20241022as the base model.",
                "position": 2215
            }
        ]
    },
    {
        "header": "Appendix DMore Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09645/extracted/6071683/figures/fig_performance_ind.png",
                "caption": "Figure 6:Performance Comparison across VBench Dimensions for Different Base Models.This visualization highlights the performance of all backbone models, including GPT-4o and Claude models, providing a comprehensive comparison in each dimension for different backbone models. Hatched portions indicate predictions within the exact range, and solid portions within an error margin of one range. Specific numerical results are provided in TableC.2and Table8",
                "position": 3113
            },
            {
                "img": "https://arxiv.org/html/2412.09645/x4.png",
                "caption": "Figure 7:A Common Failure Pattern in Tool Selection.As shown in the figure, Gemini frequently selected an incorrect tool for evaluation. In this case, the model should have selected the “Aesthetic Quality” tool, but it incorrectly chose “Subject Consistency,” leading to inaccuracies in subsequent assessments.",
                "position": 3128
            },
            {
                "img": "https://arxiv.org/html/2412.09645/x5.png",
                "caption": "Figure 8:Common Failures in Generating Sub-Aspects and Finalizing Responses.The figure highlights two critical failures: first, Gemini fails to propose new sub-aspects based on observations from previous rounds, instead engaging in repetitive and meaningless loops without strictly adhering to the provided instructions. Second, this repetitive behavior leads to a non-stopping loop, ultimately failing to generate a meaningful final response to the user’s query.",
                "position": 3131
            },
            {
                "img": "https://arxiv.org/html/2412.09645/x6.png",
                "caption": "Figure 9:A Case of Open-Ended User Query Evaluation.This figure illustrates the Evaluation Agent’s response to the user query, “Can the model generate variations of existing artwork while maintaining the original style?”",
                "position": 3147
            },
            {
                "img": "https://arxiv.org/html/2412.09645/x7.png",
                "caption": "Figure 10:A Case of Open-Ended User Query Evaluation.This figure illustrates the Evaluation Agent’s response to the user query, “How precisely can the user specify object relationships?”",
                "position": 3150
            },
            {
                "img": "https://arxiv.org/html/2412.09645/x8.png",
                "caption": "Figure 11:A Case of Open-Ended User Query Evaluation.This figure illustrates the Evaluation Agent’s response to the user query, “How well the model can generate a specific number of objects?”",
                "position": 3153
            }
        ]
    },
    {
        "header": "Appendix EMore Results",
        "images": []
    }
]