[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02341/x1.png",
                "caption": "Figure 1:Context Position Bias Probing.Left:A short video segment (visualized as a4×44\\times 4grid) is embedded within a much longer padded sequence and processed chronologically.Right:Performance is plotted against each frame’s L1 distance from the question token. The middle-position drop indicates a strong positional bias (“lost-in-the-middle”). TheUpper Boundshows performance without padding, revealing degradation under long-context settings.",
                "position": 87
            },
            {
                "img": "https://arxiv.org/html/2602.02341/x2.png",
                "caption": "Figure 2:Comparison of prior methods with our proposed two-stage method.",
                "position": 128
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02341/x3.png",
                "caption": "Figure 3:Overview of our two-stage training framework.Stage 1:Short clips are concatenated to form a pseudo-long video. The target model generates the query (qiq_{i}) and preferred response (yi+y_{i}^{+}) conditioned on theanchorclip and its caption, while dispreferred responses (yi−y_{i}^{-}) are generated by prompting the model to answer based on non-anchor (distractor) clips, simulating temporal misalignment errors.Stage 2:For unlabeled long videos, an LLM generates the query (qiq_{i}) and reasoning trace (rir_{i}) based on scene synthetic captions.\nThetarget MLLMthen generates the preferred response (yi+y_{i}^{+}) based on corresponding scene context,\nand generates dispreferred responses (yi−y_{i}^{-}) under degraded context (e.g., partial or irrelevant scenes).",
                "position": 150
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02341/figs/niah.png",
                "caption": "(a)Baseline Model with 0% acc in around 1k Frames.",
                "position": 789
            },
            {
                "img": "https://arxiv.org/html/2602.02341/figs/niah.png",
                "caption": "(a)Baseline Model with 0% acc in around 1k Frames.",
                "position": 792
            },
            {
                "img": "https://arxiv.org/html/2602.02341/figs/niah_ours.png",
                "caption": "(b)LongVPO (Ours)",
                "position": 797
            },
            {
                "img": "https://arxiv.org/html/2602.02341/x4.png",
                "caption": "Figure 5:Comparison of Stage 1 training using SFT and DPO. Additional results are provided in the appendix.",
                "position": 826
            },
            {
                "img": "https://arxiv.org/html/2602.02341/x4.png",
                "caption": "Figure 5:Comparison of Stage 1 training using SFT and DPO. Additional results are provided in the appendix.",
                "position": 829
            },
            {
                "img": "https://arxiv.org/html/2602.02341/x5.png",
                "caption": "Figure 6:Performance scaling of LongVPO and InternVL2.5-8B with respect to increasing input frame counts.",
                "position": 834
            },
            {
                "img": "https://arxiv.org/html/2602.02341/x6.png",
                "caption": "Figure 7:Qualitative comparison on long video understanding. More details are in the appendix.",
                "position": 921
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix Overview",
        "images": []
    },
    {
        "header": "Appendix ACore Experimental Design for Context Position Bias Probing (Main Fig.1)",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02341/x7.png",
                "caption": "Figure 8:Core experimental design underpinning the context-bias analysis in Fig.1.",
                "position": 1852
            },
            {
                "img": "https://arxiv.org/html/2602.02341/figs/context_bias_10x10.png",
                "caption": "Figure 9:Compared to the main Fig.1, we shorten the input video context length (by padding blank frames to a10×1010\\times 10grid rather than12×1212\\times 12), yet the same \"lost in the middle\" phenomenon persists.",
                "position": 1855
            }
        ]
    },
    {
        "header": "Appendix BQualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02341/x8.png",
                "caption": "Figure 10:Long Video Understanding: Visual Semantic Understanding",
                "position": 1862
            },
            {
                "img": "https://arxiv.org/html/2602.02341/x9.png",
                "caption": "Figure 11:Long Video Understanding: Cross-temporal Scene Association",
                "position": 1865
            },
            {
                "img": "https://arxiv.org/html/2602.02341/x10.png",
                "caption": "Figure 12:Long Video Understanding: Temporal Order Analysis",
                "position": 1868
            },
            {
                "img": "https://arxiv.org/html/2602.02341/x11.png",
                "caption": "Figure 13:Long Video Understanding: Detail Comprehension",
                "position": 1871
            }
        ]
    },
    {
        "header": "Appendix CImplementation Details",
        "images": []
    },
    {
        "header": "NeurIPS Paper Checklist",
        "images": []
    }
]