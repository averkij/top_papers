[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Model Architecture",
        "images": []
    },
    {
        "header": "4Scaling Law Study for FLAME-MoE",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20225/x1.png",
                "caption": "(a)IsoFLOP profiles.",
                "position": 269
            },
            {
                "img": "https://arxiv.org/html/2505.20225/x1.png",
                "caption": "(a)IsoFLOP profiles.",
                "position": 272
            },
            {
                "img": "https://arxiv.org/html/2505.20225/x2.png",
                "caption": "(b)Parametric loss.",
                "position": 277
            },
            {
                "img": "https://arxiv.org/html/2505.20225/x3.png",
                "caption": "(c)Fitted scaling law.",
                "position": 282
            },
            {
                "img": "https://arxiv.org/html/2505.20225/x4.png",
                "caption": "(d)Generalization ability.",
                "position": 287
            }
        ]
    },
    {
        "header": "5Pretraining FLAME-MoE",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20225/x5.png",
                "caption": "(a)400M-1x (2.0e19 #FLOPs).",
                "position": 682
            },
            {
                "img": "https://arxiv.org/html/2505.20225/x5.png",
                "caption": "(a)400M-1x (2.0e19 #FLOPs).",
                "position": 685
            },
            {
                "img": "https://arxiv.org/html/2505.20225/x6.png",
                "caption": "(b)400M-4x (8.0e19 #FLOPs).",
                "position": 690
            },
            {
                "img": "https://arxiv.org/html/2505.20225/x7.png",
                "caption": "(c)1B-1x (2.4e20 #FLOPs).",
                "position": 695
            },
            {
                "img": "https://arxiv.org/html/2505.20225/x8.png",
                "caption": "Figure 3:Training efficiency of FLAME-MoE-1.7B-10.3B under different parallelization strategies (EP = Expert Parallel, PP = Pipeline Parallel). Dense-1.4B is also included here as a comparison.",
                "position": 702
            },
            {
                "img": "https://arxiv.org/html/2505.20225/x8.png",
                "caption": "",
                "position": 705
            },
            {
                "img": "https://arxiv.org/html/2505.20225/x9.png",
                "caption": "",
                "position": 709
            },
            {
                "img": "https://arxiv.org/html/2505.20225/x10.png",
                "caption": "",
                "position": 713
            },
            {
                "img": "https://arxiv.org/html/2505.20225/x11.png",
                "caption": "",
                "position": 717
            }
        ]
    },
    {
        "header": "6Empirical Analyses",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20225/x12.png",
                "caption": "Figure 4:Evolution of specialization scores for the top-2 most specialized tokens across Experts 0, 1, 6, 9 at the final layer in FLAME-MoE-1.7B-10.3B on the validation set.",
                "position": 764
            },
            {
                "img": "https://arxiv.org/html/2505.20225/x13.png",
                "caption": "Figure 5:Expert co-activation in FLAME-MoE-1.7B-10.3B at the final checkpoint on the validation set. The heatmap shows pairwise co-activation scores among the 16 experts with the highest co-activation across layers 2, 6, 12, and 18. Expert IDs are shown on the axes.",
                "position": 791
            },
            {
                "img": "https://arxiv.org/html/2505.20225/x14.png",
                "caption": "Figure 6:Router saturation across training for FLAME-MoE-1.7B-10.3B. Each subplot shows the average expert selection overlap with the final checkpoint using different top-kùëòkitalic_kvalues (1, 2, 4, 8).",
                "position": 819
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining Efficiency",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20225/extracted/6472968/figures/gpu-utilization/pp-1_ep-2.png",
                "caption": "(a)EP=2, PP=1",
                "position": 1487
            },
            {
                "img": "https://arxiv.org/html/2505.20225/extracted/6472968/figures/gpu-utilization/pp-1_ep-2.png",
                "caption": "(a)EP=2, PP=1",
                "position": 1490
            },
            {
                "img": "https://arxiv.org/html/2505.20225/extracted/6472968/figures/gpu-utilization/pp-1_ep-8.png",
                "caption": "(b)EP=8, PP=1",
                "position": 1495
            }
        ]
    },
    {
        "header": "Appendix BExpert Specialization",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20225/x15.png",
                "caption": "Figure 8:Evolution of expert specialization scores over the course of training in the final layer of FLAME-MoE-1.7B-10.3B. Each subplot tracks the specialization score of one expert across training steps, measured for its top-1 and top-2 most specialized tokens. The score reflects how consistently each expert is selected for specific tokens.",
                "position": 1512
            }
        ]
    },
    {
        "header": "Appendix CExpert Co-activation",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20225/x16.png",
                "caption": "Figure 9:Evolution of expert co-activation in FLAME-MoE-1.7B-10.3B at 10%, 50%, and 100% of pretraining (steps 1,100, 5,500, and 11,029), shown across layers 2, 6, 12, and 18. Each heatmap visualizes pairwise co-activation frequencies among the 16 most frequently co-activated experts at that layer and training stage.",
                "position": 1519
            },
            {
                "img": "https://arxiv.org/html/2505.20225/x17.png",
                "caption": "Figure 10:Expert co-activation patterns in FLAME-MoE models of varying scales at the final checkpoint on the validation set. Each heatmap shows pairwise co-activation scores among the 16 most frequently co-activated experts in the first MoE layer.",
                "position": 1525
            },
            {
                "img": "https://arxiv.org/html/2505.20225/x18.png",
                "caption": "Figure 11:Expert co-activation patterns in FLAME-MoE models of varying scales at the final checkpoint on the validation set. Each heatmap shows pairwise co-activation scores among the 16 most frequently co-activated experts in the final MoE layer.",
                "position": 1528
            },
            {
                "img": "https://arxiv.org/html/2505.20225/x19.png",
                "caption": "Figure 12:Router saturation over the course of training for FLAME-MoE models of different scales under top-1 routing. Each line represents a different MoE layer, showing the trend of average expert selection overlap with the final checkpoint.",
                "position": 1541
            },
            {
                "img": "https://arxiv.org/html/2505.20225/x20.png",
                "caption": "Figure 13:Router saturation over the course of training for FLAME-MoE models of different scales under top-6 routing. Each line represents a different MoE layer, showing the trend of average expert selection overlap with the final checkpoint.",
                "position": 1544
            }
        ]
    },
    {
        "header": "Appendix DRouter Saturation",
        "images": []
    }
]