[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02222/x1.png",
                "caption": "Figure 1:Muon(Jordanet¬†al.,2024)explicitly expands the Pareto frontier over AdamW on the compute-time tradeoff by retaining data efficiency at large batch sizes.",
                "position": 98
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Muon Improves the Compute-Time Tradeoff",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02222/x2.png",
                "caption": "Figure 2:Muon expands the Pareto frontier over AdamW on the compute-time tradeoff at various loss thresholds on Python (top) and DCLM (bottom).",
                "position": 227
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x3.png",
                "caption": "",
                "position": 232
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x4.png",
                "caption": "",
                "position": 233
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x5.png",
                "caption": "",
                "position": 236
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x6.png",
                "caption": "",
                "position": 237
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x7.png",
                "caption": "",
                "position": 238
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x8.png",
                "caption": "Figure 3:Relative data efficiency of Muon over AdamW. (Top) Token ratio vs batch size plots for different target losses across model sizes. (Middle) Corresponding token difference plots. (Bottom) Token difference across model sizes at fixed target losses",
                "position": 272
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x9.png",
                "caption": "",
                "position": 277
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x10.png",
                "caption": "",
                "position": 278
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x11.png",
                "caption": "",
                "position": 281
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x12.png",
                "caption": "",
                "position": 282
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x13.png",
                "caption": "",
                "position": 283
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x14.png",
                "caption": "",
                "position": 286
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x15.png",
                "caption": "",
                "position": 287
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x16.png",
                "caption": "",
                "position": 288
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x17.png",
                "caption": "Figure 4:Token ratios to lossgive a clearer picture of the practical advantages of Muon over AdamW, compared totokens to loss",
                "position": 321
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x18.png",
                "caption": "",
                "position": 326
            }
        ]
    },
    {
        "header": "3Choosing Hyperparameters for Muon",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02222/extracted/6410712/hyperparam_surfaces.png",
                "caption": "Figure 6:Telescoping algorithm applied to weight decay and learning rate. The loss for each model is shifted to a small offset and then presented on a logarithmic scale to exaggerate the minimum for visualization. The red vertical lines highlight the optimal hyperparameters for comparison. The narrowest model (smallestnùëõnitalic_n) has the most grid points (the topmost surface) and the coarsest mesh with cell sizeœµ=1/(m‚àí1)italic-œµ1ùëö1\\epsilon=1/(m-1)italic_œµ = 1 / ( italic_m - 1 )(see Eq.5), withmùëömitalic_mbeing the number of sampled points for each hyperparameter. Submeshes at each subsequent doubling of width (lower surfaces) shrink in size, highlighting how the minima of each surface remain close across widths. Yellow regions indicate higher losses; color scales differ per stage for clarity. The shift in the red lines, marking the minima at each of the four layers, is proportional toŒ±/nùõºùëõ\\alpha/nitalic_Œ± / italic_nin Eq.4. The largest grid is8√ó8888\\times 88 √ó 8, followed by5√ó5555\\times 55 √ó 5,3√ó3333\\times 33 √ó 3and1√ó1111\\times 11 √ó 1, approximately following our geometric schedule in Alg.1. The third level, with cell sizeœµ1/4superscriptitalic-œµ14\\epsilon^{1/4}italic_œµ start_POSTSUPERSCRIPT 1 / 4 end_POSTSUPERSCRIPThas extra points plotted to demonstrate that the true minimum is contained in the internal3√ó3333\\times 33 √ó 3subgrid. The final layer of the telescope, before training the final model, with cell sizeœµ1/8superscriptitalic-œµ18\\epsilon^{1/8}italic_œµ start_POSTSUPERSCRIPT 1 / 8 end_POSTSUPERSCRIPTis3√ó3333\\times 33 √ó 3rather than1√ó1111\\times 11 √ó 1to demonstrate that the selected point is a true minimum (is locally flat).",
                "position": 512
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AContributions",
        "images": []
    },
    {
        "header": "Appendix BReduction of Shampoo and Soap to Muon",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02222/x19.png",
                "caption": "Figure 7:Comparison of the best 1B model training runs on DCLM under AdamW and Muon in steps and wall time. (Top) Muon has a consistently lower training loss compared to Adam given the same number of steps. The difference does not vanish at the end of training (0.020.020.020.02nats at2.5√ó2.5\\times2.5 √óChinchilla-optimal). (Bottom) Muon obtains a target loss faster than Adam.",
                "position": 1358
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x20.png",
                "caption": "",
                "position": 1363
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x21.png",
                "caption": "",
                "position": 1366
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x22.png",
                "caption": "",
                "position": 1367
            }
        ]
    },
    {
        "header": "Appendix CInitial Verification of Muon‚Äôs Performance",
        "images": []
    },
    {
        "header": "Appendix DDeeper Analysis of the Critical Batch Size",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02222/x23.png",
                "caption": "Figure 8:Steps-to-loss vs batch size plots with 500M parameter models for target losses near convergence (log-log scale).",
                "position": 1397
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x24.png",
                "caption": "",
                "position": 1402
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x25.png",
                "caption": "",
                "position": 1403
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x26.png",
                "caption": "",
                "position": 1406
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x27.png",
                "caption": "",
                "position": 1407
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x28.png",
                "caption": "",
                "position": 1408
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x29.png",
                "caption": "Figure 9:Token-optimal batch size plots: architecture ablation (Gemma 3 vs Gemma 2)",
                "position": 1444
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x30.png",
                "caption": "",
                "position": 1449
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x31.png",
                "caption": "",
                "position": 1452
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x32.png",
                "caption": "",
                "position": 1453
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x33.png",
                "caption": "Figure 10:Token-optimal batch size plots: dataset ablation (Python+DCLM and DCLM)",
                "position": 1459
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x34.png",
                "caption": "",
                "position": 1464
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x35.png",
                "caption": "",
                "position": 1467
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x36.png",
                "caption": "",
                "position": 1468
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x37.png",
                "caption": "Figure 11:Token-optimal batch size plots: model size ablation (1B, 2B, and 4B)",
                "position": 1474
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x38.png",
                "caption": "",
                "position": 1479
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x39.png",
                "caption": "",
                "position": 1482
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x40.png",
                "caption": "",
                "position": 1483
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x41.png",
                "caption": "",
                "position": 1486
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x42.png",
                "caption": "",
                "position": 1487
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x43.png",
                "caption": "Figure 12:Compute-time tradeoff plots for all thresholds between Muon vs AdamW: Python (top) and DCLM (bottom).",
                "position": 1500
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x44.png",
                "caption": "",
                "position": 1505
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x45.png",
                "caption": "",
                "position": 1508
            },
            {
                "img": "https://arxiv.org/html/2505.02222/x46.png",
                "caption": "",
                "position": 1509
            }
        ]
    },
    {
        "header": "Appendix EAdditional Compute-Time Tradeoff Plots",
        "images": []
    },
    {
        "header": "Appendix FA Simple Parametric Model of the Critical Batch Size",
        "images": []
    },
    {
        "header": "Appendix GMuon Implementation",
        "images": []
    },
    {
        "header": "Appendix HParameterization of Telescoping and Distribution of Losses",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02222/x47.png",
                "caption": "Figure 13:NcsubscriptùëÅùëêN_{c}italic_N start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPTis the calibration level of the telescope - the level corresponding to the largest width we perform the telescoping up to. Two figures of merit: (left) the percentage of compute saved using telescoping muP as opposed to performing a full grid search on the full model size, forGùê∫Gitalic_Ggrid points, andNùëÅNitalic_Nlevels the base model width. Savings are largest for large grid sweeps (Gùê∫Gitalic_Glarge), large models (NùëÅNitalic_Nlarge), and small truncation ratios (Nc/NsubscriptùëÅùëêùëÅN_{c}/Nitalic_N start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT / italic_Nsmall). (right) The percentage of the total compute spent on the final model run. Because we perform the same amount of compute at each stage of the telescope, this is maximized for smallNc/NsubscriptùëÅùëêùëÅN_{c}/Nitalic_N start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT / italic_N, following the procedure suggested byYanget¬†al.(2022). This measures a form of exploration-exploitation trade off.",
                "position": 1799
            },
            {
                "img": "https://arxiv.org/html/2505.02222/extracted/6410712/loss_values.png",
                "caption": "Figure 14:The distribution of loss values at each level of the telescope in Fig.6, truncated at 2.5 to omit outliers from the initial broad sweep. We note that as the telescope level increases, the distribution becomes tighter around its mean, demonstrating that we are producing a precise estimate of the optimal hyperparameters, and the loss continues to decrease demonstrating that we maintain accuracy. The minimum loss values at each point fit well (R2‚âà1superscriptùëÖ21R^{2}\\approx 1italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ‚âà 1) to a Chinchilla style loss function (L‚Å¢(d)=A/dŒ±+Eùêøùëëùê¥superscriptùëëùõºùê∏L(d)=A/d^{\\alpha}+Eitalic_L ( italic_d ) = italic_A / italic_d start_POSTSUPERSCRIPT italic_Œ± end_POSTSUPERSCRIPT + italic_E) with parameter count (dùëëditalic_d) exponent ofŒ±=0.31ùõº0.31\\alpha=0.31italic_Œ± = 0.31, close to the0.340.340.340.34inHoffmannet¬†al.(2022). Because the tokens are held constant across all scales, the second loss term (B/TŒ≤ùêµsuperscriptùëáùõΩB/T^{\\beta}italic_B / italic_T start_POSTSUPERSCRIPT italic_Œ≤ end_POSTSUPERSCRIPT, forTùëáTitalic_Ttokens andBùêµBitalic_BandŒ≤ùõΩ\\betaitalic_Œ≤fit parameters inHoffmannet¬†al.(2022)) is the same for all models and hence included in the fit constant (E=1.31ùê∏1.31E=1.31italic_E = 1.31) here.",
                "position": 1802
            }
        ]
    },
    {
        "header": "Appendix IThe Maximal Update Parameterization",
        "images": []
    },
    {
        "header": "Appendix JHyperparameter Drift under MuP Scaling",
        "images": []
    }
]