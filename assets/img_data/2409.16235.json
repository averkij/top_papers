[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16235/extracted/5876831/figures/data_bars.png",
                "caption": "Figure 1:Percentage attributed to each data category in the first training phase (left) and annealing phase (right).",
                "position": 167
            },
            {
                "img": "https://arxiv.org/html/2409.16235/extracted/5876831/figures/scaling_law_parallel.png",
                "caption": "Figure 2:Joint Scaling laws obtained when varying the percentage of parallel data.",
                "position": 237
            },
            {
                "img": "https://arxiv.org/html/2409.16235/extracted/5876831/figures/scaling_law_wiki.png",
                "caption": "Figure 3:Joint Scaling laws obtained when repeating vs not-repeating Wikipedia.",
                "position": 248
            },
            {
                "img": "https://arxiv.org/html/2409.16235/extracted/5876831/figures/euro_llm_languages.png",
                "caption": "Figure 4:Percentage of the training corpus attributed to each language, excluding English which accounts to 50% in the first phase and 32.5% during annealing. 5% of the corpus is left for datasets composed of code and math in the first phase and 7% during annealing.",
                "position": 258
            }
        ]
    },
    {
        "header": "3Tokenizer",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16235/extracted/5876831/figures/euro_llm_tokenizer_test_paper.png",
                "caption": "Figure 5:Fertility (pieces / word) obtained with the Mistral, LLaMa-3, Gemma, and EuroLLM tokenizers for a subset of the EuroLLM languages.",
                "position": 279
            }
        ]
    },
    {
        "header": "4Modeling",
        "images": []
    },
    {
        "header": "5Post Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16235/extracted/5876831/figures/euro_llm_hellaswag.png",
                "caption": "Table 3:Example of a dialogue with EuroLLM-1.7B-Instruct. We mark the system, user and model control tokens.",
                "position": 464
            }
        ]
    },
    {
        "header": "6Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16235/extracted/5876831/figures/euro_llm_hellaswag.png",
                "caption": "Figure 6:Results on the Hellaswag (top) and Arc Challenge (bottom) benchmarks. The results were obtained using 10-shot and 25-shot prompts for Hellaswag and Arc Challenge, respectively.",
                "position": 589
            },
            {
                "img": "https://arxiv.org/html/2409.16235/extracted/5876831/figures/euro_llm_arc.png",
                "caption": "",
                "position": 593
            },
            {
                "img": "https://arxiv.org/html/2409.16235/extracted/5876831/figures/euro_llm_flore_en_xx.png",
                "caption": "Figure 7:Comet-22scores on theFlores-200dataset on EN-XX (top) and XX-EN (bottom) language pairs. All models were fine-tuned with the EuroBlocks dataset and the translations were obtained using 0-shot prompts and greedy search.",
                "position": 608
            },
            {
                "img": "https://arxiv.org/html/2409.16235/extracted/5876831/figures/euro_llm_flore_xx_en.png",
                "caption": "",
                "position": 612
            },
            {
                "img": "https://arxiv.org/html/2409.16235/extracted/5876831/figures/euro_llm_wmt.png",
                "caption": "Figure 8:Comet-22scores on the WMT-23 and WMT-24 datasets. All models were fine-tuned with the EuroBlocks dataset and the translations were obtained using 0-shot prompts and greedy search.",
                "position": 616
            }
        ]
    },
    {
        "header": "7Conclusions and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]