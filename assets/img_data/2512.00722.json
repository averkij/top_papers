[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.00722/x1.png",
                "caption": "Figure 1.(a)(b) Pareto frontiers on KV cache selection in long-context input and reasoning scenarios.",
                "position": 172
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.00722/x2.png",
                "caption": "Figure 2.Overview ofSpeContext. (a) Three challenges in existing algorithm paradigm in the long-context reasoning scenario. (b) Key Insight: Distilled language model exhibits similar information focus. (c)Contributions from Section4to Section6",
                "position": 178
            },
            {
                "img": "https://arxiv.org/html/2512.00722/x3.png",
                "caption": "Figure 3.Architecture ofSpeContext.",
                "position": 206
            },
            {
                "img": "https://arxiv.org/html/2512.00722/x4.png",
                "caption": "Figure 4.(a) Inference dataflow of LLM. (b) Existing works on KV cache optimization.",
                "position": 232
            }
        ]
    },
    {
        "header": "2.Background and Related Work",
        "images": []
    },
    {
        "header": "3.Motivation",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.00722/x5.png",
                "caption": "Figure 5.(a) We design the lightweight retrieval head by pruning redundancy and adopt the head-level attention weights for selection. (b)âˆ¼\\sim(e) The detailed implementations of four attention mechanisms supported by the lightweight retrieval head.",
                "position": 362
            }
        ]
    },
    {
        "header": "4.Lightweight Retrieval Head Design",
        "images": []
    },
    {
        "header": "5.Asynchronous Prefetch Dataflow",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.00722/x6.png",
                "caption": "Figure 6.(a) The latency of prefetching with different KV budget and a LLM layer inference. (b) Overlap rate of selected tokens in adjacent generation with different KV budget.",
                "position": 424
            },
            {
                "img": "https://arxiv.org/html/2512.00722/x7.png",
                "caption": "Figure 7.Elastic loading effectively reduces the KV transfer, makingSpeContextoutperform previous works.",
                "position": 451
            }
        ]
    },
    {
        "header": "6.Adaptive Memory Management",
        "images": []
    },
    {
        "header": "7.Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.00722/x8.png",
                "caption": "Figure 8.Accuracy in LongBench on Llama3.1-8B.",
                "position": 690
            },
            {
                "img": "https://arxiv.org/html/2512.00722/x9.png",
                "caption": "Figure 9.Average score on LongWriter benchmark.",
                "position": 780
            },
            {
                "img": "https://arxiv.org/html/2512.00722/x10.png",
                "caption": "Figure 10.End-to-end throughput with a single request (a) in the cloud environment (b) in the edge environment.",
                "position": 966
            },
            {
                "img": "https://arxiv.org/html/2512.00722/x11.png",
                "caption": "Figure 11.Ablation study of three contributions.",
                "position": 984
            }
        ]
    },
    {
        "header": "8.Conclusion",
        "images": []
    },
    {
        "header": "Appendix ALongWriter Benchmark",
        "images": []
    }
]