[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09349/x1.png",
                "caption": "Figure 1:Our method demonstrates its ability to produce diverse animations and preserve consistency of appearance.",
                "position": 78
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09349/x2.png",
                "caption": "Figure 2:The overview of proposed DisPose.",
                "position": 184
            }
        ]
    },
    {
        "header": "4DisPose",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09349/x3.png",
                "caption": "Figure 3:Qualitative comparisons between our method and the state-of-the-art models on the TikTok dataset.",
                "position": 571
            },
            {
                "img": "https://arxiv.org/html/2412.09349/x4.png",
                "caption": "Figure 4:Qualitative comparison of our approach with the dense control-based method.",
                "position": 574
            },
            {
                "img": "https://arxiv.org/html/2412.09349/x5.png",
                "caption": "Figure 5:Comparison of our reference-based dense motion field and existing dense conditions.",
                "position": 660
            },
            {
                "img": "https://arxiv.org/html/2412.09349/x6.png",
                "caption": "Figure 6:The demonstration of cross ID animation from the proposed method.",
                "position": 663
            },
            {
                "img": "https://arxiv.org/html/2412.09349/x7.png",
                "caption": "Figure 7:Qualitative analysis of semantic correspondence. Given a red source point in an image (far left), we use its diffusion feature to retrieve the corresponding point in the image on the right.",
                "position": 666
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09349/x8.png",
                "caption": "Figure 8:Qualitative results of our method for multi-view video generation.",
                "position": 697
            }
        ]
    },
    {
        "header": "7Limitations and Future Works",
        "images": []
    },
    {
        "header": "8Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASampling from Watershed",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09349/x9.png",
                "caption": "Figure 9:More Qualitative Comparisons.",
                "position": 1298
            },
            {
                "img": "https://arxiv.org/html/2412.09349/x10.png",
                "caption": "Figure 10:More Qualitative Comparisons.",
                "position": 1301
            }
        ]
    },
    {
        "header": "Appendix BMore Quantitative Results",
        "images": []
    },
    {
        "header": "Appendix CMore Ablation Analyses",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09349/extracted/6065057/image/abs.png",
                "caption": "Figure 11:Qualitative results about motion field guidance and keypoints correspondence.",
                "position": 1318
            }
        ]
    },
    {
        "header": "Appendix DMore details of motion field guidance",
        "images": []
    },
    {
        "header": "Appendix EMore Ablation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09349/extracted/6065057/image/abs_pipeline.png",
                "caption": "Figure 12:Different hybrid ControlNet architectures.",
                "position": 1492
            }
        ]
    },
    {
        "header": "Appendix FMore Performance Comparisons.",
        "images": []
    },
    {
        "header": "Appendix GTrainable Parameters and Inference Time",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09349/extracted/6065057/image/flow1.png",
                "caption": "Figure 13:Body matched motion field visualization.",
                "position": 1518
            },
            {
                "img": "https://arxiv.org/html/2412.09349/extracted/6065057/image/flow2.png",
                "caption": "Figure 14:Body mismatched motion field visualization.",
                "position": 1521
            },
            {
                "img": "https://arxiv.org/html/2412.09349/extracted/6065057/image/flow_noise.png",
                "caption": "Figure 15:Analysis of background noise.",
                "position": 1524
            }
        ]
    },
    {
        "header": "Appendix HAnalysis of Background Noise.",
        "images": []
    }
]