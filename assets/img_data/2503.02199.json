[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.02199/x1.png",
                "caption": "Figure 1:Illustration of the “Blind Faith in Text” phenomenon in Vision-Language Models (VLMs). These models demonstrate a strong tendency to trust textual data, when it is inconsistent with the visual data or even incorrect.",
                "position": 172
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Empirical Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.02199/x2.png",
                "caption": "Figure 3:Model behaviors over different models when text is corrupted, matched or irrelevant.",
                "position": 607
            },
            {
                "img": "https://arxiv.org/html/2503.02199/x3.png",
                "caption": "Figure 4:Text Preference Ratio (TPR) of all models under different text variations. Most models exhibit high text preference bias when the textual information is relevant even if they are incorrect, especially for open models. Among the proprietary models,Claude-Sonnetexhibits the strongest robustness to corrupted text.",
                "position": 610
            },
            {
                "img": "https://arxiv.org/html/2503.02199/x4.png",
                "caption": "Figure 5:The effect of different factors (prompting, language model size, text relevance) on text bias.Left:Instructional prompts influence modality preference slightly; text preference drops from16.8%percent16.816.8\\%16.8 %to14.2%percent14.214.2\\%14.2 %with “Focus on Image” vs. “Focus on Text” inQwenVL-2-7B.Middle: Scaling the language models (7B, 13B, 34B) inLLaVA-NeXTmodels decreases text bias but only marginally.Right:Increasing text relevance to the query with BM25 retrieval, raises text bias.",
                "position": 952
            },
            {
                "img": "https://arxiv.org/html/2503.02199/x4.png",
                "caption": "",
                "position": 955
            },
            {
                "img": "https://arxiv.org/html/2503.02199/x5.png",
                "caption": "",
                "position": 959
            },
            {
                "img": "https://arxiv.org/html/2503.02199/x6.png",
                "caption": "",
                "position": 963
            },
            {
                "img": "https://arxiv.org/html/2503.02199/x7.png",
                "caption": "Figure 6:Effect of token order on text bias: Placing text tokens before image tokens increases text bias inPhi3.5.",
                "position": 1005
            },
            {
                "img": "https://arxiv.org/html/2503.02199/extracted/6247452/figures/llava-next-7b_vs_behave.png",
                "caption": "Figure 7:Effect of uni-modality certainty on model modality preference. Image/Text certainties are divided into three quantile bins, with higher values indicating higher certainty. Models favor visual data when image certainty is high and text certainty is low, and vice versa. When both certainties are low, models often produceOtheranswers instead of favoring one modality alone.",
                "position": 1008
            },
            {
                "img": "https://arxiv.org/html/2503.02199/extracted/6247452/figures/llava-next-7b_vs_behave.png",
                "caption": "",
                "position": 1011
            },
            {
                "img": "https://arxiv.org/html/2503.02199/extracted/6247452/figures/gpt4o_vs_behavior.png",
                "caption": "",
                "position": 1016
            }
        ]
    },
    {
        "header": "4Investigated Solutions",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.02199/x8.png",
                "caption": "Figure 8:Left:The effect of text-only data in SFT.Right:The effect of data volume in SFT.",
                "position": 1245
            },
            {
                "img": "https://arxiv.org/html/2503.02199/x8.png",
                "caption": "",
                "position": 1248
            },
            {
                "img": "https://arxiv.org/html/2503.02199/x9.png",
                "caption": "",
                "position": 1253
            }
        ]
    },
    {
        "header": "5Theoretical Analysis",
        "images": []
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion and Discussion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails of Theoretical Analysis",
        "images": []
    },
    {
        "header": "Appendix BExperimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.02199/extracted/6247452/figures/examples/164969000.png",
                "caption": "Table 6:Illustration of matching, corrupted, and irrelevant information in a sample from VQAv2.",
                "position": 2663
            },
            {
                "img": "https://arxiv.org/html/2503.02199/extracted/6247452/figures/examples/docvqa_49177.png",
                "caption": "Table 7:Illustration of matching, corrupted, and irrelevant information in a sample from DocVQA.",
                "position": 2725
            },
            {
                "img": "https://arxiv.org/html/2503.02199/extracted/6247452/figures/examples/mathvista_img2.png",
                "caption": "Table 8:Illustration of matching, corrupted, and irrelevant information in a sample from MathVista.",
                "position": 2782
            },
            {
                "img": "https://arxiv.org/html/2503.02199/extracted/6247452/figures/examples/adobe.png",
                "caption": "Table 9:Illustration of matching, corrupted, and irrelevant information in a sample from Brand Recognition.",
                "position": 2840
            }
        ]
    },
    {
        "header": "Appendix CExperimental Results",
        "images": []
    }
]