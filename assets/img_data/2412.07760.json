[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07760/x1.png",
                "caption": "Figure 1:Examples synthesized by SynCamMaster.SynCamMaster generates multiple videos of the same dynamic scene from diverse viewpoints. Videos results are on ourproject page.",
                "position": 151
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07760/x2.png",
                "caption": "Figure 2:Overview of SynCamMaster.Based on a pre-trained text-to-video model, two components are newly introduced: the camera encoder projects the normalized camera extrinsic parameters into embedding space; the multi-view synchronization module, as plugged in each Transformer block, modulates inter-view features under the guidance of inter-camera relationship. Only new components are trainable, while the pre-trained text-to-video model remains frozen.",
                "position": 282
            },
            {
                "img": "https://arxiv.org/html/2412.07760/x3.png",
                "caption": "Figure 3:Data collection process.(a) Illustration of extracting multi-view image data from videos with camera movements, images are from DL3DV-10K(Ling et al.,2024); (b) Example of the rendered multi-view videos from diverse viewpoints; (c) Utilizing general video data as regularization.",
                "position": 325
            },
            {
                "img": "https://arxiv.org/html/2412.07760/extracted/6059181/figures/fig_camera.png",
                "caption": "Figure 4:Illustration of the rendering scene.",
                "position": 335
            }
        ]
    },
    {
        "header": "4Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07760/extracted/6059181/figures/fig_comparison.jpg",
                "caption": "Figure 5:Comparison with state-of-the-art methods.The reference multi-view images of baseline methods (indicated in the blue box) are generated by SynCamMaster. It shows that SynCamMaster generates consistent content (e.g., the details in the red box) from different viewpoints of the same scene, and achieves excellent inter-view synchronization.",
                "position": 398
            },
            {
                "img": "https://arxiv.org/html/2412.07760/extracted/6059181/figures/fig_ablation_data.jpg",
                "caption": "Figure 6:Ablation on the joint training strategy.The captions on both sides represent the composition of the training set, where ”Mono. Video” refers to general monocular videos. It shows that training with the auxiliary multi-view image data and general video data significantly improves the generalization ability and fidelity of the synthesized videos.",
                "position": 612
            },
            {
                "img": "https://arxiv.org/html/2412.07760/extracted/6059181/figures/fig_ablation_prog.jpg",
                "caption": "Figure 7:Ablation on progressive training.",
                "position": 700
            },
            {
                "img": "https://arxiv.org/html/2412.07760/extracted/6059181/figures/fig_v2mv_combine.jpg",
                "caption": "Figure 8:Results of the extension on novel view video synthesis.",
                "position": 709
            }
        ]
    },
    {
        "header": "5Conclusion and Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AIntroduction of the Base Text-to-Video Generation Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07760/extracted/6059181/figures/fig_basemodel.jpg",
                "caption": "Figure 9:Overview of the base text-to-video generation model.",
                "position": 1613
            }
        ]
    },
    {
        "header": "Appendix BData Construction",
        "images": []
    },
    {
        "header": "Appendix CImplementation Details",
        "images": []
    },
    {
        "header": "Appendix DMore Analysis and Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07760/extracted/6059181/figures/fig_plucker.jpg",
                "caption": "Figure 10:Comparasion on using different camera representations.",
                "position": 1698
            },
            {
                "img": "https://arxiv.org/html/2412.07760/extracted/6059181/figures/fig_epi.jpg",
                "caption": "Figure 11:Performance comparison of SynCamMaster with epipolar attention and full attention.",
                "position": 1729
            },
            {
                "img": "https://arxiv.org/html/2412.07760/extracted/6059181/figures/fig_comparison_appendix1.jpg",
                "caption": "Figure 12:More comparison with state-of-the-art methods.",
                "position": 1866
            },
            {
                "img": "https://arxiv.org/html/2412.07760/extracted/6059181/figures/fig_appendix_ours1.jpg",
                "caption": "Figure 13:More synthesized results of SynCamMaster.",
                "position": 1876
            },
            {
                "img": "https://arxiv.org/html/2412.07760/extracted/6059181/figures/fig_appendix_ours2.jpg",
                "caption": "Figure 14:More synthesized results of SynCamMaster.",
                "position": 1879
            },
            {
                "img": "https://arxiv.org/html/2412.07760/extracted/6059181/figures/fig_failure.jpg",
                "caption": "Figure 15:Visualization of failure cases.",
                "position": 1890
            }
        ]
    },
    {
        "header": "Appendix EMore Results",
        "images": []
    }
]