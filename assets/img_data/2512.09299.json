[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09299/x1.png",
                "caption": "Figure 1:Overview of the VABench framework, illustrating its three main components: (1) The audio-video generation tasks being evaluated (T2AV, I2AV, and stereo), (2) the detailed taxonomy of evaluation contexts (e.g., human sounds, complex scenes), and (3) the evaluation pipeline.",
                "position": 118
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3VABench",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09299/img/distribution_new.png",
                "caption": "Figure 2:Data distribution of VABench. The sunburst chart illustrates the hierarchical breakdown of our dataset across the seven major content categories and their sub-divisions.",
                "position": 209
            },
            {
                "img": "https://arxiv.org/html/2512.09299/x2.png",
                "caption": "Figure 3:VABench’s seven content categories, illustrated with example text prompts and representative images.",
                "position": 212
            },
            {
                "img": "https://arxiv.org/html/2512.09299/x3.png",
                "caption": "Figure 4:Overview of the pipeline for benchmark data curation. This process is used to generate the text conditions for T2AV tasks and the image conditions for I2AV tasks.",
                "position": 249
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09299/x4.png",
                "caption": "Figure 5:Qualitative comparison of model performance. We visualize pairwise comparisons across three tasks (I2AV, Stereo, T2AV) by showing key video frames and audio waveforms.",
                "position": 833
            },
            {
                "img": "https://arxiv.org/html/2512.09299/x5.png",
                "caption": "(a)Audio QA",
                "position": 858
            },
            {
                "img": "https://arxiv.org/html/2512.09299/x5.png",
                "caption": "(a)Audio QA",
                "position": 861
            },
            {
                "img": "https://arxiv.org/html/2512.09299/x6.png",
                "caption": "(b)Vision QA",
                "position": 867
            },
            {
                "img": "https://arxiv.org/html/2512.09299/x7.png",
                "caption": "Figure 7:Comparative radar chart of three models: Phase Coherence (Coh Low/Mid/High), Mono Compatibility (Mono Compat), Soundstage Width (Width), Transient Synchronization (Transient Sync), Level Stability, Envelope Correlation (Env Corr), and Imaging Stability. Higher values indicate better performance.",
                "position": 894
            },
            {
                "img": "https://arxiv.org/html/2512.09299/x8.png",
                "caption": "Figure 8:Human preference consistency validation. Each subplot shows one evaluation dimension, where each point denotes a model’s win rate (x: human, y: VABench). A reference line indicates their correlation, with the Pearson coefficient (ρ\\rho) annotated.",
                "position": 907
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Additional evaluation metrics",
        "images": []
    },
    {
        "header": "7Audio-Video Generation Models in Evaluation",
        "images": []
    },
    {
        "header": "8Detail Analysis of Different Tasks",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09299/x9.png",
                "caption": "Figure 9:Doppler-effect video for analysis. From top to bottom, the results correspond to the outputs of Veo3, Sora2 and Wan2.5, respectively.",
                "position": 4419
            },
            {
                "img": "https://arxiv.org/html/2512.09299/x10.png",
                "caption": "Figure 10:Spectrograms of the video generated by three models. From top to bottom, the results correspond to Veo3, Sora2, and Wan2.5, respectively.",
                "position": 4437
            }
        ]
    },
    {
        "header": "9Qualitative Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09299/x11.png",
                "caption": "Figure 11:Lightning video for analysis. From top to bottom, the results correspond to the outputs of Veo3, Wan2.5 and Kling+MMAudio, respectively.",
                "position": 4448
            },
            {
                "img": "https://arxiv.org/html/2512.09299/x12.png",
                "caption": "Figure 12:Spectrograms of the lightning video generated by the three models. From top to bottom, the results correspond to Veo3, Wan2.5, and Kling+MMAudio, respectively. Yellow vertical lines mark the approximate timestamps of visible lightning strikes, while blue vertical lines indicate the thunder events (excluding those occurring at the very beginning of the video).",
                "position": 4465
            },
            {
                "img": "https://arxiv.org/html/2512.09299/x13.png",
                "caption": "Figure 13:Stereo-sound video for analysis. From top to bottom, the results correspond to the outputs of Veo3, Sora2 and Wan2.5, respectively.",
                "position": 4487
            },
            {
                "img": "https://arxiv.org/html/2512.09299/x14.png",
                "caption": "Figure 14:Stereophonic analysis of the video generated by Veo3",
                "position": 4490
            },
            {
                "img": "https://arxiv.org/html/2512.09299/x15.png",
                "caption": "Figure 15:Stereophonic analysis of the video generated by Sora2",
                "position": 4493
            },
            {
                "img": "https://arxiv.org/html/2512.09299/x16.png",
                "caption": "Figure 16:Stereophonic analysis of the video generated by Wan2.5",
                "position": 4496
            },
            {
                "img": "https://arxiv.org/html/2512.09299/x17.png",
                "caption": "Figure 17:A sample from Veo3’s generated results, illustrating both the Doppler effect and stereophonic audio.",
                "position": 4519
            }
        ]
    },
    {
        "header": "10Special samples Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09299/x18.png",
                "caption": "Figure 18:Analysis of the sample generated by Veo3, showing the Doppler effect and left–right channel characteristics.",
                "position": 4527
            },
            {
                "img": "https://arxiv.org/html/2512.09299/x19.png",
                "caption": "Figure 19:Video generated by Sora2, showing dual-channel audio construction reflecting the intended emotional characteristics.",
                "position": 4550
            },
            {
                "img": "https://arxiv.org/html/2512.09299/x20.png",
                "caption": "Figure 20:Analysis of the Sora2-generated sample, showing dual-channel emotional rendering and left–right spectral characteristics.",
                "position": 4553
            },
            {
                "img": "https://arxiv.org/html/2512.09299/x21.png",
                "caption": "(a)Sample generated by Veo3, showing a strong preference toward Caucasian facial features.",
                "position": 4577
            },
            {
                "img": "https://arxiv.org/html/2512.09299/x21.png",
                "caption": "(a)Sample generated by Veo3, showing a strong preference toward Caucasian facial features.",
                "position": 4580
            },
            {
                "img": "https://arxiv.org/html/2512.09299/x22.png",
                "caption": "(b)Sample generated by Seedance, demonstrating a tendency to produce subjects with Asian appearances.",
                "position": 4586
            },
            {
                "img": "https://arxiv.org/html/2512.09299/x23.png",
                "caption": "Figure 22:Video example generated by Veo3 on the T2AV task.",
                "position": 4682
            },
            {
                "img": "https://arxiv.org/html/2512.09299/x24.png",
                "caption": "Figure 23:Video example generated by Seedance+MMAudio on the T2AV task.",
                "position": 4718
            },
            {
                "img": "https://arxiv.org/html/2512.09299/x25.png",
                "caption": "Figure 24:Video example generated by Wan2.5 on the I2AV task.",
                "position": 4760
            },
            {
                "img": "https://arxiv.org/html/2512.09299/x26.png",
                "caption": "Figure 25:Video example generated by Kling+ThinkSound on the I2AV task.",
                "position": 4796
            }
        ]
    },
    {
        "header": "11MLLM Based Evaluation Cases",
        "images": []
    }
]