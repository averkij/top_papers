[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20622/x1.png",
                "caption": "Figure 1.Example interaction with SketchDynamics. (A) A user sketches a storyboard of a traffic-light scene (red stop, yellow flashing, green go). (B-Left) The system proactively raises clarification questions (e.g., whether the car starts moving immediately or after a delay, and whether a car SVG should be provided), and the user responds by selecting options or uploading assets. (B-Right) The user further refines the generated frames by specifying that the yellow light flashes twice and by sketching a velocity–time curve to indicate changes in the car’s speed. (C) At the bottom, the final video clips illustrate the produced animation sequence.",
                "position": 143
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": []
    },
    {
        "header": "2.Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20622/figs/RW.png",
                "caption": "Figure 2.Prior systems(Daviset al.,2008; Kaziet al.,2016,2014b,2014a)treat sketches as symbols to be mapped to animation commands (Top). In contrast, we treat sketches as semantic expressions of motion intent (Bottom), leveraging VLM-based commonsense reasoning and human-AI clarification to interpret why and how motion happens.",
                "position": 213
            },
            {
                "img": "https://arxiv.org/html/2601.20622/x2.png",
                "caption": "Figure 3.Study illustration showing the three stages of our investigation: (1) sketch input and direct generation, (2) clarification through disambiguation cues, and (3) iterative refinement with contextual editing. Each stage involved eight new participants in the study.",
                "position": 251
            }
        ]
    },
    {
        "header": "3.SketchDynamics",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20622/figs/Exploration1.png",
                "caption": "Figure 4.Interface design for the first Stage. (A) The user sketches a frame-level storyboard to explain the law of reflection, drawing incident and reflected light rays on a mirror surface. (B) adds brief notes at the bottom to specify the cues, and (C) generates a preview video from the storyboard.",
                "position": 280
            }
        ]
    },
    {
        "header": "4.Stage 1: Initial Sketch to Animation Authoring",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20622/figs/Stage1Examples.png",
                "caption": "Figure 5.Selected sketches with scripts and animation excerpts from participants’ storyboards in Stage 1. Above the sketch is the script(N/A means no script); the gray text below describes the animation. These examples illustrate common ways sketches were used to convey animation intentions (e.g., translation, scaling, rotation, appearance). The categories shown here are not meant as a strict taxonomy but as illustrative examples; in practice, participants’ free-form drawings were far more varied and often mixed multiple notations within a single sketch. Some animations shown are refined versions to better reflect participants’ intended outcomes.",
                "position": 325
            },
            {
                "img": "https://arxiv.org/html/2601.20622/figs/ambiguity.png",
                "caption": "Figure 6.Illustration of ambiguity clarification. From left to right, the examples show sketch ambiguities ranging from low to high. Top: different types of ambiguities in storyboards, Bottom: the corresponding types of information needed to resolve them, mapped to our clarification strategies.",
                "position": 374
            }
        ]
    },
    {
        "header": "5.Stage 2: Clarification Guide Interpretation",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20622/figs/clarification_cue.png",
                "caption": "Figure 7.Example workflow of the clarification cue mechanism. The system detects ambiguities in sketches, classifies them by level, prepares them as JSON for the interface generator, and prompts users with lightweight clarifications (e.g., confirmation, choice, value input, or asset upload). User responses are used as a supplementary prompt for regeneration and stored in an intent disambiguation memory to avoid repeated queries in future generations.",
                "position": 427
            },
            {
                "img": "https://arxiv.org/html/2601.20622/figs/cue_usage.png",
                "caption": "Figure 8.Heatmap of clarification cue usage across 24 tasks (T1 to T24). Theyy-axis displays four types of clarification cues: Quick confirm, Multiple choice, Fill value, and Text/Upload. Each block in the grid represents the total Cue Count (frequency of use) for a specific cue type within a specific task. Color intensity corresponds to this count, with the scale shown in the legend on the right (darker orange indicates a higher count, up to 3).",
                "position": 459
            }
        ]
    },
    {
        "header": "6.Stage 3: Refinement with Visual Context",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20622/figs/Refinement.png",
                "caption": "Figure 9.Example of the refinement cue workflow. In this case, the user intends the Earth to move along its orbit. The process begins with (1) reviewing the initial video. Next, (2) the system automatically extracts a set of representative keyframes, from which the user selects the one most relevant to their intended edit. (3) On the chosen keyframe, the user adds an arrow to indicate the desired motion. Finally, (4) the system updates the underlying code accordingly and regenerates the video, producing an edited version where the Earth rotates along the orbit.",
                "position": 503
            },
            {
                "img": "https://arxiv.org/html/2601.20622/x3.png",
                "caption": "Figure 10.Authoring process of two participants. The actions are sequentially listed (only resolved clarification cues are listed), and representative steps are selected for illustration.",
                "position": 517
            }
        ]
    },
    {
        "header": "7.Generalizability and Extensibility",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20622/x4.png",
                "caption": "Figure 11.Demonstration of extensibility of SketchDynamics. (A) Video generation example of a car driving along a forest road and exploding. (B) 3D dynamic scene example of block collision simulation. Top: user-drawn sketch storyboard. Middle: interactive clarification and refinement cues. Bottom: corresponding output video frames.",
                "position": 559
            }
        ]
    },
    {
        "header": "8.Discussion",
        "images": []
    },
    {
        "header": "9.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20622/x5.png",
                "caption": "Figure 12.Self-defined Likert scale results across the three stages. Alignment: The generated results reflected what I had in mind. Ease: The interface was easy to use and understand. Flow: The workflow felt smooth without disrupting my creative process. Control: I felt in control of how my input was interpreted. Effort: The effort required to use the system was reasonable. Explore: The system encouraged me to try new ways of expressing my ideas.",
                "position": 1672
            }
        ]
    },
    {
        "header": "Appendix AUser Rating in Three Stages",
        "images": []
    },
    {
        "header": "Appendix BVisual Understanding Capability Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20622/figs/appendix_fig/garden.png",
                "caption": "Table 1.Examples of sketch types and the animation intentions they convey",
                "position": 1883
            },
            {
                "img": "https://arxiv.org/html/2601.20622/figs/appendix_fig/CHIText.png",
                "caption": "",
                "position": 1946
            },
            {
                "img": "https://arxiv.org/html/2601.20622/figs/appendix_fig/shape.png",
                "caption": "",
                "position": 1971
            },
            {
                "img": "https://arxiv.org/html/2601.20622/figs/appendix_fig/move.png",
                "caption": "",
                "position": 1996
            },
            {
                "img": "https://arxiv.org/html/2601.20622/figs/appendix_fig/ghost.png",
                "caption": "",
                "position": 2021
            }
        ]
    },
    {
        "header": "Appendix CUser Study Survey and Interview Questions",
        "images": []
    }
]