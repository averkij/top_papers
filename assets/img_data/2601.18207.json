[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18207/x1.png",
                "caption": "Figure 1:Search agents interleave reasoning and retrieval for question answering (QA). We study QA over scientific literature, contributing an environment for training agents with RL with verifiable rewards (RLVR). We release a training dataset of factoid QA (yellow boxes), a retrieval corpus (purple), and benchmarks.",
                "position": 156
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18207/x2.png",
                "caption": "Figure 2:Left: the ten question-answering categories defined with experts. Right: example question-answer pairs, which are sufficient supervision for RLVR training methods.",
                "position": 212
            },
            {
                "img": "https://arxiv.org/html/2601.18207/x3.png",
                "caption": "Figure 3:Data generation pipeline process. Left, generating the categories fromFigure˜2: LLM summarizes categories from human-written questions in BioASQKritharaet al.(2023); humans brainstorm categories in parallel; humans synthesize both sources into final categories. Right, QA generation: abstracts from PubMed are sampled and passed to an LLM. The LLM s prompted with categories (and other guidance) to generate QAs. A second LLM paraphrases the QAs to limit exact keyword matching.",
                "position": 233
            }
        ]
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18207/x4.png",
                "caption": "Figure 4:Three interesting behaviours that we observe in search agent traces. We bold some words for emphasis. Since traces are long, we abbreviate them, as indicated by ‘[…]’. These are discussed further inSection˜4.3.",
                "position": 424
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Appendix ADataset and Code Availability",
        "images": []
    },
    {
        "header": "Appendix BEthical considerations",
        "images": []
    },
    {
        "header": "Appendix CStatement on use of LLMs",
        "images": []
    },
    {
        "header": "Appendix DFurther explanation of reinforcement learning with verifiable rewards (RLVR)",
        "images": []
    },
    {
        "header": "Appendix EData construction pipeline",
        "images": []
    },
    {
        "header": "Appendix FSystem prompt for Search-R1 LLM training",
        "images": []
    },
    {
        "header": "Appendix GResults: per-category performance",
        "images": []
    },
    {
        "header": "Appendix HResults: a note on PaperQA baseline",
        "images": []
    },
    {
        "header": "Appendix ITraining RLVR details",
        "images": []
    }
]