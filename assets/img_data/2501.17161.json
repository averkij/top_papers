[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.17161/x1.png",
                "caption": "Figure 1:A comparative study of RL and SFT on the visual navigation environmentV-IRL(Yang et¬†al.,2024a)for OOD generalization.OOD curves represent performance on the same task, usinga different textual action space. See detailed descriptions of the task in Section5.1.",
                "position": 269
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.17161/x2.png",
                "caption": "Figure 2:An example of the sequential revision formulation with a verifier.The model generate the next answerùêØt+1outsubscriptsuperscriptùêØoutùë°1\\mathbf{v}^{\\text{out}}_{t+1}bold_v start_POSTSUPERSCRIPT out end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPTconditioned onall previous answers and information(ùêØiout,ùêØtver,0‚â§i‚â§t)subscriptsuperscriptùêØoutùëñsubscriptsuperscriptùêØverùë°0ùëñùë°(\\mathbf{v}^{\\text{out}}_{i},\\mathbf{v}^{\\text{ver}}_{t},0\\leq i\\leq t)( bold_v start_POSTSUPERSCRIPT out end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_v start_POSTSUPERSCRIPT ver end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , 0 ‚â§ italic_i ‚â§ italic_t )from the verifier.",
                "position": 328
            }
        ]
    },
    {
        "header": "4Evaluation Tasks",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.17161/x3.png",
                "caption": "Figure 4:Demonstration of one navigation task inV-IRL.Agent navigates from place to place following the given linguistic navigation instructions inV-IRL. The navigation procedure is shown at the top, with the navigation instructions displayed below. Visual observation-related information is highlighted ingreen, while action-related information is marked inorange.",
                "position": 399
            }
        ]
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.17161/x4.png",
                "caption": "Figure 5:Success rate (%) - GFLOPs trendlines for RL and SFT onGeneralPointsandV-IRL.The top row shows in-distribution performance, while the bottom row shows out-of-distribution performance. Results are presented for both pure language (-L) and vision-language (-VL) variants of each task. ForGeneralPoints, we report the episode success rate, while forV-IRL, we report per-step accuracy with overall success rate inFigures1and19. Detailed evaluation setups (and curve smoothing) are provided inSectionC.3.",
                "position": 431
            },
            {
                "img": "https://arxiv.org/html/2501.17161/x5.png",
                "caption": "Figure 6:Comparison of out-of-distribution performance under rule variants.We report the success rate forGeneralPointsandper-step-accuracyforV-IRL. For each subplot, RL and SFT are trained with equal computation, and their shared initial checkpoint (marked as Init) is set as baseline. Detailed setups are provided inSectionC.3.",
                "position": 434
            },
            {
                "img": "https://arxiv.org/html/2501.17161/x6.png",
                "caption": "Figure 7:Comparison of out-of-distribution performance under visual variants.Similar toFigures5and6, we present both the performance dynamics (shown as lines) and final performance (shown as bars) for visual out-of-distribution evaluations. The previous state-of-the-art onV-IRLVLN mini benchmark(Yang et¬†al.,2024a)is marked inorange. Detailed evaluation setups (and curve smoothing) are provided inSectionC.3.",
                "position": 461
            },
            {
                "img": "https://arxiv.org/html/2501.17161/x7.png",
                "caption": "Figure 8:Recognition vs. success rate for RL and SFT under different variants inGP-VL.We report both in-distribution (red) and OOD (blue) performance of recognition (y-axis) and episode success rate (x-axis). We denote the training compute of each data point via transparency (color bar) while connected (‚ãÜ‚ãÜ\\star‚ãÜ-‚àò\\circ‚àò) pairs are evaluated using same checkpoints. As scaling up post-training compute, RL improves both recognition and overall accuracy, while SFT shows opposite effect.",
                "position": 464
            },
            {
                "img": "https://arxiv.org/html/2501.17161/x8.png",
                "caption": "Figure 9:RL experiments onGP-Lwithout SFT initialization.All trials fail due to poor instruction following capability of the base model.",
                "position": 503
            },
            {
                "img": "https://arxiv.org/html/2501.17161/x9.png",
                "caption": "Figure 10:In-distribution vs. OOD performance growth onGP-L.We record RL experiments with different number of verification iterations (VIter) as scaling up training compute (color transparency).",
                "position": 519
            }
        ]
    },
    {
        "header": "6Conclusion, Discussion and Limitations",
        "images": []
    },
    {
        "header": "7Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails on the General Points Environment",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.17161/extracted/6162311/figures/general-point.jpeg",
                "caption": "Figure 11:An example of our prompt updatefor constructingùêØt+1insubscriptsuperscriptùêØinùë°1\\mathbf{v}^{\\text{in}}_{t+1}bold_v start_POSTSUPERSCRIPT in end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPTusingùêØtin,ùêØtoutsubscriptsuperscriptùêØinùë°subscriptsuperscriptùêØoutùë°\\mathbf{v}^{\\text{in}}_{t},\\mathbf{v}^{\\text{out}}_{t}bold_v start_POSTSUPERSCRIPT in end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_v start_POSTSUPERSCRIPT out end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTandùêØtversubscriptsuperscriptùêØverùë°\\mathbf{v}^{\\text{ver}}_{t}bold_v start_POSTSUPERSCRIPT ver end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. This example provides an optional vision input for VLMs, adding a visual recognition challenge. Thebrownparts marks the task and related information, and thepurpleparts denote the state(st)subscriptùë†ùë°(s_{t})( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )specific info. Theblueandreddescribe the output from themodelandverifier, respectively.",
                "position": 1501
            }
        ]
    },
    {
        "header": "Appendix BDetails on the V-IRL Environment",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.17161/extracted/6162311/figures/virl_2x2grid/2x2grid_example.jpeg",
                "caption": "Figure 13:An example of our prompt updatefor constructingùêØt+1insubscriptsuperscriptùêØinùë°1\\mathbf{v}^{\\text{in}}_{t+1}bold_v start_POSTSUPERSCRIPT in end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPTusingùêØtin,ùêØtoutsubscriptsuperscriptùêØinùë°subscriptsuperscriptùêØoutùë°\\mathbf{v}^{\\text{in}}_{t},\\mathbf{v}^{\\text{out}}_{t}bold_v start_POSTSUPERSCRIPT in end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_v start_POSTSUPERSCRIPT out end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTandùêØtversubscriptsuperscriptùêØverùë°\\mathbf{v}^{\\text{ver}}_{t}bold_v start_POSTSUPERSCRIPT ver end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. Thebrownparts marks the task and related information, and thepurpleparts denote the state(st)subscriptùë†ùë°(s_{t})( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )specific info. Thebrownparts marks the task and related information, and thepurpleparts denote the state(st)subscriptùë†ùë°(s_{t})( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )specific info. Theblueandreddescribe the output from themodelandverifier, respectively.",
                "position": 1740
            }
        ]
    },
    {
        "header": "Appendix CExperimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.17161/x10.png",
                "caption": "Figure 15:SFT experiments onGP-Lwith suboptimal trajectories.Similar to results inFigure5, SFT overfits the training data even we increase the trajectory diversity.",
                "position": 2029
            },
            {
                "img": "https://arxiv.org/html/2501.17161/x11.png",
                "caption": "Figure 16:Ablation studies onGeneralPoints-VLSFT.We ablate the learning rate and report the in-distribution episode success rate (%percent\\%%) of all experiments. None of the experiments shows an increasing trend beyond30%percent3030\\%30 %success rate.",
                "position": 2091
            },
            {
                "img": "https://arxiv.org/html/2501.17161/x12.png",
                "caption": "Figure 17:Ablation studies onGeneralPoints-VLRL.EchoingFigure16, we ablate the learning rate and rreport the in-distribution episode success rate (%percent\\%%) of the two experiments. All components are tunable here.",
                "position": 2129
            },
            {
                "img": "https://arxiv.org/html/2501.17161/x13.png",
                "caption": "Figure 18:Overall success rate (%) - GFLOPs forV-IRL-VLunder rule variants.Due to the nature of the task requiring aggregating a trajectory of correct actions, neither training method achieves reasonable out-of-distribution performance.",
                "position": 2141
            },
            {
                "img": "https://arxiv.org/html/2501.17161/x13.png",
                "caption": "Figure 18:Overall success rate (%) - GFLOPs forV-IRL-VLunder rule variants.Due to the nature of the task requiring aggregating a trajectory of correct actions, neither training method achieves reasonable out-of-distribution performance.",
                "position": 2144
            },
            {
                "img": "https://arxiv.org/html/2501.17161/x14.png",
                "caption": "Figure 19:Out-of-distribution per-step accuracy (%) - GFLOPs forV-IRL-VLunder rule variants with overfited initial checkpoint.Evaluation metric details can be found inSectionC.3.",
                "position": 2149
            },
            {
                "img": "https://arxiv.org/html/2501.17161/extracted/6162311/figures/virl_2x2grid/2x2grid_fail.jpeg",
                "caption": "Figure 21:Failed example ofV-IRLtransition due to overfitting.This phenomenon happens more frequently during scaling up supervised fine-tuning.",
                "position": 2220
            }
        ]
    },
    {
        "header": "Appendix DAdditional Experimental Results",
        "images": []
    }
]