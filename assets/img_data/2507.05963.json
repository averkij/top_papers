[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.05963/x1.png",
                "caption": "Figure 1.Given text prompts, motion trajectories, and reference images for human entities (e.g., man, woman) and non-human entities (e.g., cup, cat, duck), Tora2 generates natural motions following the specified trajectories and textual descriptions for each entity while preserving their unique identities. We highly recommend viewing the videos in the appendix.",
                "position": 102
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": []
    },
    {
        "header": "2.Related work",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.05963/x2.png",
                "caption": "Figure 2.An overview of Tora2, which consists of a decoupled personalization extractorÂ (DPE), a trajectory extractor, a video diffusion transformer, and a gated self-attention mechanism for entity binding.\nThe DPE generates personalization embeddings by combining high-frequency detail information extracted via a decoupled strategy for both human and non-human objects with the low-frequency semantic features obtained from DINOv2. The trajectory extractor encodes provided trajectories into motion embeddings, which are bound to visual entities using a gated self-attention mechanism. These bound motion and personalization conditions are then fed into the video diffusion transformer, employing a motion-guidance fuser and an additional cross-attention layer to achieve both motion and appearance customization for multi entities.",
                "position": 172
            }
        ]
    },
    {
        "header": "3.Methodology",
        "images": []
    },
    {
        "header": "4.Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.05963/x3.png",
                "caption": "Figure 3.Qualitative comparison of appearance and motion customization for multiple entities. The depth map from the first frame of Tora2 is used as a condition for Flux.1 to ensure consistent entity positioning. The joint control of Tora2 distinctly illustrates superior performance in entity fidelity and motion smoothing.",
                "position": 446
            },
            {
                "img": "https://arxiv.org/html/2507.05963/x4.png",
                "caption": "Figure 4.The ablation of personalization embeddings extraction. The proposed DPE adheres to the actions described in the prompts and achieves stable entity fidelity.",
                "position": 456
            },
            {
                "img": "https://arxiv.org/html/2507.05963/x5.png",
                "caption": "Figure 5.Ablation study of cross-model binding strategies. The binding mechanisms facilitate precise motion and appearance control in scenarios involving multiple entities.",
                "position": 461
            }
        ]
    },
    {
        "header": "5.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]