[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2DSD Datasource, Human Ranking, & EXIF Metadata",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05673/extracted/6517417/files/geo_dist.png",
                "caption": "Figure 1:Geographic Distribution by Consented Image Count for AI Training (Log Scale)",
                "position": 113
            }
        ]
    },
    {
        "header": "3DSD Segmentation & Annotation",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05673/extracted/6517417/files/bee_sample.png",
                "caption": "Figure 2:Sample from our dataset showing an image and its corresponding human segmentation",
                "position": 170
            },
            {
                "img": "https://arxiv.org/html/2506.05673/extracted/6517417/files/shot_types_radial_bar_sharp.png",
                "caption": "Figure 3:Shot types radial bar chart showing proportional distribution of different camera angles.",
                "position": 191
            },
            {
                "img": "https://arxiv.org/html/2506.05673/extracted/6517417/files/mood_pie_chart.png",
                "caption": "Figure 4:Mood distribution pie chart highlighting the relative emotional tone across captions.",
                "position": 194
            },
            {
                "img": "https://arxiv.org/html/2506.05673/extracted/6517417/files/ontology_graph_sharp.png",
                "caption": "Figure 5:Ontology graph organizing key caption concepts under high-level photography themes.",
                "position": 197
            },
            {
                "img": "https://arxiv.org/html/2506.05673/extracted/6517417/files/label_histogram.png",
                "caption": "Figure 6:Top 20 most frequently annotated object labels in the DSD dataset. The dominance of both human-related and environmental labels indicates a well-rounded annotation scope.",
                "position": 207
            },
            {
                "img": "https://arxiv.org/html/2506.05673/extracted/6517417/files/cooccurrence_heatmap.png",
                "caption": "Figure 7:Co-occurrence heatmap of the top 15 labels. Darker and higher-valued cells indicate stronger co-occurrence across images, suggesting semantic clustering opportunities.",
                "position": 213
            },
            {
                "img": "https://arxiv.org/html/2506.05673/x1.png",
                "caption": "Figure 8:Latent semantic distances between 20 words in our image scene description.",
                "position": 219
            }
        ]
    },
    {
        "header": "4Benchmarking the DSD",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05673/extracted/6517417/files/label_diversity.jpg",
                "caption": "Figure 9:Relationship between AI and human label counts per image, with point colors indicating F1 scores.",
                "position": 690
            },
            {
                "img": "https://arxiv.org/html/2506.05673/extracted/6517417/files/metrics_distribution.jpg",
                "caption": "(a)Distribution of precision, recall, and F1 scores across all analyzed images.",
                "position": 696
            },
            {
                "img": "https://arxiv.org/html/2506.05673/extracted/6517417/files/metrics_distribution.jpg",
                "caption": "(a)Distribution of precision, recall, and F1 scores across all analyzed images.",
                "position": 699
            },
            {
                "img": "https://arxiv.org/html/2506.05673/extracted/6517417/files/overall_metrics.jpg",
                "caption": "(b)Summary of average precision, recall, and F1 score across the dataset.",
                "position": 704
            },
            {
                "img": "https://arxiv.org/html/2506.05673/extracted/6517417/files/train_loss_vs_epoch.png",
                "caption": "Figure 11:Training and validation loss curves during fine-tuning of LLAVA-NEXT are shown, with training loss represented by an orange solid line and validation loss by a blue dashed line. The model reaches optimal performance with a validation loss of 1.83 at step 1750 (epoch 2.9), after which continued training yields minimal improvement despite further decrease in training loss.",
                "position": 726
            },
            {
                "img": "https://arxiv.org/html/2506.05673/extracted/6517417/files/sony_headphones.jpg",
                "caption": "Figure 12:Comparison of base and fine-tuned LLAVA-NEXT model outputs. The image shows a pair of Sony WH-1000XM3 headphones with black finish and copper accents.",
                "position": 859
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]