[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2The PVUW 2025 Challenge",
        "images": []
    },
    {
        "header": "3MOSE Track Top Solution",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.11326/extracted/6362130/figures/m1_2.jpg",
                "caption": "Figure 1:Overview of the PGMR Framework. Inference and Pseudo-Label-Based Model Selection: Employing five models to conduct inference operations, and the model with optimal performance for different video contents is intelligently selected.",
                "position": 509
            },
            {
                "img": "https://arxiv.org/html/2504.11326/extracted/6362130/figures/m2.png",
                "caption": "Figure 2:Overview of Team DeepSegMa‚Äôs method.",
                "position": 627
            },
            {
                "img": "https://arxiv.org/html/2504.11326/x1.png",
                "caption": "Figure 3:Network Architecture of FVOS.",
                "position": 673
            },
            {
                "img": "https://arxiv.org/html/2504.11326/extracted/6362130/figures/m3_3.png",
                "caption": "Figure 4:Test time data augmentation and multi-scale magnification operations. (a) original image. (b) clockwise by 90‚àò. (c) clockwise by 180‚àò. (d) clockwise by 270‚àò. (e) horizontal flipping. (f) multi-scale magnification.",
                "position": 676
            }
        ]
    },
    {
        "header": "4MeViS Track Top Solution",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.11326/x2.png",
                "caption": "Figure 5:The architecture of Sa2VA[46].The model first encodes the input texts, visual prompts, images, and videos into token embeddings. These tokens are then processed through a large language model (LLM). The output text tokens are used to generate the ‚Äú[SEG]‚Äù token and associated language outputs. The SAM 2 decoder receives the image and video features from the SAM 2 encoder, along with the ‚Äú[SEG]‚Äù token, to generate corresponding image and video masks.",
                "position": 766
            },
            {
                "img": "https://arxiv.org/html/2504.11326/x3.png",
                "caption": "Figure 6:Overview ofReferDINO-Plus. For each video-description pair, we input it into ReferDINO to derive the object masksMrsubscriptùëÄùëüM_{r}italic_M start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPTand the corresponding scoresSrsubscriptùëÜùëüS_{r}italic_S start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPTacross the frames. Then, we select the mask with the highest score as the prompt for SAM2, producing refined masksMssubscriptùëÄùë†M_{s}italic_M start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT. Finally, we fuse the two series of masks through theconditional mask fusionstrategy.",
                "position": 902
            }
        ]
    },
    {
        "header": "5Conclusion and Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]