[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.18671/x1.png",
                "caption": "Figure 1:Comparison of TAPTRv3 with prior methods.Evaluation on four challenging test sets, where ‘Avg. L.’ denotes average video length. Circle colors represent training data composition, ‘Real T. Data’ and ‘Syn. T. Data’ denote the real and synthetic data, larger circle indicates more data. Results show TAPTRv3 achieves the best performance on most datasets. Even with only 11K synthetic training data, TAPTRv3 remains competitive against BootsTAPIR that is trained on 15M additional real data.",
                "position": 80
            },
            {
                "img": "https://arxiv.org/html/2411.18671/x2.png",
                "caption": "Figure 2:Illustration of our key contributions.The darker arrow indicates larger attention weight.\n(a) Point-level feature in TAPTRv2 causes ambiguous attention in cross-attention because of the large variation and distractions. The red point is the tracking point, the green points are sampling points in cross-attention.\n(b) Patch-level spatial context features help eliminate distractions in TAPTRv3.\n(c) TAPTRv2 limits temporal attention in a small window.\n(d) TAPTRv3 extends temporal attention to arbitrary length while utilizing visibility to further adjust the temporal attention weights. Rectangles indicate visibility predictions, the darker the larger.",
                "position": 91
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.18671/x3.png",
                "caption": "Figure 3:Overview (a) and core components (b) (c) of TAPTRv3.After the user has specified the point to track, the point query preparation stage prepares the content feature and spatial context features for this point in the initial frame. When TAPTRv3 receives a new frame, the sequential point tracking stage will regard the content feature and the specified location as the point query, and regard the new frame’s corresponding image feature map as a set of keys and values. The points query, keys, and values will be sent to the multi-layer transformer decoder to detect the tracking point in the new frame. The location prediction will further be used to update the point query’s positional part, providing a better initial position for tracking in the next frame.\nFor clarity, the global matching module is not plotted.",
                "position": 134
            },
            {
                "img": "https://arxiv.org/html/2411.18671/x4.png",
                "caption": "Figure 4:Illustration of patch-level similarity calculation.",
                "position": 242
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.18671/x5.png",
                "caption": "Figure 5:Visualization of ground truth, TAPTRv3, and TAPTRv2.In the early stages of tracking, both TAPTRv2 and TAPTRv3 can produce accurate tracking results. As the camera shifts and the tracking points disappear from the video for an extended period (over 100 frames), TAPTRv2 completely loses the tracking, whereas TAPTRv3 maintains stable tracking. Best view in electronic version.",
                "position": 825
            }
        ]
    },
    {
        "header": "5Visualization",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.18671/x6.png",
                "caption": "Figure 6:Detailed visual illustration of global matching.",
                "position": 1833
            },
            {
                "img": "https://arxiv.org/html/2411.18671/x7.png",
                "caption": "Figure 7:Detailed visual illustration of different methods for computing patch-level similarity.",
                "position": 1843
            },
            {
                "img": "https://arxiv.org/html/2411.18671/x8.png",
                "caption": "Figure 8:Detailed visual illustration of different methods for the updating of spatial context features.",
                "position": 1848
            },
            {
                "img": "https://arxiv.org/html/2411.18671/x9.png",
                "caption": "Figure 9:Visual comparison between TAPTRv2 and TAPTRv3.Best view in electronic version. From the third image in the first row (36th frame), TAPTRv2 loses tracking of the turtle’s flippers, and in the last few frames loses tracking of the turtle shell and the point on the fish below the turtle. TAPTRv3, on the other hand, maintains stable and accurate tracking throughout the video.",
                "position": 1877
            },
            {
                "img": "https://arxiv.org/html/2411.18671/x10.png",
                "caption": "Figure 10:Visual comparison between TAPTRv2 and TAPTRv3.Best view in electronic version. When the goldfish is about to swim out of the frame from right to left (119th frame), TAPTRv2 loses many target tracking points. Afterward, the goldfish swims back from left to right, and starting from the 358th frame, the video shows the other side of the goldfish, where the original target tracking points are occluded. However, TAPTRv2 incorrectly estimates them as visible or on another fish. TAPTRv3, on the other hand, maintains the correct estimation. Until the last dozens of frames, when the goldfish turns around again, TAPTRv3 successfully detects the initial target tracking points, estimates them as visible, and provides accurate positions.",
                "position": 1882
            },
            {
                "img": "https://arxiv.org/html/2411.18671/x11.png",
                "caption": "Figure 11:Visual comparison between TAPTRv2 and TAPTRv3.Best view in electronic version. Over time, TAPTRv2 incorrectly estimates the location and visibility of points on jellyfish, and the error accumulates, while TAPTRv3’s results are more accurate.",
                "position": 1888
            },
            {
                "img": "https://arxiv.org/html/2411.18671/x12.png",
                "caption": "Figure 12:Visual comparison between TAPTRv3 with and without the auto-triggered global matching.After the occluder appears and then disappears, TAPTRv3 without auto-triggered global matching takes about 70 frames to successfully re-track the target tracking points. However, with the help of global matching, this process takes only two frames.",
                "position": 1893
            },
            {
                "img": "https://arxiv.org/html/2411.18671/x13.png",
                "caption": "Figure 13:Visual comparison between TAPTRv3 with and without the auto-triggered global matching.After the occluder appears and then disappears, TAPTRv3 without auto-triggered global matching takes about 14 frames to successfully re-track the target tracking points. However, with the help of global matching, this process takes only two frames.",
                "position": 1898
            },
            {
                "img": "https://arxiv.org/html/2411.18671/x14.png",
                "caption": "Figure 14:Additional visualizations of TAPTRv3’s robust predictions.",
                "position": 1903
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]