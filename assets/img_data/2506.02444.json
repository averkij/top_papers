[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.02444/extracted/6514268/assets/figs/teaser_v2.png",
                "caption": "Figure 1:Different HOI generation methods.\nApproaches like MDM[53]rely on limited mocap data without visual guidance, resulting in blurred boundaries and compromised plausibility and consistency.\nMethods like Animate Anyone[18]leverage large-scale visual priors but exhibit distortions and inconsistencies because of inadequate physical awareness.\nOur method marries visual priors with 3D motion constraints and eliminates dependency on pre-defined object models or pose guidance.",
                "position": 112
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.02444/extracted/6514268/assets/figs/pipeline_v2.png",
                "caption": "Figure 2:Our method comprises: (1) A synchronous diffusion model that jointly generates HOI videos and motions (Sec.3.3). (2) A vision-aware interaction diffusion model that generates 3D hand pose trajectories and point clouds from the formerâ€™s outputs (Sec.3.4), then feeds them back into the synchronized denoising process to establish closed-loop optimization (Sec.3.5).",
                "position": 180
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.02444/extracted/6514268/assets/figs/comparison_video_no_more.png",
                "caption": "Figure 3:Visualization of videos. Red boxes highlight artifacts such as deformation, hallucinations, distortion, and implausible motions.",
                "position": 570
            },
            {
                "img": "https://arxiv.org/html/2506.02444/extracted/6514268/assets/figs/comparison_motion.png",
                "caption": "Figure 4:Quantitative comparison of motion generation results. Our method achieves smoother motions, sharper object point clouds, and higher instruction compliance.",
                "position": 574
            },
            {
                "img": "https://arxiv.org/html/2506.02444/extracted/6514268/assets/figs/user_study_video.png",
                "caption": "",
                "position": 641
            },
            {
                "img": "https://arxiv.org/html/2506.02444/extracted/6514268/assets/figs/user_study_motion.png",
                "caption": "",
                "position": 644
            },
            {
                "img": "https://arxiv.org/html/2506.02444/extracted/6514268/assets/figs/zeroshot_large.png",
                "caption": "Figure 6:Zero-shot inference on real-world data. We collect data with everyday objects and generate high-fidelity, plausible HOI videos and 3D motions, demonstrating the generalizability of our method.",
                "position": 648
            },
            {
                "img": "https://arxiv.org/html/2506.02444/extracted/6514268/assets/figs/ablation_training_loss_short.png",
                "caption": "Figure 7:Video loss curves of different variants during the training process.",
                "position": 766
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments and Disclosure of Funding",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "SViMo: Synchronized Diffusion for Video and Motion Generation in Hand-object Interaction Scenarios (Appendix)",
        "images": []
    },
    {
        "header": "Appendix APseudo-code for the Training and Inference Phases",
        "images": []
    },
    {
        "header": "Appendix BMore Implementation Details",
        "images": []
    },
    {
        "header": "Appendix CAdditional Experimential Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.02444/extracted/6514268/assets/figs/appendix_ours.png",
                "caption": "Figure 8:Visualization of the generated videos and motions. We provide vivid demonstrations in the video of the supplementary materials.",
                "position": 2150
            },
            {
                "img": "https://arxiv.org/html/2506.02444/extracted/6514268/assets/figs/appendix_compare1.png",
                "caption": "Figure 9:Comparison of video generation results. The artifacts in the videos generated by the baseline methods are highlighted inred. Refer to the video in the supplementary material for vivid demonstrations.",
                "position": 2163
            },
            {
                "img": "https://arxiv.org/html/2506.02444/extracted/6514268/assets/figs/appendix_compare2.png",
                "caption": "Figure 10:Comparison of video generation results. The artifacts in the videos generated by the baseline methods are highlighted inred. Refer to the video in the supplementary material for vivid demonstrations.",
                "position": 2168
            },
            {
                "img": "https://arxiv.org/html/2506.02444/extracted/6514268/assets/figs/appendix_compare3.png",
                "caption": "Figure 11:Comparison of video generation results. The artifacts in the videos generated by the baseline methods are highlighted inred. Refer to the video in the supplementary material for vivid demonstrations.",
                "position": 2172
            },
            {
                "img": "https://arxiv.org/html/2506.02444/extracted/6514268/assets/figs/appendix_failurecases.png",
                "caption": "Figure 12:Visualization of failure cases.Redboxes highlight artifacts.",
                "position": 2185
            }
        ]
    },
    {
        "header": "Appendix DLimitations and Future Work",
        "images": []
    },
    {
        "header": "Appendix EBroader Impacts",
        "images": []
    }
]