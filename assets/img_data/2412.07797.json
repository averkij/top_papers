[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07797/x1.png",
                "caption": "",
                "position": 74
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07797/x2.png",
                "caption": "Figure 2:The architecture of the model:(a)RVQ-VAEis a hierarchical residual vector quantization variational autoencoder that discretizes continuous motion sequences with high precision; (b) aRq Hierarchical Causal Transformergenerates base motion sequences autoregressively while inferring residuals across layers.",
                "position": 128
            }
        ]
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07797/x3.png",
                "caption": "Figure 3:The architecture of prompt engineering for enhanced model inference",
                "position": 148
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07797/x4.png",
                "caption": "Figure 4:Comparison of the generation quality between our model and the latest SOTA motion generation models[16,18].",
                "position": 705
            },
            {
                "img": "https://arxiv.org/html/2412.07797/extracted/6047150/3931729069774_.pic.jpg",
                "caption": "Figure 5:HumanML3D User Study",
                "position": 795
            },
            {
                "img": "https://arxiv.org/html/2412.07797/x5.png",
                "caption": "(a)w/o PnQ condition",
                "position": 963
            },
            {
                "img": "https://arxiv.org/html/2412.07797/x5.png",
                "caption": "(a)w/o PnQ condition",
                "position": 966
            },
            {
                "img": "https://arxiv.org/html/2412.07797/x6.png",
                "caption": "(b)w/ PnQ condition",
                "position": 972
            }
        ]
    },
    {
        "header": "5Limitations and Discussion",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AOverview",
        "images": []
    },
    {
        "header": "Appendix BAblation study of Mogo architecture",
        "images": []
    },
    {
        "header": "Appendix CUser Study Eval Metrics",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07797/extracted/6047150/user_study_metrics.jpg",
                "caption": "Figure 7:User study eval metrics",
                "position": 1925
            }
        ]
    },
    {
        "header": "Appendix DTraining Loss and FID",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07797/extracted/6047150/training_loss.png",
                "caption": "(a)Training Process Loss",
                "position": 1935
            },
            {
                "img": "https://arxiv.org/html/2412.07797/extracted/6047150/training_loss.png",
                "caption": "(a)Training Process Loss",
                "position": 1938
            },
            {
                "img": "https://arxiv.org/html/2412.07797/extracted/6047150/training_process_fid.png",
                "caption": "(b)Training Process FID",
                "position": 1944
            }
        ]
    },
    {
        "header": "Appendix EPrompt Engineering",
        "images": []
    },
    {
        "header": "Appendix FLength Restriction",
        "images": []
    },
    {
        "header": "Appendix GVisualizations of Mogoâ€™s Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07797/extracted/6047150/more_visual.png",
                "caption": "Figure 9:Demonstration of generative capabilities for open vocabulary and ultra-long sequences.",
                "position": 2039
            }
        ]
    },
    {
        "header": "Appendix HCode",
        "images": []
    }
]