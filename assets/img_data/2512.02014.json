[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02014/x1.png",
                "caption": "Figure 1:We presentTuna, a native unified multimodal model built on a unified visual representation, enabling diverse multimodal understanding and generation capabilities such as image and video understanding, image and video generation, and image editing.",
                "position": 213
            }
        ]
    },
    {
        "header": "2Our Method:Tuna",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02014/assets_our/overview4.png",
                "caption": "Figure 2:Overview of theTunaarchitecture. Our model employs a VAE encoder and a representation encoder to construct unified visual representations, which are then combined with text tokens and processed by an LLM decoder. The decoder performs autoregressive text generation for understanding tasks and flow-matching-based visual generation for generation tasks.∗During visual generation, noise is added to the visual tokens to enable diffusion-based generation.",
                "position": 259
            },
            {
                "img": "https://arxiv.org/html/2512.02014/x2.png",
                "caption": "Figure 3:Attention masks in the LLM decoder for understanding and generation tasks.∗indicates that the visual tokens are noised.",
                "position": 340
            },
            {
                "img": "https://arxiv.org/html/2512.02014/x3.png",
                "caption": "Table 1:Comparisons betweenTunaand baseline models on multimodal understanding benchmarks. Results with model size greater than 13B aregrayed.Bold: best results among each section.Underline: second-best. * indicates the results based on our evaluation scripts.",
                "position": 362
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02014/x3.png",
                "caption": "Table 2:Image generation results on GenEval.†refers to methods using LLM rewriters.Bold: best results among each section.Underline: second-best.",
                "position": 805
            },
            {
                "img": "https://arxiv.org/html/2512.02014/x3.png",
                "caption": "Table 3:Image generation results on DPG-Bench.Bold: best results among each section.Underline: second-best.",
                "position": 1172
            },
            {
                "img": "https://arxiv.org/html/2512.02014/x3.png",
                "caption": "Table 4:Image editing results on ImgEdit-Bench and GEdit-Bench. For ImgEdit-Bench, we test editing performance across various dimensions, including ‘Add’, ‘Adjust’, ‘Extract’, ‘Replace’, ‘Remove’, ‘Background’, ‘Style’, ‘Hybrid’, and ‘Action’. For GEdit-Bench, “G-SC” and “G-PQ” denote “G-Semantic Consistency” and “G-Perceptual Quality”, respectively.Bold: best results among each section.Underline: second-best.",
                "position": 1502
            },
            {
                "img": "https://arxiv.org/html/2512.02014/x3.png",
                "caption": "Table 5:Experimental results on video understanding benchmarks. #Frames denotes the number of frames used during inference.Bold: best results.Underline: second-best.",
                "position": 1752
            },
            {
                "img": "https://arxiv.org/html/2512.02014/x3.png",
                "caption": "Table 6:Video generation results on VBench. Full column names: QS: Quality Score, SS: Semantic Score, SC: Subject Consistency, BC: Background Consistency, TF: Temporal Flickering, MS: Motion Smoothness, DD: Dynamic Degree, AQ: Aesthetic Quality, IQ: Imaging Quality, OC: Object Class, MO: Multiple Objects, HA: Human Action, C: Color, SR: Spatial Relationship, S: Scene, AS: Appearance style, TS: Temporal Style, OC’: Overall Consistency.Bold: best results among each section.Underline: second-best.",
                "position": 1883
            },
            {
                "img": "https://arxiv.org/html/2512.02014/x3.png",
                "caption": "Figure 4:Comparison betweenTunaand Show-o2 on how unified visual representations are produced.",
                "position": 2329
            },
            {
                "img": "https://arxiv.org/html/2512.02014/x4.png",
                "caption": "(a)Alignment to SigLIP 2",
                "position": 2344
            },
            {
                "img": "https://arxiv.org/html/2512.02014/x4.png",
                "caption": "(a)Alignment to SigLIP 2",
                "position": 2347
            },
            {
                "img": "https://arxiv.org/html/2512.02014/x5.png",
                "caption": "(b)Alignment to SD3-Medium",
                "position": 2352
            },
            {
                "img": "https://arxiv.org/html/2512.02014/x6.png",
                "caption": "Figure 6:Qualitative comparison betweenTunaand baseline models on image generation tasks. The instructions that are correctly reflected in our results but failed in some of the baseline models arebolded.",
                "position": 2359
            },
            {
                "img": "https://arxiv.org/html/2512.02014/x7.png",
                "caption": "Figure 7:Qualitative comparison betweenTunaand baseline models on image editing tasks.",
                "position": 2362
            },
            {
                "img": "https://arxiv.org/html/2512.02014/x8.png",
                "caption": "Figure 8:Qualitative results forTunaon the task of text-to-video generation.",
                "position": 2365
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledgment",
        "images": []
    }
]