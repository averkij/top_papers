[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04328/x1.png",
                "caption": "",
                "position": 82
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04328/x2.png",
                "caption": "Figure 2:OlaArchitecture.Olasupports omni-modal inputs including text, image, video, and audio, capable of processing the inputs simultaneously with competitive performance on understanding tasks for all these modalities. Meanwhile,Olasupports user-friendly real-time streaming decoding for texts and speeches thanks to the text detokenizer and the speech decoder.",
                "position": 92
            },
            {
                "img": "https://arxiv.org/html/2502.04328/x3.png",
                "caption": "Figure 3:Progressive modality alignment helps to learn better omni-modal models.We compare our progressive alignment strategy with two baseline training pipelines on Image QA(MMBench[40]), Video QA(VideoMME[21]), and ASR(LibriSpeech[54]): 1) direct mixing where all instruction tuning data is merged and trained in a single stage, and 2) balanced sampling where we upsample certain sources to make the training data more balanced among modalities. The experiment is conducted on a subsampled training set for efficiency and we train models for the same number of steps for fair comparisons. The score is normalized based on the score of progressive alignment to calculate the relative score and the ASR score is inverted as lower is better for the WER metric.",
                "position": 98
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Ola: Omni-Modal Understanding",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04328/x4.png",
                "caption": "Figure 4:Illustrations of theOlaProgressive Modality Alignment.We visualize the relationships among modalities in the left part. Speech acts as the connection between language and audio knowledge, while video constructs the bridge with highly relevant visual and audio information. Therefore, we design the progressive alignment training strategy from primary to periphery. Furthermore, we design the cross-modality video-audio data to better capture the relationships among modalities.",
                "position": 130
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04328/x5.png",
                "caption": "Figure 5:Generative results on speech and visual understanding tasks.We illustrate results on speech and video understanding and show the strong ability of omni-modalOlacompared with conventional vision-language models.",
                "position": 842
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AMore Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04328/x6.png",
                "caption": "Figure 6:Showcases on Text and Audio Understanding.",
                "position": 891
            },
            {
                "img": "https://arxiv.org/html/2502.04328/x7.png",
                "caption": "Figure 7:Showcases on Video Understanding.",
                "position": 904
            }
        ]
    },
    {
        "header": "Appendix BMore Showcases",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]