[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.12628/sec/pic/logo.png",
                "caption": "",
                "position": 100
            }
        ]
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.12628/x1.png",
                "caption": "Figure 1:Overview of training paradigms combining real-world and simulated data.VLA models are commonly trained via supervised fine-tuning (SFT) on real-world demonstrations, or via reinforcement learning (RL) in simulation followed by sim-to-real transfer.\nOther approaches adopt SFT-based sim–real co-training by mixing real and simulated demonstrations.\nIn contrast, we propose an RL-based sim–real co-training (RL-Co) framework, which initializes the model with sim–real SFT and subsequently performs RL in simulation while using real-world SFT as a regularization signal.",
                "position": 144
            }
        ]
    },
    {
        "header": "IIRelated Works",
        "images": []
    },
    {
        "header": "IIIPreliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.12628/x2.png",
                "caption": "Figure 2:Overview of the proposed two-stage sim-real co-training framework.We establish a digital-twin setup whereTsimT_{\\text{sim}}serves as a digital cousin toTrealT_{\\text{real}}despite visual discrepancies.\nInStage I, we initialize the VLA policy by supervising it on a mixture of real and simulated data (ratioα\\alpha). This rapidly injects real-world knowledge and prepares the policy for simulation interaction.\nInStage II, we perform RL fine-tuning in the simulator to explore and improve performance, simultaneously employing a real-world SFT loss as a regularizer to prevent the forgetting of real-world behaviors.",
                "position": 413
            }
        ]
    },
    {
        "header": "IVMethod",
        "images": []
    },
    {
        "header": "VExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.12628/x3.png",
                "caption": "Figure 3:Visualization of our tabletop manipulation tasks.The top row shows images captured by a third-person camera in the real-world setup, while the bottom row presents the corresponding simulated views. Both real and simulated images are sampled from the task execution.",
                "position": 593
            },
            {
                "img": "https://arxiv.org/html/2602.12628/x4.png",
                "caption": "Figure 4:Analysis of the co-training ratio (α\\alpha) and regularization weight (β\\beta).We vary the co-training ratioα\\alphaand evaluate the resulting performance on thePick and PlaceandOpen Drawertasks.\nIn addition, we fixα=0.5\\alpha=0.5forPick and Placeandα=0.95\\alpha=0.95forOpen Drawer, reporting RL co-training results under different regularization weightsβ\\beta.\nPerformance is measured by success rate, with shaded regions indicating standard deviation.",
                "position": 892
            },
            {
                "img": "https://arxiv.org/html/2602.12628/x5.png",
                "caption": "Figure 5:Ablation study on simulation SFT initialization.We report the simulation success rate during RL training for models trained with and without simulation SFT initialization.\nEach RL training process is run with three independent random seeds, and results are presented as the mean success rate with shaded regions indicating the standard deviation.",
                "position": 941
            },
            {
                "img": "https://arxiv.org/html/2602.12628/x6.png",
                "caption": "Figure 6:Ablation study on real-world supervision.We ablate real-world supervised training in Stage I and Stage II separately and report the resulting real-world success rates.",
                "position": 965
            },
            {
                "img": "https://arxiv.org/html/2602.12628/x7.png",
                "caption": "Figure 7:Effect of the number of real-world demonstrations.We vary the amount of real-world demonstrations for theOpen Drawertask and evaluate all training paradigms using theπ0.5\\pi_{0.5}model.\nPerformance is reported in terms of success rate, with shaded regions indicating the standard deviation.",
                "position": 990
            }
        ]
    },
    {
        "header": "VIConclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.12628/x8.png",
                "caption": "Figure 8:Real-world Setup.The real-world evaluation platform includes a tabletop workspace, a Franka Panda robotic manipulator fixed to the table, and a RGB camera for visual perception. All objects are positioned on the table surface.",
                "position": 2061
            },
            {
                "img": "https://arxiv.org/html/2602.12628/x9.png",
                "caption": "Figure 9:Visualization of Four Tabletop Manipulation Tasks.For each task, we present one successful trajectory and uniformly sample seven frames along the execution.\nEach row corresponds to a single trajectory shown from start to completion.",
                "position": 2092
            },
            {
                "img": "https://arxiv.org/html/2602.12628/x10.png",
                "caption": "Figure 10:Manipulated Objects in Simulation and the Real World.The left panel shows the objects used in simulation, while the right panel presents the real-world objects.\nAll simulated objects are used during training.\nThe real-world objects are divided into training objects and unseen objects for generalization evaluation.",
                "position": 2142
            },
            {
                "img": "https://arxiv.org/html/2602.12628/x11.png",
                "caption": "Figure 11:Initial Regions for Manipulative Objects.For thePick and Placetask, the bowl is placed within the orange region, while the objects are initialized in the blue region.\nFor thePush Cubetask, each cube is initialized within its corresponding orange region.\nFor theOpen / Close Drawertasks, the front edge of the drawer is initialized within the orange region.",
                "position": 2203
            },
            {
                "img": "https://arxiv.org/html/2602.12628/x12.png",
                "caption": "Figure 12:Simulation Training Results.We report the simulation success rates across all settings during RL training.",
                "position": 2422
            }
        ]
    },
    {
        "header": "VIIAppendix",
        "images": []
    }
]