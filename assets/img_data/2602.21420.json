[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21420/x1.png",
                "caption": "Figure 1:ACE Method Overview.Top:Incorrect rollouts fall into three regimes based on the confidence shiftci=log⁡(πθ​(yi|x)/πref​(yi|x))c_{i}=\\log(\\pi_{\\theta}(y_{i}|x)/\\pi_{\\mathrm{ref}}(y_{i}|x)).Bottom-left:Standard GRPO assigns a uniform penalty|A^−||\\hat{A}^{-}|to all errors regardless of regime.Bottom-right:ACE modulates the penalty viaSoftplus​(ci)\\text{Softplus}(c_{i}), strongly penalizing overconfident errors while leaving self-correcting errors nearly untouched.",
                "position": 220
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4The ACE Method",
        "images": []
    },
    {
        "header": "5Experiments: ACE Expands the Reasoning Boundary",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21420/experiment_plots/fig_benchmark_combined.png",
                "caption": "Figure 2:Performance Comparison across Benchmarks.Pass@kkcurves for all five methods on MATH-500 (left column) and AIME 2025 (right column) across three model families: Qwen2.5-Math-7B (top row), Qwen3-8B-Base (middle row), and Llama-3.1-8B-Instruct (bottom row). ACE-GRPO and ACE-DAPO consistently outperform their respective baselines (GRPO and DAPO) across all sampling budgets, model families, and benchmarks, with larger gains at higherkkvalues. ACE-DAPO achieves the best overall performance, confirming that ACE’s rollout-level correction composes with DAPO’s token-level diversity preservation and generalizes across model families.",
                "position": 1203
            },
            {
                "img": "https://arxiv.org/html/2602.21420/experiment_plots/fig2_overconfident_error_dynamics.png",
                "caption": "Figure 3:Overconfident Error Dynamics.Left: Overconfident error fraction (OEF) over training. Right: Mean overconfidence magnitude forci>0c_{i}>0errors. ACE-GRPO effectively suppresses both metrics compared to standard GRPO.",
                "position": 1253
            },
            {
                "img": "https://arxiv.org/html/2602.21420/experiment_plots/fig_entropy_combined.png",
                "caption": "Figure 4:Entropy Dynamics.Token-level entropy over the first 20 training steps. Left: On Qwen2.5-Math-7B, ACE-GRPO retains substantially more entropy than standard GRPO, which suffers rapid entropy collapse. Right: On Qwen3-8B-Base, ACE-GRPO maintains more stable entropy, demonstrating consistency across architectures. We report entropy dynamics for the two Qwen models only; Llama-3.1-8B-Instruct is excluded because its lower baseline accuracy makes the entropy signal less directly comparable (see §5for discussion).",
                "position": 1287
            }
        ]
    },
    {
        "header": "6Limitations and Future Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProof of Theorem1(Selective Regularization Decomposition)",
        "images": []
    },
    {
        "header": "Appendix BGradient Quality Analysis",
        "images": []
    },
    {
        "header": "Appendix CImplementation Details",
        "images": []
    },
    {
        "header": "Appendix DSensitivity toα\\alpha",
        "images": []
    },
    {
        "header": "Appendix ETraining Hyperparameters",
        "images": []
    }
]