[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02240/x1.png",
                "caption": "Figure 1:Overview ofReasonMap-Plus.ReasonMap-Pluscomprises4,0184{,}018questions from55extended question types and maps from3030cities across1313countries.",
                "position": 132
            }
        ]
    },
    {
        "header": "3ReasonMap-PlusConstruction",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02240/x2.png",
                "caption": "Figure 2:Overview ofRewardMap. The framework enhances fine-grained visual understanding and reasoning in MLLMs through reinforcement learning with Group Relative Policy Optimization (GRPO). It consists of two key components: (1) adifficulty-aware reward design(Section4.2), which combines format, correctness, and detail rewards with difficulty-based weighting; and (2) amulti-stage RL curriculum(Section4.3), which schedules training data from simple perception tasks to complex reasoning tasks, ensuring effective optimization tackling sparse rewards.",
                "position": 191
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02240/x3.png",
                "caption": "Figure 3:Qualitative comparisons among reference models, baseline, and our proposedRewardMap. We crop and zoom in on the transit map for clearer presentation.",
                "position": 676
            },
            {
                "img": "https://arxiv.org/html/2510.02240/x4.png",
                "caption": "Figure 4:Comparison of training rewards between baseline RL andRewardMap. The yellow curve denotes the reward trajectory ofRewardMap, while the blue curve corresponds to the baseline RL trained solely onReasonMap.",
                "position": 1008
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ADataset Construction Details",
        "images": []
    },
    {
        "header": "Appendix BRewardMapDetails",
        "images": []
    },
    {
        "header": "Appendix CEvaluation Details",
        "images": []
    },
    {
        "header": "Appendix DLarge Language Model Usage Statement",
        "images": []
    }
]