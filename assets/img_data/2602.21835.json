[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21835/fig/teaserfigure_UniVBench.png",
                "caption": "Figure 1:Overview of theUniVBenchevaluation setting across 8 Dimensions, 21 Sub-Dimensions, and 6 Tasks. Given a source video, T2V synthesizes a video using its ground-truth caption, while V2V reconstructs the video based solely on the model’s self-generated understanding text, enabling a direct diagnosis of perception–generation coupling. UniVBench supports six unified tasks—video captioning (V2T), text-to-video generation (T2V), reference-image video generation (R2V), text-instruction video editing (TV2V), reference-image video editing (RV2V), and video reconstruction (V2V).",
                "position": 104
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3UniVBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21835/x1.png",
                "caption": "Figure 2:Workflow of UniV-Eval. The system accepts arbitrary inputs within a task setting and performs dynamic evaluation after planning and decomposition. The final results are delivered as a fine-grained checklist, providing traceable feedback for training optimization.",
                "position": 725
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21835/x2.png",
                "caption": "Figure 3:Case Study Analysis of UniVBench in T2V and Reconstruction Task. T2V generation uses the ground truth text of the video, while V2V reconstruction relies on model’s understanidng text. The generated videos are selected from OmniVideo",
                "position": 1050
            },
            {
                "img": "https://arxiv.org/html/2602.21835/x3.png",
                "caption": "Figure 4:An example of evaluation using different metrics, where the blue-highlighted part shows that UniV-Eval provides more detailed, traceable validation and assessment.",
                "position": 1098
            },
            {
                "img": "https://arxiv.org/html/2602.21835/x4.png",
                "caption": "Figure 5:Human expert annotations used to validate the reliability of UniV-Eval.",
                "position": 1108
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Limitation and Future Works",
        "images": []
    },
    {
        "header": "7Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AEvaluation Cases",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21835/x5.png",
                "caption": "Figure A1:Examples of T2V generation results across different baselines",
                "position": 2150
            },
            {
                "img": "https://arxiv.org/html/2602.21835/x6.png",
                "caption": "Figure A2:Examples of T2V generation results across different baselines",
                "position": 2154
            },
            {
                "img": "https://arxiv.org/html/2602.21835/x7.png",
                "caption": "Figure A3:Examples of R2V generation results of Seedance-Lite",
                "position": 2158
            }
        ]
    },
    {
        "header": "Appendix BMore Details of UniVBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21835/x8.png",
                "caption": "Figure B4:The meta-data distribution of video content.",
                "position": 2172
            },
            {
                "img": "https://arxiv.org/html/2602.21835/x9.png",
                "caption": "Figure B5:The script format used to generate the coherent video captions. Red font indicates the content model needs to fill in. Green font indicates the explanation of each field.",
                "position": 2183
            },
            {
                "img": "https://arxiv.org/html/2602.21835/x10.png",
                "caption": "Figure B6:Evaluation case of LLM as judge and human",
                "position": 2186
            },
            {
                "img": "https://arxiv.org/html/2602.21835/x11.png",
                "caption": "Figure B7:Captioning prompts used to generate detailed video captions.",
                "position": 2189
            },
            {
                "img": "https://arxiv.org/html/2602.21835/x12.png",
                "caption": "Figure B8:Captioning prompts used to generate detailed video captions.",
                "position": 2193
            },
            {
                "img": "https://arxiv.org/html/2602.21835/x13.png",
                "caption": "Figure B9:Captioning prompts used to generate detailed video captions.",
                "position": 2197
            },
            {
                "img": "https://arxiv.org/html/2602.21835/x14.png",
                "caption": "Figure B10:Captioning prompts used to generate detailed video captions.",
                "position": 2201
            },
            {
                "img": "https://arxiv.org/html/2602.21835/x15.png",
                "caption": "Figure B11:Captioning prompts used to generate detailed video captions.",
                "position": 2205
            },
            {
                "img": "https://arxiv.org/html/2602.21835/x16.png",
                "caption": "Figure B12:Captioning prompts used to generate detailed video captions.",
                "position": 2209
            },
            {
                "img": "https://arxiv.org/html/2602.21835/x17.png",
                "caption": "Figure B13:Captioning prompts used to generate detailed video captions.",
                "position": 2213
            }
        ]
    },
    {
        "header": "Appendix CEvaluation System Prompt",
        "images": []
    },
    {
        "header": "Appendix DEvaluation Cost",
        "images": []
    },
    {
        "header": "Appendix EPotential LLM-as-Judge Bias",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21835/x18.png",
                "caption": "Figure F14:Captioning prompts used to generate detailed video captions.",
                "position": 2284
            },
            {
                "img": "https://arxiv.org/html/2602.21835/x19.png",
                "caption": "Figure F15:Evaluation prompts used for V2T task.",
                "position": 2288
            },
            {
                "img": "https://arxiv.org/html/2602.21835/x20.png",
                "caption": "Figure F16:Evaluation prompts used for R2V task.",
                "position": 2291
            },
            {
                "img": "https://arxiv.org/html/2602.21835/x21.png",
                "caption": "Figure F17:Evaluation prompts used for TV2V task.",
                "position": 2294
            },
            {
                "img": "https://arxiv.org/html/2602.21835/x22.png",
                "caption": "Figure F18:Evaluation prompts used for RV2V task.",
                "position": 2297
            },
            {
                "img": "https://arxiv.org/html/2602.21835/x23.png",
                "caption": "Figure F19:Evaluation prompts used for V2V task.",
                "position": 2300
            },
            {
                "img": "https://arxiv.org/html/2602.21835/x24.png",
                "caption": "Figure F20:Evaluation Json template used for V2T task.",
                "position": 2303
            },
            {
                "img": "https://arxiv.org/html/2602.21835/x25.png",
                "caption": "Figure F21:Evaluation Json template used for V2T task.",
                "position": 2306
            }
        ]
    },
    {
        "header": "Appendix FEvaluation cases",
        "images": []
    }
]