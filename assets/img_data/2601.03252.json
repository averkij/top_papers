[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.03252/x1.png",
                "caption": "",
                "position": 170
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.03252/x2.png",
                "caption": "Figure 2:Overview of InfiniDepth.(a) Feature Query: given an input image and a continuous query 2D coordinate, we extract feature tokens from multiple layers of the ViT encoder, and query local features for the coordinate at each scale through bilinear interpolation.\n(b) Depth Decoding: given the multi-scale local features queried at the continuous coordinate, we hierarchically fuse features from high spatial resolution to low spatial resolution, and decode the fused feature to the depth value through a MLP head.",
                "position": 241
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.03252/x3.png",
                "caption": "Figure 3:Advantage of Our Infinite Depth Query.Theblueandredhighlighted regions represent areas with different depths, surface normals, and viewing directions.\nPer-pixel depth prediction leads to strong density imbalance in these regions due to perspective projection and surface orientation, while Infinite Depth Query applies sub-pixel query with adaptive weights to generate uniformly distributed point clouds.",
                "position": 329
            },
            {
                "img": "https://arxiv.org/html/2601.03252/x4.png",
                "caption": "Figure 4:Normal map from implicit fields throughtorch autograd,indicating high-quality internal geometry of our model.",
                "position": 376
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.03252/x5.png",
                "caption": "Figure 5:Qualitative comparisons for relative depth estimation.The first two rows show prediction results on Synth4K, while the last row shows real-world data with low resolution RGB inputs. The boxes highlight detail regions upsampled to higher resolution for baselines, while our method directly predicts at this resolution.\nMore comparisons can be found in thesupp..",
                "position": 1270
            },
            {
                "img": "https://arxiv.org/html/2601.03252/x6.png",
                "caption": "Figure 6:Qualitative comparisons for metric depth estimation.The boxes highlight the high-frequency geometric details.",
                "position": 1276
            },
            {
                "img": "https://arxiv.org/html/2601.03252/x7.png",
                "caption": "Figure 7:Qualitative ablations on depth representation.The boxes highlight the fine-detail regions.",
                "position": 1392
            },
            {
                "img": "https://arxiv.org/html/2601.03252/x8.png",
                "caption": "Figure 8:NVS driven by InfiniDepth and ADGaussian[song2025adgaussian].The boxes highlight regions with geometric holes in the ADGaussian predictions. Refer tosupp.for more results.",
                "position": 1434
            }
        ]
    },
    {
        "header": "5Conclusion and Discussions",
        "images": []
    },
    {
        "header": "Appendix AMethod Details",
        "images": []
    },
    {
        "header": "Appendix BDataset Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.03252/x9.png",
                "caption": "Figure 9:RGB images, depth maps and high-frequency masks in Synth4K.Each row from top to bottom shows samples from Synth4K’s five games:CyberPunk 2077,Marvel’s Spider-Man 2,Miles Morales,Dead Island, andWatch Dogs.",
                "position": 1905
            },
            {
                "img": "https://arxiv.org/html/2601.03252/x10.png",
                "caption": "Figure 10:More RGB images in Synth4K.Each row from top to bottom shows RGB images from Synth4K’s five games:CyberPunk 2077,Marvel’s Spider-Man 2,Miles Morales,Dead Island, andWatch Dogs.",
                "position": 1911
            },
            {
                "img": "https://arxiv.org/html/2601.03252/x11.png",
                "caption": "Figure 11:Point cloud comparisons for relative depth estimation.Each row from top to bottom shows point clouds predicted by our relative depth model and other SOTA models, including MoGe, MoGe-2 and PPD.",
                "position": 1917
            },
            {
                "img": "https://arxiv.org/html/2601.03252/x12.png",
                "caption": "Figure 12:Depth map comparisons for relative depth estimation.Each row from top to bottom shows depth maps predicted by our relative depth model and other SOTA models, including Marigold, DepthAnythingV2, PPD and MoGe-2.",
                "position": 1923
            },
            {
                "img": "https://arxiv.org/html/2601.03252/x13.png",
                "caption": "Figure 13:Single-View Novel View Synthesis (NVS) under large viewpoint shifts.Each row from top to bottom shows novel view synthesis results from our method and ADGaussian.",
                "position": 1929
            }
        ]
    },
    {
        "header": "Appendix CExperiments Details",
        "images": []
    },
    {
        "header": "Appendix DMore Results",
        "images": []
    }
]