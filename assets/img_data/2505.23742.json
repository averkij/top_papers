[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23742/x1.png",
                "caption": "Figure 1.We present MAGREF, a flexible video generation framework that synthesizes novel videos conditioned on a set of reference images and a text prompt. MAGREF maintains visual consistency across multiple subjects and diverse scenes while adhering precisely to the given textual instructions.",
                "position": 96
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": []
    },
    {
        "header": "2.Related Work",
        "images": []
    },
    {
        "header": "3.Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23742/x2.png",
                "caption": "Figure 2.Illustration of our data curation process.\nWe present a three-stage pipeline to collect high-quality video data for network training.",
                "position": 181
            },
            {
                "img": "https://arxiv.org/html/2505.23742/x3.png",
                "caption": "Figure 3.Overview of our framework. Our method is built upon the pre-trained Wan2.1(Wan et¬†al.,2025)video generation base model. We introduce a region-aware dynamic masking strategy to encode multi-reference images and concatenate them with noise latents along the channel dimension.",
                "position": 186
            },
            {
                "img": "https://arxiv.org/html/2505.23742/x4.png",
                "caption": "Figure 4.Illustration of our region-aware dynamic masking strategy. We concatenate the reference images into a single composite image, which is then encoded by a VAE (Eùê∏Eitalic_E) to obtain the reference features. A pixel-wise binary mask is downsampled to spatially indicate the presence of each reference region. We introduce random shuffling strategy to increase the robustness dealing various number of subjects.",
                "position": 282
            }
        ]
    },
    {
        "header": "4.Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23742/x5.png",
                "caption": "Figure 5.Qualitative evaluation results of our method on test cases involving a single ID. Our model consistently generates videos that maintain the subject‚Äôs identity while accurately following the input text prompt.",
                "position": 311
            },
            {
                "img": "https://arxiv.org/html/2505.23742/x6.png",
                "caption": "Figure 6.Qualitative evaluation results of our method on test cases involving multiple concepts, such as persons, animations, and scenes. Our model is capable of understanding and encoding multiple subjects based on the reference images.",
                "position": 363
            },
            {
                "img": "https://arxiv.org/html/2505.23742/x7.png",
                "caption": "Figure 7.Comparison of our model versus existing approaches on single ID generation. We compare MAGREF with existing subject-consistent video generation model. MAGREF demonstrates superior performance compared to open-source models and achieves competitive results relative to commercial models such as(Kling,2025)and(Hailuo,2025). Our method balances between the input concept and prompt, MAGREF follows the instruction ‚Äúwearing glasses‚Äù while maintaining the input identity.",
                "position": 380
            },
            {
                "img": "https://arxiv.org/html/2505.23742/x8.png",
                "caption": "Figure 8.Comparison of our model versus existing works on multi-subject generation. We compare MAGREF with existing subject-consistent video generation model. Our MAGREF is able to generate videos conditioned on flexible references images, maintaining high-quality ID even conditioned on multiple persons.",
                "position": 383
            },
            {
                "img": "https://arxiv.org/html/2505.23742/x9.png",
                "caption": "Figure 9.Ablation study on masking and concatenation schemes.Left: Comparison of different masking mechanisms. Our proposed dynamic masking mechanism maintains identity consistency and visual coherence under varying reference conditions (Top row). In contrast, the vanilla masking mechanism, which concatenates reference images along the channel dimension, results in temporal inconsistency and identity drift (The second row: our re-implementation; Bottom row: SkyReels-A2(Fei et¬†al.,2025)).Right: Comparison of different concatenation mechanisms. Pixel-wise channel concatenation preserves fine-grained reference features, improving consistency with reference images. In contrast, token-wise concatenation dilutes identity-specific cues and weakens identity preservation (The second row: our re-implementation; Bottom row: Phantom(Liu et¬†al.,2025)).",
                "position": 563
            }
        ]
    },
    {
        "header": "5.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]