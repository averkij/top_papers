[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09713/x1.png",
                "caption": "Figure 1:We presentStroke3D (§Section3), a novel framework that generates rigged 3D meshes from user-drawn strokes and language instructions. Crucially, all examples shown are generated from real human inputs via our provided canvas tool, skinned by automatic skinning tools(Blender,2025). We show versatile downstream applications, including generation from different viewpoints, structural editing by adding strokes or modifying joint positions, and final animation. Skeleton color represents the depth in 3D space.",
                "position": 100
            },
            {
                "img": "https://arxiv.org/html/2602.09713/x2.png",
                "caption": "Figure 2:Skeleton-Caption Pipeline (§Section3.1).We render the skeleton and its corresponding mesh together into orthogonal projections using pyrender or Blender. This provides the necessary visual context to represent the object’s identity and pose clearly. These views are then fed into a Vision-Language Model (VLM) to generate detailed descriptions of the object’s identity and pose.",
                "position": 115
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09713/x3.png",
                "caption": "Figure 3:Overview of Stroke3D (§Section3). During the training phase, Sk-VAE encodes a skeleton graph into a latent space. Subsequently, Sk-DiT is trained to generate these latent embeddings, conditioned on the corresponding 2D strokes and text prompt. After training with TextuRig, we leverage SKA-DPO to further refine SKDream with a skeleton-mesh alignment reward signal. The right side illustrates the implementation details of our models.",
                "position": 153
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09713/x4.png",
                "caption": "Figure 4:Qualitative comparison of skeleton generation (§Section4.3).Unlike existing rigging methods that take 3D meshes as input, our approach utilizes 2D projections of a 3D skeleton. This method produces plausible skeletons that more faithfully adhere to the ground truth.",
                "position": 355
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09713/x5.png",
                "caption": "Figure 5:Qualitative comparison of skeletal-conditioned multi-view generation (§Section4.3).Our method produces higher-quality views that more faithfully adhere to the input skeleton. For simplicity, two of the four generated views are shown.",
                "position": 457
            },
            {
                "img": "https://arxiv.org/html/2602.09713/x6.png",
                "caption": "(a)Skeleton Annotation in mesh generation.Data in SKdream shows low quality even incomplete (the bird) skeleton with corresponding mesh.",
                "position": 546
            },
            {
                "img": "https://arxiv.org/html/2602.09713/x6.png",
                "caption": "(a)Skeleton Annotation in mesh generation.Data in SKdream shows low quality even incomplete (the bird) skeleton with corresponding mesh.",
                "position": 549
            },
            {
                "img": "https://arxiv.org/html/2602.09713/x7.png",
                "caption": "(b)Breakdown of TextuRig categories.",
                "position": 554
            },
            {
                "img": "https://arxiv.org/html/2602.09713/figs/training_loss_plot_final.png",
                "caption": "Figure 7:Ablation study on structural condition (§Section4.3). The model converges faster with the structural condition. The pink and blue regions highlight intervals of rapid loss decrease.",
                "position": 565
            },
            {
                "img": "https://arxiv.org/html/2602.09713/x8.png",
                "caption": "Figure 8:Qualitative demonstration of animation stability after rigging.Our method preserves consistent motion dynamics after binding meshes to the skeleton by auto-skinning tools.",
                "position": 577
            },
            {
                "img": "https://arxiv.org/html/2602.09713/figs/averaged_chamfer_statistics_with_count.png",
                "caption": "Figure 9:Sensitivity curve with dropped joints.",
                "position": 584
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendices",
        "images": []
    },
    {
        "header": "Appendix ADataset Processing and Alignment",
        "images": []
    },
    {
        "header": "Appendix BSkeleton Generation Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09713/x9.png",
                "caption": "Figure 10:Qualitative analysis of skeleton generation.(a) shows the effect of different textual captions on inference results.\n(b) shows the influence of varying viewpoints.\n(c) shows the model’s ability to generalize across viewpoints and orientations for common categories.",
                "position": 1611
            }
        ]
    },
    {
        "header": "Appendix CTraining Objective of Sk-VAE",
        "images": []
    },
    {
        "header": "Appendix DMore results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09713/x10.png",
                "caption": "Figure 11:Qualitative evaluation of generation robustness using our Canvas Tool.We present results generated from real-world 2D strokes to validate practical usability. The results demonstrate that Stroke3D is (1) robust to input noise, preserving geometric fidelity even with perturbed or jittery strokes; (2) view-invariant, supporting generation from arbitrary camera angles; and (3) generalizable to rare concepts, successfully lifting complex, out-of-distribution subjects (e.g., ’Samurai’, ’Turtle’) into rigged 3D models.",
                "position": 1897
            }
        ]
    },
    {
        "header": "Appendix EETHICS AND REPRODUCIBILITY STATEMENTS",
        "images": []
    },
    {
        "header": "Appendix FConclusion",
        "images": []
    }
]