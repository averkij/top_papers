[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08153/x1.png",
                "caption": "",
                "position": 91
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08153/x2.png",
                "caption": "Figure 2:Comparison between general scene videos in Koala-36M and videos with distinct physical phenomena in WISA-32K.",
                "position": 115
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3WISA-32K",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08153/x3.png",
                "caption": "Figure 3:Pipeline of WISA-32K. We first define 17 common physical phenomena and, based on this, manually collect 32,000 video samples that clearly illustrate these phenomena. Then, we perform shot detection and aesthetic filtering on the raw videos. Text description are extracted using Qwen2-VL, and detailed physical annotations are generated with GPT-4o mini.",
                "position": 175
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08153/x4.png",
                "caption": "Figure 4:Overview of the proposed WISA. WISA introduces the Physical Module and Physical Classifier, which leverage structured physical annotations to guide and assist T2V models in generating physics-aware videos. Specifically, for qualitative physical categories, WISA constructs a Mixture-of-Physical-Experts Attention within the Physical Module, where each attention head corresponds to a specific physical phenomenon. The relevant physical expert is activated by the input qualitative physical category. The Physical Classifier predicts the physical categories relevant to the video and is supervised by inputted categories to understand abstract physical principles.",
                "position": 225
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08153/x5.png",
                "caption": "Figure 5:Qualitative comparison between WISA and existing T2V methods. WISA exhibit better alignment with real-world physical laws.",
                "position": 318
            },
            {
                "img": "https://arxiv.org/html/2503.08153/x6.png",
                "caption": "Figure 6:More samples generated by WISA, covering additional physical phenomena.",
                "position": 321
            },
            {
                "img": "https://arxiv.org/html/2503.08153/x7.png",
                "caption": "Figure 7:Human evaluation on VideoPhy prompts.",
                "position": 512
            },
            {
                "img": "https://arxiv.org/html/2503.08153/x8.png",
                "caption": "Figure 8:Attention maps of different physical experts.",
                "position": 522
            }
        ]
    },
    {
        "header": "6Limitation",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining Detail",
        "images": []
    },
    {
        "header": "Appendix BThe Definition of Physical Categories",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08153/extracted/6270100/fig/appendix_datasets_correct.png",
                "caption": "Figure 9:Accuracy of qualitative physical category annotations.",
                "position": 1127
            }
        ]
    },
    {
        "header": "Appendix CDataset Property Analysis",
        "images": []
    },
    {
        "header": "Appendix DMore Examples and Annotation",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08153/x9.png",
                "caption": "Figure 10:The video data and its detailed annotations in WISA-32K.",
                "position": 1145
            }
        ]
    },
    {
        "header": "Appendix EAnnotation Prompts",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08153/x10.png",
                "caption": "Figure 11:Prompts for annotating textual physical descriptions and quantitative physical properties",
                "position": 1155
            },
            {
                "img": "https://arxiv.org/html/2503.08153/x11.png",
                "caption": "Figure 12:Prompts for annotating qualitative physics categories",
                "position": 1158
            },
            {
                "img": "https://arxiv.org/html/2503.08153/x12.png",
                "caption": "Figure 13:Prompts for annotating qualitative physics categories",
                "position": 1161
            }
        ]
    },
    {
        "header": "Appendix FWord Cloud",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08153/extracted/6270100/fig/word_cloud.png",
                "caption": "Figure 14:Word cloud generated from textual physical description, where larger words indicate higher frequencies in the dataset text",
                "position": 1171
            },
            {
                "img": "https://arxiv.org/html/2503.08153/x13.png",
                "caption": "Figure 15:Human and machine evaluation results do not fully align.",
                "position": 1182
            }
        ]
    },
    {
        "header": "Appendix GDiscussion of Quantitative Evaluation",
        "images": []
    }
]