[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13928/x1.png",
                "caption": "Figure 1:Improvement over the base-VLM grouped by the test ability domain of benchmarks.OurS-VCOdelivers the most significant overall improvement across nearly all domains, with particularly strong gains in reducing visual hallucinations. In vision-centric and general capability domains,S-VCOalso achieves considerable performance boosts over the base-VLM, outperforming existing preference tuning methods including DPO and mDPO (discussed in more detail inSection5.2).",
                "position": 101
            },
            {
                "img": "https://arxiv.org/html/2502.13928/x2.png",
                "caption": "Figure 2:Upper Part: A comparison of a VLM’s perplexity (PPL) for generating a text caption when the input isan image matching the text,an image contradicting the text,\norno image input at all. Intuitively, the PPL should be lowest when the image matches the text. However, the current VLM exhibits the lowest PPL without any image input and the highest PPL given the matching image.Lower Part: This counterintuitive pattern holds across1,00010001,0001 , 000random examples with these visual counterfactual pairs extracted from CounterCurate dataset (Section4.1).",
                "position": 104
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13928/x3.png",
                "caption": "Figure 3:Upper Part:MVCof visual counterfactual images [b)] in comparison to the image pair data used in prior work [a)] (Section4).MVC’s image pair differs in meaningful visual details that are also grounded in the associated texts [b)], while corrupting original images with random cropping or adding noise leads to images that are not aligned with the texts directly derived from language preference data [a)].Lower Part:S-VCOin comparison to existing VLM preference tuning paradigms DPO and visual-conditional PO (Section3).Unlike prior methods that treat visual supervision as uni-modal preferences,S-VCOconsiders the contrast of the image-text pair as a whole. It rewards the model for attending to matching images and rejecting contradictory ones (Section3.1), while using a symmetrical mechanism to switch the role of each image-text pair, thus avoiding shortcut learning (Section3.2).",
                "position": 149
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3S-VCO: Symmetrical Visual Contrastive Optimization",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13928/x4.png",
                "caption": "Figure 4:MVCdataset outline:a).Types of visual counterfactuals sourced fromZhang et al. (2024); Liu et al. (2024c);b).Our vision-centric filter that keeps only image pairs whose CLIP-similarity>0.7absent0.7>0.7> 0.7to select hard samples for current VLMs, while ensuring meaningful visual differences with DINOv2-similarity<0.5absent0.5<0.5< 0.5;c).Rewriting captions into conversational queries and responses without changing the explicit minimal visual contrasts.",
                "position": 317
            }
        ]
    },
    {
        "header": "4Minimal Visual Contrasts Dataset",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13928/x5.png",
                "caption": "Figure 5:Qualitative examples extracted from various benchmarkscomparingbase-VLMs(LV-INTorLV-1.5)to the results after finetuning withDPO,mDPOorS-VCOonMVCdataset.Accurate captions of visual information are highlighted.\nOur methodS-VCOdemonstrates superior understanding offine-grained visual details(e.g., identifying the absence of a toothbrush) and showsstrong resilience to visual hallucinations(e.g., recognizing marker-drawings, fire hydrants, slide-phones). Furthermore,S-VCOexcels in more advancedvisual reasoning(e.g., interpreting drive-lane conditions & regulations, estimating object sizes & distances), and capturescomplex sceneswith greater detailedness and depth (e.g., identifying weather through the window, recognizing oncoming vehicles in low-light settings).",
                "position": 658
            },
            {
                "img": "https://arxiv.org/html/2502.13928/x6.png",
                "caption": "Figure 6:Trend of improvement over the base-VLM as benchmarks become increasingly visually dependent.A metric’s visual dependency is measured as the performance drop of the base-VLM when no image input is provided.S-VCOexhibits the most significant trend of improvements with increasing task visual dependency, highlighting how its objective design (Section3) enhances model’s focus on critical visual details.MVCdataset also strengthens existing preference tuning methods (DPO and mDPO), while SFT (Section5.3) degrades performance on more visually demanding benchmarks.",
                "position": 662
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADataset Details",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    },
    {
        "header": "Appendix CPrompt forMVCLanguage Augmentation",
        "images": []
    }
]