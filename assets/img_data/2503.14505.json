[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14505/x1.png",
                "caption": "",
                "position": 123
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14505/x2.png",
                "caption": "Figure 2:Due to the conservation of knowledge in video and text modalities, our model generalizes to generate group dance videos by modulating the prompt. To show this, the prompt is set to“[DANCERS]dancing in a studio with a white backdrop, captured from a front view,”where[DANCERS]denotes the description for each number of dancers.",
                "position": 171
            },
            {
                "img": "https://arxiv.org/html/2503.14505/extracted/6289646/fig/long.png",
                "caption": "Figure 3:We generate longer dance videos that are 2 times longer than the videos used for training. For each row, we use synthetic in-the-wild music tracks with a keyword “K-pop,” a type of music not existing in AIST[46], and use a prompt“a professional dancer dancing K-pop ….”This shows our method is highly generalizable, even extending to longer videos with an unheard cateory of the music. The beat and style alignment can be more clearly observed in the videos on the project page.",
                "position": 174
            }
        ]
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4MusicInfuser",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14505/x3.png",
                "caption": "Figure 4:Overall architecture of MusicInfuser. Our framework adapts a pre-trained diffusion model with audio embedding using ZICA blocks (Sec.4.1) and HR-LoRA blocks (Sec.4.2). The placement of ZICA blocks is selected based on layer adaptability (Sec.4.6).",
                "position": 287
            },
            {
                "img": "https://arxiv.org/html/2503.14505/x4.png",
                "caption": "Figure 5:Comparison of audio-driven generation with MM-Diffusion[39]. Our method produces fewer artifacts (shown in the first and third rows), while generating more realistic dance videos with more natural movements (first row) and more dynamic motion (second and third rows). Note that we use the same music track for each row, and the spectrogram is stretched for MM-Diffusion since we generate longer videos. For our method, we use the fixed caption “a professional dancer dancing …” across all music tracks.",
                "position": 324
            },
            {
                "img": "https://arxiv.org/html/2503.14505/x5.png",
                "caption": "Figure 6:Speed control. The audio input is slowed down (the top row) or sped up (the bottom row) by 0.75 times and 1.25 times, respectively. This shows that speeding up generally results in more movements. Also see the change in the dynamicity, as speeding up the audio also increases the tone of the music.",
                "position": 422
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14505/x6.png",
                "caption": "Figure 7:Videos generated with three distinct in-the-wild music tracks created with SUNO AI. For each row, we use in-the-wild music tracks generated with a word “K-pop,” an unseen category.",
                "position": 704
            },
            {
                "img": "https://arxiv.org/html/2503.14505/x7.png",
                "caption": "Figure 8:By changing the seed, our method can produce diverse results given the same music and text. The generated choreography of each dance is different from each other. We use the fixed prompt “a professional dancer dancing ….”",
                "position": 734
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AVideo Results",
        "images": []
    },
    {
        "header": "Appendix BBeta Distribution",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14505/extracted/6289646/fig/beta_uniform.png",
                "caption": "Figure 9:Beta distributions.",
                "position": 1448
            }
        ]
    },
    {
        "header": "Appendix CDifficulty Control",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14505/x8.png",
                "caption": "Figure 10:Changes in the complexity of choreography.",
                "position": 1458
            }
        ]
    },
    {
        "header": "Appendix DLimitations",
        "images": []
    },
    {
        "header": "Appendix EComparison with the Base Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14505/x9.png",
                "caption": "Figure 11:MusicInfuser infuses listening capability into the text-to-video model (Mochi[44]), while preserving the prompt adherence and improving overall consistency and realism (frames 2 and 5 of the top example, and frames 2–4 of the bottom example).",
                "position": 1475
            }
        ]
    },
    {
        "header": "Appendix FAblation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14505/x10.png",
                "caption": "Figure 12:Ablation study. The prompt is set to “a male dancer dancing in an art gallery with some paintings, captured from a front view”. The seed and music are set the same across all methods.",
                "position": 1486
            },
            {
                "img": "https://arxiv.org/html/2503.14505/x11.png",
                "caption": "Figure 13:Ablation study. The prompt is set to “a male dancer wearing a suit dancing in the middle of a New York City, captured from a front view”. The seed and music are set the same across all methods.",
                "position": 1489
            }
        ]
    },
    {
        "header": "Appendix GLayer Adaptability",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14505/x12.png",
                "caption": "Figure 14:Layer adaptability graph from[26], showing imaging and aesthetic quality.",
                "position": 1499
            }
        ]
    },
    {
        "header": "Appendix HTest Music Tracks",
        "images": []
    },
    {
        "header": "Appendix IIn-the-Wild Music Tracks",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14505/x13.png",
                "caption": "Figure 15:Videos generated with three distinct in-the-wild music tracks.",
                "position": 1569
            },
            {
                "img": "https://arxiv.org/html/2503.14505/extracted/6289646/fig/failure.png",
                "caption": "Figure 16:Failure cases. Our model inherits some issues from the base model, such as failing to generate fine details (e.g., fingers and faces) and being fooled by the silhouette of the dancers.",
                "position": 1572
            }
        ]
    },
    {
        "header": "Appendix JPrompts",
        "images": []
    },
    {
        "header": "Appendix KConcurrent Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14505/x14.png",
                "caption": "Figure 17:More music-and-text-to-video generation results.",
                "position": 1914
            }
        ]
    },
    {
        "header": "Appendix LMore Results",
        "images": []
    }
]