[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.13177/figures/teaser.jpg",
                "caption": "",
                "position": 101
            }
        ]
    },
    {
        "header": "IINTRODUCTION",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.13177/x1.jpg",
                "caption": "Figure 2:ROOM data generation pipeline. The system consists of four main stages: (1) Medial Axis Extraction from segmented CT lung models, (2) Automated Sampling along skeletal branches with higher density at bifurcations and high-curvature regions, (3) Data Synthesis generating synchronized multi-modal sensor streams fromt\\lx@text@underscore​0t_{\\lx@text@underscore}0tot\\lx@text@underscore​nt_{\\lx@text@underscore}ntimesteps, and (4) Sensor Noise Modeling applying realistic noise characteristics matching real bronchoscopy imagery through frequency-domain analysis.",
                "position": 162
            }
        ]
    },
    {
        "header": "IIRelated Work",
        "images": []
    },
    {
        "header": "IIIMethod",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.13177/x2.png",
                "caption": "Figure 3:Visual comparison of ROOM outputs compared to real data.Left:Real bronchoscopy data captured from a continuum robot showing specular highlights from wet mucosal surfaces and directional lighting.Center:ROOM’s photorealistic rendering using Blender’s path tracing with Principled BSDF shaders, accurately reproducing tissue surface properties and lighting conditions.Right:Naive PyBullet-based rendering lacking photorealistic materials and lighting.",
                "position": 199
            },
            {
                "img": "https://arxiv.org/html/2509.13177/figures/robot.png",
                "caption": "Figure 4:Continuum robot model used in ROOM simulation. The bronchoscope is modelled as a flexible, cable-driven continuum robot with constant curvature bending and three degrees of freedom: tendon actuation for bending curvature (q1), axial rotation for bending plane (q2), and linear insertion depth (q3). The physics-based simulation incorporates realistic friction models, actuator noise, and collision dynamics calibrated to clinical bronchoscope behaviour.",
                "position": 284
            }
        ]
    },
    {
        "header": "IVApplications",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.13177/figures/fig5_depth.jpg",
                "caption": "Figure 6:Comparative monocular depth estimation results on ROOM synthetic bronchoscopy sequences.Top rows show L1 error maps between predicted depth estimation and ground truth depth, where warmer colours indicate higher absolute errors, while bottom rows display corresponding RGB inputs with challenging specular highlights and limited texture. Five state-of-the-art models are evaluated: Metric3DV2, Depth Anything V2 (DAV2 Monocular/Relative), Unidepth, EndoOmni, EndoDAC, BREA-Depth, revealing significant performance degradations due to the realistic sensor noise from the simulator and systematic errors concentrated at geometric transitions and specular regions.",
                "position": 521
            },
            {
                "img": "https://arxiv.org/html/2509.13177/figures/fig6_finetune.jpg",
                "caption": "Figure 7:Monocular depth estimation examples of pre-trained models and fine-tuned on ROOM.We show examples on a phantom-based dataset with ground truth[36]as well as real images. Please note that the real image does not have depth ground truth available.",
                "position": 638
            },
            {
                "img": "https://arxiv.org/html/2509.13177/x3.jpg",
                "caption": "Figure 8:Vision-based navigation examples.We demonstrate qualitative results of the relative monocular depth predictions (scaled with ground-truth scale), as input for a sampling-based local planner.Left:projection of the collision-free path.Right:3D visualisation of the path, with the spheres indicating the collision-bodies used by the planner.",
                "position": 652
            }
        ]
    },
    {
        "header": "VDiscussion",
        "images": []
    },
    {
        "header": "VIConclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]