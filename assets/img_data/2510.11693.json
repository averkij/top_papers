[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Latent Cross-Modal Alignment in MLLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11693/x1.png",
                "caption": "Figure 1:The anisotropy estimates of Qwen2.5-Omni-3B embeddings across text, image, audio, and video modalities. The vanilla model exhibits typical representation degeneration (anisotropy) for all modalities. After applyingtext-onlycontrastive learning, embeddings across modalities become more isotropic, indicating latentlanguage-centriccross-modal alignment within the model.",
                "position": 162
            },
            {
                "img": "https://arxiv.org/html/2510.11693/x1.png",
                "caption": "",
                "position": 165
            },
            {
                "img": "https://arxiv.org/html/2510.11693/x2.png",
                "caption": "",
                "position": 169
            },
            {
                "img": "https://arxiv.org/html/2510.11693/x3.png",
                "caption": "",
                "position": 173
            },
            {
                "img": "https://arxiv.org/html/2510.11693/x4.png",
                "caption": "Figure 2:Layer-wise vision-language kernel alignment before and after text-only contrastive learning, evaluated on Qwen-VL models with 7B (28 layers) and 3B (36 layers) parameters. Note the 3B model has more layers than the 7B model.",
                "position": 212
            }
        ]
    },
    {
        "header": "3Language-centric Omnimodal Representation Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11693/x5.png",
                "caption": "Figure 3:The power of language-centric omnimodal representation learning: Before text-only contrastive learning (CL), representations across modalities in multimodal large language models (MLLMs) exhibit anisotropy, collapsing into a confined subspace. Text-only CL disperses textual representations by increasing their separation, effectively reducing anisotropy. Notably, this process generalizes to alleviate anisotropy in non-textual modalities, despite the absence of direct supervision.",
                "position": 232
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11693/x6.png",
                "caption": "Figure 4:Performance comparison ofLCO-Embagainst the state-of-the-art open-source and proprietary embedding models, where we visualize the average performance of MIEB-Lite and its English-only subsets.LCO-Emb-VL andLCO-Emb-Omni denotesLCO-Embtrained from the Qwen2.5-VL and Qwen2.5-Omni backbones, respectively, while “T” and “M” represent thetext-onlyandmultimodalvariants ofLCO-Emb, respectively.",
                "position": 260
            },
            {
                "img": "https://arxiv.org/html/2510.11693/x6.png",
                "caption": "",
                "position": 263
            },
            {
                "img": "https://arxiv.org/html/2510.11693/x7.png",
                "caption": "",
                "position": 267
            },
            {
                "img": "https://arxiv.org/html/2510.11693/x8.png",
                "caption": "Figure 5:Ablation comparison between thetext-onlyvariants ofLCO-Embwith advanced open-source (E5-V[30]) and proprietary (Voyage Multimodal 3[65]) embedding models onMIEB-Sub18.LCO-Emb-VL andLCO-Emb-Omni denoteLCO-Embtrained from Qwen2.5-VL and Qwen2.5-Omni backbones, respectively.",
                "position": 625
            },
            {
                "img": "https://arxiv.org/html/2510.11693/x8.png",
                "caption": "",
                "position": 628
            },
            {
                "img": "https://arxiv.org/html/2510.11693/x9.png",
                "caption": "",
                "position": 632
            },
            {
                "img": "https://arxiv.org/html/2510.11693/x10.png",
                "caption": "",
                "position": 636
            },
            {
                "img": "https://arxiv.org/html/2510.11693/x11.png",
                "caption": "",
                "position": 641
            },
            {
                "img": "https://arxiv.org/html/2510.11693/x12.png",
                "caption": "",
                "position": 645
            },
            {
                "img": "https://arxiv.org/html/2510.11693/x13.png",
                "caption": "",
                "position": 649
            }
        ]
    },
    {
        "header": "5Generation-Representation Scaling Law",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11693/x14.png",
                "caption": "Figure 6:Scaling relationship between generation benchmark performance (X-axis) and representation benchmark performance after language-centric contrastive learning (Y-axis).",
                "position": 962
            },
            {
                "img": "https://arxiv.org/html/2510.11693/x15.png",
                "caption": "Figure 7:Retrieval performance of Qwen2.5-VL-3B fine-tuned on various continual generative finetuning strategies before CL on SeaDoc benchmark, where “PC” represents PixmoCaps and “H” denotes high-resolution. The results suggest that enhancing the generative ability of MLLMs before CL can enhance their embedding capability.",
                "position": 1167
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails of Additional Multimodal Data",
        "images": []
    },
    {
        "header": "Appendix BDetails of MIEB-Lite Benchmark",
        "images": []
    },
    {
        "header": "Appendix CDetails of MIEB-Sub18 Benchmark",
        "images": []
    },
    {
        "header": "Appendix DImpact of LoRA Hyperparameters forLCO-Emb",
        "images": []
    },
    {
        "header": "Appendix EDetails of the data construction process of SeaDoc",
        "images": []
    },
    {
        "header": "Appendix FRelationship Between Generative Loss and Conditional Entropy",
        "images": []
    }
]