[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Problem Formalisms",
        "images": []
    },
    {
        "header": "3PromptQuine",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.17930/x1.png",
                "caption": "Figure 1:Optimization challenges in our ICL-initialized landscape using Llama-3-8B-Instruct for subjectivity classification.Left: Randomizing pruning order in hill-climbing search leads to varying task performance, highlighting the multimodal nature.Middle: Evolutionary search (ES) outperforms random search (RS) in identifying high-quality solutions, withTAPruningresult as a dashed line.Right: Relative success rate of RS over ES approaches zero as task difficulty increases, particularly when solution sparsity is enhanced by expanding the context. Further studies, with consistent conclusions, are provided in AppendixD.2.",
                "position": 348
            },
            {
                "img": "https://arxiv.org/html/2506.17930/x2.png",
                "caption": "Figure 2:Overview of thePromptQuineframework. Similar to natural selection, our framework evolves prompts by copying and mutating them (i.e., pruning random tokens). Guided by task-specific selection pressures, it progressively optimizes itself. Notably, the generation ofunnatural language prompts, despite introducing unexpected variations, consistently outperforms manually designed prompts, representing a step towards open-ended self-improvement in AI[102,83].",
                "position": 362
            }
        ]
    },
    {
        "header": "4Task Designs & Results",
        "images": []
    },
    {
        "header": "5A Deeper Look at Pruning Effects on ICL",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.17930/x3.png",
                "caption": "Figure 3:The percentage of label word presence is surprisingly high for ourPromptQuine-pruned ICL prompts. We obtain these parsing results by exactly matching the label words within the pruned prompts. We verify that the ICL prompts used in the analysis do not contain these words in their exemplar inputs.",
                "position": 811
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusions",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelated Work",
        "images": []
    },
    {
        "header": "Appendix BA Pilot Study with TAPruning",
        "images": []
    },
    {
        "header": "Appendix CAlternative Prompt Pruning Design Choices",
        "images": []
    },
    {
        "header": "Appendix DPromptQuineAdditional Details",
        "images": []
    },
    {
        "header": "Appendix ETowards More Open-Ended Prompt Designs",
        "images": []
    }
]