[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03454/x1.png",
                "caption": "Figure 1:Qualitative example of the use-case for contextual visual personalization in VLMs. Note that our CoViP effectively responds to the question while integrating the mentioned personal details from the given multimodal contexts.",
                "position": 139
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Previous Works on VLM Personalization",
        "images": []
    },
    {
        "header": "3CoViP: Contextualized Visual Personalization via Image Captioning",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03454/x2.png",
                "caption": "Figure 2:Illustration of the proposed personalized image captioning benchmark construction.",
                "position": 559
            }
        ]
    },
    {
        "header": "4Diagnostic Evaluation of General Personalization Capability",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03454/x3.png",
                "caption": "Figure 3:Visualization of diagnostic personalization tasks.",
                "position": 1247
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Discussions",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03454/x4.png",
                "caption": "Figure 4:Results of the human preference evaluation. Here, Win denotes the win rates of CoViP compared to the baseline.",
                "position": 1592
            },
            {
                "img": "https://arxiv.org/html/2602.03454/x5.png",
                "caption": "Figure 5:Visualization of comparative results for visual-triggered personalization diagnostics. Scores indicate relative performance.",
                "position": 1602
            }
        ]
    },
    {
        "header": "7Conclusions and Limitations",
        "images": []
    },
    {
        "header": "Impact Statements",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03454/x6.png",
                "caption": "Figure 6:Scatter plot of recognition versus retrieval on the proposed benchmark. Recognition is measured by the F1 score of entity name inclusion between generated captions and ground-truth dialogues, while retrieval is measured by positive MCQA accuracy. Here,mmdenotes the slope of the linear regression line.",
                "position": 1645
            }
        ]
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03454/x7.png",
                "caption": "Figure S.1:Overview of our personalized image captioning framework. Given a query image, a VLM generates a personalized caption by referencing only the positive concept images and their associated dialogues within the context. An external LLM then evaluates whether the generated caption alone provides sufficient information to correctly answer a set of MCQs, using accuracy as the evaluation metric. During RL-based post-training, this accuracy is further leveraged as a VR signal.",
                "position": 2129
            }
        ]
    },
    {
        "header": "Appendix ARelated Works",
        "images": []
    },
    {
        "header": "Appendix BDetailed Description of Evaluation Protocol in Personalized Image Captioning Benchmark",
        "images": []
    },
    {
        "header": "Appendix CImplementation Details of the Post-training Pipeline",
        "images": []
    },
    {
        "header": "Appendix DExperimental Configurations",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03454/x8.png",
                "caption": "Figure S.2:Used real images to construct our database.",
                "position": 2465
            },
            {
                "img": "https://arxiv.org/html/2602.03454/x9.png",
                "caption": "Figure S.3:Visualization of query images generated with varying numbers of concept images.",
                "position": 2468
            },
            {
                "img": "https://arxiv.org/html/2602.03454/x10.png",
                "caption": "Figure S.4:Quality filtering results obtained withGemini-2.5 Flash. ‘Yes’ denotes the proportion of retained query images after filtering.",
                "position": 2493
            }
        ]
    },
    {
        "header": "Appendix EAdditional Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03454/x11.png",
                "caption": "Figure S.5:Reward trajectory ablations on VR designs, on-policy RL algorithms, and prompt templates during post-training.",
                "position": 2581
            }
        ]
    },
    {
        "header": "Appendix FAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03454/x12.png",
                "caption": "Figure S.6:Image-only visualization of an example from our proposed personalized image captioning benchmark for the case of the three-concept setting. Note that the number of negative samples is three times the number of positive samples.",
                "position": 3874
            },
            {
                "img": "https://arxiv.org/html/2602.03454/x13.png",
                "caption": "Figure S.7:Template of the survey form for human evaluation.",
                "position": 3893
            },
            {
                "img": "https://arxiv.org/html/2602.03454/x14.png",
                "caption": "Figure S.8:Snapshot of the user interface used for collecting human evaluation results.",
                "position": 3896
            }
        ]
    },
    {
        "header": "Appendix GSpecifications on Evaluation Settings",
        "images": []
    }
]