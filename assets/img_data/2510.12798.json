[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12798/x1.png",
                "caption": "Figure 1:1) Detectors excel in localization but lack language understanding. MLLMs understand language well but struggle with localization. 2) Differences in optimization difficulty between detectors and MLLMs.",
                "position": 173
            }
        ]
    },
    {
        "header": "2Task Formulation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12798/x2.png",
                "caption": "Figure 2:Design philosophy of Rex-Omni for coordinate prediction. It illustrates our chosen approach: (a) adopting a direct coordinate prediction strategy, and (b) employing a quantized relative coordinate format represented by special tokens for efficient and robust spatial encoding.",
                "position": 224
            },
            {
                "img": "https://arxiv.org/html/2510.12798/x3.png",
                "caption": "Figure 3:Overview of the Rex-Omni Model Architecture. Rex-Omni is constructed upon the Qwen2.5-VL-3B backbone with minimal architectural modifications. Notably, the last 1,000 tokens of the original vocabulary are repurposed to serve as dedicated special tokens, representing quantized coordinate values from 0 to 999.",
                "position": 353
            }
        ]
    },
    {
        "header": "3Training Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12798/x4.png",
                "caption": "Figure 4:Pipelines of our two primary data engines. The figure illustrates the processes of the Grounding Data Engine (top) and the Referring Data Engine (bottom), which are custom-designed to produce extensive, high-quality grounding and referring data for Rex-Omni’s training.",
                "position": 568
            }
        ]
    },
    {
        "header": "4Training Pipelines",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12798/x5.png",
                "caption": "Figure 5:Overview of Rex-Omni’s two-stage training pipeline. The first stage involves supervised fine-tuning (SFT) on 22 million samples to establish fundamental coordinate prediction capabilities. This is followed by GRPO-based reinforcement post-training, which leverages geometry-aware rewards and behavior-aware optimization to refine precision and correct SFT-induced behavioral deficiencies.",
                "position": 718
            }
        ]
    },
    {
        "header": "5Benchmark Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12798/x6.png",
                "caption": "Figure 6:Visualization of detection predictions from different models on common and long-tailed object detection benchmarks, using COCO and LVIS, respectively.",
                "position": 1248
            },
            {
                "img": "https://arxiv.org/html/2510.12798/x7.png",
                "caption": "Figure 7:Visualization of dense and tiny object detection predictions. This figure presents a qualitative comparison of various models on the VisDrone and Dense200 datasets.",
                "position": 1751
            },
            {
                "img": "https://arxiv.org/html/2510.12798/x8.png",
                "caption": "Figure 8:Visualization of model predictions on referring object detection benchmarks.",
                "position": 2024
            },
            {
                "img": "https://arxiv.org/html/2510.12798/x9.png",
                "caption": "Figure 9:Qualitative comparison of visual prompting predictions between T-Rex2 and Rex-Omni.",
                "position": 2326
            },
            {
                "img": "https://arxiv.org/html/2510.12798/x10.png",
                "caption": "Figure 10:Qualitative comparison of object pointing predictions from different models.",
                "position": 2451
            },
            {
                "img": "https://arxiv.org/html/2510.12798/x11.png",
                "caption": "Figure 11:Qualitative comparison of layout grounding predictions from different models. The figure illustrates the models’ ability to localize and interpret various layout elements.",
                "position": 2861
            },
            {
                "img": "https://arxiv.org/html/2510.12798/x12.png",
                "caption": "Figure 12:Visualization of OCR results across models.",
                "position": 3118
            },
            {
                "img": "https://arxiv.org/html/2510.12798/figures/compare_spatial.jpg",
                "caption": "Figure 13:Visualization of Spatial Pointing results across models. Masks indicate correct areas.",
                "position": 3201
            },
            {
                "img": "https://arxiv.org/html/2510.12798/x13.png",
                "caption": "Figure 14:Qualitative comparison of keypoint detection predictions from different models.",
                "position": 3354
            }
        ]
    },
    {
        "header": "6In-depth Analysis of Rex-Omni",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12798/x14.png",
                "caption": "Figure 15:Model performance across SFT and GRPO training stages. F1@mIoU scores are shown for three datasets (Dense200, COCO, LVIS) with training data volume increasing.",
                "position": 3375
            },
            {
                "img": "https://arxiv.org/html/2510.12798/x15.png",
                "caption": "Figure 16:Illustration of challenges in MLLM-based object detection and their mitigation. The left side qualitatively compares predictions from SFT and GRPO models, highlighting GRPO’s effectiveness in reducing duplicate outputs. The right side visualizes the large-box prediction failure mode, where models inappropriately predict overly large bounding boxes encompassing multiple objects.",
                "position": 3587
            },
            {
                "img": "https://arxiv.org/html/2510.12798/figures/analysis_speed.png",
                "caption": "Figure 17:Inference speed and output token analysis. The plot shows the average generation time (seconds) and the average number of output tokens as a function of the number of predicted boxes. The experiment was conducted on a single NVIDIA A100 GPU with vLLM deployment in BF16 precision, without model acceleration or compression.",
                "position": 3992
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Related Work",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12798/x16.png",
                "caption": "Figure 18:Visualization results of Rex-Omni on common and long-tailed object detection task.",
                "position": 6064
            },
            {
                "img": "https://arxiv.org/html/2510.12798/x17.png",
                "caption": "Figure 19:Visualization results of Rex-Omni on dense object detection task.",
                "position": 6067
            },
            {
                "img": "https://arxiv.org/html/2510.12798/x18.png",
                "caption": "Figure 20:Visualization results of Rex-Omni on object referring task.",
                "position": 6070
            },
            {
                "img": "https://arxiv.org/html/2510.12798/x19.png",
                "caption": "Figure 21:Visualization results of Rex-Omni on object pointing task.",
                "position": 6073
            },
            {
                "img": "https://arxiv.org/html/2510.12798/x20.png",
                "caption": "Figure 22:Visualization results of Rex-Omni on layout grounding task.",
                "position": 6076
            },
            {
                "img": "https://arxiv.org/html/2510.12798/x21.png",
                "caption": "Figure 23:Visualization results of Rex-Omni on OCR task.",
                "position": 6079
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]