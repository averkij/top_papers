[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15857/x1.png",
                "caption": "Figure 1:The architecture of BLIP3o-NEXT (left) and its reinforcement learning pipeline (right). BLIP3o-NEXT adopts an Autoregressive (AR) + Diffusion design, where the AR module autoregressively generates image conditions for the diffusion model. The model is jointly optimized with both AR and diffusion objectives. During reinforcement learning, rollouts are rendered from the diffusion transformer, and policy optimization is performed directly on the AR model, enabling seamless integration with existing RL infrastructures originally developed for language models.",
                "position": 109
            }
        ]
    },
    {
        "header": "2Overview of Architectures for Native Image Generation",
        "images": []
    },
    {
        "header": "3Image Generation with Reinforcement Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15857/x2.png",
                "caption": "Figure 2:Training reward for GenEval and visual text rendering.",
                "position": 398
            },
            {
                "img": "https://arxiv.org/html/2510.15857/x3.png",
                "caption": "Figure 3:Qualitative results of multiple object composition before and after GRPO.",
                "position": 402
            },
            {
                "img": "https://arxiv.org/html/2510.15857/x4.png",
                "caption": "Figure 4:Qualitative results of text rendering before and after GRPO.",
                "position": 406
            }
        ]
    },
    {
        "header": "4Image Editing",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15857/x5.png",
                "caption": "Figure 5:Comparison of VAE feature integration strategies in BLIP3o-NEXT. In theVAE as cross-attention inputssetup, the flattened VAE tokens are appended to the multimodal tokens produced by the autoregressive model. Empirically, we find that combining both methods yields the best visual consistency.",
                "position": 454
            },
            {
                "img": "https://arxiv.org/html/2510.15857/x6.png",
                "caption": "Figure 6:Qualitative results for image editing comparing models with and without VAE latent condition.",
                "position": 667
            }
        ]
    },
    {
        "header": "5Training Recipe and Evaluation",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]