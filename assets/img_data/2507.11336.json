[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.11336/x1.png",
                "caption": "",
                "position": 89
            },
            {
                "img": "https://arxiv.org/html/2507.11336/x2.png",
                "caption": "",
                "position": 89
            },
            {
                "img": "https://arxiv.org/html/2507.11336/x3.png",
                "caption": "",
                "position": 89
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3UGC Video Detail Caption Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.11336/x4.png",
                "caption": "Figure 1:Benchmark Collation Pipeline. This pipeline unifies disparate raw video annotation data into a standardised format for consistent processing. QA pairs are then generated from question templates with fixed rules. To ensure quality, manual validation is performed at all key stages, with multiple people cycling through low quality and ambiguous annotated content to assess whether to re-labelling.",
                "position": 225
            },
            {
                "img": "https://arxiv.org/html/2507.11336/x5.png",
                "caption": "Figure 2:Demonstration of tasks forUGC-VideoCapbenchmark. It has many meta information for audio and visual. And we select some of them to be our benchmark QA pairs which are visual detail description, audio track detail description and final detail caption.",
                "position": 237
            }
        ]
    },
    {
        "header": "4UGC Video Captioner",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.11336/x6.png",
                "caption": "Figure 4:Left:Distillation Stage. We use Gemini-2.5-flash to generate 20k omni video detail caption.Right:Reinforcement Learning Stage. We use GRPO algorithm and new caption reward model to enhance the detail caption ability.",
                "position": 470
            }
        ]
    },
    {
        "header": "5Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]