[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07961/x1.png",
                "caption": "",
                "position": 136
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07961/x2.png",
                "caption": "Figure 2:Overview of Geo4D.During training,\nvideo conditions are injected by locally concatenating the latent feature of the video with diffused geometric features𝐳t𝐗,𝐳t𝐃,𝐳t𝐫superscriptsubscript𝐳𝑡𝐗superscriptsubscript𝐳𝑡𝐃superscriptsubscript𝐳𝑡𝐫\\bm{\\mathbf{z}}_{t}^{\\bm{\\mathbf{X}}},\\bm{\\mathbf{z}}_{t}^{\\bm{\\mathbf{D}}},%\n\\bm{\\mathbf{z}}_{t}^{\\bm{\\mathbf{r}}}bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_X end_POSTSUPERSCRIPT , bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_D end_POSTSUPERSCRIPT , bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_r end_POSTSUPERSCRIPTand are injected globally via cross-attention in the denoising U-Net, after CLIP encoding and a query transformer. The U-Net is fine-tuned via Eq.2. During inference, iteratively denoised latent features𝐳^0𝐗,𝐳^0𝐃,𝐳^0𝐫superscriptsubscript^𝐳0𝐗superscriptsubscript^𝐳0𝐃superscriptsubscript^𝐳0𝐫\\hat{\\bm{\\mathbf{z}}}_{0}^{\\bm{\\mathbf{X}}},\\hat{\\bm{\\mathbf{z}}}_{0}^{\\bm{%\n\\mathbf{D}}},\\hat{\\bm{\\mathbf{z}}}_{0}^{\\bm{\\mathbf{r}}}over^ start_ARG bold_z end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_X end_POSTSUPERSCRIPT , over^ start_ARG bold_z end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_D end_POSTSUPERSCRIPT , over^ start_ARG bold_z end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_r end_POSTSUPERSCRIPTare decoded by the fine-tuned VAE decoder, followed by multi-modal alignment optimization for coherent 4D reconstruction.",
                "position": 231
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07961/x3.png",
                "caption": "Figure 3:Qualitative resultscomparing Geo4D with MonST3R[113]. Attribute to our group-wise inference manner and prior geometry knowledge from pretrained video diffusion, our model successfully produces consistent 4D geometry under fast motion (first row) and deceptive reflection in the water (second row).",
                "position": 849
            },
            {
                "img": "https://arxiv.org/html/2504.07961/x4.png",
                "caption": "Figure 4:Qualitative video depth resultscomparing Geo4D with MonST3R[113]and DepthCrafter[22]. Owing to our proposed multi-modal training and alignment, as well as the prior knowledge from diffusion, our method can infer a more detailed structure (first row) and a more accurate spatial arrangement from video (second row).",
                "position": 886
            }
        ]
    },
    {
        "header": "5Discussion and Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Implementation details",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07961/x5.png",
                "caption": "Figure 5:Additional qualitatively results.Our method generalizes well to various scenes with different 4D objects and performs robustly against different camera and object motion.",
                "position": 2982
            }
        ]
    },
    {
        "header": "7Visualization",
        "images": []
    }
]