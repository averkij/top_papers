[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07961/x1.png",
                "caption": "",
                "position": 136
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07961/x2.png",
                "caption": "Figure 2:Overview of Geo4D.During training,\nvideo conditions are injected by locally concatenating the latent feature of the video with diffused geometric featuresğ³tğ—,ğ³tğƒ,ğ³tğ«superscriptsubscriptğ³ğ‘¡ğ—superscriptsubscriptğ³ğ‘¡ğƒsuperscriptsubscriptğ³ğ‘¡ğ«\\bm{\\mathbf{z}}_{t}^{\\bm{\\mathbf{X}}},\\bm{\\mathbf{z}}_{t}^{\\bm{\\mathbf{D}}},%\n\\bm{\\mathbf{z}}_{t}^{\\bm{\\mathbf{r}}}bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_X end_POSTSUPERSCRIPT , bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_D end_POSTSUPERSCRIPT , bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_r end_POSTSUPERSCRIPTand are injected globally via cross-attention in the denoising U-Net, after CLIP encoding and a query transformer. The U-Net is fine-tuned via Eq.2. During inference, iteratively denoised latent featuresğ³^0ğ—,ğ³^0ğƒ,ğ³^0ğ«superscriptsubscript^ğ³0ğ—superscriptsubscript^ğ³0ğƒsuperscriptsubscript^ğ³0ğ«\\hat{\\bm{\\mathbf{z}}}_{0}^{\\bm{\\mathbf{X}}},\\hat{\\bm{\\mathbf{z}}}_{0}^{\\bm{%\n\\mathbf{D}}},\\hat{\\bm{\\mathbf{z}}}_{0}^{\\bm{\\mathbf{r}}}over^ start_ARG bold_z end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_X end_POSTSUPERSCRIPT , over^ start_ARG bold_z end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_D end_POSTSUPERSCRIPT , over^ start_ARG bold_z end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_r end_POSTSUPERSCRIPTare decoded by the fine-tuned VAE decoder, followed by multi-modal alignment optimization for coherent 4D reconstruction.",
                "position": 231
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07961/x3.png",
                "caption": "Figure 3:Qualitative resultscomparing Geo4D with MonST3R[113]. Attribute to our group-wise inference manner and prior geometry knowledge from pretrained video diffusion, our model successfully produces consistent 4D geometry under fast motion (first row) and deceptive reflection in the water (second row).",
                "position": 849
            },
            {
                "img": "https://arxiv.org/html/2504.07961/x4.png",
                "caption": "Figure 4:Qualitative video depth resultscomparing Geo4D with MonST3R[113]and DepthCrafter[22]. Owing to our proposed multi-modal training and alignment, as well as the prior knowledge from diffusion, our method can infer a more detailed structure (first row) and a more accurate spatial arrangement from video (second row).",
                "position": 886
            }
        ]
    },
    {
        "header": "5Discussion and Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Implementation details",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07961/x5.png",
                "caption": "Figure 5:Additional qualitatively results.Our method generalizes well to various scenes with different 4D objects and performs robustly against different camera and object motion.",
                "position": 2982
            }
        ]
    },
    {
        "header": "7Visualization",
        "images": []
    }
]