[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09952/x1.png",
                "caption": "Figure 1:Pre-training paradigm comparison.Existing object-level pre-training methods usually follow a generative masked auto-encoding (MAE) paradigm. Their scene-level counterparts mostly leverage the contrastive learning paradigm. We propose a unified pre-training method that is applicable and effective to both object- and scene-level point clouds and models.",
                "position": 85
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09952/x2.png",
                "caption": "Figure 2:UniPre3D pre-training pipeline.Our proposed pre-training task involves predicting Gaussian parameters from the input point cloud. The 3D backbone network is expected to extract representative features, and 3D Gaussian splatting is implemented to render images for direct supervision. To incorporate additional texture information and adjust task complexity, we introduce a pre-trained image model and propose a scale-adaptive fusion block to accommodate varying data scales.",
                "position": 149
            },
            {
                "img": "https://arxiv.org/html/2506.09952/x3.png",
                "caption": "Figure 3:Visualization of UniPre3D pre-training outputs.The first row presents the input point clouds, followed by the reference view images in the second row. The third row displays the rendered images, which are supervised by the ground truth images shown in the fourth row. In the rightmost column, we illustrate a schematic diagram of the view selection principle for both object- and scene-level samples.",
                "position": 298
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09952/x4.png",
                "caption": "Figure 4:Visualization of UniPre3D pre-training outputs on object-level experiments.The first column presents the input point clouds, followed by the reference view images in the second column. The remaining rows display the rendered images.",
                "position": 2415
            },
            {
                "img": "https://arxiv.org/html/2506.09952/x5.png",
                "caption": "Figure 5:Visualization of UniPre3D pre-training outputs on scene-level experiments.The first column presents the input point clouds, followed by the reference view images in the second and third columns. The remaining columns display the rendered images (upper rows) and their ground truths (lower rows).",
                "position": 2418
            }
        ]
    },
    {
        "header": "Appendix BSupplementary Visualizations",
        "images": []
    }
]