[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Model Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04032/x1.png",
                "caption": "Figure 1:Architecture ofjina-vlm. Images are resized to fit a grid of up to 12 overlapping tiles, plus a global thumbnail. Each tile is a square 378×\\times378 crop; adjacent tiles overlap by 112 pixels with a stride of 266 pixels between tile origins. A 4×\\times3 grid therefore spans 1176×\\times910 pixels, and images exceeding this effective resolution are downscaled to fit the tile budget. Each tile produces 729 patches via SigLIP2(siglip2). The VL connector concatenates features from layers 24 and 18, the third- and ninth-to-last layers, then applies 2×\\times2 attention pooling to reduce 729 tokens to 182 before projecting to the decoder dimension. Visual tokens are combined with text embeddings for the Qwen3 decoder(qwen3).",
                "position": 110
            }
        ]
    },
    {
        "header": "4Training",
        "images": []
    },
    {
        "header": "5Evaluation",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04032/selected_samples/visualwebinstruct.png",
                "caption": "Figure 2:Answer questions given web documents.",
                "position": 1097
            },
            {
                "img": "https://arxiv.org/html/2512.04032/selected_samples/tatqa.png",
                "caption": "Figure 3:Financial table requiring numerical reasoning over text.",
                "position": 1116
            },
            {
                "img": "https://arxiv.org/html/2512.04032/selected_samples/docvqa.png",
                "caption": "Figure 4:Document image with question about textual fields.",
                "position": 1135
            },
            {
                "img": "https://arxiv.org/html/2512.04032/selected_samples/textvqa.png",
                "caption": "Figure 5:Photo with textual question needing OCR reading.",
                "position": 1154
            },
            {
                "img": "https://arxiv.org/html/2512.04032/selected_samples/vqav2.png",
                "caption": "Figure 6:General visual question answering on natural images.",
                "position": 1173
            },
            {
                "img": "https://arxiv.org/html/2512.04032/selected_samples/tallyqa.png",
                "caption": "Figure 7:Scene requiring counting and spatial reasoning accuracy.",
                "position": 1192
            },
            {
                "img": "https://arxiv.org/html/2512.04032/selected_samples/clevr.png",
                "caption": "Figure 8:Synthetic shapes testing compositional spatial reasoning.",
                "position": 1211
            },
            {
                "img": "https://arxiv.org/html/2512.04032/selected_samples/screenqa.png",
                "caption": "Figure 9:User interface screenshot with structured textual elements.",
                "position": 1230
            },
            {
                "img": "https://arxiv.org/html/2512.04032/selected_samples/pathvqa.png",
                "caption": "Figure 10:Microscopic pathology image for medical VQA.",
                "position": 1249
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]