[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02395/figures/skywork_logo.png",
                "caption": "",
                "position": 57
            },
            {
                "img": "https://arxiv.org/html/2512.02395/figures/teaser_bright_short.png",
                "caption": "Figure 1:Skywork-R1V4 30B (A3B) demonstrates exceptional proficiency in code-based image manipulation, text and image search, and web browsing, achieving performance on high-resolution perception benchmarks that rivals or surpasses larger-scale and specialized models, while also showing advantages in multimodal Deepsearch tasks.",
                "position": 65
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02395/x1.png",
                "caption": "(a)Our data processing pipeline.",
                "position": 133
            },
            {
                "img": "https://arxiv.org/html/2512.02395/x1.png",
                "caption": "(a)Our data processing pipeline.",
                "position": 136
            },
            {
                "img": "https://arxiv.org/html/2512.02395/figures/ciyun.png",
                "caption": "(b)Distribution of data functionalities.",
                "position": 141
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02395/figures/efficiency.png",
                "caption": "Figure 3:Comparison of model efficiency. The first row presents the results from single-round inference without tool usage. The reported time, average tokens, and tokens per second (TPS) are averaged across samples within each benchmark. The second row shows the results from multi-round inference with code and search tools enabled.",
                "position": 635
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion and Future Directions",
        "images": []
    },
    {
        "header": "6Contributions",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02395/figures/general_plan.jpg",
                "caption": "Figure 4:Plan Mode.",
                "position": 782
            },
            {
                "img": "https://arxiv.org/html/2512.02395/figures/search.png",
                "caption": "Figure 5:Skywork-R1V4 enables dynamic visual exploration by iteratively cropping and querying different regions of an image to locate target objects. Starting from a panoramic view of Paris, the model strategically zooms into high-activity zones (e.g., parks and sidewalks), progressively refining its focus until it successfully identifies a small white dog — demonstrating adaptive reasoning and spatial navigation for fine-grained visual understanding.",
                "position": 848
            },
            {
                "img": "https://arxiv.org/html/2512.02395/figures/teaser_chongqing.jpg",
                "caption": "Figure 6:Skywork-R1V4demonstrates flexible integration of image search and text search to accurately determine the geographic location of a photograph — here, identifying the JINC SAINT Hotel in Chongqing’s Yuzhong District by cross-referencing visual architectural cues with textual and geospatial data.",
                "position": 906
            },
            {
                "img": "https://arxiv.org/html/2512.02395/figures/case_interleave.jpg",
                "caption": "Figure 7:Skywork-R1V4 demonstrates interleaved thinking by dynamically alternating between image operations (e.g., cropping to focus on the crown) and multimodal search tools (image + text search) to identify key visual elements, and retrieve contextual knowledge — enabling precise, grounded answers to complex visual queries.",
                "position": 955
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]