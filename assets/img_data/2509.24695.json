[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24695/x1.png",
                "caption": "Figure 1:An overview of generated videos and inference latency and memory of SANA-Video.\nThe generation latency is measured under 50 denoising steps. Linear attention is more efficient for video generation and our block linear attention maintains a fixed memory requirement for long videos.",
                "position": 149
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3SANA-Video",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24695/x2.png",
                "caption": "Figure 2:Overview of SANA-Video.Fig.(a) A high-level block-wise autoregressive training pipeline based on our block causal KV cache. (Details in Sec.3.3).\nFig.(b) Our model pipeline, containing an Autoencoder, Re-writer, Linear DiT, and a text encoder.\nFig.(c) The detailed design of the added 3D RoPE in linear attention and the temporal convolution in our Linear DiT’s Mix-FFN.",
                "position": 217
            },
            {
                "img": "https://arxiv.org/html/2509.24695/x3.png",
                "caption": "Figure 3:Analysis of Linear Attention with RoPE.(a) Visual comparison of attention maps. First two plots compare vanilla softmax attention (Wan) to our linear attention without positional encoding. The latter two plots show our method’s effect: applying RoPE after the ReLU kernel results in a sparser, more localized attention pattern. (b) Training loss for the QK sum (Eq.2denominator). Removing RoPE from the denominator (green line) ensures training stability, as discussed in Sec.3.2.",
                "position": 260
            },
            {
                "img": "https://arxiv.org/html/2509.24695/x4.png",
                "caption": "Figure 4:Overview of Block Linear Attention.(a) We compare the attention compute mechanism among vanilla attention, linear attention and causal linear attention. (b) The illustration of block causal Mix-FFN in processing the adjacent blocks.",
                "position": 354
            },
            {
                "img": "https://arxiv.org/html/2509.24695/x5.png",
                "caption": "Figure 5:Data filtering paradigm of SANA-Video.",
                "position": 587
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24695/x6.png",
                "caption": "Figure 6:SANA-Video configuration ablation studies.(a) Training loss curves with and without 3D RoPE.\n(b) Training loss curves with and without temporal 2D Convolution.\n(c) Latency comparison of SANA-Video between linear and full attention.\n(d) Comparison of monotonically increasing versus random timestep sampling in autoregressive block training. Note that monotonically increasing sampling improves consistency across blocks.",
                "position": 803
            },
            {
                "img": "https://arxiv.org/html/2509.24695/x7.png",
                "caption": "Table 5:Comparison of autoregressive video generation methods on VBench.",
                "position": 824
            },
            {
                "img": "https://arxiv.org/html/2509.24695/x7.png",
                "caption": "Figure 7:Latency comparison of our model on BF16 and NVFP4 precision.",
                "position": 865
            }
        ]
    },
    {
        "header": "5Applications and Deployment",
        "images": []
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Appendix ALLM Usage",
        "images": []
    },
    {
        "header": "Appendix BMore Implementation Details",
        "images": []
    },
    {
        "header": "Appendix CMore Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24695/x8.png",
                "caption": "Figure 8:Qualitative comparison among T2V methods.SANA-Video has comparable motion control and video-text semantic alignment with state-of-the-art small diffusion models.",
                "position": 1062
            },
            {
                "img": "https://arxiv.org/html/2509.24695/x9.png",
                "caption": "Figure 9:Qualitative comparison among I2V methods.SANA-Video has better motion control and video-text semantic alignment.",
                "position": 1079
            },
            {
                "img": "https://arxiv.org/html/2509.24695/x10.png",
                "caption": "Figure 10:Visualization of image-to-video generation.SANA-Video can keep consistent with the first frame while generating realistic motion.",
                "position": 1084
            },
            {
                "img": "https://arxiv.org/html/2509.24695/x11.png",
                "caption": "Figure 11:The impact of motion score on I2V task.Higher motion score can lead to larger motion.",
                "position": 1100
            },
            {
                "img": "https://arxiv.org/html/2509.24695/x12.png",
                "caption": "Figure 12:Long video visualization of SANA-Video.",
                "position": 1113
            }
        ]
    },
    {
        "header": "Appendix DData Processing Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24695/x13.png",
                "caption": "Figure 13:An overview of the captioning pipeline.",
                "position": 1135
            },
            {
                "img": "https://arxiv.org/html/2509.24695/x14.png",
                "caption": "Figure 14:Analysis of the influence of SFT.Fine-tuning on the human preferred SFT data can improve the video details and adherence to the laws of physics.",
                "position": 1164
            }
        ]
    },
    {
        "header": "Appendix EWorld Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24695/x15.png",
                "caption": "Figure 15:Visualization of world model task generation.",
                "position": 1172
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]