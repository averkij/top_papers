[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21639/x1.png",
                "caption": "Figure 1:Performance comparison of OCRVerse on text-centric OCR tasks (top row) and vision-centric OCR tasks (bottom row). Since existing OCR methods primarily focus on text-centric scenarios, we compare against both specialized OCR models and general-purpose models for text-centric benchmarks, while comparing only against general-purpose models for vision-centric benchmarks.",
                "position": 122
            },
            {
                "img": "https://arxiv.org/html/2601.21639/x1.png",
                "caption": "",
                "position": 125
            },
            {
                "img": "https://arxiv.org/html/2601.21639/x2.png",
                "caption": "",
                "position": 130
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21639/x3.png",
                "caption": "Figure 2:Comprehensive data coverage of OCRVerse for holistic OCR.Left (Text-centric data):Nine document scenarios including natural scenes, books, magazines, papers, reports, slides, exam papers, notes, and newspapers, covering high-frequency text scenarios in daily life.Right (Vision-centric data):Six specialized scenarios including charts, webpages, icons, geometry, circuits, and molecules, focusing on professional structured content that requires code-level representations.",
                "position": 231
            },
            {
                "img": "https://arxiv.org/html/2601.21639/x4.png",
                "caption": "Figure 3:Multi-stage data construction pipeline integrating text-centric and vision-centric sources. Text-centric pipeline (top) processes open-source data, real-world PDFs, and synthetic data through cleaning and VLM-based annotation. Vision-centric pipeline (bottom) collects chart, webpage, and SVG data, etc., applies quality filtering, and generates annotations through visualization rendering and structure extraction.",
                "position": 255
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21639/x5.png",
                "caption": "Figure 4:OCRVerse training pipeline.Stage 1:SFT with unified cross-domain data.Stage 2:RL with domain-specific data construction and personalized reward mechanisms for text-centric (rule-based) and vision-centric (visual fidelity) optimization.",
                "position": 295
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]