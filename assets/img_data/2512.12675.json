[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.12675/media/scone_wo_bg_cropped.png",
                "caption": "",
                "position": 144
            },
            {
                "img": "https://arxiv.org/html/2512.12675/x1.png",
                "caption": "Figure 1:The distinction problem and challenges.(a) Problem.State-of-the-art methods have limitations in distinguishing target subjects specified by the instruction.(b) Challenge 1: semantic deficiency in generation.Reference image information from the understanding and generation experts in the unified model is used to compute semantic similarity with instruction.(c) Challenge 2: biased understanding and misaligned generation.“Und.” and “Und.+Gen.” indicate whether texture information from generation expert in the unified model is included to collaborate with understanding expert.\nThe unified model is BAGEL[deng2025bagel].",
                "position": 166
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.12675/x2.png",
                "caption": "Figure 2:Our motivation.(a) visualizes the early similarity between image token hidden states from the understanding and generation experts and text token hidden states within the unified model, showing that the former attends to semantic regions while the latter is less sensitive.\n(b) illustrates the collaboration between the understanding and generation experts within the unified model through end-to-end training.",
                "position": 208
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3The Scone model",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.12675/x3.png",
                "caption": "Figure 3:Understanding bridge strategy.Step 1: Understanding bridge formation.Early semantic alignment and attention masking enable the understanding expert to serve as thesemantic bridge.Step 2: Understanding bridge guidance.The generation expert is optimized under the guidance of the semantic bridge, enabled by unified understanding-generation modeling.",
                "position": 271
            }
        ]
    },
    {
        "header": "4The SconeEval benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.12675/x4.png",
                "caption": "Figure 4:Overview of our SconeEval benchmark.Char”: character, Obj”: object, “Sce”: scene. SconeEval evaluates target subject identification and generation in complex visual contexts. It provides 409 test cases across three domains with 19 case types and 6 subtasks, covering composition, distinction, and distinction & composition tasks.",
                "position": 399
            },
            {
                "img": "https://arxiv.org/html/2512.12675/x5.png",
                "caption": "Table 2:Quantitative comparison of existing models on OmniContext[wu2025omnigen2]benchmark.“Char. + Obj.” indicates Character + Object.†{\\dagger}indicates our base model.\nBest scores in each group are highlighted inbold.",
                "position": 455
            },
            {
                "img": "https://arxiv.org/html/2512.12675/x5.png",
                "caption": "Figure 5:Multi-candidate editing in our SconeEval benchmark construction.Edit images to create multi-candidate cases through subject addition. Task difficulty increases with the complexity of reference images and instructions.",
                "position": 731
            },
            {
                "img": "https://arxiv.org/html/2512.12675/x6.png",
                "caption": "Table 3:Quantitative comparison of existing models on our SconeEval benchmark.†{\\dagger}indicates our base model. “COM”: Composition score. “DIS”: Distinction score.\nBest scores in each group are highlighted inbold.",
                "position": 770
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.12675/x6.png",
                "caption": "Figure 6:Qualitative comparison of existing models on OmniContext[wu2025omnigen2]benchmark.",
                "position": 1146
            },
            {
                "img": "https://arxiv.org/html/2512.12675/x7.png",
                "caption": "Figure 7:Qualitative comparison of existing models on SconeEval benchmark.",
                "position": 1149
            },
            {
                "img": "https://arxiv.org/html/2512.12675/x8.png",
                "caption": "Figure 8:Stability measured by the standard deviation of scores on the SconeEval Benchmark.",
                "position": 1276
            }
        ]
    },
    {
        "header": "6Conclusion and future direction",
        "images": []
    },
    {
        "header": "Appendix AAdditional details of motivation",
        "images": []
    },
    {
        "header": "Appendix BAdditional details of training data",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.12675/x9.png",
                "caption": "Figure 9:Prompt for distinction scoring in SconeEval benchmark.It determines whether the described subject from the reference imageappearsin the target image.",
                "position": 1333
            },
            {
                "img": "https://arxiv.org/html/2512.12675/x10.png",
                "caption": "Figure 10:Representative similarity and masked images for each layer group.The similarity visualizations of instruction token hidden states and image token hidden states from understanding and generation experts are based on experiments with our base model, BAGEL[deng2025bagel]. The masked images are obtained by retaining the top 50% of regions for better observation.",
                "position": 1336
            }
        ]
    },
    {
        "header": "Appendix CTwo-step decoupling instruction construction in SconeEval benchmark",
        "images": []
    },
    {
        "header": "Appendix DParameter study of threshold in stage II",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.12675/x11.png",
                "caption": "Figure 11:Limitation of our Scone.",
                "position": 1429
            },
            {
                "img": "https://arxiv.org/html/2512.12675/x12.png",
                "caption": "Figure 12:Examples of synthesized data with 3 input images.This includes 4 case types, such as combinations of characters interacting with each other and objects, characters with multiple objects, characters in a scene, and different objects placed within a scene.",
                "position": 1440
            },
            {
                "img": "https://arxiv.org/html/2512.12675/x13.png",
                "caption": "Figure 13:Examples of synthesized data with 4 input images.This includes 9 case types, such as combinations of multiple characters, characters interacting with objects, different objects grouped together, characters in a scene, and objects placed within scenes, as well as various mixes of these elements.",
                "position": 1443
            },
            {
                "img": "https://arxiv.org/html/2512.12675/x14.png",
                "caption": "Figure 14:Data filtering for refined single-candidate data.(a) Prompt for training data filtering.Key components of the prompt are shown.(b) Results of training data filtering.Data is scored from 0 to 4, and only samples with a score of 4 are selected.",
                "position": 1446
            },
            {
                "img": "https://arxiv.org/html/2512.12675/x15.png",
                "caption": "Figure 15:Multi-candidate single-subject data construction.(a) Prompt for instruction construction.The prompt instructs the vision-language model to identify subjects, provide distinct descriptions, and generate instructions.(b) Example demonstration.This includes 2 case types: Character and Object, each containing both cross-category and intra-category candidate subjects in the reference images.",
                "position": 1449
            },
            {
                "img": "https://arxiv.org/html/2512.12675/x16.png",
                "caption": "Figure 16:Multi-candidate multi-subject data construction.(a) Prompts for subject replacement.The prompt instructs the language model to replace the original subject description with a new, distinct description corresponding to the new multi-candidate reference images.(b) Example demonstration.This includes 5 case types: Character+Character, Character+Object, Object+Object, Character+Scene, and Object+Scene, each containing both cross-category and intra-category candidate subjects in the reference images.",
                "position": 1454
            },
            {
                "img": "https://arxiv.org/html/2512.12675/x17.png",
                "caption": "Figure 17:Comparison between two-step decoupling and direct strategies for instruction construction.The two-step decoupling strategy separates the process into an image-to-text step and a text-to-text step, reducing cross-image interference and avoiding errors such as incorrect reference image indices, ambiguity of target subjects, and the introduction of unrelated subjects, which occur in the direct strategy.",
                "position": 1459
            },
            {
                "img": "https://arxiv.org/html/2512.12675/x18.png",
                "caption": "Figure 18:Prompts for instruction construction in SconeEval benchmark.(a) Prompt for subject identification.For Character or Object images, provide a clear and concise description; for Scene images, describe the overall setting and key objects.(b) Prompt for instruction generation.Generate instructions based on the provided subject descriptions, emphasizing interactions between subjects and between subjects and the scene.",
                "position": 1462
            }
        ]
    },
    {
        "header": "Appendix ELimitation",
        "images": []
    }
]