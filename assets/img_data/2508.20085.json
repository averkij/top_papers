[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20085/x1.png",
                "caption": "Figure 1:HERMES exhibits a rich spectrum of mobile bimanual dexterous manipulation skills.The robot is able to navigate over extended distances in both indoor and outdoor environments, and effectively execute a variety of complex manipulation tasks in unstructured, real-world scenarios, drawing upon behaviors learned from only one-shot human motion.",
                "position": 115
            }
        ]
    },
    {
        "header": "IIntroduction",
        "images": []
    },
    {
        "header": "IIRelated Work",
        "images": []
    },
    {
        "header": "IIISystem Design",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20085/x2.png",
                "caption": "Figure 2:System Design.We construct a unified setup of mobile bimanual robots equipped with dexterous hands in both simulation and the real world. Through high-fidelity simulation, this robotic platform is capable of enabling sim2real transfer across a wide range of complex manipulation tasks.",
                "position": 202
            },
            {
                "img": "https://arxiv.org/html/2508.20085/x3.png",
                "caption": "Figure 3:The main pipeline of HERMES.HERMES comprises a four-stage pipeline for achieving mobile bimanual dexterous manipulation through sim2real transfer. First, we acquire a one‑shot human demonstration drawn from diverse sources. Then, in stage 2, we train a state-based RL teacher policy, then apply DAgger to distill it into a vision‑based student policy. Following this, HERMES execute long‑horizon navigation using ViNT, followed by closed-loop PnP to finely adjust the robot’s pose and achieve precise alignment in stage 3. Once localization is achieved, the student policy is deployed in a zero‑shot fashion directly in the real world.",
                "position": 223
            }
        ]
    },
    {
        "header": "IVReinforcement Learning Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20085/x4.png",
                "caption": "Figure 4:Pose extraction from videos.We utilize FoundationPose to extract the pose trajectories of multiple objects and employ WiLoR to capture the poses of both hands along with the positions of their finger joints.",
                "position": 254
            },
            {
                "img": "https://arxiv.org/html/2508.20085/x5.png",
                "caption": "Figure 5:The visualization of hand motion trajectory.We utilize WiLoR along with a PnP algorithm to precisely transform the estimated hand poses into the robot’s frame.",
                "position": 257
            },
            {
                "img": "https://arxiv.org/html/2508.20085/x6.png",
                "caption": "Figure 6:Object-centric distance chain.This reward term is computed by tracking the temporal variations of vectors formed between the object center and each fingertip as well as the palm of both hands.",
                "position": 290
            }
        ]
    },
    {
        "header": "VSim-to-real Transfer",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20085/x7.png",
                "caption": "Figure 7:Depth image visualization.We present a visual comparison between simulated and real-world depth maps across two different tasks. Notably, after applying our preprocessing pipeline, the depth representations of the hand and object exhibit a strong semantic correspondence, highlighting the efficacy of HERMES in bridging the sim2real gap.",
                "position": 391
            },
            {
                "img": "https://arxiv.org/html/2508.20085/x8.png",
                "caption": "Figure 8:Depth intensity distribution.The horizontal axis represents the depth values, while the vertical axis indicates their corresponding proportions. This figure illustrates that the depth distributions derived from simulation and real-world images exhibit a notable resemblance in value patterns.",
                "position": 394
            },
            {
                "img": "https://arxiv.org/html/2508.20085/x9.png",
                "caption": "Figure 9:Hybrid Sim2real Control.We leverage real-world observations to infer actions and utilize simulation to compute the corresponding joint values, which are subsequently mapped onto the real robots. This hybrid strategy effectively mitigates the sim2real gap.",
                "position": 460
            }
        ]
    },
    {
        "header": "VINavigation Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20085/x10.png",
                "caption": "Figure 10:The pipeline of closed-loop PnP localization.We first employ the Efficient LoFTR to extract dense visual correspondence, followed by estimating the transformation between the current frame and the goal location via solving the PnP problem. Subsequently, we use PID controller to execute the action. This entire process is executed in a closed-loop manner and continues iteratively until the spatial discrepancy between the robot’s current pose and the goal falls below a predefined threshold.",
                "position": 509
            }
        ]
    },
    {
        "header": "VIIExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20085/x11.png",
                "caption": "Figure 11:The training curve of HERMES.The horizontal axis denotes the training steps, while the vertical axis represents the normalized task length successfully accomplished by the policy.Teleoprefers to one-shot human motion teleoperation in simulation,Human videodenotes trajectories extracted from video data, andMocapcorresponds to motion derived from mocap datasets. HERMES not only demonstrates the capability to accomplish diverse manipulation tasks originating from various forms of human motion, but also exhibits superior sample efficiency throughout training. All results are evaluated across 3 seeds.",
                "position": 574
            },
            {
                "img": "https://arxiv.org/html/2508.20085/x12.png",
                "caption": "Figure 12:The comparison of kinematic retargeting and HERMES.The raw trajectories extracted from human videos and mocap data are insufficient to complete the task through mere kinematic retargeting. On the other hand, HERMES not only learns to follow these reference trajectories but also masters the nuances of object interaction.",
                "position": 583
            },
            {
                "img": "https://arxiv.org/html/2508.20085/x13.png",
                "caption": "Figure 13:Simulation training visualization.We visualize the majority of the training tasks. Leveraging a single reference trajectory in conjunction with a general reward design, HERMES can convert diverse human motion sources into robot feasible behaviors via RL training.",
                "position": 664
            },
            {
                "img": "https://arxiv.org/html/2508.20085/x14.png",
                "caption": "Figure 14:The wall-time training efficiency.HERMES also enjoys high wall-time efficiency under parallel training. All results are evaluated across 3 seeds.",
                "position": 667
            },
            {
                "img": "https://arxiv.org/html/2508.20085/x15.png",
                "caption": "Figure 15:The visualization of navigation results.The left two columns depict a comparison between the target image and the terminal image achieved by our method. The right two columns present the point clouds captured at the end of navigation by ViNT and HERMES, compared against the point cloud of the target position. This figure illustrates that ViNT exhibits a noticeable mismatch between the captured and target point clouds at the end of navigation, whereas HERMES achieves a close alignment, which demonstrates the high localization accuracy of our approach.",
                "position": 678
            },
            {
                "img": "https://arxiv.org/html/2508.20085/x16.png",
                "caption": "Figure 16:The localization ability of HERMES in textureless scenarios.Even in environments with sparse visual features, HERMES remains capable of executing fine-grained positional adjustments and achieving precise localization through the closed-loop PnP mechanism.",
                "position": 814
            },
            {
                "img": "https://arxiv.org/html/2508.20085/x17.png",
                "caption": "Figure 17:Real-world mobile manipulation results.Dark-colored bars correspond to HERMES, whereas the light-colored bars correspond to only using ViNT. HERMES is capable of performing a wide array of complex mobile bimanual dexterous manipulation tasks. In contrast, when relying solely on ViNT for localization, the trained manipulation policy fails to complete the tasks.",
                "position": 852
            }
        ]
    },
    {
        "header": "VIIIConclusion",
        "images": []
    },
    {
        "header": "IXlimitations and future work",
        "images": []
    },
    {
        "header": "XAcknowledgment",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20085/x18.png",
                "caption": "Figure 18:Object generalization visualization.To help the policy adapt to different object shapes, we randomize each object’s geometry during training to prompt the policy to adjust its actions accordingly.",
                "position": 1807
            },
            {
                "img": "https://arxiv.org/html/2508.20085/x19.png",
                "caption": "Figure 19:Closed-loop PnP visualization.Across diverse scenarios, the closed-loop PnP algorithm iteratively refines the robot’s pose and ultimately aligns it with the desired target pose with high precision.",
                "position": 1813
            },
            {
                "img": "https://arxiv.org/html/2508.20085/x20.png",
                "caption": "Figure 20:DAgger training efficiency.HERMES attains high sample efficiency and asymptotic performance across different types of tasks.",
                "position": 1832
            },
            {
                "img": "https://arxiv.org/html/2508.20085/x21.png",
                "caption": "Figure 21:The comparison of sent target joint position and the actual reached joint position in both simulation and real-world.The red, blue, and green curves exhibit similar overall trends. Compared with the blue and green curves, the red curve shows a deviation in joint position at the same time step, while the blue and green traces overlaps throughout. This discrepancy points to the gap between the simulated and real-world dynamics, and adopting the hybrid control keeping the dynamics consistency between the simulation and the real robot.",
                "position": 1840
            },
            {
                "img": "https://arxiv.org/html/2508.20085/x22.png",
                "caption": "Figure 22:Feature matcher visualization of Efficient LoFTR.Efficient LoFTR is capable of establishing correspondences between the target RGB image and the current RGB frame at a high frequency.",
                "position": 1887
            },
            {
                "img": "https://arxiv.org/html/2508.20085/x23.png",
                "caption": "Figure 23:Comparison of Depth-anything generated images between simulation and real-world.Depth-Anything can produce depth images that are semantically aligned between simulation and real-world.",
                "position": 1903
            },
            {
                "img": "https://arxiv.org/html/2508.20085/x24.png",
                "caption": "Figure 24:Depth intensity distribution of Depth-anything generated images.The red curve shows the depth-value distribution extracted from real-world RGB images, whereas the blue curve corresponds to that obtained from simulated images. Despite their semantic alignment, the depth maps estimated by Depth-Anything reveal a pronounced quantitative gap between the simulated and real-world domains.",
                "position": 1906
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]