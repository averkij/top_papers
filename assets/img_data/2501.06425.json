[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.06425/x1.png",
                "caption": "Figure 1:Tensor Product Attention (TPA) in theTensor ProducTATTenTionTransformer (T6). Different from multi-head attention, in each layer, firstly the hidden state goes through different linear layers to get the latent factor matricesùêÄùêÄ\\mathbf{A}bold_A‚Äôs andùêÅùêÅ\\mathbf{B}bold_B‚Äôs for query, key, and value. We additionally apply RoPE toùêÅQsubscriptùêÅùëÑ\\mathbf{B}_{Q}bold_B start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPTandùêÅKsubscriptùêÅùêæ\\mathbf{B}_{K}bold_B start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPTfor query and key. Then the multi-head query, key, and value vectors are attained by the tensor product ofùêÄ(‚ãÖ)subscriptùêÄ‚ãÖ\\mathbf{A}_{(\\cdot)}bold_A start_POSTSUBSCRIPT ( ‚ãÖ ) end_POSTSUBSCRIPTandùêÅ(‚ãÖ)subscriptùêÅ‚ãÖ\\mathbf{B}_{(\\cdot)}bold_B start_POSTSUBSCRIPT ( ‚ãÖ ) end_POSTSUBSCRIPT. Finally, the output of TPA is produced by scaled dot-product attention followed by linear projection of concatenated results of multiple heads.",
                "position": 102
            },
            {
                "img": "https://arxiv.org/html/2501.06425/x2.png",
                "caption": "(a)Training Loss",
                "position": 137
            },
            {
                "img": "https://arxiv.org/html/2501.06425/x2.png",
                "caption": "(a)Training Loss",
                "position": 140
            },
            {
                "img": "https://arxiv.org/html/2501.06425/x3.png",
                "caption": "(b)Validation Loss",
                "position": 145
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Tensor Product Attention",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.06425/x4.png",
                "caption": "(a)Training Loss",
                "position": 1237
            },
            {
                "img": "https://arxiv.org/html/2501.06425/x4.png",
                "caption": "(a)Training Loss",
                "position": 1240
            },
            {
                "img": "https://arxiv.org/html/2501.06425/x5.png",
                "caption": "(b)Validation Loss",
                "position": 1245
            },
            {
                "img": "https://arxiv.org/html/2501.06425/x6.png",
                "caption": "(a)Validation Perplexity of Medium Models",
                "position": 1252
            },
            {
                "img": "https://arxiv.org/html/2501.06425/x6.png",
                "caption": "(a)Validation Perplexity of Medium Models",
                "position": 1255
            },
            {
                "img": "https://arxiv.org/html/2501.06425/x7.png",
                "caption": "(b)Validation Perplexity of Large Models",
                "position": 1260
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProofs of Theorems",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.06425/x8.png",
                "caption": "(a)Training Loss",
                "position": 3177
            },
            {
                "img": "https://arxiv.org/html/2501.06425/x8.png",
                "caption": "(a)Training Loss",
                "position": 3180
            },
            {
                "img": "https://arxiv.org/html/2501.06425/x9.png",
                "caption": "(b)Validation Loss",
                "position": 3185
            },
            {
                "img": "https://arxiv.org/html/2501.06425/x10.png",
                "caption": "(c)Validation Perplexity",
                "position": 3190
            }
        ]
    },
    {
        "header": "Appendix BMore on Experiments",
        "images": []
    }
]