[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21144/extracted/6313754/fig/overview.png",
                "caption": "",
                "position": 75
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21144/extracted/6313754/fig/video_driven.png",
                "caption": "Figure 2:Pipeline of upper-body video generation with hybrid control fusion, which takes both explicit facial keypoints and implicit body keypoints to conduct feature warping, while rendered hand image further inject into generator for improving the quality of hand generation.",
                "position": 135
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21144/extracted/6313754/fig/audio2motion.png",
                "caption": "Figure 3:Illustration of hierarchical audio2motion diffusion model, including facial motion prediction with style control at bottom, and upper-body motion prediction with hands at top.",
                "position": 240
            },
            {
                "img": "https://arxiv.org/html/2503.21144/extracted/6313754/fig/face_offset.png",
                "caption": "Figure 4:Illustration of face refine network, the left of figure shows the architecture, while the right demonstrates that more precise facial keypoints are located by adding implicit offset.",
                "position": 327
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21144/extracted/6313754/fig/visualization_main.png",
                "caption": "Figure 5:Qualitative comparisons of upper-body video generation under self-driven reenactment setup. Our approach significantly outperforms the GAN-based comparison methods, and achieves comparable quality with the diffusion-based method EchoMimicV2.",
                "position": 538
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]