[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21060/extracted/6484331/figures/teaser4.0.png",
                "caption": "Figure 1:Styl3R.Given unposed sparse-view images and an arbitrary style image, our method predicts stylized 3D Gaussians in less than a second using a feed-forward network.",
                "position": 121
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21060/extracted/6484331/figures/pipeline_v4.png",
                "caption": "Figure 2:Overview of Styl3R.Our model consists of a structure branch and an appearance branch that output different attributes of Gaussians. For the structure branch, sparse unposed images are encoded by a shared content encoder, then content tokens of each image are separately fed into their structure decoders with information sharing between other views. Attributes that govern the structure of the scene are then regressed from structure heads. For the color branch, a style image is encoded by the style encoder, then the output style tokens perform cross attention with content tokens from all viewpoints in the stylization decoder.\nFinally the color of Gaussians are predicted from these blended tokens output by this decoder, which compose all Gaussian parameters along with the output from structure branch.\nApart from style image, the appearance branch can also accept a content image which gives the Gaussians their original colors.",
                "position": 298
            },
            {
                "img": "https://arxiv.org/html/2505.21060/x1.png",
                "caption": "Figure 3:Novel View Transfer Comparision on RE10K.Despite limited image overlap, our method generates stylized novel views that more faithfully capture style details while preserving the original scene structure.\nIn comparison, StyleRF[22]and StyleGaussian[23]tend to produce over-smoothed results that deviate from the true color tone of the reference style.\nARF[51]suffers from style overflow, leading to significant loss of content appearance.\nAs a 2D baseline, StyTr2[4]operates directly on ground-truth novel views, but fails to retain fine structural details of the scene.",
                "position": 411
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21060/x2.png",
                "caption": "Figure 4:Cross-dataset generalization on Tanks and Temples dataset.Our model achieves superior or comparable zero-shot style transfer on out-of-distribution data, outperforming style-free baselines such as StyleRF[22]and StyleGaussian[23]that require per-scene optimization, and matching the performance of ARF[51], which further demands per-scene and per-style optimization.",
                "position": 442
            },
            {
                "img": "https://arxiv.org/html/2505.21060/x3.png",
                "caption": "Table 2:Quantitative Results.Performance comparison of Styl3R with 2D and 3D baselines on RE10K in terms of view consistency. Stylization time refers to processing time excluding IO time.",
                "position": 445
            },
            {
                "img": "https://arxiv.org/html/2505.21060/x3.png",
                "caption": "Figure 5:Visual Comparison.Visualizations of different views produced by StyTr2[4]and our method. The highlighted regions (lamp and bed sheet) show noticeable color discrepancies in StyTr2, while our approach maintains consistent color across views.",
                "position": 557
            },
            {
                "img": "https://arxiv.org/html/2505.21060/extracted/6484331/figures/identity_loss2.0.png",
                "caption": "Table 3:NVS comparison on RE10K.* denotes 0-degree spherical harmonics, as used in our model, while[46]defaults to 4-degree.",
                "position": 579
            },
            {
                "img": "https://arxiv.org/html/2505.21060/extracted/6484331/figures/identity_loss2.0.png",
                "caption": "Figure 6:Ablations.NVS results w/o and w/ identity loss during stylization fine-tuning. The former fails to retain the true color tone of the scene.",
                "position": 647
            },
            {
                "img": "https://arxiv.org/html/2505.21060/extracted/6484331/figures/ablation_h3-h42.0.png",
                "caption": "Figure 7:Ablations.Stylization results of model trained with content loss consist of different layers. Usingrelu3_1| andrelu4_1| in content loss preserve the original scene appearance more faithfully.",
                "position": 654
            },
            {
                "img": "https://arxiv.org/html/2505.21060/extracted/6484331/figures/multi-view2.0.png",
                "caption": "Figure 8:Ablations.Results for 2-view and 4-view models when inputting 2 and 8 content images. The 2-view model fails to align Gaussians across multiple views when provided with 8 input views.",
                "position": 694
            },
            {
                "img": "https://arxiv.org/html/2505.21060/extracted/6484331/figures/style_interpolation3.0.png",
                "caption": "Figure 9:Application.Style Interpolation with 2 style images by interpolating their style tokens. It can be observed that the style of the scene smoothly transit from one to another.",
                "position": 697
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AMore Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21060/x4.png",
                "caption": "Figure 10:Novel View Transfer Comparision on RE10K.Our method faithfully preserves style and scene structure, even with limited image overlap. In contrast, StyleRF[22]and StyleGaussian[23]produce over-smoothed results with inaccurate color tones, while ARF[51]suffers from style overflow.",
                "position": 1592
            },
            {
                "img": "https://arxiv.org/html/2505.21060/x5.png",
                "caption": "Figure 11:Additional Results on RE10K from Our Method.",
                "position": 1596
            },
            {
                "img": "https://arxiv.org/html/2505.21060/x6.png",
                "caption": "Figure 12:Additional Results on RE10K from Our Method.",
                "position": 1599
            },
            {
                "img": "https://arxiv.org/html/2505.21060/x7.png",
                "caption": "Figure 13:Additional Results on RE10K from Our Method.",
                "position": 1602
            },
            {
                "img": "https://arxiv.org/html/2505.21060/x8.png",
                "caption": "Figure 14:Additional Results on RE10K from Our Method.",
                "position": 1605
            }
        ]
    },
    {
        "header": "Appendix BMore Visual Results",
        "images": []
    }
]