[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04144/x1.png",
                "caption": "",
                "position": 114
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04144/x2.png",
                "caption": "Figure 2:Overall architecture of ourChirpy3D.(Top)During training, we fine-tune a text-to-multi-view diffusion model (e.g., MVDream) with only 2D images of birds. We aim to learn the underlying part information by modeling a continuous part-aware latent space. This is achieved by learning a set of species embeddingsùíÜùíÜ\\bm{e}bold_italic_e, project them into part latentsùíçùíç\\bm{l}bold_italic_lthrough learnablefùëìfitalic_f, decode into word embeddingsùíïùíï\\bm{t}bold_italic_tthrough learnablegùëîgitalic_gand insert into text prompt. We train the diffusion model with diffusion loss (Eq.5) and multiple loss objectives ‚Äì‚Ñíregsubscript‚Ñíreg\\mathcal{L}_{\\text{reg}}caligraphic_L start_POSTSUBSCRIPT reg end_POSTSUBSCRIPT(Eq.2) to model part latents as Gaussian distribution,‚Ñíattnsubscript‚Ñíattn\\mathcal{L}_{\\text{attn}}caligraphic_L start_POSTSUBSCRIPT attn end_POSTSUBSCRIPT(Eq.6) for part disentanglement, and our proposed‚Ñíclsubscript‚Ñícl\\mathcal{L}_{\\text{cl}}caligraphic_L start_POSTSUBSCRIPT cl end_POSTSUBSCRIPT(Eq.4) to enhance visual coherency.fùëìfitalic_fandgùëîgitalic_gare trainable modules. For efficient training, we added LoRA layers into cross-attention layers of the U-Net.(Bottom)During inference, we can first preview multi-view images by selecting desired part latents as condition\nbefore turning them into 3D representations (e.g., NeRF) through SDS loss‚ÑíSDSsubscript‚ÑíSDS\\mathcal{L}_{\\text{SDS}}caligraphic_L start_POSTSUBSCRIPT SDS end_POSTSUBSCRIPT.",
                "position": 148
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04144/x3.png",
                "caption": "Figure 3:As we do not have images of unseen part latents, we use real natural images as our proxy. We extract cross-attention feature mapsFùêπFitalic_Fof two noised latents, then minimize the discrepancy between the two feature maps. This will encourage the model to compute similar feature maps for any given part latents, which indirectly stabilizes the denoising process for unseen latents.",
                "position": 233
            },
            {
                "img": "https://arxiv.org/html/2501.04144/x4.png",
                "caption": "Figure 4:(a)Seen part selection generation. Unseen part synthesis via(b)novel sampling and(c)interpolation.",
                "position": 336
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/cls/subject_ti_2x.jpg",
                "caption": "(a)Textual Inversion",
                "position": 363
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/cls/subject_ti_2x.jpg",
                "caption": "(a)Textual Inversion",
                "position": 366
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/cls/subject_partcraft_2x.jpg",
                "caption": "(b)PartCraft",
                "position": 371
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/cls/subject_ours_2x.jpg",
                "caption": "(c)Chirpy3D (Ours)",
                "position": 376
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04144/x5.png",
                "caption": "Figure 6:Visual comparison of part composition.A,B,C,D,E,Fùê¥ùêµùê∂ùê∑ùê∏ùêπA,B,C,D,E,Fitalic_A , italic_B , italic_C , italic_D , italic_E , italic_Frepresentcardinal, wilson warbler, least auklet, california gull, horned lark, and song sparrowrespectively. Red circles indicate changed parts. All generated (including sources & targets) by the same seed.",
                "position": 456
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/interpolation/interpolate_ti.jpg",
                "caption": "(a)Textual Inversion",
                "position": 497
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/interpolation/interpolate_ti.jpg",
                "caption": "(a)Textual Inversion",
                "position": 500
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/interpolation/interpolate_partcraft.jpg",
                "caption": "(b)PartCraft",
                "position": 506
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/interpolation/interpolate_ours.jpg",
                "caption": "(c)Chirpy3D (Ours)",
                "position": 512
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/random/tsne_part_ti.png",
                "caption": "(a)Textual Inversion",
                "position": 531
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/random/tsne_part_ti.png",
                "caption": "(a)Textual Inversion",
                "position": 534
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/random/tsne_partcraft.png",
                "caption": "(b)PartCraft",
                "position": 540
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/random/tsne_ours_kl0001.png",
                "caption": "(c)Chirpy3D (Ours)",
                "position": 546
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/random/random_ti.jpg",
                "caption": "(a)Textual Inversion",
                "position": 610
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/random/random_ti.jpg",
                "caption": "(a)Textual Inversion",
                "position": 613
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/consistency/consistency_partcraft.jpg",
                "caption": "(b)PartCraft",
                "position": 618
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/random/random_ours.jpg",
                "caption": "(c)Chirpy3D (Ours)",
                "position": 623
            },
            {
                "img": "https://arxiv.org/html/2501.04144/x6.png",
                "caption": "Figure 10:NeRF rendering of learned 3D objects.",
                "position": 635
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/consistency/consistency_baseline.jpg",
                "caption": "(a)Ours without‚Ñíclsubscript‚Ñícl\\mathcal{L}_{\\text{cl}}caligraphic_L start_POSTSUBSCRIPT cl end_POSTSUBSCRIPT",
                "position": 647
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/consistency/consistency_baseline.jpg",
                "caption": "(a)Ours without‚Ñíclsubscript‚Ñícl\\mathcal{L}_{\\text{cl}}caligraphic_L start_POSTSUBSCRIPT cl end_POSTSUBSCRIPT",
                "position": 650
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/consistency/consistency_ours.jpg",
                "caption": "(b)Ours with‚Ñíclsubscript‚Ñícl\\mathcal{L}_{\\text{cl}}caligraphic_L start_POSTSUBSCRIPT cl end_POSTSUBSCRIPT",
                "position": 656
            },
            {
                "img": "https://arxiv.org/html/2501.04144/x7.png",
                "caption": "Figure 12:A hybrid(middle)betweensiberian husky(left)andpapillon(right), trained with Stanford Dogs[27].",
                "position": 669
            }
        ]
    },
    {
        "header": "5Discussion and Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix B3D Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04144/x8.png",
                "caption": "Figure 13:Optimization-based 3D generation with NeRF or 3DGS.",
                "position": 1696
            },
            {
                "img": "https://arxiv.org/html/2501.04144/x9.png",
                "caption": "Figure 14:Image-to-3D using front view and side view of generated object.",
                "position": 1706
            }
        ]
    },
    {
        "header": "Appendix COther domains",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/fg3d_dog.jpg",
                "caption": "(a)Mixingchowwithgolden retriever,pomeranianwithpug.",
                "position": 1717
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/fg3d_dog.jpg",
                "caption": "(a)Mixingchowwithgolden retriever,pomeranianwithpug.",
                "position": 1720
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/fg3d_animal.jpg",
                "caption": "(b)Hamster-cat,Hamster-horse,Elephant-horse",
                "position": 1726
            }
        ]
    },
    {
        "header": "Appendix DDerivation of regularization loss",
        "images": []
    },
    {
        "header": "Appendix EMulti-view generation on common token and fine-grained token",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/reason_gs75_bird_10.jpg",
                "caption": "(a)a bird, 3d asset.",
                "position": 1798
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/reason_gs75_bird_10.jpg",
                "caption": "(a)a bird, 3d asset.",
                "position": 1801
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/reason_gs75_cardinal_10.jpg",
                "caption": "(b)a cardinal, 3d asset.",
                "position": 1807
            }
        ]
    },
    {
        "header": "Appendix FMulti-view generation on existing species",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/supp_cls_ti.jpg",
                "caption": "(a)Textual Inversion",
                "position": 1819
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/supp_cls_ti.jpg",
                "caption": "(a)Textual Inversion",
                "position": 1822
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/supp_cls_partcraft.jpg",
                "caption": "(b)PartCraft",
                "position": 1828
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/supp_cls_ours_d32_kl001.jpg",
                "caption": "(c)Chirpy3D (Ours)",
                "position": 1834
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/supp_cls_target.jpg",
                "caption": "(d)",
                "position": 1839
            }
        ]
    },
    {
        "header": "Appendix GMulti-view generation on novel species (random sampling)",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/supp_random_ti.jpg",
                "caption": "(a)Textual Inversion",
                "position": 1851
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/supp_random_ti.jpg",
                "caption": "(a)Textual Inversion",
                "position": 1854
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/supp_random_partcraft.jpg",
                "caption": "(b)PartCraft",
                "position": 1860
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/supp_random_ours_d32_kl001.jpg",
                "caption": "(c)Chirpy3D (Ours)",
                "position": 1866
            }
        ]
    },
    {
        "header": "Appendix HMulti-view generation on novel species (interpolation)",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04144/x10.png",
                "caption": "(a)Textual Inversion",
                "position": 1878
            },
            {
                "img": "https://arxiv.org/html/2501.04144/x10.png",
                "caption": "(a)Textual Inversion",
                "position": 1881
            },
            {
                "img": "https://arxiv.org/html/2501.04144/x11.png",
                "caption": "(b)PartCraft",
                "position": 1887
            },
            {
                "img": "https://arxiv.org/html/2501.04144/x12.png",
                "caption": "(c)Chirpy3D (Ours)",
                "position": 1893
            }
        ]
    },
    {
        "header": "Appendix IQualitative comparisons of visual coherency before and after applying feature consistency loss",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/supp_consistency_random_baseline.jpg",
                "caption": "(a)",
                "position": 1905
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/supp_consistency_random_baseline.jpg",
                "caption": "(a)",
                "position": 1908
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/supp_consistency_random_ours_d4.jpg",
                "caption": "(b)",
                "position": 1914
            }
        ]
    },
    {
        "header": "Appendix JFurther Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04144/x13.png",
                "caption": "Figure 21:Inverse part latents used to reconstruct the image with 1,000 learning steps.",
                "position": 1926
            },
            {
                "img": "https://arxiv.org/html/2501.04144/x14.png",
                "caption": "Figure 22:Cross-attention map of multi-view images. The cross-attention map is averaged over all layers with size16√ó16161616\\times 1616 √ó 16.",
                "position": 1934
            },
            {
                "img": "https://arxiv.org/html/2501.04144/x15.png",
                "caption": "Figure 23:Visualizing part latent space via t-SNE embeddings.",
                "position": 1969
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/scale1.jpg",
                "caption": "(a)0.10.10.10.1",
                "position": 1979
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/scale1.jpg",
                "caption": "(a)0.10.10.10.1",
                "position": 1982
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/scale01.jpg",
                "caption": "(b)0.010.010.010.01",
                "position": 1988
            },
            {
                "img": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/scale001.jpg",
                "caption": "(c)0.0010.0010.0010.001",
                "position": 1994
            }
        ]
    },
    {
        "header": "Appendix KAblation Study",
        "images": []
    }
]