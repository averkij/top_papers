[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01686/Figures/logo_v4.png",
                "caption": "",
                "position": 50
            },
            {
                "img": "https://arxiv.org/html/2512.01686/Figures/Figure1_v5.png",
                "caption": "",
                "position": 66
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01686/Figures/Figure3_v5.png",
                "caption": "Figure 2:Overview of our image customization pipeline. The input reference images are encoded as token sequencesc1:nc_{1:n}along with the noise latentztz_{t}and the text latentzpz_{p}, passed to the stream of diffusion transformer blocks. We calculate a custom regional RoPE from the layout condition and apply it to the encoded references. During training, we calculate a Masked Condition Loss between the cross-attention map and the given layout condition, encouraging the model to position references within the layout.",
                "position": 244
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01686/Figures/Figure2_v2.png",
                "caption": "Figure 3:Given a list of textual descriptions for each panel, the finetuned LLM outputs a spatial layout for each panel and characters as a set of bounding boxes. Note that our layout, compared to the layout generated from the same prompt using GPT-4[Achiam2023GPT4TR], occupies most of the panel region, correctly orders the panel (top-to-bottom, right-to-left), and draws plausible character boxes, constituting a “good comic layout”.",
                "position": 307
            },
            {
                "img": "https://arxiv.org/html/2512.01686/Figures/Figure4a.png",
                "caption": "(a)Reference Image with original RoPE",
                "position": 325
            },
            {
                "img": "https://arxiv.org/html/2512.01686/Figures/Figure4a.png",
                "caption": "(a)Reference Image with original RoPE",
                "position": 328
            },
            {
                "img": "https://arxiv.org/html/2512.01686/Figures/Figure4b.png",
                "caption": "(b)Generated Image",
                "position": 333
            },
            {
                "img": "https://arxiv.org/html/2512.01686/Figures/Figure4c.png",
                "caption": "(c)Reference Image with regional RoPE",
                "position": 339
            },
            {
                "img": "https://arxiv.org/html/2512.01686/Figures/Figure4d.png",
                "caption": "(d)Generated Image",
                "position": 344
            },
            {
                "img": "https://arxiv.org/html/2512.01686/Figures/Figure5.png",
                "caption": "Figure 5:Comparison between theCAMsof our model trained without the masked condition loss (top) and with the masked condition loss (bottom). Using our new loss helps to position the attention around the target layout, which is evident in the third column (Transformer layer = 2), naturally inducing the model during training to position the character.",
                "position": 351
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01686/Figures/Figure9_v3.png",
                "caption": "Figure 6:Image level qualitative comparison against other methods, along with their respective scores. Our method shows enhanced visual consistency with respect to the reference images, which is reflected in the similar aesthetic scores.",
                "position": 646
            },
            {
                "img": "https://arxiv.org/html/2512.01686/Figures/Figure6.png",
                "caption": "Figure 7:Story-level qualitative comparison of DreamingComics. Methods marked with “∗\\ast” use the layout condition generated from our layout generator. Methods marked with “†\\dagger” use the reference images. Better viewed with zoom-in.",
                "position": 709
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    }
]