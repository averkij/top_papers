[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3SFTMix",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05248/x1.png",
                "caption": "Figure 1:The overall pipeline of the proposed SFTMix recipe for LLM instruction tuning. Given an SFT datasetğ’Ÿğ’Ÿ\\mathcal{D}caligraphic_D, we (1) train a reference LLM onğ’Ÿğ’Ÿ\\mathcal{D}caligraphic_Dusing NTP and (2) computeConfâ¢(ğ’´i|ğ’³i)Confconditionalsubscriptğ’´ğ‘–subscriptğ’³ğ‘–\\text{Conf}\\,(\\mathcal{Y}_{i}\\,|\\,\\mathcal{X}_{i})Conf ( caligraphic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )for(ğ’³i,ğ’´i)âˆˆğ’Ÿsubscriptğ’³ğ‘–subscriptğ’´ğ‘–ğ’Ÿ(\\mathcal{X}_{i},\\mathcal{Y}_{i})\\in\\mathcal{D}( caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) âˆˆ caligraphic_Dbased on the training dynamics of the reference LLM. On this basis, we (3) divideğ’Ÿğ’Ÿ\\mathcal{D}caligraphic_Dinto a confident subsetğ’Ÿcsuperscriptğ’Ÿc\\mathcal{D}^{\\text{c}}caligraphic_D start_POSTSUPERSCRIPT c end_POSTSUPERSCRIPTand a relatively unconfident oneğ’Ÿusuperscriptğ’Ÿu\\mathcal{D}^{\\text{u}}caligraphic_D start_POSTSUPERSCRIPT u end_POSTSUPERSCRIPTof equal size. Finally, given pairs of examples from each subset, we (4) interpolate their one-hot encodings and predicted representations linearly at the token level and (5) incorporate a Mixup-based regularizationâ„“Mixupâ¢(ğ’Ÿc,ğ’Ÿu)subscriptâ„“Mixupsuperscriptğ’Ÿcsuperscriptğ’Ÿu\\ell_{\\text{Mixup}}(\\mathcal{D}^{\\text{c}},\\mathcal{D}^{\\text{u}})roman_â„“ start_POSTSUBSCRIPT Mixup end_POSTSUBSCRIPT ( caligraphic_D start_POSTSUPERSCRIPT c end_POSTSUPERSCRIPT , caligraphic_D start_POSTSUPERSCRIPT u end_POSTSUPERSCRIPT )in addition to the NTP lossâ„“NTPâ¢(ğ’Ÿ)subscriptâ„“NTPğ’Ÿ\\ell_{\\text{NTP}}(\\mathcal{D})roman_â„“ start_POSTSUBSCRIPT NTP end_POSTSUBSCRIPT ( caligraphic_D )during LLM instruction tuning.",
                "position": 250
            },
            {
                "img": "https://arxiv.org/html/2410.05248/x2.png",
                "caption": "(a)",
                "position": 295
            },
            {
                "img": "https://arxiv.org/html/2410.05248/x2.png",
                "caption": "(a)",
                "position": 298
            },
            {
                "img": "https://arxiv.org/html/2410.05248/x3.png",
                "caption": "(b)",
                "position": 303
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05248/x4.png",
                "caption": "Figure 3:Examples from the extraction category in MT-Bench. Compared to its NTP-tuned counterpart, Llama-3.1-8B instruction-tuned on Alpaca-52K using SFTMix accurately interprets the queries from both turns and correctly extracts the relevant information from the prompt.",
                "position": 516
            }
        ]
    },
    {
        "header": "5Ablation and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05248/x5.png",
                "caption": "Figure 4:Distribution of confidence in datasets of varying qualities by Llama-3.1-8B.",
                "position": 887
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperiment Details",
        "images": []
    }
]