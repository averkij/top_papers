[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3SFTMix",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05248/x1.png",
                "caption": "Figure 1:The overall pipeline of the proposed SFTMix recipe for LLM instruction tuning. Given an SFT dataset𝒟𝒟\\mathcal{D}caligraphic_D, we (1) train a reference LLM on𝒟𝒟\\mathcal{D}caligraphic_Dusing NTP and (2) computeConf⁢(𝒴i|𝒳i)Confconditionalsubscript𝒴𝑖subscript𝒳𝑖\\text{Conf}\\,(\\mathcal{Y}_{i}\\,|\\,\\mathcal{X}_{i})Conf ( caligraphic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )for(𝒳i,𝒴i)∈𝒟subscript𝒳𝑖subscript𝒴𝑖𝒟(\\mathcal{X}_{i},\\mathcal{Y}_{i})\\in\\mathcal{D}( caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ∈ caligraphic_Dbased on the training dynamics of the reference LLM. On this basis, we (3) divide𝒟𝒟\\mathcal{D}caligraphic_Dinto a confident subset𝒟csuperscript𝒟c\\mathcal{D}^{\\text{c}}caligraphic_D start_POSTSUPERSCRIPT c end_POSTSUPERSCRIPTand a relatively unconfident one𝒟usuperscript𝒟u\\mathcal{D}^{\\text{u}}caligraphic_D start_POSTSUPERSCRIPT u end_POSTSUPERSCRIPTof equal size. Finally, given pairs of examples from each subset, we (4) interpolate their one-hot encodings and predicted representations linearly at the token level and (5) incorporate a Mixup-based regularizationℓMixup⁢(𝒟c,𝒟u)subscriptℓMixupsuperscript𝒟csuperscript𝒟u\\ell_{\\text{Mixup}}(\\mathcal{D}^{\\text{c}},\\mathcal{D}^{\\text{u}})roman_ℓ start_POSTSUBSCRIPT Mixup end_POSTSUBSCRIPT ( caligraphic_D start_POSTSUPERSCRIPT c end_POSTSUPERSCRIPT , caligraphic_D start_POSTSUPERSCRIPT u end_POSTSUPERSCRIPT )in addition to the NTP lossℓNTP⁢(𝒟)subscriptℓNTP𝒟\\ell_{\\text{NTP}}(\\mathcal{D})roman_ℓ start_POSTSUBSCRIPT NTP end_POSTSUBSCRIPT ( caligraphic_D )during LLM instruction tuning.",
                "position": 250
            },
            {
                "img": "https://arxiv.org/html/2410.05248/x2.png",
                "caption": "(a)",
                "position": 295
            },
            {
                "img": "https://arxiv.org/html/2410.05248/x2.png",
                "caption": "(a)",
                "position": 298
            },
            {
                "img": "https://arxiv.org/html/2410.05248/x3.png",
                "caption": "(b)",
                "position": 303
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05248/x4.png",
                "caption": "Figure 3:Examples from the extraction category in MT-Bench. Compared to its NTP-tuned counterpart, Llama-3.1-8B instruction-tuned on Alpaca-52K using SFTMix accurately interprets the queries from both turns and correctly extracts the relevant information from the prompt.",
                "position": 516
            }
        ]
    },
    {
        "header": "5Ablation and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05248/x5.png",
                "caption": "Figure 4:Distribution of confidence in datasets of varying qualities by Llama-3.1-8B.",
                "position": 887
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperiment Details",
        "images": []
    }
]