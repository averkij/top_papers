[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.14659/x1.png",
                "caption": "",
                "position": 116
            },
            {
                "img": "https://arxiv.org/html/2511.14659/img/nora1.5-new.png",
                "caption": "",
                "position": 122
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3NORA-1.5",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.14659/x2.png",
                "caption": "Figure 1:Training pipeline ofNORA-1.5where firstly a VLA model is pre-trained through imitation learning and subsequently a preference dataset of the actions is created for preference optimization.WMstands for WM-guided goal-based reward (Eq.6) andGTAstands for the reward based on ground-truth action (Eq.7).",
                "position": 203
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.14659/img/nora2-fast.png",
                "caption": "Figure 2:Comparing FAST+ with flow-matching.",
                "position": 869
            },
            {
                "img": "https://arxiv.org/html/2511.14659/img/failed_2.png",
                "caption": "(a)NORA-1.5without DPO. The gripper trajectory exhibits frequent fixations and zig-zag motions, often resulting in failed grasps and grasp attempts toward distractor objects.",
                "position": 1558
            },
            {
                "img": "https://arxiv.org/html/2511.14659/img/failed_2.png",
                "caption": "(a)NORA-1.5without DPO. The gripper trajectory exhibits frequent fixations and zig-zag motions, often resulting in failed grasps and grasp attempts toward distractor objects.",
                "position": 1561
            },
            {
                "img": "https://arxiv.org/html/2511.14659/img/success.png",
                "caption": "(b)NORA-1.5with reward-driven DPO post-training. The gripper trajectory becomes smoother and more consistent, with\nfewer corrective strokes and more reliable target grasps.",
                "position": 1567
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix AEvaluation Settings and Metrics",
        "images": []
    },
    {
        "header": "Appendix BRelated Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.14659/x3.png",
                "caption": "(a)",
                "position": 1763
            },
            {
                "img": "https://arxiv.org/html/2511.14659/x3.png",
                "caption": "(a)",
                "position": 1766
            },
            {
                "img": "https://arxiv.org/html/2511.14659/x4.png",
                "caption": "(b)",
                "position": 1772
            },
            {
                "img": "https://arxiv.org/html/2511.14659/x5.png",
                "caption": "Figure 5:Examples ofNORA-1.5executing evaluation tasks with Galaxea A1 robotic arm in the real world.",
                "position": 1779
            }
        ]
    },
    {
        "header": "Appendix CModel Architecture and Training Details",
        "images": []
    },
    {
        "header": "Appendix DGalaxea Data Collection",
        "images": []
    },
    {
        "header": "Appendix EBaselines",
        "images": []
    }
]