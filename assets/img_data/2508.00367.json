[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.00367/x1.png",
                "caption": "Figure 1:Comparison of importance metrics for token pruning (average over 7 video-text retrieval benchmarks inTable2). Pruning with a conventional attention-based score (Attn) yields poor speed-accuracy trade-offs on UMT-L and is incompatible with FlashAttention (FA). In contrast, our proposed representation shift accelerates both vanilla UMT-L and UMT-L with FlashAttention, achieving superior trade-offs compared to downscaling to UMT-B and attention-based scores.",
                "position": 69
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.00367/x2.png",
                "caption": "Figure 2:Illustration of representation shift for token importance. We compute the L2 distance between token representations before and after the MLP layer to quantify how much each token is emphasized by the transformation.",
                "position": 121
            },
            {
                "img": "https://arxiv.org/html/2508.00367/x3.png",
                "caption": "Table 1:Comparison of FlashAttention[16]with standard self-attention. Throughputs are measured with NVIDIA RTX A6000. ImageNet[18]and MSRVTT[68]are used for image and video understanding, respectively.",
                "position": 124
            },
            {
                "img": "https://arxiv.org/html/2508.00367/x3.png",
                "caption": "(a)DeiT-S",
                "position": 173
            },
            {
                "img": "https://arxiv.org/html/2508.00367/x3.png",
                "caption": "(a)DeiT-S",
                "position": 176
            },
            {
                "img": "https://arxiv.org/html/2508.00367/x4.png",
                "caption": "(b)UMT-B",
                "position": 182
            }
        ]
    },
    {
        "header": "2Related works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.00367/x5.png",
                "caption": "Figure 4:Visualization of representation shift. Given the image (left), we visualize (right) the representation shift (Œî‚Äãùê±\\Delta\\mathbf{x}roman_Œî bold_x) of each token before and after the attention layer.",
                "position": 296
            },
            {
                "img": "https://arxiv.org/html/2508.00367/x6.png",
                "caption": "",
                "position": 299
            },
            {
                "img": "https://arxiv.org/html/2508.00367/x7.png",
                "caption": "",
                "position": 301
            },
            {
                "img": "https://arxiv.org/html/2508.00367/x8.png",
                "caption": "",
                "position": 302
            },
            {
                "img": "https://arxiv.org/html/2508.00367/x9.png",
                "caption": "(a)Operation choice",
                "position": 320
            },
            {
                "img": "https://arxiv.org/html/2508.00367/x9.png",
                "caption": "(a)Operation choice",
                "position": 323
            },
            {
                "img": "https://arxiv.org/html/2508.00367/x10.png",
                "caption": "",
                "position": 327
            },
            {
                "img": "https://arxiv.org/html/2508.00367/x11.png",
                "caption": "(b)Distance metric",
                "position": 334
            },
            {
                "img": "https://arxiv.org/html/2508.00367/x12.png",
                "caption": "",
                "position": 338
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.00367/x13.png",
                "caption": "Figure 6:Qualitative comparison between attention scores (Attn) and representation shift (Ours). Given each sample, we visualize (a) the attention scores with respect to the class token and (b) representation shift in the [1,5,9] layers of the DeiT-B[52].",
                "position": 1128
            },
            {
                "img": "https://arxiv.org/html/2508.00367/x14.png",
                "caption": "Figure 7:Visualization of representation shift in ResNet-50[21].",
                "position": 1258
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements.",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]