[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05564/x1.png",
                "caption": "",
                "position": 117
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05564/x2.png",
                "caption": "Figure 2:Overview of our proposed ProPhy framework. ProPhy uses a progressive physical alignment design, consisting of theSemantic Expert Blockand theRefinement Expert Block. During inference, the model runs end-to-end and aligns physics categories through our proposed blocks.",
                "position": 169
            },
            {
                "img": "https://arxiv.org/html/2512.05564/x3.png",
                "caption": "Figure 3:Pipeline for annotating token-level physical attributes using a VLM.",
                "position": 253
            },
            {
                "img": "https://arxiv.org/html/2512.05564/x4.png",
                "caption": "Figure 4:Study of the attention localization capabilities of VDM and VLM. The VDM cross-attention maps are obtained by adding 10% noise and then denoising. As shown, despite minor imperfections, the VLM-based approach more accurately identifies the locations of the corresponding physical phenomena.",
                "position": 256
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05564/x5.png",
                "caption": "Figure 5:Qualitative comparison among ProPhy, CogVideoX, Wan2.1, and existing physics-aware methods.",
                "position": 562
            },
            {
                "img": "https://arxiv.org/html/2512.05564/x6.png",
                "caption": "Figure 6:Analysis of the semantic router.rrrepresents the Pearson correlation coefficient calculated between different distributions.",
                "position": 717
            },
            {
                "img": "https://arxiv.org/html/2512.05564/x7.png",
                "caption": "Figure 7:Refinement router expert maps. High-activation regions accurately localize where corresponding physical events occur, demonstrating the REB’s fine-grained physical alignment.",
                "position": 720
            },
            {
                "img": "https://arxiv.org/html/2512.05564/x8.png",
                "caption": "Figure 8:Physical attribute transfer via expert inversion. Flipping semantic-router logits injects incorrect physical cues, causing implausible behaviors (e.g., a rigid car door fluttering), revealing that different experts encode distinct physical priors.",
                "position": 723
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BAnalysis and Ablation",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05564/x9.png",
                "caption": "Figure 9:Principal component analysis of the activation distribution of the Semantic Router under different input prompt categories.",
                "position": 831
            },
            {
                "img": "https://arxiv.org/html/2512.05564/x10.png",
                "caption": "Figure 10:Details of the two types of user inputs used to obtain token-level physical properties annotations. The angle brackets ’<><>’ are replaced with the specified physical phenomenon or the given video.",
                "position": 844
            },
            {
                "img": "https://arxiv.org/html/2512.05564/x11.png",
                "caption": "Figure 11:Human analysis of fine-grained physical annotation accuracy.",
                "position": 847
            },
            {
                "img": "https://arxiv.org/html/2512.05564/x12.png",
                "caption": "Figure 12:Qualitative ablation analysis on the functional roles of each module.",
                "position": 863
            }
        ]
    },
    {
        "header": "Appendix CLimitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05564/x13.png",
                "caption": "Figure 13:Comparison between ProPhy with different backbones and previous methods, including the baseline. More generated examples are provided in the supplementary video.",
                "position": 895
            },
            {
                "img": "https://arxiv.org/html/2512.05564/x14.png",
                "caption": "Figure 14:Examples of videos generated by ProPhy in response to text prompts involving complex physical phenomena.",
                "position": 898
            }
        ]
    },
    {
        "header": "Appendix DSocial Impact",
        "images": []
    }
]