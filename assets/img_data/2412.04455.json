[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04455/x1.png",
                "caption": "",
                "position": 144
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04455/x2.png",
                "caption": "Figure 2:Overview of Code-as-Monitor. Given task instructions and prior information, the Constraint Generator derives the next subgoal and corresponding textual constraints based on multi-view observations.\nThe Painter maps these constraints onto images as constraint elements.\nThe Monitor generates monitor code from these images and tracks them for real-time monitoring.\nIf any constraint is violated, it outputs the reason for failure and triggers re-planning.\nThis framework unifiesreactiveandproactivefailure detection via constraints, more generally abstracts relevant entities/parts through constraint elements, and ensures precise and real-time monitoring via code evaluation.",
                "position": 224
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04455/x3.png",
                "caption": "Figure 3:Constraint Element Pipeline. Given a constraint, our modelConSeggenerates instance-level and part-level masks across multiple views, which are projected into 3D space.\nThrough a series of heuristics, the desired elements are produced. Once all elements are obtained, they are annotated onto the original multi-view images. Here we display the annotation result of one element.",
                "position": 263
            },
            {
                "img": "https://arxiv.org/html/2412.04455/x4.png",
                "caption": "Figure 4:ConSegarchitecture. Here we display the part-level segmentation, which will output the desired element type and mask.",
                "position": 369
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04455/x5.png",
                "caption": "Figure 5:Example of Real-world Evaluation.\nThe red bounding box shows the current grasp target, which may shift due to environmental changes.CaMmonitors and adapts to these changes in real-time, resulting in a closed-loop system with an open-loop policy.",
                "position": 684
            },
            {
                "img": "https://arxiv.org/html/2412.04455/x6.png",
                "caption": "Figure 6:Visual comparison between ourConSegand LISA[28]at instance and part level. The red masks are the segmentation results.",
                "position": 1042
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AConstraint Painter",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04455/x7.png",
                "caption": "Figure 7:Dataset Collection Pipeline. Our data is sourced from BridgeData V2[32]. The data collection process consists of three steps:(1)Using GPT-4o[1]to decompose the task instruction based on the initial observation from the first frame of the trajectory, generating subgoals along with two types of constraints for each subgoal (i.e., constraints during execution and upon completion) and object-part associations.(2)Utilizing external references (e.g., gripper open/close states) to assign subgoals, constraints, and object-part associations to each frame. (3) Leveraging off-the-shelf models (e.g., Grounded SAM[51], Semantic SAM[33]) to generate instance- and part-level masks¬†(blue mask in this figure) automatically, followed by manual filtering to curate the final dataset.",
                "position": 2338
            }
        ]
    },
    {
        "header": "Appendix BEnvironment Configuration",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04455/x8.png",
                "caption": "Figure 8:Environmental Setup of three simulators and one real-world setting.\nFor CLIPort[56]and OmniGibson[32], we provide third-person front and top views and the robot platforms are the UR5 arm and Fetch, respectively.\nRLBench[24]offers four camera views, including front left shoulder, right shoulder, and wrist views, with the robot platform being Franka equipped with a gripper.\nWe provide a wrist and a third-person front view for the real-world setting, utilizing a UR5 robot equipped with a Leap Hand[53].",
                "position": 2404
            },
            {
                "img": "https://arxiv.org/html/2412.04455/x9.png",
                "caption": "Figure 9:CLIPort task demonstration. we present three types of tasks in our experiments, including the starting and ending frames.",
                "position": 2433
            },
            {
                "img": "https://arxiv.org/html/2412.04455/x10.png",
                "caption": "Figure 10:Omnigibson task demonstration. we present three types of tasks in our experiments, including the starting and ending frames.",
                "position": 2647
            }
        ]
    },
    {
        "header": "Appendix CEvaluation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04455/x11.png",
                "caption": "Figure 11:RLBench task demonstration. we present six types of tasks in our experiments, including the starting and ending frames.",
                "position": 2731
            }
        ]
    },
    {
        "header": "Appendix DLimitations and Future Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04455/x12.png",
                "caption": "Figure 12:Demonstration of ‚ÄúStack in Order‚Äù. We show how our framework detects failures and assists in recovery when the placement positions predicted by the policy for the ‚ÄúStack in Order‚Äù task are subject to a uniform[0,q]0ùëû[0,q][ 0 , italic_q ]cm interference. Red boxes indicate the occurrence of failures, while green boxes signify successful task execution.",
                "position": 3299
            },
            {
                "img": "https://arxiv.org/html/2412.04455/x13.png",
                "caption": "Figure 13:Demonstration of ‚ÄúStack in Order‚Äù. We show how our framework performs failure detection and aids in recovery when, in the ‚ÄúStack in Order‚Äù task, there is a probabilitypùëùpitalic_pthat blocks will fall due to being released by the robot‚Äôs suction cup at each step. Red boxes indicate the occurrence of failures, while green boxes signify successful task execution.",
                "position": 3304
            },
            {
                "img": "https://arxiv.org/html/2412.04455/x14.png",
                "caption": "Figure 14:Demonstration of ‚ÄúSweep Half the Blocks‚Äù and comparison to baseline. We show our framework can precisely count the blocks within a specified area and timely halts the policy execution to complete the task. In contrast, DoReMi[16]fails to stop the policy execution in time, leading to task failure. Red boxes indicate the occurrence of failures, while green boxes signify successful task execution.",
                "position": 3309
            },
            {
                "img": "https://arxiv.org/html/2412.04455/x15.png",
                "caption": "Figure 15:Demonstration of ‚ÄúUse Rope to Close the Opening Square‚Äù and comparison to baseline. We show that our framework effectively detects when the rope closes the opening square and promptly stops the policy execution to complete the task successfully. Conversely, DoReMi fails to halt the policy execution on time; although it eventually succeeds in closing the opening, the excessive execution time results in task failure. Red boxes indicate the occurrence of failures, while green boxes signify successful task execution.",
                "position": 3314
            },
            {
                "img": "https://arxiv.org/html/2412.04455/x16.png",
                "caption": "Figure 16:Demonstration of ‚ÄúSlot Pen‚Äù. We show how our framework detects failures and assists in recovery when facing point-level disturbances. Red boxes indicate the occurrence of failures, light green indicates the recovery with subgoal success and dark green boxes signify successful task execution.",
                "position": 3326
            },
            {
                "img": "https://arxiv.org/html/2412.04455/x17.png",
                "caption": "Figure 17:Demonstration of ‚ÄúStow Book‚Äù. We show how our framework detects failures and assists in recovery when facing line-level disturbances. Red boxes indicate the occurrence of failures, light green indicates the recovery with subgoal success and dark green boxes signify successful task execution.",
                "position": 3331
            },
            {
                "img": "https://arxiv.org/html/2412.04455/x18.png",
                "caption": "Figure 18:Demonstration of ‚ÄúPour Tea‚Äù. We show how our framework detects failures and assists in recovery when facing surface-level disturbances. Red boxes indicate the occurrence of failures, light green indicates the recovery with subgoal success and dark green boxes signify successful task execution.",
                "position": 3336
            },
            {
                "img": "https://arxiv.org/html/2412.04455/x19.png",
                "caption": "Figure 19:Demonstration of ‚ÄúClear all objects on the table except for animals‚Äù. We show that our framework achieves both reactive failure detection (e.g., detecting unexpected failures when humans remove objects from the robot‚Äôs grasp) and proactive failure detection (e.g., identifying target object movement during grasping to prevent foreseeable failures).\nThis effectively enhances the task success rate and reduces the execution time.",
                "position": 3349
            },
            {
                "img": "https://arxiv.org/html/2412.04455/x20.png",
                "caption": "Figure 20:Visualization of constraint-aware segmentation for the RoboFail Dataset[41]. This dataset is not included in the training data.",
                "position": 3363
            },
            {
                "img": "https://arxiv.org/html/2412.04455/x21.png",
                "caption": "Figure 21:Visualization of constraint-aware segmentation for the Open6DOF[11]. This dataset is not included in the training data.",
                "position": 3368
            },
            {
                "img": "https://arxiv.org/html/2412.04455/x22.png",
                "caption": "Figure 22:Visualization of constraint-aware segmentation for the RT-1 dataset[3]. This dataset is not included in the training data.",
                "position": 3373
            },
            {
                "img": "https://arxiv.org/html/2412.04455/x23.png",
                "caption": "Figure 23:Visualization of constraint-aware segmentation for the Omnigibsom simulator.",
                "position": 3378
            },
            {
                "img": "https://arxiv.org/html/2412.04455/x24.png",
                "caption": "Figure 24:Prompt of monitor code generation. We use this prompt, combined with additional task instructions, the current subgoal, and images from two perspectives, to enable an off-the-shelf VLM,i.e., GPT-4o[1], to generate Python code for monitoring.",
                "position": 3390
            }
        ]
    },
    {
        "header": "Appendix EMore Demonstrations and Prompts",
        "images": []
    }
]