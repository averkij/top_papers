[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19949/x1.png",
                "caption": "",
                "position": 105
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19949/x2.png",
                "caption": "Figure 2:Overview of the Probe.We extract video features using various video foundation models and keep the features frozen. We sample four frames from the original video clip and fetch the corresponding feature maps from the video features. We train the probe by taking these spatial features as input, and task the probe to estimate point maps, depth maps and camera poses. Our probe model consists of a shallow transformer and three readout heads. We measure the estimation errors as the main indicators of 3D awareness.",
                "position": 179
            }
        ]
    },
    {
        "header": "3Approach",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19949/x3.png",
                "caption": "Figure 3:CO3Dv2 qualitative results.For each scene, we show input frames and the unprojected 3D points prediction. Fast3R, WAN2.1-14B, and Open-Sora2.0 best preserve intricate details (e.g., the truckâ€™s gripper) and reconstruct the overall structure.",
                "position": 399
            },
            {
                "img": "https://arxiv.org/html/2512.19949/x4.png",
                "caption": "Figure 4:DL3DV qualitative results.On this more challenging dataset, DINOv2 sometimes fails catastrophically. Top video generators often retain coherent geometry, where WAN2.1-14B produces the sharpest and most accurate point clouds overall.",
                "position": 402
            },
            {
                "img": "https://arxiv.org/html/2512.19949/figures/heat-time-layer-wan.png",
                "caption": "(a)WAN2.1",
                "position": 417
            },
            {
                "img": "https://arxiv.org/html/2512.19949/figures/heat-time-layer-wan.png",
                "caption": "(a)WAN2.1",
                "position": 420
            },
            {
                "img": "https://arxiv.org/html/2512.19949/figures/heat-time-layer-opensora.png",
                "caption": "(b)Open-Sora2.0",
                "position": 425
            },
            {
                "img": "https://arxiv.org/html/2512.19949/figures/heat-time-layer-cogvideox.png",
                "caption": "(c)CogVideoX",
                "position": 430
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19949/x5.png",
                "caption": "Figure 6:Data scaling on CO3Dv2 and DL3DV.For each dataset we report point map error, depth error, and AUC@30 against the fraction of data used to train the model. The horizontal dashed line denotes the performance of the original VGGT trained with 100% of the 3D data. VidFM VGGT typically outperforms this full-data baseline with less than 10% of the 3D training data.",
                "position": 1517
            },
            {
                "img": "https://arxiv.org/html/2512.19949/x5.png",
                "caption": "",
                "position": 1525
            },
            {
                "img": "https://arxiv.org/html/2512.19949/x6.png",
                "caption": "",
                "position": 1530
            },
            {
                "img": "https://arxiv.org/html/2512.19949/x7.png",
                "caption": "",
                "position": 1534
            },
            {
                "img": "https://arxiv.org/html/2512.19949/x8.png",
                "caption": "",
                "position": 1535
            },
            {
                "img": "https://arxiv.org/html/2512.19949/x9.png",
                "caption": "",
                "position": 1537
            },
            {
                "img": "https://arxiv.org/html/2512.19949/x10.png",
                "caption": "",
                "position": 1539
            }
        ]
    },
    {
        "header": "Appendix AAblation on Probe Size",
        "images": []
    },
    {
        "header": "Appendix BData Scaling for VidFM VGGT",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19949/figures/corr-pmap-3Dconsistency-xiang.png",
                "caption": "Figure 7:3D awareness vs. multi-view consistency.Scatter plot of3D Probe Error(lower is better) versusCross-view Correspondence Error(lower is better). Within the family of video diffusion models, the 3D probe error positively correlates with the multi-view correspondence error. DINOv2 and V-JEPA achieve great multi-view correspondence, while performing significantly worse in 3D probing experiments. This suggests that cross-view feature similarity may not be a sufficient proxy for measuring 3D awareness, especially when comparing across families of models.",
                "position": 1581
            }
        ]
    },
    {
        "header": "Appendix CAnalysis on Multi-view Consistency",
        "images": []
    }
]