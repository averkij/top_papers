[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06951/assets/logo.png",
                "caption": "",
                "position": 114
            },
            {
                "img": "https://arxiv.org/html/2509.06951/",
                "caption": "",
                "position": 122
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06951/x4.png",
                "caption": "Figure 1:The comparison of varied paradigms for manipulation policies.The earliest end-to-end manipulation policies are illustrated inFig.1(a), such as ACT(Zhao et al.,2023)and DP(Chi et al.,2023).\nPolicies shown inFig.1(b) introduce VLM, aiming to empower the action expert with the general capabilities of VLM in scene and instruction understanding, likeπ0\\pi_{0}(Black et al.,2024)and gr00t-N1(Bjorck et al.,2025). There are also approaches, as seen inFig.1(c), e.g., VPP(Hu et al.,2024)and Genie Envisioner(Liao et al.,2025b), that leverage video diffusion models to guide action execution through video prediction.\nAs depicted inFig.1(d), we adopts an integrated architecture of understanding, generation, and execution, empowering the action execution module with capabilities in both scene and instruction comprehension as well as dynamic temporal prediction.",
                "position": 159
            }
        ]
    },
    {
        "header": "2Theℱ1\\mathcal{F}_{1}Framework: Bridging Perception, Foresight, to Actions",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06951/x5.png",
                "caption": "Figure 2:Overview ofℱ1\\mathcal{F}_{1}framework. It employs the Mixture-of-Transformer(Liang et al.,2025)architecture comprising three core components: an understanding expert, a generation expert, and an action expert.\nThe understanding expert processes instructions and observations to generate a foresight image.\nThe visual foresight is then fed to action expert to predict a target action via the predictive inverse dynamics modeling(Tian et al.,2024c; Du et al.,2023).",
                "position": 224
            },
            {
                "img": "https://arxiv.org/html/2509.06951/x6.png",
                "caption": "Figure 3:Visualization of Residual VQ-VAE from 16×\\times16 to 256×\\times256 resolution.",
                "position": 255
            },
            {
                "img": "https://arxiv.org/html/2509.06951/x7.png",
                "caption": "Figure 4:Overview of real-world robot experiments.\nWe conduct 12 real-world robotic experiments on three different platforms: Genie-1, Franka, and ARX LIFT II.\nThe experiments on Genie-1 are designed to evaluate the model’s ability to handle task diversity.\nThe Franka experiments assess the model’s quick adaptation capabilities, while the ARX LIFT II tasks are used to benchmark its performance on challenging long-horizon manipulation problems.",
                "position": 390
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06951/x8.png",
                "caption": "Table 4:Ablation Studies on the Simulation Benchmark.We design five model variants to verify the effects of different components ofℱ1\\mathcal{F}_{1}: (a) Frozen-Gen, (b) Cotrain-Scratch, (c) No-Gen, (d) 2-Scales, and (e) 6-Scales.",
                "position": 1045
            },
            {
                "img": "https://arxiv.org/html/2509.06951/x8.png",
                "caption": "Figure 5:Ablation Studies on Real-world Tasks.We compareℱ1\\mathcal{F}_{1}withπ0\\pi_{0}as well as a variant that removes Pretrain Stage II.\nFor each task, we conduct 15 trials to ensure statistical reliability.\nThe results show that without Pretrain Stage II,ℱ1\\mathcal{F}_{1}suffers a substantial performance drop, even falling belowπ0\\pi_{0}.\nIn contrast, incorporating Pretrain Stage II leads to marked improvements, withℱ1\\mathcal{F}_{1}significantly surpassing the baseline.",
                "position": 1310
            },
            {
                "img": "https://arxiv.org/html/2509.06951/x9.png",
                "caption": "Figure 6:Visualization of dynamic manipulation task.A kitchen environment is set up with a moving belt, where the robot must grasp a specified food item according to a language prompt while objects continuously move along the belt.",
                "position": 1323
            }
        ]
    },
    {
        "header": "4Relationships between Generation Quality and Actions",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06951/figures/fig8_metrics_all_final.png",
                "caption": "Figure 8:Generation Quality across Training Steps.We report the evolution of generation quality along three dimensions: scene consistency (left), object consistency (middle), and task progress following (right). The x-axis denotes training steps, and different curves correspond to distinct task subsets (AgibotWorld, Bridge, Fractal, and LIBERO). Higher scores indicate a better alignment between the generated future observations and the ground-truth task progression.",
                "position": 1538
            },
            {
                "img": "https://arxiv.org/html/2509.06951/x10.png",
                "caption": "Figure 9:Visualization of Generated Future Images.It demonstrates the ability ofℱ1\\mathcal{F}_{1}to generate plausible next-step frames for various manipulation tasks, including supermarket item pickup, supermarket packing, and folding shorts. Each row compares a Ground Truth frame with a Prediction fromℱ1\\mathcal{F}_{1}, showcasing its accurate foresight across diverse scenarios.",
                "position": 1586
            },
            {
                "img": "https://arxiv.org/html/2509.06951/figures/fig10_correlation_by_dataset.png",
                "caption": "Figure 10:Correlation between Image and Action Token Accuracy.It shows the relationship between image token accuracy and action token accuracy across the four LIBERO suites.\nEach subfigure corresponds to one suite, and action accuracy is reported at different tolerance levels (τ\\tau=0.01, 0.02, 0.05).\nThe consistent positive correlation across all settings suggests that higher-quality visual foresight is closely aligned with improved action prediction reliability.",
                "position": 1660
            }
        ]
    },
    {
        "header": "5Related work",
        "images": []
    },
    {
        "header": "6Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADataset Details",
        "images": []
    },
    {
        "header": "Appendix BTraining Details",
        "images": []
    },
    {
        "header": "Appendix CReal-world Task Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06951/x11.png",
                "caption": "Figure 11:Basic Pick-and-Place Manipulation.Examples of fundamental grasping and placement tasks across different object types and target containers.",
                "position": 2966
            },
            {
                "img": "https://arxiv.org/html/2509.06951/x12.png",
                "caption": "Figure 12:Fine-Grained Precision Manipulation.The robot is instructed to pick up the flower and insert it into the vase, which requites the precise control for handling delicate objects with narrow target constraints.",
                "position": 2985
            },
            {
                "img": "https://arxiv.org/html/2509.06951/x13.png",
                "caption": "Figure 13:Dual-Arm Coordination and Handover.Examples of bimanual coordination and human-robot interaction tasks demonstrating inter-arm object transfer and collaborative handover capabilities.",
                "position": 3005
            },
            {
                "img": "https://arxiv.org/html/2509.06951/x14.png",
                "caption": "Figure 14:Dynamic Environment Adaptation.Example of real-time tracking and motion prediction capabilities in continuously changing environments. The robot successfully acquires specific food items from a moving belt.",
                "position": 3024
            },
            {
                "img": "https://arxiv.org/html/2509.06951/x15.png",
                "caption": "Figure 15:Long-Horizon Sequential Manipulation.Example of extended multi-step task execution requiring comprehensive planning, tool usage, and task coherence maintenance.",
                "position": 3042
            }
        ]
    },
    {
        "header": "Appendix DDeploy Platform and Latency Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06951/x16.png",
                "caption": "Figure 16:Prompt Template for Future Observation Quality Evaluation.We adopt a MLLM as evaluator, i.e., Qwen2.5-VL-32B-Instruct, and design a prompt template from three aspects to evaluate the quality of generated future observation: 1) Scene Consistency, 2) Object Consistency, and 3) Task Progress Following.",
                "position": 3113
            }
        ]
    },
    {
        "header": "Appendix EPrompt Template",
        "images": []
    }
]