[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.16975/x1.png",
                "caption": "Figure 1:Scaling trend for Over-Encoded models and baselines on OLMo2. We plot the loss with 400B tokens‚Äô training. For over-encoding, input vocabulary size is extended from 0.1 to 1.2 and 12.8 million (12√ó12\\times12 √óand128√ó128\\times128 √ólarger than baseline), referred to as OE-1.2M and OE-12.8M. We observe OE-12.8M with 400M parameters matches the baseline with 1B parameters.",
                "position": 160
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.16975/x2.png",
                "caption": "Figure 2:Performance comparison for models trained on CFG data. The left panel compares 1-gram and 3-gram tokenizers, showing that 3-gram improves larger (85M parameters) models but harms smaller (2.4M parameters) ones. The right panel examines 3-gram usage in encoders and decoders, revealing consistent gains with 3-gram encoders regardless of model size, while 3-gram decoders degrade performance in smaller models.",
                "position": 208
            },
            {
                "img": "https://arxiv.org/html/2501.16975/x3.png",
                "caption": "Figure 3:Illustration of 2-gram encoding/decoding GPT. Note that 2-gram decoding only preserves the predicted next 1 token though next 2 is predicted, which keeps inference cost identical to the vanilla model.",
                "position": 224
            },
            {
                "img": "https://arxiv.org/html/2501.16975/x4.png",
                "caption": "Figure 4:Training curves for OE-12.8M and baseline model on OLMo2-1B. The metrics are smoothed via exponential moving average with weight 0.99 for loss and 0.9 for downstream tasks. We observe significant convergence acceleration for the OE model:5.7√ó5.7\\times5.7 √óon loss,3.2√ó3.2\\times3.2 √óon MMLU-Var,3.0√ó3.0\\times3.0 √óon Hellaswag,2.6√ó2.6\\times2.6 √óon ARC-Challenge,3.1√ó3.1\\times3.1 √óon ARC-Easy and3.9√ó3.9\\times3.9 √óon PIQA.",
                "position": 350
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.16975/x5.png",
                "caption": "Figure 5:Log-linear relationship is observed between vocabulary sizemùëömitalic_mand training loss‚Ñí‚Ñí\\mathcal{L}caligraphic_L,i.e.‚Ñí=2.6754‚àí0.0256√ólog10‚Å°m‚Ñí2.67540.0256subscript10ùëö\\mathcal{L}=2.6754-0.0256\\times\\log_{10}{m}caligraphic_L = 2.6754 - 0.0256 √ó roman_log start_POSTSUBSCRIPT 10 end_POSTSUBSCRIPT italic_m. The values are collected with 500B tokens‚Äô training on OLMoE-1.3B models.",
                "position": 452
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APytorch Implementation",
        "images": []
    },
    {
        "header": "Appendix BMore Experimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.16975/extracted/6161742/figures/cfg_example.png",
                "caption": "Figure 6:Left Panel: CFG rules used in our experiments;Right Panel:an example of the generated sequences using the rules. This figure is taken from(Allen-Zhu & Li,2024).",
                "position": 1264
            },
            {
                "img": "https://arxiv.org/html/2501.16975/x6.png",
                "caption": "Figure 9:All metrics for OLMo2-1B, comparing OE-12.8M and baseline.",
                "position": 1304
            },
            {
                "img": "https://arxiv.org/html/2501.16975/x7.png",
                "caption": "Figure 11:All metrics for OLMoE-1.3B, comparing OE-12.8M and baseline.",
                "position": 1316
            },
            {
                "img": "https://arxiv.org/html/2501.16975/x8.png",
                "caption": "Figure 12:All metrics for OLMoE-7B, comparing OE-12.8M and baseline.",
                "position": 1319
            },
            {
                "img": "https://arxiv.org/html/2501.16975/x9.png",
                "caption": "Figure 13:All metrics for OLMoE-1.3B, comparing OT-12.8 and OE-12.8M.",
                "position": 1322
            }
        ]
    },
    {
        "header": "Appendix COver-Decoding",
        "images": []
    }
]