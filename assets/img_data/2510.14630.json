[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14630/x1.png",
                "caption": "Figure 1:Comparison of our single-token MLP-Mixer generator against transformer-based baselines (DiT, SiT), as well as representation-aligned models like REPA. RepTok attains competitive generative performance while reducing training cost by over 90% owing to its compact latent space and lightweight architecture. All results reported without CFG.\nFor fair comparison, we use our encoder and decoder that have been trained on general data.",
                "position": 171
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14630/x2.png",
                "caption": "Figure 2:Overview of our pipeline. (a) Joint fine-tuning of the[cls]token of SSL encoder‚Ñ∞\\mathcal{E}and training of the generative decoderùíü\\mathcal{D}for image reconstruction. (b) Training of the generation modelùí¢\\mathcal{G}to synthesize frozen encoder outputs, which constitute the latent spacez=‚Ñ∞‚Äã(x)z=\\mathcal{E}(x). (c) Inference pipeline, where the latent spacezzis first generated and subsequently decoded into the pixel space.",
                "position": 207
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/qualitative/imagenet/original/imagenet_batch0_idx96_sub0.jpg",
                "caption": "Figure 3:We introduceRepTok, a compact visual tokenizer that builds upon pre-trained SSL representations. Our approach augments these representations with additional necessary information to enable images to be faithfully encoded as a single continuous token, which allows for both high-fidelity image reconstruction and synthesis. The third row indicates the number of tokens for reconstruction.",
                "position": 256
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/qualitative/imagenet/titok/imagenet_idx96_sub0.jpg",
                "caption": "",
                "position": 289
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/qualitative/imagenet/flextok/imagenet_batch0_idx96_sub3.jpg",
                "caption": "",
                "position": 290
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/qualitative/imagenet/flextok/imagenet_batch0_idx96_sub2.jpg",
                "caption": "",
                "position": 291
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/qualitative/imagenet/flextok/imagenet_batch0_idx96_sub1.jpg",
                "caption": "",
                "position": 292
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/qualitative/imagenet/ours/imagenet_ema_idx96_sub0.jpg",
                "caption": "",
                "position": 294
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/qualitative/mscoco/original/mscoco_batch0_idx6_sub0.jpg",
                "caption": "",
                "position": 295
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/qualitative/mscoco/titok/mscoco_idx6_sub0.jpg",
                "caption": "",
                "position": 296
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/qualitative/mscoco/flextok/mscoco_batch0_idx6_sub3.jpg",
                "caption": "",
                "position": 297
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/qualitative/mscoco/flextok/mscoco_batch0_idx6_sub2.jpg",
                "caption": "",
                "position": 298
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/qualitative/mscoco/flextok/mscoco_batch0_idx6_sub1.jpg",
                "caption": "",
                "position": 299
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/qualitative/mscoco/ours/mscoco_ema_idx6_sub0.jpg",
                "caption": "",
                "position": 300
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/qualitative/imagenet/original/imagenet_batch0_idx52_sub0.jpg",
                "caption": "",
                "position": 303
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/qualitative/imagenet/titok/imagenet_idx52_sub0.jpg",
                "caption": "",
                "position": 304
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/qualitative/imagenet/flextok/imagenet_batch0_idx52_sub3.jpg",
                "caption": "",
                "position": 305
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/qualitative/imagenet/flextok/imagenet_batch0_idx52_sub2.jpg",
                "caption": "",
                "position": 306
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/qualitative/imagenet/flextok/imagenet_batch0_idx52_sub1.jpg",
                "caption": "",
                "position": 307
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/qualitative/imagenet/ours/imagenet_ema_idx52_sub0.jpg",
                "caption": "",
                "position": 309
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/qualitative/mscoco/original/mscoco_batch0_idx53_sub0.jpg",
                "caption": "",
                "position": 310
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/qualitative/mscoco/titok/mscoco_idx53_sub0.jpg",
                "caption": "",
                "position": 311
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/qualitative/mscoco/flextok/mscoco_batch0_idx53_sub3.jpg",
                "caption": "",
                "position": 312
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/qualitative/mscoco/flextok/mscoco_batch0_idx53_sub2.jpg",
                "caption": "",
                "position": 313
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/qualitative/mscoco/flextok/mscoco_batch0_idx53_sub1.jpg",
                "caption": "",
                "position": 314
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/qualitative/mscoco/ours/mscoco_ema_idx53_sub0.jpg",
                "caption": "",
                "position": 315
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/cat-tiger0.jpg",
                "caption": "Figure 4:Latent space interpolation.We observe smooth transitions not only in semantic content but also in spatial configuration. This indicates that our method successfully integrates low-level spatial information while preserving the properties of the pretrained encoder‚Äôs latent space, and facilitates generation within the learned representation. We provide more samples in the Appendix.",
                "position": 341
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/cat-tiger1.jpg",
                "caption": "",
                "position": 352
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/cat-tiger2.jpg",
                "caption": "",
                "position": 353
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/cat-tiger3.jpg",
                "caption": "",
                "position": 354
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/cat-tiger4.jpg",
                "caption": "",
                "position": 355
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/cat-tiger5.jpg",
                "caption": "",
                "position": 356
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/room0.jpg",
                "caption": "",
                "position": 359
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/room1.jpg",
                "caption": "",
                "position": 360
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/room2.jpg",
                "caption": "",
                "position": 361
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/room3.jpg",
                "caption": "",
                "position": 362
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/room4.jpg",
                "caption": "",
                "position": 363
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/room5.jpg",
                "caption": "",
                "position": 364
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/frozen-unfrozen/original_idx87_sub0.jpg",
                "caption": "Figure 5:Fine-tuning the[cls]token. From left: GT, frozen, finetuned.",
                "position": 391
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/frozen-unfrozen/frozen_idx87_sub0.jpg",
                "caption": "",
                "position": 394
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/frozen-unfrozen/unfrozen_idx87_sub0.jpg",
                "caption": "",
                "position": 395
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/frozen-unfrozen/original_idx63_sub0.jpg",
                "caption": "",
                "position": 397
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/frozen-unfrozen/frozen_idx63_sub0.jpg",
                "caption": "",
                "position": 398
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/frozen-unfrozen/unfrozen_idx63_sub0.jpg",
                "caption": "",
                "position": 399
            },
            {
                "img": "https://arxiv.org/html/2510.14630/x3.png",
                "caption": "Table 1:State-of-the-art comparison between tokenizers for reconstruction and class-conditional ImageNet generation.‚Ä†\\daggermetrics sourced fromBachmann et¬†al. (2025).",
                "position": 421
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14630/x3.png",
                "caption": "Figure 6:Uncurated MLP-Mixer ImageNet generations (CFG=3.53.5). More samples in the Appendix.",
                "position": 537
            },
            {
                "img": "https://arxiv.org/html/2510.14630/x3.png",
                "caption": "",
                "position": 540
            },
            {
                "img": "https://arxiv.org/html/2510.14630/x4.png",
                "caption": "",
                "position": 544
            },
            {
                "img": "https://arxiv.org/html/2510.14630/x5.png",
                "caption": "Figure 7:Left:T2I training days vs gFID, zero-shot evaluation on MS-COCOLin et¬†al. (2014). Data sourced from MicroDiTSehwag et¬†al. (2024).Right:Scaling the frozen language backbones results in improved performance. Language models: CLIP, InternVL, and Gemma-2B.",
                "position": 758
            },
            {
                "img": "https://arxiv.org/html/2510.14630/x6.png",
                "caption": "Table 4:Our approach generalizes to other self-supervised encoders. We compare 10k FID on class-conditional ImageNetDeng et¬†al. (2009).",
                "position": 762
            },
            {
                "img": "https://arxiv.org/html/2510.14630/x6.png",
                "caption": "Figure 8:RepTok text-to-image results with a transformer-based latent space model (CFG scale 3.5).",
                "position": 828
            },
            {
                "img": "https://arxiv.org/html/2510.14630/x7.png",
                "caption": "Figure 9:The parameterŒª\\lambdaof the cosine similarity loss inEquation3allows us to trade off between pixel-wise reconstruction and generation capabilities. Relaxed constraints (lowŒª\\lambda) improve pixel-wise reconstruction (PSNR in right plot), but result in poor generation capabilities (gFID in right plot).",
                "position": 831
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Supplementary Material: Meaningful Image Content is Worth One Token",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/ocean-oasis0.jpg",
                "caption": "Figure S1:More single token latent space interpolation results.We observe smooth transitions not only in semantic content but also in object spatial configuration, and especially in object rotation.",
                "position": 1682
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/ocean-oasis1.jpg",
                "caption": "",
                "position": 1693
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/ocean-oasis2.jpg",
                "caption": "",
                "position": 1694
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/ocean-oasis3.jpg",
                "caption": "",
                "position": 1695
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/ocean-oasis4.jpg",
                "caption": "",
                "position": 1696
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/ocean-oasis5.jpg",
                "caption": "",
                "position": 1697
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/golden-retriever0.jpg",
                "caption": "",
                "position": 1700
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/golden-retriever1.jpg",
                "caption": "",
                "position": 1701
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/golden-retriever2.jpg",
                "caption": "",
                "position": 1702
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/golden-retriever3.jpg",
                "caption": "",
                "position": 1703
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/golden-retriever4.jpg",
                "caption": "",
                "position": 1704
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/golden-retriever5.jpg",
                "caption": "",
                "position": 1705
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/swan-duck-unclip-0.jpg",
                "caption": "Figure S2:Qualitative interpolation comparison tounCLIPRamesh et¬†al. (2022); Rombach et¬†al. (2022)The results show that representations from unCLIP primarily capture semantic information and lack low-level detail, leading to less coherent transitions.\nIn contrast, our approach preserves both semantic and structural continuity, enabling visually consistent interpolations. We use the pretrained Stable Diffusion 2.1 unCLIP checkpoint.",
                "position": 1710
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/swan-duck-unclip-1.jpg",
                "caption": "",
                "position": 1725
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/swan-duck-unclip-2.jpg",
                "caption": "",
                "position": 1726
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/swan-duck-unclip-3.jpg",
                "caption": "",
                "position": 1727
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/swan-duck-unclip-4.jpg",
                "caption": "",
                "position": 1728
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/swan-duck-unclip-5.jpg",
                "caption": "",
                "position": 1729
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/swan-duck0.jpg",
                "caption": "",
                "position": 1737
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/swan-duck1.jpg",
                "caption": "",
                "position": 1738
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/swan-duck2.jpg",
                "caption": "",
                "position": 1739
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/swan-duck3.jpg",
                "caption": "",
                "position": 1740
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/swan-duck4.jpg",
                "caption": "",
                "position": 1741
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/interpolation/new/swan-duck5.jpg",
                "caption": "",
                "position": 1743
            },
            {
                "img": "https://arxiv.org/html/2510.14630/x8.png",
                "caption": "Figure S3:Effect of decoder inference steps(left) andeffect of CFG scales(right). We evaluate both on the COCO validation setLin et¬†al. (2014). More decoder inference steps yield better decoding results. CLIP score rises with larger CFG scales, while FID improves only within a moderate range.",
                "position": 1752
            }
        ]
    },
    {
        "header": "Appendix BAdditional results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/cls-reg/001-orig.jpg",
                "caption": "Figure S4:[cls]vs[reg]qualitative comparison.",
                "position": 1761
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/cls-reg/001-cls.jpg",
                "caption": "",
                "position": 1770
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/cls-reg/001-reg.jpg",
                "caption": "",
                "position": 1771
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/cls-reg/004-orig.jpg",
                "caption": "",
                "position": 1774
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/cls-reg/004-cls.jpg",
                "caption": "",
                "position": 1775
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/cls-reg/004-reg.jpg",
                "caption": "",
                "position": 1776
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/cls-reg/008-orig.jpg",
                "caption": "",
                "position": 1779
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/cls-reg/008-cls.jpg",
                "caption": "",
                "position": 1780
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/compression/cls-reg/008-reg.jpg",
                "caption": "",
                "position": 1781
            },
            {
                "img": "https://arxiv.org/html/2510.14630/x9.png",
                "caption": "Figure S5:Comparing SSL priors over training steps.Our approach generalizes to different self-supervised methods. While the unregularized model without prior knowledge shows remarkable pixel-wise reconstruction, the latent space is not amenable for generation (seeSection4.2andFigureS7).",
                "position": 1839
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/generation/ablation-generation/dog_5k.jpg",
                "caption": "Figure S6:Uncuratedclass-conditional ImageNet generation results over training iterations (5k, 10k, 30k, and 80k). Note that our model produces good results as early as 30k training steps.",
                "position": 1856
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/generation/ablation-generation/dog_10k.jpg",
                "caption": "",
                "position": 1866
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/generation/ablation-generation/dog_30k.jpg",
                "caption": "",
                "position": 1867
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/generation/ablation-generation/dog_80k.jpg",
                "caption": "",
                "position": 1868
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/generation/ablation-generation/churches_5k.jpg",
                "caption": "",
                "position": 1871
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/generation/ablation-generation/churches_10k.jpg",
                "caption": "",
                "position": 1872
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/generation/ablation-generation/churches_30k.jpg",
                "caption": "",
                "position": 1873
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/generation/ablation-generation/churches_80k.jpg",
                "caption": "",
                "position": 1874
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/generation/ours-vs-scratch/flower-gt.jpg",
                "caption": "Figure S7:Qualitative comparison between a randomly-initialized encoder and ours. Generation refers to class-conditional samples with the same class as the corresponding GT image. While random initialization achieves stronger pixel-level reconstruction, it lacks the structured priors of pre-trained self-supervised encoders, resulting in poor generative performance. In contrast, our method balances reconstruction and generation.",
                "position": 1879
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/generation/ours-vs-scratch/flower-scratch-recon.jpg",
                "caption": "",
                "position": 1895
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/generation/ours-vs-scratch/flower-ours-recon.jpg",
                "caption": "",
                "position": 1896
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/generation/ours-vs-scratch/flower-scratch-gen.jpg",
                "caption": "",
                "position": 1897
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/generation/ours-vs-scratch/flower-ours-gen.jpg",
                "caption": "",
                "position": 1898
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/generation/ours-vs-scratch/cargo-ship-gt.jpg",
                "caption": "",
                "position": 1901
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/generation/ours-vs-scratch/cargo-ship-scratch-recon.jpg",
                "caption": "",
                "position": 1902
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/generation/ours-vs-scratch/cargo-ship-ours-recon.jpg",
                "caption": "",
                "position": 1903
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/generation/ours-vs-scratch/cargo-ship-scratch-gen.jpg",
                "caption": "",
                "position": 1904
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/generation/ours-vs-scratch/cargo-ship-ours-gen.jpg",
                "caption": "",
                "position": 1905
            },
            {
                "img": "https://arxiv.org/html/2510.14630/x10.png",
                "caption": "Figure S8:Additional text-to-image generation results with a CFG scale of7.57.5and RepTok encoder-decoder trained on the COYO dataset.",
                "position": 1910
            },
            {
                "img": "https://arxiv.org/html/2510.14630/x11.png",
                "caption": "Figure S9:T2I generation results (CFG scale 3.5), using RepTok solely trained on ImageNet data with a latent space transformer. The autoencoder also transfers effectively to T2I tasks, producing visually compelling results.",
                "position": 1914
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/generation/uncurated/husky.jpg",
                "caption": "Figure S10:Uncuratedclass-conditional generation results of RepTok with CFG scale of3.53.5.",
                "position": 1917
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/generation/uncurated/snail.jpg",
                "caption": "",
                "position": 1921
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/generation/uncurated/sports-car.jpg",
                "caption": "",
                "position": 1923
            },
            {
                "img": "https://arxiv.org/html/2510.14630/figures/generation/uncurated/stingray.jpg",
                "caption": "",
                "position": 1925
            }
        ]
    },
    {
        "header": "Appendix CLimitations",
        "images": []
    }
]