[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18527/x1.png",
                "caption": "Figure 1:Diffusion-based multi-view image generation methods use a specific reference view for predicting subsequent views, which becomes problematic when overlap between the reference view and the predicted view is minimal, affecting image quality and multi-view consistency. Our MV-AR addresses this by using the preceding view with significant overlap for conditioning.",
                "position": 95
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18527/x2.png",
                "caption": "Figure 2:The overall pipeline of out MV-AR. The text and shape conditions are concatenated before the start token as the context. The text condition can either expect the model to generate multi-view images following other conditions, such as image, or describe the target object. The start token signals the model to begin generating multi-view images. Then, camera pose and image conditions are integrated. The camera pose serves as the shift position embedding, using its angular data to guide the generation of the specific view. After warping by IWC, the image conditions are added token by token within the model. It should be noted that our MV-AR can accommodate these multi-modal conditions simultaneously after progressive learning.",
                "position": 243
            },
            {
                "img": "https://arxiv.org/html/2506.18527/x3.png",
                "caption": "Figure 3:Qualitative study of text to multi-view generationwith diffusion-based method[36].",
                "position": 379
            },
            {
                "img": "https://arxiv.org/html/2506.18527/x4.png",
                "caption": "Figure 4:Qualitative comparison on image-conditioned multi-view generationwith diffusion-based method[23,22,21,17]. Era3D[17]generates results in canonical space, so there are certain view differences compared with other methods.",
                "position": 424
            }
        ]
    },
    {
        "header": "4Experience",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18527/x5.png",
                "caption": "Table 1:Ablation on text-condition module on GSO. We report FID, IS, and CLIP-Score on the generated multi-view images of 30 GSO objects.††{\\dagger}†means the model is finetuned only on Objaverse subset for fair comparison.",
                "position": 448
            },
            {
                "img": "https://arxiv.org/html/2506.18527/x5.png",
                "caption": "Figure 5:Visualization of our shape-conditioned multi-view generation.Given a geometric shape, our approach robustly generates multi-view images that are geometrically consistent with it, while ensuring multi-view coherence.",
                "position": 509
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]