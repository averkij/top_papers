[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09585/x1.png",
                "caption": "",
                "position": 77
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09585/x2.png",
                "caption": "Figure 2:Probing reveals a relationship between representation quality and performance.(a)We observe that increasing the amount of data and training solely with the next-token prediction objective enhances the visual representation quality within the LLM, resulting in improved performance, underscoring the effectiveness of our probing setup.(b)OurOLA-VLMexhibits superior visual representations and performance compared to LLaVA-1.5[41]under the same settings, demonstrating the effectiveness of minimizing the predictive embedding objective during training.",
                "position": 96
            },
            {
                "img": "https://arxiv.org/html/2412.09585/x3.png",
                "caption": "Figure 3:Probing Visual Representations across LLM layers in Multimodal Large Language Models.(1)As shown in the first row, the multi-encoder baseline has the best probing performance owing to the additional feature inputs. The performance of probes trained on our OLA-VLM falls between the two baselines, demonstrating the effectiveness of our embedding distillation approach in learning an improved projector while only using a single encoder during inference.(2)We observe that the probing performance for single encoder models trained solely with natural language supervision improves as the training data of the base MLLM increases, indicating that the LLM improves its visual representations of the world with more training data. In the last row, we observe that our OLA-VLM (base setting) outperforms the LLaVA-1.5 model trained with more data during the PT stage, demonstrating the effectiveness of our approach.",
                "position": 100
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Visually Probing Language Embeddings",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09585/x4.png",
                "caption": "Figure 4:Architecture for OLA-VLM. During Pre-Training (PT), we optimize an embedding loss at specific layers for each target encoder: layersd‚ààùîªùëëùîªd\\in\\mathbb{D}italic_d ‚àà blackboard_D,s‚ààùïäùë†ùïäs\\in\\mathbb{S}italic_s ‚àà blackboard_S, andg‚ààùîæùëîùîæg\\in\\mathbb{G}italic_g ‚àà blackboard_Gfor the depth, segmentation, and generation tasks, respectively. We use a resampler-based embedding predictor[28], denoted asùêè{task}lsubscriptsuperscriptùêèùëôtask\\mathbf{P}^{l}_{\\{\\text{task}\\}}bold_P start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT start_POSTSUBSCRIPT { task } end_POSTSUBSCRIPTat each layerlùëôlitalic_l, to output predictions. Each predictor takes in two inputs: a set of learnable queries (ùêêtasksuperscriptùêêtask\\mathbf{Q}^{{\\text{task}}}bold_Q start_POSTSUPERSCRIPT task end_POSTSUPERSCRIPT) and the token sequence from layerlùëôlitalic_l, with special tokens for other tasks omitted. The final loss is the sum of embedding losses across all selected layers and the next-token prediction objective. During IFT, we train with only the next-token prediction objective while keeping the special tokens frozen so as not to affect their task-specific nature.",
                "position": 164
            }
        ]
    },
    {
        "header": "4Embedding Visual Information into LLM",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09585/x5.png",
                "caption": "(a) depth embedding loss",
                "position": 537
            },
            {
                "img": "https://arxiv.org/html/2412.09585/x5.png",
                "caption": "(a) depth embedding loss",
                "position": 540
            },
            {
                "img": "https://arxiv.org/html/2412.09585/x6.png",
                "caption": "(b) seg embedding loss",
                "position": 545
            },
            {
                "img": "https://arxiv.org/html/2412.09585/x7.png",
                "caption": "(c) gen embedding loss",
                "position": 550
            },
            {
                "img": "https://arxiv.org/html/2412.09585/x8.png",
                "caption": "Figure 6:Visualizing Embedding Predictor Outputs after the PT stage.The quality of the decoded representations indicates the effectiveness of our embedding optimization.",
                "position": 850
            }
        ]
    },
    {
        "header": "6Future Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09585/x9.png",
                "caption": "Figure I:Qualitative Examples for the Count task in CV-Bench.Our OLA-VLM can accurately predict the presence of one picture and one tree, unlike LLaVA-1.5[41].",
                "position": 2229
            },
            {
                "img": "https://arxiv.org/html/2412.09585/x10.png",
                "caption": "Figure II:Ground-truth outputs from the target models used for Probing MLLMs.",
                "position": 2244
            },
            {
                "img": "https://arxiv.org/html/2412.09585/x11.png",
                "caption": "Figure III:Qualitative Examples for the Depth task in CV-Bench.Our OLA-VLM can accurately predict that the lamp and keyboard ar closer to the camera in the respective samples.",
                "position": 2248
            },
            {
                "img": "https://arxiv.org/html/2412.09585/x12.png",
                "caption": "Figure IV:Qualitative Examples for the Relation task in CV-Bench.Our OLA-VLM can accurately predict that the positions of the trees and the bottle in the respective samples.",
                "position": 2252
            },
            {
                "img": "https://arxiv.org/html/2412.09585/x13.png",
                "caption": "Figure V:Qualitative Examples for the Distance task in CV-Bench.Our OLA-VLM can accurately predict that the distances between the respective pair of objects.",
                "position": 2256
            },
            {
                "img": "https://arxiv.org/html/2412.09585/x14.png",
                "caption": "Figure VI:Layerwise visualizations for thedepthprobes.For LLaVA-1.5[41], the probes generate blob-like outputs up to the eighth layer, with visualizations progressively improving in the middle layers, aligning with the findings presented in Sec.3of the main text. Notably, probes trained on OLA-VLM begin producing distinguishable object shapes and boundaries as early as the third layer, attributed to the enhanced projector design and the incorporation of embedding losses.",
                "position": 2260
            },
            {
                "img": "https://arxiv.org/html/2412.09585/x15.png",
                "caption": "Figure VII:Layerwise visualizations for thesegprobes.The LLaVA-1.5 probes often fail to segment the third car in the background for the first sample during the initial layers (layers two to eight), whereas the OLA-VLM probes demonstrate relatively better performance in this scenario. However, for the second sample, both models‚Äô probes struggle to segment the background regions effectively, highlighting an opportunity for improvement in future work.",
                "position": 2274
            },
            {
                "img": "https://arxiv.org/html/2412.09585/",
                "caption": "Figure VIII:Layerwise visualizations for thegenprobes.The probe outputs for both the models are of fairly good quality.",
                "position": 2279
            }
        ]
    },
    {
        "header": "Appendix BVisualizing Probe Outputs",
        "images": []
    }
]