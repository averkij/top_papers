[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01610/x1.png",
                "caption": "Figure 1:APanel-of-Peers (PoP)generates candidate responses from multiple LVLMs. The panel’s scoring of these responses is used to build a preference set, which is used to tune one or all the members of the panel, improving their accuracy individually. PoP significantly outperforms other forms of reaching consensus across many benchmarks.",
                "position": 86
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x2.png",
                "caption": "Figure 2:Illustration of the overall learning from feedback from peers approach. Our post-alignment strategy involves rejection sampling, supervised finetuning, and preference optimization methods to learn from peers. See text for details.",
                "position": 95
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Learning with a Panel-of-Peers",
        "images": []
    },
    {
        "header": "4Experiment Settings",
        "images": []
    },
    {
        "header": "5Results and Ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01610/x3.png",
                "caption": "Table 2:Evaluation on 15 vision-language benchmarks.We compare the performance of the regular Panel-of-PeersPoP. We have separated the benchmarks into five categories. Columns show three training iterations for= PoP-Mistral,= PoP-Vicuna, and= PoP-LLaMA3.†\\daggerindicates that the training set has been observed in our data mixture. For single-try Panel-of-Peers (st-PoP) see the appendix.",
                "position": 427
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x4.png",
                "caption": "",
                "position": 443
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x5.png",
                "caption": "",
                "position": 444
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x6.png",
                "caption": "",
                "position": 446
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x7.png",
                "caption": "",
                "position": 447
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x8.png",
                "caption": "",
                "position": 448
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x9.png",
                "caption": "",
                "position": 450
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x10.png",
                "caption": "",
                "position": 451
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x11.png",
                "caption": "",
                "position": 452
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x12.png",
                "caption": "",
                "position": 454
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x13.png",
                "caption": "",
                "position": 455
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x14.png",
                "caption": "",
                "position": 456
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x18.png",
                "caption": "",
                "position": 784
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x19.png",
                "caption": "",
                "position": 784
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x20.png",
                "caption": "",
                "position": 784
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x21.png",
                "caption": "Figure 3:Panel-of-Peers as a reward model.Average scores of the Panel-of-Peers used as a reward model on 15 selected benchmarks",
                "position": 814
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x21.png",
                "caption": "Figure 3:Panel-of-Peers as a reward model.Average scores of the Panel-of-Peers used as a reward model on 15 selected benchmarks",
                "position": 817
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x22.png",
                "caption": "Figure 4:Self-improvement iterations.Average scores of PoP learning at different iterations of self-improvement over 15 selected benchmarks",
                "position": 823
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x23.png",
                "caption": "Figure 5:Learning a new skill from peersWe start with a model with a limited knowledge of OCR (≈\\approx0% - 100%) and use PoP to teach the model OCR knowledge.",
                "position": 829
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x24.png",
                "caption": "(a)Types of Panel-of-Peers.",
                "position": 858
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x24.png",
                "caption": "(a)Types of Panel-of-Peers.",
                "position": 861
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x25.png",
                "caption": "(b)Alignment objective.",
                "position": 866
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x26.png",
                "caption": "(c)Reward aggregation methods.",
                "position": 871
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x27.png",
                "caption": "(d)Prompt scoring methods",
                "position": 876
            }
        ]
    },
    {
        "header": "6Discussion and Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Detatils",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01610/x28.png",
                "caption": "Figure A.2:Learning a New Skill from Peers (OCR).We start with a model with very limited OCR knowledge (≈0%\\approx 0\\%) and use PoP to iteratively teach OCR skills. The performance is evaluated across multiple categories, including Chart & OCR, General Knowledge, Math & Science, Hallucination, and Vision-Centric tasks.",
                "position": 2374
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x29.png",
                "caption": "Figure B.1:Evaluation results of our approach on 15 selected benchmarks in the OpenVLM Leaderboard.The figure displays 49 selected LVLMs (until 2024.10.30) in descending order of average score. When calculating the average score, the scores of each benchmark are normalized to the range of 0 to 100.",
                "position": 2394
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x30.png",
                "caption": "Table B.1:Evaluation on 15 vision-language benchmarks.We compare the performance of the single-try Panel-of-Peers (st-PoP). We have separated the benchmarks into five categories. Columns show three training iterations for= PoP-Mistral,= PoP-Vicuna, and= PoP-LLaMA3.†\\daggerindicates that the training set has been observed in our data mixture.",
                "position": 2442
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x31.png",
                "caption": "",
                "position": 2458
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x32.png",
                "caption": "",
                "position": 2459
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x33.png",
                "caption": "",
                "position": 2461
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x34.png",
                "caption": "",
                "position": 2462
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x35.png",
                "caption": "",
                "position": 2463
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x36.png",
                "caption": "",
                "position": 2465
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x37.png",
                "caption": "",
                "position": 2466
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x38.png",
                "caption": "",
                "position": 2467
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x39.png",
                "caption": "",
                "position": 2469
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x40.png",
                "caption": "",
                "position": 2470
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x41.png",
                "caption": "",
                "position": 2471
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x45.png",
                "caption": "",
                "position": 2799
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x46.png",
                "caption": "",
                "position": 2799
            },
            {
                "img": "https://arxiv.org/html/2509.01610/x47.png",
                "caption": "",
                "position": 2799
            }
        ]
    },
    {
        "header": "Appendix BAdditional Experiments",
        "images": []
    }
]