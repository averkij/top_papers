[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05167/x1.png",
                "caption": "Figure 1:Presto-S. Our goal is to distill the initial “real” score model (grey)μ𝜽subscript𝜇𝜽\\mu_{\\bm{\\theta}}italic_μ start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPTinto a few-step generator (light blue)Gϕsubscript𝐺bold-italic-ϕG_{\\bm{\\phi}}italic_G start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPTto minimize the KL divergence between the distribution ofGϕsubscript𝐺bold-italic-ϕG_{\\bm{\\phi}}italic_G start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT’s outputs and the real distribution. This requires that we train an auxillary “fake” score modelμ𝝍subscript𝜇𝝍\\mu_{\\bm{\\psi}}italic_μ start_POSTSUBSCRIPT bold_italic_ψ end_POSTSUBSCRIPT(dark blue) that estimates the score of thegenerator’sdistribution at each gradient step. Formally: (1) real audio is corrupted with Gaussian noise sampled from the generator noise distributionpgen⁢(σinf)subscript𝑝gensuperscript𝜎infp_{\\text{{\\color[rgb]{0.75390625,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n0.75390625,0,0}gen}}}(\\sigma^{\\text{inf}})italic_p start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT ( italic_σ start_POSTSUPERSCRIPT inf end_POSTSUPERSCRIPT )which is then (2) passed into the generator to get its output. Noise is then added to this generation according to threedifferentnoise distributions: (3)pDMD⁢(σtrain)subscript𝑝DMDsuperscript𝜎trainp_{\\text{{\\color[rgb]{0.984375,0.7421875,0}\\definecolor[named]{pgfstrokecolor}%\n{rgb}{0.984375,0.7421875,0}DMD}}}(\\sigma^{\\text{train}})italic_p start_POSTSUBSCRIPT DMD end_POSTSUBSCRIPT ( italic_σ start_POSTSUPERSCRIPT train end_POSTSUPERSCRIPT ), which is (4) passed into both the real and fake score models to calculate the distribution matching gradient∇ϕℒDMDsubscript∇italic-ϕsubscriptℒDMD\\nabla_{\\phi}\\mathcal{L}_{\\text{DMD}}∇ start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT DMD end_POSTSUBSCRIPT; (5)pDSM⁢(σtrain/inf)subscript𝑝DSMsuperscript𝜎train/infp_{\\text{{\\color[rgb]{0.1796875,0.37109375,0.49609375}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0.1796875,0.37109375,0.49609375}DSM}}}(\\sigma^{\\text{%\ntrain/inf}})italic_p start_POSTSUBSCRIPT DSM end_POSTSUBSCRIPT ( italic_σ start_POSTSUPERSCRIPT train/inf end_POSTSUPERSCRIPT ), which is used to (6) train the fake score model on thegenerator’sdistribution withℒfake-DSMsubscriptℒfake-DSM\\mathcal{L}_{\\text{fake-DSM}}caligraphic_L start_POSTSUBSCRIPT fake-DSM end_POSTSUBSCRIPT; and (7) an adversarial distributionpGAN⁢(σtrain)subscript𝑝GANsuperscript𝜎trainp_{\\text{{\\color[rgb]{0.57421875,0.81640625,0.3125}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0.57421875,0.81640625,0.3125}GAN}}}(\\sigma^{\\text{train}})italic_p start_POSTSUBSCRIPT GAN end_POSTSUBSCRIPT ( italic_σ start_POSTSUPERSCRIPT train end_POSTSUPERSCRIPT ), which along with the real audio is (8) passed into a least-squares discriminator built on the fake score model’s intermediate activations to calculateℒGANsubscriptℒGAN\\mathcal{L}_{\\text{GAN}}caligraphic_L start_POSTSUBSCRIPT GAN end_POSTSUBSCRIPT.",
                "position": 126
            }
        ]
    },
    {
        "header": "2Background & Related Work",
        "images": []
    },
    {
        "header": "3Presto!",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05167/x2.png",
                "caption": "Figure 2:Training/Inference distributions for EDM models, in decibel SNR space.",
                "position": 404
            },
            {
                "img": "https://arxiv.org/html/2410.05167/x3.png",
                "caption": "Figure 3:Baseline layer dropping (left) vs.Presto-L(right). Standard layer dropping uses the noise levelσ𝜎\\sigmaitalic_σto set the budget of layers to drop, starting from the back of the DiT blocks.Presto-Lshifts this dropping by one to the second-to-last block and adds explicit budget conditioning.",
                "position": 518
            },
            {
                "img": "https://arxiv.org/html/2410.05167/x4.png",
                "caption": "Figure 4:Hidden activation variance vs. layer depth. Each line is a unique noise level.",
                "position": 532
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05167/x5.png",
                "caption": "Figure 5:Continuous generator inputs vs. discrete inputs. Continuous inputs shows more consistent scaling with compute, while generally performing better in both quality and text relevance.",
                "position": 756
            },
            {
                "img": "https://arxiv.org/html/2410.05167/x6.png",
                "caption": "Figure 6:Presto-Lresults.Presto-Limproves both the latencyandthe overall performance across all metrics, against both the leading layer dropping baseline and the base model.",
                "position": 771
            },
            {
                "img": "https://arxiv.org/html/2410.05167/x7.png",
                "caption": "Figure 7:Presto-Shidden activation var.",
                "position": 994
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05167/x8.png",
                "caption": "Figure 8:Step distillation losses for early distillation for multiple combination methods.Presto-LSis the only setup that avoids generator degradation and high variance distribution matching gradients.",
                "position": 2236
            },
            {
                "img": "https://arxiv.org/html/2410.05167/x9.png",
                "caption": "Figure 9:Generations fromPresto-LSfrom thesametext prompt and latent code (i.e. starting noise and added noise during sampling), only varying theρ𝜌\\rhoitalic_ρparameter between (7 and 1000). Purely shifting the noise schedule for 4-step sampling allows for perceptually distinct outputs.",
                "position": 2295
            },
            {
                "img": "https://arxiv.org/html/2410.05167/x10.png",
                "caption": "Figure 10:Presto-Lablation. Each individual change of our layer distillation vs ASE is beneficial.",
                "position": 2515
            },
            {
                "img": "https://arxiv.org/html/2410.05167/x11.png",
                "caption": "Figure 11:Failure mode of 1-2 step discrete models vs. continuous models (each row is same random seed and text prompt), with 2-step generation. Hip-Hop adjacent generations noticeably drop high frequency information, and render percussive transients (hi-hats, snare drums) poorly.",
                "position": 2529
            },
            {
                "img": "https://arxiv.org/html/2410.05167/x12.png",
                "caption": "Figure 12:Violin plot from our listening test. Presto-LS is preferred over other baselines (p<0.05𝑝0.05p<0.05italic_p < 0.05).",
                "position": 2539
            },
            {
                "img": "https://arxiv.org/html/2410.05167/x13.png",
                "caption": "Figure 13:Rejection sampling eval metrics vs. rejection ratio. BasePresto-LSin red. CLAP rejection sampling improves both CLAP score and overall quality, while reducing diversity.",
                "position": 2549
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]