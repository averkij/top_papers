[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.13992/",
                "caption": "",
                "position": 83
            },
            {
                "img": "https://arxiv.org/html/2508.13992/x2.png",
                "caption": "Figure 1:Overview of the MMAU-Pro benchmark. MMAU-Pro provides comprehensive coverage across all three core audio domains-speech, sound, and music-and extends evaluation to their mixtures. It further includes multi-audio reasoning, long-form audio (up to 10 minutes), voice-chat QA, spatial audio understanding, open-ended QA, and multimodal instruction following, offering a broad and realistic assessment of audio intelligence.",
                "position": 95
            }
        ]
    },
    {
        "header": "Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.13992/x3.png",
                "caption": "Figure 2:(Left)Distribution of audio perception skills required for questions in the MMAU-Pro across the domains of sound, speech, and music.(Right)Distribution of auditory reasoning skills required for questions in MMAU-Pro. Each question in MMAU-Pro demands the model to apply one or more of the perception and reasoning skills to generate a reliable and accurate response.",
                "position": 129
            }
        ]
    },
    {
        "header": "Related Work",
        "images": []
    },
    {
        "header": "The MMAU-Pro Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.13992/x4.png",
                "caption": "Figure 3:Overview of dataset‐construction pipeline for MMAU-Pro.",
                "position": 161
            }
        ]
    },
    {
        "header": "Experimental Setup",
        "images": []
    },
    {
        "header": "Results and Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.13992/x5.png",
                "caption": "Figure 4:Performance comparison of AF3, Qwen2.5-Omni-7B, and Gemini-2.5 Flash on the top 3 skills on MMAU-Pro with the highest and lowest average performance. Frontier models perform well on Style & Genre Recognition, Knowledge-based, and Timbre/Instrument Recognition, and underperform on Quantitative Reasoning, Temporal Event Reasoning, and Speech Activity & Turn-Taking.",
                "position": 1023
            },
            {
                "img": "https://arxiv.org/html/2508.13992/x6.png",
                "caption": "Figure 5:Performance comparison between AF3 and Qwen2.5-Omni-7B on theinstruction-followingsubset of MMAU-Pro. Qwen2.5-Omni-7B, trained on both text-only and multimodal audio–text data, outperforms AF3 on five of six subtasks—Change Cases, Detectable Format, Length Constraints, Keywords, and Multi-Part Response, underscoring the value of incorporating text-only data for fine-tuning and instruction tuning.",
                "position": 1043
            },
            {
                "img": "https://arxiv.org/html/2508.13992/x7.png",
                "caption": "Figure 6:Impact of question rephrasing on MMAU-Pro performance. We comparePhi4-MM-InstructandQwen2.5-Omni-3Bon the original questions, Qwen3-235B-A22B paraphrases, and GPT4o rewrites. Both models show consistent gains (up to 5%) on the rephrased versions, highlighting their sensitivity to surface-level language cues and the influence of LLM-generated phrasing on benchmark results.",
                "position": 1059
            },
            {
                "img": "https://arxiv.org/html/2508.13992/x8.png",
                "caption": "Figure 7:Accuracy by music culture for five LALMs on the MMAU-Pro benchmark. Each bar group shows per-culture performance for AF3, Gemini-2.5 Flash, Qwen2.5-Omni-3B, Qwen2.5-Omni-7B, and Phi4-MM-Instruct, highlighting significant drops on underrepresented traditions.",
                "position": 1113
            },
            {
                "img": "https://arxiv.org/html/2508.13992/x9.png",
                "caption": "Figure 8:Dataset composition by music culture in MMAU-Pro.",
                "position": 1116
            },
            {
                "img": "https://arxiv.org/html/2508.13992/x10.png",
                "caption": "Figure 9:Effect of number of options on MCQ accuracy. On MMAU-Pro, both Audio Flamingo 3 and Qwen2.5-Omni-7B perform better with three options than with ten. The larger answer set - with more plausible distractors-raises difficulty and pushes models to rely on audio evidence rather than language-prior shortcuts.",
                "position": 1125
            }
        ]
    },
    {
        "header": "Conclusion, Limitations and Future Work",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "Author Contributions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ACurating the Audio Set",
        "images": []
    },
    {
        "header": "Appendix BAnnotation Guidelines",
        "images": []
    },
    {
        "header": "Appendix CPrompts",
        "images": []
    },
    {
        "header": "Appendix DOpen-ended QA Evaluation Ablations",
        "images": []
    },
    {
        "header": "Appendix ESkill wise examples and their definitions",
        "images": []
    }
]