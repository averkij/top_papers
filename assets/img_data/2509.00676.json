[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.00676/x1.png",
                "caption": "",
                "position": 135
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.00676/figures/Figure1.png",
                "caption": "Figure 1:LLaVA-Critic-R1 is trained on top of the base model Qwen-2.5-VL-7B. Building upon a stronger reasoning VLM, ThinkLite-VL-7B, we further develop LLaVA-Critic-R1+{\\!\\!+}by applying the same RL critic training procedure.Left:Performance comparison of LLaVA-Critic-R1 with other base and reasoning VLMs on multiple visual reasoning, visual understanding, and visual reward benchmarks. LLaVA-Critic-R1 not only significantly outperforms other models in critic performance, but also demonstrates stronger policy capabilities.Right:Performance improvement of critic training and test-time self-critic scaling on five common visual reasoning and visual understanding benchmarks. Critic training alone significantly improves the base model’s performance. Building upon this, leveraging the dual policy and critic capabilities of LLaVA-Critic-R1 for a \"Best-of-128\" self-critic scaling procedure at test time leads to a further substantial boost in performance.",
                "position": 141
            },
            {
                "img": "https://arxiv.org/html/2509.00676/figures/Figure1.png",
                "caption": "",
                "position": 144
            },
            {
                "img": "https://arxiv.org/html/2509.00676/figures/Figure2.png",
                "caption": "",
                "position": 148
            }
        ]
    },
    {
        "header": "2Critic Model Training with Reinforcement Learning",
        "images": []
    },
    {
        "header": "3Main Results: Critic Training and Test-Time Scaling",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.00676/figures/mathvista_scale_final.png",
                "caption": "(a)MathVista",
                "position": 1021
            },
            {
                "img": "https://arxiv.org/html/2509.00676/figures/mathvista_scale_final.png",
                "caption": "(a)MathVista",
                "position": 1024
            },
            {
                "img": "https://arxiv.org/html/2509.00676/figures/mathverse_scale_final.png",
                "caption": "(b)MathVerse",
                "position": 1029
            },
            {
                "img": "https://arxiv.org/html/2509.00676/figures/mathvision_scale_final.png",
                "caption": "(c)MathVision",
                "position": 1034
            },
            {
                "img": "https://arxiv.org/html/2509.00676/figures/mmmu_scale_final.png",
                "caption": "(d)MMMU",
                "position": 1047
            },
            {
                "img": "https://arxiv.org/html/2509.00676/figures/mmstar_scale_final.png",
                "caption": "(e)MMStar",
                "position": 1053
            }
        ]
    },
    {
        "header": "4Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.00676/figures/hall_case1.png",
                "caption": "Table 6:An example where Qwen-2.5-VL follows the thinking template but fails in reasoning, while LLaVA-Critic-R1+{\\!\\!+}correctly interprets chart details and derives the accurate answer.",
                "position": 1390
            },
            {
                "img": "https://arxiv.org/html/2509.00676/figures/correlation.png",
                "caption": "Figure 3:Correlation between LLaVA-Critic-R1’s critic and policy performance. This figure plots the performance of LLaVA-Critic-R1’s two key capabilities as a function of critic training steps. The blue curve represents the policy performance on general visual tasks, while the red curve denotes the critic performance on visual reward benchmarks. The strong positive correlation between the two performance metrics throughout the training process provides empirical evidence that improvements in the model’s critic ability are directly linked to enhancements in its policy ability.",
                "position": 2333
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.00676/figures/zerosub_case1.png",
                "caption": "Table 11:An example where Qwen-2.5-VL fails in reasoning despite following the thinking template, while LLaVA-Critic-R1+{\\!\\!+}succeeds in reasoning correctly. LLaVA-Critic-R1+{\\!\\!+}accurately counts the number of figurines in the image and ultimately arrived at the correct answer",
                "position": 3914
            },
            {
                "img": "https://arxiv.org/html/2509.00676/figures/osworld_state.png",
                "caption": "Table 12:An example where Qwen-2.5-VL fails in grounding for GUI agent tasks on OSWorld benchmark. TheResulting Stateimages show the interface after executing the proposed action. Our model follows one of the possible valid trajectories to successfully click theFormatbutton, whereas Qwen-2.5-VL clicks on an incorrect location.",
                "position": 3981
            },
            {
                "img": "https://arxiv.org/html/2509.00676/figures/osworld_qwen25_action.png",
                "caption": "",
                "position": 4020
            },
            {
                "img": "https://arxiv.org/html/2509.00676/figures/osworld_llava_critic_r1_action.png",
                "caption": "",
                "position": 4034
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]