[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05592/x1.png",
                "caption": "",
                "position": 186
            },
            {
                "img": "https://arxiv.org/html/2510.05592/logos/github.png",
                "caption": "",
                "position": 191
            },
            {
                "img": "https://arxiv.org/html/2510.05592/logos/huggingface.png",
                "caption": "",
                "position": 193
            },
            {
                "img": "https://arxiv.org/html/2510.05592/logos/gradio.png",
                "caption": "",
                "position": 195
            },
            {
                "img": "https://arxiv.org/html/2510.05592/x2.png",
                "caption": "",
                "position": 197
            },
            {
                "img": "https://arxiv.org/html/2510.05592/x3.png",
                "caption": "Figure 1:Left:Performance ofAgentFlowwith a 7B-scale backbone before and after Flow-GRPO tuning across ten diverse reasoning benchmarks. Flow-GRPO substantially improves performance by enhancing planning quality and tool-calling reliability.Right:AgentFlowachieves consistent gains over top baselines, including base LLMs, tool-integrated RL models, and training-free agentic systems. All 7B results use Qwen2.5-7B-Base/Instruct as the backbone and tools.",
                "position": 209
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05592/x4.png",
                "caption": "Figure 2:(a)Overview ofAgentFlow, a trainable agentic system for in-the-flow planning and tool use. Four modules (planner, executor, verifier, generator) coordinate via a shared evolving memoryMMand toolsetKK, given a queryqq. The planner policy is optimized on-policyinsidethe system’s multi-turn loop to enable adaptive, long-horizon reasoning.(b)A single state transition, showing the actionata^{t}, execution resultete^{t}, and verifier signalvtv^{t}that update the memory fromMtM^{t}toMt+1M^{t+1}.",
                "position": 230
            }
        ]
    },
    {
        "header": "2Preliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05592/x5.png",
                "caption": "Figure 3:Comparison of two paradigms of LLMs with tool use.(a) Monolithic tool-integrated reasoning models train a single policy to interleave reasoning (e.g.,<think>) and tool calls (e.g.,<tool_call>) within a single, full-context trajectory. (b) Agentic systems decompose tasks across multiple specialized modules (e.g., planner, coder) that collaborate. These systems are typically training-free, orchestrated by handcrafted logic or prompting.",
                "position": 271
            }
        ]
    },
    {
        "header": "3In-the-Flow Agentic System Optimization",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05592/x6.png",
                "caption": "Figure 4:Optimization for our proposed agentic systemAgentFlow.Given a queryqq, an evolving memoryMM, and a toolsetKK, the policy model generates actions that target sub-goals and select tools. It is trained viaFlow-based Group Refined Policy Optimization(Flow-GRPO), which enables multi-turn reinforcement learning and stable optimization under collaborative dynamics.",
                "position": 325
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05592/x7.png",
                "caption": "Figure 5:Tool call ratio change by Flow-GRPO fine-tuning.",
                "position": 1084
            },
            {
                "img": "https://arxiv.org/html/2510.05592/x7.png",
                "caption": "Figure 5:Tool call ratio change by Flow-GRPO fine-tuning.",
                "position": 1087
            },
            {
                "img": "https://arxiv.org/html/2510.05592/x8.png",
                "caption": "Figure 6:Calling error rate.",
                "position": 1092
            },
            {
                "img": "https://arxiv.org/html/2510.05592/x9.png",
                "caption": "Figure 7:One case study example.Initially failed with repetitive errors (left),AgentFlow, trained with Flow-GRPO, explores a new solution pathway at turn 4 after two failed attempts (right).",
                "position": 1110
            },
            {
                "img": "https://arxiv.org/html/2510.05592/x10.png",
                "caption": "Figure 8:Training dynamics and efficiency of Flow-GRPO.",
                "position": 1200
            },
            {
                "img": "https://arxiv.org/html/2510.05592/x10.png",
                "caption": "",
                "position": 1203
            },
            {
                "img": "https://arxiv.org/html/2510.05592/x11.png",
                "caption": "",
                "position": 1207
            },
            {
                "img": "https://arxiv.org/html/2510.05592/x12.png",
                "caption": "Figure 9:Flow-GRPO fine-tuning offers consistent gains onAgentFlowas the backbone model size scales from 3B to 7B.",
                "position": 1230
            },
            {
                "img": "https://arxiv.org/html/2510.05592/x13.png",
                "caption": "Figure 10:Average turns and accuracy with increasedTmaxT_{\\text{max}}.",
                "position": 1238
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Table of Contents",
        "images": []
    },
    {
        "header": "Appendix ATraining Algorithm ofAgentFlow",
        "images": []
    },
    {
        "header": "Appendix BTheoretical Analysis of Flow-GRPO",
        "images": []
    },
    {
        "header": "Appendix CExperimental Details",
        "images": []
    },
    {
        "header": "Appendix DMore Discussion about Experiment Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05592/x14.png",
                "caption": "Figure 11:Tool scaling study.AgentFlow’s performance improves when its tools are upgraded from Qwen-2.5-7B-Instruct to GPT-4o.",
                "position": 2688
            },
            {
                "img": "https://arxiv.org/html/2510.05592/x14.png",
                "caption": "Figure 11:Tool scaling study.AgentFlow’s performance improves when its tools are upgraded from Qwen-2.5-7B-Instruct to GPT-4o.",
                "position": 2691
            },
            {
                "img": "https://arxiv.org/html/2510.05592/x15.png",
                "caption": "Figure 12:Tool call optimization on Musique.AgentFlow’s planner increases Web Search usage after Flow-GRPO training.",
                "position": 2696
            }
        ]
    },
    {
        "header": "Appendix EInstruction Templates inAgentFlow",
        "images": []
    },
    {
        "header": "Appendix FCase Studies",
        "images": []
    }
]