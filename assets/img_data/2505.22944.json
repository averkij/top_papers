[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22944/extracted/6491122/figures/Teaser2.jpg",
                "caption": "",
                "position": 74
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22944/extracted/6491122/figures/Pipeline.jpg",
                "caption": "Figure 2:ATI takes an image and user specified trajectories as inputs. The point-wise trajectories are injected into the latent condition for the generation. Videos are decoded from the latent denoised from the DiT.",
                "position": 83
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22944/extracted/6491122/figures/FeaturePlace.png",
                "caption": "Figure 3:Trajectory Instruction module computes a latent feature from a point’s trajectory. During inference, given the point’s location in the first frame (i.e., the input image), we sample the feature at that location using bilinear interpolation. We then compute a spatial Gaussian distribution for each visible point on its corresponding location in every subsequent frame.",
                "position": 142
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22944/extracted/6491122/figures/example1.jpg",
                "caption": "Figure 4:Object Motion Control. Left: the input image overlaid with user‑specified trajectories—green dots mark each trajectory’s start point, and arrows mark each end point. Endpoint color encodes trajectory length, indicating that some trajectories span only part of the generated video. Right: five frames uniformly sampled from the generated video. Dot colors serve only to distinguish between trajectories.",
                "position": 311
            },
            {
                "img": "https://arxiv.org/html/2505.22944/extracted/6491122/figures/example2.jpg",
                "caption": "Figure 5:Video generation results with camera control. Left: Input image superimposed with user specified trajectories. Right: Five frames uniformly sampled from the generated video.",
                "position": 314
            },
            {
                "img": "https://arxiv.org/html/2505.22944/extracted/6491122/figures/example3.jpg",
                "caption": "Figure 6:Video generation results with coherent control of camera and object motion. Left: Input image superimposed with user specified trajectories. Right: Five frames uniformly sampled from the generated video.",
                "position": 317
            },
            {
                "img": "https://arxiv.org/html/2505.22944/extracted/6491122/figures/Comparison.jpg",
                "caption": "Figure 7:Qualitative comparison for ATI video generation with different backend models.",
                "position": 320
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]