[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22663/x1.png",
                "caption": "Figure 1:Various architectures of UMMs and its corresponding cross-modal interaction patterns. We arrange the models in order of increasing architecture decoupling. The row below illustrates the layer-wise cross-modal interaction intensity, with generation tasks shown inblueand understanding tasks inred; higher values indicate stronger interaction. The last column corresponds to HunyuanImage-3.0 and Qwen3-VL-8B, representing the interaction behavior of current SOTA task-specific generation and understanding methods. We observe that as decoupling increases, the negative correlation in cross-modal interaction patterns between understanding and generation tasks persists, but these patterns increasingly resemble those of task-specific models, leading to improved performance.",
                "position": 105
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22663/x2.png",
                "caption": "Figure 2:The pipeline of cross-modal interaction intensity calculation. We take text-to-image as an example, for each row, we compute the sum of all text token values, then average across all image tokens to obtain the intensity.",
                "position": 150
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22663/x3.png",
                "caption": "Figure 3:Training loss curve of Emu3 and Janus-Pro under various AIA coefficient. NTP and AIA means next-token-prediction and attention interaction alignment loss respectively. Note that we only show the NTP loss curve (excluding AIA loss) as it serves as the primary indicator of final performance andthe periodic drops in the Emu3 loss curve are due to learning rate schedule.",
                "position": 191
            }
        ]
    },
    {
        "header": "3Attention Interaction Alignment",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22663/x4.png",
                "caption": "Table 2:System-level comparison on widely used image understanding and generation benchmarks.†\\daggermeans the result is re-implemented by ourselves. Types represent whether the model uses diffusion, autoregressive, or masked prediction for training.(Gray)means the result reported in original papers while nobody can re-implemented.",
                "position": 221
            },
            {
                "img": "https://arxiv.org/html/2511.22663/x4.png",
                "caption": "Figure 4:Cross-Modal Attention Patterns Visualization of Different Single-Task Models.",
                "position": 959
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22663/x5.png",
                "caption": "Figure 5:Visualization of cross-modal attention patterns modification after AIA training. Task-specific models are Qwen3-VL-8B for understanding and HunyuanImage-3.0 for generation, with understanding tasks shown inblueand generation tasks inred.",
                "position": 995
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "FDetails on layer-wise target boundaryTlT_{l}and Huber thresholdδl\\delta_{l}",
        "images": []
    }
]