[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.10392/x1.png",
                "caption": "Figure 1:Comparison of 3DVG methods on ScanRefer dataset[3]. Our TSP3D surpasses existing methods in both accuracy and inference speed, achieving the first efficient 3DVG framework.",
                "position": 83
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.10392/x2.png",
                "caption": "Figure 2:Illustration of TSP3D. TSP3D bulids on multi-level sparse convolutional architecture. It iteratively upsamples the voxel features with text-guided pruning (TGP), and fuses multi-level features via completion-based addition (CBA).\n(a) to (d) on the right side illustrate various options for feature upsampling.\n(a) refers to simple concatenation with text features, which is fast but less accurate.\n(b) refers to feature interaction through cross-modal attention mechanisms, which is constrained by the large number of voxels.\n(c) represents our proposed TGP, which first prunes voxel features under textual guidance and thus enables efficient interaction between voxel and text features.\n(d) shows a simplified version of TGP that removes farthest point sampling and interpolation, combines multi-modal feature interactions into a whole and moves it before pruning.",
                "position": 193
            },
            {
                "img": "https://arxiv.org/html/2502.10392/x3.png",
                "caption": "Figure 3:Illustration of completion-based addition. The upper figure (b) illustrates an example of over-pruning on the target. The lower figure (c) shows the completed features predicted by CBA.",
                "position": 258
            },
            {
                "img": "https://arxiv.org/html/2502.10392/x4.png",
                "caption": "",
                "position": 268
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.10392/x5.png",
                "caption": "Figure 4:Visualization of the text-guided pruning process. In each example, the voxel features after scene-level TGP, target-level TGP and the last upsampling layer are presented from top to bottom. The blue boxes represent the ground truth of the target, and the red boxes denote the bounding boxes of relevant objects. TSP3D reduces the amount of voxel features through two stages of pruning and progressively guides the network focusing towards the target.",
                "position": 949
            },
            {
                "img": "https://arxiv.org/html/2502.10392/x6.png",
                "caption": "Figure 5:Visualization of the completion-based addition process. The blue points represent the voxel features output by the target-level TGP, while the red points are the completion features predicted by the CBA. The blue boxes indicate the ground truth boxes. CBA adaptively supplements situations where excessive pruning has occurred.",
                "position": 952
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AVisual Feature Resolution of Different Architectures",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.10392/x7.png",
                "caption": "Figure 6:Feature resolution progression of point-based EDA and sparse convolutional TSP3D-B.SA,FP,SpConv, andFUrepresent set abstraction, feature propagation, sparse convolution, and feature upsampling, respectively. For the point-based architecture, the downsampling process is aggressive, with the first downsampling reducing 50,000 points directly to 2,048 points. Furthermore, the final scene representation consists of only 1,024 points. In contrast, the sparse convolutional architecture performs progressive downsampling and refines the scene representation through a multi-level structure. This approach not only provides a high-resolution scene representation but also achieves faster inference speed compared to the point-based architecture.",
                "position": 1537
            }
        ]
    },
    {
        "header": "Appendix BDetailed Computational Cost of Different Architectures",
        "images": []
    },
    {
        "header": "Appendix CDetailed Results on ScanRefer",
        "images": []
    },
    {
        "header": "Appendix DQualitative Comparisons",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.10392/x8.png",
                "caption": "Figure 7:Qualitative results of EDA[35]and our TSP3D on the ScanRefer dataset[3]. In each description, the red annotations indicate the target object. The orange annotations in (a) refer to relevant objects, while the yellow annotations in (d) denote the appearance or attributes of the target. TSP3D demonstrates exceptional performance in locating relevant objects, narrow or small targets, identifying categories, and distinguishing appearance and attributes.",
                "position": 1919
            }
        ]
    },
    {
        "header": "Appendix ELimitations and Future Work",
        "images": []
    }
]