[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04759/x1.png",
                "caption": "Figure 1:Overview of the proposed PG-Occ framework.\nThe radar chart compares occupancy prediction accuracy across multiple methods, showing the superior performance of PG-Occ.\nThe central panel highlights the key components: progressive Gaussian modeling with online feed-forward densification, anisotropy-aware sampling with adaptive receptive fields, and open-vocabulary retrieval conditioned on prompt inputs.\nThe bottom row illustrates an example progression from the current input view through successive densification stages to the final occupancy prediction.",
                "position": 113
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04759/x2.png",
                "caption": "Figure 2:Architecture of the proposed PG-Occ framework. The scene is represented as feature Gaussian blobs, starting from a base layer and progressively refined and densified throughBBlayers. Multi-camera inputs are processed to extract spatio-temporal features, which guide the update and refinement of the Gaussians, which are then voxelized to produce an any-resolution 3D occupancy field, enabling both geometric reconstruction and open-vocabulary semantic understanding.",
                "position": 185
            },
            {
                "img": "https://arxiv.org/html/2510.04759/fig/fig3.jpg",
                "caption": "Figure 3:Illustration of the Progressive Online Densification (POD) and Anisotropy-aware Feature Sampling (AFS) modules. POD leverages depth-aware densification to progressively add and refine 3D Gaussians. AFS exploits the anisotropic properties of Gaussians, sampling feature points within anisotropy-aware receptive fields to enable more effective spatio-temporal feature extraction.",
                "position": 234
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04759/x3.png",
                "caption": "Figure 4:Illustration of PG-Occ predictions. Given camera inputs and text prompts, the method predicts depth (column 2), produces open-vocabulary semantic labels (column 3), and generates the final semantic occupancy map (column 4). Additional visualizations are provided inSectionC.1.",
                "position": 407
            },
            {
                "img": "https://arxiv.org/html/2510.04759/x4.png",
                "caption": "Figure 5:Depth estimation error metrics on the nuScenes validation set. The best results denoted inbold. Abs Rel is used as the primary evaluation metric.",
                "position": 711
            },
            {
                "img": "https://arxiv.org/html/2510.04759/x4.png",
                "caption": "Figure 6:SOTA comparison on thenuScenes retrievaldataset.",
                "position": 825
            },
            {
                "img": "https://arxiv.org/html/2510.04759/x5.png",
                "caption": "Figure 7:Qualitative comparison of 3D occupancy prediction using PG-Occ and prior methods.\nThis figure presents a visual comparison between PG-Occ (ours), GaussTR, and Ground Truth data in reconstructing urban scenes. PG-Occ achieves more accurate and perceptually coherent 3D occupancy predictions, capturing finer structural details and producing thicker, more realistic surfaces. The red and blue bounding boxes highlight regions where PG-Occ notably outperforms previous SOTA fixed-query methods, demonstrating improved fidelity and spatial consistency.",
                "position": 955
            }
        ]
    },
    {
        "header": "5Conclusion and Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Progressive Gaussian Transformer withAnisotropy-aware Sampling for OpenVocabulary Occupancy Prediction",
        "images": []
    },
    {
        "header": "Supplementary Material",
        "images": []
    },
    {
        "header": "Appendix AVideo Demonstration",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experiment Results",
        "images": []
    },
    {
        "header": "Appendix CAdditional Visualization Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04759/fig/cap_v1.png",
                "caption": "Figure 8:PG-Occ capabilities. Given only single-frame multi-view inputs and using only feed-forward passes, PG-Occ can:\n(1) estimate depth (row 2);\n(2) render open-vocabulary model features (row 3);\n(3) predict 3D occupancy in a zero-shot manner (rows 4);\n(4) predict semantic 3D occupancy in a zero-shot manner (rows 5);\n(5) support open-vocabulary text queries (rows 6).",
                "position": 2290
            },
            {
                "img": "https://arxiv.org/html/2510.04759/x6.png",
                "caption": "",
                "position": 2301
            },
            {
                "img": "https://arxiv.org/html/2510.04759/x7.png",
                "caption": "Figure 10:Qualitative comparisons of zero-shot semantic occupancy estimation from an ego-centric multi-camera perspective. Each row shows input images from multiple viewpoints (top), corresponding occupancy predictions by GaussTR (left bottom), the ground truth occupancy (middle bottom), and our PG-Occ method (right bottom). Dashed boxes and lines highlight specific objects—such as pedestrians, cars, bicycles, and traffic lights—that have been successfully detected and reconstructed. Our approach demonstrates superior detection and reconstruction of small or distant objects, better preserves spatial relationships, and provides more accurate object shapes compared with GaussTR. Colors indicate semantic categories as defined in the legend. For best inspection of fine details, we recommend viewing the color version and zooming in.",
                "position": 2314
            },
            {
                "img": "https://arxiv.org/html/2510.04759/x8.png",
                "caption": "Figure 11:Qualitative zero-shot semantic occupancy results on the third perspective for two views. For each view (View 1 and View 2), we show the predictions of our method (PG-Occ) alongside the Ground Truth. The results demonstrate that PG-Occ accurately captures semantic occupancy patterns across different perspectives.",
                "position": 2409
            }
        ]
    },
    {
        "header": "Appendix DAdditional Implementation Details",
        "images": []
    }
]