[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.09213/extracted/5998024/Images/intro_example.png",
                "caption": "Figure 1:Blue texts are useful information that should be extract to help determine the answer. Red texts are factual errors that potentially mislead the LLMs.",
                "position": 152
            },
            {
                "img": "https://arxiv.org/html/2411.09213/extracted/5998024/Images/DataGenFlow.png",
                "caption": "Figure 2:The overall construction process of MedRGB. The green OpenAI symbol implies that the block involves data generation using the GPT-4o model.",
                "position": 230
            }
        ]
    },
    {
        "header": "Related Work",
        "images": []
    },
    {
        "header": "Medical Retrieval-Augmented Generation Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.09213/extracted/5998024/Images/noise_1.png",
                "caption": "Figure 11:Sufficiency test main question accuracy.",
                "position": 461
            },
            {
                "img": "https://arxiv.org/html/2411.09213/extracted/5998024/Images/noise_2.png",
                "caption": "Figure 12:Sufficiency test percentage of information insufficiency (histogram) and noise detection rate (line graph).",
                "position": 464
            }
        ]
    },
    {
        "header": "Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.09213/extracted/5998024/Images/inin_1.png",
                "caption": "Figure 13:Integration test main question accuracy.",
                "position": 787
            },
            {
                "img": "https://arxiv.org/html/2411.09213/extracted/5998024/Images/inin_2.png",
                "caption": "Figure 14:Integration test sub-question GPT-based score (filled histogram) and exact-match accuracy (shaded histogram).",
                "position": 790
            },
            {
                "img": "https://arxiv.org/html/2411.09213/extracted/5998024/Images/fact_1.png",
                "caption": "Figure 15:Robustness test main question accuracy.",
                "position": 853
            },
            {
                "img": "https://arxiv.org/html/2411.09213/extracted/5998024/Images/fact_2.png",
                "caption": "Figure 16:Robustness test sub-question gpt-based score (filled histogram), exact-match accuracy (shaded histogram), and factual error detection rate(line graph).",
                "position": 856
            }
        ]
    },
    {
        "header": "Discussion",
        "images": []
    },
    {
        "header": "Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.09213/extracted/5998024/Images/suff_example.png",
                "caption": "Figure 17:Example of model’s step-by-step reasoning process in Sufficiency test. With all-noise context, the model is refuse to answer the question, despite having the internal knowledge to do it, as shown in the without-retrieval example. In case of all-signal context, the model has difficulty in accepting all documents are relevant, as the ”signal criteria” become stricter.",
                "position": 4110
            },
            {
                "img": "https://arxiv.org/html/2411.09213/extracted/5998024/Images/inin_example.png",
                "caption": "Figure 18:Example of model’s step-by-step reasoning process in Integration test. We see that the model is able to integrate information from multiple sub-questions to arrive at the correct answer. In the other hand, not having enough sub-questions may restricts the model reasoning scope, leading to incorrect answer.",
                "position": 4113
            },
            {
                "img": "https://arxiv.org/html/2411.09213/extracted/5998024/Images/fact_example.png",
                "caption": "Figure 19:Example of model’s step-by-step reasoning process in Robustness test. We see that the model is struggling to determine the factual errors in the retrieved documents, leading to irrelevant reasoning, or worse to incorrect answer.",
                "position": 4116
            }
        ]
    },
    {
        "header": "Appendix BPrompt Templates",
        "images": []
    }
]