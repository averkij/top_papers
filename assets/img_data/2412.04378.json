[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04378/x1.png",
                "caption": "Figure 1:Overall VladVA framework:a generative LVLM is adapted into a discriminative model. At test time, the vision features (𝐟vsubscript𝐟𝑣\\mathbf{f}_{v}bold_f start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT) are produced by passing the image𝐱vsubscript𝐱𝑣\\mathbf{x}_{v}bold_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPTalongside the handcrafted and/or soft prompt𝐱pvsubscriptsuperscript𝐱𝑣𝑝\\mathbf{x}^{v}_{p}bold_x start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPTto the LVLM. The last token of this sequence will contain the summarized representation. Analogously, the textual features (𝐟tsubscript𝐟𝑡\\mathbf{f}_{t}bold_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT) are produced by passing the prompt corresponding to the language modality (𝐱ptsuperscriptsubscript𝐱𝑝𝑡\\mathbf{x}_{p}^{t}bold_x start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT) through the LLM of the LVLM (i.e.the weights of the LLM are fully shared) alongside the short caption (𝐱qs⁢h⁢o⁢r⁢tsuperscriptsubscript𝐱𝑞𝑠ℎ𝑜𝑟𝑡\\mathbf{x}_{q}^{short}bold_x start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s italic_h italic_o italic_r italic_t end_POSTSUPERSCRIPT). During training, the LVLM will also take as input a long, detailed caption𝐱𝐪𝐥𝐨𝐧𝐠superscriptsubscript𝐱𝐪𝐥𝐨𝐧𝐠\\mathbf{x_{q}^{long}}bold_x start_POSTSUBSCRIPT bold_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_long end_POSTSUPERSCRIPT.",
                "position": 153
            },
            {
                "img": "https://arxiv.org/html/2412.04378/x2.png",
                "caption": "Figure 2:Entropy of the output probability distributionat the next-to-be-predicted token location using a LLaVA-1.5-7B\nfor a set of 50 prompts for both images and captions.",
                "position": 166
            },
            {
                "img": "https://arxiv.org/html/2412.04378/x3.png",
                "caption": "Figure 3:Cumulative variance of the image and text embedding matricesover a set of 50 prompts on Flickr30k.\nEmbeddings that capture more information about the input translate\ninto a cumulative variance that requires more principal components to be explained,i.e.a higher rank embedding matrix.",
                "position": 170
            },
            {
                "img": "https://arxiv.org/html/2412.04378/x4.png",
                "caption": "Figure 4:Top-k next-to-be-predicted tokensbefore and after the proposed fine-tuning approach. On the right, we show the output probability distribution for each case.\nWe observe that, when using the optimal prompt, the representations of the next token can encode diverse\nand more discriminative information, making potentially better-quality embeddings. This behavior\nis further improved by the proposed fine-tuning method.",
                "position": 176
            },
            {
                "img": "https://arxiv.org/html/2412.04378/x5.png",
                "caption": "Figure 5:Image and text retrieval score on Flickr30kover a\nset of 50 image-text\nprompts ordered by their entropy scores (Fig.2).\nWe can observe that prompts with high average entropy scores\ncorrelate positively with the zero-shot retrieval performance.",
                "position": 182
            },
            {
                "img": "https://arxiv.org/html/2412.04378/x6.png",
                "caption": "Figure 6:Attention map between the summary and vision tokens shown for a set of heads. Notice that post-training, the attention maps densify. This behavioral change can be interpreted as follows: For generative tasks, at every step in the generation process, the model has the chance to look back at the vision tokens, selectively attending to the regions of interest at the current step. In contrast, in a discriminative setting, the model must compress all information present in the image within the summary token.",
                "position": 297
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Ablation studies",
        "images": []
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "7Results for additional model sizes and architectures",
        "images": []
    },
    {
        "header": "8Compositionality evaluation on Winoground",
        "images": []
    },
    {
        "header": "9Zero-shot image recognition on ImageNet",
        "images": []
    },
    {
        "header": "10Qualitative text generation examples post discriminative adaptation",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04378/x7.png",
                "caption": "Figure 7:Qualitative comparison on image captioningof the base LLaVA-1.5-7B model and its fine-tuned versions using both E5-V[23]and our proposed method. We show that with our method, the LLaVA-1.5-7B better retains its captioning capabilities, while E5-V fine-tuning appears to result in less informative captions.",
                "position": 1865
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]