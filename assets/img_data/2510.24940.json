[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24940/x1.png",
                "caption": "Figure 1:Illustration of how implicit CoT approaches improve CoT efficiency. “Ans.” is the answer. Curl arrows represent that the tokens are autoregressively generated.rir_{i}s are explicit reasoning tokens.",
                "position": 106
            },
            {
                "img": "https://arxiv.org/html/2510.24940/x2.png",
                "caption": "Figure 2:Overview of the proposedSemCoT. Eachcyan boxis a hidden text embedding within model components, with the text content and model type varying based on the box’s position in the figure.Fireandsnowflakesigns mean the component is trained and frozen, respectively.",
                "position": 160
            },
            {
                "img": "https://arxiv.org/html/2510.24940/x3.png",
                "caption": "Figure 3:Ablation study ofSemCoT.",
                "position": 702
            },
            {
                "img": "https://arxiv.org/html/2510.24940/x4.png",
                "caption": "Figure 4:Parameter sensitivity ofSemCoT.",
                "position": 709
            },
            {
                "img": "https://arxiv.org/html/2510.24940/x5.png",
                "caption": "Figure 5:Case study comparingSemCoTvs COCONUT.\nPCA plots of implicit reasoning embeddings for 3 SVAMP queries with 20 semantic variants each.SemCoT(blue) has tighter clustering than COCONUT (orange), showing its ability to generate semantic aligned reasoning.",
                "position": 727
            },
            {
                "img": "https://arxiv.org/html/2510.24940/x6.png",
                "caption": "Figure 6:Ablation study results on Llama-2-7b-chat-hf[47]. We show the performance of ourSemCoTcompared to its three variants on all five adopted datasets in the Section4.1of the paper.",
                "position": 2914
            },
            {
                "img": "https://arxiv.org/html/2510.24940/x7.png",
                "caption": "Figure 7:Ablation study results on Mistral-7B-Instruct-v0.2[20]. We show the performance of ourSemCoTcompared to its three variants on all five adopted datasets in the Section4.1of the paper.",
                "position": 2917
            },
            {
                "img": "https://arxiv.org/html/2510.24940/x8.png",
                "caption": "Figure 8:Parameter analysis experiment results for Llama-2-7b-chat-hf[47]. The accuracy and inference time of our method using Llama when varying the number of implicit tokens",
                "position": 2927
            },
            {
                "img": "https://arxiv.org/html/2510.24940/x9.png",
                "caption": "Figure 9:Parameter analysis experiment results for Mistral-7B-Instruct-v0.2[20]. The accuracy and inference time of our method using Mistral when varying the number of implicit tokens",
                "position": 2930
            },
            {
                "img": "https://arxiv.org/html/2510.24940/x10.png",
                "caption": "Figure 10:Case study forSoftCoT[54]on Llama-2-7b-chat-hf[47].",
                "position": 2940
            },
            {
                "img": "https://arxiv.org/html/2510.24940/x11.png",
                "caption": "Figure 11:Case study forSoftCoT[54]on Mistral-7B-Instruct-v0.2[20].",
                "position": 2943
            },
            {
                "img": "https://arxiv.org/html/2510.24940/x12.png",
                "caption": "Figure 12:Case study forSemCoTon Llama-2-7b-chat-hf[47].",
                "position": 2946
            },
            {
                "img": "https://arxiv.org/html/2510.24940/x13.png",
                "caption": "Figure 13:Case study forSemCoTon Mistral-7B-Instruct-v0.2[20].",
                "position": 2949
            },
            {
                "img": "https://arxiv.org/html/2510.24940/x14.png",
                "caption": "Figure 14:Case study for COCONUT[54]on Llama-2-7b-chat-hf[47].",
                "position": 2952
            },
            {
                "img": "https://arxiv.org/html/2510.24940/x15.png",
                "caption": "Figure 15:Case study for COCONUT[54]on Mistral-7B-Instruct-v0.2[20].",
                "position": 2955
            },
            {
                "img": "https://arxiv.org/html/2510.24940/x16.png",
                "caption": "Figure 16:Case study for CODI[42]on Llama-2-7b-chat-hf[47].",
                "position": 2958
            },
            {
                "img": "https://arxiv.org/html/2510.24940/x17.png",
                "caption": "Figure 17:Case study for CODI[42]on Mistral-7B-Instruct-v0.2[20].",
                "position": 2961
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]