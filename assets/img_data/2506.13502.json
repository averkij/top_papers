[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.13502/extracted/6545694/figs/bow-logo.png",
                "caption": "",
                "position": 91
            },
            {
                "img": "https://arxiv.org/html/2506.13502/x1.png",
                "caption": "Figure 1:An overview of our proposedBOWRL framework. Given the context, \"I like fruits a lot, so for lunch, I ate two\", rather than predicting the next word \"pears\" directly, the policy model first generates an intermediate reasoning trajectory. A frozen judge model then returns the next-token probability distribution solely based on this reasoning trajectory. The policy model is finally optimized through the reward for its generated reasoning trajectory. The reward is calculated based on the probability of the gold next token in the next-token probability distribution returned by the judge.",
                "position": 125
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Bottlenecked Next Word Exploration",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Analysis",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APrompting Details",
        "images": []
    }
]