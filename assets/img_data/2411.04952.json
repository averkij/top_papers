[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04952/x1.png",
                "caption": "Figure 1:Comparison of multi-modal document understanding pipelines.\nPrevious works focus on(a) Single-page DocVQAthat cannot handle many long documents\nor(b) Text-based RAGthat ignores visual information.\nOur(c)M3DocRAGframework\nretrieves relevant documents and answers questions using multi-modal retrieval and MLM components, so that it can efficiently handle many long documents while preserving visual information.",
                "position": 134
            }
        ]
    },
    {
        "header": "1Introduction and Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04952/x2.png",
                "caption": "Figure 2:Comparison of existing DocVQA datasets (left;e.g., DocVQA[42]) and ourM3DocVQAdataset (right).\nIn contrast to previous DocVQA datasets that have questions that are specific to a single provided PDF (e.g., “What was the gross profit in the year 2009?”),M3DocVQAhas information-seeking questions that benchmark open-domain question answering capabilities across more than 3,000 PDF documents (i.e., 40,000+ pages).",
                "position": 167
            },
            {
                "img": "https://arxiv.org/html/2411.04952/x3.png",
                "caption": "Figure 3:OurM3DocRAGframework (Sec.2) consists of three stages:\n(1) document embedding (Sec.2.1),\n(2) page retrieval (Sec.2.2),\nand (3) question answering (Sec.2.3).\nIn(1) document embedding, we extract visual embedding (with ColPali) to represent each page from all PDF documents.\nIn(2) page retrieval, we retrieve the top-K pages of high relevance (MaxSim scores) with text queries.\nIn an open-domain setting, we create approximate page indices for faster search.\nIn(3) question answering, we conduct visual question answering with multi-modal LM (e.g.Qwen2-VL) to obtain the final answer.",
                "position": 197
            }
        ]
    },
    {
        "header": "2M3DocRAG: A Unified Framework for Multi-modal, Multi-page, Multi-document Understanding",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04952/x4.png",
                "caption": "Figure 4:Illustration of PDF collections inM3DocVQA.\nWe first collect the URLs of all supporting contexts (Wikipedia documents) of individual questions of MultimodalQA[54]. Then, we create PDF versions from their URLs by rendering them in a web browser.",
                "position": 350
            }
        ]
    },
    {
        "header": "3M3DocVQA: A New Benchmark for Open-domain Document Understanding",
        "images": []
    },
    {
        "header": "4Experiment Setup",
        "images": []
    },
    {
        "header": "5Results and Key Findings",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04952/x5.png",
                "caption": "Figure 5:Qualitative example of ColPali + Qwen2-VL 7B onM3DocVQA. Image regions relevant to the question/answer are highlighted with orange boxes. The answer information is only stored visually within the game logo, where a man is leaning on a motorcycle.",
                "position": 1162
            },
            {
                "img": "https://arxiv.org/html/2411.04952/x6.png",
                "caption": "Figure 6:Qualitative example of ColPali + Qwen2-VL 7B onM3DocVQA. Image regions relevant to the question/answer are highlighted with orange boxes. The question requires multi-page/document reasoning.",
                "position": 1165
            },
            {
                "img": "https://arxiv.org/html/2411.04952/x7.png",
                "caption": "Figure 7:Qualitative example of ColPali + Qwen2-VL 7B onM3DocVQA. Image regions relevant to the question/answer are highlighted with orange boxes.\nThe VQA component could combine both the retrieved knowledge (Tropi was transferred on 11 July 2017) and its own knowledge (Valencia CF has a logo with a bat) to provide the final answer.",
                "position": 1169
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Ethical Considerations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]