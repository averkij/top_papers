[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10128/x1.png",
                "caption": "",
                "position": 155
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10128/x2.png",
                "caption": "Figure 1:Overview of the ViCrit framework.\nStarting from high-quality image–caption pairs, we synthetically inject visual hallucinations by minimally altering noun phrases.\nThe model is trained to localize incorrect spans in the caption given the image, receiving a verifiable reward through exact string matching.\nThis fine-grained perceptual objective improves visual perception in vision-language models (VLMs) and generalizes to downstream reasoning tasks across diverse visual domains.",
                "position": 166
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10128/x3.png",
                "caption": "Figure 2:Instead of asking the model to write a paragraph-long caption that is hard to grade (e.g., the 200-word example above), ViCrit feeds the model an almost-correct caption containing a single, deliberately inserted visual hallucination and trains it to locate that error. The short, token-level response is just as demanding in terms of visual perception, yet it is far easier to verify automatically.",
                "position": 234
            }
        ]
    },
    {
        "header": "3ViCrit RL Training",
        "images": []
    },
    {
        "header": "4ViCrit Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10128/x4.png",
                "caption": "Figure 3:Data examples from ViCrit-Bench, which involve four image categories and eight visual hallucination types. We manually verify each image’s long caption, and carefully inject different kinds of proper visual hallucinations.",
                "position": 299
            },
            {
                "img": "https://arxiv.org/html/2506.10128/extracted/6534130/figures/task_distribution.png",
                "caption": "Figure 4:Hallucination task distribution of ViCrit-Bench.",
                "position": 347
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10128/x5.png",
                "caption": "Figure 5:Correlation between average VLM task performance and ViCrit-Bench performance (Task Avg. and Overall columns in Table2). Each point represents a different model, and the fitted linear regression line highlights a strong positive relationship, indicating that better ViCrit-Bench results are associated with higher stronger VLM capabilities.",
                "position": 1128
            },
            {
                "img": "https://arxiv.org/html/2506.10128/x6.png",
                "caption": "Figure 6:Two examples demonstrate the behavioral differences between models before and after training with the ViCrit task. It can be seen that ViCrit-RL-72B pays closer attention to image details and arrives at the correct final reasoning through its enhanced perception capabilities.",
                "position": 1134
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix APrompts used in experiments",
        "images": []
    },
    {
        "header": "Appendix BComparison with SFT",
        "images": []
    }
]