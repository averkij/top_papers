[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11404/x1.png",
                "caption": "Figure 1:Chain-of-Thought in different space. (a) Language CoT paradigm predicts sub-tasks as intermediate reasoning. (b) Visual CoT paradigm synthesizes a goal image to provide guidance for action policy. (c) Our proposed Action CoT directly operates in action space and provides homogeneous action guidance.",
                "position": 74
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11404/x2.png",
                "caption": "Figure 2:Architectural Overview of ACoT-VLA.\nThe framework consists of three main components operating on features from a shared VLM backbone.\n(a) The Explicit Action Reasoner (EAR) is a Transformer-based module that synthesizes a coarse reference trajectory, providing explicit action-space guidance.\n(b) The Implicit Action Reasoner (IAR) employs a cross-attention mechanism with learnable queries to extract latent action priors from the VLMâ€™s internal representations.\n(c) The Action-Guided Prediction (AGP) head synergistically integrates both explicit and implicit guidances via cross-attention to condition the final denoising process, producing the executable action sequence.",
                "position": 145
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11404/x3.png",
                "caption": "Figure 3:Visualization of three manipulation tasks in real world.",
                "position": 1330
            },
            {
                "img": "https://arxiv.org/html/2601.11404/figures/fig4.png",
                "caption": "Figure 4:Evaluation results of real-world experiments.",
                "position": 1356
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADataset Description",
        "images": []
    },
    {
        "header": "Appendix BTraining & Evaluation Details",
        "images": []
    },
    {
        "header": "Appendix CMore Experimental Results",
        "images": []
    },
    {
        "header": "Appendix DLimitations & Future Works",
        "images": []
    },
    {
        "header": "Appendix ELLM Usage Statement",
        "images": []
    }
]