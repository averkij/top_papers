[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21337/x1.png",
                "caption": "",
                "position": 164
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21337/x2.png",
                "caption": "Figure 2:Data collection and cleaning pipeline.(a) We crawl the Wikipedia category tree of buildings, collecting façade images, construction years, GPS coordinates, textual descriptions, and pageview statistics.\n(b) The raw crawl of 90k images is refined through deduplication, a CLIP-based building filter, and a light manual audit, yielding 55k clean façades.\n(c) Examples of discarded non-building or duplicate samples.",
                "position": 389
            }
        ]
    },
    {
        "header": "3Dataset and Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21337/x3.png",
                "caption": "Figure 3:Dataset statistics.This figure provides an overview of our dataset’s key characteristics.\n(a) Continent Distribution shows the geographical origins of the images.\n(b) Built Year Distribution illustrates the age of the structures.\n(c) Pageview Distribution represents the buildings’ popularity.\n(d) Renovation Distribution indicates the extent of reconstruction.\n(e) Rural/Urban Distribution reflects the population density of a building’s location.",
                "position": 459
            },
            {
                "img": "https://arxiv.org/html/2512.21337/x4.png",
                "caption": "Figure 4:YearCLIParchitecture.An image encoderfvf_{v}(CLIP) extracts 224×\\times224 facade features. We then fuse the feature with a GPS embedding from the location encoderflf_{l}(RFF + MLP, optional input) via a learnable zero-convolution. Parallel text branches encode (i) seven coarse style classesfcf_{c}and (ii) a bank of reasoning promptsfrf_{r}describing roofs, walls, heights, etc. All frozen encoders feed a trainable regressorg​(⋅)g(\\cdot)that performs coarse-to-fine ordinal regression. It predicts a construction year (here 1687), selects the best-matching style/reason tokens, and outputs a readable rationale.",
                "position": 547
            }
        ]
    },
    {
        "header": "4Our YearCLIP Model",
        "images": []
    },
    {
        "header": "5Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21337/x5.png",
                "caption": "Figure 5:Prediction error scatter plots for representative models.(a) ConvNeXt-B (CNN), (b) Swin-B (Transformer), (c) YearCLIP (ours, CLIP-based), (d) Gemini1.5-pro (VLM), and (e) Gemma3-27B (VLM).\nThe horizontal axis shows predicted construction year, vertical axis shows groundtruth.\nEach point represents a single building.\nThe red diagonal line indicates perfect prediction.",
                "position": 1448
            },
            {
                "img": "https://arxiv.org/html/2512.21337/x6.png",
                "caption": "Figure 6:Explainable age predictions withYearCLIP.Powered by Reason-enhanced NumCLIP, the system predictsconstruction yearwithin ±15 yr ofground truthand provides rationales that highlightstylisticandhistoriccues. CLIP baselines miss or misassign these signals, whereas our location + reason pipeline yields transparent, verifiable explanations.",
                "position": 1476
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Implementation details",
        "images": []
    },
    {
        "header": "8Training Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21337/x7.png",
                "caption": "Figure 7:YearCLIParchitecture.An image encoderfvf_{v}(CLIP) extracts 224×\\times224 facade features. We then fuse the feature with a GPS embedding from the location encoderflf_{l}(RFF + MLP, optional input) via a learnable zero-convolution. Parallel text branches encode (i) seven coarse style classesfcf_{c}and (ii) a bank of reasoning promptsfrf_{r}describing roofs, walls, heights, etc. All frozen encoders feed a trainable regressorg​(⋅)g(\\cdot)that performs coarse-to-fine ordinal regression. It outputs the predicted construction year (here 1687), selects the best-fit style/reason tokens, and outputs a readable rationale.",
                "position": 1604
            }
        ]
    },
    {
        "header": "9Full Table Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21337/x8.png",
                "caption": "Figure 8:Comparison of basic methods based on Mean Absolute Error (MAE).The figure displays MAE values for various models, categorized by method type: CNN-based (green), Transformer-based (blue), and CLIP-based (yellow). Methods are positioned along the MAE axis, ranging from 40 to 60, with labels indicating model names.",
                "position": 1903
            },
            {
                "img": "https://arxiv.org/html/2512.21337/Figure/gemini_failure_case.png",
                "caption": "Figure 9:Top 100 prediction errors by Gemini2.0-Flash.This figure shows the 100 building images with the highest Mean Absolute Error (MAE) when predicted by Gemini2.0-Flash. Each image is labeled with the ground truth year (Year), predicted year (Pred), and Wikipedia page views (Views) as popularity indicator. These challenging cases illustrate common failure modes including ancient buildings, heavily renovated structures, and architecturally ambiguous facades.",
                "position": 2776
            }
        ]
    },
    {
        "header": "10LLM Usage Statement",
        "images": []
    }
]