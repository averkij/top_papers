[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20354/x1.png",
                "caption": "",
                "position": 190
            },
            {
                "img": "https://arxiv.org/html/2601.20354/x2.png",
                "caption": "Figure 1:(Top): Error cases around spatial perception, reasoning, and interaction from GPT-Image-1(OpenAI,2024b), Qwen-Image(Wuet al.,2025a), and Bagel(Denget al.,2025).(Bottom): A comparison of prompt and evaluation formats across current benchmarks(Weiet al.,2025).",
                "position": 192
            },
            {
                "img": "https://arxiv.org/html/2601.20354/x3.png",
                "caption": "Figure 2:Overview of the SpatialGenEval benchmark and key results. The benchmark is structured around(a)10 spatial sub-domains and(b)25 real-world scenes.(c)The evaluation of 23 SOTA T2I models shows the overall performance ranking and(d)a detailed capability breakdown.",
                "position": 228
            }
        ]
    },
    {
        "header": "2SpatialGenEval Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20354/x4.png",
                "caption": "Figure 3:Examples of SpatialGenEval. Each image is generated from an information-dense prompt covering all 10 spatial sub-domains and evaluated with 10 corresponding multiple-choice questions.",
                "position": 479
            },
            {
                "img": "https://arxiv.org/html/2601.20354/x5.png",
                "caption": "Figure 4:SpatialGenEval Construction Pipeline.(a)The process begins by selecting one of 25 real-world scenes and combining it with the definitions of all 10 spatial sub-domains.(b)The MLLM sequentially synthesizes an information-dense prompt that integrates all 10 constraints, along with 10 corresponding omni-dimensional QA pairs.(c)T2I models generate an image from the prompt, which is then evaluated against the QA pairs to yield a fine-grained spatial intelligence score.",
                "position": 520
            }
        ]
    },
    {
        "header": "3Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20354/x6.png",
                "caption": "Table 2:SpatialGenEval leaderboard based on Qwen 2.5 VL (72B). “Random”: random selection.",
                "position": 597
            },
            {
                "img": "https://arxiv.org/html/2601.20354/x6.png",
                "caption": "Figure 5:Distribution of error types across scenes (left, based on all T2I models) and some examples of T2I models (right) in our SpatialGenEval.",
                "position": 1072
            },
            {
                "img": "https://arxiv.org/html/2601.20354/x7.png",
                "caption": "Table 6:Quantitative fine-tuned results of recent T2I models on SpatialGenEval.",
                "position": 1257
            },
            {
                "img": "https://arxiv.org/html/2601.20354/x7.png",
                "caption": "Figure 6:Qualitative comparisons of recent T2I models on SpatialGenEval.",
                "position": 1385
            }
        ]
    },
    {
        "header": "4Supervised Fine-Tuning (SFT)",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20354/x8.png",
                "caption": "Figure 7:(Top)Ablations of the contribution of three different SpatialT2I subsets to fine-tuning performance. The selected subsets are arranged from top to bottom in order of their performance.(Bottom)Data scaling trend observed when incrementally adding better subsets to the training data.",
                "position": 1428
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20354/x9.png",
                "caption": "Figure 8:Samples of generated results from all selected 25 scenes in SpatialGenEval.",
                "position": 2247
            },
            {
                "img": "https://arxiv.org/html/2601.20354/x10.png",
                "caption": "Figure 9:Illustration depicting the human annotation interface, where the experts are presented with the prompt and corresponding question-answer pairs for final refinement.",
                "position": 2250
            },
            {
                "img": "https://arxiv.org/html/2601.20354/x11.png",
                "caption": "Figure 10:A data sample from the SpatialGenEval benchmark, where the generated image is challenged by 10 question-answer pairs across 4 key dimensions: Spatial Foundation (S1, S2), Spatial Perception (S3-S5), Spatial Reasoning (S6-S8), and Spatial Interaction (S9, S10).\nThe generated images are from 12 top-performing T2I models from top-left to bottom-right,i.e., Qwen-Image(Wuet al.,2025a), GPT-4o(OpenAI,2024b), Flux.1-krea(Black Forest Labs,2024), InfinityHanet al.(2025), Bagel(Denget al.,2025), Flux.1-dev(Black Forest Labs,2024), OmniGen2(Wuet al.,2025b), NextStep-1(NextStep Teamet al.,2025), DALL-E-3(Rameshet al.,2021), SD-3-M(Rombachet al.,2022), UniWorld-V1(Linet al.,2025), SD-3.5-L(Rombachet al.,2022).",
                "position": 2689
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]