[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.11409/x1.png",
                "caption": "Figure 1:Comparison of Nemotron-H 4B model accuracy w.r.t. inference throughput (left), and training budget for the base model (right) to similarly-sized community models. Inference throughput is measured at an input and output sequence length of 65536 and 1024, respectively.",
                "position": 178
            },
            {
                "img": "https://arxiv.org/html/2504.11409/x1.png",
                "caption": "",
                "position": 181
            },
            {
                "img": "https://arxiv.org/html/2504.11409/x2.png",
                "caption": "",
                "position": 185
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.11409/x3.png",
                "caption": "Figure 2:Overview of pruning and distillation for hybrid architectures. Starting from a pretrained LLM, we first evaluate the importance of Mamba heads and channels, FFN neurons, and embedding channels. We then rank them, trim the least important neurons, and distill the knowledge from the original LLM to the pruned model. Attention layers are not pruned since they amount to only 8% of the total number of layers.",
                "position": 306
            },
            {
                "img": "https://arxiv.org/html/2504.11409/x4.png",
                "caption": "Figure 3:Mamba group structure visualization showing broadcasting and originalBt⁢xtsubscript𝐵𝑡subscript𝑥𝑡B_{t}x_{t}italic_B start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTcomputation. Colors represent distinct entries. The Figure illustrates how only within-group head permutations can preserve SSM semantics. As a counter example, if H3 and H8 were to be swapped, the resultingBt⁢xtsubscript𝐵𝑡subscript𝑥𝑡B_{t}x_{t}italic_B start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTwould NOT be any permutation of the original (no permutation)Bt⁢xtsubscript𝐵𝑡subscript𝑥𝑡B_{t}x_{t}italic_B start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT.",
                "position": 309
            },
            {
                "img": "https://arxiv.org/html/2504.11409/x5.png",
                "caption": "Figure 4:Layer importance measured as the KLD between logits of the full model and a model with that layer removed, averaged over a small training subset. Vertical dotted lines indicate layer types: self-attention (green), FFN (blue), and Mamba2 (red).",
                "position": 676
            },
            {
                "img": "https://arxiv.org/html/2504.11409/x5.png",
                "caption": "Figure 4:Layer importance measured as the KLD between logits of the full model and a model with that layer removed, averaged over a small training subset. Vertical dotted lines indicate layer types: self-attention (green), FFN (blue), and Mamba2 (red).",
                "position": 679
            },
            {
                "img": "https://arxiv.org/html/2504.11409/x6.png",
                "caption": "Figure 5:Accuracy drop relative to the 8B model across progressively depth-only pruned variants (48, 44, 40, 36, and 26 layers). Each model is directly pruned from the 8B and distilled using 126B tokens.",
                "position": 684
            }
        ]
    },
    {
        "header": "4Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.11409/x7.png",
                "caption": "Figure 6:Left:Correlation matrix showing relationships between performance metrics and model components—FFN, embedding dimension (desubscript𝑑𝑒d_{e}italic_d start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT), and Mamba parameters (varying both headsmhsubscript𝑚ℎm_{h}italic_m start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPTand head dimensionmdsubscript𝑚𝑑m_{d}italic_m start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT)—across 125 4B variants with fixed depth (52 layers).Right:Model parameter correlations for a fixed 4B parameter budget—highlighting trade-offs where increasing one component reduces others.",
                "position": 1208
            },
            {
                "img": "https://arxiv.org/html/2504.11409/x7.png",
                "caption": "Figure 6:Left:Correlation matrix showing relationships between performance metrics and model components—FFN, embedding dimension (desubscript𝑑𝑒d_{e}italic_d start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT), and Mamba parameters (varying both headsmhsubscript𝑚ℎm_{h}italic_m start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPTand head dimensionmdsubscript𝑚𝑑m_{d}italic_m start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT)—across 125 4B variants with fixed depth (52 layers).Right:Model parameter correlations for a fixed 4B parameter budget—highlighting trade-offs where increasing one component reduces others.",
                "position": 1211
            },
            {
                "img": "https://arxiv.org/html/2504.11409/x8.png",
                "caption": "Figure 7:Impact of pruning Mamba heads (mhsubscript𝑚ℎm_{h}italic_m start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT) versus Mamba head channels (mdsubscript𝑚𝑑m_{d}italic_m start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT) in isolation, with the rest of the network unchanged. Pruningmhsubscript𝑚ℎm_{h}italic_m start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPTconsistently outperformsmdsubscript𝑚𝑑m_{d}italic_m start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPTpruning across LM loss, latency, and throughput—establishing it as the preferred target for optimization.",
                "position": 1216
            },
            {
                "img": "https://arxiv.org/html/2504.11409/x9.png",
                "caption": "Figure 8:Throughput and latency comparisons across four models: Phi-4-Mini-4B, Qwen-2.5-3B, Nemotron-H 8B, and Nemotron-H 4B (ours). Relative throughput and latency represents are measured for an input and output context length of 65536 and 1024, respectively.",
                "position": 1654
            }
        ]
    },
    {
        "header": "Conclusions",
        "images": []
    },
    {
        "header": "5Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]