[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.06458/x1.png",
                "caption": "Table 1:Example of user instruction where all subject LLMs failed.Responses from four LLMs are shown. All responses incorrectly include hashtags, despite a constraint explicitly requesting to not do so. Constraints in the instruction are highlighted in blue, and errors in LLM responses are highlighted in red.",
                "position": 248
            },
            {
                "img": "https://arxiv.org/html/2410.06458/x2.png",
                "caption": "Figure 1:RealInstructBenchmark Workflow.Real-user original instruction is input into the Subject LLM, which generates a response. Using the original instruction, decomposed constraints, and the generated response, model-based evaluation assesses the quality of the response against each constraint one at a time, and then aggregates the results into an instruction-level metric.",
                "position": 303
            }
        ]
    },
    {
        "header": "2RealInstructDataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.06458/x3.png",
                "caption": "Figure 2:TheDeCRIMpipeline.Initially, the LLM generates a response to a user request. The Decomposer breaks down the request into granular constraints. A Critic model then gives feedback on whether the response meets all constraints. If it does, the response is output; if not, the feedback is used by LLM to refine the response. ThisCritique–Refinecycle repeats until all constraints are satisfied or the maximum number of iterations is reached.",
                "position": 513
            }
        ]
    },
    {
        "header": "3Self-correction withDecompose, Critique, and Refine(DeCRIM)",
        "images": []
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Results and Discussion",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "8Ethical Considerations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ARelated Work",
        "images": []
    },
    {
        "header": "Appendix BExtra Analysis and Discussions ofDeCRIM",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.06458/x4.png",
                "caption": "Figure 3:Two-stepDecompose-then-Generate(DtG) prompt:Inspired by the two-step Rephrase and Respond (RaR)(Deng et al.,2023), DtG first instructs the LLM to decompose multi-constrained instructions into an enumerated list of constraints. Then, DtG uses this decomposition as if it were the model’s own \"reasoning and planning\" (leveraging the model’s user and assistant tokens) to generate the final response. Like RaR, this process can be done in one or two steps, with the two-step method being more effective.",
                "position": 3384
            }
        ]
    },
    {
        "header": "Appendix CDefinition of Task, Context, and Constraints inRealInstruct",
        "images": []
    },
    {
        "header": "Appendix DRealInstructdata construction details",
        "images": []
    },
    {
        "header": "1) Task Overview:",
        "images": []
    },
    {
        "header": "2) Step-by-step task:",
        "images": []
    },
    {
        "header": "3) General Instructions",
        "images": []
    },
    {
        "header": "4) Non-exhaustive list of possible constraint types to be considered",
        "images": []
    },
    {
        "header": "5) Examples",
        "images": []
    },
    {
        "header": "Appendix ERealInstructConstraints Categorization",
        "images": []
    },
    {
        "header": "Appendix FRealInstructData Samples",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.06458/x5.png",
                "caption": "Table 10:Sample Elements fromRealInstructDataset - Part 1",
                "position": 4919
            },
            {
                "img": "https://arxiv.org/html/2410.06458/x6.png",
                "caption": "Table 11:Sample Elements fromRealInstructDataset - Part 2",
                "position": 4929
            }
        ]
    },
    {
        "header": "Appendix GImplementation details forDeCRIMpipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.06458/extracted/5912014/latex/figures/code_prompt.png",
                "caption": "",
                "position": 5043
            }
        ]
    },
    {
        "header": "Appendix HExtra Experimental Details for LLM-as-a-Judge Validation for Constraint Satisfaction",
        "images": []
    }
]