[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18882/extracted/6564301/figures/assets/home.png",
                "caption": "",
                "position": 117
            },
            {
                "img": "https://arxiv.org/html/2506.18882/extracted/6564301/figures/assets/github.png",
                "caption": "",
                "position": 118
            },
            {
                "img": "https://arxiv.org/html/2506.18882/extracted/6564301/figures/assets/huggingface.png",
                "caption": "",
                "position": 119
            },
            {
                "img": "https://arxiv.org/html/2506.18882/extracted/6564301/figures/assets/gradio.png",
                "caption": "",
                "position": 120
            },
            {
                "img": "https://arxiv.org/html/2506.18882/x1.png",
                "caption": "Figure 1:(Left) Given multiple images from the same viewpoint under varying lighting, our method outperforms existing universal photometric stereo methods inaccuracyanddetail, even exceeding the professional 3D scanner. (Right) On the DiliGenT benchmark, with similar decoders, higher feature consistency (SSIM and CSIM) in encoder features corresponds to better accuracy, which is measured by the percentage of pixels with angular error under9.25∘superscript9.259.25^{\\circ}9.25 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT.",
                "position": 131
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18882/x2.png",
                "caption": "Figure 2:Overview of theLiNo-UniPSarchitecture, featuring a Light-Normal Contextual Encoder, Decoder, and loss computation. The encoder includes light registered wavelet-aware downsampler, an enhanced Light-Normal Contextual Attention Module, a DPT-Based[45]Fusion Module, and a WaveUpSampler, which together encode and fuse pixel and wavelet domain features for normal-consistent outputs. The Decoder is similar to SDM-UniPS[31].",
                "position": 188
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18882/x3.png",
                "caption": "Figure 3:We visualize the attention maps of lighting registers on the encoder’s final-layer feature maps for both real (left) and synthetic (right) data. The first and second rows correspond to HDRI and point light registers, respectively, showing attention regions that rotate from left to right, demonstrating the model’s ability to capture light from different directions. The HDRI register exhibits broader attention regions, while the point light register focuses more sharply, consistent with the low-frequency nature of HDRI and the high-frequency characteristics of point lights.",
                "position": 224
            },
            {
                "img": "https://arxiv.org/html/2506.18882/x4.png",
                "caption": "Figure 4:Features from different method encoders; rightmost column is variance.",
                "position": 228
            },
            {
                "img": "https://arxiv.org/html/2506.18882/x4.png",
                "caption": "Figure 4:Features from different method encoders; rightmost column is variance.",
                "position": 231
            },
            {
                "img": "https://arxiv.org/html/2506.18882/x5.png",
                "caption": "Figure 5:Results of object inference with masks in Luces and DiLiGenT datasets.",
                "position": 236
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18882/x6.png",
                "caption": "Figure 6:Results on the DiLiGenT102superscript102{10^{2}}10 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTdataset: a matrix comparing performance where rows/columns correspond to shapes/materials. Values indicate Mean Angular Error (MAE) (mean/median). For enhanced detail visibility, viewing the electronic version in color is recommended.",
                "position": 576
            },
            {
                "img": "https://arxiv.org/html/2506.18882/x7.png",
                "caption": "Figure 7:Our method outperforms other universal PS methods in challenging synthetic scenes with no masks, detailed textures, complex geometry, and high reflectivity.",
                "position": 769
            },
            {
                "img": "https://arxiv.org/html/2506.18882/x8.png",
                "caption": "Figure 8:High-resolution (>2K) surface normal recovery from real captured images under varying lighting conditions. Top: Canandwood from[28], masked object; bottom: Coins and keyboard from[31], unmasked scene.",
                "position": 951
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18882/x9.png",
                "caption": "Figure A1:The objects are displayed sequentially from left to right, representing Level 1 to Level 5. Across Levels 1 through 5, there is a progressive rise in geometric complexity. Specifically, Level 5 features exceptionally complex surface geometry due to the utilization of normal maps in its rendering process.",
                "position": 2005
            },
            {
                "img": "https://arxiv.org/html/2506.18882/x10.png",
                "caption": "Figure A2:Effect of normal mapping on rendered surface detail. The top two rows display renderings without normal maps, while the bottom two rows showcase the same scenes rendered with normal map. It is evident that employing normal maps (bottom rows) results in significantly more high-frequency surface normal detail compared to renderings without (top rows).",
                "position": 2070
            },
            {
                "img": "https://arxiv.org/html/2506.18882/x11.png",
                "caption": "Figure A3:20 multi-light images of one scene and their respective lighting configurations.",
                "position": 2086
            },
            {
                "img": "https://arxiv.org/html/2506.18882/x12.png",
                "caption": "Figure A4:Visual comparison of different datasets. The spatial resolution of the images corresponding to each column is indicated beneath it.",
                "position": 2099
            },
            {
                "img": "https://arxiv.org/html/2506.18882/x13.png",
                "caption": "Figure A5:Attention maps for light registers tokens, showcasing their ability to focus on illumination-related regions within the input.",
                "position": 2151
            },
            {
                "img": "https://arxiv.org/html/2506.18882/x14.png",
                "caption": "Figure A6:Attention maps for our different light registers tokens reveals distinct focusing patterns.\nThe HDRI register token typically attends to broader, more diffuse regions, while the point light register token and the area light register token exhibit similar behaviors, both demonstrating a more localized and sharper attentional focus.",
                "position": 2169
            },
            {
                "img": "https://arxiv.org/html/2506.18882/x15.png",
                "caption": "Figure A7:For near-planar objects possessing intricate concave and convex surface details, our LiNO-UniPS tends to invert the predicted surface normals. The objects are from DiLiGenT-ΠΠ\\Piroman_Π[52]",
                "position": 2177
            },
            {
                "img": "https://arxiv.org/html/2506.18882/x16.png",
                "caption": "Figure A8:Post-PCA visualization of features extracted by different method encoders for the CowPNG from the DiLiGenT[48]. Metrics displayed to the right of each row is (CSIM/SSIM), higher values indicate higher feature similarity.",
                "position": 2222
            },
            {
                "img": "https://arxiv.org/html/2506.18882/x17.png",
                "caption": "Figure A9:Top row: Example from the input multi-light images.\nBottom row: Surface normal map reconstructed by our LiNO-UniPS.\nThe data utilized is from SDM-UniPS[31], and the resolution is 4K .",
                "position": 2225
            },
            {
                "img": "https://arxiv.org/html/2506.18882/x18.png",
                "caption": "Figure A10:Real-world data with masks and corresponding LiNO-UniPS reconstruction results; data sourced from UniPS[28]and SDM-UniPS[31].",
                "position": 2231
            },
            {
                "img": "https://arxiv.org/html/2506.18882/x19.png",
                "caption": "Figure A11:Comparison of different Universal PS methods on our PS-Verse Testdata, showcasing ground truth normals, reconstruction normals, and corresponding error maps.\nThe error maps depict the Mean Angular Error (MAE), measured in degrees; lower MAE values signify a more accurate reconstruction.",
                "position": 2234
            }
        ]
    },
    {
        "header": "Appendix A1Appendix",
        "images": []
    }
]