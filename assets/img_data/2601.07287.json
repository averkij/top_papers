[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.07287/x1.png",
                "caption": "Figure 1:Visualization of semantic alignment within the Wan2.1-I2V[57], quantified by the cosine similarity between visual features and keyword textual features. The features are sampled at evenly spaced inference steps and network layers. The heatmap reveals that the initial and final layers exhibit stronger and more accurate alignment with the target words, while several intermediate layers show noticeably degraded and noisy responses.",
                "position": 115
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.07287/x2.png",
                "caption": "(a)Moran’s I of similarity between visual and textual features.",
                "position": 162
            },
            {
                "img": "https://arxiv.org/html/2601.07287/x2.png",
                "caption": "(a)Moran’s I of similarity between visual and textual features.",
                "position": 165
            },
            {
                "img": "https://arxiv.org/html/2601.07287/fig/Std.png",
                "caption": "(b)Standard deviation of similarity between visual and textual features.",
                "position": 171
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Background and Problem Formulation",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.07287/x3.png",
                "caption": "Figure 3:Overview of the Focal Guidance framework. FG consists of two main components: Fine-grained Semantic Guidance and Attention Cache. (a) Fine-grained Semantic Guidance enhances the accuracy of information conditioning and reduces the model’s learning complexity by coupling the fine-grained relationships among the VAE-encoded reference frame, image encoder features, and text conditions. (b) Attention Cache leverages the semantic-responsive layers’ attention patterns to guide the injection of conditions into layers with weak semantic responses.",
                "position": 286
            }
        ]
    },
    {
        "header": "4Method: Focal Guidance Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.07287/x4.png",
                "caption": "Figure 4:Qualitative comparison of controllability across mainstream open-source I2V models. Existing methods often fail to reliably ground the text instruction in the first-frame reference, leading to instruction non-compliance and hallucinated (or duplicated) visual elements. Our FG approach strengthens text–reference alignment, enabling more accurate instruction following and improved controllability.(All visual examples in this paper are from public benchmark datasets.)",
                "position": 319
            },
            {
                "img": "https://arxiv.org/html/2601.07287/x5.png",
                "caption": "Figure 5:Qualitative ablations on Wan2.1-I2V. We randomly sample cases along three dimensions—Human Motion, Dynamic Attributes, and Human Interaction. With FG, text–reference alignment is strengthened, motions and attributes follow instructions more faithfully.",
                "position": 366
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusion, Limitation and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.07287/x6.png",
                "caption": "1.Figure:Illustrative qualitative examples generated by FG-Wan2.1-I2V 14B across three dimensions: human motion, human interaction, and dynamic attribute changes. These cases demonstrate the model’s ability to produce realistic, temporally consistent, and semantically coherent video outputs under diverse scenarios.",
                "position": 1734
            },
            {
                "img": "https://arxiv.org/html/2601.07287/x7.png",
                "caption": "Figure A.2:Visualization of reference images in our benchmark. We manually annotate subject bounding boxes on the original-resolution images and derive two canonical crops16:9and1:1. Based on these annotations we can generate adaptive resolution reference images for image to video generation.",
                "position": 1753
            },
            {
                "img": "https://arxiv.org/html/2601.07287/fig/image_size_distribution.png",
                "caption": "Figure B.3:Resolution statistics of reference images.",
                "position": 1766
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]