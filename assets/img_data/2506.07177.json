[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07177/x1.png",
                "caption": "Figure 1:Frame Guidance enables training-free controllable video generation using flexible frame-level inputs.\nIt supports diverse applications, including keyframe-guided generation, stylization, and looping, using general frame-level inputs such as depth maps, sketches, and color blocks.",
                "position": 156
            },
            {
                "img": "https://arxiv.org/html/2506.07177/x2.png",
                "caption": "Figure 2:Overview of Frame Guidance. The proposed Frame Guidance steers the video generation process of a VDM by applying gradient-based guidance to selected frames, resulting in a temporally coherent controlled video. Our method is training-free, model-agnostic, and supports a wide range of frame-level conditions, such as depth maps, sketches, styles, and color blocks.",
                "position": 162
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07177/x3.png",
                "caption": "Figure 3:Frame Guidance for keyframe-guided video generation task.(Left)Illustration of our method withlatent slicingand spatial down-sampling (Section4.1), and gradient propagation with L2 loss (blue arrows)Section4.3).(Right)Visualization of thevideo latent optimization(VLO;Section4.2) and the generated video frames during inference with guidance.",
                "position": 251
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07177/x4.png",
                "caption": "Figure 4:(a)GPU memory required for guidancewhen using full latent sequence, sliced latents, and latent slicing with spatial down-sampling.(b)Temporal localityof CausalVAEs from various models. Each latent (y-axis) is primarily affected by a small subset of temporally local video frames.(c)Video generation phase. We measure the L2 distance in the low-frequency region through the inference steps. Layouts are mostly determined during the first few steps.(d)Guidance influencethrough the inference steps. Yellow box indicates the latent corresponding to the guided frame.",
                "position": 290
            },
            {
                "img": "https://arxiv.org/html/2506.07177/x5.png",
                "caption": "",
                "position": 299
            },
            {
                "img": "https://arxiv.org/html/2506.07177/x6.png",
                "caption": "",
                "position": 304
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07177/x7.png",
                "caption": "Figure 5:Qualitative comparison on keyframe-guided video generation tasks. Yellow box indicates the keyframe condition. Orange box in (a) shows a disconnection in SVD-Interp. Red box in (d) visualizes a failure case for the CogX-Interp baseline for dynamic human motion.",
                "position": 506
            },
            {
                "img": "https://arxiv.org/html/2506.07177/x8.png",
                "caption": "Figure 6:Keyframe-guided generation results. (Left)Human evaluation.(Right)Quantitative results.IùêºIitalic_I,MùëÄMitalic_M, andFùêπFitalic_Fdenote initial, middle, and final frames, respectively.\n‚ÄúTrain-free‚Äù indicates whether the backbone VDM is fine-tuned for the frame interpolation task, not a base I2V model.",
                "position": 510
            },
            {
                "img": "https://arxiv.org/html/2506.07177/x9.png",
                "caption": "Figure 7:Qualitative comparison on stylized video generation.\nOurs generates high-quality videos that follow the reference style, whereas baselines fail to produce motion or show poor alignment.",
                "position": 675
            },
            {
                "img": "https://arxiv.org/html/2506.07177/x10.png",
                "caption": "Figure 8:Stylized video generation results.(Left)Human evaluation.(Right)Quantitative results.",
                "position": 680
            },
            {
                "img": "https://arxiv.org/html/2506.07177/x11.png",
                "caption": "Figure 9:Examples of other applications. (a) Object movement guided by masked region. (b) Video style transfer with SDEdit[27]. (c) Guidance using multiple types of inputs: depth map and sketch.",
                "position": 833
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ABackgrounds",
        "images": []
    },
    {
        "header": "Appendix BExperimental details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07177/x12.png",
                "caption": "Figure 10:A screenshot of questionnaires from our human evaluation on keyframe-guided generation.",
                "position": 1626
            },
            {
                "img": "https://arxiv.org/html/2506.07177/x13.png",
                "caption": "Figure 11:Human evaluation results on keyframe-guided generation including Wan-I2V.",
                "position": 1629
            },
            {
                "img": "https://arxiv.org/html/2506.07177/x14.png",
                "caption": "Figure 12:A screenshot of questionnaires from our human evaluation on stylized video generation task.",
                "position": 1658
            },
            {
                "img": "https://arxiv.org/html/2506.07177/x15.png",
                "caption": "Figure 13:Human evaluation results on stylized video generation including overall preference.",
                "position": 1661
            },
            {
                "img": "https://arxiv.org/html/2506.07177/extracted/6523324/figure/style_references/01_line_1.jpeg",
                "caption": "Table 2:Text prompts[26]used for stylized video generation.",
                "position": 1713
            },
            {
                "img": "https://arxiv.org/html/2506.07177/extracted/6523324/figure/style_references/01_line_1.jpeg",
                "caption": "Table 3:Style references and style prompts[26]used for stylized video generation.",
                "position": 1794
            },
            {
                "img": "https://arxiv.org/html/2506.07177/extracted/6523324/figure/style_references/02_digital_art_3.png",
                "caption": "",
                "position": 1831
            },
            {
                "img": "https://arxiv.org/html/2506.07177/extracted/6523324/figure/style_references/0002.png",
                "caption": "",
                "position": 1842
            },
            {
                "img": "https://arxiv.org/html/2506.07177/extracted/6523324/figure/style_references/anime_2.jpeg",
                "caption": "",
                "position": 1851
            },
            {
                "img": "https://arxiv.org/html/2506.07177/extracted/6523324/figure/style_references/05_oil_paint_2.jpg",
                "caption": "",
                "position": 1862
            },
            {
                "img": "https://arxiv.org/html/2506.07177/extracted/6523324/figure/style_references/06_digital_art_2.jpeg",
                "caption": "",
                "position": 1871
            },
            {
                "img": "https://arxiv.org/html/2506.07177/x16.png",
                "caption": "Figure 14:Qualitative comparisonof keyframe-guided video generation. Orange arrows indicate temporally disconnected frames, and red boxes highlight poor keyframe similarity. Our method generates temporally coherent videos while maintaining semantic similarity to the keyframes.",
                "position": 1901
            },
            {
                "img": "https://arxiv.org/html/2506.07177/x17.png",
                "caption": "Figure 15:Stylized video generated by Frame Guidance using style loss. These videos are generated by CogVideoX-T2V.",
                "position": 1904
            },
            {
                "img": "https://arxiv.org/html/2506.07177/x18.png",
                "caption": "Figure 16:Loop video generated by Frame Guidance using loop loss. These videos are generated by Wan-14B T2V.",
                "position": 1907
            },
            {
                "img": "https://arxiv.org/html/2506.07177/x19.png",
                "caption": "Figure 17:Frame Guidance with a color block image allows the generation of a video with a complex scene. These videos are generated by CogVideoX-I2V.",
                "position": 1910
            },
            {
                "img": "https://arxiv.org/html/2506.07177/x20.png",
                "caption": "Figure 18:Frame Guidance is model-agnostic. It is compatible with both SVD[3]and LTX-2B[15]. For SVD, since it does not use a temporally compressed VAE, we skip latent slicing. Some saturation observed in the LTX-2B results occasionally occurs due to the model itself.",
                "position": 1913
            }
        ]
    },
    {
        "header": "Appendix CMore discussions",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07177/x21.png",
                "caption": "Figure 19:Video reconstruction with temporally sliced latent.(a)Decoding the full latent sequence successfully reconstructs the original video.(b)‚Äì(c)Using 4 or 3-length latent around the target latent (frame) is sufficient for accurate reconstruction.(d)With only 2-length latent, there is slight degradation, therefore, we adpot 3-length latent for the main experiments.",
                "position": 1922
            },
            {
                "img": "https://arxiv.org/html/2506.07177/x22.png",
                "caption": "Figure 20:Shortcut-based approaches[17,35,28]lead to temporal disconnects in video generation.",
                "position": 2119
            }
        ]
    },
    {
        "header": "Appendix DLimitations and societal impacts",
        "images": []
    }
]