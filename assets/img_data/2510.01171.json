[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01171/x1.png",
                "caption": "Figure 1:We show thattypicality biasin preference data is a fundamental and pervasive cause ofmode collapse, reducing output diversity. As a solution, we proposeVerbalized Sampling (VS), a principled prompting method that returns distributions of responses, to improve diversity.",
                "position": 307
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01171/x2.png",
                "caption": "Figure 3:Qualitative and quantitative examples on different tasks. Forstory writing, VS improves the output diversity. For thedonation dialogue simulationtask, VS simulates a donation amount distribution much closer to the human distribution, and generates more realistic persuasion behaviors (e.g., resistances and change of minds, see Table14). On the task ofenumerative open-ended QA, we ask the model to “generate US states”. We first query a pretraining corpus (RedPajama) to establish a “reference” distribution of US state names in the pretraining data. The verbalized probability distribution generated by VS, when averaged over 10 trials, closely aligns with this reference pretraining distribution (KL=0.12). In contrast, direct prompting collapses into a few modes, repeatedly outputting states like California and Texas. See §G.9for more detail.",
                "position": 372
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Typicality Bias Causes Mode Collapse",
        "images": []
    },
    {
        "header": "4Method: Verbalized Sampling",
        "images": []
    },
    {
        "header": "5Creative Writing",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01171/x3.png",
                "caption": "Figure 4:a-c: Average semantic diversity scores(%) in poem (a), story (b) and joke (c) across methods and models. Our methods consistently outperform the baselines. We performed a one-tailed t-test between VS-Standard and the baselines (*p<0.05p<0.05, **p<0.01p<0.01, ***p<0.001p<0.001).d: Diversity vs. Quality trade-offfor the poem task, where VS-Multi and VS-CoT approach the Pareto front.e-f: Emergent Trendwhere larger models benefit more from VS. We show differences in diversity(e)and quality(f)over Direct across small (GPT-4.1-Mini, Gemini-2.5-Flash) and large (GPT-4.1, Gemini-2.5-Pro) models.g-i: Tunable Diversityshows the diversity tuning results on Gemini-2.5-Flash across tasks. Unlike baseline methods in dashed lines, we can tune the diversity level with VS: as the probability threshold decreases, diversity increases.",
                "position": 668
            },
            {
                "img": "https://arxiv.org/html/2510.01171/x4.png",
                "caption": "Figure 5:Image diversity using captions generated by different methods.We use different methods to generate descriptive captions given the topic, and then visualize these captions with images. Direct Prompting(top row)consistently converges on captions that will produce photorealistic images within a narrow range of scenarios, typically landscapes like deserts. In contrast, our Verbalized Sampling method(bottom row)produces captions with higher diversity in both artistic style and narrative setting. It produces images such as a watercolor under a storybook sky, a retrofuturist scene in a neon desert, and a baroque oil painting under storm clouds.",
                "position": 773
            },
            {
                "img": "https://arxiv.org/html/2510.01171/x5.png",
                "caption": "Figure 6:Ablation study on temperature for poem generation across GPT-4.1 and Gemini-2.5-Flash models.We setk=5k=5across experiments. Each plot shows the diversity-quality trade-off for three methods (Direct, Sequence, VS-Standard) at different temperature values (tt). VS-Standard can be combined with temperature to further improve the trade-off, consistently outperforming baselines across both models.",
                "position": 836
            },
            {
                "img": "https://arxiv.org/html/2510.01171/x6.png",
                "caption": "Figure 7:Diversity scores across post-training stages of Tulu-70B.“Tulu-Final-70B” is the model after RLVR. The red dashed line indicates the base model’s diversity level (45.4%). Baseline\nprompting methods experience major diversity drops (mode collapse) after SFT and DPO,\nwith direct prompting showing the most severe drop. In contrast, VS maintains a higher\ndiversity scores throughout all training stages, demonstrating that it can mitigatemode collapse.",
                "position": 850
            }
        ]
    },
    {
        "header": "6Dialogue Simulation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01171/x7.png",
                "caption": "Figure 8:VS performance in Persuasive Dialogue Simulation.(a) Donation Amount Distributionssimulated by small, large, and reasoning models with direct and VS, compared against fine-tuned model (green) and human (blue).\nWe see that VS simulates donation distributions more similar to human, especially for the larger and reasoning-focused models.(b) Linguistic Alignmenton Distinct-1/2/3, semantic diversity, and readability. Black dashed lines denote human levels; closer values indicate better stylistic match.\nVS achieves higher diversity than the direct prompting, approaching human levels. But the readability score remains higher, suggesting room for improvement.",
                "position": 901
            }
        ]
    },
    {
        "header": "7Open-Ended QA",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01171/x8.png",
                "caption": "Figure 9:Results on theOpen-Ended QAtask averaged across models. We perform one-tailed t-test between VS-Standard and baselines (*p<0.05p<0.05, **p<0.01p<0.01, ***p<0.001p<0.001).(a)shows the average KL divergence between the response distribution and the corresponding pretraining distribution. VS achieves lower KL divergence compared to baseline methods, indicating closer alignment with the pretraining distribution.(b)shows the average Coverage-N across all models. This means VS can generate a broader range of correct answers than the baselines.(c)shows the average precision across all models. VS methods maintain answer quality comparable to baseline approaches.",
                "position": 966
            }
        ]
    },
    {
        "header": "8Synthetic Data Generation",
        "images": []
    },
    {
        "header": "9Conclusion",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AContribution Statement",
        "images": []
    },
    {
        "header": "Appendix BLimitations",
        "images": []
    },
    {
        "header": "Appendix CFuture Directions",
        "images": []
    },
    {
        "header": "Appendix DUse of Large Language Models",
        "images": []
    },
    {
        "header": "Appendix ETypicality Bias Causes Mode Collapse",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01171/x9.png",
                "caption": "Figure 10:Typicality bias rate across different preference datasets and base models.Typicality bias rate measures how often the human-preferred response in a preference pair is assigned a higher likelihood by a base model.\nAll models show a systematic, above-chance bias (agreement >50%), with larger models generally exhibiting a stronger effect.\nWe also show the 95% confidence intervals.\nThe consistent above-chance preference shows that there exists atypicality biasesin human preference data.",
                "position": 3080
            }
        ]
    },
    {
        "header": "Appendix FQualitative Examples",
        "images": []
    },
    {
        "header": "Appendix GDetailed Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01171/x10.png",
                "caption": "Figure 11:Semantic diversity (%) and quality scores on thePoem Continuationtask averaged across models (higher is better). We perform one-tailed t-test between VS-Standard and baselines (*p<0.05p<0.05, **p<0.01p<0.01, ***p<0.001p<0.001). This figure shows that VS and its variants improve diversity while achieving comparable quality.",
                "position": 4579
            },
            {
                "img": "https://arxiv.org/html/2510.01171/x10.png",
                "caption": "",
                "position": 4582
            },
            {
                "img": "https://arxiv.org/html/2510.01171/x11.png",
                "caption": "",
                "position": 4586
            },
            {
                "img": "https://arxiv.org/html/2510.01171/x12.png",
                "caption": "Figure 12:Semantic diversity (%) and quality scores on theStory Generationtask averaged across models. We perform one-tailed t-test between VS-Standard and baselines (*p<0.05p<0.05, **p<0.01p<0.01, ***p<0.001p<0.001). VS and its variants also improve diversity while achieving comparable quality for story generation.",
                "position": 5333
            },
            {
                "img": "https://arxiv.org/html/2510.01171/x12.png",
                "caption": "",
                "position": 5336
            },
            {
                "img": "https://arxiv.org/html/2510.01171/x13.png",
                "caption": "",
                "position": 5340
            },
            {
                "img": "https://arxiv.org/html/2510.01171/x14.png",
                "caption": "Figure 13:Semantic diversity (%) and quality scores on theJoke Writintask averaged across models (higher is better). We perform one-tailed t-test between VS-Standard and baselines (*p<0.05p<0.05, **p<0.01p<0.01, ***p<0.001p<0.001). This figure shows that VS and its variants improve diversity while comparable quality.",
                "position": 6089
            },
            {
                "img": "https://arxiv.org/html/2510.01171/x14.png",
                "caption": "",
                "position": 6092
            },
            {
                "img": "https://arxiv.org/html/2510.01171/x15.png",
                "caption": "",
                "position": 6096
            },
            {
                "img": "https://arxiv.org/html/2510.01171/x16.png",
                "caption": "Figure 14:Example interfaces of the Prolific human study for poem (top) and story (bottom).",
                "position": 7006
            },
            {
                "img": "https://arxiv.org/html/2510.01171/x17.png",
                "caption": "",
                "position": 7010
            },
            {
                "img": "https://arxiv.org/html/2510.01171/x18.png",
                "caption": "Table 24:Average KL divergence across models for each method in the dice roll experiment. The best result is inblue; the second-best isgreen.",
                "position": 8165
            },
            {
                "img": "https://arxiv.org/html/2510.01171/x18.png",
                "caption": "Figure 15:Dice roll distributions from direct, sequence, and verbalized sampling prompting with Gemini-2.5-Pro. The red dashed line marks the expected uniform distribution: VS aligns most closely, sequence follows, while direct prompting collapses to a single mode (e.g., 4).",
                "position": 8209
            },
            {
                "img": "https://arxiv.org/html/2510.01171/x19.png",
                "caption": "Figure 16:Average diversity and quality results with GPT-4.1 on thenegative synthetic data generationtask.(a)and(b)shows incorrect answer rate and coverage (both are the higher the better), with VS-Standard outperforming all baselines and VS-CoT achieving the best results.(c)and(d)shows average semantic diversity across prompting methods and semantic similarity for synthetic negative solutions across 50 GSM8K questions. Lower similarity indicates greater semantic diversity.",
                "position": 8596
            },
            {
                "img": "https://arxiv.org/html/2510.01171/x20.png",
                "caption": "(a)Claude-4-Sonnet",
                "position": 9364
            },
            {
                "img": "https://arxiv.org/html/2510.01171/x20.png",
                "caption": "(a)Claude-4-Sonnet",
                "position": 9367
            },
            {
                "img": "https://arxiv.org/html/2510.01171/x21.png",
                "caption": "(b)GPT-4.1",
                "position": 9373
            }
        ]
    },
    {
        "header": "Appendix HAblation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01171/x22.png",
                "caption": "Figure 18:Analysis of the number of candidates (kk) for poem generation across GPT-4.1 and Gemini-2.5-Flash.Each plot illustrates the diversity-quality trade-off askkis varied from 1 to 20. Increasingkkgenerally improves diversity but lowers quality. VS-Standard consistently provides the best trade-off compared to the two baseline, approaching the Pareto front.",
                "position": 9391
            },
            {
                "img": "https://arxiv.org/html/2510.01171/x23.png",
                "caption": "Figure 19:Top-p sampling analysis for poem generation across GPT-4.1 and Gemini-2.5-Flash.The plots show the quality-diversity trade-off for varyingppvalues. VS-Standard demonstrates a superior performance, with an optimal balance often found atp=0.95p=0.95. The inset provides a zoomed-in view of each method’s performance curve.",
                "position": 9406
            },
            {
                "img": "https://arxiv.org/html/2510.01171/x24.png",
                "caption": "Figure 20:Min-p sampling analysis for poem generation across Qwen3-235B and Llama-3.1-70B-Instruct.The plots show the quality-diversity trade-off for varying min-p values. Increasing min-p enhances diversity while reducing quality. VS-Standard outperforms the baselines, establishing a much more favorable Pareto front on both open-source models.",
                "position": 9415
            },
            {
                "img": "https://arxiv.org/html/2510.01171/figures/creative_writing/verbalized_sampling_prob_format.png",
                "caption": "Figure 21:Ablation of probability formats for Verbalized Sampling on the Poem Continuation Task.We evaluate VS-Standard (blue) and VS-Multi (red) on two models across two metrics:(a, c)Diversity (↑\\uparrow) and(b, d)Quality (↑\\uparrow). Subplotsa–breport results on GPT-4.1, whilec-dshow results on Gemini 2.5 Flash. Prompt formats include Implicit, Explicit, Relative, Percentage, Confidence, NLL, and Perplexity.",
                "position": 9477
            },
            {
                "img": "https://arxiv.org/html/2510.01171/x25.png",
                "caption": "Figure 22:Ablation of probability formats for Verbalized Sampling on the Open-Ended QA Task.We evaluate VS-Standard (blue) and VS-Multi (red) on two models across three metrics:(a, d)KL Divergence (↓),(b, e)Coverage-N (↑), and(c, f)Precision (↑). Subplotsa–creport results on GPT-4.1, whiled–fshow results on Gemini 2.5 Flash.",
                "position": 9481
            },
            {
                "img": "https://arxiv.org/html/2510.01171/x26.png",
                "caption": "Figure 23:Diversity tuning results for Poem Continuation Task.Comparison of diversity scores across probability\ntuning parameters for GPT-4.1 (left) and Gemini 2.5 Flash\n(right). Notably, whileVS-Multiinitially falls behindVS-Standardat higher probability thresholds, its diversity improves more with diversity tuning. As the threshold decreases,VS-Multi’s diversity score catches up to that for GPT-4.1 (left) or even surpassesVS-Standardfor Gemini-2.5-Flash (right), demonstrating the effectiveness of the tuning process. We attribute this trend to a reduced cognitive burden, which allowsVS-Multito generate more diverse results with greater capability. BothVS-StandardandVS-Multimaintain a consistent performance advantage over theDirectandSequencebaselines, confirming that probability tuning provides effective diversity control across different models.",
                "position": 9520
            },
            {
                "img": "https://arxiv.org/html/2510.01171/x27.png",
                "caption": "Figure 24:Diversity tuning results for Story Generation.Comparison of diversity scores across probability\ntuning parameters for GPT-4.1 (left) and Gemini 2.5 Flash\n(right). The continuous y-axis shows the full range of\ndiversity values. VS-Standard and VS-Multi maintain consistent\nperformance advantages over baselines while exhibiting\ncomplementary tuning behaviors. The results demonstrate that\ndiversity tuning provides diversity control\nacross different models, with optimal parameter\nranges varying based on the specific creative task.",
                "position": 9528
            },
            {
                "img": "https://arxiv.org/html/2510.01171/x28.png",
                "caption": "Figure 25:Diversity tuning results for Joke Writing.Comparison of diversity scores across probability\ntuning parameters for GPT-4.1 (left) and Gemini 2.5 Flash\n(right). The x-axis shows probability thresholds in descending\norder from 1.0 to 0.001. VS-Standard and VS-Multi consistently\noutperform Direct and Sequence baselines across all parameter\nsettings. Both VS\nvariants show controllable diversity curves,\nwith VS-Standard achieving slightly higher peak diversity values.",
                "position": 9542
            },
            {
                "img": "https://arxiv.org/html/2510.01171/x29.png",
                "caption": "Figure 26:Diversity tuning results for Open-Ended QA on Coverage-N.Results are shown for GPT-4.1 (left) and Gemini-2.5-Flash (right) across probability tuning parameters. Coverage-N measures the proportion of ground truth covered in the response distribution (higher is better). Both VS-Standard and VS-Multi consistently outperform the sequence baseline, with coverage increasing as probability decreases until≤0.1\\leq 0.1, where the distribution becomes heavily tailed.",
                "position": 9590
            },
            {
                "img": "https://arxiv.org/html/2510.01171/x30.png",
                "caption": "Figure 27:Diversity tuning results for Open-Ended QA on KL Divergence over uniform distribution.Results are shown for GPT-4.1 (left) and Gemini-2.5-Flash (right) across probability tuning parameters. VS-Standard and VS-Multi achieve consistently lower divergence than the sequence baseline. The overall trend shows decreasing KL Divergence as probability decreases, indicating closer alignment with uniform distribution.",
                "position": 9594
            },
            {
                "img": "https://arxiv.org/html/2510.01171/x31.png",
                "caption": "Figure 28:Diversity tuning results for Open-Ended QA on Precision.Results are shown for GPT-4.1 (left) and Gemini-2.5-Flash (right) across probability tuning parameters.",
                "position": 9598
            }
        ]
    },
    {
        "header": "Appendix IExperimental Details",
        "images": []
    }
]