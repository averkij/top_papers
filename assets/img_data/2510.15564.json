[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15564/x1.png",
                "caption": "Figure 1.Some high-quality 3D scene layouts generated by our vision-guided system not only exhibit strong performance in indoor environments but can also be extended to outdoor scenes. The complete text prompts are provided in AppendixA.2.1.",
                "position": 245
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": []
    },
    {
        "header": "2.Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15564/x2.png",
                "caption": "Figure 2.Overview of our method. We first transforms a text prompt into a detailed 2D guide image using a fine-tuned model, ensuring stylistic consistency with our asset library. This image is then analyzed for semantic, geometric, and relational information, guiding the retrieval, transformation estimation, and optimization of 3D assets into the final, coherent layout. See AppendixA.1.7for additional visualizations of intermediate steps.",
                "position": 363
            }
        ]
    },
    {
        "header": "3.Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15564/x3.png",
                "caption": "Figure 3.Overview of our high-quality 3D scene layout dataset: (a) Representative 3D scenes with interior layouts. (b) Diverse 3D assets from our collection. (c) Structured metadata schema for scenes and assets. (d) Comparison with 3D-Future, highlighting our dataset’s superior variety and complexity.",
                "position": 406
            },
            {
                "img": "https://arxiv.org/html/2510.15564/figs/scene_graph.png",
                "caption": "Figure 4.(a) Scene graph constraints extracted by our algorithm. (b) Close-up of the support relationship tree structure (highlighted in red box in (a)).",
                "position": 450
            },
            {
                "img": "https://arxiv.org/html/2510.15564/x4.png",
                "caption": "Figure 5.Coarse-to-fine view selection. Coarse selection ranks views by keypoint match quality, while fine selection uses homography transformation to identify the most viewpoint-similar match (selectingv1visv_{1}^{\\text{vis}}in this example).",
                "position": 507
            },
            {
                "img": "https://arxiv.org/html/2510.15564/x5.png",
                "caption": "Figure 6.Top-view illustration of candidates’ geometric enhancement. Each row compares orientation estimations for different query scenarios, showing ground truth (GT), OBB-based (v∗obbv_{\\ast}^{\\text{obb}}), and vision-based (v1visv_{1}^{\\text{vis}}) estimations. The best estimation (vbestv_{\\text{best}}) is highlighted, demonstrating the adaptive integration of geometric guidance.",
                "position": 526
            },
            {
                "img": "https://arxiv.org/html/2510.15564/x6.png",
                "caption": "Figure 7.Internal placement logic illustration. Left: query object (yellow outline). Right: internal subspaces of the target container. Objects are placed in the nearest subspace based on the vertical distancedverticald^{\\text{vertical}}between the centers of the object and the subspace along the gravity direction(G).",
                "position": 556
            },
            {
                "img": "https://arxiv.org/html/2510.15564/x7.png",
                "caption": "Figure 8.Comparison of our generated 3D scene layouts with other state-of-the-art methods, illustrating the richness of our 3D generated layouts. More examples of our generated layouts are shown in AppendixA.5.",
                "position": 623
            }
        ]
    },
    {
        "header": "4.Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15564/figs/pose_exp.png",
                "caption": "Figure 9.Comparison of performance in category and instance level rotation estimation with other methods.",
                "position": 893
            },
            {
                "img": "https://arxiv.org/html/2510.15564/x8.png",
                "caption": "Figure 10.Comparison between Finetuned Flux and Vanilla Flux generated images. Given identical prompts (left column), Finetuned Flux (second column) generates images with objects that more closely resemble assets in our 3D library (third column), compared to Vanilla Flux (right column). This alignment improves retrieval accuracy and pose estimation, enabling more precise scene parsing and strengthening system robustness.",
                "position": 903
            },
            {
                "img": "https://arxiv.org/html/2510.15564/x9.png",
                "caption": "Figure 11.Additional results showcasing our method’s ability to generate coherent 3D layouts from diverse guide images. The generated scenes (bottom) demonstrate high fidelity to the input’s spatial arrangement and style.",
                "position": 1011
            },
            {
                "img": "https://arxiv.org/html/2510.15564/x10.png",
                "caption": "Figure 12.It showcases some re-editing examples that we generated using the Image Generation model. Using the text prompts from the second column, we re-paint the local information within the red box of the images in the first column using Flux, thereby controlling the 3D layout. This control over local information can achieve a very robust effect.",
                "position": 1115
            }
        ]
    },
    {
        "header": "5.Application",
        "images": []
    },
    {
        "header": "6.Conclusion and Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15564/x11.png",
                "caption": "Figure 13.Example scene visualizations with 3D layout and 2D visual similarity comparisons. The first column shows generated scene images. The second column displays their corresponding grid-based layout visualizations, with each color representing a different furniture category. The third column presents the nearest neighbors based on 3D layout similarity, and the fourth column shows the nearest visual (2D) neighbors from the training set.",
                "position": 3256
            },
            {
                "img": "https://arxiv.org/html/2510.15564/x12.png",
                "caption": "Figure 14.Additional visualization of key intermediate steps. The process begins with a comprehensive scene analysis where (a) objects are detected via grounding-dino-1.5 and SAM, guided by categories parsed by GPT-4o, and (b) segmentation masks are generated. Concurrently, (c) RANSAC is employed to fit orthogonal floor and wall planes (ceiling as floor’s opposite normal), establishing a robust geometric frame for the scene. For each segmented object, we (d) retrieve the top-2 candidate assets from our library based on semantic category, visual similarity, and size compatibility. Our rotation estimation module then combines (e) a strong initial candidate from visual-semantic feature matching with (f) constraints from Oriented Bounding Boxes (OBBs), which are geometrically corrected using scene graph logic. This fusion results in (g) the final pose, a high-quality input for the subsequent scene layout refinement stage.",
                "position": 3267
            },
            {
                "img": "https://arxiv.org/html/2510.15564/x13.png",
                "caption": "Figure 15.More 3D scenes from our high-quality, handcrafted dataset. These scenes showcase a diverse range of room functions and include both indoor and outdoor assets, illustrating the variety and detail of our manual scene construction.",
                "position": 3490
            },
            {
                "img": "https://arxiv.org/html/2510.15564/x14.png",
                "caption": "Figure 16.Dataset asset overview. Left: examples of asset classes such as backrest chairs and TV cabinets. Right: bar chart showing the number of assets per class, highlighting the most common categories.",
                "position": 3622
            },
            {
                "img": "https://arxiv.org/html/2510.15564/x15.png",
                "caption": "Figure 17.Statistics of our high-quality, manually arranged dataset, encompassing 21 different scene types. The chart illustrates the number of scenes, total objects, average objects per scene, and class count for each scene type, highlighting the dataset’s diversity and complexity.",
                "position": 3625
            },
            {
                "img": "https://arxiv.org/html/2510.15564/x16.png",
                "caption": "Figure 18.Analysis of Failure Cases. This figure illustrates two primary limitations of our method. Top Row: Discrepancy between Generated Content and Asset Library. (a) The image generator creates an object with a novel topology—a hybrid of a wardrobe and a bookshelf. (b) Our system retrieves the closest semantic match from the asset library, a standard wardrobe, which lacks the open shelves depicted. This semantic-structural mismatch prevents the correct placement of child objects (e.g., books), leading to layout inconsistencies. Bottom Row: Pose Estimation Ambiguity from Severe Occlusion. (c) An object, correctly identified as a chair, is heavily occluded, revealing only its backrest. (d) While feature matching can be performed on this partial view, the limited information introduces ambiguity, as multiple poses could yield a similar appearance, potentially leading to inaccurate rotation estimation.",
                "position": 3635
            },
            {
                "img": "https://arxiv.org/html/2510.15564/x17.png",
                "caption": "Figure 19.Additional 3D generated scene layouts by our system.",
                "position": 3647
            }
        ]
    },
    {
        "header": "Appendix ASupplementary Materials",
        "images": []
    }
]