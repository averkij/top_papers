[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02287/x1.png",
                "caption": "Figure 1:Example opening messages in each language from the generated dialogues. In English, it reads ‘Good day! You have spoken to Klaus Customer Support, Martin here. How can I help you today?’.",
                "position": 106
            }
        ]
    },
    {
        "header": "2Methods",
        "images": []
    },
    {
        "header": "3Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02287/x2.png",
                "caption": "Figure 2:Cross-language ranking stability measured by Kendall’sτ\\tau. Error bars show 95% bootstrap confidence intervals. Numbers below bars indicate rank inversions (out of 15 possible pairwise inversions among 6 models); asterisks denote statistical significance via permutation test (*p<0.05p<0.05). Surface-level metrics (Grammar, Readability, Fluency) maintain high stability (τ≥0.62\\tau\\geq 0.62) with minimal inversions. Pragmatic dimensions show systematic breakdown: Coherence exhibits near-zero or negative correlations, and LRA shows significant rank scrambling across all Finno-Ugric pairs (9*, 6*, 7* inversions). English pairs included for context, though ceiling effects limit their informativeness for Coherence.",
                "position": 383
            }
        ]
    },
    {
        "header": "4Discussion and Outlook",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAutomatic Metrics",
        "images": []
    },
    {
        "header": "Appendix BDialogue Generation",
        "images": []
    },
    {
        "header": "Appendix CHuman Labeling",
        "images": []
    },
    {
        "header": "Appendix DLLM As A Judge",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02287/x3.png",
                "caption": "Figure 3:Label recovery accuracy (LRA) across categories by model for sampled dialogues in Estonian, Finnish, Hungarian, and English. Performance varies substantially by category complexity: simple binary parameters (Agent Experience, Agent Type) show consistent accuracy across all languages, while complex semantic categories (Industry: 40+ options, Problem: 20+ types) exhibit poor and inconsistent performance in all languages including English. This pattern suggests that complex parameter recovery may exceed current model capabilities regardless of target language, limiting LRA’s utility as a cross-linguistic diagnostic. Unlike surface metrics and coherence assessment, where clear stability differences emerge, LRA instability appears task-dependent rather than language-dependent.",
                "position": 1333
            }
        ]
    },
    {
        "header": "Appendix ECross-language ranking stability",
        "images": []
    },
    {
        "header": "Appendix FMeta-Prompt Sensitivity",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02287/x4.png",
                "caption": "Figure 4:Comparison between different LLM judges over Finnish dialogues across LLM-as-a-judge metrics.",
                "position": 2240
            }
        ]
    },
    {
        "header": "Appendix GAppendix: Judge Model Ablation Study",
        "images": []
    }
]