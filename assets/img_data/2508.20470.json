[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20470/x1.png",
                "caption": "Figure 1:Droplet3Dachieves creative 3D content generation based on both image and text input.Commonsense priors including spatial consistency and semantic knowledge facilitate the 3D generation abilities of our method.",
                "position": 126
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3TheDroplet3D-4MDataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20470/x2.png",
                "caption": "Figure 2:A sample fromDroplet3D-4Mcomprises a 85-frame multi-view rendered video and a fine-grained, multi-view-level text annotation.Theblue,red, andgreentext illustrate viewpoint and appearance changes at three consecutive time steps, with corresponding left-side color markers indicating the specific moments of transformation.",
                "position": 273
            },
            {
                "img": "https://arxiv.org/html/2508.20470/x3.png",
                "caption": "Figure 3:The pipeline we proposed to curate theDroplet3D-4Mdataset.The proposed pipeline encompasses a three key stages: multi-granularity rendering, filtering, and caption generating.",
                "position": 528
            },
            {
                "img": "https://arxiv.org/html/2508.20470/x4.png",
                "caption": "Figure 4:An example of reinforcement learning samples for GRPO.We constructed a constrained model to annotate thethinkandanswerprocesses. For the think process, we provided annotations on themes, materials, functions, details, OCR, etc. For the answer process, we identified two types of scoring points based on dense multi-view level captions: scoring points marked inbluedenote those aligned with the Think process, while those marked inredindicate detailed descriptions of perspective changes.",
                "position": 584
            },
            {
                "img": "https://arxiv.org/html/2508.20470/x5.png",
                "caption": "Figure 5:An caption illustration fromDroplet3D-4M.Each 3D model is ultimately rendered into an encompassing multi-view video comprising 85 frames with uniformly distributed angles. The caption typically consists of two paragraphs: the first describes the overall style and key details of the object, while the second provides a multi-view-level description, emphasizing the appearance variations induced by the circular capture. Expressions related to viewpoint changes are highlighted in different colors.",
                "position": 603
            },
            {
                "img": "https://arxiv.org/html/2508.20470/x6.png",
                "caption": "Figure 6:The aesthetics distribution and the image quality distribution ofDroplet3D-4M.These distributions demonstrate that our dataset achieveshigh scoresin both aesthetics and image quality, indicating an overallhigh-quality standardfor the dataset.",
                "position": 669
            }
        ]
    },
    {
        "header": "4The Droplet3D Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20470/x7.png",
                "caption": "Figure 7:Droplet3DFramework: Inheriting spatial-semantic priors from massive videos for high-fidelity 3D generation.Droplet3Demploys DropletVideo as its video backbone, effectively leveraging its commonsense priors of spatial consistency and semantic knowledge. This enables a novel 3D object generation paradigm conditioned on the joint input of an initial image and dense text, while achieving superior generalization performance.",
                "position": 681
            },
            {
                "img": "https://arxiv.org/html/2508.20470/x8.png",
                "caption": "Figure 8:Overview of theDroplet3DTechniques.To enhance performance,Droplet3Dsupports a alignment module to transfer the user input to fit the model. Subsequently, the aligned text and image features are fed into the backbone network to generate multi-view images with 3D consistency. At the end, the created multi-view content can be integrated into reconstruction modules for various 3D modalities, thereby generating production-ready 3D assets.",
                "position": 701
            },
            {
                "img": "https://arxiv.org/html/2508.20470/x9.png",
                "caption": "Figure 9:The surrounding multi-view video generation backbone ofDroplet3D. It consists of two components: a 3D causal VAE and a vision-text modality-expert Transformer architecture.",
                "position": 711
            },
            {
                "img": "https://arxiv.org/html/2508.20470/x10.png",
                "caption": "Figure 10:An illustration of rewritten samples.Text describing viewpoint changes is highlighted inred.Greentext indicates how user-provided brief or detailed descriptions have been rewritten into a form consistent with the training samples, showcasing their appearance across different viewpoints to provide more specific design requirements.",
                "position": 761
            },
            {
                "img": "https://arxiv.org/html/2508.20470/x11.png",
                "caption": "Figure 11:Image alignment module converts users’ arbitrary inputs into canonical perspectives.It employs LoRA fine-tuning based on the FLUX.1-Kontext-dev model to rectify arbitrary input viewpoints into canonical perspectives, such as front, back, left, and right.",
                "position": 768
            }
        ]
    },
    {
        "header": "5Implementation and Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20470/x12.png",
                "caption": "Figure 12:Comparison of generated content from different generative methods that support simultaneous image and text input.The experimental cases demonstrate that the generation performance ofDroplet3Dis significantly superior to that of the two baseline methods.",
                "position": 830
            },
            {
                "img": "https://arxiv.org/html/2508.20470/x13.png",
                "caption": "Figure 13:Comparison betweenDroplet3Dand its predecessor, DropletVideo.The performance enhancement ofDroplet3Dover DropletVideo in terms of spatial consistency underscores the critical role of continued training on theDroplet3D-4Mdataset.",
                "position": 887
            },
            {
                "img": "https://arxiv.org/html/2508.20470/x14.png",
                "caption": "Figure 14:Comparison between DropletVideo and other video backbones.DropletVideo demonstrates superior performance in circumnavigation shooting tasks. Using the chimney inredcircle as an example, DropletVideo successfully generates surrounding-camera-motion videos, whereas Step and WanX only support minor rotational movements, and the similarly-sized Cogvideox-Fun lacks this capability entirely. Concurrently, DropletVideo also surpasses all competing models in preserving the original image’s style, such as its background color.",
                "position": 925
            },
            {
                "img": "https://arxiv.org/html/2508.20470/x15.png",
                "caption": "Figure 15:Controllable-Creativity based on the initial image of the Panda Astronaut case and different given texts.The three rows in the figure respectively demonstrate the generation based on the givenspace backpack,orange backpack, andenergy ball.",
                "position": 1015
            },
            {
                "img": "https://arxiv.org/html/2508.20470/x16.png",
                "caption": "Figure 16:Controllable-Creativity based on the initial image of the Castle case and different given texts.The three rows in the figure respectively demonstrate the generation based on the givenstone door,blue gate, anda garden with a red door.",
                "position": 1018
            },
            {
                "img": "https://arxiv.org/html/2508.20470/x17.png",
                "caption": "Figure 17:Controllable-Creativity based on the initial image of the Battle Axe case and different given texts.The three rows in the figure respectively demonstrate the generation based on the given symmetrical side, a purple crystal, and a green life gem.",
                "position": 1021
            },
            {
                "img": "https://arxiv.org/html/2508.20470/x18.png",
                "caption": "Figure 18:Controllable-Creativity based on the initial image of the Coin Onion case and different given texts.The three rows in the figure respectively demonstrate the generation based on the givenwind-up key,crystal orb, andQR code.",
                "position": 1024
            },
            {
                "img": "https://arxiv.org/html/2508.20470/x19.png",
                "caption": "Figure 19:Droplet3Ddemonstrates its lifting capability on 2D sketch paintings.Our model can transform objects such as characters and buildings into three-dimensional representations based on simple line drawings. From up to bottom: a sketch of a girl dressed in a minimalist style, a Japanese anime character, and a traditional Chinese architectural structure.",
                "position": 1060
            },
            {
                "img": "https://arxiv.org/html/2508.20470/x20.png",
                "caption": "Figure 20:Droplet3Ddemonstrates its lifting capability on 2D images styled as comics paintings.Our model can elevate objects within a given 2D comic into 3D, thereby achieving a cross-dimensional effect. From top to bottom: a gentle girl in a comic style, a male student with animal ears, an American comic-style superhero, and a scarecrow in the style of Hayao Miyazaki.",
                "position": 1063
            },
            {
                "img": "https://arxiv.org/html/2508.20470/x21.png",
                "caption": "Figure 21:Mesh reconstruction based on content generated byDroplet3D.Note that the models used for mesh generation and texture mapping originate from the open-source Hunyuan3D-2.0 and Hunyuan3D-2.1, respectively.",
                "position": 1083
            },
            {
                "img": "https://arxiv.org/html/2508.20470/x22.png",
                "caption": "Figure 22:3D Gaussian splatting reconstruction based on content generated byDroplet3D.We employed the native optimization-based approach for this implementation.",
                "position": 1098
            },
            {
                "img": "https://arxiv.org/html/2508.20470/x23.png",
                "caption": "Figure 23:Gaussian splatting edition based on scene-level content generated byDroplet3D.Compared to existing mainstream object-level 3D generation methods,Droplet3Dnot only generates individual objects but also demonstrates significant potential for scene-level content generation.",
                "position": 1113
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]