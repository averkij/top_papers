[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10377/figures/overview-all.png",
                "caption": "Figure 1:Hardware co-design scaling law for on-device LLMs. Architectural choices and hardware platforms jointly shape the loss-latency Pareto frontier, revealing Pareto-optimal configurations under system constraints.",
                "position": 463
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10377/figures/1_roofline.png",
                "caption": "(a)Roofline Model",
                "position": 612
            },
            {
                "img": "https://arxiv.org/html/2602.10377/figures/1_roofline.png",
                "caption": "(a)Roofline Model",
                "position": 615
            },
            {
                "img": "https://arxiv.org/html/2602.10377/figures/2_nas.png",
                "caption": "(b)Neural Architecture Search",
                "position": 621
            },
            {
                "img": "https://arxiv.org/html/2602.10377/figures/3_pareto.png",
                "caption": "(c)Pareto Frontier",
                "position": 627
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Formulating Hardware Co-Design Law for on-Device LLM",
        "images": []
    },
    {
        "header": "4Pareto-Optimal Architecture Search",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10377/figures/overview_plas.png",
                "caption": "Figure 3:Overview of Pareto-optimal LLM Architecture Search framework (PLAS). The framework integrates (1) empirical loss modeling via scaling law fitting, (2) roofline-based latency estimation, and (3) Pareto frontier construction to enable hardware-aware architecture selection.",
                "position": 915
            },
            {
                "img": "https://arxiv.org/html/2602.10377/figures/loss_fit_scatter_after.png",
                "caption": "Figure 4:Scaling law fit quality. TrainingR2=0.975R^{2}=0.975(138 configurations); validationR2=0.952R^{2}=0.952(32 held-out configurations).",
                "position": 962
            },
            {
                "img": "https://arxiv.org/html/2602.10377/figures/1_pareto_three_scenarios.png",
                "caption": "Figure 5:Pareto frontiers under prefill (1,024 tokens), decode (16 tokens), and total latency optimization on NVIDIA Jetson Orin, comparing FP16 and INT8 precision.",
                "position": 1049
            },
            {
                "img": "https://arxiv.org/html/2602.10377/figures/Pareto.jpg",
                "caption": "Figure 6:Different applications live in completely different regions of the Pareto frontier.",
                "position": 1107
            },
            {
                "img": "https://arxiv.org/html/2602.10377/figures/5_architecture_trends_combined.png",
                "caption": "Figure 7:Architecture parameter evolution along Pareto frontiers under prefill-optimized (top), decode-optimized (middle), and total latency-optimized (bottom) objectives, comparing FP16 and INT8 precision. As the latency budget increases, optimal configurations exhibit systematic shifts in depth, width, expert count, and FFN expansion ratio.",
                "position": 1114
            },
            {
                "img": "https://arxiv.org/html/2602.10377/figures/2_total_latency_loss.png",
                "caption": "(a)Pareto frontier",
                "position": 1176
            },
            {
                "img": "https://arxiv.org/html/2602.10377/figures/2_total_latency_loss.png",
                "caption": "(a)Pareto frontier",
                "position": 1179
            },
            {
                "img": "https://arxiv.org/html/2602.10377/figures/training_curves.png",
                "caption": "(b)Training dynamics",
                "position": 1185
            }
        ]
    },
    {
        "header": "5Theoretical Framework for Hardware-Aware Architecture Optimization",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AArchitecture Search Space",
        "images": []
    },
    {
        "header": "Appendix BPre-training Details",
        "images": []
    },
    {
        "header": "Appendix CScaling Law Coefficients",
        "images": []
    },
    {
        "header": "Appendix DLatency Modeling Details",
        "images": []
    },
    {
        "header": "Appendix EProblem Formulation and Roofline Analysis",
        "images": []
    },
    {
        "header": "Appendix FCase D1: Decode, Latency-Constrained",
        "images": []
    },
    {
        "header": "Appendix GCase D2: Decode, Memory-Constrained",
        "images": []
    },
    {
        "header": "Appendix HCase D3: Decode, Dual-Constrained",
        "images": []
    },
    {
        "header": "Appendix ICase P1: Prefill, Latency-Constrained",
        "images": []
    },
    {
        "header": "Appendix JCase P2: Prefill, Memory-Constrained",
        "images": []
    },
    {
        "header": "Appendix KCase P3: Prefill, Dual-Constrained",
        "images": []
    },
    {
        "header": "Appendix LSummary",
        "images": []
    }
]