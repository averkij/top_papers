[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15641/x1.png",
                "caption": "",
                "position": 119
            }
        ]
    },
    {
        "header": "IIntroduction",
        "images": []
    },
    {
        "header": "IIRelated Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15641/x2.png",
                "caption": "Figure 2:Model overview.A user instruction with a query and an accompanying video are provided as inputs.\nKey nouns extracted from the query are passed to afrozen Grounded-SAM2module to obtain object-level segmentation masks and embeddings (left).\nIn parallel, afrozen video diffusion encoderprocesses the video together with the masks and produces multi-scale features; we retain the intermediate representation at10% of the diffusion trajectoryas context tokens (right).\nThe query is further encoded by a text encoder.\nWe then assembletext tokens,segmentation tokens, andvideo tokens—including entity and timestamp tags—using ourMixed Tokens Strategy, and feed the merged sequence into a large language model (bottom).\nDuring training, Grounded-SAM2, the video diffusion backbone, and the LLM are kept frozen, while the LLM is adapted only viaLoRA.",
                "position": 203
            }
        ]
    },
    {
        "header": "IIIMethod",
        "images": []
    },
    {
        "header": "IVExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15641/x3.png",
                "caption": "Figure 3:Qualitative comparisons on three tasks.We show results onTemporal Grounding,Grounded VideoQA, andOpen-ended VideoQA.\nGiven “Video with Segmentation” and timestamp tokens, our model (OURS) produces more precise localizations (e.g.,32.0s--58.0s) and more specific, object-aware answers (e.g., “a small red airplane”) than prior methods (LLaVA-ST,VTimeLLM,TimeChat,VideoLLaMA).\nCheckmarks indicate correct predictions; crosses indicate errors.",
                "position": 997
            }
        ]
    },
    {
        "header": "VConclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]