[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18876/x1.png",
                "caption": "",
                "position": 66
            },
            {
                "img": "https://arxiv.org/html/2510.18876/x2.png",
                "caption": "",
                "position": 68
            },
            {
                "img": "https://arxiv.org/html/2510.18876/figs/logo.png",
                "caption": "",
                "position": 73
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18876/x3.png",
                "caption": "Figure 1:Performance comparison.GARachieves strong performances not only on region-level understanding, but also on general multimodal benchmarks.",
                "position": 153
            },
            {
                "img": "https://arxiv.org/html/2510.18876/x4.png",
                "caption": "Figure 2:Illustration of our GAR, which is superior atleveraging necessary global contextto (a) generateprecisecaptions, wheregreenis correct andredmeans wrong, (b) model complex interactions amongmultiple prompts, and perform reasoning such as (c) recognizing non-entities.\nColors of<Prompt0>,<Prompt1>, and<Prompt2>correspond to masks with respective colors.\nImages are sampled from[39],[23], and[30]for (a), (b), and (c), respectively.",
                "position": 170
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Grasp Any Region",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18876/x5.png",
                "caption": "Figure 3:Illustration of our GAR.It leverages a single-pass visual encoder to create a holistic feature map of the entire scene, thus preserving global context.\nSimultaneously, an “RoI-Aligned Feature Replay” mechanism extracts high-fidelity features for specific objects of interest.\nBoth the global context features and the detailed local features are then fed into an LLM to accurately infer complex relationships and interactions betweenmultiple objects.",
                "position": 270
            },
            {
                "img": "https://arxiv.org/html/2510.18876/x6.png",
                "caption": "Figure 4:Illustration of our training data pipeline, which mainly includes two rounds of captioning and judging.\nSpecifically, (1) starting from using the seed dataset to train a seed captioner, we first construct 456K fine-grained descriptions.\nSubsequently, (2) we utilize both datasets to obtain a fine-grained captioner, and leverage the annotations of the Panoptic Scene Graph (PSG) dataset[55]to provide sufficient relation-aware captions and question-answering pairs.\nFinally, our GAR models are trained with all three parts.",
                "position": 324
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18876/x7.png",
                "caption": "Figure 5:Qualitative comparisonson DLC-Bench[22], wheregreenindicates correct descriptions andredmeans errors.\nWe compare our GAR-8B with DAM-3B[22].\nThanks to the encoded global contexts, our GAR-8B produces much more accurate descriptions.",
                "position": 1620
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AOverview",
        "images": []
    },
    {
        "header": "Appendix BDetails of GAR-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18876/x8.png",
                "caption": "Figure 6:Statistics of our GAR-Bench.\nWe (a) slightly prioritize reasoning over perception, and build challenging questions through (b) multiple visual prompts (even have 2 questions with 7 prompts and 9 prompts) and (c) small areas of each prompt with an average of 4.4%.",
                "position": 2633
            },
            {
                "img": "https://arxiv.org/html/2510.18876/x9.png",
                "caption": "Figure 7:Qualitative comparisonson the “relation” protocol of ourGAR-Bench-VQA, including two failure cases (bottom).\nNotably, in the right case of the middle row, the person (<Prompt0>) is actuallynotreading the book (<Prompt1>), since she is looking at the camera.\nOurGAR-8Bmanages to recognize such details while both Gemini-2.5-Pro[10]and OpenAI-o3[33]fail.\nFrom the last two cases, we can tell that models are still struggling with understanding complex relationships withmore than two objects.\nAll images are sampled from[39].",
                "position": 2939
            },
            {
                "img": "https://arxiv.org/html/2510.18876/x10.png",
                "caption": "Figure 8:Qualitative comparisonson the “non-entity recognition” protocol of ourGAR-Bench-VQA, including two failure cases (bottom).\nFrom the last two cases, we can tell that models are sometimes still struggling with recognizing non-entities, especially distinguishing reflection from the mirror (<Prompt2>) and other surfaces (<Prompt0>and<Prompt1>).",
                "position": 2948
            }
        ]
    },
    {
        "header": "Appendix CMore Experiments",
        "images": []
    },
    {
        "header": "Appendix DQualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18876/x11.png",
                "caption": "Figure 9:Qualitative results ofdetailed video captioningon VideoRefer-BenchD{}^{\\text{D}}[61], including one failure case with a low “temporal description” score.\nVideos are sampled from[5].",
                "position": 3053
            },
            {
                "img": "https://arxiv.org/html/2510.18876/x12.png",
                "caption": "Figure 10:Qualitative results ofdetailed video understandingon VideoRefer-BenchQ{}^{\\text{Q}}[61], including one failure case in the “future prediction” protocol.",
                "position": 3059
            }
        ]
    },
    {
        "header": "Appendix ELimitation and Failure Cases",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18876/x13.png",
                "caption": "Figure 11:Incorrecttext-onlyjudging resultsusing LLaMA3.1-8B[13]on DLC-Bench[22].\nThe model is required to judge whether thedescriptionis consistent with the ground-truthcategory name.\nWe illustrate bothcorrectandwrongresults.\nProviding extra cropped images and masks to GPT-4o[31]effectively eliminates this issue.",
                "position": 3076
            }
        ]
    },
    {
        "header": "Appendix FDiscussion on DLC-Bench",
        "images": []
    },
    {
        "header": "Appendix GPrompt Templates",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18876/x14.png",
                "caption": "Figure 12:Prompt for judging the description and the ground-truth category.",
                "position": 3103
            },
            {
                "img": "https://arxiv.org/html/2510.18876/x15.png",
                "caption": "Figure 13:Prompt for generating relation-aware caption.",
                "position": 3108
            },
            {
                "img": "https://arxiv.org/html/2510.18876/x16.png",
                "caption": "Figure 14:Prompt for generating question-answering pairs.",
                "position": 3112
            },
            {
                "img": "https://arxiv.org/html/2510.18876/x17.png",
                "caption": "Figure 15:Prompt for generating multiple-choice questions.",
                "position": 3117
            }
        ]
    },
    {
        "header": "Appendix HUse of LLMs",
        "images": []
    }
]