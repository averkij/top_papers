[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.16802/x1.png",
                "caption": "Figure 1:Overview of our study on reference-guided LLM-as-a-Judge for LLM alignment. Conceptual plots illustrating (I) the improvement in average accuracy from reference-guided evaluation (ยง3.3) and (II) the reference-guided self-improvement (ยง4).",
                "position": 111
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Developing Reference-Guided LLM-Judges",
        "images": []
    },
    {
        "header": "4Reference-Guided Self-Improvement",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.16802/x2.png",
                "caption": "Figure 3:Comparison of reference-free and reference-guided self-improvement across task categories on AlpacaEval and Arena-Hard.",
                "position": 859
            },
            {
                "img": "https://arxiv.org/html/2602.16802/x3.png",
                "caption": "",
                "position": 862
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AFull Experimental Results for Evaluation Protocols",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.16802/x4.png",
                "caption": "Figure 4:Aggregate performance by dataset for Larger Models (>>9B parameters, including GPT-4o variants; top panel) and Smaller Models (9B parameters; bottom panel).RefEvaldemonstrates consistent improvements across most datasets for both model groups.",
                "position": 2207
            },
            {
                "img": "https://arxiv.org/html/2602.16802/x5.png",
                "caption": "",
                "position": 2211
            },
            {
                "img": "https://arxiv.org/html/2602.16802/x6.png",
                "caption": "",
                "position": 2213
            },
            {
                "img": "https://arxiv.org/html/2602.16802/x7.png",
                "caption": "Figure 5:Evaluation accuracy of 11 open-source LLM-judges usingRefEvalandRefMatchwith single references from various frontier models, and their voted versions. Horizontal dashed lines indicate reference-free baselines. Results are averaged over five datasets.",
                "position": 2609
            },
            {
                "img": "https://arxiv.org/html/2602.16802/x8.png",
                "caption": "",
                "position": 2613
            }
        ]
    },
    {
        "header": "Appendix BAdditional Results on Pointwise Scoring",
        "images": []
    },
    {
        "header": "Appendix CAdditional Analyses and Ablations",
        "images": []
    },
    {
        "header": "Appendix DDataset Details",
        "images": []
    },
    {
        "header": "Appendix ETraining Details",
        "images": []
    },
    {
        "header": "Appendix FModel Registry",
        "images": []
    },
    {
        "header": "Appendix GDetails of Instruction Classification for AlpacaEval and Arena-Hard",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.16802/alpacaeval_cls_dist.jpeg",
                "caption": "Figure 6:Distribution of different instruction types in AlpacaEval.",
                "position": 3244
            },
            {
                "img": "https://arxiv.org/html/2602.16802/arenahard_cls_dist.jpeg",
                "caption": "Figure 7:Distribution of different instruction types in Arena-Hard.",
                "position": 3247
            }
        ]
    },
    {
        "header": "Appendix HPrompt Templates",
        "images": []
    }
]