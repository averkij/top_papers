[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13595/extracted/6178131/figures/mmteb_overview_wide_centered.png",
                "caption": "Figure 1:An overview of MMTEB. The boxes represent the overall task categories with a sample of task categories represented within each. Blue borders represent closely-related task categories.",
                "position": 353
            }
        ]
    },
    {
        "header": "2MMTEB Construction",
        "images": []
    },
    {
        "header": "3Experimental Settings",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13595/extracted/6178131/figures/performance-x-parameters.png",
                "caption": "Figure 2:Mean performance across tasks on MTEB(Multilingual) according to the number of parameters. The circle size denotes the embedding size, while the color denotes the maximum sequence length of the model. To improve readability, only certain labels are shown. We refer to the public leaderboard\nfor interactive visualization. We see that the notably smaller model obtains comparable performance to Mistral 7B and GritLM-7B, note that these overlap in the figure due to the similarity of the two models.",
                "position": 534
            }
        ]
    },
    {
        "header": "4Analysis and Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13595/extracted/6178131/figures/perf_n_speakers_plot.png",
                "caption": "Figure 3:Performance rank of top 3 multilingual models on languages inMTEB(Europe) and MTEB(Indic) and by the number of native speakers.\nWe see that Mistral-based models are outperformed by multilingual-e5-large-instruct on lower-resource languages, despite it having substantially fewer parameters.",
                "position": 1144
            },
            {
                "img": "https://arxiv.org/html/2502.13595/extracted/6178131/figures/multi_english_comparison.png",
                "caption": "Figure 4:Performance difference onMTEB(eng, v1)(flag) andMTEB(Multilingual)(globe).",
                "position": 1158
            },
            {
                "img": "https://arxiv.org/html/2502.13595/extracted/6178131/figures/old_lite_comparison.png",
                "caption": "Figure 5:Performance onMTEB(eng, v1)andMTEB(eng, v2)",
                "position": 1166
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13595/x1.png",
                "caption": "Figure 6:Number of tasks per language. For readability, we remove English (290 tasks) and only plot the 100 languages with the most tasks.",
                "position": 1224
            }
        ]
    },
    {
        "header": "Ethical Considerations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix Table of Contents",
        "images": []
    },
    {
        "header": "Appendix AContributions",
        "images": []
    },
    {
        "header": "Appendix BOverview and Construction of Tasks",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13595/extracted/6178131/figures/wikiretrieval_test.png",
                "caption": "Figure 7:Comparison of MRR on synthetic retrieval and gold (GermanQuAD). The synthetic dataset was generated using GPT4-turbo.",
                "position": 6728
            }
        ]
    },
    {
        "header": "Appendix CBenchmark Optimizations",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13595/extracted/6178131/figures/task_plot_BiorxivClusteringP2P.png",
                "caption": "(a)",
                "position": 7052
            },
            {
                "img": "https://arxiv.org/html/2502.13595/extracted/6178131/figures/task_plot_BiorxivClusteringP2P.png",
                "caption": "(a)",
                "position": 7055
            },
            {
                "img": "https://arxiv.org/html/2502.13595/extracted/6178131/figures/task_plot_BiorxivClusteringS2S.png",
                "caption": "(b)",
                "position": 7061
            },
            {
                "img": "https://arxiv.org/html/2502.13595/extracted/6178131/figures/task_plot_MedrxivClusteringP2P.png",
                "caption": "(c)",
                "position": 7067
            },
            {
                "img": "https://arxiv.org/html/2502.13595/extracted/6178131/figures/task_plot_MedrxivClusteringS2S.png",
                "caption": "(d)",
                "position": 7073
            },
            {
                "img": "https://arxiv.org/html/2502.13595/extracted/6178131/figures/task_plot_RedditClustering.png",
                "caption": "(e)",
                "position": 7079
            },
            {
                "img": "https://arxiv.org/html/2502.13595/extracted/6178131/figures/task_plot_RedditClusteringP2P.png",
                "caption": "(f)",
                "position": 7085
            },
            {
                "img": "https://arxiv.org/html/2502.13595/extracted/6178131/figures/task_plot_StackExchangeClustering.png",
                "caption": "(g)",
                "position": 7091
            },
            {
                "img": "https://arxiv.org/html/2502.13595/extracted/6178131/figures/task_plot_StackExchangeClusteringP2P.png",
                "caption": "(h)",
                "position": 7097
            },
            {
                "img": "https://arxiv.org/html/2502.13595/extracted/6178131/figures/task_plot_TwentyNewsgroupsClustering.png",
                "caption": "(i)",
                "position": 7103
            },
            {
                "img": "https://arxiv.org/html/2502.13595/x2.png",
                "caption": "Figure 9:Ranking of different models on subsampled versions of the datasets using hard negatives. We see that NQ can be reduced to just two documents per query (relevant + 1 hard negative) while still maintaining the rank while TREC-COVID is less stable.",
                "position": 7250
            },
            {
                "img": "https://arxiv.org/html/2502.13595/x3.png",
                "caption": "",
                "position": 7253
            },
            {
                "img": "https://arxiv.org/html/2502.13595/x4.png",
                "caption": "Figure 10:Absolute scores of different models on subsampled versions of the datasets using hard negatives. NQ has 1 relevant document per query while TREC-COVID has 500+ relevant documents per query which is why we see NQ scores gradually increasing whereas TREC-COVID scores vary.",
                "position": 7257
            },
            {
                "img": "https://arxiv.org/html/2502.13595/x5.png",
                "caption": "",
                "position": 7260
            }
        ]
    },
    {
        "header": "Appendix DTask Overview",
        "images": []
    },
    {
        "header": "Appendix EFull results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13595/extracted/6178131/figures/rank_by_lang_size.png",
                "caption": "Figure 12:Modelsâ€™ rank on theMTEB(Multilingual)by the total number of speakers of a language.Trendlines represent moving average with a window size of 10",
                "position": 9376
            }
        ]
    },
    {
        "header": "Appendix FNew Metrics",
        "images": []
    },
    {
        "header": "Appendix GModels",
        "images": []
    },
    {
        "header": "Appendix HBenchmark Construction and Overview",
        "images": []
    }
]