[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03126/x1.png",
                "caption": "Figure 1:Overview of AnimeShooter. It isa reference-guided multi-shot animation datasetfeaturing comprehensive hierarchical annotations and strong coherence across shots. At the story level, each sample includes an overall storyline, main scene descriptions, and detailed character profiles with reference images. At the shot level, consecutive shots are annotated with specific scenes, involved characters, and rich visual captions (both narrative and descriptive). A specific subset, AnimeShooter-audio, additionally provides synchronized audios for each shot with corresponding audio descriptions and sound sources.",
                "position": 109
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3AnimeShooter Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03126/x2.png",
                "caption": "Figure 2:Video collection and annotation pipeline. We curate relevant videos from YouTube and segment them into 1-minute segments using boundary detection. Each segment serves as an individual sample representing a self-contained narrative unit (one story). We use Gemini to further decompose the story into consecutive shots with visual consistency based on transitions, and generate structured story script. Corresponding reference images are generated by Sa2VA and InternVL.",
                "position": 148
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03126/x3.png",
                "caption": "Figure 3:Overview of the model architecture. The two core components include the autoregressive backbone stemming from pretrained MLLM, and a video generator initialized from a pretrained DiT. To stitch these two components, we add a Q-Former as the adapter. This framework can generate multi-shot video in autoregressive manner.",
                "position": 271
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03126/x4.png",
                "caption": "Figure 4:Qualitatively comparisons on multi-shot animation generation. Our method delivers the best visual quality, including character-reference consistency and multi-shot consistency.",
                "position": 499
            },
            {
                "img": "https://arxiv.org/html/2506.03126/x5.png",
                "caption": "Figure 5:Visualization of using different references in MLLM(before LoRA Enhancement). Shared caption: “Shot-1: The man walks down a cobblestone street lined with blooming cherry trees, holding a vintage leather journal under his arm.”, “Shot-2: He pauses at a flower shop, steps inside, and begins carefully selecting flowers.”, “Shot-3: At the counter, he wraps the bouquet in paper.”, “Shot-4: He tucks the flowers into his bicycle basket and pedals away past pastel-colored storefronts.”",
                "position": 502
            }
        ]
    },
    {
        "header": "6Conclusion and Limitation",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails for Data Curation",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03126/x6.png",
                "caption": "Figure 6:The prompt used for multi-shot captioning.",
                "position": 1182
            },
            {
                "img": "https://arxiv.org/html/2506.03126/x7.png",
                "caption": "Figure 7:The prompt used for reference image generation. Top subfigure: Segmentation prompt for Sa2VA. Bottom subfigure: Filtering prompt for InternVL.",
                "position": 1192
            },
            {
                "img": "https://arxiv.org/html/2506.03126/x8.png",
                "caption": "Figure 8:The prompt used for constructing AnimeShooter-audio.",
                "position": 1202
            }
        ]
    },
    {
        "header": "Appendix BDetails for Model Training",
        "images": []
    },
    {
        "header": "Appendix CDetails for Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03126/x9.png",
                "caption": "Figure 9:Example of IP-specific dataset for evaluation.",
                "position": 1250
            },
            {
                "img": "https://arxiv.org/html/2506.03126/x10.png",
                "caption": "Figure 10:The prompt used for constructing evaluation dataset.",
                "position": 1253
            },
            {
                "img": "https://arxiv.org/html/2506.03126/x11.png",
                "caption": "Figure 11:The prompt used for MLLM assessment.",
                "position": 1296
            },
            {
                "img": "https://arxiv.org/html/2506.03126/x12.png",
                "caption": "Figure 12:Additional qualitative results.",
                "position": 1313
            },
            {
                "img": "https://arxiv.org/html/2506.03126/x13.png",
                "caption": "Figure 13:Additional qualitative results.",
                "position": 1316
            }
        ]
    },
    {
        "header": "Appendix DIntegration of Audio Generation Capabilities for AnimeShooterGen",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03126/x14.png",
                "caption": "Figure 14:The prompt used for generating descriptive audio captions.",
                "position": 1327
            }
        ]
    },
    {
        "header": "Appendix EPotential Negative Social Impacts",
        "images": []
    },
    {
        "header": "Appendix FData Access and License",
        "images": []
    }
]