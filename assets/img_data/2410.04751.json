[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Demystifying Intriguing Properties of LVLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.04751/x1.png",
                "caption": "Figure 1:We demonstrate the extent to which group-wise visual tokens capture region-specific information (PIL) forLLaVA-1.5-7Bon the MMStar(Chen et al.,2024a)and MME(Fu et al.,2023). Darker regions indicate areas where the model retains more localized information for those specific groups.",
                "position": 229
            },
            {
                "img": "https://arxiv.org/html/2410.04751/x2.png",
                "caption": "Figure 2:We present the performance across different grid sizes (2, 4, 8, 14) on the MMVP, MM-Vet, MathVista, and AI2D datasets, using three models:LLaVA-1.5,LLaVA-NeXT, andLLaVA-OneVision.",
                "position": 529
            },
            {
                "img": "https://arxiv.org/html/2410.04751/x3.png",
                "caption": "Figure 3:We present examples of shuffled images with different grid sizes (2, 4, 8, 14) derived from a MathVista dataset image. As the grid size increases, the chart image becomes more artistically styled.",
                "position": 540
            },
            {
                "img": "https://arxiv.org/html/2410.04751/x4.png",
                "caption": "Figure 4:We present performance on the GSM8K dataset using 8-shot Chain-of-Thought prompting. Additionally, we demonstrate that scaling up the instruction-tuning dataset enables LLVMs to solve text-only math reasoning problems more effectively.",
                "position": 713
            },
            {
                "img": "https://arxiv.org/html/2410.04751/x5.png",
                "caption": "Figure 5:We present examples of images (left) synthesized bySDXL-Lightningand (right) occluded using three methods:Random,Salient, andNon-Salient. The original images are from the MathVista and MME datasets. Occluded areas are marked in black to indicate zero pixel values.",
                "position": 724
            },
            {
                "img": "https://arxiv.org/html/2410.04751/x6.png",
                "caption": "Figure 6:We present robustness performance under occlusion conditions. (a) ViT variant vision encoders demonstrate greater robustness to occlusion compared toResNet-50. (b) LLVMs also show robustness to occlusion, benefiting from the use of ViT encoders.",
                "position": 740
            },
            {
                "img": "https://arxiv.org/html/2410.04751/x7.png",
                "caption": "Figure 7:We present how alignment preservation changes (CLIP→→\\rightarrow→LLaVA) in the representation space across various LLM families,BLOOM(Le Scao et al.,2023),OLMo(Groeneveld et al.,2024),Gemma(Team et al.,2024),Vicuna(Chiang et al.,2023), with different parameter sizes on the DOCCI dataset.",
                "position": 829
            },
            {
                "img": "https://arxiv.org/html/2410.04751/x8.png",
                "caption": "Figure 8:We report the degree of utility of group-wise visual tokens for LLaVA 1.5 7B on the MM-Vet dataset. Darker regions indicate that the LLVM relies heavily on information from those specific group parts.",
                "position": 881
            },
            {
                "img": "https://arxiv.org/html/2410.04751/x9.png",
                "caption": "Figure 9:We present the results of (left) layer-wise importance and (right) modality importance within the layers.",
                "position": 891
            }
        ]
    },
    {
        "header": "4Discussions",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADescription of Evaluation Benchmarks",
        "images": []
    },
    {
        "header": "Appendix BDescription of Evaluation LVLMs",
        "images": []
    }
]