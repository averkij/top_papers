[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18890/x1.png",
                "caption": "",
                "position": 105
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Large Space-Time Reconstruction Model (4D-LRM)",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18890/x2.png",
                "caption": "((a))Generative 4D Modeling.",
                "position": 165
            },
            {
                "img": "https://arxiv.org/html/2506.18890/x2.png",
                "caption": "((a))Generative 4D Modeling.",
                "position": 168
            },
            {
                "img": "https://arxiv.org/html/2506.18890/x3.png",
                "caption": "((b))Generic 4D Reconstruction.",
                "position": 173
            },
            {
                "img": "https://arxiv.org/html/2506.18890/x4.png",
                "caption": "Figure 3:Overview of4D-LRM.4D-LRMadopts a unified treatment of space and time, representing a dynamic object as a cloud of anisotropic 4D Gaussians[82]. We train a simple Transformer to regress 4D Gaussian primitives from a set of images with camera poses and timestamps. Each input image is tokenized by patchifying the temporally posed frames. The resulting multi-view image tokens are concatenated in temporal order and passed through a series of transformer blocks. Optional set ofN洧녜Nitalic_Nlearnable free Gaussian tokens append the image tokens for greater generative flexibility.",
                "position": 239
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18890/x5.png",
                "caption": "Figure 4:Different types of camera setups in evaluations:Alternating Canonical Views(4 camera poses, 24/24 timestamps seen, 24 input views in total); Frame Interpolation (4 camera poses, 12/24 timestamps seen, 24 input views in total);Two Rotating Cameras(24 camera poses, 24/24 timestamps seen, 24 input views in total);Single View Video[53,78](4 camera poses on the first frame plus a single view video for subsequent frames, 24/24 timestamps seen, 27 input views in total);Random Input Views(random poses, 24/24 timestamps seen, 24 input views in total).",
                "position": 316
            },
            {
                "img": "https://arxiv.org/html/2506.18890/x6.png",
                "caption": "Figure 5:Visual comparison with GS-LRM[92]using (a) all input views across time and (b) three random views from the same timestamp. 4D-LRM reconstructs novel view-time combinations by learning spatiotemporal representations from sparse inputs, outperforming per-frame reconstruction by effectively sharing information across both space and time.",
                "position": 549
            },
            {
                "img": "https://arxiv.org/html/2506.18890/x7.png",
                "caption": "Figure 6:Qualitative examples of 4D-LRM taking views with missing frames, includingFrame Interpolation(only half timestamps seen, 24 input views) andRandom Views at Random Frames(24 input views from random views and times).",
                "position": 775
            },
            {
                "img": "https://arxiv.org/html/2506.18890/x8.png",
                "caption": "Figure 7:Visual comparisons of 4D-LRM(-Free) to generation-based 4D models. For fair comparisons, we initialize each model with the first frame with the ground truth multi-view images.",
                "position": 1070
            }
        ]
    },
    {
        "header": "4Analyses and Discussions",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18890/x9.png",
                "caption": "((a))풮tsubscript洧랞洧노\\mu_{t}italic_풮 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTw/o interpolation.",
                "position": 1100
            },
            {
                "img": "https://arxiv.org/html/2506.18890/x9.png",
                "caption": "((a))풮tsubscript洧랞洧노\\mu_{t}italic_풮 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTw/o interpolation.",
                "position": 1103
            },
            {
                "img": "https://arxiv.org/html/2506.18890/x10.png",
                "caption": "((b))풮tsubscript洧랞洧노\\mu_{t}italic_풮 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTw/ interpolation.",
                "position": 1108
            },
            {
                "img": "https://arxiv.org/html/2506.18890/x11.png",
                "caption": "((c))풖tsubscript풖洧노\\Sigma_{t}roman_풖 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTw/o interpolation.",
                "position": 1113
            },
            {
                "img": "https://arxiv.org/html/2506.18890/x12.png",
                "caption": "((d))풖tsubscript풖洧노\\Sigma_{t}roman_풖 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTw/ interpolation.",
                "position": 1118
            },
            {
                "img": "https://arxiv.org/html/2506.18890/x13.png",
                "caption": "((a))PSNR vs. #Training Steps.",
                "position": 1125
            },
            {
                "img": "https://arxiv.org/html/2506.18890/x13.png",
                "caption": "((a))PSNR vs. #Training Steps.",
                "position": 1128
            },
            {
                "img": "https://arxiv.org/html/2506.18890/x14.png",
                "caption": "((b))SSIM vs. #Training Steps.",
                "position": 1133
            },
            {
                "img": "https://arxiv.org/html/2506.18890/x15.png",
                "caption": "((c))LPIPS vs. #Training Steps.",
                "position": 1138
            },
            {
                "img": "https://arxiv.org/html/2506.18890/x16.png",
                "caption": "((a))PSNR vs. #Input View.",
                "position": 1201
            },
            {
                "img": "https://arxiv.org/html/2506.18890/x16.png",
                "caption": "((a))PSNR vs. #Input View.",
                "position": 1204
            },
            {
                "img": "https://arxiv.org/html/2506.18890/x17.png",
                "caption": "((b))SSIM vs. #Input View.",
                "position": 1209
            },
            {
                "img": "https://arxiv.org/html/2506.18890/x18.png",
                "caption": "((c))LPIPS vs. #Input View.",
                "position": 1214
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion and Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix A4D-LRM Implementation and Training Details",
        "images": []
    },
    {
        "header": "Appendix BAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18890/x19.png",
                "caption": "Figure 11:A typical failure case as 4D-LRM sometimes struggles with non-linear motion trajectories. When an object follows a non-linear path, the structures cannot be efficiently captured by a single ellipsoidal Gaussian. As a result, the model requires multiple Gaussians placed along the trajectory to approximate the motion, increasing complexity and often leading to artifacts if not properly aligned.",
                "position": 3111
            },
            {
                "img": "https://arxiv.org/html/2506.18890/x20.png",
                "caption": "Figure 12:Qualitative examples of 4D-LRM under varying camera setups. We show the performance of 4D-LRM when taking input views captured with different camera configurations, demonstrating its robustness to diverse spatial arrangements and viewpoints.",
                "position": 3114
            },
            {
                "img": "https://arxiv.org/html/2506.18890/x21.png",
                "caption": "",
                "position": 3118
            },
            {
                "img": "https://arxiv.org/html/2506.18890/x22.png",
                "caption": "Figure 13:Additional frame interpolation examples. We insert 4칑\\times칑denser frames between Alternating Canonical Views as input.",
                "position": 3122
            },
            {
                "img": "https://arxiv.org/html/2506.18890/x23.png",
                "caption": "",
                "position": 3126
            }
        ]
    },
    {
        "header": "Appendix CBroader Impact",
        "images": []
    }
]