[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.15369/figure/openvision_3_logo.png",
                "caption": "",
                "position": 74
            },
            {
                "img": "https://arxiv.org/html/2601.15369/figure/logo_browser.png",
                "caption": "",
                "position": 120
            },
            {
                "img": "https://arxiv.org/html/2601.15369/x1.png",
                "caption": "",
                "position": 126
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.15369/figure/recon_outside_loss_compare.png",
                "caption": "((a))",
                "position": 607
            },
            {
                "img": "https://arxiv.org/html/2601.15369/figure/recon_outside_loss_compare.png",
                "caption": "((a))",
                "position": 610
            },
            {
                "img": "https://arxiv.org/html/2601.15369/figure/recon_inside_loss_compare.png",
                "caption": "((b))",
                "position": 619
            },
            {
                "img": "https://arxiv.org/html/2601.15369/figure/caption_loss_compare.png",
                "caption": "((c))",
                "position": 628
            },
            {
                "img": "https://arxiv.org/html/2601.15369/figure/contrastive_loss_compare.png",
                "caption": "((d))",
                "position": 637
            },
            {
                "img": "https://arxiv.org/html/2601.15369/figure/wo_und_recon_outside_loss_compare.png",
                "caption": "((a))",
                "position": 651
            },
            {
                "img": "https://arxiv.org/html/2601.15369/figure/wo_und_recon_outside_loss_compare.png",
                "caption": "((a))",
                "position": 654
            },
            {
                "img": "https://arxiv.org/html/2601.15369/figure/wo_und_recon_inside_loss_compare.png",
                "caption": "((b))",
                "position": 663
            },
            {
                "img": "https://arxiv.org/html/2601.15369/figure/wo_und_caption_loss_compare.png",
                "caption": "((c))",
                "position": 672
            },
            {
                "img": "https://arxiv.org/html/2601.15369/figure/wo_und_contrastive_loss_compare.png",
                "caption": "((d))",
                "position": 681
            },
            {
                "img": "https://arxiv.org/html/2601.15369/x2.png",
                "caption": "Figure 4:Qualitative results of class-conditional ImageNet-256 generation.Under the RAE framework, ourOpenVision 3is able to generate high quality images.",
                "position": 777
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]