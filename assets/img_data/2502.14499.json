[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14499/x1.png",
                "caption": "Figure 1:Diagram of\\mlgym, a unified framework designed to integrate diverse and open-ended AI research tasks into a single platform for developing and evaluating LLM agents on these tasks.",
                "position": 306
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14499/x2.png",
                "caption": "Table 1:Comparison of\\mlgymand\\mlgym-Bench with other related LLM agent frameworks and benchmarks. Algorithmic Tasks refers to the inclusion of tasks that require coming up with new algorithms such as reinforcement learning, game theory or SAT problems. Open-ended Research refers to the inclusion of tasks that are not fully solved by the research community and where multiple new solutions could be discovered such as language modeling, game theory or SAT problems. Flexible Artifacts refers to the allowance of different research artifacts such as model weights, reinforcement learning algorithms, or code capturing an agent‚Äôs strategy.",
                "position": 382
            }
        ]
    },
    {
        "header": "3\\mlgym",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14499/x2.png",
                "caption": "Table 2:List of tools available to agents. Required arguments are enclosed in<>absent<>< >and optional arguments are in[][][ ].",
                "position": 589
            }
        ]
    },
    {
        "header": "4\\mlgym-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14499/x2.png",
                "caption": "Table 3:List of tasks included in\\mlgym-Bench along with their respective problem setting, domain, and datasets.",
                "position": 735
            }
        ]
    },
    {
        "header": "5Experimental Setup",
        "images": []
    },
    {
        "header": "6Evaluation",
        "images": []
    },
    {
        "header": "7Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14499/x2.png",
                "caption": "Figure 2:Performance profiles comparing Best Attempt@4 and Best Submission@4 across all models and tasks. The x-axis shows the performance ratio thresholdœÑùúè\\tauitalic_œÑand the y-axis shows the fraction of tasks where a model achieves performance withinœÑùúè\\tauitalic_œÑof the best model.",
                "position": 993
            },
            {
                "img": "https://arxiv.org/html/2502.14499/x3.png",
                "caption": "Table 4:AUP@4 scores for the best attempt and best submission across all models. Best scores are highlighted inblue.",
                "position": 1011
            },
            {
                "img": "https://arxiv.org/html/2502.14499/x3.png",
                "caption": "Table 5:Best Attempt@4 scores for all models. Best scores are highlighted inblue.Note:‚àû\\infty‚àûindicates that the model was not able to produce even a single valid solution for submission or validation.",
                "position": 1041
            },
            {
                "img": "https://arxiv.org/html/2502.14499/x3.png",
                "caption": "Table 6:Best Submission@4 scores for all models. Best scores are highlighted inblue.Note:‚àû\\infty‚àûindicates that the model was not able to produce even a single valid solution for submission or validation.",
                "position": 1066
            },
            {
                "img": "https://arxiv.org/html/2502.14499/x3.png",
                "caption": "Figure 3:Best Attempt AUP@4 vs cost for all models. The x-axis shows the API cost in USD and the y-axis shows the AUP@4 score.",
                "position": 1094
            },
            {
                "img": "https://arxiv.org/html/2502.14499/x4.png",
                "caption": "Figure 4:Termination Error Distribution by model. The size of the bars corresponds to the number of times each model triggered an exit status.",
                "position": 1157
            },
            {
                "img": "https://arxiv.org/html/2502.14499/x4.png",
                "caption": "Figure 4:Termination Error Distribution by model. The size of the bars corresponds to the number of times each model triggered an exit status.",
                "position": 1160
            },
            {
                "img": "https://arxiv.org/html/2502.14499/x5.png",
                "caption": "Figure 5:Number of Failed and Incomplete runs per model. The criteria for marking a run as incomplete or failed is described inSection¬†7.4.1",
                "position": 1165
            },
            {
                "img": "https://arxiv.org/html/2502.14499/x6.png",
                "caption": "Figure 6:Action distribution across all runs. We group the actions into categories following the grouping defined inSection¬†3.5andSection¬†7.4.2.",
                "position": 1206
            },
            {
                "img": "https://arxiv.org/html/2502.14499/x6.png",
                "caption": "Figure 6:Action distribution across all runs. We group the actions into categories following the grouping defined inSection¬†3.5andSection¬†7.4.2.",
                "position": 1209
            },
            {
                "img": "https://arxiv.org/html/2502.14499/x7.png",
                "caption": "Figure 7:Action distribution for each model. We group the actions into categories following the grouping defined inSection¬†3.5andSection¬†7.4.2.",
                "position": 1214
            },
            {
                "img": "https://arxiv.org/html/2502.14499/x8.png",
                "caption": "Figure 8:Action distribution for each step. We group the actions into categories following the grouping defined inSection¬†3.5andSection¬†7.4.2.",
                "position": 1238
            }
        ]
    },
    {
        "header": "8Discussion and Limitations",
        "images": []
    },
    {
        "header": "9Ethical Considerations",
        "images": []
    },
    {
        "header": "10Conclusions",
        "images": []
    },
    {
        "header": "11Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "12Additional Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14499/x9.png",
                "caption": "Table 7:Computational resources required for each task in\\mlgym-bench.",
                "position": 2567
            },
            {
                "img": "https://arxiv.org/html/2502.14499/x9.png",
                "caption": "Table 8:Model pricing, token usage and context length details. Model Pricing is in USD per 1M tokens.‚àóLlama3.1: FP8 endpoint by Together666https://www.together.ai/pricing",
                "position": 2597
            },
            {
                "img": "https://arxiv.org/html/2502.14499/x9.png",
                "caption": "Figure 9:Number of Failed and Incomplete runs per task. The criteria for marking a run as incomplete or failed is described inSection¬†7.4.1",
                "position": 2621
            },
            {
                "img": "https://arxiv.org/html/2502.14499/x10.png",
                "caption": "Figure 10:Action Distribution for each task. We group the actions into categories following the grouping defined inSection¬†3.5andSection¬†7.4.2.",
                "position": 2643
            },
            {
                "img": "https://arxiv.org/html/2502.14499/extracted/6219596/assets/memory_s3.png",
                "caption": "Table 9:Individual and Aggregate Ranking of models based on Best Attempt@4. We use the BORDA method to compute the aggregate ranks.",
                "position": 2653
            },
            {
                "img": "https://arxiv.org/html/2502.14499/extracted/6219596/assets/memory_s3.png",
                "caption": "Table 10:Individual and Aggregate Ranking of models based on Best Subimission@4. We use the BORDA method to compute the aggregate ranks.",
                "position": 2677
            },
            {
                "img": "https://arxiv.org/html/2502.14499/extracted/6219596/assets/memory_s3.png",
                "caption": "Figure 11:Example of retrieving the best training configuration from memory and restarting exploration from it.",
                "position": 2710
            },
            {
                "img": "https://arxiv.org/html/2502.14499/extracted/6219596/assets/memory_s4.png",
                "caption": "",
                "position": 2714
            },
            {
                "img": "https://arxiv.org/html/2502.14499/extracted/6219596/assets/memory_s5.png",
                "caption": "Figure 12:Example of retrieving the best training configuration from memory and restarting exploration from it.",
                "position": 2718
            },
            {
                "img": "https://arxiv.org/html/2502.14499/extracted/6219596/assets/memory_s6.png",
                "caption": "",
                "position": 2722
            }
        ]
    },
    {
        "header": "13Prompts",
        "images": []
    }
]