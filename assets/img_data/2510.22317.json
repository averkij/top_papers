[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Olifant: Background and architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22317/images/mblm.png",
                "caption": "Figure 1:Schematic conversion of an instance base with four examples of two context words predicting a third word, into a prefix trie with next-token distribution information stored at all nodes. Grey nodes, representing subsets of the instance base of which the majority class does not conflict with the parent node, are not stored when the prefix trie is used for decision-tree classification (the IGTree mode ofolifant).",
                "position": 139
            },
            {
                "img": "https://arxiv.org/html/2510.22317/images/memlm-variants.png",
                "caption": "Figure 2:Schematic visualization of classification in the three memory-based language modeling variants. The larger triangle represents the entire prefix trie; grayed zones representkk-NN classification; nodes and edges represent downward-pass decision-tree traversal of the prefix trie.",
                "position": 151
            },
            {
                "img": "https://arxiv.org/html/2510.22317/images/scaling.png",
                "caption": "Figure 3:Schematic visualization of the three uncoupled scaling dimensions in training Transformer-based LMs, versus the single coupled scaling dimension involved when training memory-based LMs, where the model equals the data.",
                "position": 236
            }
        ]
    },
    {
        "header": "3Learning curve analyses",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22317/images/contexts.png",
                "caption": "Figure 4:Learning curves in terms of correctly predicted tokens of TRIBL2 with increasing context widths.",
                "position": 259
            },
            {
                "img": "https://arxiv.org/html/2510.22317/images/learningcurves.png",
                "caption": "Figure 5:Learning curves on next-token prediction accuracy of the threeolifantvariants and the two GPT systems (with fixed training data sets but increasing model sizes). Dashed and dotted lines are regression functions fitted to all TRIBL2 measurements (dotted) and measurements from 1 billion training tokens (dashed).",
                "position": 279
            },
            {
                "img": "https://arxiv.org/html/2510.22317/images/latencies.png",
                "caption": "Figure 6:Next-token prediction accuracy latencies (s). Lower is better.",
                "position": 298
            },
            {
                "img": "https://arxiv.org/html/2510.22317/images/emissions.png",
                "caption": "Figure 7:CO2emission equivalents (g) of the various models on predicting the EduFineWeb validation set of 10,000 lines (512,660 tokens). Grey horizontal lines represent real-world CO2emission examples.",
                "position": 311
            },
            {
                "img": "https://arxiv.org/html/2510.22317/images/treesize.png",
                "caption": "Figure 8:Tree size (in nodes) of IGTree and TRIBL2 as a function of training set size. The dashed grey line represents the parityx=yx=yline.",
                "position": 414
            }
        ]
    },
    {
        "header": "4Memorization",
        "images": []
    },
    {
        "header": "5Explainability",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22317/images/timbloutput.png",
                "caption": "Figure 9:Example output of TRIBL2 classifying a single instance and producing a most likely prediction and a distribution of predictions, based on 20 equally distant nearest neighbors. Token information on word-initial status is left out for legibility.",
                "position": 472
            }
        ]
    },
    {
        "header": "6Perplexity and a distributional analysis ofolifantpredictions",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22317/images/perplexity.png",
                "caption": "Figure 10:Perplexity learning curves of the different systems. Both axes are logarithmic.",
                "position": 488
            },
            {
                "img": "https://arxiv.org/html/2510.22317/images/frequencies.png",
                "caption": "Figure 11:Bar-graph visualization of the distribution of next-token prediction frequencies of occurrence in a 100 million-token training set by TRIBL2 and IGTree, both trained on 100 million instances, and by GPT2-XL, all tested on the fixed validation set.",
                "position": 497
            }
        ]
    },
    {
        "header": "7Discussion and conclusions",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]