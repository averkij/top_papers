[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22638/x1.png",
                "caption": "Figure 1:Learning from mixed verbal feedback.The instructionğ’™{\\bm{x}}is â€œWrite a Python function flatten(lst) that returns a flat list of integersâ€. The reference policyÏ€ref\\pi_{\\textrm{ref}}may assign low probability to the ideal responseğ’+{\\bm{o}}^{+}, making purely positive response-feedback pairs(ğ’+,ğ’„+)({\\bm{o}}^{+},{\\bm{c}}^{+})rare in the training data collected fromÏ€ref\\pi_{\\textrm{ref}}andpenvp_{\\textrm{env}}. This resembles the setting of text-to-image generation, where thelanguage priorenables models to combine seen captions (analogous to mixed feedbackğ’„1{\\bm{c}}_{1}andğ’„2{\\bm{c}}_{2}) and generate rare images (analogous to purely positive responseğ’+{\\bm{o}}^{+}) such as â€œa banana surfing on the oceanâ€ (Figure4). Motivated by this, our modelÏ€Î¸\\pi_{\\theta}is trained as a feedback-conditional policy (FCP), and when conditioning on user-defined positiveğ’„+{\\bm{c}}^{+}, there isÏ€Î¸â€‹(ğ’|ğ’™,ğ’„+)âˆÏ€refâ€‹(ğ’|ğ’™)â‹…penvâ€‹(ğ’„+|ğ’™,ğ’)\\pi_{\\theta}({\\bm{o}}|{\\bm{x}},{\\bm{c}}^{+})\\propto\\pi_{\\textrm{ref}}({\\bm{o}}|{\\bm{x}})\\cdot p_{\\textrm{env}}({\\bm{c}}^{+}|{\\bm{x}},{\\bm{o}}).",
                "position": 108
            }
        ]
    },
    {
        "header": "2Learning directly from verbal feedback",
        "images": []
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22638/x2.png",
                "caption": "(a)Average accuracy over five math benchmarks measured at intermediate checkpoints.",
                "position": 430
            },
            {
                "img": "https://arxiv.org/html/2509.22638/x2.png",
                "caption": "(a)Average accuracy over five math benchmarks measured at intermediate checkpoints.",
                "position": 433
            },
            {
                "img": "https://arxiv.org/html/2509.22638/x3.png",
                "caption": "(b)Scalar scores assigned byGPT-5-nanoto model rollouts during training.",
                "position": 438
            },
            {
                "img": "https://arxiv.org/html/2509.22638/x4.png",
                "caption": "(a)Average response length over training steps.",
                "position": 530
            },
            {
                "img": "https://arxiv.org/html/2509.22638/x4.png",
                "caption": "(a)Average response length over training steps.",
                "position": 533
            },
            {
                "img": "https://arxiv.org/html/2509.22638/x5.png",
                "caption": "(b)Cross-entropy loss over training steps.",
                "position": 538
            }
        ]
    },
    {
        "header": "4Ablation studies",
        "images": []
    },
    {
        "header": "5Discussion and future directions",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22638/x6.png",
                "caption": "Figure 4:Learning from mixed captions in text-to-image generation.During training, models learn from realistic image-caption pairs such as â€œa banana and an apple on the tableâ€ or â€œa man surfing on an ocean waveâ€. They can leverage language priors to recombine these captions and generate novel concepts, such as â€œa banana surfing on the oceanâ€ (images shown are generated with Gemini 2.5 Flash Image). By analogy to Figure1, this illustrates how diverse verbal feedback can be treated as a conditioning signal, motivating our feedback-conditional learning paradigm.",
                "position": 1998
            }
        ]
    },
    {
        "header": "Appendix AAdditional derivations and discussions",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22638/x7.png",
                "caption": "(a)SFT (behavior cloning)",
                "position": 2078
            },
            {
                "img": "https://arxiv.org/html/2509.22638/x7.png",
                "caption": "(a)SFT (behavior cloning)",
                "position": 2081
            },
            {
                "img": "https://arxiv.org/html/2509.22638/x8.png",
                "caption": "(b)CFT (forward dynamics)",
                "position": 2086
            },
            {
                "img": "https://arxiv.org/html/2509.22638/x9.png",
                "caption": "(c)Our FCP(inverse dynamics)",
                "position": 2091
            }
        ]
    },
    {
        "header": "Appendix BRelated work",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22638/x10.png",
                "caption": "Figure 6:Prompt template used to elicit feedback fromGPT-5-nano, includingreal-world user-style feedback,professional reviewer-style feedback, and a scalar score.",
                "position": 2203
            }
        ]
    },
    {
        "header": "Appendix CDetailed experimental setup",
        "images": []
    }
]