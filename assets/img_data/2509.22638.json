[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22638/x1.png",
                "caption": "Figure 1:Learning from mixed verbal feedback.The instruction𝒙{\\bm{x}}is “Write a Python function flatten(lst) that returns a flat list of integers”. The reference policyπref\\pi_{\\textrm{ref}}may assign low probability to the ideal response𝒐+{\\bm{o}}^{+}, making purely positive response-feedback pairs(𝒐+,𝒄+)({\\bm{o}}^{+},{\\bm{c}}^{+})rare in the training data collected fromπref\\pi_{\\textrm{ref}}andpenvp_{\\textrm{env}}. This resembles the setting of text-to-image generation, where thelanguage priorenables models to combine seen captions (analogous to mixed feedback𝒄1{\\bm{c}}_{1}and𝒄2{\\bm{c}}_{2}) and generate rare images (analogous to purely positive response𝒐+{\\bm{o}}^{+}) such as “a banana surfing on the ocean” (Figure4). Motivated by this, our modelπθ\\pi_{\\theta}is trained as a feedback-conditional policy (FCP), and when conditioning on user-defined positive𝒄+{\\bm{c}}^{+}, there isπθ​(𝒐|𝒙,𝒄+)∝πref​(𝒐|𝒙)⋅penv​(𝒄+|𝒙,𝒐)\\pi_{\\theta}({\\bm{o}}|{\\bm{x}},{\\bm{c}}^{+})\\propto\\pi_{\\textrm{ref}}({\\bm{o}}|{\\bm{x}})\\cdot p_{\\textrm{env}}({\\bm{c}}^{+}|{\\bm{x}},{\\bm{o}}).",
                "position": 108
            }
        ]
    },
    {
        "header": "2Learning directly from verbal feedback",
        "images": []
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22638/x2.png",
                "caption": "(a)Average accuracy over five math benchmarks measured at intermediate checkpoints.",
                "position": 430
            },
            {
                "img": "https://arxiv.org/html/2509.22638/x2.png",
                "caption": "(a)Average accuracy over five math benchmarks measured at intermediate checkpoints.",
                "position": 433
            },
            {
                "img": "https://arxiv.org/html/2509.22638/x3.png",
                "caption": "(b)Scalar scores assigned byGPT-5-nanoto model rollouts during training.",
                "position": 438
            },
            {
                "img": "https://arxiv.org/html/2509.22638/x4.png",
                "caption": "(a)Average response length over training steps.",
                "position": 530
            },
            {
                "img": "https://arxiv.org/html/2509.22638/x4.png",
                "caption": "(a)Average response length over training steps.",
                "position": 533
            },
            {
                "img": "https://arxiv.org/html/2509.22638/x5.png",
                "caption": "(b)Cross-entropy loss over training steps.",
                "position": 538
            }
        ]
    },
    {
        "header": "4Ablation studies",
        "images": []
    },
    {
        "header": "5Discussion and future directions",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22638/x6.png",
                "caption": "Figure 4:Learning from mixed captions in text-to-image generation.During training, models learn from realistic image-caption pairs such as “a banana and an apple on the table” or “a man surfing on an ocean wave”. They can leverage language priors to recombine these captions and generate novel concepts, such as “a banana surfing on the ocean” (images shown are generated with Gemini 2.5 Flash Image). By analogy to Figure1, this illustrates how diverse verbal feedback can be treated as a conditioning signal, motivating our feedback-conditional learning paradigm.",
                "position": 1998
            }
        ]
    },
    {
        "header": "Appendix AAdditional derivations and discussions",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22638/x7.png",
                "caption": "(a)SFT (behavior cloning)",
                "position": 2078
            },
            {
                "img": "https://arxiv.org/html/2509.22638/x7.png",
                "caption": "(a)SFT (behavior cloning)",
                "position": 2081
            },
            {
                "img": "https://arxiv.org/html/2509.22638/x8.png",
                "caption": "(b)CFT (forward dynamics)",
                "position": 2086
            },
            {
                "img": "https://arxiv.org/html/2509.22638/x9.png",
                "caption": "(c)Our FCP(inverse dynamics)",
                "position": 2091
            }
        ]
    },
    {
        "header": "Appendix BRelated work",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22638/x10.png",
                "caption": "Figure 6:Prompt template used to elicit feedback fromGPT-5-nano, includingreal-world user-style feedback,professional reviewer-style feedback, and a scalar score.",
                "position": 2203
            }
        ]
    },
    {
        "header": "Appendix CDetailed experimental setup",
        "images": []
    }
]