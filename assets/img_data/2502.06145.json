[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06145/x1.png",
                "caption": "",
                "position": 69
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06145/x2.png",
                "caption": "Figure 2:The framework ofAnimate Anyone 2. We capture environmental information from the source video. The environment is formulated as regions devoid of characters and incorporated as model input, enabling end-to-end learning of character-environment fusion. To preserve object interactions, we additionally inject features of objects interacting with the character. These object features are extracted by a lightweight object guider and merged into the denoising process via spatial blending. To handle more diverse motions, we propose a pose modulation approach to better represent the spatial relationships between body limbs.",
                "position": 131
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06145/x3.png",
                "caption": "Figure 3:Different coefficients for mask formulation.",
                "position": 219
            },
            {
                "img": "https://arxiv.org/html/2502.06145/x4.png",
                "caption": "Figure 4:Qualitative Results.Animate Anyone 2achieves consistent character animation while enabling the integration and interaction between characters and their environments, thereby realizing environment affordance.",
                "position": 309
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06145/x5.png",
                "caption": "Figure 5:Qualitative comparion for character animation. We normalize the background to a uniform color.",
                "position": 433
            },
            {
                "img": "https://arxiv.org/html/2502.06145/x6.png",
                "caption": "Figure 6:Qualitative comparion. Our method demonstrates superior environment integration and object interaction.",
                "position": 506
            },
            {
                "img": "https://arxiv.org/html/2502.06145/x7.png",
                "caption": "Figure 7:Ablation study of environment formulation.",
                "position": 555
            },
            {
                "img": "https://arxiv.org/html/2502.06145/x8.png",
                "caption": "Figure 8:Qualitative ablation of object modeling method.",
                "position": 559
            },
            {
                "img": "https://arxiv.org/html/2502.06145/x9.png",
                "caption": "Figure 9:Qualitative ablation of pose modulation.",
                "position": 573
            }
        ]
    },
    {
        "header": "5Discussion and Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]