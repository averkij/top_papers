[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19316/figures/jpg/teaser666.jpg",
                "caption": "Figure 1:(a)Comparison betweenKoreand current methods for knowledge injection.(b)Performance of various methods on LLaVA-v1.5 (7B).Redandblueshading correspond to knowledge adaptation and retention evaluations, respectively.",
                "position": 113
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19316/figures/jpg/method2.jpg",
                "caption": "Figure 2:Overview ofKore, a synergistic method for knowledge-oriented augmentation and constraint.Kore-augmentationautomatically converts each piece of knowledge into profound and structured knowledge.Kore-constraintminimizes interference with previous knowledge by initializing an adapter with null space that stores covariance matrix of previous knowledge.",
                "position": 161
            },
            {
                "img": "https://arxiv.org/html/2510.19316/figures/jpg/augmentation_comparison666.jpg",
                "caption": "Figure 3:Comparison ofKore-augmentation(left)and general augmentation methods(right).",
                "position": 173
            },
            {
                "img": "https://arxiv.org/html/2510.19316/figures/jpg/capture_knowledge666.jpg",
                "caption": "Figure 4:Performance (higher is better) on(a)MME(Fu et al.,2023)and(b)ScienceQA(Lu et al.,2022)after reconstruction.(c)Covariance matrix visualization for 4 different input activations in the 0-th block. We down-sample the heatmaps into 32×32. Similar patterns are marked in red circles.",
                "position": 257
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19316/figures/jpg/knowledge_type666.jpg",
                "caption": "Figure 5:Comparison betweenKoreand baseline methods on fine-grained knowledge types.",
                "position": 528
            },
            {
                "img": "https://arxiv.org/html/2510.19316/figures/specific_knowledge.png",
                "caption": "Figure 6:Performance of knowledge adaptation (K.A) and retention (K.R) under specific knowledge-oriented constraints.",
                "position": 793
            },
            {
                "img": "https://arxiv.org/html/2510.19316/figures/specific_knowledge.png",
                "caption": "Figure 7:Performance comparison of corresponding tasks under specific knowledge-oriented constraints.",
                "position": 847
            },
            {
                "img": "https://arxiv.org/html/2510.19316/figures/rank555.png",
                "caption": "Figure 8:Comparison of different ranks forKorewith LLaVA-v1.5 (7B).",
                "position": 1029
            }
        ]
    },
    {
        "header": "5Limitations & Future Discussion",
        "images": []
    },
    {
        "header": "6CONCLUSION",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Reproducibility statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AThe Use of Large Language Models inKore",
        "images": []
    },
    {
        "header": "Appendix BMore details about setup and Experimental operation",
        "images": []
    },
    {
        "header": "Appendix CProof ofKore",
        "images": []
    },
    {
        "header": "Appendix DMore details about analysis of ability to capture knowledge",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19316/figures/layer2.png",
                "caption": "Figure 9:Covariance matrix visualization for “mlp.down_proj”, “mlp.gate_proj”,“self_attn.v_proj”and “self_attn.o_proj”\nweights in the2-th layeronPOPE, HallusionBench and MMBench.",
                "position": 2683
            },
            {
                "img": "https://arxiv.org/html/2510.19316/figures/layer30.png",
                "caption": "Figure 10:Covariance matrix visualization for “mlp.down_proj”, “mlp.gate_proj”,“self_attn.v_proj”and “self_attn.o_proj”\nweights in the30-th layeronPOPE, HallusionBench and MMBench.",
                "position": 2687
            }
        ]
    },
    {
        "header": "Appendix EMore experimental results aboutKore",
        "images": []
    },
    {
        "header": "Appendix FConvergence comparison of various methods via loss curves.",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19316/figures/training_loss_comparison.png",
                "caption": "Figure 11:The training loss curves onEvokeof Full-FT, LoRA, EWC, O-LoRA, SEFE andKore.It should be clarified that Full‑FT, LoRA, EWC, O‑LoRA, and SEFE are trained using the knowledge injection dataset fromEvoke, whereasKoreis trained using theKore-74Kdataset. The scale of the training data differs between these setups, resulting in varying numbers of iteration steps per epoch. Consequently,Koreexhibits a rapid decrease in loss during the first epoch. The purpose of reporting this loss graph is to provide readers with an intuitive understanding of the convergence of various methods.",
                "position": 5053
            }
        ]
    },
    {
        "header": "Appendix Gcase study",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19316/figures/jpg/case1.jpg",
                "caption": "Figure 12:Case Study of News.",
                "position": 5064
            },
            {
                "img": "https://arxiv.org/html/2510.19316/figures/jpg/case2.jpg",
                "caption": "Figure 13:Case Study of Entity.",
                "position": 5067
            },
            {
                "img": "https://arxiv.org/html/2510.19316/figures/pipeline.png",
                "caption": "Figure 14:Overview of construction pipeline forKore-74K.The entire data construction process is automated, with only the question templates being manually crafted.",
                "position": 5080
            }
        ]
    },
    {
        "header": "Appendix HMore details aboutKore-augmentation",
        "images": []
    }
]