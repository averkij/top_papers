[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09088/x1.png",
                "caption": "",
                "position": 130
            },
            {
                "img": "https://arxiv.org/html/2601.09088/x2.png",
                "caption": "",
                "position": 132
            },
            {
                "img": "https://arxiv.org/html/2601.09088/x3.png",
                "caption": "",
                "position": 134
            },
            {
                "img": "https://arxiv.org/html/2601.09088/x4.png",
                "caption": "Figure 1:Performance of DASD-4B-Thinking on benchmark datasets. All metrics for the comparison models are taken from their official reports.",
                "position": 140
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09088/x5.png",
                "caption": "Figure 2:Overall training pipeline of DASD-4B-Thinking.",
                "position": 241
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Temperature-scheduled Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09088/x6.png",
                "caption": "Figure 3:Comparison of probability distribution and training loss with data sampled fromgpt-oss-120bunder different temperatures. We randomly sampled 50K mathematical reasoning responses at both low (T=0.6) and high (T=1.0) temperatures. To characterize the overall likelihood of a response, we computethe geometric mean of its token-level probabilities.\n(a) Probability distributions of sampled responses: the upper panel displays the density of probability distribution, while the lower panel shows the probability intervals covered by the sampled responses. (b) SFT training loss curves for the student model trained on responses sampled at these temperatures.",
                "position": 339
            },
            {
                "img": "https://arxiv.org/html/2601.09088/x7.png",
                "caption": "Figure 4:Comparison of probability distribution and training loss with data sampled fromQwen3-Next-80B-A3B-Thinkingunder different\ntemperatures.",
                "position": 443
            }
        ]
    },
    {
        "header": "4Divergence-aware Sampling",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09088/x8.png",
                "caption": "Figure 5:Joint comparison of the three models’ predicted probabilities. An example of output probabilities: the x-axis indexes sentences, and the y-axis shows predicted probabilities. Foreground lines plot the probabilities of the three models, while background colors indicate the inferred source of each sentence. By comparing probability differences, every sentence is categorized into one of four source types.",
                "position": 518
            },
            {
                "img": "https://arxiv.org/html/2601.09088/x9.png",
                "caption": "Figure 6:Position-wise distribution over the four sentence types for our internally trained model (left two panels) and the open-source DeepSeek-Distill-Qwen3-8B (right two panels).The x-axis denotes the sentence position, and the y-axis denotes the predicted probabilities of the four sentence types. Solid lines (—–) indicate probabilities when the answer is correct, while dashed lines(- - -) indicate probabilities when the answer is incorrect.Δ\\Deltadenotes the area difference between the solid and dashed curves, which reflects the influence of each sentence type on answer correctness.",
                "position": 560
            }
        ]
    },
    {
        "header": "5Mixed-policy Distillation",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09088/x10.png",
                "caption": "Figure 7:The ratio between cut-off responses under different token lengths.",
                "position": 699
            }
        ]
    },
    {
        "header": "6Overall Training Recipe",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09088/x11.png",
                "caption": "Figure 8:Data distribution.",
                "position": 816
            }
        ]
    },
    {
        "header": "7Experimental Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09088/x12.png",
                "caption": "Figure 9:Performance versus model size on AIME25 (left) and LCB v5 (right). Each point denotes a model, with the x-axis representing model size (in number of parameters) and the y-axis indicating benchmark score. Points positioned toward the top-left indicate superior efficiency—i.e., higher performance at smaller scale.",
                "position": 991
            },
            {
                "img": "https://arxiv.org/html/2601.09088/x12.png",
                "caption": "",
                "position": 994
            },
            {
                "img": "https://arxiv.org/html/2601.09088/x13.png",
                "caption": "",
                "position": 998
            },
            {
                "img": "https://arxiv.org/html/2601.09088/x14.png",
                "caption": "Figure 10:Comparison of response probability distributions with/without divergence-aware sampling using different temperatures.",
                "position": 1283
            }
        ]
    },
    {
        "header": "8Conclusion and Future Work",
        "images": []
    },
    {
        "header": "9Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]