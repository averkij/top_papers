[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.09978/x1.png",
                "caption": "",
                "position": 71
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.09978/x2.png",
                "caption": "Figure 2:The overview of our method. We follow a render-edit-aggregate optimization pipeline as in Instruct-NeRF2NeRF[9]. We introduce a Weighted Alpha Blending Equation (WABE) to overcome the motion occlusion problem and our novel loss functions to enhance the spatial-temporal consistency. Our edited avatars can generate high-quality and consistent 4D renderings and can be controlled by other actors.",
                "position": 119
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Preliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.09978/x3.png",
                "caption": "Figure 3:Illustration of the Weighted alpha blending equation (WABE), which is adjusted to suppress non-visible parts while enhancing visible parts. Lower left: results when WABE is disabled. Lower right: when WABE is enabled, motion-occluded regions like teeth and tongue can be successfully optimized.",
                "position": 191
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.09978/x4.png",
                "caption": "Figure 4:Our results on novel view synthesis. We show our edited results using the text prompt“Turn her into the Tolkien Elf”.",
                "position": 226
            },
            {
                "img": "https://arxiv.org/html/2501.09978/x5.png",
                "caption": "Figure 5:Comparison on novel view synthesis. Our method produces more high-quality and multi-view consistent results than baselines.",
                "position": 230
            },
            {
                "img": "https://arxiv.org/html/2501.09978/x6.png",
                "caption": "Figure 6:Our results on self-reenactment. Self-reenactment renders held-out unseen head pose and expressions from 16 training camera viewpoints. The bottom part shows the text prompts.",
                "position": 290
            },
            {
                "img": "https://arxiv.org/html/2501.09978/x7.png",
                "caption": "Figure 7:Comparison of self-reenactment. Our edited avatar can correctly produce detailed facial features under unseen expressions and head poses from the same subject.",
                "position": 294
            },
            {
                "img": "https://arxiv.org/html/2501.09978/x8.png",
                "caption": "Figure 8:Our results on cross-identity reenactment. Cross-identity reenactment animates the avatar to render images with unseen head poses and expressions from sequences of a different actor. The bottom part shows the text prompts.",
                "position": 358
            },
            {
                "img": "https://arxiv.org/html/2501.09978/x9.png",
                "caption": "Figure 9:Comparison of Cross-identity reenactment. Different edited avatars are controlled by the same source actor. Our method can render high-quality results with novel expressions, while baseline methods suffer from artifacts.",
                "position": 362
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.09978/x10.png",
                "caption": "Figure 10:Ablation study of WABE.",
                "position": 404
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]