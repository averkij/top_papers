[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13816/x1.png",
                "caption": "Figure 1:Our goal is to analyze LLMs’ cognition of knowledge boundaries across different languages by inspecting their representations of knowledge boundaries in multiple languages. The right-hand side of the figure illustrates the representations when an LLM encodes known and unknown questions in English and Lao.",
                "position": 141
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Datasets",
        "images": []
    },
    {
        "header": "4Knowledge Boundary Probing",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13816/x2.png",
                "caption": "Figure 2:In-distribution and OOD performance of layer-wise probes trained onQwen2.5-7brepresentations, across different methods. All scores are averaged across all languages.",
                "position": 254
            },
            {
                "img": "https://arxiv.org/html/2504.13816/x3.png",
                "caption": "Figure 3:The performance of probing models on the 3rdand 19thlayers ofQwen2.5-7Bwith different training and testing language combinations. The y-axis and x-axis represent the languages used for training and inference of the probing models, respectively.",
                "position": 285
            },
            {
                "img": "https://arxiv.org/html/2504.13816/x4.png",
                "caption": "",
                "position": 289
            }
        ]
    },
    {
        "header": "5Training-free Transfer",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13816/extracted/6372948/LDA.png",
                "caption": "Figure 4:We project the 19thlayer ofQwen2.5-7Bonto a linear subspace where x-axis encodes languages, y-axis encodes topic-aware true/falsity, and z-axis encodes binary truth/falsity. We visualize the same scatter plot with three different ground-truth label sets annotated respectively.",
                "position": 407
            },
            {
                "img": "https://arxiv.org/html/2504.13816/x5.png",
                "caption": "Figure 5:Performance of layer-wise probes trained on Khmer for different languages post-projection. We use linear projection to project each language’s representations onto the subspace of Khmer.\nIt is observed that Khmer probes perform better on OOD languages than on the Khmer language itself.",
                "position": 457
            },
            {
                "img": "https://arxiv.org/html/2504.13816/x6.png",
                "caption": "Figure 6:After SFT’ed on Khmer-English translation pairs,Qwen2.5-7Bexhibits unexpected enhancement in Chinese representations, which otherwise would achieve∼0.90similar-toabsent0.90\\sim 0.90∼ 0.90or lower accuracy across probes.",
                "position": 525
            }
        ]
    },
    {
        "header": "6Training-based Enhancement",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALanguage-neutrality of Entity-aware binary Answerability",
        "images": []
    },
    {
        "header": "Appendix BTransferability across Layers Full Results",
        "images": []
    },
    {
        "header": "Appendix CTransferability Pattern Change after SFT’ed on Question Pairs",
        "images": []
    },
    {
        "header": "Appendix DPattern 2 Full Visualization",
        "images": []
    },
    {
        "header": "Appendix EImplementation Details of SFT-based Method",
        "images": []
    },
    {
        "header": "Appendix FDetails of Data Construction ofFreshQAParallel",
        "images": []
    },
    {
        "header": "Appendix GDetails of Data Construction ofSeaRefuse",
        "images": []
    },
    {
        "header": "Appendix HAdditional Results onSeaRefuse",
        "images": []
    },
    {
        "header": "Appendix IAdditional Results onFreshQAParallel",
        "images": []
    },
    {
        "header": "Appendix JNon-parallel Approximation for Mean-shifting",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13816/x7.png",
                "caption": "Figure 7:Projecting Qwen-2.5-7B’s 19thlayer representations onto two languages axes (x,y), and one answerability axis (z) - determined by whether the entities in the question exist in real world.",
                "position": 1155
            },
            {
                "img": "https://arxiv.org/html/2504.13816/x8.png",
                "caption": "Figure 8:7B model performance on FreshQA before and after bilingual translation training on km-en question pairs. The results are averaged across all languages, aligning with the setting throughout the paper.",
                "position": 1158
            },
            {
                "img": "https://arxiv.org/html/2504.13816/x9.png",
                "caption": "Figure 9:72B model performance on FreshQA before and after bilingual translation training on km-en question pairs. The results are averaged across all languages, aligning with the setting throughout the paper.",
                "position": 1161
            },
            {
                "img": "https://arxiv.org/html/2504.13816/x10.png",
                "caption": "Figure 10:Llama-3.1-8B training-Free alignment methods performance.",
                "position": 1164
            },
            {
                "img": "https://arxiv.org/html/2504.13816/x11.png",
                "caption": "Figure 11:Llama-3.1-8B training-Free alignment methods performance, after the model has been SFT’ed on Khmer-Thai translation.",
                "position": 1167
            },
            {
                "img": "https://arxiv.org/html/2504.13816/x12.png",
                "caption": "Figure 12:Llama-3.1-8B training-Free alignment methods performance, after the model has been SFT’ed on Lao-English translation.",
                "position": 1170
            },
            {
                "img": "https://arxiv.org/html/2504.13816/x13.png",
                "caption": "Figure 13:14B model performance on our augmentedFreshQAParallelbefore and after bilingual translation training on km-en question pairs.",
                "position": 1173
            },
            {
                "img": "https://arxiv.org/html/2504.13816/x14.png",
                "caption": "Figure 14:Full transferability results for probes trained on all languages, afterQwen2.5-7Bhas been fine-tuned on Khmer-English translation.",
                "position": 1177
            }
        ]
    },
    {
        "header": "Appendix KPooling Methods",
        "images": []
    }
]