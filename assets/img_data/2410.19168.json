[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.19168/extracted/5950092/figures/mmau_logo.png",
                "caption": "",
                "position": 130
            },
            {
                "img": "https://arxiv.org/html/2410.19168/x1.png",
                "caption": "Figure 1:Overview of the MMAU Benchmark. MMAU provides comprehensive coverage across three key domains: speech, sounds, and music, featuring diverse audio samples. It challenges multimodal LLMs with tasks across 27 distinct skills, requiring advanced audio perception, reasoning, and domain-specific knowledge.",
                "position": 147
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.19168/x2.png",
                "caption": "Figure 2:Examples from the MMAU benchmark illustrating the diverse range of reasoning and information extraction tasks across the domains of sound, speech, and music. Each task involves rich, context-specific audio paired with human-annotated QA pairs that require expert-level knowledge and reasoning abilities. The benchmark covers a wide range of challenges, illustrating the breadth and depth of MMAU’s evaluation scope.",
                "position": 160
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.19168/x3.png",
                "caption": "Figure 3:(Left)Distribution of skills required for information extraction questions in the MMAU benchmark across the domains of sound, speech, and music.(Right)Distribution of skills required for reasoning questions in the MMAU benchmark across the same domains. Each question in MMAU demands the model to apply one or more of these skills to generate a reliable and accurate response. AppendixHprovides example questions demanding these skills and the specific tasks associated with them. This chart underscores the diverse cognitive abilities necessary for success in the benchmark, mirroring the complexity and expert-level reasoning involved.",
                "position": 198
            },
            {
                "img": "https://arxiv.org/html/2410.19168/x4.png",
                "caption": "",
                "position": 207
            }
        ]
    },
    {
        "header": "3The MMAU Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.19168/x5.png",
                "caption": "Figure 4:MMAU Benchmark Construction Pipeline.",
                "position": 310
            },
            {
                "img": "https://arxiv.org/html/2410.19168/x6.png",
                "caption": "Table 2:Comparison of MMAU with existing audio understanding and reasoning benchmarks across various statistics. MMAU covers all three domains—speech, sound, and music—while having the highest number of information extraction and complex reasoning tasks.",
                "position": 313
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.19168/x6.png",
                "caption": "Table 3:Performance comparison of various LALMs and LLMs on the test subset of MMAU across sound, speech, and music domains. Human evaluation results are shown for the MMAU test-mini split. We also mark if the training data used to train these models include either of speech, sound or music. The best-performing models in each category are highlighted inbold, and the second-best scores areunderlined.",
                "position": 553
            }
        ]
    },
    {
        "header": "5Results and Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.19168/x6.png",
                "caption": "Figure 5:Performance comparison on the MMAU test with Gaussian noise replacing the original audio. Models like MuLLaMa and SALMONN show little change in performance, indicating limited reliance on audio input, while others show a significant drop, suggesting greater audio dependence.",
                "position": 929
            },
            {
                "img": "https://arxiv.org/html/2410.19168/x7.png",
                "caption": "Figure 6:Accuracy distribution for Gemini Pro across easy, medium, and hard questions, categorized by skill type. The graph highlights how LALMs excel in some skills across all difficulty levels (e.g., Phonemic Stress Pattern Analysis) but struggle with others (e.g., Temporal Reasoning) regardless of difficulty.",
                "position": 946
            },
            {
                "img": "https://arxiv.org/html/2410.19168/x8.png",
                "caption": "Figure 7:Distribution of human-annotated error types across 500 instances for Qwen2-Audio-Instruct (Left) and Gemini Prov1.5(Right). AppendixKprovides detailed definitions of each error type.",
                "position": 959
            },
            {
                "img": "https://arxiv.org/html/2410.19168/x9.png",
                "caption": "",
                "position": 968
            }
        ]
    },
    {
        "header": "6Conclusion, Limitations and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    },
    {
        "header": "Appendix BAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.19168/x10.png",
                "caption": "Figure 8:Qualitative analysis of the options selected by MS-CLAP. Correct results are highlighted in green, while predicted results are shown in red. MS CLAP behaves like a bag-of-words model when selecting the correct options.",
                "position": 2193
            },
            {
                "img": "https://arxiv.org/html/2410.19168/extracted/5950092/figures/interface.png",
                "caption": "Table 5:Comparison of ALEs and LALMs Performance Across Multiple Difficulty Levels",
                "position": 2203
            }
        ]
    },
    {
        "header": "Appendix CAnnotation Details",
        "images": []
    },
    {
        "header": "Appendix DModel Details",
        "images": []
    },
    {
        "header": "Appendix EDataset Details",
        "images": []
    },
    {
        "header": "Appendix FAnnotation Tool",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.19168/extracted/5950092/figures/interface.png",
                "caption": "Figure 9:Snapshot of the annotation tool used by the annotators to annotate the correct answers for each audio-question pair.",
                "position": 2550
            }
        ]
    },
    {
        "header": "Appendix GComparison",
        "images": []
    },
    {
        "header": "Appendix HAdditional Information on Skills",
        "images": []
    },
    {
        "header": "Appendix IFailure cases",
        "images": []
    },
    {
        "header": "Appendix JBenchmark Evaluation",
        "images": []
    },
    {
        "header": "Appendix KAdditional Details on Error Types",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.19168/x11.png",
                "caption": "Figure 10:Prompts/Instructions used for generating contrasting options for MMAU.",
                "position": 3984
            },
            {
                "img": "https://arxiv.org/html/2410.19168/x12.png",
                "caption": "Figure 11:Prompts/Instructions used for generating captions using Qwen2-Audio.",
                "position": 3987
            },
            {
                "img": "https://arxiv.org/html/2410.19168/x13.png",
                "caption": "Figure 12:Prompts/Instructions used for generating hypothesis using question-choice pairs.",
                "position": 3990
            }
        ]
    },
    {
        "header": "Appendix LPrompts",
        "images": []
    }
]