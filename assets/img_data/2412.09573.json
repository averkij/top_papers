[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09573/x1.png",
                "caption": "Figure 1:Givenuncalibratedsparse-view images, our FreeSplatter can reconstruct pixel-wise 3D Gaussians, enabling both high-fidelity novel view rendering and instant camera pose estimation in mere seconds. FreeSplatter can deal with both object-centric (up) and scene-level (down) scenarios.",
                "position": 84
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09573/x2.png",
                "caption": "Figure 2:FreeSplatter pipeline.Given input views{ùë∞n‚à£n=1,‚Ä¶,N}conditional-setsuperscriptùë∞ùëõùëõ1‚Ä¶ùëÅ\\left\\{{\\bm{I}}^{n}\\mid n=1,\\ldots,N\\right\\}{ bold_italic_I start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ‚à£ italic_n = 1 , ‚Ä¶ , italic_N }without any known camera extrinsics or intrinsics, we first patchify them into image tokens, and then feed all tokens into a sequence of self-attention blocks to exchange information among multiple views. Finally, we decode the output image tokens intoNùëÅNitalic_NGaussian maps{ùëÆn‚à£n=1,‚Ä¶,N}conditional-setsuperscriptùëÆùëõùëõ1‚Ä¶ùëÅ\\left\\{{\\bm{G}}^{n}\\mid n=1,\\ldots,N\\right\\}{ bold_italic_G start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ‚à£ italic_n = 1 , ‚Ä¶ , italic_N }, from which we can render novel views, as well as recovering camera focal lengthfùëìfitalic_fand poses{ùë∑n‚à£n=1,‚Ä¶,N}conditional-setsuperscriptùë∑ùëõùëõ1‚Ä¶ùëÅ\\left\\{{\\bm{P}}^{n}\\mid n=1,\\ldots,N\\right\\}{ bold_italic_P start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ‚à£ italic_n = 1 , ‚Ä¶ , italic_N }with simple iterative solvers.",
                "position": 138
            },
            {
                "img": "https://arxiv.org/html/2412.09573/x3.png",
                "caption": "Figure 3:Object-centric Sparse-view Reconstruction.We show 6 samples from the Google Scanned Objects dataset. To be noted, the results of LGM and InstantMesh (\\nth2 and\\nth3 rows) are generated with ground truth camera poses (and intrinsics), while our results (\\nth4 row) are generated in a completely pose-free manner.",
                "position": 214
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09573/x4.png",
                "caption": "Figure 4:Scene-level Reconstruction on ScanNet++.The results of pixelSplat and MVSplat are obtained with ground truth input poses, while the results of Splat3R and ours are pose-free.",
                "position": 292
            },
            {
                "img": "https://arxiv.org/html/2412.09573/x5.png",
                "caption": "Figure 5:Scene-level Reconstruction on CO3Dv2.",
                "position": 313
            },
            {
                "img": "https://arxiv.org/html/2412.09573/x6.png",
                "caption": "Figure 6:3D content creation with FreeSplatter.\\nth1 and\\nth2 rows: Image-to-3D results using Zero123++ (input image, Gaussian visualization, two novel views).\\nth3 row: Text-to-3D results using MVDream (prompt shown above; two Gaussian visualizations, two novel views).",
                "position": 623
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09573/x7.png",
                "caption": "Figure 7:Comparison with PF-LRM on its evaluation datasets.The test samples in the first 3 rows are from the GSO evaluation set of PF-LRM, while the samples in the last 4 rows are from the OmniObject3D evaluation set of PF-LRM. Our FreeSplatter-O synthesizes significantly better visual details than PF-LRM.",
                "position": 1912
            },
            {
                "img": "https://arxiv.org/html/2412.09573/x8.png",
                "caption": "Figure 8:Comparison on image-to-3D generation with Zero123++ v1.2(Shi et¬†al.,2023).",
                "position": 1931
            },
            {
                "img": "https://arxiv.org/html/2412.09573/x9.png",
                "caption": "Figure 9:Comparison on image-to-3D generation with Hunyuan3D Std(Yang et¬†al.,2024b).",
                "position": 1934
            },
            {
                "img": "https://arxiv.org/html/2412.09573/x10.png",
                "caption": "Figure 10:Use input image to enhance the image-to-3D generation results with Zero123++ v1.1(Shi et¬†al.,2023).The input image is often more high-quality and contains richer visual details than the generated views, but its camera pose is unknown, making it impossible for pose-dependent LRMs to leverage it. The capability of using the input image alongside generated views of our FreeSplatter is particularly valuable for challenging content like human faces, where Zero123++ often struggles to generate.",
                "position": 1937
            },
            {
                "img": "https://arxiv.org/html/2412.09573/x11.png",
                "caption": "Figure 11:Our FreeSplatter can faithfully recover the pre-defined camera poses of existing multi-view diffusion models.We use gray pyramids to visualize the ground truth pre-defined camera poses of the diffusion models, and colorful pyramids to visualize the estimated poses.œÜùúë\\varphiitalic_œÜandŒ∏ùúÉ\\thetaitalic_Œ∏denote the pre-defined azimuth and elevation angles, respectively. Since Zero123++ v1.2 and Hunyuan3D Std generate images at pre-defined fixed Field-of-View (fov), we mark their pre-defined fov angles and corresponding focal lengths (in pixel units) too.",
                "position": 1940
            },
            {
                "img": "https://arxiv.org/html/2412.09573/x12.png",
                "caption": "Figure 12:Zero-shot pose-free reconstruction results on ABO and OmniObject3D.Both datasets are unseen for FreeSplatter-O. Our model can faithfully estimate the input camera poses and render high-fidelity novel views.",
                "position": 2054
            },
            {
                "img": "https://arxiv.org/html/2412.09573/x13.png",
                "caption": "Figure 13:Zero-shot pose-free reconstruction and view synthesis results on RealEstate10K.Our FreeSplatter-S model was not trained on RealEstate10K, we directly utilize it for zero-shot view synthesis on this dataset. We can observe that our model can faithfully reconstruct the input views at the estimated input poses, and the rendered novel views align well with the ground truth.",
                "position": 2057
            },
            {
                "img": "https://arxiv.org/html/2412.09573/x14.png",
                "caption": "Figure 14:Zero-shot generalization of FreeSplatter-S on various datasets.We show 2 examples for DTU, MVImgNet, Tanks & Temples and Sora-generated videos, respectively.",
                "position": 2060
            },
            {
                "img": "https://arxiv.org/html/2412.09573/x15.png",
                "caption": "Figure 15:Ablation on pixel-alignment loss.We show two samples from the GSO dataset.",
                "position": 2123
            },
            {
                "img": "https://arxiv.org/html/2412.09573/x16.png",
                "caption": "Figure 16:Illustration on the influence of input view number.We show the visual comparison of FreeSplatter-O results with varying numbers of input views (n=1‚àí6ùëõ16n=1-6italic_n = 1 - 6). From left to right: input views, reconstructed Gaussians, and rendered target views at 4 fixed viewpoints. Additional input views increase Gaussian density and improve previously uncovered regions, with diminishing returns beyond n=4 when object coverage becomes sufficient.",
                "position": 2126
            },
            {
                "img": "https://arxiv.org/html/2412.09573/x17.png",
                "caption": "Figure 17:Ablation study on view embedding addition.Red/blue boxes indicate views added with reference/source view embeddings respectively. For each case, we visualize the image rendered with identity camera (i.e., reference pose) on the right.",
                "position": 2129
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]