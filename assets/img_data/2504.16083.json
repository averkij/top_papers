[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16083/x1.png",
                "caption": "Figure 1:Dynamic sparse attention pipelines leverage sparse loading with dense computation(Zheng et al.,2023)to enable hardware-efficient acceleration. MMInference adopts a bottom-up system-algorithm co-design that accounting for both the mathematical equivalence constraints of sparse loading and the locality properties of real-world attention patterns.",
                "position": 269
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16083/x2.png",
                "caption": "(a)VLMs’ attention incurs heavy cost.",
                "position": 279
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x2.png",
                "caption": "(a)VLMs’ attention incurs heavy cost.",
                "position": 282
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x3.png",
                "caption": "(b)VLMs’ attention is sparse.",
                "position": 288
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x4.png",
                "caption": "(c)Sparsity of VLMs’ attention is dynamic.",
                "position": 294
            }
        ]
    },
    {
        "header": "2Attention Heads in VLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16083/x5.png",
                "caption": "(a)Grid pattern.",
                "position": 325
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x5.png",
                "caption": "(a)Grid pattern.",
                "position": 328
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x6.png",
                "caption": "(b)Q-Boundary pattern.",
                "position": 334
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x7.png",
                "caption": "(c)2D-Boundary pattern.",
                "position": 340
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x8.png",
                "caption": "(d)Permuted Grid pattern.",
                "position": 346
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x9.png",
                "caption": "(e)Permuted Q-Boundary pattern.",
                "position": 352
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x10.png",
                "caption": "(f)Permuted 2D-Boundary pattern.",
                "position": 358
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x11.png",
                "caption": "Figure 4:The framework of MMInference, encompassing both inter- and intra-modality sparse attention patterns.",
                "position": 385
            }
        ]
    },
    {
        "header": "3MMInference",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16083/x12.png",
                "caption": "(a)MMInference in V-NIAH",
                "position": 911
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x12.png",
                "caption": "(a)MMInference in V-NIAH",
                "position": 914
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x13.png",
                "caption": "(b)FullAttention in V-NIAH",
                "position": 920
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x14.png",
                "caption": "(c)MMInference in MM-NIAH",
                "position": 926
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x15.png",
                "caption": "(d)FullAttention in MM-NIAH",
                "position": 932
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x16.png",
                "caption": "(a)All Textual Context",
                "position": 963
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x16.png",
                "caption": "(a)All Textual Context",
                "position": 966
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x17.png",
                "caption": "(b)Visual Context Inserted",
                "position": 971
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x18.png",
                "caption": "(c)More Visual Context",
                "position": 977
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x19.png",
                "caption": "(d)All Visual Context",
                "position": 982
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x20.png",
                "caption": "Figure 7:End-to-End Latency.",
                "position": 1036
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x21.png",
                "caption": "Figure 8:The sparse index does not effectively extrapolate from text to the visual modality. However, an index built within the same modality can generalize across modality boundaries.",
                "position": 1099
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AModality-Aware Sparse Attention Search Algorithm",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16083/x22.png",
                "caption": "(a)K-Boundary pattern.",
                "position": 2129
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x22.png",
                "caption": "(a)K-Boundary pattern.",
                "position": 2132
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x23.png",
                "caption": "(b)No-Boundary pattern.",
                "position": 2137
            }
        ]
    },
    {
        "header": "Appendix BPattern Analysis",
        "images": []
    },
    {
        "header": "Appendix CExperiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16083/x24.png",
                "caption": "(a)A-shape",
                "position": 2176
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x24.png",
                "caption": "(a)A-shape",
                "position": 2179
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x25.png",
                "caption": "(b)SF-fixed",
                "position": 2184
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x26.png",
                "caption": "(c)SF-strided",
                "position": 2189
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x27.png",
                "caption": "(d)Tri-shape",
                "position": 2195
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x28.png",
                "caption": "(e)Vertical-Slash (MInference)",
                "position": 2200
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x29.png",
                "caption": "(a)Before Permutation",
                "position": 2280
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x29.png",
                "caption": "(a)Before Permutation",
                "position": 2283
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x30.png",
                "caption": "(b)Row-wise Permutation",
                "position": 2288
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x31.png",
                "caption": "(c)Column-wise Permutation",
                "position": 2294
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x32.png",
                "caption": "(a)Mix-modality",
                "position": 2301
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x32.png",
                "caption": "(a)Mix-modality",
                "position": 2304
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x33.png",
                "caption": "(b)Q-wise Permutation",
                "position": 2309
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x34.png",
                "caption": "(c)K-wise Permutation",
                "position": 2314
            }
        ]
    },
    {
        "header": "Appendix DBenchmark Details",
        "images": []
    },
    {
        "header": "Appendix EAdditional Experiments Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16083/x35.png",
                "caption": "(a)A-shape",
                "position": 2466
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x35.png",
                "caption": "(a)A-shape",
                "position": 2469
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x36.png",
                "caption": "(b)Tri-shape",
                "position": 2474
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x37.png",
                "caption": "(c)SF-fixed",
                "position": 2480
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x38.png",
                "caption": "(d)SF-strided",
                "position": 2485
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x39.png",
                "caption": "(e)MInference",
                "position": 2491
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x40.png",
                "caption": "(a)A-shape",
                "position": 2506
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x40.png",
                "caption": "(a)A-shape",
                "position": 2509
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x41.png",
                "caption": "(b)Tri-shape",
                "position": 2514
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x42.png",
                "caption": "(c)MInference",
                "position": 2520
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x43.png",
                "caption": "(d)MMInference w/o Inter-modality",
                "position": 2525
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x44.png",
                "caption": "(a)Natten",
                "position": 2540
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x44.png",
                "caption": "(a)Natten",
                "position": 2543
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x45.png",
                "caption": "(b)Permutated Natten",
                "position": 2548
            }
        ]
    },
    {
        "header": "Appendix FSparse Attention in DiT",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16083/x46.png",
                "caption": "Figure 16:The latency breakdown of a single attention kernel for four sparse attention patterns and FlashAttention(Dao,2024)across different context windows in a single A100, including the index time for dynamic sparse approximation and building dynamic sparsity. At 1M tokens, the latency for Grid is 358ms.",
                "position": 2569
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x47.png",
                "caption": "(a)Qwen2.5-VL on EgoSchema",
                "position": 2971
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x47.png",
                "caption": "(a)Qwen2.5-VL on EgoSchema",
                "position": 2974
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x48.png",
                "caption": "(b)VideoChat on EgoSchema",
                "position": 2980
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x49.png",
                "caption": "(c)Qwen2.5-VL on VideoMME",
                "position": 2986
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x50.png",
                "caption": "(d)VideoChat on VideoMME",
                "position": 2992
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x51.png",
                "caption": "(a)Qwen2.5-VL on Mix-modality",
                "position": 2999
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x51.png",
                "caption": "(a)Qwen2.5-VL on Mix-modality",
                "position": 3002
            },
            {
                "img": "https://arxiv.org/html/2504.16083/x52.png",
                "caption": "(b)VideoChat on Mix-modality",
                "position": 3008
            }
        ]
    },
    {
        "header": "Appendix GKernel Implementation",
        "images": []
    }
]