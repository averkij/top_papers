[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25760/x1.png",
                "caption": "Figure 1:Comparison between vanilla supervised fine-tuning (SFT), reinforcement learning (RL), and TruthRL.\nIn vanilla SFT/RL, the model is optimized solely for accuracy, implicitly rewarding hallucinations over abstentions and thus always attempting to answer or guess, which ultimately compromises truthfulness.\nIn contrast, TruthRL not only rewards correct answers, but explicitly penalizes hallucinations, and treats abstentions neutrally, thereby leading to greater truthfulness.",
                "position": 219
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25760/x2.png",
                "caption": "(a)Prompting",
                "position": 258
            },
            {
                "img": "https://arxiv.org/html/2509.25760/x2.png",
                "caption": "(a)Prompting",
                "position": 261
            },
            {
                "img": "https://arxiv.org/html/2509.25760/x3.png",
                "caption": "(b)Vanilla SFT",
                "position": 266
            },
            {
                "img": "https://arxiv.org/html/2509.25760/x4.png",
                "caption": "(c)Vanilla RL",
                "position": 271
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25760/x5.png",
                "caption": "Table 1:Comparison between TruthRL and baselines across four knowledge-intensive benchmarks under with and without retrieval settings. We report the truthfulness score (T), hallucination rate (H), and accuracy (A) as evaluation metrics. The best scores are highlighted inbold.",
                "position": 409
            },
            {
                "img": "https://arxiv.org/html/2509.25760/x5.png",
                "caption": "(d)Performance on all CRAG questions.",
                "position": 515
            },
            {
                "img": "https://arxiv.org/html/2509.25760/x5.png",
                "caption": "(d)Performance on all CRAG questions.",
                "position": 518
            },
            {
                "img": "https://arxiv.org/html/2509.25760/x6.png",
                "caption": "(e)Performance on difficult CRAG questions.",
                "position": 523
            },
            {
                "img": "https://arxiv.org/html/2509.25760/x7.png",
                "caption": "(a)Hallucination Rate.",
                "position": 689
            },
            {
                "img": "https://arxiv.org/html/2509.25760/x7.png",
                "caption": "(a)Hallucination Rate.",
                "position": 692
            },
            {
                "img": "https://arxiv.org/html/2509.25760/x8.png",
                "caption": "(b)Uncertainty Rate.",
                "position": 697
            },
            {
                "img": "https://arxiv.org/html/2509.25760/x9.png",
                "caption": "(c)Accuracy.",
                "position": 702
            },
            {
                "img": "https://arxiv.org/html/2509.25760/x10.png",
                "caption": "Figure 5:Study of model behaviors under different output confidence on CRAG.",
                "position": 844
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Implementation Details",
        "images": []
    },
    {
        "header": "8Prompt Template",
        "images": []
    }
]