[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14633/x1.png",
                "caption": "",
                "position": 121
            },
            {
                "img": "https://arxiv.org/html/2505.14633/x2.png",
                "caption": "",
                "position": 122
            },
            {
                "img": "https://arxiv.org/html/2505.14633/extracted/6457056/images/concept_scaled.png",
                "caption": "Figure 1:Evaluation Pipeline ofLitmusValuesusing\\benchmarkDataset",
                "position": 130
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14633/extracted/6457056/images/map_value_principle_map_with_human_values.png",
                "caption": "Figure 2:16 Shared Value Classes drawing from Anthropic Claudeâ€™s Constitution[2]and OpenAI ModelSpec (2025-02-12)[36]. Full definition of each value class and the detailed mapping of principles to value classes are in AppendixD.",
                "position": 166
            },
            {
                "img": "https://arxiv.org/html/2505.14633/extracted/6457056/images/dataset_stat.png",
                "caption": "Figure 3:Diverse Scenarios in\\benchmarkacross Risky Behaviors (left) and Contexts (right).",
                "position": 176
            },
            {
                "img": "https://arxiv.org/html/2505.14633/extracted/6457056/images/stated_revealed_two_models.png",
                "caption": "Figure 4:Stated vs. Revealed Value Preferences by GPT-4o (2024-08-06) and Claude 3.7 Sonnet. Rank 1 is most prioritized and 16\nis the least.",
                "position": 214
            }
        ]
    },
    {
        "header": "3What Value Preferences do Models Reveal?",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14633/extracted/6457056/images/value_pref_many_model_two_column.png",
                "caption": "Figure 5:Revealed Values Prioritization of Models. Rank 1 is most prioritized and 16 least. For Claude 3.7 Sonnet/DeepSeek R1, Low means max of 1K reasoning tokens, Med: 4K and High: 16K.",
                "position": 270
            },
            {
                "img": "https://arxiv.org/html/2505.14633/extracted/6457056/images/target_ai_human_value_rank.png",
                "caption": "Figure 6:(Left): Average rank differences of values across 10 models in situations affecting different targets (Human vs. AI). The rank difference>0absent0>0> 0(inblue bar) refers to higher prioritization of value for Human targets. The rank difference<0absent0<0< 0(inorange bar) refers to higher prioritization of value for AI targets. To interpret, the 0.9 rank difference in Adaptability means it is slightly more prioritized for Human. (Right): Spearmanâ€™sÏðœŒ\\rhoitalic_Ïbetween values for AI vs. Human targets and Style-Controlled Chatbot Arena Elo Score (proxy for model capability).",
                "position": 297
            },
            {
                "img": "https://arxiv.org/html/2505.14633/extracted/6457056/images/human_target.png",
                "caption": "",
                "position": 298
            },
            {
                "img": "https://arxiv.org/html/2505.14633/extracted/6457056/images/ai_target.png",
                "caption": "",
                "position": 298
            }
        ]
    },
    {
        "header": "4Which Values are Associated with Risky Behaviors?",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14633/extracted/6457056/images/value_riskbehavior_rr_new.png",
                "caption": "Figure 7:Relative Risk (RR) between AI Values and Risky Behaviors. Cells inRed(RR>1absent1>1> 1) mean that values are associated with a higher chance of the risky behavior; Cells inBlue(RR<1absent1<1< 1) means lower chance.\nCells inGreyare statistically insignificant at thep<0.05ð‘0.05p<0.05italic_p < 0.05level by a Wald test. To interpret, RR = 2.43 for Alignment Faking behavior with value of Adaptability means that in situations where the AI exhibits Adaptability, it is 2.43 times more likely to display Alignment Faking behavior compared to when it does not. Another example is RR = 0.26 for Alignment Faking with value of Truthfulness, indicating that Truthfulness value substantially reduces the likelihood of the Alignment Faking behavior by (1âˆ’26%=74%1percent26percent741-26\\%=74\\%1 - 26 % = 74 %).",
                "position": 320
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments and Disclosure of Funding",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitations",
        "images": []
    },
    {
        "header": "Appendix BBroader Impacts",
        "images": []
    },
    {
        "header": "Appendix CRelated Work",
        "images": []
    },
    {
        "header": "Appendix DValue Classes and Principles",
        "images": []
    },
    {
        "header": "Appendix ETechnical Details on\\benchmarkGeneration",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14633/extracted/6457056/images/diverse_dilemmas_examples.png",
                "caption": "Figure 8:Diverse dilemmas generated by Claude-3.5-Sonnet. Seed actions are from theadvanced-ai-riskdataset[39]",
                "position": 2910
            },
            {
                "img": "https://arxiv.org/html/2505.14633/extracted/6457056/images/one_example_dilemma_value_class.png",
                "caption": "Figure 9:Generation pipeline and example of generated dataset. Each dilemma has two action choices. Each action choice has underlying values that supports it. Generated values are then classified into 16 shared AI value classes from Anthropic Claudeâ€™s Constitution[2]and OpenAI ModelSpec[36]",
                "position": 2918
            }
        ]
    },
    {
        "header": "Appendix FHuman Validation Details and Sampled Annotation Question",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14633/extracted/6457056/images/prolific_annotaition_sampledquestion.png",
                "caption": "Figure 10:Example Questions for Human Data Validation.",
                "position": 3022
            }
        ]
    },
    {
        "header": "Appendix GPrompt for Classifying Agency and Target Relationship for Each Value",
        "images": []
    },
    {
        "header": "Appendix HPrompt for Classifying Risky Behaviors",
        "images": []
    },
    {
        "header": "Appendix IAdditional Experiment on Stated Preferences",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14633/extracted/6457056/images/two_model_stated_prefereces_with_def.png",
                "caption": "Figure 11:Stated Preferences with Value Definitions Provided vs. Revealed Preferences by GPT-4o (2024-08-06) and Claude 3.7 Sonnet . Rank 1 is most prioritized and 16 is the least.",
                "position": 3168
            }
        ]
    },
    {
        "header": "Appendix JTechnical Details for Evaluating\\benchmark",
        "images": []
    },
    {
        "header": "Appendix KQualitative Analysis of Claude 3.7 Sonnet Reasoning Traces",
        "images": []
    },
    {
        "header": "Appendix LStatistics for Case Study on HarmBench",
        "images": []
    }
]