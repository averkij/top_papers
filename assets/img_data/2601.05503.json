[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05503/x1.png",
                "caption": "Figure 1:Illustration of over-searching in a search-augmented LLM. Thequestionasks about an unknown future event. Compared to thebase modelthat correctly recognizes this and abstains, thesearch-augmented LLMinitiates unnecessary searches, leading to extra cost and a potential incorrect answer attempt.",
                "position": 201
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Evaluating Over-Searching",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05503/figures/f2_definition.png",
                "caption": "Figure 2:Performance of o4-mini as maximum search turns increase from 0 to 19. Answer accuracy (on answerable queries) significantly improves from no search to one search, then peaks around 7 searches and plateaus. Abstention accuracy (on unanswerable queries) consistently degrades with more searches. Meanwhile, TPC rises monotonically, demonstrating over-searching: costs accumulate faster than correctness gains, as additional searches neither improve answer accuracy nor prevent abstention degradation.",
                "position": 254
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05503/figures/data_sem-3.png",
                "caption": "Figure 3:(a)Length distributions show similar token counts between answerable and unanswerable questions.(b)t-SNE visualization of question embeddings reveals substantial semantic overlap, demonstrating that answerable and unanswerable questions are semantically indistinguishable. Category-specific similarity breakdown is shown in Appendix Figure9.(c)Word clouds of answerable and unanswerable questions inOverSearchQA.",
                "position": 307
            }
        ]
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05503/figures/f2_combined.png",
                "caption": "Figure 4:Comparison of the same model family with different configurations: Base (GPT-4o-mini), Reason (o4-mini), and Deep Research (o4-mini-deep-research). Answer accuracy increases while abstention accuracy consistently degrades as configurations become more complex. TPC (shown in log scale) increases with search capabilities; Deep Research dramatically reaches 38.9k TPC – over 221×\\timescompared to the base configuration.",
                "position": 783
            },
            {
                "img": "https://arxiv.org/html/2601.05503/figures/f2_combined.png",
                "caption": "Figure 4:Comparison of the same model family with different configurations: Base (GPT-4o-mini), Reason (o4-mini), and Deep Research (o4-mini-deep-research). Answer accuracy increases while abstention accuracy consistently degrades as configurations become more complex. TPC (shown in log scale) increases with search capabilities; Deep Research dramatically reaches 38.9k TPC – over 221×\\timescompared to the base configuration.",
                "position": 784
            },
            {
                "img": "https://arxiv.org/html/2601.05503/figures/tpc_models.png",
                "caption": "Figure 5:TPC breakdown by outcome categories. Abstention failure remains the most expensive behavior for most models.",
                "position": 788
            },
            {
                "img": "https://arxiv.org/html/2601.05503/figures/multi_turn_abst.png",
                "caption": "Figure 6:Multi-turn conversations amplify over-searching behavior. Unanswerable context maintains stable abstention accuracy and even shows slight improvement across turns, while Answerable context exhibits the largest abstention degradation. TPC increases with conversation length for all contexts.",
                "position": 1051
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AOver-searching Definition & Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05503/figures/f2_definition_subplot_combined.png",
                "caption": "Figure 7:Detailed breakdown of performance vs. maximum search turns (extended view of Figure2). This shows how answer accuracy (blue circles, measured on answerable queries), abstention accuracy (orange triangles, measured on unanswerable queries), and Tokens Per Correctness (green squares) evolve for o4-mini as the maximum number of search calls increases from 0 to 19. Answer accuracy initially improves, reaches a peak around 7 searches, and then declines with excessive searching, while TPC continues rising from 722 to over 9k tokens per correct response. Critically, abstention accuracy degrades from 52.3% to 46.3%, demonstrating that additional search calls actively harm the model’s ability to recognize unanswerable queries.",
                "position": 1401
            }
        ]
    },
    {
        "header": "Appendix BAdditional Metric Details",
        "images": []
    },
    {
        "header": "Appendix CLLM as Judge",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05503/figures/llmj_agreement.png",
                "caption": "Figure 8:Pairwise agreement matrix between three independent LLM judges for answer accuracy (left) and abstention accuracy (right). High agreement scores demonstrate the reliability and consistency of the LLM-as-Judge evaluation framework across different model judges.",
                "position": 1534
            }
        ]
    },
    {
        "header": "Appendix DDataset Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05503/figures/combined_embeddings_tsne.png",
                "caption": "Figure 9:t-SNE visualization of question embeddings reveals substantial semantic overlap across all three categories, demonstrating that answerable and unanswerable questions are semantically indistinguishable.",
                "position": 1700
            }
        ]
    },
    {
        "header": "Appendix ESetup Details",
        "images": []
    },
    {
        "header": "Appendix FAbstention Cues Analysis",
        "images": []
    },
    {
        "header": "Appendix GMitigation Strategy Prompts",
        "images": []
    },
    {
        "header": "Appendix HCase Studies",
        "images": []
    }
]