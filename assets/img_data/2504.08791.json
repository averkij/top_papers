[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Prima.cpp: Parallel Architecture and Scheduler Design",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.08791/x1.png",
                "caption": "Figure 1:Piped-ring parallelism. In this case, 6 devices handle a 36-layer model. With a layer window size of 2, the model is splitted into 18 segments, which are assigned to 6 devices in a ring order, so each device needs 3 rounds to predict one token.",
                "position": 315
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.08791/",
                "caption": "Figure 2:Normalized token latency overkùëòkitalic_k.",
                "position": 797
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.08791/x3.png",
                "caption": "Figure 3:Illustration of model layers loaded into memory in pipeline parallelism with prefetching. In this case, the device handles 6 model layers but its available memory can only hold 3. The green blocks show the layers loaded into memory, while white blocks indicate those not yet loaded.",
                "position": 1219
            },
            {
                "img": "https://arxiv.org/html/2504.08791/x4.png",
                "caption": "Figure 4:Illustration of model layers loaded into memory in piped-ring parallelism with fast disk.",
                "position": 1229
            },
            {
                "img": "https://arxiv.org/html/2504.08791/x5.png",
                "caption": "Figure 5:Illustration of model layers loaded into memory in piped-ring parallelism with slow disk.",
                "position": 1235
            },
            {
                "img": "https://arxiv.org/html/2504.08791/x6.png",
                "caption": "Figure 6:Timeline of (a,b) piped-ring parallelism on homogeneous devices with fast and slow disks; (c,e) piped-ring parallelism on heterogeneous devices with same and different window sizes; and (d,e) vanilla pipeline parallelism on heterogeneous devices with and without prefetching.",
                "position": 1242
            },
            {
                "img": "https://arxiv.org/html/2504.08791/x7.png",
                "caption": "Figure 7:Comparison of token latency and TTFT for llama.cpp, exo, dllama, and prima.cpp.",
                "position": 2395
            },
            {
                "img": "https://arxiv.org/html/2504.08791/x8.png",
                "caption": "Figure 8:Layer assignment and token latency over different number of devices.",
                "position": 2516
            },
            {
                "img": "https://arxiv.org/html/2504.08791/x9.png",
                "caption": "(a)",
                "position": 2539
            },
            {
                "img": "https://arxiv.org/html/2504.08791/x9.png",
                "caption": "(a)",
                "position": 2542
            },
            {
                "img": "https://arxiv.org/html/2504.08791/x10.png",
                "caption": "(b)",
                "position": 2548
            },
            {
                "img": "https://arxiv.org/html/2504.08791/x11.png",
                "caption": "(c)",
                "position": 2554
            },
            {
                "img": "https://arxiv.org/html/2504.08791/x12.png",
                "caption": "(d)",
                "position": 2560
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]