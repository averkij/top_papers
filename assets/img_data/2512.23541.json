[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23541/x1.png",
                "caption": "Figure 1:Method Overview.The model receives a visual goal (left), imagines how to achieve it via a goal-conditioned world model (top), and executes the planned actions in the real world (right).",
                "position": 91
            },
            {
                "img": "https://arxiv.org/html/2512.23541/x2.png",
                "caption": "Figure 2:System Overview.We propose Act2Goal, a goal-conditioned policy that integrates a visual world model with multi-scale temporal control to address long-horizon manipulation.\nAfter large-scale offline imitation learning, the model shows high performance on seen settings and strong generalization to unseen scenarios.\nThe reward-free online autonomous improvement stage further improve model’s performance through rollout-goal relabel-optimize loop.",
                "position": 95
            }
        ]
    },
    {
        "header": "IIRelated Works",
        "images": []
    },
    {
        "header": "IIIFrom World Model To General Goal-conditioned Policy",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23541/x3.png",
                "caption": "Figure 3:Model Architecture.This figure presents the network architecture of Act2Goal model. On the left, multi-view input frames, including current observation and goal, are encoded into latents via a video encoder and concatenated with noisy latents, then refined into MSTH latent frames through Video DiT blocks. On the right, the robot state and multi-scale features from the world model are fed via cross-attention into isomorphic Action DiT blocks, generating MSTH-structured actions.",
                "position": 188
            }
        ]
    },
    {
        "header": "IVExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23541/x4.png",
                "caption": "Figure 4:Real World Evaluation.This figure illustrates in-domain and out-of-domain test configurations for three real-world tasks: Whiteboard Word Writing, Dessert Plating, and Plug-In Operation. For each task, Head View Goal displays the target, while Model Rollouts shows the robot’s execution process; these setups are used to evaluate the model’s generalization ability using the success rate as the metric.",
                "position": 443
            },
            {
                "img": "https://arxiv.org/html/2512.23541/x5.png",
                "caption": "Figure 5:Online Autonomous Improvement Scenarios.This figure illustrates four OOD scenarios from the RoboTwin 2.0 benchmark, corresponding to the hard testing modes of Move Can Pot, Pick Dual Bottles, Place Empty Cup, and Place Shoe. These scenarios serve as the testbed for verifying the effectiveness of autonomous improvement.",
                "position": 677
            },
            {
                "img": "https://arxiv.org/html/2512.23541/x6.png",
                "caption": "Figure 6:Online Training Performance in Robotwin 2.0.This figure presents two key findings from Robotwin 2.0 simulations:\n(Left) Multi-round success rates of four hard-mode scenarios, showing consistent improvement over 3 rounds before convergence.\n(Right) Performance of three data selection strategies for rollouts: using all rollouts yields optimal results, while even failed-only rollouts enable noticeable improvement.",
                "position": 689
            },
            {
                "img": "https://arxiv.org/html/2512.23541/x7.png",
                "caption": "Figure 7:Online Training Performance in Real-World Unseen Scenarios.When tasked with drawing an unseen pattern, the model initially performs poorly. However, as online training progresses (from left to right, top to bottom), the drawing quality improves steadily.",
                "position": 698
            },
            {
                "img": "https://arxiv.org/html/2512.23541/x8.png",
                "caption": "Figure 8:Examples of generated videos.This figure illustrates the generation capability of our goal-conditioned world model through three head-view video clips. The left and right sides show the current observation and the target goal, respectively. For each sequence, we uniformly sample three proximal and three distal frames. The second row provides a zoomed-in view of the red-highlighted regions in the first row. Red arrows in the third and fourth rows highlight the motion of key objects. The distal frames in the bottom row correspond to longer temporal horizons compared to those in the third row.",
                "position": 702
            }
        ]
    },
    {
        "header": "VConclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    }
]