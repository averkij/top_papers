[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02204/x1.png",
                "caption": "Figure 1:Architecture of\\methodName.\\methodNameprocesses interleaved text-image discrete token sequences as input and generates interleaved multimodal outputs. Text tokens are predicted via next-token modeling, while visual tokens are generated through next-scale prediction.",
                "position": 222
            },
            {
                "img": "https://arxiv.org/html/2601.02204/x2.png",
                "caption": "Figure 2:\\methodNamevisualization.Our approach generates high-fidelity images via a pure discrete autoregressive framework, achieving production-grade visual quality.",
                "position": 225
            },
            {
                "img": "https://arxiv.org/html/2601.02204/x3.png",
                "caption": "Figure 3:\\methodNamevisualization.Utilizing next-scale prediction for visual generation, our model can efficiently synthesizes high-quality1024×10241024\\times 1024images under 5 seconds.",
                "position": 228
            },
            {
                "img": "https://arxiv.org/html/2601.02204/x4.png",
                "caption": "Figure 4:Edit results of\\methodNameon EditCanvas benchmark.",
                "position": 231
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Model Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02204/x5.png",
                "caption": "Figure 5:Multi-Scale 3D RoPE for interleaved text-image sequences.Text tokens employ diagonal positions (e.g., position 3 → [3,3,3]), while vision tokens utilize normalized spatial coordinates with augmented scale indices. For clarity, we show 2 scales using square images as examples. Note that the next-scale prediction paradigm excludes 1×1 features in the input, beginning with 2×2 feature maps.",
                "position": 375
            },
            {
                "img": "https://arxiv.org/html/2601.02204/x6.png",
                "caption": "Figure 6:Architecture of the optional diffusion decoder.",
                "position": 427
            }
        ]
    },
    {
        "header": "3Training Odyssey",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02204/x7.png",
                "caption": "Figure 7:Overall training pipeline and the evolution of the Geneval score.The plot below tracks the Geneval score[ghosh2023geneval]against the cumulative number of training tokens (in trillions). Shaded regions distinguish the distinct stages of our pipeline.",
                "position": 449
            },
            {
                "img": "https://arxiv.org/html/2601.02204/x8.png",
                "caption": "Figure 8:Tokenizer Comparison.We compare the training loss and generative performance (GenEval[ghosh2023geneval]) of our dual branch tokenizer against a single branch VQGAN baseline over 40k steps. Despite slightly lower reconstruction PSNR, the dual-branch architecture demonstrates faster convergence and superior generative capability due to semantically aligned latent structures.",
                "position": 467
            },
            {
                "img": "https://arxiv.org/html/2601.02204/x9.png",
                "caption": "Figure 9:Loss curves for total loss, vision loss and text loss during the lightweight alignment/SFT experiments.Losses are computed using cross-entropy loss.",
                "position": 484
            },
            {
                "img": "https://arxiv.org/html/2601.02204/x10.png",
                "caption": "Figure 10:Ablations of self-correction strategy.Direct application of self-correction degrades performance, while combining it with residual features yields substantial improvements. Using self-correction on 100% training samples and 60% tokens per scale achieves optimal performance.",
                "position": 495
            },
            {
                "img": "https://arxiv.org/html/2601.02204/x11.png",
                "caption": "Figure 11:Motivation for scale-reweight strategy.Left: Comparison of vision loss curves at 256-level and 512-level pre-training stages, including the first 9 VAR scales. A loss increase is observed in lower scales during 512-level training. Right:(a)Initial state before 512-level training;(b)Generation results after training on 200M tokens. Image quality degrades with increased artifacts and structural anomalies, indicating a need for reweighted loss scaling.",
                "position": 551
            },
            {
                "img": "https://arxiv.org/html/2601.02204/x12.png",
                "caption": "Figure 12:Prefix-tuning strategy for RL fine-tuning in\\methodName.Only the policies for the coarse scales are optimized, while policies for the finer scales remain frozen. This approach stabilizes training by focusing high-variance RL updates on the most semantically critical generation steps.",
                "position": 569
            }
        ]
    },
    {
        "header": "4Infrastructure",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02204/x13.png",
                "caption": "Figure 13:Illustration of different packing strategy.Fixed max computation budget packing reduces inter-GPU idle time and improving overall training throughput.",
                "position": 747
            }
        ]
    },
    {
        "header": "5Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02204/x14.png",
                "caption": "Figure 14:Hierarchical distribution and visual samples of the pre-training dataset.The left panel illustrates the data composition, where the inner ring represents primary categories and the outer ring details fine-grained sub-categories. The right panel presents representative visual samples across top categories.",
                "position": 814
            },
            {
                "img": "https://arxiv.org/html/2601.02204/x15.png",
                "caption": "Figure 15:Task distribution of the image editing dataset.The data is mainly categorized into two streams: Traditional Editing (left) and Subject-Driven Editing (right). The left panel illustrates the frequency of specific editing operations (e.g., Add, Remove, Replace) colored by their high-level semantic types (Local, Global, View, Text, Style). The right panel displays the distribution of subject-driven tasks, highlighting the diversity in preserving object identity (ID Keep), common objects, abstract content, and visual styles.",
                "position": 824
            },
            {
                "img": "https://arxiv.org/html/2601.02204/x16.png",
                "caption": "Figure 16:\\methodNametext-to-image visualization.",
                "position": 1073
            },
            {
                "img": "https://arxiv.org/html/2601.02204/x17.png",
                "caption": "Figure 17:Edit results of\\methodNameon EditCanvas benchmark.",
                "position": 1076
            }
        ]
    },
    {
        "header": "6Model Performance",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02204/x18.png",
                "caption": "Figure 18:\\methodNameinterleaved generation results.Our unified sequential modeling framework can produce text descriptions and corresponding images in an alternating sequence. We show diverse applications including storytelling (top), recipe instructions (middle), and dynamic scene generation (bottom).",
                "position": 2619
            },
            {
                "img": "https://arxiv.org/html/2601.02204/x19.png",
                "caption": "Figure 19:Qualitative comparison of text-to-image generation with and without reasoning.Left: Baseline outputs generated directly from prompts, often failing to capture implicit constraints or cultural context (e.g., generating a Red Panda instead of a Giant Panda for \"China’s national treasure\"). Middle: The input prompt coupled with the generated reasoning traces. Right: Final outputs leveraging the thinking process.",
                "position": 2629
            },
            {
                "img": "https://arxiv.org/html/2601.02204/x20.png",
                "caption": "Figure 20:Visualization of in-context learning.The model can learn the transformation pattern in the first two example pairs and applies a similar modification to the input image.",
                "position": 2645
            },
            {
                "img": "https://arxiv.org/html/2601.02204/x21.png",
                "caption": "Figure 21:Qualitative comparison of VQ decoder versus diffusion decoders on image reconstruction.The VQ decoder yields satisfactory reconstruction results, although its performance on fine details is moderate. Serving as an optional enhancement, the diffusion decoder improves the reconstruction quality on local details, such as small text and small faces, as its size increases.",
                "position": 2748
            },
            {
                "img": "https://arxiv.org/html/2601.02204/x22.png",
                "caption": "Figure 22:Qualitative comparison of VQ decoder versus diffusion decoders on text-to-image scenario.(a) shows the reconstruction from the baseline VQ decoder. (b)-(d) display results from our diffusion decoders with varying capacities: (b) the 1B parameter UNet-based model, (c) the 12B parameter Transformer-based model, and (d) the 18B parameter Transformer-based model. Note the progression in detail fidelity as model size increases.",
                "position": 2751
            },
            {
                "img": "https://arxiv.org/html/2601.02204/x23.png",
                "caption": "Figure 23:Qualitative comparison of VQ decoder versus diffusion decoders on image editing scenario.",
                "position": 2754
            }
        ]
    },
    {
        "header": "7Conclusion, Limitations and Future Directions",
        "images": []
    },
    {
        "header": "8Additional Implementation Details and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02204/x24.png",
                "caption": "Figure 24:Inference efficiency comparison between\\methodNameand MMDiT across different image resolutions.Both models use the same number of sampling steps for fair comparison. We evaluate two scale scheduling strategies for\\methodName: (1)Uniform Steps, where the scale resolution increases linearly with each step; (2)Realistic Steps, which reflects our actual training configuration that allocates more steps to lower scales and fewer steps to higher resolutions. The annotations show the FLOPs ratio of MMDiT to\\methodName. As resolution increases, the efficiency advantage of\\methodNamebecomes more pronounced, achieving up to6×6\\timesreduction in total FLOPs.",
                "position": 3741
            },
            {
                "img": "https://arxiv.org/html/2601.02204/x25.png",
                "caption": "Figure 25:Distribution of EditCanvas Tasks.The chart illustrates the hierarchical structure of the dataset, dividing tasks into Traditional Editing (blue, 67.8%) and Subject-Driven Editing (orange, 32.2%). The outer layers represent the sub-categories and specific fine-grained editing tasks.",
                "position": 3940
            }
        ]
    },
    {
        "header": "9EditCanvas benchmark",
        "images": []
    }
]