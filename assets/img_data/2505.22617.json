[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22617/x1.png",
                "caption": "Figure 1:Left:Entropy collapse and performance saturation. Over95%percent9595\\%95 %entropy drop/performance gains take place at the early stage of RL training. The model then reaches a plateau with little improvement.Right:The predictable relationship between validation performance and policy entropy. Without intervention, the policy ‚Äútrades‚Äù entropy for performance exponentially, showing clear ceilings that hinder further policy enhancement.",
                "position": 206
            }
        ]
    },
    {
        "header": "2The Predictable ‚ÄúCollapse‚Äù of Policy Entropy",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22617/x2.png",
                "caption": "Figure 2:Avg. entropy consumption/performance gain (%) in 11 RL runs with different models.",
                "position": 376
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x3.png",
                "caption": "Figure 3:Fitting curves between policy entropy and validation performance on math task. We conduct validation every 4 rollout steps until convergence.",
                "position": 419
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x3.png",
                "caption": "",
                "position": 422
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x4.png",
                "caption": "",
                "position": 426
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x5.png",
                "caption": "",
                "position": 430
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x6.png",
                "caption": "Figure 4:Fitting curves between policy entropy and validation performance in coding task. We conduct validation every 4 rollout steps until convergence.",
                "position": 436
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x6.png",
                "caption": "",
                "position": 439
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x7.png",
                "caption": "",
                "position": 443
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x8.png",
                "caption": "",
                "position": 447
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x9.png",
                "caption": "(a)Math Task",
                "position": 457
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x9.png",
                "caption": "(a)Math Task",
                "position": 460
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x10.png",
                "caption": "(b)Code Task",
                "position": 465
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x11.png",
                "caption": "Figure 6:Training Qwen2.5-7B with different RL algorithms.",
                "position": 486
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x12.png",
                "caption": "(a)Coef.aùëéaitalic_afor math task",
                "position": 503
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x12.png",
                "caption": "(a)Coef.aùëéaitalic_afor math task",
                "position": 506
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x13.png",
                "caption": "(b)Coef.bùëèbitalic_bfor math task",
                "position": 511
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x14.png",
                "caption": "(c)Coef.aùëéaitalic_afor code task",
                "position": 516
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x15.png",
                "caption": "(d)Coef.bùëèbitalic_bfor code task",
                "position": 521
            }
        ]
    },
    {
        "header": "3Dynamics Analysis of Policy Entropy",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22617/x16.png",
                "caption": "Figure 8:Left:The dynamics of policy entropy (step-wise entropy difference) and covariance during on-policy GRPO training. They show similar trends as expected from the theoretical results.Right:Different prompt groups show distinct covariance behaviors. Easier prompts with higher accuracy have higher covariances as well, while harder prompts have lower covariances.",
                "position": 742
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x16.png",
                "caption": "",
                "position": 745
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x17.png",
                "caption": "",
                "position": 749
            }
        ]
    },
    {
        "header": "4Entropy Control by Covariance Regularization",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22617/x18.png",
                "caption": "Figure 9:The policy entropy and validation accuracy of adding entropy loss whereLent=L‚àíŒ±‚Å¢‚Ñã‚Å¢(œÄŒ∏)subscriptùêøentùêøùõº‚ÑãsubscriptùúãùúÉL_{\\text{ent}}=L-\\alpha\\mathcal{H}(\\pi_{\\theta})italic_L start_POSTSUBSCRIPT ent end_POSTSUBSCRIPT = italic_L - italic_Œ± caligraphic_H ( italic_œÄ start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ).LùêøLitalic_Lis the original loss andŒ±ùõº\\alphaitalic_Œ±is the coefficient of entropy loss.",
                "position": 779
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x18.png",
                "caption": "",
                "position": 782
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x19.png",
                "caption": "",
                "position": 786
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x20.png",
                "caption": "Figure 10:The policy entropy and validation accuracy of adding KL penalty between policy and reference model whereLKL=L+Œ≤ùîªKL(œÄŒ∏||œÄref)L_{\\text{KL}}=L+\\beta\\mathbb{D}_{\\text{KL}}(\\pi_{\\theta}||\\pi_{\\text{ref}})italic_L start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT = italic_L + italic_Œ≤ blackboard_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT ( italic_œÄ start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT | | italic_œÄ start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT ).LùêøLitalic_Lis the original loss andŒ≤ùõΩ\\betaitalic_Œ≤is the coefficient of KL loss.",
                "position": 792
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x20.png",
                "caption": "",
                "position": 795
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x21.png",
                "caption": "",
                "position": 799
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x22.png",
                "caption": "Figure 12:Training Qwen2.5-7B (Top) / Qwen2.5-32B (bottom) with GRPO with/without our methods.Left:Entropy dynamics. Our methods uplift policy entropy from collapse, enabling sustained exploration.Middle:Our method also incentivizes longer responses compared with vanilla GRPO.Right:The policy model consistently outperforms the baseline on testsets, avoiding performance plateaus.",
                "position": 1042
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x22.png",
                "caption": "",
                "position": 1045
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x23.png",
                "caption": "",
                "position": 1049
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x24.png",
                "caption": "",
                "position": 1053
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x25.png",
                "caption": "",
                "position": 1058
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x26.png",
                "caption": "",
                "position": 1062
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x27.png",
                "caption": "",
                "position": 1066
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x28.png",
                "caption": "Figure 13:Differences in entropy dynamics of Qwen2.5-7B under varying KL coefficients and Clip ratios, evaluatedClip-Cov(left) andKL-Cov(right) settings, respectively.",
                "position": 1212
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x28.png",
                "caption": "",
                "position": 1215
            },
            {
                "img": "https://arxiv.org/html/2505.22617/x29.png",
                "caption": "",
                "position": 1219
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining Details for Different Models",
        "images": []
    },
    {
        "header": "Appendix BMore Fitting Results",
        "images": []
    },
    {
        "header": "Appendix CFitting Results of Training with Different Dataset.",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22617/x30.png",
                "caption": "Figure 14:Training Qwen2.5-7B with different data.",
                "position": 2137
            }
        ]
    },
    {
        "header": "Appendix DFitting Results of Instruct Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22617/x31.png",
                "caption": "Figure 15:Training Qwen2.5 instruct models on math task.",
                "position": 2147
            }
        ]
    },
    {
        "header": "Appendix EProof",
        "images": []
    }
]