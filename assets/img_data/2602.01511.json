[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01511/x1.png",
                "caption": "Figure 1:The overall framework forRubric-ARM.",
                "position": 239
            }
        ]
    },
    {
        "header": "4Rubric-ARM: Alternating RL for Rubric Generation and Judging",
        "images": []
    },
    {
        "header": "5Theoretical Analysis",
        "images": []
    },
    {
        "header": "6Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01511/x2.png",
                "caption": "Figure 2:Performance of different judge and reward models on WritingPreferenceBench.",
                "position": 879
            },
            {
                "img": "https://arxiv.org/html/2602.01511/x3.png",
                "caption": "Figure 3:Comparison of trained policy models on IFBench. Results of baselines except Rubric-RM (IterDPO) are from OpenRubricsliu2025openrubrics.",
                "position": 1489
            },
            {
                "img": "https://arxiv.org/html/2602.01511/x4.png",
                "caption": "Figure 4:Comparison of trained policy models on Create Writing Benchmark v3. Results of baselines except Rubric-RM are from RuscaRL(zhou2025breaking).",
                "position": 1492
            },
            {
                "img": "https://arxiv.org/html/2602.01511/x5.png",
                "caption": "Table 7:Computing speed on 100 samples (vLLM).\nResults with “” were taken fromliu2025openrubrics.",
                "position": 1569
            },
            {
                "img": "https://arxiv.org/html/2602.01511/x5.png",
                "caption": "Figure 5:Performance of iterative DPO withRubric-ARMacross three iterations.",
                "position": 1652
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails for Group Relative Policy Optimization (GRPO)",
        "images": []
    },
    {
        "header": "Appendix BDetailed Theoretical Derivations",
        "images": []
    },
    {
        "header": "Appendix CImplementation Details",
        "images": []
    },
    {
        "header": "Appendix DAdditional Experimental Results",
        "images": []
    },
    {
        "header": "Appendix EPrompts",
        "images": []
    }
]