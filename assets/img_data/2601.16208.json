[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16208/x1.png",
                "caption": "",
                "position": 127
            },
            {
                "img": "https://arxiv.org/html/2601.16208/x2.png",
                "caption": "",
                "position": 127
            },
            {
                "img": "https://arxiv.org/html/2601.16208/x3.png",
                "caption": "",
                "position": 127
            },
            {
                "img": "https://arxiv.org/html/2601.16208/x4.png",
                "caption": "",
                "position": 127
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16208/x5.png",
                "caption": "Figure 1:RAE converges faster than VAE in text-to-image pretraining.We train Qwen-2.5 1.5B + DiT 2.4B models from scratch on both RAE (SigLIP-2) and VAE (FLUX) latent spaces for up to 60k iterations.\nRAE converges significantly faster than VAE on both GenEval (4.0×) and DPG-Bench (4.6×).",
                "position": 145
            },
            {
                "img": "https://arxiv.org/html/2601.16208/x6.png",
                "caption": "Figure 2:RAE decoders trained on more data (web, synthetic & text) generalize across domains.Decoders trained only on ImageNet reconstruct natural images well but struggle with text-rendering scenes (see second column). Adding web and text data greatly improves text reconstruction while maintaining natural-image quality. We also observe that both the language-supervised model and the SSL model learn representations suitable for reconstructing diverse images, including natural languages. Compared to proprietary VAEs, our RAE models achieve competitive overall fidelity.",
                "position": 152
            }
        ]
    },
    {
        "header": "2Scaling Decoder Training Beyond ImageNet",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16208/x7.png",
                "caption": "Figure 3:Overview of training pipeline.Left: RAE decoder training stage. We train a decoder on the representations (yellow tokens) produced by the frozen RAE encoder.Right: End-to-end unified training of the autoregressive model, diffusion transformer, and learnable query tokens (gray tokens) using cross-entropy (CE) loss for text prediction and a flow-matching objective for image prediction.",
                "position": 339
            }
        ]
    },
    {
        "header": "3RAE is Simpler in T2I",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16208/x8.png",
                "caption": "(a)Noise-augmented decoding gains diminish with training",
                "position": 464
            },
            {
                "img": "https://arxiv.org/html/2601.16208/x8.png",
                "caption": "(a)Noise-augmented decoding gains diminish with training",
                "position": 467
            },
            {
                "img": "https://arxiv.org/html/2601.16208/x9.png",
                "caption": "(b)DiTDH\\text{DiT}^{\\text{DH}}advantage saturates as DiT scales",
                "position": 475
            }
        ]
    },
    {
        "header": "4Training Diffusion Model with RAE vs. VAE",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16208/x10.png",
                "caption": "(a)Scaling DiT models with fixed LLM (Qwen2.5 1.5B)",
                "position": 590
            },
            {
                "img": "https://arxiv.org/html/2601.16208/x10.png",
                "caption": "(a)Scaling DiT models with fixed LLM (Qwen2.5 1.5B)",
                "position": 593
            },
            {
                "img": "https://arxiv.org/html/2601.16208/x11.png",
                "caption": "(b)Scaling LLM and DiT jointly",
                "position": 599
            },
            {
                "img": "https://arxiv.org/html/2601.16208/x12.png",
                "caption": "Figure 6:RAE-based models outperform VAE-based models and are less prone to overfitting.We train both models for 256 epochs and observe that (1) RAE-based models consistently achieve higher performance, and (2) VAE-based models begin to overfit rapidly after 64 epochs.",
                "position": 686
            },
            {
                "img": "https://arxiv.org/html/2601.16208/x13.png",
                "caption": "Figure 7:RAE-based models outperform VAEs across different settings.Left: When fine-tuning only the DiT versus the full LLM+DiT system, RAE models consistently achieve higher GenEval scores.Right: RAE models maintain their advantage over VAE across all DiT model scales (0.5B–9.8B parameters), with the performance gap widening as model size increases.",
                "position": 705
            }
        ]
    },
    {
        "header": "5Implications for Unified Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16208/x14.png",
                "caption": "Figure 8:Test-time scaling in latent space.Our framework allows the LLM to directly evaluate and select generation results within the latent space, bypassing the decode-re-encode process.",
                "position": 728
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation",
        "images": []
    },
    {
        "header": "Appendix BModels",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16208/x15.png",
                "caption": "Figure 9:Diffusion loss during finetuning (256 epochs).RAE overfits less and later than VAE: the VAE loss plunges early to very low values, while the RAE loss decreases more gradually and plateaus at higher values, indicating reduced overfitting.",
                "position": 2478
            },
            {
                "img": "https://arxiv.org/html/2601.16208/x16.png",
                "caption": "Figure 10:Extended finetuning to 512 epochs.RAE maintains robust performance even with 512 epochs of training, while VAE suffers catastrophic overfitting after 64 epochs.",
                "position": 2490
            }
        ]
    },
    {
        "header": "Appendix CAdditional Results",
        "images": []
    }
]