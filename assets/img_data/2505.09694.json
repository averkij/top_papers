[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.09694/x1.png",
                "caption": "Figure 1:Comparison between general video generation and embodied video generation. Unlike general videos, embodied videos typically feature more structured scenes, consistent motion patterns, and clearer task logic.",
                "position": 97
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3TheEWMBenchBenchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.09694/x2.png",
                "caption": "Figure 2:Overview of theEWMBenchbenchmark design. The framework begins with unified world initialization, where generative models are instructed to produce predictive video frames based on initial scene images, task instructions, and optional action trajectories. These generated video frames are subsequently evaluated using multi-dimensional metrics, focusing on scene consistency, motion dynamics, and semantic alignment.",
                "position": 308
            },
            {
                "img": "https://arxiv.org/html/2505.09694/x3.png",
                "caption": "Figure 3:Overview of the constructed dataset.Left: Task scenes spanning household, commercial, and industrial environments.Middle: Diverse task-specific trajectory variations within each scene.Right: Broad semantic coverage across various manipulation contexts.",
                "position": 340
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.09694/x4.png",
                "caption": "Figure 4:Evaluation Results of Video Generative Models.",
                "position": 395
            },
            {
                "img": "https://arxiv.org/html/2505.09694/x5.png",
                "caption": "Figure 5:Typical examples.EWMBenchscores align well with scene and motion accuracy, demonstrating the interpretability and robustness of the proposed metrics.",
                "position": 534
            },
            {
                "img": "https://arxiv.org/html/2505.09694/x6.png",
                "caption": "Figure 6:(A) Aggregated human rankings of model predictions. (B) Comparison of rankings produced byEWMBenchand VBench, highlightingEWMBench’s closer alignment with human judgments.(C) Complementarity of trajectory metrics.",
                "position": 547
            }
        ]
    },
    {
        "header": "5Conclusions and Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.09694/x7.png",
                "caption": "Figure 7:Dataset overview.The first two rows display selected task scenarios and their associated action motion trajectories. The third row categorizes object properties (e.g., fluid, articulated, rigid, deformable, multi-body) using color-coded legends and representative examples.",
                "position": 1257
            },
            {
                "img": "https://arxiv.org/html/2505.09694/x8.png",
                "caption": "Figure 8:Feature map comparison across models.DINOv2 fine-tuned on embodied data captures agents and tools with sharper spatial coherence, enabling more reliable scene stability evaluation.",
                "position": 1332
            },
            {
                "img": "https://arxiv.org/html/2505.09694/x9.png",
                "caption": "Figure 9:Scene Consistency failure cases.Despite camera movement and background drift, VBench assigns high scores (0.88–0.91). Our metric, however, detects the instability through decreased cosine similarity.",
                "position": 1335
            },
            {
                "img": "https://arxiv.org/html/2505.09694/x10.png",
                "caption": "Figure 10:Domain-adapted model failure case.The robot hand moves toward the correct region but fails to close the gripper on the object, resulting in empty grasping.",
                "position": 1452
            },
            {
                "img": "https://arxiv.org/html/2505.09694/x11.png",
                "caption": "Figure 11:Despite being given explicit camera viewpoint control instructions, COSMOS fails to maintain a consistent viewpoint throughout the video. This indicates a limitation in its ability to follow spatial constraints, leading to unstable or drifting perspectives.",
                "position": 1460
            },
            {
                "img": "https://arxiv.org/html/2505.09694/x12.png",
                "caption": "Figure 12:Examples illustrating the poor task understanding and temporal instability of the LTX model. The middle column shows LTX-generated frames, which often exhibit abrupt visual changes and scene inconsistencies. In contrast, the rightmost column presents examples with better scene preservation, highlighting the gap in temporal coherence.",
                "position": 1463
            },
            {
                "img": "https://arxiv.org/html/2505.09694/x13.png",
                "caption": "Figure 13:Generated videos from COSMOS and LTX models often depict human hands instead of robotic arms, indicating a bias toward human hand representations in their training data. This bias hinders the models’ ability to correctly generalize to robotic manipulation tasks, where accurate mapping to robotic arms is essential.",
                "position": 1466
            },
            {
                "img": "https://arxiv.org/html/2505.09694/x14.png",
                "caption": "Figure 14:An example from OpenSora illustrating unstable robotic arm motion. Although the scene is semantically aligned with the manipulation task, the arm exhibits visible jitter and lacks smooth trajectory control, consistent with the motion instability discussed in the main text.",
                "position": 1472
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]