[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01481/extracted/5895746/figs/logo.png",
                "caption": "",
                "position": 85
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01481/x1.png",
                "caption": "Figure 1:Overview of SonicSim, our toolkit for speech research. SonicSim provides a customizable data generator based on Habitat-sim, allowing users to generate realistic and physically plausible audio data in a controlled manner.",
                "position": 124
            },
            {
                "img": "https://arxiv.org/html/2410.01481/x2.png",
                "caption": "Figure 2:Overview of the SonicSet dataset. It covers a wide range of scenarios, speakers, and noise.",
                "position": 127
            }
        ]
    },
    {
        "header": "2Related works",
        "images": []
    },
    {
        "header": "3Moving sound source suite",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01481/x3.png",
                "caption": "Figure 3:An automatic simulation pipeline for moving sound sources. The positions of the sound sources, noises and microphones are all randomly generated. The five RIRs shown in the figure are just for demonstration purposes; the actual process involves more RIRs.",
                "position": 303
            }
        ]
    },
    {
        "header": "4Benchmark I: speech separation",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01481/x4.png",
                "caption": "Figure 4:Overall pipeline for speech separation and enhancement.",
                "position": 341
            },
            {
                "img": "https://arxiv.org/html/2410.01481/x5.png",
                "caption": "Table 2:Comparative performance evaluation of models trained on different datasets using real-recorded audio withenvironmental noise. The results are reported separately for “trained on LRS2-2Mix”, “trained on Libri2Mix” and “trained on SonicSet”, distinguished by a slash. The relative length is indicated below the value by horizontal bars.",
                "position": 369
            },
            {
                "img": "https://arxiv.org/html/2410.01481/x6.png",
                "caption": "Table 3:Comparative performance evaluation of models trained on different datasets using real-recorded audio withmusical noise. The results are reported separately for “trained on LRS2-2Mix”, “trained on Libri2Mix” and “trained on SonicSet”, distinguished by a slash.",
                "position": 372
            },
            {
                "img": "https://arxiv.org/html/2410.01481/x7.png",
                "caption": "Table 4:Comparison of existing speech separation methods on the SonicSet dataset. The performance of each model is listed separately for results under “environmental noise” and “musical noise”, distinguished by a slash.",
                "position": 375
            }
        ]
    },
    {
        "header": "5Benchmark II: speech enhancement",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01481/x8.png",
                "caption": "Table 5:Comparative performance evaluation of models trained on different datasets using the RealMAN dataset. The results are reported separately for “trained on VoiceBank-DEMAND”, “trained on DNS Challenge” and “trained on SonicSet”, distinguished by a slash.",
                "position": 426
            },
            {
                "img": "https://arxiv.org/html/2410.01481/x9.png",
                "caption": "Table 6:Comparison of speech enhancement methods using the SonicSet test set. The metrics are listed separately under “environmental noise” and “musical noise”, distinguished by a slash.",
                "position": 429
            }
        ]
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADataset analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01481/x10.png",
                "caption": "Figure 5:An example from the SonicSet data set. Each set of data includes three moving speaker audio files, two types of noise, and the trajectories of the sound source movement.",
                "position": 1467
            }
        ]
    },
    {
        "header": "Appendix BSonicSet demo",
        "images": []
    },
    {
        "header": "Appendix CBenchmark I: speech separation",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01481/x11.png",
                "caption": "Figure 6:Recording audio using realistic scene images. These images show the layout of the actual physical environment used to record audio.",
                "position": 1817
            }
        ]
    },
    {
        "header": "Appendix DBenchmark II: speech enhancement",
        "images": []
    }
]