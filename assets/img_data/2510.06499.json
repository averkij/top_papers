[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.06499/x1.png",
                "caption": "",
                "position": 141
            },
            {
                "img": "https://arxiv.org/html/2510.06499/x2.png",
                "caption": "",
                "position": 145
            }
        ]
    },
    {
        "header": "1  Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.06499/figs/teaser2.png",
                "caption": "Figure 1:The scaling on LLM RL is fundamentally bottlenecked by the scarcity of high-quality RL data. While pretraining leverages>>1T diverse web tokens, RL datasets remain limited to<<10B tokens with limited diversity. We proposeWebscale-RLdata pipeline to fundamentally improve the scalability of RL data: we convert the pretraining corpora to verifiable query and ground-truth answer pairs, scaling RL data to pretraining levels while preserving the diversity. The experiments show that RL withWebscale-RLdata is significantly more effective and efficient than continual pretraining and data refinement baselines.",
                "position": 160
            }
        ]
    },
    {
        "header": "2  Related Works",
        "images": []
    },
    {
        "header": "3  Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.06499/figs/pipeline.png",
                "caption": "Figure 2:Overview of theWebscale-RLdata pipeline that systematically converts large-scale pretraining data into RL data while preserving the scale and diversity of web data. The pipeline maintains a domain-specific demonstration library for few-shot examples for high quality generation and assigns multiple personas to each document to encourage reflecting different viewpoints. The generated QA pairs are verified for correctness and leakage prevention to ensure the reliability of the RL dataset.",
                "position": 245
            }
        ]
    },
    {
        "header": "4  Webscale-RL Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.06499/figs/domain_pie.png",
                "caption": "Figure 3:Left: The domain distribution ofWebscale-RLdataset. Right: The comparison on question embedding ofWebscale-RLand Nemotron data. We randomly sample 5K questions from each dataset and visualize the embedding (by Qwen3-Embedding) reduced to 2D using UMAP.",
                "position": 385
            },
            {
                "img": "https://arxiv.org/html/2510.06499/figs/webscale_nemotron.png",
                "caption": "",
                "position": 394
            }
        ]
    },
    {
        "header": "5  Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.06499/figs/mmlu_pro.png",
                "caption": "Figure 4:Scaling comparison between Webscale-RL training and continual pretraining with the original pretraining corpora. We report the performances on MMLU-pro (left), Big-Bench (middle) and average on all benchmarks (right). The token number for RL training is calculated based on the original pretraining corpus used to generate theWebscale-RLdataset. The each data point in continual pretraining baselines are followed by a SFT training using the same 10K high-quality examples. The RL training onWebscale-RLconsistently outperforms continual pretraining at different training scales and exhibits better scaling efficiency.",
                "position": 542
            },
            {
                "img": "https://arxiv.org/html/2510.06499/figs/bbh.png",
                "caption": "",
                "position": 551
            },
            {
                "img": "https://arxiv.org/html/2510.06499/figs/avg.png",
                "caption": "",
                "position": 556
            }
        ]
    },
    {
        "header": "6  Conclusion",
        "images": []
    },
    {
        "header": "7  Reproducibility statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AUsage of LLMs",
        "images": []
    },
    {
        "header": "Appendix BDetails of Dataset Construction and Training",
        "images": []
    }
]