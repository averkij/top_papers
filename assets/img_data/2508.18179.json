[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3SEAM: Semantically Equivalent Across Modalities Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.18179/x1.png",
                "caption": "Figure 1:SEAMincludes 16 tasks in chess, chemistry, music, and graph theory domains with paired visual-spatial and textual-symbolic representations that are semantically equivalent.",
                "position": 141
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.18179/figs/acc_vs_agr.png",
                "caption": "Figure 2:Correlation between final answer agreement rates with language and vision inputs and average accuracy across various VLMs. Models are color-coded by family. Random Baseline denotes how often two models with identical accuracyppwould agree by random chance, which can be thought of as a lower bound for cross-modal agreement of real multimodal models.",
                "position": 490
            },
            {
                "img": "https://arxiv.org/html/2508.18179/figs/heatmap.png",
                "caption": "Figure 3:Comparison of model accuracy across different modalities in each domain.",
                "position": 511
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExtended Related Work",
        "images": []
    },
    {
        "header": "Appendix BAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.18179/x2.png",
                "caption": "Figure 4:Effect of performance from visual transformations on Qwen2.5-VL models.",
                "position": 2168
            },
            {
                "img": "https://arxiv.org/html/2508.18179/figs/heatmap_cross_model.png",
                "caption": "Figure 5:Cross-model agreement on semantically equivalent tasks presented in different modalities. Correlation matrices show pairwise agreement scores between GPT-5, GPT-5-mini, GPT-5-nano, Claude-4.1-Opus, Claude-4-Sonnet, and Claude-3.7-Sonnet models when solving the same problems in language (left), vision (center), and vision-language (right) formats, with darker blue indicating higher agreement.",
                "position": 2178
            },
            {
                "img": "https://arxiv.org/html/2508.18179/x3.png",
                "caption": "Figure 6:Case-study of text-based versus vision-based processing in VLMs, showing two error cases: (left) Qwen2.5-VL-72B-Instruct incorrect counting of carbon atoms in a chemical structure due to SMILES tokenization, and (right) gemma-3-12b-it hallucinated graph paths in a simple path counting problem. Both examples show discrepancies between text-only and vision-only representatinos.",
                "position": 2185
            },
            {
                "img": "https://arxiv.org/html/2508.18179/x4.png",
                "caption": "Figure 7:Visual perception error example.",
                "position": 2217
            }
        ]
    },
    {
        "header": "Appendix CReproducibility",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.18179/x5.png",
                "caption": "Figure 8:T-SNE visualization of modality-specific embeddings from Qwen models.",
                "position": 3310
            }
        ]
    },
    {
        "header": "Appendix DInternal Representation Alignment",
        "images": []
    }
]