[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19906/extracted/6310774/figures/teaser2.png",
                "caption": "",
                "position": 100
            },
            {
                "img": "https://arxiv.org/html/2503.19906/x1.png",
                "caption": "Figure 2:The overall training pipeline of our method.We first generate 2D images from different domains using a 2D diffusion model. These images are then used to train 4D GANs for each domain. Subsequently, the trained 4D GANs generate image-4D representation pairs across domains, which are used to train DIT and the rendering model.",
                "position": 107
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19906/x2.png",
                "caption": "Figure 3:The pipeline of dataset generation.We use text prompts to transform images from the realistic domain to the target domain while ensuring pose and expression consistency with SDEdit[47]and landmark-guided ControlNet[84]. This enables direct reuse of the original mesh, avoiding errors in non-realistic domain extraction. After domain transfer, we train 4D GANs to generate image-parametric triplane pairs, which serve as data for the next stage. The parametric triplane comprises dynamic and static components, with the dynamic region aligned to the mesh.",
                "position": 134
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19906/x3.png",
                "caption": "Figure 4:The pipeline of DiT.We first train a VAE to compress the parametric triplane into a latent space, and then train a DiT to denoise the noisy latent. We incorporate features from DINO[6]and CLIP[57]into the DiT to guide the generation process.",
                "position": 184
            },
            {
                "img": "https://arxiv.org/html/2503.19906/x4.png",
                "caption": "Figure 5:The pipeline of motion-aware cross-domain renderer.We use an encoder to extract the feature from the source image. This feature is sent to a ViT to predict results under the guidance of generated parametric triplane and motion embedding. Finally, a decoder processes the output of the ViT and fuses it with the results of rasterization to produce the final output.",
                "position": 220
            },
            {
                "img": "https://arxiv.org/html/2503.19906/x5.png",
                "caption": "Figure 6:Qualitative comparison with SOTA methods. The leftmost column in the figure shows the input images, with the bottom-right corner representing the target image. The first row displays the results of self-reenactment, while the following three rows show the results of cross-reenactment. It can be observed that our method achieves superior performance in terms of expression and pose consistency, as well as identity preservation.",
                "position": 223
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19906/x6.png",
                "caption": "Figure 7:Visualization of ablation study on data generation methods. It is only when combining SDEdit and ControlNet that we can ensure the generated images retain both the same expression and pose as the original images. The corresponding prompts are shown above images.",
                "position": 406
            },
            {
                "img": "https://arxiv.org/html/2503.19906/x7.png",
                "caption": "Figure 8:Visualization of ablation study on motion-aware cross-domain renderer. The Next3D rendering approach involves using a CNN as a replacement for our render model.",
                "position": 409
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ALimitations",
        "images": []
    },
    {
        "header": "Appendix BExploration of the 4D Representation",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19906/x8.png",
                "caption": "Figure S1:Visualization of generation results of different 4D GANs, including Next3D[64]and GenHead[15], on the unrealistic domain. We use the domain of Lego here. GenHead tends to produce artifacts, whereas Next3D achieves much better results, generating more plausible content.",
                "position": 1627
            },
            {
                "img": "https://arxiv.org/html/2503.19906/extracted/6310774/figures/supp/model_design.png",
                "caption": "Figure S2:Visualization of different model results. Model A and Model B are two different end-to-end method which not use the Dit. For more details, please refer the Sec.C.",
                "position": 1636
            }
        ]
    },
    {
        "header": "Appendix CEffectiveness of model design",
        "images": []
    },
    {
        "header": "Appendix DMore implementation details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19906/x9.png",
                "caption": "Figure S3:Visualization of reconstruction results of our VAE. The domain is Yoda, 3D-Animation, Zommbie, and Counterfeit, respectively. The ground truth images are generated with the Next3D.",
                "position": 1934
            },
            {
                "img": "https://arxiv.org/html/2503.19906/x10.png",
                "caption": "Figure S4:Visualization of rendering process of Next3D. After rasterization, a CNN is employed to remove artifacts introduced during the rasterization process, which is critical for final performance, as mentioned in the Next3D[64].",
                "position": 1977
            },
            {
                "img": "https://arxiv.org/html/2503.19906/extracted/6310774/figures/supp/shape.png",
                "caption": "Figure S5:The geometry results of our method.",
                "position": 2064
            },
            {
                "img": "https://arxiv.org/html/2503.19906/x11.png",
                "caption": "Figure S6:Qualitative comparison with state-of-the-art methods. The leftmost column of the figure presents the input images, with the bottom-right corner indicating the target image. The first row illustrates the results of self-reenactment, while the subsequent rows showcase the results of cross-reenactment. Our method demonstrates superior performance in terms of expression and pose consistency, as well as identity preservation. For more visual results, please refer to ourvideo results.",
                "position": 2067
            }
        ]
    },
    {
        "header": "Appendix EAdditional Comparisons and Visual Results",
        "images": []
    }
]