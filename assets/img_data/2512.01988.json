[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01988/x1.png",
                "caption": "Figure 1:Motivation ofArtemis. Comparison between current perception-policy models and human perception. (a) Query: find the shortest player. (b) Perception–policy models depend on ungrounded language reasoning, leading to wrong localization. (c) Humans perform structured visual reasoning, progressively refining attention to identify the correct player.",
                "position": 108
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01988/x2.png",
                "caption": "Figure 2:Overview of the Artemis framework for RL-based perception-policy learning. Rollouts generated by MLLM are encouraged to perceive structured visual evidence before decision-making, guided by the structured visual reasoning reward, while the outcome rewards supervise the format and answer generation. GRPO is employed to optimize the unified perception-policy learning framework.",
                "position": 164
            }
        ]
    },
    {
        "header": "3Artemis Framework",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01988/x3.png",
                "caption": "Figure 3:Impact of SFT and RL training strategies on COCO detection (bars) and visual grounding (lines).",
                "position": 1797
            },
            {
                "img": "https://arxiv.org/html/2512.01988/x3.png",
                "caption": "Figure 3:Impact of SFT and RL training strategies on COCO detection (bars) and visual grounding (lines).",
                "position": 1800
            },
            {
                "img": "https://arxiv.org/html/2512.01988/x4.png",
                "caption": "Figure 4:Impact of cold start strategies on COCO detection (bars) and visual grounding (lines).",
                "position": 1805
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails of Outcome Rewards",
        "images": []
    },
    {
        "header": "Appendix BDataset Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01988/x5.png",
                "caption": "Figure 5:A data example from Artemis-RFT. Our dataset contains two task types, visual grounding and object detection, and our unified Artemis perception policy learning framework is jointly trained on both. For the grounding example, purple boxes denote the reasoning objects; the numeric labels are added only for illustration and do not exist in the raw data. The solution is indicated by a green box. In this example, box ➂ is the solution and also serves as the key reasoning object, so only the green box is shown where the two overlap. For the detection example, we display only the green box that corresponds to the solution.",
                "position": 2760
            }
        ]
    },
    {
        "header": "Appendix CPrompt Settings",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01988/x6.png",
                "caption": "Figure 6:Qualitative visual grounding results (Case 1) of Artemis compared with Perception-R1 and VLM-R1 on the RefCOCO/+/g datasets. For brevity, full prompts are omitted.Green: ground truth;Purple: Artemis reasoning boxes;Red: Artemis answer;Blue: Perception-R1;Pink: VLM-R1.",
                "position": 2916
            },
            {
                "img": "https://arxiv.org/html/2512.01988/x7.png",
                "caption": "Figure 7:Qualitative visual grounding results (Case 2) of Artemis compared with Perception-R1 and VLM-R1 on the RefCOCO/+/g datasets. For brevity, full prompts are omitted.Green: ground truth;Purple: Artemis reasoning boxes;Red: Artemis answer;Blue: Perception-R1;Pink: VLM-R1.",
                "position": 2926
            },
            {
                "img": "https://arxiv.org/html/2512.01988/x8.png",
                "caption": "Figure 8:Qualitative visual grounding results (Case 3) of Artemis compared with Perception-R1 and VLM-R1 on the RefCOCO/+/g datasets. For brevity, full prompts are omitted.Green: ground truth;Purple: Artemis reasoning boxes;Red: Artemis answer;Blue: Perception-R1;Pink: VLM-R1.",
                "position": 2936
            },
            {
                "img": "https://arxiv.org/html/2512.01988/x9.png",
                "caption": "Figure 9:Qualitative visual grounding results (Case 4) of Artemis compared with Perception-R1 and VLM-R1 on the RefCOCO/+/g datasets. For brevity, full prompts are omitted.Green: ground truth;Purple: Artemis reasoning boxes;Red: Artemis answer;Blue: Perception-R1;Pink: VLM-R1.",
                "position": 2946
            },
            {
                "img": "https://arxiv.org/html/2512.01988/x10.png",
                "caption": "Figure 10:Qualitative visual counting results (Case 1) of Artemis compared with Perception-R1 and Qwen2.5-VL on the Pixmo datasets. For brevity, full prompts are omitted.Green: ground truth;Purple: Artemis reasoning boxes;Red: Artemis answer;Blue: Perception-R1;Cyan: Qwen2.5-VL.",
                "position": 2956
            },
            {
                "img": "https://arxiv.org/html/2512.01988/x11.png",
                "caption": "Figure 11:Qualitative visual counting results (Case 2) of Artemis compared with Perception-R1 and Qwen2.5-VL on the Pixmo datasets. For brevity, full prompts are omitted.Green: ground truth;Purple: Artemis reasoning boxes;Red: Artemis answer;Blue: Perception-R1;Cyan: Qwen2.5-VL.",
                "position": 2966
            },
            {
                "img": "https://arxiv.org/html/2512.01988/x12.png",
                "caption": "Figure 12:Qualitative mathematical perception results of Artemis compared with Qwen2.5-VL on MATHGLANCE, showing examples from the mathematical shape Classification and mathematical shape Counting tasks. For brevity, full prompts are omitted.Green: ground truth;Cyan: Qwen2.5-VL;Red: Artemis answer.",
                "position": 2976
            },
            {
                "img": "https://arxiv.org/html/2512.01988/x13.png",
                "caption": "Figure 13:Qualitative mathematical perception results of Artemis compared with Qwen2.5-VL on MATHGLANCE, showing examples from the mathematical Visual Grounding and mathematical Relation Identification tasks. For brevity, full prompts are omitted.Green: ground truth;Cyan: Qwen2.5-VL;Red: Artemis answer.",
                "position": 2984
            },
            {
                "img": "https://arxiv.org/html/2512.01988/x14.png",
                "caption": "Figure 14:Qualitative object detection results of Artemis on COCO Val 2017.Green: ground truth;Red: Artemis answer;",
                "position": 2991
            },
            {
                "img": "https://arxiv.org/html/2512.01988/x15.png",
                "caption": "Figure 15:Qualitative reasoning-based grounding results of Artemis on the LISA grounding test set.Green: ground truth;Purple: Artemis reasoning boxes;Red: Artemis answer.",
                "position": 2998
            }
        ]
    },
    {
        "header": "Appendix DQualitative Results",
        "images": []
    }
]