[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.18314/images/teaser.png",
                "caption": "",
                "position": 122
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.18314/images/main_pipeline.png",
                "caption": "Figure 2:Pipeline. From multi-view images, a diffusion predictor yields per-view material maps. We reconstruct the objectâ€™s geometry using 3D Gaussian Splatting. Then we project 2D materials to 3D via ray tracing, and refine per Gaussian materials with our Neural Merger that has a softmax output layer, choosing between the projected values. We then supervise the produced material maps using the predicted 2D material maps. Additionally, using deferred shading we supervise by a PBR-based photometric rendering loss with the multi-view ground truth images of the object.",
                "position": 180
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.18314/images/overall_comparison.png",
                "caption": "Figure 3:Relighting Comparisonbetween our method, an extended version of R3DGS[gao:24]and IRGS[gu:25]. The objects are all relit under the same environment maps. In IRGS, reconstructed scene geometry might partially occlude the environment map.",
                "position": 318
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.18314/images/Experiment_G_Buffers.png",
                "caption": "Figure 4:Material Mapsproduced by our method compared to extended R3DGS[gao:24], which can also predict metallic material maps, IRGS[gu:25], and the DiffusionRenderer material output produced on the test images that are not used for training. We show four images each, where the top left is the base color, top right is the roughness, bottom left is metallic, and bottom right are the normals.",
                "position": 357
            },
            {
                "img": "https://arxiv.org/html/2512.18314/images/real_world_relighting.png",
                "caption": "Figure 5:Real-World Comparisonof our method, extended R3DGS[gao:24]and IRGS[gu:25]. The ground truth images show the object masked (top) and unmasked (bottom) to give a better understanding of the object and the surrounding lighting.",
                "position": 360
            },
            {
                "img": "https://arxiv.org/html/2512.18314/images/ablation_figure.png",
                "caption": "Figure 6:Qualitative Comparisonof our ablations. The differences are most apparent in the base color, which has the largest impact on the qualitative appearance during relighting and consists of three channels rather than one.",
                "position": 576
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledgements",
        "images": []
    },
    {
        "header": "Overview",
        "images": []
    },
    {
        "header": "AAdditional Videos and Real-World Objects",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.18314/images/thumbnail.png",
                "caption": "Figure 7:Thumbnail showing six videos that can be viewedhere. Three videos show relighting comparison and three show material prediction comparisons.",
                "position": 719
            },
            {
                "img": "https://arxiv.org/html/2512.18314/images/Real_world_additional.png",
                "caption": "Figure 8:Additional real objects reconstructed with our method, Extended R3DGS[gao:24]and IRGS[gu:25]. The figure includes relighting under two environments, base color and normal maps.",
                "position": 753
            },
            {
                "img": "https://arxiv.org/html/2512.18314/images/MLP_softmax_difference.png",
                "caption": "Figure 9:The impact of theSoftmaxlayer in the Neural Merger. Without it, lighting and shadow patterns leak into the material maps, leading to inconsistent relighting.",
                "position": 756
            },
            {
                "img": "https://arxiv.org/html/2512.18314/images/DiffusionRenderer_tonemapping.png",
                "caption": "Figure 10:Tone mapping applied by DiffusionRenderer significantly alters the appearance of material maps. The alpha mask removes background content and focuses on the region of interest.",
                "position": 759
            }
        ]
    },
    {
        "header": "BNeural Merger Ablation",
        "images": []
    },
    {
        "header": "CDiffusionRenderer Tone Mapping Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.18314/images/Missed_Gaussians.png",
                "caption": "Figure 11:Super sampling avoids missed Gaussians and ensures reliable projection of material supervision. Lower sampling rates cause holes and unstable geometry.",
                "position": 833
            },
            {
                "img": "https://arxiv.org/html/2512.18314/images/MLP_input.png",
                "caption": "Figure 12:The input to the Neural Merger includes positional codes and projected material parameters.",
                "position": 876
            }
        ]
    },
    {
        "header": "DImplementation Details",
        "images": []
    }
]