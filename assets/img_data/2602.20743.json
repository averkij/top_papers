[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.20743/x1.png",
                "caption": "Figure 1:Overview of our approach. We perform reflective prompt optimization using the GEPA algorithmAgrawal et al. (2025). Our method adapts a base seed prompt into an optimized prompt defining the privacy and utility task requirements. The optimization operates in a strict fixed budget environment while learning sufficiently strong patterns to adapt to the anonymization objective.",
                "position": 176
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Our Approach",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.20743/img/pixtral.png",
                "caption": "Table 1:Results on open source models across all benchmark tasks. The best overall privacy–utility trade-off is shown inbold. Otherwise, the best score for each individual component isunderlined. Prompt optimization consistently yields stronger privacy-utility trade-offs than static anonymization baselines and task-specific prompts, often improving privacy substantially while preserving or even improving utility across diverse tasks and model families.",
                "position": 404
            }
        ]
    },
    {
        "header": "5Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.20743/img/OpenAI.png",
                "caption": "Table 2:Performance of optimized Qwen3-30B-A3B compared to GPT-5 based methods and OpenPII. Models not adaptable on other tasks are marked with -.",
                "position": 540
            },
            {
                "img": "https://arxiv.org/html/2602.20743/img/Qwen.png",
                "caption": "Figure 3:Optimized anonymization prompt forQwen3-30B-A3B on the MedQA task.",
                "position": 617
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethical Considerations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APrompt Optimization Ablation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.20743/x2.png",
                "caption": "Figure 4:A comparison of learning behavior of our modified GEPA implementation against each separated component and a state-of-the-art prompt optimizer reference (MIPROv2). Results are measures withGemma-3-27b-it on SynthPAI (top) andMistral-Small-3.2-24B on TAB (bottom).",
                "position": 1174
            },
            {
                "img": "https://arxiv.org/html/2602.20743/x3.png",
                "caption": "",
                "position": 1178
            }
        ]
    },
    {
        "header": "Appendix BBackbone Evaluation Robustness",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.20743/img/google.png",
                "caption": "Table 3:Evaluation results on original texts (encoded asprivacy/utility). Tasks requiring a backbone LLM (Gemini-2.5-flash) are marked with†\\dagger.",
                "position": 1201
            }
        ]
    },
    {
        "header": "Appendix CModel Scaling",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.20743/img/Qwen.png",
                "caption": "Table 4:Performance of Qwen-2.5-7b compared to Qwen3-30B-A3B and their seed prompt version (encoded asprivacy/utility).",
                "position": 1237
            }
        ]
    },
    {
        "header": "Appendix DImplementation Details",
        "images": []
    },
    {
        "header": "Appendix EComputational Costs.",
        "images": []
    },
    {
        "header": "Appendix FHardware and Code",
        "images": []
    },
    {
        "header": "Appendix GScientific Artifacts",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.20743/img/OpenAI.png",
                "caption": "Table 6:Additional qualitative examples for each text anonymization method.",
                "position": 2184
            }
        ]
    },
    {
        "header": "Appendix HTask Details",
        "images": []
    }
]