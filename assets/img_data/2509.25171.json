[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25171/Figures/fig1.png",
                "caption": "Figure 1:Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion.Our framework has two key components:(1) a tree search algorithmto generate a replay buffer of diffusion trajectories optimized for one or more reward functions using the current policy and(2) an off-policy RL algorithmfor discrete diffusion fine-tuning using the optimized replay buffer.",
                "position": 302
            }
        ]
    },
    {
        "header": "3Enhancing Reinforcement Learning with Structured Search",
        "images": []
    },
    {
        "header": "4TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning",
        "images": []
    },
    {
        "header": "5Multi-Objective Fine-Tuning withTR2-D2",
        "images": []
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25171/Figures/fig2-with-docking.png",
                "caption": "Figure 2:Peptide docking results and comparison of multi-objective fine-tuning with and without MCTS.(Top)Docked peptides to TfR, GLP-1R, and GLAST with docking scores (↓\\downarrow) and polar contacts within3.53.5Å annotated.(Bottom)Average multi-reward values of 50 sequences sampled from the fine-tuned model after each fine-tuning epoch are plotted over a total of10001000epochs, and a running average is shown with the smooth line.",
                "position": 819
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Declarations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Overview of Appendix",
        "images": []
    },
    {
        "header": "Appendix ARelated Works",
        "images": []
    },
    {
        "header": "Appendix BExtended Theoretical Background",
        "images": []
    },
    {
        "header": "Appendix CTheoretical Proofs",
        "images": []
    },
    {
        "header": "Appendix DRegulatory DNA Experiment Details",
        "images": []
    },
    {
        "header": "Appendix EPeptide Experiment Details",
        "images": []
    },
    {
        "header": "Appendix FHyperparameter Discussion and Ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25171/Figures/peptide-protein.png",
                "caption": "Figure 3:Multi-objective reward curves for fine-tuning toward high binding affinity to proteins TfR, GLP-1R, and GLAST.Average reward values of5050sequences sampled from the fine-tuned model after each fine-tuning epoch are plotted over a total of10001000epochs, and a running average is shown with the smooth line. The mean buffer reward is computed after every buffer resampling step (every1010epochs). We observe that the multi-objective fine-tuning method effectively enables optimization of rewards for diverse therapeutic targets.",
                "position": 4627
            },
            {
                "img": "https://arxiv.org/html/2509.25171/Figures/mcts-iteration-ablation.png",
                "caption": "Figure 4:Ablation study on the number of iterations of MCTSNiterN_{\\text{iter}}per buffer generation step for multi-objective peptide generation.Average reward values of5050sequences sampled from the fine-tuned model after each fine-tuning epoch are plotted over a total of10001000epochs, and a running average is shown with the smooth line. The mean buffer reward is computed after every buffer resampling step (every1010epochs). We observe a steady increase in the mean rewards stored in the buffer with a larger number of iterations.",
                "position": 4630
            },
            {
                "img": "https://arxiv.org/html/2509.25171/Figures/peptide-child-ablation.png",
                "caption": "Figure 5:Ablation study on the number of children nodesMMexplored in each iteration of MCTS.Average reward values of5050sequences sampled from the fine-tuned model after each fine-tuning epoch are plotted over a total of10001000epochs, and a running average is shown with the smooth line. The mean buffer reward is computed after every buffer resampling step (every1010epochs). We observe a steady increase in the mean rewards stored in the buffer as the number of child sequences explored increases.",
                "position": 4633
            },
            {
                "img": "https://arxiv.org/html/2509.25171/Figures/peptide-resample-ablation.png",
                "caption": "Figure 6:Ablation study on the number of training epochsNresampleN_{\\text{resample}}between each buffer resampling step.Average reward values of5050sequences sampled from the fine-tuned model after each fine-tuning epoch are plotted over a total of10001000epochs, and a running average is shown with the smooth line. The mean buffer reward is computed after every buffer resampling step (every1010epochs). We observe a steady increase in the mean rewards as theNresampleN_{\\text{resample}}increases, indicating that the model can inherit the ability to generate high-reward sequences seen in the buffer with more training iterations.",
                "position": 4636
            }
        ]
    },
    {
        "header": "Appendix GAlgorithms",
        "images": []
    }
]