[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.10687/extracted/6139890/pics/motivation.png",
                "caption": "Figure 1:The motivation behind our method. Human motion, similar to that of robots, involves planning the ”end-effector” (EE), typically the hands, towards the target position. The rest of the body then cooperates accordingly with the EE, abiding by inverse kinematics principles.",
                "position": 80
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.10687/extracted/6139890/pics/pipeline_stage1.png",
                "caption": "Figure 2:Overview of the stage 1 hand motion generation framework. The framework includes serveral DiT blocks as backbone. Audio embeddings are injected via cross-attention, style and speed embeddings are added on timestep, previous motion latent sequence is concatenated on current noisy motion latent sequence for smooth transition. Hand masks that mask out invisible hands frames are directly added on noisy motion latent.",
                "position": 157
            },
            {
                "img": "https://arxiv.org/html/2501.10687/extracted/6139890/pics/pipeline_stage2.png",
                "caption": "Figure 3:The overview of the Stage 2 video generation pipeline, which is based on the Parallel Reference Network structure. The ReferenceNet extracts visual features from both the reference image and motion frames. The MANO maps and keypoint maps generated in Stage 1 are passed through the denoising Backbone Network to guide the character’s motion. Additionally, trainable hand confidence embeddings enhance the quality of the generated hands. The audio embeddings are injected to ensure synchronization between audio and visual elements.",
                "position": 195
            },
            {
                "img": "https://arxiv.org/html/2501.10687/extracted/6139890/pics/distri_comp.png",
                "caption": "Figure 4:The distribution of the generated hand positions from co-speech gesture generation methods based on Talkshow dataset. From left to right: Ours MANO based,Ours SMPL based, Talkshow, Diffsheg.",
                "position": 438
            }
        ]
    },
    {
        "header": "4Experimets",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.10687/extracted/6139890/pics/quality_emtd2.png",
                "caption": "Figure 5:The qualitative comparisons with pose-driven body animation methods, based on the EMTD dataset.",
                "position": 964
            },
            {
                "img": "https://arxiv.org/html/2501.10687/extracted/6139890/pics/quality_demo.png",
                "caption": "Figure 6:The qualitative comparisons with audio-driven body animation methods.",
                "position": 984
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]