[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Video Instruction-Following Data Synthesis",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02713/x1.png",
                "caption": "Figure 1:Video sources in the proposedLLaVA-Video-178K. (Left) The relationship between 10 video sources we have utilized and other existing video-language datasets. (Right) Filtering logic for video sources. The detail of filtering logic: ‚ë† Sorted by Views, ‚ë° Number of scenes greater than 2, ‚ë¢ Video duration between 5 seconds and 180 seconds, ‚ë£ Ratio of scenes to video duration less than or equal to 0.5, ‚ë§ Resolution greater than 480p, ‚ë• 50 samples for each category.",
                "position": 482
            },
            {
                "img": "https://arxiv.org/html/2410.02713/x2.png",
                "caption": "Figure 2:The video detail description creation pipeline. A three-level creation pipeline is considered, with each level developed via a recurrent approach.\nNote thattùë°titalic_tis the index of time internal at its own level, andTùëáTitalic_Tis the last time internal index.\n(a) To generate the caption for time internaltùë°titalic_tat level-1, we condition on the current frames in this internal, the caption for time internalt‚àí1ùë°1t-1italic_t - 1, and the most recent description summary at level-2 if applicable.\n(b) To generate caption for time internaltùë°titalic_tat level-2, we condtion on the previous caption at level-2, and captions from three most recent time internals at level-1.\n(c) To generate the overall caption at the last time internalTùëáTitalic_Tat level-3, we condtion on the the most recent caption at level-2 and the current caption from level-1.",
                "position": 498
            },
            {
                "img": "https://arxiv.org/html/2410.02713/x3.png",
                "caption": "Figure 3:Question types for video question answering in data creation. For each type, we provide its name and an example question.",
                "position": 537
            },
            {
                "img": "https://arxiv.org/html/2410.02713/x4.png",
                "caption": "Figure 4:One example to illustrate the video instruction-following data.",
                "position": 563
            },
            {
                "img": "https://arxiv.org/html/2410.02713/x5.png",
                "caption": "Figure 5:Distribution of data across different datasets and question types (Caption, Open-ended, and Multi-Choice).",
                "position": 576
            },
            {
                "img": "https://arxiv.org/html/2410.02713/x6.png",
                "caption": "Figure 6:(Left) Visualization of the video duration. (Middle) Visualization of the number of words in the video caption. (Right) Visualization of caption length versus video duration.",
                "position": 703
            },
            {
                "img": "https://arxiv.org/html/2410.02713/x7.png",
                "caption": "Figure 7:(Left) Display of YouTube Shorts across four video categories. (Right) Distribution of 5 uniformly chosen video categories.",
                "position": 706
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAuthor Contributions",
        "images": []
    },
    {
        "header": "Appendix BVideo Representations",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02713/x8.png",
                "caption": "Figure 8:Video representations. A different number of tokens are utilized to represent frames.",
                "position": 2251
            }
        ]
    },
    {
        "header": "Appendix CData",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02713/x9.png",
                "caption": "Figure 9:Generating video captions with or without historical context.",
                "position": 2282
            }
        ]
    },
    {
        "header": "Appendix DBeyond Singularity: Extensive Sampling Matters",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02713/x10.png",
                "caption": "Table 10:LLaVA-Videolearns to understand theoptical illusionin the video.",
                "position": 3167
            },
            {
                "img": "https://arxiv.org/html/2410.02713/x11.png",
                "caption": "Table 11:LLaVA-Videolearns to understand the the video inspecial domain.",
                "position": 3221
            },
            {
                "img": "https://arxiv.org/html/2410.02713/x12.png",
                "caption": "",
                "position": 3271
            },
            {
                "img": "https://arxiv.org/html/2410.02713/x13.png",
                "caption": "Table 12:LLaVA-Videolearns to understand theunusual actionin the video.",
                "position": 3300
            },
            {
                "img": "https://arxiv.org/html/2410.02713/x14.png",
                "caption": "Table 13:LLaVA-Videolearns to understand thephysical lawsin the video.",
                "position": 3354
            }
        ]
    },
    {
        "header": "Appendix ECapabilities",
        "images": []
    }
]