[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.06218/figs/icon.png",
                "caption": "",
                "position": 96
            },
            {
                "img": "https://arxiv.org/html/2510.06218/x1.png",
                "caption": "Figure 1:Overview of the EgoNight.EgoNight integrates diverse video sources spanning synthetic environments, real-world indoor and outdoor scenes, recorded under both daytime and nighttime conditions, with spatial and temporal alignment.\nIt consists of three benchmarks: (i)egocentric VQAas the primary focus, (ii)day–night correspondence retrieval, and (iii)egocentric depth estimation, all targeting the challenges of low-light egocentric vision. The day–night alignment (illustrated on the right with VQA examples) enables rigorous analysis of illumination gaps in MLLMs.",
                "position": 112
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3EgoNight Dataset & Benchmarks",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.06218/x2.png",
                "caption": "Figure 2:EgoNight construction and EgoNight-VQA annotation.EgoNight integrates EgoNight-Synthetic, EgoNight-Sofia, and EgoNight-Oxford sources. Annotation is achieved via a novel three-stage day-augmented Auto QA generation pipeline with 300+ hours of human refinement, resulting in over 3600 high-quality QA pairs.",
                "position": 194
            },
            {
                "img": "https://arxiv.org/html/2510.06218/x3.png",
                "caption": "Figure 3:QA types with examples.The first eight arepairedtypes, where the same question–answer applies to both day and night clips; the last four areunpaired, evaluated only at night. QA Types have various durations, with static or spatial tasks (e.g., 1 and 3) using short clips, while dynamic or temporal tasks (e.g., 4 and 5) use full videos.",
                "position": 272
            },
            {
                "img": "https://arxiv.org/html/2510.06218/x4.png",
                "caption": "Figure 4:Statistics of EgoNight-VQA benchmark.(a) Distribution of QA pairs across QA types and sources. (b) Video duration distribution. (c) Task difficulty levels cross scenarios. (d) Scenario coverage. (e) Illumination coverage.",
                "position": 298
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.06218/x5.png",
                "caption": "Figure 5:Performance analysis of MLLMs on EgoNight-VQA.(a) Day–night performance gap across paired QA types, showing consistent degradation at night. (b) Nighttime performance across all 12 QA types.",
                "position": 536
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.06218/figs/more_synthetic.png",
                "caption": "Figure 6:More examples and modalities of synthetic datasets.",
                "position": 1456
            },
            {
                "img": "https://arxiv.org/html/2510.06218/x6.png",
                "caption": "Figure 8:The prompts used during auto-labeling.",
                "position": 1561
            },
            {
                "img": "https://arxiv.org/html/2510.06218/x7.png",
                "caption": "Figure 9:More QA examples from EgoNight-Synthetic dataset.",
                "position": 1790
            },
            {
                "img": "https://arxiv.org/html/2510.06218/x8.png",
                "caption": "Figure 10:More QA examples from EgoNight-Sofia dataset.",
                "position": 1795
            },
            {
                "img": "https://arxiv.org/html/2510.06218/x9.png",
                "caption": "Figure 11:More QA examples from EgoNight-Oxford dataset.",
                "position": 1800
            },
            {
                "img": "https://arxiv.org/html/2510.06218/x10.png",
                "caption": "Figure 12:More QA examples with day and night answer produced by the same model.",
                "position": 2807
            },
            {
                "img": "https://arxiv.org/html/2510.06218/figs/SpatialRetrievalQualitative.png",
                "caption": "Figure 13:Qualitative Result on one meta sample of spatial retrieval. The query video clip and the database video clips are visualized in the image. The table below the figure shows the similarity score between the query and the database clips calculated with different methods. The most similar one is inbold, and correct retrieval is ingreen, and the incorrect one is inred.",
                "position": 2821
            },
            {
                "img": "https://arxiv.org/html/2510.06218/x11.png",
                "caption": "Figure 14:Qualitative results of monodepth estimation in day and night on EgoNight-Synthetic dataset according to different difficulty levels.",
                "position": 2908
            },
            {
                "img": "https://arxiv.org/html/2510.06218/x12.png",
                "caption": "Figure 15:Qualitative results of monodepth estimation in day and night on EgoNight-Sofia, indoor.",
                "position": 2913
            },
            {
                "img": "https://arxiv.org/html/2510.06218/x13.png",
                "caption": "Figure 16:Qualitative results of monodepth estimation in day and night on EgoNight-Sofia, outdoor.",
                "position": 2918
            },
            {
                "img": "https://arxiv.org/html/2510.06218/x14.png",
                "caption": "Figure 17:Qualitative results of monodepth estimation in day and night on EgoNight-Oxford dataset, note that DAV2 and SVGGT are shortened for Depth Anything V2 and StreamVGGT, respectively.",
                "position": 2923
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]