[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16334/x1.png",
                "caption": "",
                "position": 169
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16334/x2.png",
                "caption": "Figure 2:Data Pipelines of OpenMMReasoner.We propose two training recipes covering both the SFT and RL phases. The pipeline begins by collecting diverse data sources and selecting teacher models to generate new answer traces. During the RL phase, we explore different algorithm choices and filtering strategies, leading to our final optimized recipe.",
                "position": 266
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Supervised Fine-tuning Recipe",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16334/x3.png",
                "caption": "Figure 3:Data Source Distribution OpenMMReasoner.Our dataset comprises diverse sources across multiple domains, aiming to balance data diversity and efficiency for optimal performance.",
                "position": 286
            }
        ]
    },
    {
        "header": "4Reinforcement Learning Recipe",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16334/x4.png",
                "caption": "Table 7:Recipe selection results of different RL training strategies and coldstart starting point on Visual Reasoning Benchmarks.Unless otherwise specified, the rollout temperature is set to 1.0 by default.",
                "position": 1155
            },
            {
                "img": "https://arxiv.org/html/2511.16334/x4.png",
                "caption": "Figure 4:Overall results across different algorithms.We conduct a systematic comparison of various algorithms under identical multimodal RL training settings. GSPO demonstrates the highest training stability, exploration capability, and overall efficiency.",
                "position": 1512
            },
            {
                "img": "https://arxiv.org/html/2511.16334/x5.png",
                "caption": "Figure 5:Training dynamics on the validation set during RL.During RL training, we observe that textual reasoning ability improves alongside visual reasoning, even when trained solely on multimodal data, indicating strong cross-domain generalization of reasoning capabilities.",
                "position": 1515
            },
            {
                "img": "https://arxiv.org/html/2511.16334/x6.png",
                "caption": "Figure 6:Token efficiency comparison with OVR.OpenMMReasoner achieve better accuracy while using significantly less token budget.",
                "position": 1541
            },
            {
                "img": "https://arxiv.org/html/2511.16334/x7.png",
                "caption": "Figure 7:Effect of rollout count on multimodal RL training stability.DAPO becomes unstable with fewer rollouts, while increasing the rollout count leads to more stable training dynamics.",
                "position": 1623
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16334/x8.png",
                "caption": "Figure 1:Rollout Analysis over RL.With the progress of RL training, the model response contains more reflection word ratio.",
                "position": 1648
            }
        ]
    },
    {
        "header": "1Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16334/images/examples/example_10_0.png",
                "caption": "Table 2:Additional evaluation results on Sampling Strategy.Scaling up with more answer generation leads to better result.",
                "position": 1718
            },
            {
                "img": "https://arxiv.org/html/2511.16334/images/examples/example_10_0.png",
                "caption": "Table 3:Reward ablation result.",
                "position": 1835
            }
        ]
    },
    {
        "header": "2Additional Result and Analysis",
        "images": []
    },
    {
        "header": "3Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16334/images/examples/example_10_0.png",
                "caption": "Table 6:An example of the coldstart data sample.",
                "position": 2100
            },
            {
                "img": "https://arxiv.org/html/2511.16334/images/examples/example_100_0.jpg",
                "caption": "Table 7:An example of the coldstart data sample.",
                "position": 2140
            },
            {
                "img": "https://arxiv.org/html/2511.16334/images/examples/mmmu_pro_0.png",
                "caption": "Table 8:An example of our model inference result",
                "position": 2180
            },
            {
                "img": "https://arxiv.org/html/2511.16334/images/examples/mmmu_pro_696.png",
                "caption": "Table 9:An example of our model inference result",
                "position": 2236
            }
        ]
    },
    {
        "header": "4Limitation and Future Work",
        "images": []
    }
]