[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23052/x1.png",
                "caption": "Figure 1:(Left) The knocking-heads attention architecture. Purple represents the original multi-head attention, while pink represents the added knocking-heads projections.TQT^{Q}andTKT^{K}within the dashed box are optional projections due to their lower importance compared toTVT^{V}. (Right) Training loss over 1T tokens for 6.1B MoE models (1.01B activated parameters): baselinevs.knocking-heads attention. KHA reduces loss spikes and maintains consistently lower training loss.",
                "position": 97
            },
            {
                "img": "https://arxiv.org/html/2510.23052/x2.png",
                "caption": "",
                "position": 106
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23052/x3.png",
                "caption": "(a)A0.44b-2.3B",
                "position": 1135
            },
            {
                "img": "https://arxiv.org/html/2510.23052/x3.png",
                "caption": "(a)A0.44b-2.3B",
                "position": 1138
            },
            {
                "img": "https://arxiv.org/html/2510.23052/x4.png",
                "caption": "(b)A1.6b-14.6B",
                "position": 1144
            },
            {
                "img": "https://arxiv.org/html/2510.23052/x5.png",
                "caption": "(c)A0.8b-6.6B(MLP)",
                "position": 1150
            },
            {
                "img": "https://arxiv.org/html/2510.23052/x6.png",
                "caption": "(d)A0.8b-6.6B(Linear)",
                "position": 1155
            },
            {
                "img": "https://arxiv.org/html/2510.23052/x7.png",
                "caption": "Figure 3:Visualization of learned knocking-heads projection weights across different layers and types. We apply 0-1 clipping to all knocking-heads projection weights exceptWg​a​t​eW^{gate}, includingTKT^{K},TQT^{Q},TVT^{V},Wu​pW^{up}, andWd​o​w​nW^{down}, for comparative analysis.",
                "position": 1169
            },
            {
                "img": "https://arxiv.org/html/2510.23052/x7.png",
                "caption": "",
                "position": 1172
            },
            {
                "img": "https://arxiv.org/html/2510.23052/x8.png",
                "caption": "",
                "position": 1176
            },
            {
                "img": "https://arxiv.org/html/2510.23052/x9.png",
                "caption": "",
                "position": 1180
            },
            {
                "img": "https://arxiv.org/html/2510.23052/x10.png",
                "caption": "",
                "position": 1184
            },
            {
                "img": "https://arxiv.org/html/2510.23052/x11.png",
                "caption": "",
                "position": 1189
            },
            {
                "img": "https://arxiv.org/html/2510.23052/x12.png",
                "caption": "",
                "position": 1193
            },
            {
                "img": "https://arxiv.org/html/2510.23052/x13.png",
                "caption": "",
                "position": 1197
            },
            {
                "img": "https://arxiv.org/html/2510.23052/x14.png",
                "caption": "",
                "position": 1201
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]