[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14711/x1.png",
                "caption": "Figure 1:Compute flows of vanilla MoE with TopK routing and ReMoE with ReLU routing. Positive values are shown in orange, and negative values in blue, with deeper colors representing larger absolute values. Zeros, indicating sparsity and computation savings, are shown in white. The red dash arrows in TopK routing indicate discontinuous operations. Compared with TopK routing MoE, ReMoE uses ReLU to make the compute flow fully differentiable.",
                "position": 139
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Our Method: ReMoE",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14711/x2.png",
                "caption": "Figure 2:Comparison between TopK and ReLU.",
                "position": 202
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x3.png",
                "caption": "Figure 3:The sparsity of ReMoE withE=8,k=1formulae-sequence𝐸8𝑘1E=8,k=1italic_E = 8 , italic_k = 1is effectively maintained around the desired target. Sparsity values for all steps are plotted without averaging or sampling. The mean and standard deviation are calculated excluding the first 100 warm-up steps.",
                "position": 310
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x4.png",
                "caption": "(a)SparsitySisubscript𝑆𝑖S_{i}italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT",
                "position": 327
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x4.png",
                "caption": "(a)SparsitySisubscript𝑆𝑖S_{i}italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT",
                "position": 330
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x5.png",
                "caption": "(b)Coefficient termλisubscript𝜆𝑖\\lambda_{i}italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTand regularization termℒr⁢e⁢gsubscriptℒ𝑟𝑒𝑔\\mathcal{L}_{reg}caligraphic_L start_POSTSUBSCRIPT italic_r italic_e italic_g end_POSTSUBSCRIPT",
                "position": 336
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x6.png",
                "caption": "(c)Language model lossℒl⁢msubscriptℒ𝑙𝑚\\mathcal{L}_{lm}caligraphic_L start_POSTSUBSCRIPT italic_l italic_m end_POSTSUBSCRIPTand overall regularizationλi⁢ℒr⁢e⁢gsubscript𝜆𝑖subscriptℒ𝑟𝑒𝑔\\lambda_{i}\\mathcal{L}_{reg}italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_r italic_e italic_g end_POSTSUBSCRIPT",
                "position": 341
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14711/x7.png",
                "caption": "Figure 5:Training curves of different routing methods.",
                "position": 469
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x7.png",
                "caption": "Figure 5:Training curves of different routing methods.",
                "position": 472
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x8.png",
                "caption": "(a)Scaling inN𝑁Nitalic_N",
                "position": 609
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x8.png",
                "caption": "(a)Scaling inN𝑁Nitalic_N",
                "position": 612
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x9.png",
                "caption": "(b)Scaling inE𝐸Eitalic_E",
                "position": 617
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x10.png",
                "caption": "(c)Scaling inG𝐺Gitalic_G",
                "position": 622
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14711/x11.png",
                "caption": "Figure 7:Correlation between expert allocation and token frequency in ReMoE. X-axis is sorted by average active expert count and token frequency is in log-scale.",
                "position": 692
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x12.png",
                "caption": "(a)Training curves of MoE and ReMoE with and without load balancing",
                "position": 705
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x12.png",
                "caption": "(a)Training curves of MoE and ReMoE with and without load balancing",
                "position": 708
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x13.png",
                "caption": "(b)Average routed tokens ratio of ReMoE w.o. LB",
                "position": 713
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x14.png",
                "caption": "(c)Average routed tokens ratio of ReMoE w. LB",
                "position": 718
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x15.png",
                "caption": "(d)Sparsity across different layers in ReMoE",
                "position": 723
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x16.png",
                "caption": "(a)Domain specialization of MoE",
                "position": 746
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x16.png",
                "caption": "(a)Domain specialization of MoE",
                "position": 749
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x17.png",
                "caption": "(b)Domain specialization of ReMoE",
                "position": 755
            }
        ]
    },
    {
        "header": "6Related Works",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AStability analysis of TopK and ReLU",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14711/x18.png",
                "caption": "Figure 10:Flip rate and flip count of MoE and ReMoE",
                "position": 1456
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x18.png",
                "caption": "",
                "position": 1459
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x19.png",
                "caption": "",
                "position": 1463
            }
        ]
    },
    {
        "header": "Appendix BInsensitivity toλ0subscript𝜆0\\lambda_{0}italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTandα𝛼\\alphaitalic_α",
        "images": []
    },
    {
        "header": "Appendix CPerformance for Longer Training",
        "images": []
    },
    {
        "header": "Appendix DSpeed Comparison of ReMoE and MoE",
        "images": []
    },
    {
        "header": "Appendix EDownstream Evaluation Results",
        "images": []
    },
    {
        "header": "Appendix FDetailed Results for Domain Specification",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14711/x20.png",
                "caption": "(a)Domain specialization of MoE",
                "position": 2392
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x20.png",
                "caption": "(a)Domain specialization of MoE",
                "position": 2395
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x21.png",
                "caption": "(b)Domain specialization of ReMoE",
                "position": 2401
            }
        ]
    },
    {
        "header": "Appendix GDomain-Level Dynamic Expert Allocation in ReMoE",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14711/x22.png",
                "caption": "Figure 12:Domain-level dynamic expert allocation",
                "position": 2510
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x23.png",
                "caption": "Figure 13:Training curves of MoE with different warmup steps",
                "position": 2950
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x23.png",
                "caption": "Figure 13:Training curves of MoE with different warmup steps",
                "position": 2953
            }
        ]
    },
    {
        "header": "Appendix HTraining MoE with Near-Dense Warmup",
        "images": []
    }
]