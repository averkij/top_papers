[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14711/x1.png",
                "caption": "Figure 1:Compute flows of vanilla MoE with TopK routing and ReMoE with ReLU routing. Positive values are shown in orange, and negative values in blue, with deeper colors representing larger absolute values. Zeros, indicating sparsity and computation savings, are shown in white. The red dash arrows in TopK routing indicate discontinuous operations. Compared with TopK routing MoE, ReMoE uses ReLU to make the compute flow fully differentiable.",
                "position": 139
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Our Method: ReMoE",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14711/x2.png",
                "caption": "Figure 2:Comparison between TopK and ReLU.",
                "position": 202
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x3.png",
                "caption": "Figure 3:The sparsity of ReMoE withE=8,k=1formulae-sequence81E=8,k=1italic_E = 8 , italic_k = 1is effectively maintained around the desired target. Sparsity values for all steps are plotted without averaging or sampling. The mean and standard deviation are calculated excluding the first 100 warm-up steps.",
                "position": 310
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x4.png",
                "caption": "(a)SparsitySisubscriptS_{i}italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT",
                "position": 327
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x4.png",
                "caption": "(a)SparsitySisubscriptS_{i}italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT",
                "position": 330
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x5.png",
                "caption": "(b)Coefficient term位isubscript\\lambda_{i}italic_位 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTand regularization termregsubscript\\mathcal{L}_{reg}caligraphic_L start_POSTSUBSCRIPT italic_r italic_e italic_g end_POSTSUBSCRIPT",
                "position": 336
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x6.png",
                "caption": "(c)Language model losslmsubscript\\mathcal{L}_{lm}caligraphic_L start_POSTSUBSCRIPT italic_l italic_m end_POSTSUBSCRIPTand overall regularization位i⑩regsubscriptsubscript\\lambda_{i}\\mathcal{L}_{reg}italic_位 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_r italic_e italic_g end_POSTSUBSCRIPT",
                "position": 341
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14711/x7.png",
                "caption": "Figure 5:Training curves of different routing methods.",
                "position": 469
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x7.png",
                "caption": "Figure 5:Training curves of different routing methods.",
                "position": 472
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x8.png",
                "caption": "(a)Scaling inNNitalic_N",
                "position": 609
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x8.png",
                "caption": "(a)Scaling inNNitalic_N",
                "position": 612
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x9.png",
                "caption": "(b)Scaling inEEitalic_E",
                "position": 617
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x10.png",
                "caption": "(c)Scaling inGGitalic_G",
                "position": 622
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14711/x11.png",
                "caption": "Figure 7:Correlation between expert allocation and token frequency in ReMoE. X-axis is sorted by average active expert count and token frequency is in log-scale.",
                "position": 692
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x12.png",
                "caption": "(a)Training curves of MoE and ReMoE with and without load balancing",
                "position": 705
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x12.png",
                "caption": "(a)Training curves of MoE and ReMoE with and without load balancing",
                "position": 708
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x13.png",
                "caption": "(b)Average routed tokens ratio of ReMoE w.o. LB",
                "position": 713
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x14.png",
                "caption": "(c)Average routed tokens ratio of ReMoE w. LB",
                "position": 718
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x15.png",
                "caption": "(d)Sparsity across different layers in ReMoE",
                "position": 723
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x16.png",
                "caption": "(a)Domain specialization of MoE",
                "position": 746
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x16.png",
                "caption": "(a)Domain specialization of MoE",
                "position": 749
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x17.png",
                "caption": "(b)Domain specialization of ReMoE",
                "position": 755
            }
        ]
    },
    {
        "header": "6Related Works",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AStability analysis of TopK and ReLU",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14711/x18.png",
                "caption": "Figure 10:Flip rate and flip count of MoE and ReMoE",
                "position": 1456
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x18.png",
                "caption": "",
                "position": 1459
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x19.png",
                "caption": "",
                "position": 1463
            }
        ]
    },
    {
        "header": "Appendix BInsensitivity to位0subscript0\\lambda_{0}italic_位 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTand伪\\alphaitalic_伪",
        "images": []
    },
    {
        "header": "Appendix CPerformance for Longer Training",
        "images": []
    },
    {
        "header": "Appendix DSpeed Comparison of ReMoE and MoE",
        "images": []
    },
    {
        "header": "Appendix EDownstream Evaluation Results",
        "images": []
    },
    {
        "header": "Appendix FDetailed Results for Domain Specification",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14711/x20.png",
                "caption": "(a)Domain specialization of MoE",
                "position": 2392
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x20.png",
                "caption": "(a)Domain specialization of MoE",
                "position": 2395
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x21.png",
                "caption": "(b)Domain specialization of ReMoE",
                "position": 2401
            }
        ]
    },
    {
        "header": "Appendix GDomain-Level Dynamic Expert Allocation in ReMoE",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14711/x22.png",
                "caption": "Figure 12:Domain-level dynamic expert allocation",
                "position": 2510
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x23.png",
                "caption": "Figure 13:Training curves of MoE with different warmup steps",
                "position": 2950
            },
            {
                "img": "https://arxiv.org/html/2412.14711/x23.png",
                "caption": "Figure 13:Training curves of MoE with different warmup steps",
                "position": 2953
            }
        ]
    },
    {
        "header": "Appendix HTraining MoE with Near-Dense Warmup",
        "images": []
    }
]