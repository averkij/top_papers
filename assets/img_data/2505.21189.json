[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21189/x1.png",
                "caption": "Figure 1:Two \"proto-tokens\" (trainable embeddings) are fed into frozen, pre-trained LLM and optimized in such a way, that the LLM predicts an arbitrary target token-sequence in a single forward pass. One of the \"proto-tokens\" (etsubscriptùëíùë°e_{t}italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT) is trained for each text separately, while the other (mùëömitalic_m) could be reused.",
                "position": 127
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments and results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21189/x2.png",
                "caption": "Figure 2:Maximum language information (HL‚Å¢MsubscriptùêªùêøùëÄH_{LM}italic_H start_POSTSUBSCRIPT italic_L italic_M end_POSTSUBSCRIPTfor a maximum text prefix that is accurately reconstructed) compressed for different models and datasets. In the left plot, a single [mem] token is used in the autoregressive setting, and in the non-autoregressive one,pùëùpitalic_pproto-token is shared between all texts within each model. In the right plot, two [mem] tokens are used andpùëùpitalic_pproto-tokens are not shared. Each small point on the plots represents a single text, larger points indicate the average within each (model, dataset) pair.",
                "position": 581
            },
            {
                "img": "https://arxiv.org/html/2505.21189/x3.png",
                "caption": "Figure 3:Reconstruction throughput comparison between autoregressive and non-autoregressive setups. For each (model, dataset) pair, the throughput is calculated as a maximum losslessly compressible length divided by the reconstruction time. To measure reconstruction time, we use PyTorch profiling tools.",
                "position": 587
            },
            {
                "img": "https://arxiv.org/html/2505.21189/x4.png",
                "caption": "Figure 4:Cosine embedding distances for different pairings of proto-tokens. We select 50 contexts from PG19 and for each context, generate 10 continuation texts. We find one solution for each of the first 9 generations and 10 different-seed solutions for the last generation.",
                "position": 599
            },
            {
                "img": "https://arxiv.org/html/2505.21189/x5.png",
                "caption": "Figure 5:We compare proto-token embedding distances for same context text pairs and different-context text pairs. Token-level distance is measured as cosine distance between TF-IDF embeddings. Semantic distance is measured as cosine distance between semantic text embeddings (see Section3for details).",
                "position": 602
            },
            {
                "img": "https://arxiv.org/html/2505.21189/x6.png",
                "caption": "Figure 6:Pairwise interpolation accuracies between10101010solutions for5555texts (5√ó10√ó9/2510925\\times 10\\times 9/25 √ó 10 √ó 9 / 2pairs in total).",
                "position": 615
            }
        ]
    },
    {
        "header": "5Discussion and Conclusions",
        "images": []
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]