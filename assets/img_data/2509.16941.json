[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.16941/images/tweet_figure.png",
                "caption": "Figure 1:SWE-Bench Prois a dataset with challenging, enterprise-level, long-horizon software engineering tasks. Frontier models, such as GPT-5 and Claude Opus 4.1, score less than 25% onSWE-Bench Prowith the SWE-Agent[22]scaffold. We design the dataset with contamination resistance, difficulty filtering, and human augmentation/verification.",
                "position": 211
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Dataset Overview",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.16941/x1.png",
                "caption": "Figure 2:SWE-Bench Prois designed to mimic real, challenging software engineering tasks â€“ with larger changes, across multiple files, sourced from professional software engineering repositories. Frontier models, such as GPT-5 and Claude Opus 4.1, score >70% of SWE-Bench Verified but less than 25% onSWE-Bench Pro. Patches are generated with SWE-Agent[22]and evaluated on the public subset ofSWE-Bench Pro.",
                "position": 276
            }
        ]
    },
    {
        "header": "4Dataset Creation",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.16941/images/fies_and_categories.png",
                "caption": "Figure 3:Distributions in the public set ofSWE-Bench Pro.SWE-Bench Procontains complex, long-horizon tasks involving several files and across a variety of task types. We include a diverse selection of feature requests as well as bug fixes, across optimization, security, UI/UX, and backend changes.",
                "position": 350
            }
        ]
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.16941/images/languages_and_repos.png",
                "caption": "Figure 4:Model performance varies across languages, and models current perform better at Python. Resolve rates across different repos in the public set ofSWE-Bench Pro.SWE-Bench Proincludes a variety of repos across different languages, with a similar number of problems per repo. Note that some categories, especially 10+ files and 500+ LOC, contain about 20-30 examples and thus have higher variance.",
                "position": 496
            }
        ]
    },
    {
        "header": "6Analysis",
        "images": []
    },
    {
        "header": "7Limitations and Future Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AExample Task Instance",
        "images": []
    },
    {
        "header": "Appendix BTrajectory Failure Mode Analysis",
        "images": []
    }
]