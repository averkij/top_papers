[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.23798/extracted/6322337/hf.png",
                "caption": "",
                "position": 107
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.23798/x1.png",
                "caption": "Figure 1:Token depth map illustrating layer-skipping patterns when applying FlexiDepth to Llama-3-8B-Instruct. The light-to-dark blue gradient indicates layer usage ranging from 16 to 32.",
                "position": 138
            },
            {
                "img": "https://arxiv.org/html/2503.23798/x2.png",
                "caption": "Figure 2:The FlexiDepth layer. Left: Full-processing path where hidden states undergo the pre-trained attention and FFN modules. Right: Skipping path where hidden states bypass the attention module and processed by a lightweight adapter. The router and adaptor (in red) are the only trainable components within the FlexiDepth Block.",
                "position": 150
            }
        ]
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.23798/x3.png",
                "caption": "Figure 3:Comparison of attention masks. Left: the vanilla attention mask. Middle: the attention mask without KV Cache, where hidden statesx1subscript洧논1x_{1}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTandx3subscript洧논3x_{3}italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPTskip this layer. Right: the attention mask with KV Cache, where despitex1subscript洧논1x_{1}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTandx3subscript洧논3x_{3}italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPTskipping this layer, their corresponding keys and values are still computed.",
                "position": 202
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": []
    },
    {
        "header": "4Limitations",
        "images": []
    },
    {
        "header": "5Related works",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALayer allocation Examples",
        "images": []
    }
]