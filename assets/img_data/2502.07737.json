[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07737/x1.png",
                "caption": "Figure 1:3D discrete token map produced by our video tokenizer. The input video consists ofone initial frame, followed bynùëõnitalic_nclips, with each clip containingFTsubscriptùêπùëáF_{T}italic_F start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPTframes.xj(i)subscriptsuperscriptùë•ùëñùëóx^{(i)}_{j}italic_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPTindicates thejt‚Å¢hsuperscriptùëóùë°‚Ñéj^{th}italic_j start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPTvideo token in theit‚Å¢hsuperscriptùëñùë°‚Ñéi^{th}italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPTclip.",
                "position": 178
            },
            {
                "img": "https://arxiv.org/html/2502.07737/x1.png",
                "caption": "Figure 1:3D discrete token map produced by our video tokenizer. The input video consists ofone initial frame, followed bynùëõnitalic_nclips, with each clip containingFTsubscriptùêπùëáF_{T}italic_F start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPTframes.xj(i)subscriptsuperscriptùë•ùëñùëóx^{(i)}_{j}italic_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPTindicates thejt‚Å¢hsuperscriptùëóùë°‚Ñéj^{th}italic_j start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPTvideo token in theit‚Å¢hsuperscriptùëñùë°‚Ñéi^{th}italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPTclip.",
                "position": 181
            },
            {
                "img": "https://arxiv.org/html/2502.07737/x2.png",
                "caption": "Figure 2:Examples of block include token-wise, row-wise, and frame-wise representations. When the block size is set to 1√ó\\times√ó1√ó\\times√ó1, it degenerates into a token, as used in vanilla AR modeling. Note that the actual token corresponds to a 3D cube, we omit the time dimension here for clarity.",
                "position": 186
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07737/x3.png",
                "caption": "Figure 3:Comparison between a vanilla autoregressive (AR) framework based on next-token prediction (left) and our semi-AR framework based on next-block prediction (right).xj(i)subscriptsuperscriptùë•ùëñùëóx^{(i)}_{j}italic_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPTindicates thejt‚Å¢hsuperscriptùëóùë°‚Ñéj^{th}italic_j start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPTvideo token in theit‚Å¢hsuperscriptùëñùë°‚Ñéi^{th}italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPTblock, with each block containingLùêøLitalic_Ltokens.\nThe dashed line in the right panel presents that theLùêøLitalic_Ltokens generated in the current step are duplicated and concatenated with prefix tokens, forming the input for the next step‚Äôs prediction during inference.",
                "position": 263
            },
            {
                "img": "https://arxiv.org/html/2502.07737/x4.png",
                "caption": "Figure 4:Causal attention mask in NTP modeling v.s. block-wise attention mask in NBP modeling. The x-axis and y-axis represent keys and queries, respectively.",
                "position": 335
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07737/x5.png",
                "caption": "Figure 5:Validation loss of various sizes of semi-AR models from 700M to 3B.",
                "position": 458
            },
            {
                "img": "https://arxiv.org/html/2502.07737/x5.png",
                "caption": "Figure 5:Validation loss of various sizes of semi-AR models from 700M to 3B.",
                "position": 461
            },
            {
                "img": "https://arxiv.org/html/2502.07737/x6.png",
                "caption": "Figure 6:Validation loss of various block sizes from 1 to 256.",
                "position": 466
            },
            {
                "img": "https://arxiv.org/html/2502.07737/x7.png",
                "caption": "Figure 7:Generation quality (FVD, lower is better) and inference speed (FPS, higher is better) of various block sizes from 1 to 256.",
                "position": 818
            },
            {
                "img": "https://arxiv.org/html/2502.07737/x8.png",
                "caption": "Figure 8:Visualization of class-conditional generation (UCF-101) results of our method. The text below each video clip is the class name.",
                "position": 861
            },
            {
                "img": "https://arxiv.org/html/2502.07737/x9.png",
                "caption": "Figure 9:Visualization of frame prediction (K600) results of our method.",
                "position": 864
            },
            {
                "img": "https://arxiv.org/html/2502.07737/x10.png",
                "caption": "Figure 10:Frame prediction results of OmniTokenizer and our method. The left part is the condition, and the right part is the predicted subsequent sequence.",
                "position": 867
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BPerformance of Video Tokenizer",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07737/x11.png",
                "caption": "Figure 11:Video reconstruction results (17 frames 128√ó\\times√ó128 resolution at 25 fps and shown at 6.25 fps) of OmniTokenizer and our method.",
                "position": 2585
            },
            {
                "img": "https://arxiv.org/html/2502.07737/x12.png",
                "caption": "Figure 12:Visualization of video generation results of various model sizes (700M, 1.2B, and 3B).",
                "position": 2588
            },
            {
                "img": "https://arxiv.org/html/2502.07737/x13.png",
                "caption": "Figure 13:Visualization of video generation results of various block sizes (1√ó\\times√ó1√ó\\times√ó1, 1√ó\\times√ó1√ó\\times√ó16 and 1√ó\\times√ó16√ó\\times√ó16).",
                "position": 2591
            },
            {
                "img": "https://arxiv.org/html/2502.07737/x14.png",
                "caption": "Figure 14:Attention weights of next-clip prediction on UCF-101. The horizontal and vertical axis represent the keys and queries, respectively. Two red lines on each axis divide the axis into three segments, corresponding to the text (classname), the first clip, and the second clip. The brightness of each pixel reflects the attention score. We downweight the attention to text tokens by5√ó5\\times5 √óto provide a more clear visualization.",
                "position": 2594
            },
            {
                "img": "https://arxiv.org/html/2502.07737/x15.png",
                "caption": "Figure 15:Spatial attention distribution for a specific query (represented byred√ó\\times√ó) on UCF-101.",
                "position": 2597
            }
        ]
    },
    {
        "header": "Appendix CVisualization",
        "images": []
    }
]