[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04734/extracted/6514725/figure/figure1.png",
                "caption": "Figure 1:Score fluctuation ranges of the Deepseek-R1-Distill series models on relevant benchmarks under variations in subtle evaluation conditions that are often overlooked. The evaluation variables involved in this figure include: the version of the evaluation dataset, the relative position of the instruction, option bias and correct-answer bias in GPQA Diamond, and Tensor Parallelism settings.",
                "position": 106
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Minor Variations, Major Fluctuations",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04734/extracted/6514725/figure/figure2.png",
                "caption": "Figure 2:As the value of N increases, the fluctuation in performance across the four models on AIME24, AIME25, and GPQA Diamond gradually decreases, generally approaching within 1 percentage point at N = 32. It is also noteworthy that DeepSeek-R1-Distill-Qwen-1.5B exhibits the largest fluctuation, while GPQA Diamond, which features a larger sample size, exhibits relatively lower performance variance.",
                "position": 352
            },
            {
                "img": "https://arxiv.org/html/2506.04734/extracted/6514725/figure/figure3.png",
                "caption": "Figure 3:Under the 1-Seed-N setting, the evaluation result fluctuations caused by varying the seed are significantly greater than the baseline reference fluctuation, further confirming that the seed parameter is a critical factor influencing model stability.",
                "position": 423
            },
            {
                "img": "https://arxiv.org/html/2506.04734/extracted/6514725/figure/figure4.png",
                "caption": "Figure 4:Fluctuations in benchmark scores across different versions of the AIME evaluation datasets. The control group—containing complete image information rendered using Asymptote—consistently outperforms other experimental groups.",
                "position": 485
            },
            {
                "img": "https://arxiv.org/html/2506.04734/extracted/6514725/figure/figure5.png",
                "caption": "Figure 5:Compared to other variables, changes in benchmark scores caused by instruction position are relatively minor. However, they still introduce variations in evaluation stability. In certain model-benchmark combinations, placing the instruction before the question yields improved performance.",
                "position": 526
            },
            {
                "img": "https://arxiv.org/html/2506.04734/extracted/6514725/figure/figure6.png",
                "caption": "Figure 6:GPQA Diamond exhibits significant evaluation variability under option and correct answer biases. In particular, the randomized answer group consistently underperforms relative to the control group, suggesting that randomizing option order may undermine the model’s selection stability. Additionally, in the answer position bias experiments, the control group (with the correct answer placed immediately after the question) consistently outperforms all other groups, indicating that placing the correct answer early may enhance model performance.",
                "position": 532
            },
            {
                "img": "https://arxiv.org/html/2506.04734/extracted/6514725/figure/figure7.png",
                "caption": "Figure 7:The impact of Tensor Parallelism (TP) variation on evaluation results is limited. However, for the sake of reproducibility, it is important to explicitly document the exact TP setting used.",
                "position": 631
            }
        ]
    },
    {
        "header": "3Honestly Representing Model Performance is What You Truly Need",
        "images": []
    },
    {
        "header": "4Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]