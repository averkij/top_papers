[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20789/x1.png",
                "caption": "",
                "position": 161
            },
            {
                "img": "https://arxiv.org/html/2601.20789/x2.png",
                "caption": "",
                "position": 165
            },
            {
                "img": "https://arxiv.org/html/2601.20789/x3.png",
                "caption": "",
                "position": 169
            },
            {
                "img": "https://arxiv.org/html/2601.20789/x4.png",
                "caption": "Figure 1:(a)Scaling and cost comparison of coding agent training approaches using self-hosted vLLM inference.(b)Repository specialization scaling law on Django, whereŒ±\\alphadenotes the fraction of Django-specific data in the training mixture. With full specialization (Œ±=1.0\\alpha=1.0), the model matches teacher performance at 8k samples; general data alone (Œ±=0.0\\alpha=0.0) requires 25k samples ‚Äì a 3.5√ó\\timesadvantage in sample efficiency.",
                "position": 171
            },
            {
                "img": "https://arxiv.org/html/2601.20789/x5.png",
                "caption": "Figure 2:Overview ofSVG(Soft Verified Generation).In the first rollout, a teacher model is prompted to make a change starting from a randomly selected function, producing a trajectory and patch. This trajectory is converted into a synthetic pull request. In the second rollout, the teacher attempts to reproduce the patch given only the PR description. Soft verification compares the two patches using line-level recall for training data selection. We user‚â•0.5r\\geq 0.5as an example threshold.",
                "position": 174
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Main Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20789/x6.png",
                "caption": "Figure 3:Scaling and cost comparison of coding agent training approaches.Top:API cost when using z.ai with cached input pricing.Bottom:vLLM cost when self-hosting the teacher model. Horizontal lines indicate the cost at which our scaling law predicts matching Devstral-Small-2 and GLM-4.5-Air performance. Exact data points are provided in Table11.",
                "position": 837
            },
            {
                "img": "https://arxiv.org/html/2601.20789/x7.png",
                "caption": "",
                "position": 841
            },
            {
                "img": "https://arxiv.org/html/2601.20789/x8.png",
                "caption": "Figure 4:Scaling law for repository specialization on Django. The specialization ratioŒ±\\alphadenotes the fraction of Django-specific data in the training mixture, with the remainder being general coding data. Dashed horizontal lines show the performance of GLM-4.5-Air and Devstral-Small-2 on Django instances, with shaded regions indicating¬±1\\pm 1standard deviation. With full specialization (Œ±=1.0\\alpha=1.0), the student model matches teacher performance at approximately 8,000 samples, significantly outperforming training on general data alone (Œ±=0.0\\alpha=0.0). Specialization performance increases with the ratio of Django-specific data.",
                "position": 902
            }
        ]
    },
    {
        "header": "5Ablations and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20789/x9.png",
                "caption": "Figure 5:Verification analysis comparing soft and hard verification approaches. Scaling curves show SWE-bench Verified performance for different verification thresholds (r=0.0,0.25,0.75,1.0r=0.0,0.25,0.75,1.0) onùêìùüê\\mathbf{T_{2}}trajectories alongside unverifiedùêìùüè\\mathbf{T_{1}}trajectories. All thresholds achieve similar performance at each scale, indicating that strict verification provides no significant benefit over soft or even unverified data.",
                "position": 919
            },
            {
                "img": "https://arxiv.org/html/2601.20789/x10.png",
                "caption": "Figure 6:SWE-bench Verified performance vs. truncation ratio at 32K context. Each point represents 3,000ùêìùüè\\mathbf{T_{1}}trajectories partitioned by truncation ratio, averaged over 3 seeds. Trajectories with truncation ratio 0.95 perform best.",
                "position": 947
            }
        ]
    },
    {
        "header": "6Robustness of Evaluations",
        "images": []
    },
    {
        "header": "7Deployment",
        "images": []
    },
    {
        "header": "8Related Work",
        "images": []
    },
    {
        "header": "9Limitations",
        "images": []
    },
    {
        "header": "10Broader Impact",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AScaling Law and Data Points",
        "images": []
    },
    {
        "header": "Appendix BAdditional Baseline Comparisons",
        "images": []
    },
    {
        "header": "Appendix CSpecialization Results at 64K Context",
        "images": []
    },
    {
        "header": "Appendix DCost Breakdown",
        "images": []
    },
    {
        "header": "Appendix EModel Card: SERA-32B",
        "images": []
    }
]