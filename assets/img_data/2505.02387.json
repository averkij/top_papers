[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02387/x1.png",
                "caption": "Figure 1:The off-the-shelf instruct model overfits to patterns in supervised data, failing to evaluate the emotional harm and lack of nuance in the rejected response. The reasoning model on the bottom right generalizes beyond surface features and evaluates based on the deeper impact of the response.",
                "position": 131
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3RM-R1",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02387/x2.png",
                "caption": "Figure 2:Training pipeline ofRM-R1.Starting from a generative reward model (GenRM),RM-R1training involves two stages:DistillationandReinforcement Learning (RL). In the Distillation stage, we use high-quality synthesized data to bootstrapRM-R1’s reasoning ability. In the RL stage,RM-R1’s reasoning ability for reward modeling is further strengthened. After distillation, a GenRM evolves into aReasRM.RM-R1further differentiates itself by being RL finetuned on preference data.",
                "position": 215
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02387/x3.png",
                "caption": "(a)Model Size",
                "position": 1420
            },
            {
                "img": "https://arxiv.org/html/2505.02387/x3.png",
                "caption": "(a)Model Size",
                "position": 1423
            },
            {
                "img": "https://arxiv.org/html/2505.02387/x4.png",
                "caption": "(b)Inference Compute",
                "position": 1428
            },
            {
                "img": "https://arxiv.org/html/2505.02387/x5.png",
                "caption": "(a)Cold Start RL",
                "position": 1693
            },
            {
                "img": "https://arxiv.org/html/2505.02387/x5.png",
                "caption": "(a)Cold Start RL",
                "position": 1696
            },
            {
                "img": "https://arxiv.org/html/2505.02387/x6.png",
                "caption": "(b)Warm Start RL",
                "position": 1701
            }
        ]
    },
    {
        "header": "6Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASupplementary Information forSection5",
        "images": []
    },
    {
        "header": "Appendix BTraining Dataset Details",
        "images": []
    },
    {
        "header": "Appendix CImplementation Details",
        "images": []
    }
]