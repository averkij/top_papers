[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20785/x1.png",
                "caption": "",
                "position": 144
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3VideoSIAH: A Fine-Grained Data Suite for Evidence-Sparse Long-Video Reasoning",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20785/x2.png",
                "caption": "Figure 2:Data Pipeline of VideoSIAH.We construct a semi-automatic data pipeline that integrates several state-of-the-art LMMs[1,43,5,12]to sequentially perform long video segmentation, video clip captioning, segment-in-a-haystack QA generation, cross-modal QA filtering, and iMCoTT generation.\nIcons with human silhouettes denote human-in-the-loop validation, where annotators inspect a small set of representative failures to refine prompting rules for QA generation, QA filtering, and iMCoTT generation.\nNote that iMCoTT traces are generated only for the cold-start SFT stage,\nwhereas RL training operates solely on the filtered QA pairs.",
                "position": 228
            }
        ]
    },
    {
        "header": "4Training Strategy",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20785/x3.png",
                "caption": "Figure 3:Ablations on Reward Design.The left panel shows training dynamics under different accuracy and time rewards,\nand the right panel shows the effect of tool-call reward on tool usage.",
                "position": 538
            },
            {
                "img": "https://arxiv.org/html/2511.20785/x4.png",
                "caption": "Figure 4:Overall Framework of LongVT.Our approach processes long-form videos in a human-like two-stage manner.\nSpecifically, LongVT is augmented with interleaved Multimodal Chain-of-Tool-Thought (iMCoTT):firstperforms a global skim over sampled video frames to form a coarse hypothesis about when evidence likely occurs;theninvokes a native video toolcrop_video(start_time, end_time)to resample finer-grained frames from a short clip via a hypothesized window and reasons again.\nOur model itself determines whether to directly answer after one turn (T1T_{1}) or continue for multiple turns (up toT5T_{5}) with self-reflection.\nDuring reinforcement learning, we jointly optimize answer correctness (Racc\\textbf{R}_{\\text{acc}}), clean formatting (Rformat\\textbf{R}_{\\text{format}}), and precise temporal grounding (Rtime\\textbf{R}_{\\text{time}}).",
                "position": 644
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7LongVT Performs Human-Aligned Thinking like Leading Proprietary LMMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20785/x5.png",
                "caption": "(a)Watching Strategy of Gemini 2.5 Pro.",
                "position": 2037
            },
            {
                "img": "https://arxiv.org/html/2511.20785/x5.png",
                "caption": "(a)Watching Strategy of Gemini 2.5 Pro.",
                "position": 2040
            },
            {
                "img": "https://arxiv.org/html/2511.20785/x6.png",
                "caption": "(b)Watching Strategy of GPT-5 Thinking.",
                "position": 2045
            }
        ]
    },
    {
        "header": "8What Motivates VideoSIAH? Unveiling the Data Contamination in Qwen-VL Series",
        "images": []
    },
    {
        "header": "9Additional VideoSIAH Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20785/x7.png",
                "caption": "(a)Video Category Distribution",
                "position": 2172
            },
            {
                "img": "https://arxiv.org/html/2511.20785/x7.png",
                "caption": "(a)Video Category Distribution",
                "position": 2175
            },
            {
                "img": "https://arxiv.org/html/2511.20785/x8.png",
                "caption": "(b)Question Category Distribution",
                "position": 2180
            }
        ]
    },
    {
        "header": "10Additional Methodological Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20785/x9.png",
                "caption": "Figure 7:Trend of Reflection-Related Words and the Corresponding Word Cloud across All Rollouts.",
                "position": 2312
            }
        ]
    },
    {
        "header": "11Reflection Trajectory: From Verbose Self-Correction to Internalized Tool Usage",
        "images": []
    },
    {
        "header": "12Additional Implementation Details",
        "images": []
    },
    {
        "header": "13Inference Efficiency Analysis",
        "images": []
    },
    {
        "header": "14Examples",
        "images": []
    },
    {
        "header": "15Failure Case Analysis",
        "images": []
    },
    {
        "header": "16Limitation and Future Direction",
        "images": []
    },
    {
        "header": "17Broader Impact",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20785/x10.png",
                "caption": "Figure 8:Prompt Template Utilized for RL.This template outlines the structural guidelines and system instructions provided to the model during the RL training phase.",
                "position": 2633
            },
            {
                "img": "https://arxiv.org/html/2511.20785/x11.png",
                "caption": "Figure 9:Evaluation Prompt for LLM-as-a-Judge.We present the full system instruction used to query the judge model. This prompt defines the scoring criteria and guidelines to ensure consistent evaluation of the model’s generated responses.",
                "position": 2636
            },
            {
                "img": "https://arxiv.org/html/2511.20785/x12.png",
                "caption": "Figure 10:Representative Data Example for SFT and RFT.The example illustrates the input format and the corresponding ground-truth response used to train the model across both fine-tuning stages.",
                "position": 2639
            },
            {
                "img": "https://arxiv.org/html/2511.20785/x13.png",
                "caption": "Figure 11:An Example of Single-turn Inference with Self-Correction.The model initially misidentifies the basin color as pink. However, through the reasoning process (highlighted in the “Thinking” block), it explicitly decides to double-check the frames, corrects the hallucinations, and outputs the correct answer (Blue).",
                "position": 2642
            },
            {
                "img": "https://arxiv.org/html/2511.20785/x14.png",
                "caption": "Figure 12:An Example of Multi-step Inference Involving Tool Interaction.In this complex query, the model initially crops an incorrect time window (297s-305s) which lacks the target visual information. Recognizing this error during the reasoning phase, it refines the parameters and calls the tool again with the correct window (344s-372s) to successfully identify the US flag.",
                "position": 2645
            },
            {
                "img": "https://arxiv.org/html/2511.20785/x15.png",
                "caption": "Figure 13:Qualitative Comparison between Textual CoT and Our Designed iMCoTT.The baseline textual CoT (left) relies on hallucinated memory, confidently providing an incorrect answer regarding the cars’ colors (“Black and Yellow”). In contrast, our model (right) actively engages with the video content via tool usage. Despite an initial mis-localization (90s-120s), the model explicitly detects the absence of the target object, self-corrects its temporal search window to the correct range (174s-190s), and accurately identifies the cars as “White and Yellow.”",
                "position": 2648
            },
            {
                "img": "https://arxiv.org/html/2511.20785/x16.png",
                "caption": "Figure 14:Failure Case of the RL-only Variant.This example demonstrates the model’s inability to maintain the logical flow after a tool interaction without prior SFT.\nAlthough the model initiates a tool call to inspect the blurred region, it fails to utilize the returned observation to answer the user’s question.\nInstead, it loses the conversational context and hallucinates a repetition of the general video description.",
                "position": 2651
            }
        ]
    },
    {
        "header": "18Ethical Considerations",
        "images": []
    }
]