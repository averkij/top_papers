[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07831/images/Logo.png",
                "caption": "",
                "position": 119
            },
            {
                "img": "https://arxiv.org/html/2512.07831/x1.png",
                "caption": "",
                "position": 143
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07831/x2.png",
                "caption": "Figure 2:Training on unified modalities benefits video generation.Unified multi-modal and multi-task joint training achieves the lowest final loss on RGB video generation, outperforming single-modality joint training and RGB finetuning baseline.",
                "position": 168
            }
        ]
    },
    {
        "header": "2Releated Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07831/x3.png",
                "caption": "Figure 3:Overview of UnityVideo.UnityVideo achieves task unification through a dynamic noise injection strategy applied to input tokens (left), andmodality unificationvia the proposed Modality-Aware AdaLN Table (center).\nSpecifically,LrL_{r}andLmL_{m}denote the learnable parameter tables for the RGB modality and auxiliary video-related modalities (e.g., depth, optical flow, DensePose, skeleton), respectively.CrC_{r}andCmC_{m}represent the prompt condition for RGB video content and in-context modaliy learning prompt, whileVrV_{r}andVmV_{m}correspond to the token sequences from the RGB and auxiliary modalities, respectively.",
                "position": 231
            },
            {
                "img": "https://arxiv.org/html/2512.07831/x4.png",
                "caption": "Figure 4:OpenUni dataset.OpenUni contains 1.3M pairs of unified multimodal data, designed to enrich video modalities with more comprehensive world perception.",
                "position": 278
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07831/x5.png",
                "caption": "Figure 5:Comparison with state-of-the-art methods across diverse tasks. UnityVideo exhibits superior physical reasoning, better adherence to control conditions, and a more detailed understanding of auxiliary modalities.",
                "position": 589
            },
            {
                "img": "https://arxiv.org/html/2512.07831/x6.png",
                "caption": "Figure 6:Unlike single-modality training,unified multimodallearning provides complementary supervision that strengthens both motion understanding and geometric perception.",
                "position": 885
            },
            {
                "img": "https://arxiv.org/html/2512.07831/x7.png",
                "caption": "Figure 7:The In-Context Learner generalizes segmentation to unseen objects, while unified training enhances depth and semantic understanding in RGB video.",
                "position": 901
            }
        ]
    },
    {
        "header": "5Limitation and Future Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07831/x8.png",
                "caption": "Figure 1:Evolution of attention patterns in UnityVideo.Analysis of attention maps shows that interactions between RGB and auxiliary modalities strengthen progressively across layers.\nMeanwhile, the model’s text-following behavior and spatial reasoning capabilities also improve, reflecting more coherent cross-modal integration.",
                "position": 971
            }
        ]
    },
    {
        "header": "Appendix AMore Analysis of Model Design",
        "images": []
    },
    {
        "header": "Appendix BMore Experiments and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07831/x9.png",
                "caption": "Figure 2:Comparison of physical understanding.UnityVideo demonstrates stronger physical reasoning and improved text alignment compared with current state-of-the-art video generation models.",
                "position": 1301
            }
        ]
    },
    {
        "header": "Appendix CDetails of OpenUni and UniBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07831/x10.png",
                "caption": "Figure 3:UniBench consists of two complementary components: (i) high-fidelity Unreal Engine depth data for evaluating depth estimation, and (ii) diverse real-world videos with rich multimodal annotations for assessing video generation quality.",
                "position": 1474
            },
            {
                "img": "https://arxiv.org/html/2512.07831/x11.png",
                "caption": "Figure 4:Representative outputs of UnityVideo on Video Estimation. The model consistently produces coherent RGB videos and aligned modalities—including densepose, optical flow, skeleton, and depth—demonstrating reliable cross-modal generation and estimation across diverse scenarios from human activities to animal motion.",
                "position": 1512
            },
            {
                "img": "https://arxiv.org/html/2512.07831/x12.png",
                "caption": "Figure 5:Representative outputs of UnityVideo on Text2Video and Control Generation. The model consistently produces coherent RGB videos and aligned modalities—including segmentation, densepose, optical flow, skeleton, and depth—demonstrating reliable cross-modal generation and estimation across various indoor and outdoor scenes with multiple subjects.",
                "position": 1517
            }
        ]
    },
    {
        "header": "Appendix DMore Visuals and Applications",
        "images": []
    }
]