[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11149/x1.png",
                "caption": "Figure 1:Illustration of our approach to supervised fine-tuning in a modern LLM training pipeline. Instead of maximizing dataset size and training for few epochs, we train for many epochs on a small random subset of SFT data, substantially reducing compute while improving downstream reasoning performance.",
                "position": 107
            },
            {
                "img": "https://arxiv.org/html/2602.11149/x2.png",
                "caption": "Figure 2:Scaling epochs versus scaling data for Olmo3-7B trained on long-CoT SFT data, averaged across AIME’24, AIME’25, and GPQA benchmarks. Each diagonal represents a fixed update budget, where epochs × samples is constant. Within any diagonal, moving toward fewer samples and more epochs consistently improves accuracy and pass@n, with gains diminishing around 32–64 epochs. Termination rate correlates strongly with accuracy and may be a primary driver of performance gains, as models that fail to terminate cannot produce a final answer.",
                "position": 110
            },
            {
                "img": "https://arxiv.org/html/2602.11149/x3.png",
                "caption": "Figure 3:The repetition advantage is consistent across models, benchmarks, and evaluation metrics. Heatmaps show normalized scores for Olmo3-7B (top) and Qwen3-8B (bottom) on AIME’24, AIME’25, and GPQA, evaluated with both Accuracy@nnand Pass@nn. Each diagonal corresponds to a fixed update budget (epochs×\\timessamples), and in all settings, performance improves when moving along a diagonal toward fewer samples and more epochs.",
                "position": 116
            }
        ]
    },
    {
        "header": "2Scaling Epochs on a Fixed Update Budget",
        "images": []
    },
    {
        "header": "3Impact of Training Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11149/x4.png",
                "caption": "Figure 4:Relationship between training set memorization and downstream performance for Olmo3-7B. Points are colored by epoch count; within each epoch group, variation reflects different dataset sizes. Token accuracy on train set increases primarily with epochs rather than total updates. Across all benchmarks, performance gains plateau once models approach full memorization, suggesting that token accuracy can serve as a stopping criterion for epoch scaling.\nThe initial token accuracy of the base model is marked with the vertical line.",
                "position": 811
            }
        ]
    },
    {
        "header": "4Probing the Repetition Advantage",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11149/x5.png",
                "caption": "Figure 5:Training dynamics for Olmo3-7B showing the relationship between loss, entropy, and downstream performance averaged over AIME’24, AIME’25, and GPQA. Points are colored by epoch count; within each group, variation reflects dataset size. As epochs increase, train loss approaches zero while validation loss rises, the classical signature of overfitting in terms of the train-validation gap. Prediction entropy also decreases, showing increased model confidence in predictions that diverge from the validation distribution. Despite these indicators, downstream accuracy improves with epoch count. Vertical lines mark base model metrics.",
                "position": 1056
            },
            {
                "img": "https://arxiv.org/html/2602.11149/x6.png",
                "caption": "Figure 6:Catastrophic forgetting under epoch scaling versus data scaling for Olmo3-7B. Multi-epoch training on 200 samples is compared against single-epoch training on increasingly large datasets, matched by total update steps. Both approaches exhibit forgetting as measured by MMLU accuracy, with epoch scaling causinglessdegradation. Combined with the large improvement in reasoning accuracy, measured on AIME’24/’25 and GPQA benchmarks, epoch scaling offers a strictly better tradeoff.",
                "position": 1078
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AHyperparameters",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11149/x7.png",
                "caption": "Figure 7:Dolci dataset results for Olmo3-7B. Scaling epochs on smaller datasets yields higher downstream accuracy than scaling the number of unique samples under a fixed update budget.",
                "position": 1536
            },
            {
                "img": "https://arxiv.org/html/2602.11149/x8.png",
                "caption": "Figure 8:Results for distillation from a Qwen3-8B teacher. Stronger teachers increase overall performance but do not eliminate the repetition advantage.",
                "position": 1540
            },
            {
                "img": "https://arxiv.org/html/2602.11149/x9.png",
                "caption": "Figure 9:Dolci dataset results for Qwen3-8B. The repetition advantage persists across dataset sizes, with gains plateauing at higher epoch counts.",
                "position": 1544
            },
            {
                "img": "https://arxiv.org/html/2602.11149/x10.png",
                "caption": "Figure 10:Results for distillation from a Qwen3-0.6B teacher. Despite weaker teacher signals, repetition continues to improve downstream accuracy.",
                "position": 1555
            },
            {
                "img": "https://arxiv.org/html/2602.11149/x11.png",
                "caption": "Figure 11:Results for distillation from a Qwen3-8B teacher. Stronger teachers increase overall performance but do not eliminate the repetition advantage.",
                "position": 1559
            },
            {
                "img": "https://arxiv.org/html/2602.11149/x12.png",
                "caption": "Figure 12:Results using only correct (positive) distilled samples from a Qwen3-8B teacher. Repetition yields consistent gains until memorization saturates.",
                "position": 1570
            },
            {
                "img": "https://arxiv.org/html/2602.11149/x13.png",
                "caption": "Figure 13:Results using incorrect (negative) distilled samples from a Qwen3-8B teacher. Overall performance is lower, and the repetition advantage is substantially diminished.",
                "position": 1574
            }
        ]
    },
    {
        "header": "Appendix BFull Results.",
        "images": []
    }
]