[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.13503/x1.png",
                "caption": "Figure 1:Overview of VBench++.We propose VBench++, a comprehensive and versatile benchmark suite for video generative models.\nWe design a comprehensive and hierarchicalEvaluation Dimension Suiteto decompose ‚Äúvideo generation quality\" into multiple well-defined dimensions to facilitate fine-grained and objective evaluation.\nFor each dimension and each content category, we carefully design aPrompt Suiteas test cases, and sampleGenerated Videosfrom a set of video generation models.\nFor each evaluation dimension, we specifically design anEvaluation Method Suite, which uses a carefully crafted method or designated pipeline for automatic objective evaluation. We also conductHuman Preference Annotationfor the generated videos for each dimension and show that VBench++ evaluation results arewell aligned with human perceptions.\nVBench++ can provide valuable insights from multiple perspectives.VBench++ supports a wide range of video generation tasks, including text-to-video and image-to-video, with an adaptive Image Suite for fair evaluation across different settings. It evaluates not only technical quality but also the trustworthiness of generative models, offering a comprehensive view of model performance. We continually incorporate more video generative models into VBench++ to inform the community about the evolving landscape of video generation.",
                "position": 110
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.13503/x2.png",
                "caption": "(a)Text-to-Video Generative Models.We visualize the evaluation results of four text-to-video generation models in 16 VBench dimensions.\nFor comprehensive numerical results, please refer to TableII.",
                "position": 124
            },
            {
                "img": "https://arxiv.org/html/2411.13503/x2.png",
                "caption": "(a)Text-to-Video Generative Models.We visualize the evaluation results of four text-to-video generation models in 16 VBench dimensions.\nFor comprehensive numerical results, please refer to TableII.",
                "position": 127
            },
            {
                "img": "https://arxiv.org/html/2411.13503/x3.png",
                "caption": "(b)Image-to-Video Generative Models.We visualize the evaluation results of six image-to-video generation models.\nSee TableIIIfor comprehensive numerical results.",
                "position": 136
            },
            {
                "img": "https://arxiv.org/html/2411.13503/x4.png",
                "caption": "(c)Trustworthiness of Video Generative Models.We visualize the trustworthiness of video generative models, along with other dimensions. For comprehensive numerical results, please refer to TableIV.",
                "position": 142
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3VBench++ Suite",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.13503/x5.png",
                "caption": "Figure 3:Prompt Suite Statistics.The two graphs provide an overview of our prompt suites.Left:the word cloud to visualize word distribution of our prompt suites.Right:the number of prompts across different evaluation dimensions and different content categories.",
                "position": 300
            },
            {
                "img": "https://arxiv.org/html/2411.13503/x5.png",
                "caption": "Figure 3:Prompt Suite Statistics.The two graphs provide an overview of our prompt suites.Left:the word cloud to visualize word distribution of our prompt suites.Right:the number of prompts across different evaluation dimensions and different content categories.",
                "position": 303
            },
            {
                "img": "https://arxiv.org/html/2411.13503/x6.png",
                "caption": "Figure 4:Interface for Human Preference Annotation.Top:prompt and question.Right:choices that annotators can make.Bottom left:control for stop and playback.",
                "position": 308
            },
            {
                "img": "https://arxiv.org/html/2411.13503/extracted/6013496/figures/i2v/fig_image_crop_pipeline_horizontal.jpg",
                "caption": "(a)Cropping Pipeline for Portrait Images.",
                "position": 566
            },
            {
                "img": "https://arxiv.org/html/2411.13503/extracted/6013496/figures/i2v/fig_image_crop_pipeline_horizontal.jpg",
                "caption": "(a)Cropping Pipeline for Portrait Images.",
                "position": 569
            },
            {
                "img": "https://arxiv.org/html/2411.13503/extracted/6013496/figures/i2v/fig_image_crop_pipeline_vertical.jpg",
                "caption": "(b)Cropping Pipeline for Landscape Images.",
                "position": 574
            },
            {
                "img": "https://arxiv.org/html/2411.13503/extracted/6013496/figures/i2v/pie_chart_image_suite.png",
                "caption": "Figure 6:Content Distribution of Image Suite.Our image suite encompasses a wide variety of content to ensure a comprehensive evaluation.",
                "position": 1234
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.13503/x7.png",
                "caption": "(a)Trustworthiness of Video Generative Models.",
                "position": 1765
            },
            {
                "img": "https://arxiv.org/html/2411.13503/x7.png",
                "caption": "(a)Trustworthiness of Video Generative Models.",
                "position": 1768
            },
            {
                "img": "https://arxiv.org/html/2411.13503/x8.png",
                "caption": "(b)Trustworthiness of Video vs. Image Models.",
                "position": 1773
            },
            {
                "img": "https://arxiv.org/html/2411.13503/x9.png",
                "caption": "Figure 8:Human Alignment of Text-to-Video (T2V) Evaluations.Our experiments show thatVBench evaluations across all dimensions closely match human perceptions.Each plot shows the alignment verification result of a specific VBench dimension. In each plot, a dot represents the human preference win ratio (horizontal axis) and VBench evaluation win ratio (vertical axis) for a particular video generation model. We linearly fit a straight line to visualize the correlation, and calculate the Spearman‚Äôs correlation coefficient (œÅùúå\\rhoitalic_œÅ) for each dimension.",
                "position": 1797
            },
            {
                "img": "https://arxiv.org/html/2411.13503/x10.png",
                "caption": "Figure 9:Human Alignment of Image-to-Video (I2V) Evaluations.Experiments show that our Image-to-Video (I2V) evaluations across all dimensions closely match human perceptions.\nEach plot shows the alignment verification result of a specific evaluation dimension, similar to Figure8.",
                "position": 1825
            },
            {
                "img": "https://arxiv.org/html/2411.13503/x11.png",
                "caption": "Figure 10:Human Alignment of Trustworthiness Evaluations.Experiments show that our trustworthiness evaluations across all dimensions align well with human perceptions.\nEach plot shows the alignment verification result of a specific evaluation dimension, similar to Figure8.",
                "position": 1839
            },
            {
                "img": "https://arxiv.org/html/2411.13503/x12.png",
                "caption": "Figure 11:More Comparisons of Video Generation Models with Other Models and Baselines.We use VBench to evaluate other models and baselines for further comparative analysis of T2V models.(a)Comparison with text-to-image (T2I) generation models.(b)Comparison withWebVid-AvgandEmpirical Maxbaselines.",
                "position": 1856
            },
            {
                "img": "https://arxiv.org/html/2411.13503/x13.png",
                "caption": "Figure 12:VBench++ Results across Eight Content Categories(best viewed in color). For each chart, we plot the VBench++ evaluation results across eight different content categories, benchmarked by ourPrompt Suite per Category. The results are linearly normalized between 0 and 1 for better visibility across categories.",
                "position": 1861
            }
        ]
    },
    {
        "header": "5Insights and Discussions",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]