[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.24159/x1.png",
                "caption": "Figure 1:Two modes of a single modelCausal and masked language modeling can be easily unified by shifting both outputs by one token to the right. Then we can train one language model on both paradigms at the same time just by modifying the input tokens, output tokens and attention masks.",
                "position": 166
            }
        ]
    },
    {
        "header": "2Method",
        "images": []
    },
    {
        "header": "3Pretraining and evaluation",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.24159/x2.png",
                "caption": "Figure 2:The effect of the causal-to-mask ratioComparison of performance of different tasks when varying the ratio of MNTP used during pre-training. We also look at the performance of the model using prefix language modeling with a partially-bidirectional attention mask. MNLI scores are reported with standard deviation error bars estimated by averaging the variations across three finetuning random seeds.",
                "position": 450
            },
            {
                "img": "https://arxiv.org/html/2410.24159/x3.png",
                "caption": "Figure 3:SST-2 in-context learning20-shots ICL results on the SST-2 validation set for models trained on the 100M BabyLM datasets with varying degrees of each objective. The demonstrations (shots) were chosen at random from the training dataset. We do 20-runs and report mean as well as standard deviation. Note that the accuracy of the majority baseline on this dataset is 51.8%.",
                "position": 618
            }
        ]
    },
    {
        "header": "5Related work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APre-training details",
        "images": []
    },
    {
        "header": "Appendix BEvaluation details",
        "images": []
    },
    {
        "header": "Appendix CText Generation with prompts fromRadford etÂ al. (2019)",
        "images": []
    },
    {
        "header": "Appendix DVarying MNTP ratio results on other datasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.24159/x4.png",
                "caption": "Figure 4:BLiMP-Supplement AccuracyComparison of BLiMP-Supplement accuracy when varying the ratio of MNTP used during pre-training. We set the temperature to apply on the logits to 1 for fair comparison between the evaluation strategies. Fused is the sum of the logits from the causal and masked evaluation.",
                "position": 1843
            },
            {
                "img": "https://arxiv.org/html/2410.24159/x5.png",
                "caption": "Figure 5:EWoK AccuracyComparison of EWoK accuracy when varying the ratio of MNTP used during pre-training. We set the temperature to apply on the logits to 1 for fair comparison between the evaluation strategies. Fused is the sum of the logits from the causal and masked evaluation. We also look at the performance of the model using a prefix masking strategy where the whole context is visible to the model.",
                "position": 1846
            }
        ]
    },
    {
        "header": "Appendix EBLiMP",
        "images": []
    },
    {
        "header": "Appendix FBLiMP Supplemental",
        "images": []
    },
    {
        "header": "Appendix GGLUE",
        "images": []
    },
    {
        "header": "Appendix HEWoK",
        "images": []
    },
    {
        "header": "Appendix ILAMBADA",
        "images": []
    }
]