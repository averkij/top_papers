[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23478/3dr1_logo.png",
                "caption": "",
                "position": 104
            }
        ]
    },
    {
        "header": "Introduction",
        "images": []
    },
    {
        "header": "Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23478/x1.png",
                "caption": "Figure 1:(a) Architecture.It takes text, multi-view images, 3D point clouds, and depth maps as input and formulates comprehensive 3D tasks as autoregressive sequence prediction.(b) Distribution of question types.Scene-30K contains diverse categories.(c) Multi-task performance.3D-R1 demonstrates strong performance across various tasks.(d) Generalizability.3D-R1 exhibits remarkable generalizability with enhanced reasoning capabilities.",
                "position": 280
            },
            {
                "img": "https://arxiv.org/html/2507.23478/x2.png",
                "caption": "Figure 2:CoT data engine.The point cloud of a scene is first sent to scene dscription generator to get a description of the scene. Then based on the description, we apply Gemini 2.5 Pro to synthetic CoT data.",
                "position": 312
            },
            {
                "img": "https://arxiv.org/html/2507.23478/x3.png",
                "caption": "Figure 3:The pipeline of Reinforcement Learning based GRPO.The policy model generatesNNitalic_Noutputs from a point cloud and question. Then perception IoU, semantic CLIP-similarity, and format-adherence rewards are computed, grouped, and combined with a KL term to a frozen reference model to update the policy.",
                "position": 327
            }
        ]
    },
    {
        "header": "The Proposed Method",
        "images": []
    },
    {
        "header": "Experiment",
        "images": []
    },
    {
        "header": "Limitation and Future Work",
        "images": []
    },
    {
        "header": "Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAblation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23478/x4.png",
                "caption": "Figure 4:Performance surfaces under different dynamic view selection weight configurations.We analyze the influence of text relevance (wtw_{t}italic_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT), spatial coverage (wcw_{c}italic_w start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT), and CLIP-based similarity (wclipw_{\\text{clip}}italic_w start_POSTSUBSCRIPT clip end_POSTSUBSCRIPT) on model performance, with the constraintwc+wclip=1w_{c}+w_{\\text{clip}}=1italic_w start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT + italic_w start_POSTSUBSCRIPT clip end_POSTSUBSCRIPT = 1. Results on 3D-QA (ScanQA) and 3D-VG (ScanRefer) reveal that optimal performance emerges whenwtw_{t}italic_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTis within the range of 0.3 to 0.4, combined with balanced visual weights.",
                "position": 3094
            }
        ]
    },
    {
        "header": "Appendix BImplementations Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23478/x5.png",
                "caption": "Figure 5:Scene-30K CoT data example 1.",
                "position": 3377
            },
            {
                "img": "https://arxiv.org/html/2507.23478/x6.png",
                "caption": "Figure 6:Scene-30K CoT data example 2.",
                "position": 3428
            },
            {
                "img": "https://arxiv.org/html/2507.23478/x7.png",
                "caption": "Figure 7:Scene-30K CoT data example 3.",
                "position": 3477
            },
            {
                "img": "https://arxiv.org/html/2507.23478/x8.png",
                "caption": "Figure 8:Scene-30K CoT data example 4.",
                "position": 3523
            },
            {
                "img": "https://arxiv.org/html/2507.23478/x9.png",
                "caption": "Figure 9:Scene-30K CoT data example 5.",
                "position": 3570
            },
            {
                "img": "https://arxiv.org/html/2507.23478/x10.png",
                "caption": "Figure 10:Qualitative results for 3D scene dense captioning (3D-DC).",
                "position": 3622
            },
            {
                "img": "https://arxiv.org/html/2507.23478/x11.png",
                "caption": "Figure 11:Qualitative results for 3D object captioning.",
                "position": 3625
            },
            {
                "img": "https://arxiv.org/html/2507.23478/x12.png",
                "caption": "Figure 12:Qualitative results for 3D visual grounding (3D-VG).",
                "position": 3628
            },
            {
                "img": "https://arxiv.org/html/2507.23478/x13.png",
                "caption": "Figure 13:Qualitative results for 3D question answering (3D-QA).",
                "position": 3631
            },
            {
                "img": "https://arxiv.org/html/2507.23478/x14.png",
                "caption": "Figure 14:Qualitative results for 3D dialogue.",
                "position": 3634
            },
            {
                "img": "https://arxiv.org/html/2507.23478/x15.png",
                "caption": "Figure 15:Qualitative results for 3D reasoning.",
                "position": 3637
            },
            {
                "img": "https://arxiv.org/html/2507.23478/x16.png",
                "caption": "Figure 16:Qualitative results for 3D planning.",
                "position": 3640
            }
        ]
    },
    {
        "header": "Appendix CVisualization",
        "images": []
    }
]