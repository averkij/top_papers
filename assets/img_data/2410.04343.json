[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.04343/extracted/5904574/figures/perf_vs_length_musique.png",
                "caption": "Figure 1:Normalized performance vs. effective context lengths on MuSiQue. Each line represents a fixed configuration, scaled by adjusting the number of documents. Red dots and dash lines represent the optimal configurations and their fitting results. Standard RAG plateaus early at104superscript10410^{4}10 start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPTtokens, in contrast, DRAG and IterDRAG show near-linear improvement as the effective context length grows.",
                "position": 161
            },
            {
                "img": "https://arxiv.org/html/2410.04343/extracted/5904574/figures/intro_barchart.png",
                "caption": "Figure 2:Evaluation accuracy of Gemini 1.5 Flash using different methods: zero-shot QA, many-shot QA, RAG (with an optimal number of documents), DRAG and IterDRAG on benchmark QA datasets. By scaling up inference compute (up to 5M tokens), DRAG consistently outperforms baselines, while IterDRAG improves upon DRAG through interleaving retrieval and iterative generation.",
                "position": 202
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Inference Scaling Strategies for RAG",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.04343/x1.png",
                "caption": "Figure 3:DRAG vs. IterDRAG. IterDRAG breaks down the input query into sub-queries and answer them to improve the accuracy of the final answer. In test-time, IterDRAG scales the computation through multiple inference steps to decompose complex queries and retrieve documents.",
                "position": 247
            }
        ]
    },
    {
        "header": "4RAG Performance and Inference Computation Scale",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.04343/extracted/5904574/figures/perf_vs_length.png",
                "caption": "Figure 4:Normalized performance vs. effective context lengths across datasets. Each line represents a fixed configuration, scaled by varying the number of documents. Red dots indicate the optimal configurations, with the dashed line showing the fitting results. The observed optimal performance can be approximated by a linear relationship with the effective context lengths.",
                "position": 539
            },
            {
                "img": "https://arxiv.org/html/2410.04343/x2.png",
                "caption": "(a)Averaged DRAG performance heatmap for different metrics.",
                "position": 562
            },
            {
                "img": "https://arxiv.org/html/2410.04343/x2.png",
                "caption": "(a)Averaged DRAG performance heatmap for different metrics.",
                "position": 565
            },
            {
                "img": "https://arxiv.org/html/2410.04343/x3.png",
                "caption": "(b)Performance vs. number of documents.",
                "position": 571
            },
            {
                "img": "https://arxiv.org/html/2410.04343/x4.png",
                "caption": "(c)Performance vs. number of shots.",
                "position": 576
            }
        ]
    },
    {
        "header": "5Inference Computation Allocation for Long-Context RAG",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.04343/x5.png",
                "caption": "Figure 6:The estimated performance using the proposed observational scaling laws vs. actual metric values in DRAG. The subplots represent different datasets, where each line corresponds to a fixed number of documents, we scale the context length by increasing the number of shots.",
                "position": 613
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARetrieval Quality",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.04343/x6.png",
                "caption": "Figure 7:Retrieval performance of DRAG on different datasets.",
                "position": 1798
            }
        ]
    },
    {
        "header": "Appendix BChain-of-Thought vs. IterDRAG.",
        "images": []
    },
    {
        "header": "Appendix CAdditional RAG Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.04343/x7.png",
                "caption": "Figure 8:IterDRAG performance heatmap for different metrics averaged across datasets.",
                "position": 1930
            },
            {
                "img": "https://arxiv.org/html/2410.04343/x8.png",
                "caption": "Figure 9:Evaluation accuracy of DRAG on TriviaQA and Natural Questions (NaturalQ.).",
                "position": 1936
            }
        ]
    },
    {
        "header": "Appendix DAdditional Results on Inference Scaling Laws for RAG",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.04343/x9.png",
                "caption": "(a)Normalized performance vs. effective context lengths on Bamboogle.",
                "position": 1975
            },
            {
                "img": "https://arxiv.org/html/2410.04343/x9.png",
                "caption": "(a)Normalized performance vs. effective context lengths on Bamboogle.",
                "position": 1978
            },
            {
                "img": "https://arxiv.org/html/2410.04343/x10.png",
                "caption": "(b)Normalized performance vs. effective context lengths on HotpotQA.",
                "position": 1984
            },
            {
                "img": "https://arxiv.org/html/2410.04343/x11.png",
                "caption": "(c)Normalized performance vs. effective context lengths on 2WikiMultiHopQA.",
                "position": 1990
            }
        ]
    },
    {
        "header": "Appendix EAdditional Results on Computation Allocation Model for RAG",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.04343/x12.png",
                "caption": "Figure 11:The estimated performance using the proposed computation allocation model vs. actual metric values in IterDRAG. The subplots represent different datasets, where each line corresponds to a fixed number of documents, we scale the context length by increasing the number of shots.",
                "position": 2004
            },
            {
                "img": "https://arxiv.org/html/2410.04343/x13.png",
                "caption": "(a)Performance vs. predicted surface for DRAG.",
                "position": 2056
            },
            {
                "img": "https://arxiv.org/html/2410.04343/x13.png",
                "caption": "(a)Performance vs. predicted surface for DRAG.",
                "position": 2059
            },
            {
                "img": "https://arxiv.org/html/2410.04343/x14.png",
                "caption": "(b)Performance vs. predicted surface for IterDRAG.",
                "position": 2064
            }
        ]
    },
    {
        "header": "Appendix FError Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.04343/x15.png",
                "caption": "Figure 14:Input prompt that comprises ofmùëömitalic_min-context examples, the test documents and query, in which each document chunk consists ofkùëòkitalic_kretrieved documents. For IterDRAG, the example answers additionally provide sub-queries and intermediate answers as demonstrations.",
                "position": 2188
            }
        ]
    },
    {
        "header": "Appendix GImplementation",
        "images": []
    }
]