[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14168/x1.png",
                "caption": "",
                "position": 75
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14168/x2.png",
                "caption": "Figure 2:Overall pipeline of FashionComposer.FashionComposer takes garments composition and optional face, text prompt, and a densepose map projected from SMPL as inputs. The text prompt is encoded and fused with UNets through cross-attention and subject-binding attention, while the garment features are extracted and injected for denoising through Feature Injection Attention.",
                "position": 159
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14168/extracted/6080147/fig/comp_quali_v3.jpg",
                "caption": "Figure 3:Qualitative comparison with multi-reference customization methods, including Emu2[27], Collage Diffusion[25], Paint by Example[34]and AnyDoor[6].",
                "position": 319
            },
            {
                "img": "https://arxiv.org/html/2412.14168/extracted/6080147/fig/comp_v2.jpg",
                "caption": "Figure 4:Qualitative comparison with garment-centric fashion image synthesis methods, including\nStableGarment[30],\nIMAGDressing-v1[26],\nand Magic Clothing[4],\nwhere ours better preserves the identity of the target objects.\nNote that all approaches do not finetune the model on the test samples.",
                "position": 323
            },
            {
                "img": "https://arxiv.org/html/2412.14168/extracted/6080147/fig/tryon.jpg",
                "caption": "Figure 5:Diverse virtual try-on resultsof FashionComposer for upper, lower, and outfit try-on tasks.",
                "position": 472
            },
            {
                "img": "https://arxiv.org/html/2412.14168/extracted/6080147/fig/ablation_ref_v2.jpg",
                "caption": "Figure 6:Qualitative comparison for the reference encoder.Reference UNet better preserves the fine details of the garments.",
                "position": 476
            },
            {
                "img": "https://arxiv.org/html/2412.14168/x3.png",
                "caption": "Figure 7:Qualitative ablation study on subject-binding attention.Bind(1) means only modifying the self-attention modules of UNet blocks with the smallest resolution. Conv-in refers to injecting the mask map through the Convolution-in layer of the reference UNet. We highlight mistakes in rows 2-3 using red boxes.",
                "position": 759
            },
            {
                "img": "https://arxiv.org/html/2412.14168/x4.png",
                "caption": "Figure 8:Qualitative ablation studyon synthesizing independently (baseline), cross-frame attention (CFA),\nand latent code alignment (LCA). Reference garments and text are omitted here.",
                "position": 777
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]