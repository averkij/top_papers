[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21876/x1.png",
                "caption": "Figure 1:Comparison of anchor video creation methods for training camera control models.(a) Previous methods(ren2025gen3c;yu2024viewcrafter) estimate the 3D point cloud (through depth estimation) using the first frame and render anchor videos with annotated camera trajectories, but suffer from region misalignment due to point-cloud estimation errors while limited to camera-pose annotated data, resulting in inefficient training.(b) Our methodcreatesanchor videosvia visibility masking based on first-frame pixel tracking. This not only guarantees accurate geometric alignment but also supports diverse data while largely reducing training costs.\nWe highlight the video regions inredandgreenboxes to compare the alignment quality.",
                "position": 141
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Background: Video Diffusion Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21876/x2.png",
                "caption": "Figure 2:EPiC Model Architecture. (a) shows an overview of our EPiC framework.\nEPiC supports multiple inference scenarios. (b) and (c) illustrate our I2V inference scenarios using full and masked point clouds, respectively. (d) depicts V2V inference scenario employing dynamic point clouds.",
                "position": 225
            }
        ]
    },
    {
        "header": "4EPiC: An Efficient Framework for Learning Precise Camera Control",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21876/x3.png",
                "caption": "Figure 3:Anchor video construction.",
                "position": 247
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21876/x4.png",
                "caption": "Figure 4:Generated videos comparing with other camera control methods for I2V and V2V tasks.",
                "position": 569
            },
            {
                "img": "https://arxiv.org/html/2505.21876/x5.png",
                "caption": "Figure 5:Qualitative examples for ablation study.",
                "position": 645
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BAdditional V2V Camera Control Quantitative Evaluation",
        "images": []
    },
    {
        "header": "Appendix CAblation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21876/x6.png",
                "caption": "Figure 6:Qualitative V2V camera control results of models trained from different data sources.",
                "position": 1947
            },
            {
                "img": "https://arxiv.org/html/2505.21876/x7.png",
                "caption": "Figure 7:Results of training with Anchor-ControlNet compared to full-finetuning.",
                "position": 2091
            }
        ]
    },
    {
        "header": "Appendix DRobustness to Different Random Seeds",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21876/x8.png",
                "caption": "Figure 8:Robustness to different random seeds",
                "position": 2103
            },
            {
                "img": "https://arxiv.org/html/2505.21876/x9.png",
                "caption": "Figure 9:Examples of text-guided scene control.",
                "position": 2107
            }
        ]
    },
    {
        "header": "Appendix EAdditional Applications: Fine-Grained Control",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21876/x10.png",
                "caption": "Figure 10:Examples of object 3D trajectory control via anchor video manipulation.",
                "position": 2115
            },
            {
                "img": "https://arxiv.org/html/2505.21876/x11.png",
                "caption": "Figure 11:Examples of Regional Animation",
                "position": 2119
            },
            {
                "img": "https://arxiv.org/html/2505.21876/x12.png",
                "caption": "Figure 12:Examples of constructed anchor videos. The source video and corresponding captions are obtained from Panda70M.",
                "position": 2123
            },
            {
                "img": "https://arxiv.org/html/2505.21876/x13.png",
                "caption": "Figure 13:Qualitative examples of I2V camera control with diverse image inputs and camera trajectories.",
                "position": 2127
            },
            {
                "img": "https://arxiv.org/html/2505.21876/x14.png",
                "caption": "Figure 14:Qualitative examples of V2V camera control on movie clips with multiple kinds of camera trajectories.",
                "position": 2131
            }
        ]
    },
    {
        "header": "Appendix FAdditional Visual Examples",
        "images": []
    },
    {
        "header": "Appendix GLimitations and Broader Impacts",
        "images": []
    }
]