[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10522/x1.png",
                "caption": "",
                "position": 96
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10522/x2.png",
                "caption": "Figure 2:Overview of the automated caption generation pipeline. For each video-audio clip (top), Qwen2-Audio uses dataset-provided keywords to produce an audio caption. For each video-music pair (bottom), it describes key attributes (e.g. genre, instruments, mood, tempo) to form a music caption.",
                "position": 180
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10522/x3.png",
                "caption": "Figure 3:The AudioX Framework. This figure depicts the AudioX framework, which employs specialized encoders and a DiT-based approach with input masking to generate high-quality audio, unifying diverse input modalities for comprehensive audio and music creation.",
                "position": 208
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10522/x4.png",
                "caption": "Figure 4:User study results of generated audio and music. The values represent the average OVL and REL scores across Text-to-Audio (on AudioCaps), Text-to-Music (on MusicCaps), Video-to-Audio (on VGGSound), Video-to-Music (on V2M-bench).",
                "position": 1120
            },
            {
                "img": "https://arxiv.org/html/2503.10522/x5.png",
                "caption": "Figure 5:Ablation study of mask ratios for each modality, with mask ratios varying from 0.2, 0.4, 0.6 to 0.8. The values represent the average Inception Score (IS) across Text-to-Audio, Text-to-Music, Video-to-Audio, Video-to-Music, and Audio Inpainting tasks.",
                "position": 1133
            },
            {
                "img": "https://arxiv.org/html/2503.10522/x6.png",
                "caption": "Figure 6:Ablation study comparing intra-modal and inter-modal performance of the unified model. The left compares single-modality models on text-to-audio, video-to-audio, and audio inpainting tasks. The right shows the effect of adding modalities on music generation, with performance improvements noted for each added modality. Results are based on the Inception Score (IS) metric.",
                "position": 1147
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10522/x7.png",
                "caption": "Figure A1:Qualitative comparison across various tasks: (a) In Text-to-Audio (T2A) and Text-to-Music (T2M) tasks, our model uniquely excels by consistently generating the “ticking” sound of a clock and accurately following the prompt ”Music performed on a steelpan instrument,” outperforming baselines in both rhythmic precision and genre fidelity. (b) Audio inpainting results demonstrate our model’s strong context-aware capabilities and its ability to effectively integrate different input modalities. (c) Video-to-Audio (V2A) results show our model’s proficiency in capturing dynamic motion sounds, such as the immersive “drifting” of a car, providing a richer auditory experience compared to baselines.",
                "position": 2426
            },
            {
                "img": "https://arxiv.org/html/2503.10522/x8.png",
                "caption": "",
                "position": 2431
            },
            {
                "img": "https://arxiv.org/html/2503.10522/x9.png",
                "caption": "",
                "position": 2439
            },
            {
                "img": "https://arxiv.org/html/2503.10522/x10.png",
                "caption": "Figure A2:Comprehensive qualitative analysis of our model’s performance across various tasks: (a) Text-to-Audio and Text-to-Music synthesis, (b) Video-to-Audio and Video-to-Music generation, and (c) Audio Inpainting and Music Completion.",
                "position": 2449
            },
            {
                "img": "https://arxiv.org/html/2503.10522/x11.png",
                "caption": "",
                "position": 2462
            },
            {
                "img": "https://arxiv.org/html/2503.10522/x12.png",
                "caption": "",
                "position": 2470
            }
        ]
    },
    {
        "header": "7Appendix",
        "images": []
    }
]