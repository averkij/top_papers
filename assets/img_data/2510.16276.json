[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16276/x1.png",
                "caption": "Figure 1:Average latency breakdown per iteration of a Reflexion-based agentic systemShinn et al. (2023)for sampled question answering.",
                "position": 150
            }
        ]
    },
    {
        "header": "2Latency Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16276/x2.png",
                "caption": "Figure 2:In this figure, we show that LLM API response times exhibit high variance, including occasional outliers. We evaluate the end-to-end latency of API calls offered by five AI companies by querying the LLMs every hour. The evaluated models include: (i) Together AI: Llama-3.1-70B, Llama-3.1-405B, Qwen2.5-72B, QwQ-32B; (ii) OpenAI: GPT-4o; (iii) Google: Gemini-1.5-Pro; (iv) Anthropic: Claude-3.7-Sonnet. (v) DeepSeek: DeepSeek-Chat.",
                "position": 205
            },
            {
                "img": "https://arxiv.org/html/2510.16276/x3.png",
                "caption": "Figure 3:In this figure, we evaluate the end-to-end latency of API calls across different dates. Due to space constraints, we report results from three representative models: Llama-3.1-70B and Llama-3.1-405B provided by Together AI, and GPT-4o from OpenAI. This figure illustrates latency variance over time for various models, with fixed input prompts and a uniform output length.",
                "position": 208
            },
            {
                "img": "https://arxiv.org/html/2510.16276/x4.png",
                "caption": "Figure 4:In this figure, we demonstrate that employing priority processing can significantly reduce the latency and variance observed in LLM API calls. We evaluate the end-to-end latency of API calls under both default and priority tiers across a range of models and dates. The left figure displays the end-to-end latency of various models under both default and priority tiers, while the right figure shows the end-to-end latency of the GPT-4o model across different dates.",
                "position": 226
            },
            {
                "img": "https://arxiv.org/html/2510.16276/x4.png",
                "caption": "",
                "position": 229
            },
            {
                "img": "https://arxiv.org/html/2510.16276/x5.png",
                "caption": "",
                "position": 234
            },
            {
                "img": "https://arxiv.org/html/2510.16276/x6.png",
                "caption": "Figure 5:The above two CDF figures illustrate the performance characteristics of the web environment from theWebWalkerQAbenchmarkWu et al. (2025b). The left figure shows the distribution of latency when fetching root URLs and subpages, highlighting the initial overhead. The right figure presents the distribution of the number of clickable subpages available from a root URL, showing a large action space.",
                "position": 253
            },
            {
                "img": "https://arxiv.org/html/2510.16276/x6.png",
                "caption": "",
                "position": 256
            },
            {
                "img": "https://arxiv.org/html/2510.16276/x7.png",
                "caption": "",
                "position": 261
            }
        ]
    },
    {
        "header": "3SpecCache",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16276/x8.png",
                "caption": "Figure 6:This figure shows the workflow of ourSpecCacheframework. In each iteration, the model input is fed to two independent and non-blocking threads, one Reflexion-based thread and one caching thread aimed at generating candidate actions. The caching thread updates the cache pool with its candidate actions. When the Reflexion-based thread selects an action, it first queries the cache pool. If the cache misses, it executes the action, retrieves the corresponding observation, and proceeds to the next iteration, updating the cache pool with the new action-observation pair.",
                "position": 288
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16276/x9.png",
                "caption": "Figure 7:This figure shows the iteration-wise latency breakdown of the Reflexion-based agentic system using o4-mini (top) and GPT-5-mini (bottom) as backbone models, evaluated on sampled questions fromWebWalkerQAWu et al. (2025b). We perform five runs for each sampled question. The sampled questions are listed in AppendixC.",
                "position": 389
            },
            {
                "img": "https://arxiv.org/html/2510.16276/x9.png",
                "caption": "",
                "position": 392
            },
            {
                "img": "https://arxiv.org/html/2510.16276/x10.png",
                "caption": "",
                "position": 397
            },
            {
                "img": "https://arxiv.org/html/2510.16276/x11.png",
                "caption": "Figure 8:This figure shows the iteration-wise latency for the agentic systems accelerated bySpecCachewhen answering sampled questions fromWebWalkerQA(top)Wu et al. (2025b)andFrames(bottom)Krishna et al. (2024). We use o4-mini as the target model and GPT-4.1-mini as the draft model. We perform five runs for each sampled question. The sampled questions are listed in AppendixC.",
                "position": 403
            },
            {
                "img": "https://arxiv.org/html/2510.16276/x11.png",
                "caption": "",
                "position": 406
            },
            {
                "img": "https://arxiv.org/html/2510.16276/x12.png",
                "caption": "",
                "position": 411
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Limitations and Future Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALatency Test Question.",
        "images": []
    },
    {
        "header": "Appendix BExamples of Math Questions",
        "images": []
    },
    {
        "header": "Appendix CSampled Questions",
        "images": []
    },
    {
        "header": "Appendix DPrompts and Trajectory",
        "images": []
    },
    {
        "header": "Appendix EAdditional LLM API Latency Measurement Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16276/x13.png",
                "caption": "Figure 9:This figure shows the latency variance across regions for models including Llama-3.1-70B, Llama-3.1-405B, GPT-4o, Gemini-1.5-Pro, and Claude-3.7-Sonnet. All requests use the same input and a fixed number of output tokens. Latency is measured by sending requests from machines located in Wisconsin (Madison), South Carolina (Clemson), and Utah.",
                "position": 2443
            },
            {
                "img": "https://arxiv.org/html/2510.16276/x14.png",
                "caption": "Figure 10:From July 23 to July 27, 2025, we evaluated the end-to-end latency of API calls provided by two AI companies by fixing the input sequence and varying the number of output tokens from 64 to 1024. The evaluated models include: (i) Together AI: Llama-3.1-70B, Llama-3.1-405B; (ii) OpenAI: GPT-4o; (iii) Google: Gemini-1.5-Pro. The figure illustrates an upward trend in LLM API latency with increasing output token count.",
                "position": 2453
            },
            {
                "img": "https://arxiv.org/html/2510.16276/x15.png",
                "caption": "(a)QwQ-32B",
                "position": 2463
            },
            {
                "img": "https://arxiv.org/html/2510.16276/x15.png",
                "caption": "(a)QwQ-32B",
                "position": 2466
            },
            {
                "img": "https://arxiv.org/html/2510.16276/x16.png",
                "caption": "(b)Llama-3.1-70B",
                "position": 2472
            },
            {
                "img": "https://arxiv.org/html/2510.16276/x17.png",
                "caption": "Figure 12:In this figure, we evaluate the end-to-end latency of API calls offered by three AI companies by querying the LLMs every hour. The evaluated models include: (i) Together AI: Llama-3.1-8B, Qwen2.5-7B, DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-14B, and Kimi-K2-Instruct; (ii) OpenAI: GPT-4o-mini; (iii) DeepSeek: DeepSeek-Reasoner. This figure shows that LLM API response times exhibit high variance, including occasional outliers.",
                "position": 2486
            },
            {
                "img": "https://arxiv.org/html/2510.16276/x18.png",
                "caption": "(a)This figure shows the number of tokens generated for answering Q1–Q4 in AppendixBacross four models: Qwen2.5-72B, QwQ-32B, GPT-4o, and Gemini-1.5-Pro. It highlights that QwQ-32B produces significantly more output tokens than the others when solving the sampled math problems.",
                "position": 2496
            },
            {
                "img": "https://arxiv.org/html/2510.16276/x18.png",
                "caption": "(a)This figure shows the number of tokens generated for answering Q1–Q4 in AppendixBacross four models: Qwen2.5-72B, QwQ-32B, GPT-4o, and Gemini-1.5-Pro. It highlights that QwQ-32B produces significantly more output tokens than the others when solving the sampled math problems.",
                "position": 2499
            },
            {
                "img": "https://arxiv.org/html/2510.16276/x19.png",
                "caption": "(b)This figure reports the latency for answering Q1–Q4 in AppendixBacross four models: Qwen2.5-72B, QwQ-32B, GPT-4o, and Gemini-1.5-Pro. This indicates that QwQ-32B exhibits the highest end-to-end latency among the evaluated models.",
                "position": 2505
            },
            {
                "img": "https://arxiv.org/html/2510.16276/x20.png",
                "caption": "(a)GPT-4o-mini",
                "position": 2519
            },
            {
                "img": "https://arxiv.org/html/2510.16276/x20.png",
                "caption": "(a)GPT-4o-mini",
                "position": 2522
            },
            {
                "img": "https://arxiv.org/html/2510.16276/x21.png",
                "caption": "(b)GPT-4.1",
                "position": 2528
            }
        ]
    },
    {
        "header": "Appendix FMore Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16276/x22.png",
                "caption": "(a)WebWalkerQA",
                "position": 2543
            },
            {
                "img": "https://arxiv.org/html/2510.16276/x22.png",
                "caption": "(a)WebWalkerQA",
                "position": 2546
            },
            {
                "img": "https://arxiv.org/html/2510.16276/x23.png",
                "caption": "(b)Frames",
                "position": 2552
            }
        ]
    },
    {
        "header": "Appendix GModel Version",
        "images": []
    }
]