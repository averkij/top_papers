[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.12186/extracted/6367151/img/splash_jpg/sample_041/input/000009_blurred.jpg",
                "caption": "Figure 1:CoMotion¬†tracks 3D poses online from monocular RGB video. Rather than detect new poses in each frame and associate them to existing tracks, CoMotion¬†updates tracks directly from incoming image features. As a result, CoMotion¬†keeps track of distinct individuals as they overlap in the camera frame (top) and occlude each other (bottom).\nArrows highlight some points of interest.",
                "position": 93
            },
            {
                "img": "https://arxiv.org/html/2504.12186/extracted/6367151/img/splash_jpg/sample_041/input/000036_blurred.jpg",
                "caption": "",
                "position": 102
            },
            {
                "img": "https://arxiv.org/html/2504.12186/extracted/6367151/img/splash_jpg/sample_041/input/000108_blurred.jpg",
                "caption": "",
                "position": 103
            },
            {
                "img": "https://arxiv.org/html/2504.12186/extracted/6367151/img/splash_jpg/results/sample_041_comotion_frame_0004.jpg",
                "caption": "",
                "position": 110
            },
            {
                "img": "https://arxiv.org/html/2504.12186/extracted/6367151/img/splash_jpg/results/sample_041_comotion_frame_0013.jpg",
                "caption": "",
                "position": 111
            },
            {
                "img": "https://arxiv.org/html/2504.12186/extracted/6367151/img/splash_jpg/results/sample_041_comotion_frame_0037.jpg",
                "caption": "",
                "position": 112
            },
            {
                "img": "https://arxiv.org/html/2504.12186/extracted/6367151/img/splash_jpg/results/sample_041_4dhumans_frame_0004.jpg",
                "caption": "",
                "position": 120
            },
            {
                "img": "https://arxiv.org/html/2504.12186/extracted/6367151/img/splash_jpg/results/sample_041_4dhumans_frame_0013.jpg",
                "caption": "",
                "position": 121
            },
            {
                "img": "https://arxiv.org/html/2504.12186/extracted/6367151/img/splash_jpg/results/sample_041_4dhumans_frame_0037.jpg",
                "caption": "",
                "position": 122
            },
            {
                "img": "https://arxiv.org/html/2504.12186/extracted/6367151/img/splash_jpg/sample_015/input/000003_blurred.jpg",
                "caption": "",
                "position": 130
            },
            {
                "img": "https://arxiv.org/html/2504.12186/extracted/6367151/img/splash_jpg/sample_015/input/000012_blurred.jpg",
                "caption": "",
                "position": 131
            },
            {
                "img": "https://arxiv.org/html/2504.12186/extracted/6367151/img/splash_jpg/sample_015/input/000027_blurred.jpg",
                "caption": "",
                "position": 132
            },
            {
                "img": "https://arxiv.org/html/2504.12186/extracted/6367151/img/splash_jpg/results/sample_015_comotion_frame_0002.jpg",
                "caption": "",
                "position": 139
            },
            {
                "img": "https://arxiv.org/html/2504.12186/extracted/6367151/img/splash_jpg/results/sample_015_comotion_frame_0005.jpg",
                "caption": "",
                "position": 140
            },
            {
                "img": "https://arxiv.org/html/2504.12186/extracted/6367151/img/splash_jpg/results/sample_015_comotion_frame_0010.jpg",
                "caption": "",
                "position": 142
            },
            {
                "img": "https://arxiv.org/html/2504.12186/extracted/6367151/img/splash_jpg/results/sample_015_4dhumans_frame_0002.jpg",
                "caption": "",
                "position": 150
            },
            {
                "img": "https://arxiv.org/html/2504.12186/extracted/6367151/img/splash_jpg/results/sample_015_4dhumans_frame_0005.jpg",
                "caption": "",
                "position": 151
            },
            {
                "img": "https://arxiv.org/html/2504.12186/extracted/6367151/img/splash_jpg/results/sample_015_4dhumans_frame_0010.jpg",
                "caption": "",
                "position": 153
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3CoMotion",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.12186/extracted/6367151/img/Overview.jpg",
                "caption": "Figure 2:Overview.CoMotion¬†estimates 3D poses for all people in a frame. An image encoder produces image featuresFtsuperscriptùêπùë°F^{t}italic_F start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT, which are passed through the detection module to identify potential new tracks. In parallel, the pose update module attends toFtsuperscriptùêπùë°F^{t}italic_F start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPTto update the existing tracks from the previous timestep. Both outputs are compared to each other to decide whether to instantiate or remove any tracks. If a detection is flagged as a new track, it is passed through the update module before being added to the final output tracks for the current frame. The inset details the pose update module.",
                "position": 209
            }
        ]
    },
    {
        "header": "4Training",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.12186/extracted/6367151/img/comparison_4D.png",
                "caption": "Figure 3:We compare predictions made by CoMotion and 4D Humans unrolled through time on a sample from PoseTrack. Due to making independent predictions per frame, we observe that 4D Humans occasionally makes abrupt changes to the estimated pose (see green track on the right).",
                "position": 676
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.12186/extracted/6367151/img/posetrack18_annot.png",
                "caption": "Figure 4:Incorrect handling of missing annotations in PoseTrack18.Due to incomplete annotations in PoseTrack18, predicted tracks may be incorrectly regarded as ‚Äúfalse positives‚Äù. We show representative samples where annotations aregreenand ‚Äúfalse positives‚Äù arered.",
                "position": 1488
            },
            {
                "img": "https://arxiv.org/html/2504.12186/extracted/6367151/img/posetrack21_annot.png",
                "caption": "Figure 5:Incorrect handling of missing annotations in PoseTrack 21.PoseTrack21 addresses the incompleteness of PoseTrack18 annotations by providing ‚Äòignore‚Äô regions to accompany the annotated tracks. For the frame on the left, the center image illustrates the annotation of the person in the center (shown ingreen) and a polygon defining the ‚Äòignore‚Äô region inblue. The right image shows predicted tracks inred, which are still penalized as false positives by the PoseTrack21 evaluation code despite being contained in the ‚Äòignore region‚Äò. This is a bug that we fix.",
                "position": 1492
            },
            {
                "img": "https://arxiv.org/html/2504.12186/x1.png",
                "caption": "Figure 6:Impact of the assumed focal length.To investigate how the focal length of the intrinsics matrix affects performance we run our model on PoseTrack videos (for which we do not have ground truth camera calibration) and report 2D PCK accuracy. We adjust the assumed focal length and observe that the network‚Äôs 2D keypoint accuracy is consistent as long as we remain within a realm of values which correspond to what one would typically find with most camera hardware apart from extremely wide-angle options such as a fish-eye lens. In the above figure, fx (the x-axis) is normalized by the image width.",
                "position": 1751
            },
            {
                "img": "https://arxiv.org/html/2504.12186/x2.png",
                "caption": "Figure 7:Comparing the per-frame runtime of CoMotion¬†with prior work on the PoseTrack21 validation set. All measurements were made on a V100 GPU using the code released by the respective authors. CoMotion¬†is significantly faster than prior work. Specifically, CoMotion¬†is approximately 1.4x faster than PARE (176ms vs 258ms) and 12x faster than 4D Humans (176ms vs 2163ms) on average.",
                "position": 1766
            },
            {
                "img": "https://arxiv.org/html/2504.12186/x3.png",
                "caption": "Figure 9:Architecture of the image encoder and detection step.We visualize the core modules involved in producing encoded image features and per-frame detections. Theimage encoderproduces a multi-resolution feature pyramid using the intermediate activations from ConvNeXtV2, ranging from1/818\\nicefrac{{1}}{{8}}/ start_ARG 1 end_ARG start_ARG 8 end_ARGto1/64164\\nicefrac{{1}}{{64}}/ start_ARG 1 end_ARG start_ARG 64 end_ARGthe input resolution. The features‚Äô channel dimensions are then expanded with 1x1 convolutions such that they can all be reshaped to1818\\frac{1}{8}divide start_ARG 1 end_ARG start_ARG 8 end_ARGresolution and summed. The image encoder also applies positional encodings derived from the uncalibrated pixel coordinates. Next, thedetection headproduces a large set of candidate SMPL parameters and associated confidences, derived from feature maps at three spatial resolutions(18,116,132)18116132(\\frac{1}{8},\\frac{1}{16},\\frac{1}{32})( divide start_ARG 1 end_ARG start_ARG 8 end_ARG , divide start_ARG 1 end_ARG start_ARG 16 end_ARG , divide start_ARG 1 end_ARG start_ARG 32 end_ARG ). We apply non-maximum suppression to return the final set of detections.",
                "position": 1848
            },
            {
                "img": "https://arxiv.org/html/2504.12186/extracted/6367151/img/pseudo_compare_00.png",
                "caption": "Figure 11:Comparison of pseudolabeled annotations.Left: input image. Middle: original pseudolabels provided byGoel et¬†al. (2023). Right: the new pseudolabels produced by running NLF(S√°r√°ndi & Pons-Moll,2024). We observe better head and foot correspondence with the new outputs(top)and more comprehensive annotations across people in the scene(bottom).",
                "position": 2177
            },
            {
                "img": "https://arxiv.org/html/2504.12186/extracted/6367151/img/pseudo_compare_01.png",
                "caption": "",
                "position": 2181
            },
            {
                "img": "https://arxiv.org/html/2504.12186/extracted/6367151/img/pseudo_compare_02.png",
                "caption": "Figure 12:Comparison of model predictions when trained with differing pseudolabels.Left: input image. Middle: detections trained on original pseudolabels provided byGoel et¬†al. (2023). Right: detections trained on new pseudolabels produced by running NLF(S√°r√°ndi & Pons-Moll,2024). The effects are subtle, but the body fitting is improved and when tested in-the-wild we notice better accuracy on extreme poses like those seen in breakdancing and gymnastics clips. The one notable regression is on closeups, as illustrated in the bottom row.",
                "position": 2262
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]