[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Problem Formulation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08872/x1.png",
                "caption": "Figure 1:Game theory could optimize user-LLM interaction. Default Interaction settles into the Prisoner’s Dilemma. Preferably, LLM would guide the conversation to the jointly optimal outcome.",
                "position": 156
            }
        ]
    },
    {
        "header": "3Game-Theoretic Alignment (GTAlign) Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08872/x2.png",
                "caption": "Figure 2:GTAligngenerates responses using game-theoretic reasoning.A mutual welfare reward, calculated from the final response, is used for reinforcement learning.",
                "position": 216
            },
            {
                "img": "https://arxiv.org/html/2510.08872/x3.png",
                "caption": "Figure 3:Directly training LLMs with RL on classic games shows high variance and no convergence. We train Qwen2.5-7B-Instruct on Sequential Prisoner’s Dilemma, Rubinstein Bargaining, and Sequential Signaling games with RL, showing the reward over training steps.",
                "position": 227
            },
            {
                "img": "https://arxiv.org/html/2510.08872/x4.png",
                "caption": "Figure 4:GTAligncan steer LLM behavior during inference. When the LLM pricing policy switches, we can steer LLM behavior by modifying the payoff matrix.",
                "position": 311
            }
        ]
    },
    {
        "header": "4Experiment Setup333Training details are in AppendixB; all prompts are in AppendixD.",
        "images": []
    },
    {
        "header": "5Main Results",
        "images": []
    },
    {
        "header": "6Ablation Studies",
        "images": []
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AThe Use of Large Language Models",
        "images": []
    },
    {
        "header": "Appendix BExperiment Details",
        "images": []
    },
    {
        "header": "Appendix CTheoretical Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08872/x5.png",
                "caption": "Figure 5:Illustration of Pareto efficiency.",
                "position": 3152
            }
        ]
    },
    {
        "header": "Appendix DPrompts",
        "images": []
    },
    {
        "header": "Appendix ELimitations and Future Directions",
        "images": []
    }
]