[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06835/x1.png",
                "caption": "",
                "position": 167
            },
            {
                "img": "https://arxiv.org/html/2512.06835/assets/huggingface_logo.png",
                "caption": "",
                "position": 167
            },
            {
                "img": "https://arxiv.org/html/2512.06835/x2.png",
                "caption": "Figure 1:DoGe decouples the self-evolving VLM cognitive process into a ”learning-application” cycle, designing two components: a learnable Thinker and a frozen Solver. In the first stage, the pass rate of the Solver is used as the quantitative reward for the Analysis (the output of the Thinker). In the second stage, standard GRPO is implemented for annealing, forming a complete iterative closed loop.",
                "position": 171
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06835/x3.png",
                "caption": "Figure 2:DoGe masks the question part and retains only the context. As shown in the example, DoGe preserves only the molecular image input. The Thinker attempts to conduct in-depth thinking without specific question input, embeds its output into the Solver, and uses the pass rate of the Solver in solving the original question as the quantitative reward criterion for the Thinker.",
                "position": 206
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06835/x4.png",
                "caption": "Figure 3:DoGe’s data synthesis framework is analogous to the learning process of humans—learning knowledge from the world and then applying it to solve problems. DoGe first collects a large amount of unlabeled data from the web and databases via tools. The data is aggregated into a Multimodal Knowledge Pool. The LVLM transforms it into learnable vision-question-answer pairs. The training data for DoGe consists of the those designed questions and variant problems synthesized from the iteratively updated Seed Problem Pool.",
                "position": 244
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06835/figures/entropy-large.png",
                "caption": "Table 1:DoGe vs. Baseline. All experiments adopted the same setup, and the specific details of the experimental parameters can be referred to in the Appendix. For Qwen2.5VL-7B, as stated in Section4, we apply GRPO Algorithm with format reward only for 15 steps to better follow instructions.",
                "position": 345
            },
            {
                "img": "https://arxiv.org/html/2512.06835/figures/entropy-large.png",
                "caption": "Figure 4:Average Entropy (DoGe vs. Baseline) during training. ”Anneal” refers to DoGe’s RL stage 2. Compared to the baseline, DoGe exhibits a higher initial policy entropy during training and consistently maintains greater exploration.",
                "position": 690
            },
            {
                "img": "https://arxiv.org/html/2512.06835/figures/entropy-large.png",
                "caption": "Figure 4:Average Entropy (DoGe vs. Baseline) during training. ”Anneal” refers to DoGe’s RL stage 2. Compared to the baseline, DoGe exhibits a higher initial policy entropy during training and consistently maintains greater exploration.",
                "position": 692
            },
            {
                "img": "https://arxiv.org/html/2512.06835/figures/data1.png",
                "caption": "Figure 5:Distribution comparison on the training data in the mathematical field. We select training data’s subset of Vision-R1 and ours method. The visualization results presented by applying text-embedding-3-large embedding to the text part of the problem and then performing t-SNE dimensionality reduction.",
                "position": 696
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6RLVR for VLMs",
        "images": []
    },
    {
        "header": "7Experiment Details",
        "images": []
    },
    {
        "header": "8Data Sample",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06835/figures/chem_raw.png",
                "caption": "",
                "position": 1784
            },
            {
                "img": "https://arxiv.org/html/2512.06835/figures/earth_raw.png",
                "caption": "",
                "position": 1800
            },
            {
                "img": "https://arxiv.org/html/2512.06835/figures/math_raw.png",
                "caption": "",
                "position": 1814
            },
            {
                "img": "https://arxiv.org/html/2512.06835/figures/earth_right.png",
                "caption": "",
                "position": 1832
            },
            {
                "img": "https://arxiv.org/html/2512.06835/figures/chem_right.png",
                "caption": "",
                "position": 1853
            },
            {
                "img": "https://arxiv.org/html/2512.06835/figures/math_right.png",
                "caption": "",
                "position": 1874
            },
            {
                "img": "https://arxiv.org/html/2512.06835/figures/case_study.png",
                "caption": "",
                "position": 1894
            }
        ]
    },
    {
        "header": "9Case Study",
        "images": []
    }
]