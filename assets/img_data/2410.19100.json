[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.19100/extracted/5952369/headerFINAL.png",
                "caption": "Figure 1:Overview of VideoWebArena. VideoWebArena is a visually grounded benchmark that tests the video understanding of agentic models across various realistic domains and environments, mirroring real-life tasks. All tasks require video input and consist of Q/A to test agentic abilities in video information retrieval, video understanding, and more.",
                "position": 156
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3VideoWebArena Environment",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.19100/extracted/5952369/video_difficulty_distribution_large_labels.png",
                "caption": "Figure 2:Left:VideoWebArena Video Difficulty Task Distribution.Right:VideoWebArena Agent Difficulty Task Distribution.",
                "position": 474
            },
            {
                "img": "https://arxiv.org/html/2410.19100/extracted/5952369/agent_difficulty_distribution_large_labels.png",
                "caption": "",
                "position": 477
            },
            {
                "img": "https://arxiv.org/html/2410.19100/extracted/5952369/final_bar.png",
                "caption": "",
                "position": 481
            },
            {
                "img": "https://arxiv.org/html/2410.19100/extracted/5952369/shop.png",
                "caption": "Table 4:Examples of Each Task in the VideoWebArena Taxonomy:Given a video tutorial, the agent is asked to perform the intent. The intermediate intent tests the multimodal agent’s ability to extract the necessary information to perform the task from the video. Skill retention tasks do not have intermediate intents as they do not require recalling specific information that factual retention tasks will require.",
                "position": 594
            },
            {
                "img": "https://arxiv.org/html/2410.19100/extracted/5952369/shop_tut.png",
                "caption": "",
                "position": 634
            },
            {
                "img": "https://arxiv.org/html/2410.19100/extracted/5952369/reddit.png",
                "caption": "",
                "position": 656
            },
            {
                "img": "https://arxiv.org/html/2410.19100/extracted/5952369/reddit_tut.png",
                "caption": "",
                "position": 661
            },
            {
                "img": "https://arxiv.org/html/2410.19100/extracted/5952369/gitlab.png",
                "caption": "",
                "position": 683
            },
            {
                "img": "https://arxiv.org/html/2410.19100/extracted/5952369/git_tut.png",
                "caption": "",
                "position": 688
            },
            {
                "img": "https://arxiv.org/html/2410.19100/extracted/5952369/map.png",
                "caption": "",
                "position": 711
            },
            {
                "img": "https://arxiv.org/html/2410.19100/extracted/5952369/map_tut.png",
                "caption": "",
                "position": 716
            },
            {
                "img": "https://arxiv.org/html/2410.19100/extracted/5952369/class.png",
                "caption": "",
                "position": 738
            },
            {
                "img": "https://arxiv.org/html/2410.19100/extracted/5952369/class_tut.png",
                "caption": "",
                "position": 743
            }
        ]
    },
    {
        "header": "4Baseline Agents",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.19100/extracted/5952369/prof_header.png",
                "caption": "Figure 4:VideoWebArena Baseline Agent Framework:We use 3 baseline agents: 1.) Video Summary Agent, where the video summary is fed in-context. 2.) Video Frame Agent, where a set number of frames and audio transcription is fed in-context. 3.) Video Agent, where the video is fed in as an .mov file in-context. The video information is put in-context along with the Set-of-Marks state representation to generate a singular action, following the multimodal SoM agent in VisualWebArena(Koh et al.,2024a).",
                "position": 804
            }
        ]
    },
    {
        "header": "5Results",
        "images": []
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AVideoWebArena Data Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.19100/extracted/5952369/proc_final.png",
                "caption": "Figure 5:Dataset Creation ProcessA walkthrough of the VideoWebArena dataset creation. From 1641 existing tasks in WebArena and VisualWebArena, the authors grouped these tasks by their intent templates. For each intent template, the authors created a new video tutorial showing how to perform the tasks. For each video, the authors made at minimum 4 factual retention tasks. This led to 1641 skill retention and 400 factual retention tasks.",
                "position": 1875
            },
            {
                "img": "https://arxiv.org/html/2410.19100/extracted/5952369/tax_final.png",
                "caption": "Figure 6:VideoWebArena Task TaxonomyWe define a taxonomy for all the tasks in our benchmark, namely splitting them into a factual and skill retention groups. Under the factual retention group, there are 4 types of tasks: Visual Perception, Audio Perception, Full Video Understanding, and Temporal Reasoning.",
                "position": 1885
            }
        ]
    },
    {
        "header": "Appendix BFailure Modes",
        "images": []
    },
    {
        "header": "Appendix CResults",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.19100/x1.png",
                "caption": "Figure 7:Abridged Example of VideoWebArena Task.A stylized example of a task in VideoWebArena: starting from (a) to (b), the task is defined, and an agent interacts with its visual input to create a plan and perform actions. From (b) to (c), it continues its actions and planning along its trajectory for the task before concluding (incorrectly) in (d), where it receives a final reward of zero for failing to complete the task correctly.",
                "position": 2114
            }
        ]
    },
    {
        "header": "Appendix DAgent Prompts",
        "images": []
    }
]