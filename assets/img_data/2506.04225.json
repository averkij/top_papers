[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04225/x1.png",
                "caption": "Figure 1.Voyageris a world-consistent video generation and reconstruction framework. Up: Voyager can generate 3D-consistent scene videos for world exploration following custom camera trajectories. Bottom: Voyager jointly generates aligned depth and RGB video for effective and direct 3D reconstruction.",
                "position": 59
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04225/x2.png",
                "caption": "Figure 2.Partial RGB images and partial depth maps rendered from point clouds at different frames. In scenarios involving complex occlusion relationships, partial RGB images often exhibit significant visual artifacts. In contrast, partial depth maps can accurately represent occlusions.",
                "position": 147
            }
        ]
    },
    {
        "header": "2.Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04225/x3.png",
                "caption": "Figure 3.Overview of Voyager: Given the input image and camera trajectories, we first render partial RGB images and depth maps for each viewpoint as the condition for video generation. Our world-consistent video diffusion model is trained to generate RGB-D frames simultaneously, thus supporting the direct reconstruction of the 3D world. The projected points are store in our world cache efficiently, which can be rendered as condition for the next round generation.",
                "position": 205
            }
        ]
    },
    {
        "header": "3.Preliminaries of Video Diffusion Models",
        "images": []
    },
    {
        "header": "4.Methodology: Voyager",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04225/x4.png",
                "caption": "Figure 4.Qualitative results on video generation. Compared to the baselines, our model can generate a more reasonable unseen region and meanwhile preserve the content in the input view.",
                "position": 494
            }
        ]
    },
    {
        "header": "5.Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04225/x5.png",
                "caption": "Figure 5.Qualitative results on Gaussian Splatting reconstruction. Our results present much more details than the compared baselines.",
                "position": 534
            }
        ]
    },
    {
        "header": "6.Application",
        "images": []
    },
    {
        "header": "7.Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04225/x6.png",
                "caption": "Figure 6.Applications: (a) Long-range video generation. (b) Image-to-3D generation. (c) World-consistent video style transfer. (d) Monocular video depth estimation.",
                "position": 787
            },
            {
                "img": "https://arxiv.org/html/2506.04225/x7.png",
                "caption": "Figure 7.Qualitative results on ablation study. We compare the video models in our three training stages. Our final model achieves the highest quality.",
                "position": 790
            },
            {
                "img": "https://arxiv.org/html/2506.04225/x8.png",
                "caption": "Figure 8.Qualitative results on ablation study. We compare the video models in our three training stages. Our final model achieves the highest quality.",
                "position": 793
            }
        ]
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04225/x9.png",
                "caption": "Figure 9.Details of world-consistent diffusion model.",
                "position": 1608
            }
        ]
    },
    {
        "header": "Appendix BWorld-Consistent Video Diffusion",
        "images": []
    },
    {
        "header": "Appendix CScalable Video Data Engine",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04225/x10.png",
                "caption": "Figure 10.Overview of our scalable video data engine.",
                "position": 1625
            },
            {
                "img": "https://arxiv.org/html/2506.04225/x11.png",
                "caption": "Figure 11.Training warp images compare.",
                "position": 1628
            },
            {
                "img": "https://arxiv.org/html/2506.04225/x12.png",
                "caption": "Figure 12.Comparison of initialization point clouds of ours and VGGT.",
                "position": 1692
            },
            {
                "img": "https://arxiv.org/html/2506.04225/x13.png",
                "caption": "Figure 13.More Results.",
                "position": 1695
            },
            {
                "img": "https://arxiv.org/html/2506.04225/x14.png",
                "caption": "Figure 14.More Visualization Results.",
                "position": 1698
            }
        ]
    },
    {
        "header": "Appendix DMore Results",
        "images": []
    }
]