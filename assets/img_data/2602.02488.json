[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02488/assets/git.png",
                "caption": "",
                "position": 139
            },
            {
                "img": "https://arxiv.org/html/2602.02488/assets/HF.png",
                "caption": "",
                "position": 139
            },
            {
                "img": "https://arxiv.org/html/2602.02488/figures/alfworld_acc_band2.png",
                "caption": "1. Jointly optimizing the reward model and the environment, in turn, benefits the policy’s learning curve, yielding higher converged accuracy.",
                "position": 144
            },
            {
                "img": "https://arxiv.org/html/2602.02488/figures/alfworld_acc_band2.png",
                "caption": "1. Jointly optimizing the reward model and the environment, in turn, benefits the policy’s learning curve, yielding higher converged accuracy.",
                "position": 147
            },
            {
                "img": "https://arxiv.org/html/2602.02488/figures/osworld_step_outcome_curve.png",
                "caption": "2. Step-wise signals from optimized reward model outperform human-labeled outcome signals. Moreover, integrated feedback is vital for long-trajectory tasks.",
                "position": 152
            },
            {
                "img": "https://arxiv.org/html/2602.02488/figures/bench_models_compare.png",
                "caption": "3. We demonstrate effectiveness of RLAnything across diverse real-world applications, including computer control, coding, and text-based games.",
                "position": 158
            },
            {
                "img": "https://arxiv.org/html/2602.02488/figures/env_reward_impro.png",
                "caption": "4. New environment tasks scale linearly, and the reward model gets stronger at evaluating both current-step correctness and outcome influence.",
                "position": 163
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02488/x1.png",
                "caption": "Figure 2:Motivation and takeaways of our RLAnything framework. First, in complex real-world applications, reinforcement learning benefits from integrating step-wise rewards with outcome rewards. Second, the reward model can be jointly optimized with the policy via outcome supervision and self-consistency signals. Third, we show that adapting environment task difficulty to the policy’s capability not only facilitates policy learning but also improves reward model training within our framework. Environment tasks leverage critic feedback from both the policy and the reward model to drive automatic, targeted adaptation, further enabling active learning from experience.",
                "position": 177
            }
        ]
    },
    {
        "header": "2RLAnything",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02488/x2.png",
                "caption": "Figure 3:Examples of environment task adaptation based on critic feedback across computer use agent, text-game agent, and coding LLM in our experiments. The critic feedback is summarized from the reward model’s evaluations and is used to automatically adapt tasks.",
                "position": 223
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02488/figures/alfworld_acc_band.png",
                "caption": "Figure 4:Each dynamic component consistently improves policy’s training curve across LLM agent and GUI agent settings.",
                "position": 604
            },
            {
                "img": "https://arxiv.org/html/2602.02488/figures/alfworld_acc_band.png",
                "caption": "",
                "position": 607
            },
            {
                "img": "https://arxiv.org/html/2602.02488/figures/osworld_acc_band.png",
                "caption": "",
                "position": 612
            },
            {
                "img": "https://arxiv.org/html/2602.02488/figures/osworld_grouped_results.png",
                "caption": "Figure 5:Results on OSWorld tasks for different models, including UI-TARS1.5-7B[qin2025ui], OpenCUA-7B[wang2025opencua], Qwen3-VL-8B-Thinking[Qwen3-VL], and our optimized model. Results are averaged over three independent runs, with the maximum number of interaction steps set to 50.",
                "position": 626
            },
            {
                "img": "https://arxiv.org/html/2602.02488/figures/process_outcome_env.png",
                "caption": "Figure 6:(a) shows the need for an integrated reward and that our optimized reward model alone provides a stronger learning signal than outcome supervision (the standard GRPO[deepseekmath]setting). (b) shows scaling with number of interaction steps.",
                "position": 629
            },
            {
                "img": "https://arxiv.org/html/2602.02488/figures/combined_one_row.png",
                "caption": "Figure 7:(a). The dynamics of accepted new tasks across three settings, including the number of accepted new tasks and the policy’s accuracy on these tasks over steps. (b). The average response length per action step during AlfWorld training.",
                "position": 663
            },
            {
                "img": "https://arxiv.org/html/2602.02488/figures/osworld_and_agenticode.png",
                "caption": "Figure 8:Left: Scaling up training for GUI agents. Right: Agentic coding results on LiveBench.",
                "position": 728
            }
        ]
    },
    {
        "header": "4Related Works",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AProof of Theorems",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experimental Results",
        "images": []
    },
    {
        "header": "Appendix CExperimental Details",
        "images": []
    }
]