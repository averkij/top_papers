[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": []
    },
    {
        "header": "IIOmni-AVSR",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07253/x1.png",
                "caption": "Fig. 1:Overview of(a)the proposedOmni-AVSRmodel and(b)its Omni-LoRA variants. Audio and video inputs are encoded by pre-trained modality-specific encoders and compressed by applying selected audio and video rates before projection into the LLM space.Omni-AVSRexplores three LoRA-based LLM adaptation strategies: 1)Omni-LoRA-Sdefines a single LoRA module for both ASR, VSR, and AVSR; 2)Omni-LoRA-Tdedicates task-specific LoRAs; 3)Omni-LoRA-STmakes use of both a shared LoRA and task-specific LoRA modules.",
                "position": 72
            }
        ]
    },
    {
        "header": "IIIExperiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07253/x2.png",
                "caption": "Fig. 2:Left: Comparison ofOmni-AVSR-STwith state-of-the-art AVSR methods in terms ofWER,activated parameters, andtraining data hourson LRS3.Right:Scalingtrend ofOmni-AVSR-STwhen we increase the LLM size on LRS3.",
                "position": 531
            },
            {
                "img": "https://arxiv.org/html/2511.07253/x2.png",
                "caption": "",
                "position": 534
            },
            {
                "img": "https://arxiv.org/html/2511.07253/x3.png",
                "caption": "",
                "position": 538
            }
        ]
    },
    {
        "header": "IVConclusion",
        "images": []
    }
]