[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03790/x1.png",
                "caption": "Figure 1:Average error rate across multiple evaluation datasets (GSM8K, MATH500, MMLU-Redux, TruthfulQA, and IFEval) as a function of inference-time floating point operations (FLOPS) in log scale.\nWe compareâˆ™âˆ™\\bulletâˆ™QAlignwithTÃ¼lu3-8B-SFTagainst four baselines:â–²â–²\\blacktriangleâ–²majority vote (MV)TÃ¼lu3-8B-DPO, and applied toTÃ¼lu3-8B-SFTthe methodsâˆ™âˆ™\\bulletâˆ™best-of-nğ‘›nitalic_n(BoN),âˆ™âˆ™\\bulletâˆ™MV, andâˆ™âˆ™\\bulletâˆ™weighted MV (WMV). All experiments use temperature 1.0 with reasoning included in model outputs. TheTÃ¼lu3-8B-DPOmodel results from preference finetuningTÃ¼lu3-8B-SFT(approximately1.75Ã—10191.75superscript10191.75\\times 10^{19}1.75 Ã— 10 start_POSTSUPERSCRIPT 19 end_POSTSUPERSCRIPTFLOPs). The costs of this process are not accounted for in this plot.",
                "position": 115
            }
        ]
    },
    {
        "header": "2Background: Language Model Alignment",
        "images": []
    },
    {
        "header": "3Test-Time Alignment via MCMC",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03790/x2.png",
                "caption": "Figure 2:Average accuracy vs.Â floating point operations (FLOPS) in log scale.\nWe compareâˆ™âˆ™\\bulletâˆ™QAlignwithLlama-3.1-8B-Instructagainst three baselines also applied toLlama-3.1-8B-Instruct:âˆ™âˆ™\\bulletâˆ™best-of-nğ‘›nitalic_n(BoN),âˆ™âˆ™\\bulletâˆ™majority vote (MV), andâˆ™âˆ™\\bulletâˆ™weighted MV (WMV).Left: Error rate (lower is better) on GSM8K test dataset.Right: Error rate on GSM-Symbolic test dataset. All experiments use temperature 1.0 with reasoning included in model outputs.",
                "position": 492
            },
            {
                "img": "https://arxiv.org/html/2504.03790/x2.png",
                "caption": "",
                "position": 495
            },
            {
                "img": "https://arxiv.org/html/2504.03790/x3.png",
                "caption": "",
                "position": 499
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion and Future work",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ABest-of-nğ‘›nitalic_nSampling as Test-Time Alignment",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03790/extracted/6334931/figures/bon/gumblehifi.png",
                "caption": "Figure 3:Distribution of the normalized maximum reward(rmax(n)âˆ’an)/bnsubscriptsuperscriptğ‘Ÿğ‘›maxsubscriptğ‘ğ‘›subscriptğ‘ğ‘›({r^{(n)}_{\\text{max}}-a_{n}})/{b_{n}}( italic_r start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT max end_POSTSUBSCRIPT - italic_a start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) / italic_b start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPTfor varyingnğ‘›nitalic_n, overlaid with the standard Gumbel distribution. The empirical distribution is estimated using 10,000 trials, each consisting ofnğ‘›nitalic_nrandom samples drawn from a Normal distribution. The fit between the empirical distribution and the Normal distribution improves asnğ‘›nitalic_nincreases, showing good agreement fornâ‰¥32ğ‘›32n\\geq 32italic_n â‰¥ 32.",
                "position": 1973
            },
            {
                "img": "https://arxiv.org/html/2504.03790/extracted/6334931/figures/bon/normalhifi.png",
                "caption": "Figure 4:Distribution of the normalized maximum reward(rmax(n)âˆ’Î¼Ï€,dâ¢(x,Î²âˆ—))/Ïƒdâ¢(x)subscriptsuperscriptğ‘Ÿğ‘›maxsubscriptğœ‡ğœ‹ğ‘‘ğ‘¥superscriptğ›½âˆ—subscriptğœğ‘‘ğ‘¥({r^{(n)}_{\\text{max}}-\\mu_{\\pi,d}(x,\\beta^{\\ast})})/\\sigma_{d}(x)( italic_r start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT max end_POSTSUBSCRIPT - italic_Î¼ start_POSTSUBSCRIPT italic_Ï€ , italic_d end_POSTSUBSCRIPT ( italic_x , italic_Î² start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ) ) / italic_Ïƒ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( italic_x )for varyingnğ‘›nitalic_n, overlaid with the standard Normal distribution. The empirical distribution is estimated using 10,000 trials, each consisting ofnğ‘›nitalic_nrandom samples drawn from a Normal distribution. While the mode of our approximation matches, the approximation does not capture the variance of the empirical distribution.",
                "position": 2104
            }
        ]
    },
    {
        "header": "Appendix BComputational Cost ofQAlign",
        "images": []
    },
    {
        "header": "Appendix CEmpirical Observations on Distribution of Rewards",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03790/extracted/6334931/figures/bon/sampled_hist.png",
                "caption": "Figure 5:Histogram of rewards assigned byTÃ¼lu3-8B-RMto1,02410241,0241 , 024responses generated byTÃ¼lu3-8B-SFTfor9999randomly sampled prompts from GSM8K. For each prompt, we fit a two-component Gaussian mixture model to characterize the reward distribution.",
                "position": 2129
            }
        ]
    },
    {
        "header": "Appendix DPrompts Used for Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03790/x4.png",
                "caption": "Figure 6:Error rate across multiple evaluation datasets (GSM8K, MATH500, MMLU-Redux, TruthfulQA, and IFEval) as a function of the floating point operations (FLOPS) in log scale.\nWe compareâˆ™âˆ™\\bulletâˆ™QAlignwithTÃ¼lu3-8B-SFTagainst four baselines:â–²â–²\\blacktriangleâ–²majority vote (MV)TÃ¼lu3-8B-DPO, and applied toTÃ¼lu3-8B-SFTthe methodsâˆ™âˆ™\\bulletâˆ™best-of-nğ‘›nitalic_n(BoN),âˆ™âˆ™\\bulletâˆ™MV, andâˆ™âˆ™\\bulletâˆ™weighted MV (WMV). All experiments use temperature 1.0 with reasoning included in model outputs. Note thatTÃ¼lu3-8B-DPOmodel is the result of doing preference finetuning on theTÃ¼lu3-8B-SFTwith 271k preference pairs. The costs associated with this process are not accounted for in this plot.",
                "position": 2187
            },
            {
                "img": "https://arxiv.org/html/2504.03790/x4.png",
                "caption": "",
                "position": 2190
            },
            {
                "img": "https://arxiv.org/html/2504.03790/x5.png",
                "caption": "",
                "position": 2194
            },
            {
                "img": "https://arxiv.org/html/2504.03790/x6.png",
                "caption": "",
                "position": 2199
            },
            {
                "img": "https://arxiv.org/html/2504.03790/x7.png",
                "caption": "",
                "position": 2203
            },
            {
                "img": "https://arxiv.org/html/2504.03790/x8.png",
                "caption": "",
                "position": 2208
            }
        ]
    },
    {
        "header": "Appendix EFull General Alignment Plots",
        "images": []
    }
]