[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01619/x1.png",
                "caption": "Figure 1:Overview: Self-evaluating code correctness using LLMs can be unreliable and collecting human-written unit tests can be laborious; we proposeUTGen, which automatically generates failing unit tests for a faulty code (triggering errors) without access to the gold solution. The generated unit tests can in turn be used for LLM debugging based on unit test feedback viaUTDebug, leading to improved downstream code accuracy.",
                "position": 166
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01619/x2.png",
                "caption": "Figure 2:UTGenTraining Pipeline:Starting with training data for code generation (problem description and gold code), we create training data for UT generation in three stages: (I) perturbing gold code to generate faulty codes, (II) generating UT inputs and filtering for failing UTs, and (III) generating and relabeling chain-of-thought rationales conditioned on the gold codeâ€™s outputs.",
                "position": 274
            }
        ]
    },
    {
        "header": "3UTGenandUTDebug: Unit Test Generation and Automated Debugging",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01619/x3.png",
                "caption": "Figure 3:Left: We highlight potential issues with debugging a faulty code using generated UTs: (I)non-failingUTs misclassify faulty code as correct; (II) UTs with incorrect outputs produce incorrect feedback and consequently, unsuccessful debugging.Right: To handle these unique challenges, we introduceUTDebugwhich (a) uses inference-time scaling to select better UT outputs based on a majority vote, and (b) generates multiple UTs for validation and backtracking, discarding edits when overall pass rate decreases (as in round 1) and accepting edits when overall pass rate improves (as in round 2).",
                "position": 356
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01619/x4.png",
                "caption": "Table 1:Evaluation on intrinsic UT generation metricsof different UT generation baselines andUTGenacross different model families on MBPP+Fix (Hard) over 3 runs. Higher is better for all three intrinsic metrics.",
                "position": 577
            },
            {
                "img": "https://arxiv.org/html/2502.01619/x4.png",
                "caption": "Figure 4:Impact of increasing number of unit tests across MBPP+Fix and MBPP+Fix (Hard) datasets using UTs generated byUTGenand randomly-sampling with Qwen2.5 model.",
                "position": 796
            }
        ]
    },
    {
        "header": "6Discussion and Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADebugging Datasets",
        "images": []
    },
    {
        "header": "Appendix BUTGenTraining",
        "images": []
    },
    {
        "header": "Appendix COverall Pipeline forUTDebug",
        "images": []
    },
    {
        "header": "Appendix DAdditional Intrinsic Evaluation",
        "images": []
    },
    {
        "header": "Appendix EPrompts",
        "images": []
    }
]