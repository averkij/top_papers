[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07597/x1.png",
                "caption": "Figure 1:The comparison between the distribution of sequence lengths in different existing large-scale markerless motion datasets with ours. Thexùë•xitalic_x-axis andyùë¶yitalic_y-axis denote the duration time (s) and percentage of video number, respectively. Our dataset (in green) contains more portion of long-sequence videos in general.",
                "position": 151
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07597/x2.png",
                "caption": "Figure 3:Shot transition detection examples. Examples (a), (b), and (c) illustrate multi-shot scenarios in online videos. (a) shows scene transitions detectable by SceneDetect. (b) illustrates significant position changes undetectable by SceneDetect but resolvable with bbox tracking-based method. (c) shows pose or orientation transition, requiring pose tracking-based methods as they cannot be addressed by either SceneDetect or bbox tracking.",
                "position": 276
            },
            {
                "img": "https://arxiv.org/html/2503.07597/x3.png",
                "caption": "Figure 4:Human orientation alignment module. Following a shot transition after the foremost purple human mesh (shot‚ë†captured by cameraC0subscriptùê∂0C_{0}italic_C start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT), the unaligned (blue) and aligned (green) motions are captured as shot‚ë°and shot‚Äú‚ë¢‚Äùby cameraC0‚Ä≤superscriptsubscriptùê∂0‚Ä≤C_{0}^{{}^{\\prime}}italic_C start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT ‚Ä≤ end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPTandC1subscriptùê∂1C_{1}italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, respectively.C0‚Ä≤=C0superscriptsubscriptùê∂0‚Ä≤subscriptùê∂0C_{0}^{{}^{\\prime}}=C_{0}italic_C start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT ‚Ä≤ end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT = italic_C start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. To achieve human orientation alignment from shot‚Äú‚ë†‚Äùto‚Äú‚ë¢‚Äù, the camera rotation matrix fromC0‚Ä≤superscriptsubscriptùê∂0‚Ä≤C_{0}^{{}^{\\prime}}italic_C start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT ‚Ä≤ end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPTtoC1subscriptùê∂1C_{1}italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTis computed and applied as the offset of human orientation.",
                "position": 338
            }
        ]
    },
    {
        "header": "4Benchmarking Multi-shot Motion Recovery",
        "images": []
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07597/x4.png",
                "caption": "Figure 6:Qualitative comparison of different HMR methods onms-Motion dataset.The side view of the rendered mesh for input mutli-shot video is shown in (a), while the top view is shown in (c). We also draw the comparison of the human trajectory as shown in (b). Our method is the most similar as GT in both rendered motion and trajectories among these methods.",
                "position": 986
            }
        ]
    },
    {
        "header": "6Conclusion and Discussion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMasked DPVO Detailed Algorithm",
        "images": []
    },
    {
        "header": "Appendix BMasked LEAP-VO Detailed Algorithm",
        "images": []
    },
    {
        "header": "Appendix CCamera Calibration Procedure",
        "images": []
    },
    {
        "header": "Appendix DImplementation Details ofHumanMM",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07597/x5.png",
                "caption": "",
                "position": 3161
            },
            {
                "img": "https://arxiv.org/html/2503.07597/x6.png",
                "caption": "",
                "position": 3163
            },
            {
                "img": "https://arxiv.org/html/2503.07597/x7.png",
                "caption": "",
                "position": 3167
            },
            {
                "img": "https://arxiv.org/html/2503.07597/x8.png",
                "caption": "",
                "position": 3178
            },
            {
                "img": "https://arxiv.org/html/2503.07597/x9.png",
                "caption": "",
                "position": 3180
            }
        ]
    },
    {
        "header": "Appendix EVisualization of Comparison between Existing Methods",
        "images": []
    }
]