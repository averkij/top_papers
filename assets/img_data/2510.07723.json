[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07723/x1.png",
                "caption": "Figure 1:Geometric comparison between SMPL estimationpatel2024camerahmr, 2D multiview generative model (MVD) PSHumanli2024pshuman, native 3D generative model Trellisxiang2024structuredand our method. 2D MVD produces high-quality details but has geometry artifacts when conditioned on inaccurate SMPL meshes. Native 3D generative model produces correct coarse structure but loses fine details and fidelity. Our method combines the strengths of both 2D and 3D generative models to produce detailed 3D human meshes with high fidelity.",
                "position": 111
            }
        ]
    },
    {
        "header": "2Related works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07723/x2.png",
                "caption": "Figure 2:Overview. Given a single human image, SyncHuman first generates multiview color and normal maps, along with an aligned sparse voxel grid, which is further transformed into a set of structured latents. Then, we propose to inject the high-quality images into the 3D latents via a Multiview Guided Decoder and output the detailed high-fidelity textured human mesh.",
                "position": 173
            },
            {
                "img": "https://arxiv.org/html/2510.07723/x3.png",
                "caption": "Figure 3:2D-3D synchronization attention.2D to 3D attention:each 3D voxel feature is orthogonally projected onto front, back, left, and right view planes to retrieve corresponding 2D features, and refines the voxel feature with cross-attention.3D to 2D attention:each 2D multiview feature is projected into 3D space to attend to a column of voxel features, enhancing the 2D features. This mutual refinement ensures that 2D generative model and 3D generative model align with each other in a shared 3D space.",
                "position": 220
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07723/x4.png",
                "caption": "Figure 4:Geometry comparisons between ECONvisrecon, Human3Diffxue2024human3diff, SIFUzhang2024sifu, PSHumanli2024pshumanand ours. Our method could reconstruct 3D shapes with complete body structure and rich details.",
                "position": 282
            },
            {
                "img": "https://arxiv.org/html/2510.07723/x5.png",
                "caption": "Figure 5:Appearance qualitative comparisons between GTAvisrecon, Human3Diffxue2024human3diff, SIFUzhang2024sifu, PSHumanli2024pshumanand our method.",
                "position": 307
            },
            {
                "img": "https://arxiv.org/html/2510.07723/x6.png",
                "caption": "Figure 6:Ablation study of the 2D-3D synchronization attention for a joint 2D-3D modeling between PSHumanli2024pshuman, fine-tuned Trellisxiang2024structured, and our model.",
                "position": 520
            },
            {
                "img": "https://arxiv.org/html/2510.07723/x7.png",
                "caption": "Figure 7:Ablation of different decoder settings. “original decoder” means the pretrained Trellisxiang2024structureddecoder, while “original decoder (tuned)” and “multiview guided” are trained on the same human scans.All models use the same structured latents but decode them with different decoders.",
                "position": 531
            },
            {
                "img": "https://arxiv.org/html/2510.07723/x8.png",
                "caption": "Figure 8:Robustness Analysis of the Generated Structure. The results demonstrate the robust reconstruction capabilities of our approach.",
                "position": 728
            }
        ]
    },
    {
        "header": "5Limitation and Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07723/x9.png",
                "caption": "Figure 9:Unnatural textures under non-uniform lighting.",
                "position": 761
            }
        ]
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails",
        "images": []
    },
    {
        "header": "Appendix BMore Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07723/x10.png",
                "caption": "Figure 10:Qualitative comparison of SyncHuman with Gaussians-based methods (LHM, IDOL) and a native 3D model (Hunyuan3D 2.5). SyncHuman achieves visually high-fidelity results.",
                "position": 2146
            },
            {
                "img": "https://arxiv.org/html/2510.07723/figure/align_vis.png",
                "caption": "Figure 11:After alignment using 2D-3D attention, the multi-view projections of the two branches can almost completely overlap.",
                "position": 2204
            },
            {
                "img": "https://arxiv.org/html/2510.07723/figure/compare.png",
                "caption": "Figure 12:Comparison of the quality of intermediate multi-view generation with and without 2D-3D attention.",
                "position": 2208
            },
            {
                "img": "https://arxiv.org/html/2510.07723/figure/uncond.png",
                "caption": "Figure 13:Visualization of the unconditional generation task.",
                "position": 2233
            }
        ]
    },
    {
        "header": "Appendix Cdiscussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07723/x11.png",
                "caption": "Figure 14:In some cases, the decoded mesh may contain some holes on the surface.",
                "position": 2249
            }
        ]
    },
    {
        "header": "NeurIPS Paper Checklist",
        "images": []
    }
]