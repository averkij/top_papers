[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15872/x1.png",
                "caption": "Figure 1:MutaGRePOverviewGiven a user request that requires writing code against a specific codebase, we search for realizable plans to solve the user‚Äôs request using LLM-guided tree search. Our search procedure uses a symbol retriever to constrain search to plans which are implementable with symbols available in the codebase and explores the search space by mutating plans. Each step of the plan consists of a natural language intent and symbols from the codebase that can be used to implement the intent. The user request along with the detailed plan serves as an enriched query that provides necessary context from the codebase to any downstream coding system to convert the plan to code. Our plan search benefits from test-time compute scaling and produces repo-grounded plans without requiring code execution.",
                "position": 109
            },
            {
                "img": "https://arxiv.org/html/2502.15872/x2.png",
                "caption": "Figure 2:A repo-grounded plan created byMutaGRePon a query from LongCodeArena. Each plan step consists of a natural language intent with top-5 symbols retrieved from the codebase that might be useful for implementing the step.",
                "position": 114
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15872/x3.png",
                "caption": "Figure 3:Overview of plan search. Each node in the tree is a repo-grounded plan. At every time step, a node is chosen for growing the tree and successors are created by mutating the chosen plan. We use an LLM to implement the successor function.",
                "position": 142
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15872/x4.png",
                "caption": "Figure 4:Mutation and grounding.The successor functionsùë†sitalic_smutates a plan (left-most column) to generate new plans (right-most column). For each modified intent (t2subscriptùë°2t_{2}italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTandt3subscriptùë°3t_{3}italic_t start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT), the grounding functiongùëîgitalic_gmaps the intent to symbols that might be used to implement the intent (B2subscriptùêµ2B_{2}italic_B start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTandB3subscriptùêµ3B_{3}italic_B start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT).",
                "position": 164
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15872/x5.png",
                "caption": "Figure 5:Unconstrainedmutation outperformsmonotonicmutation, especially at lower budgets. Here, we show the symbol recall (% of ground truth symbols in the generated plans) of each mutation strategy using best-first search with the oracle scoring function and branching factor of3333. This figure also illustrates gains from scaling test-time compute (by increasing budget).",
                "position": 410
            },
            {
                "img": "https://arxiv.org/html/2502.15872/x6.png",
                "caption": "Figure 6:Comparison of Search Strategies.Informed(best-first) search outperformsuninformed(depth-first) and linear search strategies and performance improves with branching factor (BF), especially for informed search.",
                "position": 462
            },
            {
                "img": "https://arxiv.org/html/2502.15872/x7.png",
                "caption": "Figure 7:Our plans consistently improve performance across all models.Qwen 2.5 Coder 32B with our plans exceeds GPT-4o‚Äôs full-repo performance despite conditioning on 120k fewer context tokens. Even models stronger than GPT-4o (e.g., O1) benefit from our GPT-4o-generated plans. The red line shows GPT-4o performance when given the full repository as context.",
                "position": 507
            },
            {
                "img": "https://arxiv.org/html/2502.15872/x8.png",
                "caption": "Figure 8:Our plans enable progress on hard tasks where even full-repo context performed poorly.Conditioning on tree-searched plans shows gains on the hardest 10% of tasks where GPT-4o with full-repo context performed poorly.",
                "position": 521
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AQualitative Examples of Synthetic Intents",
        "images": []
    },
    {
        "header": "Appendix BMore Analysis of Hard Tasks on LongCodeArena",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15872/x9.png",
                "caption": "Figure 9:Performance comparison on the most challenging LongCodeArena tasks (bottom 10th percentile by full-repo performance). For each repository, we show the performance of GPT-4o when given: the full repository as context (red), ReACT-generated plans (orange), or our tree-searched plans (blue). Bars show average performance across 5 samples, while the bottom of error bars indicates worst-case performance. Our tree-searched plans (using unconstrained successor function and branching factor=3) consistently outperform both baselines, with worst-case performance often exceeding the baselines‚Äô average performance. All scores are API overlap percentages measuring alignment with reference solutions.",
                "position": 1215
            }
        ]
    },
    {
        "header": "Appendix CSuccessor Function Prompts",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15872/x10.png",
                "caption": "Figure 10:Monotonic Successor Function Prompt",
                "position": 1231
            },
            {
                "img": "https://arxiv.org/html/2502.15872/x11.png",
                "caption": "Figure 11:Unconstrained Successor Function Prompt",
                "position": 1234
            }
        ]
    },
    {
        "header": "Appendix DScoring Function Prompts",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15872/x12.png",
                "caption": "Figure 12:Likert Scoring Function Prompt",
                "position": 1244
            },
            {
                "img": "https://arxiv.org/html/2502.15872/x13.png",
                "caption": "Figure 13:Plan-based Code Generation Prompt",
                "position": 1254
            },
            {
                "img": "https://arxiv.org/html/2502.15872/x14.png",
                "caption": "Figure 14:Full-repository Context Code Generation Prompt",
                "position": 1257
            },
            {
                "img": "https://arxiv.org/html/2502.15872/x15.png",
                "caption": "Figure 15:Instruction-only Code Generation Prompt",
                "position": 1260
            }
        ]
    },
    {
        "header": "Appendix ECode Generation Prompts",
        "images": []
    }
]