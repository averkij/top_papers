[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19355/extracted/6307507/Figures/brownstoat.png",
                "caption": "",
                "position": 56
            },
            {
                "img": "https://arxiv.org/html/2503.19355/x1.png",
                "caption": "(a)",
                "position": 89
            },
            {
                "img": "https://arxiv.org/html/2503.19355/x2.png",
                "caption": "(b)",
                "position": 92
            },
            {
                "img": "https://arxiv.org/html/2503.19355/x3.png",
                "caption": "Figure 2:Task examples from the proposed STKit-Bench along with predictions from ST-VLM.",
                "position": 105
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19355/extracted/6307507/Figures/direction2.png",
                "caption": "Figure 3:Movement directions as clockwise directions.",
                "position": 322
            },
            {
                "img": "https://arxiv.org/html/2503.19355/x4.png",
                "caption": "Figure 4:Pseudo-label generation pipeline.For the geometric reconstruction branch, a canonicalized 4D scene is reconstructed using MonST3R[43]and Metric3Dv2[16].\nFor the semantic understanding branch, the object bounding boxes, segmentation masks, and trajectories are extracted using Grounded-SAM2[32].\nFinally, by integrating each branch, 2D object masks are lifted to 3D and trajectories are computed by tracking 3D barycenters in the 4D scene.",
                "position": 326
            }
        ]
    },
    {
        "header": "4STKit-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19355/x5.png",
                "caption": "",
                "position": 741
            },
            {
                "img": "https://arxiv.org/html/2503.19355/x5.png",
                "caption": "",
                "position": 744
            },
            {
                "img": "https://arxiv.org/html/2503.19355/x6.png",
                "caption": "",
                "position": 750
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19355/x7.png",
                "caption": "Figure 6:Comparison on LLaVA-OneVision and ST-VLM for spatio-temporal understanding.mIoU is multiplied by 100.",
                "position": 1157
            },
            {
                "img": "https://arxiv.org/html/2503.19355/x8.png",
                "caption": "(a)",
                "position": 1162
            },
            {
                "img": "https://arxiv.org/html/2503.19355/x8.png",
                "caption": "(a)",
                "position": 1165
            },
            {
                "img": "https://arxiv.org/html/2503.19355/x9.png",
                "caption": "(b)",
                "position": 1170
            }
        ]
    },
    {
        "header": "6Analysis",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]