[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19356/x1.png",
                "caption": "",
                "position": 67
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Qualcomm IVD",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19356/x2.png",
                "caption": "Figure 2:Temporal relationship between the end of the video and optimal answer timing. The horizontal axis represents seconds from the optimal time to answer to the the end of the video.",
                "position": 691
            }
        ]
    },
    {
        "header": "4Baseline Streaming Approach",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19356/x3.png",
                "caption": "Figure 3:Evaluations of the public and finetuned VideoLLaMA2.1-7B-AV[5]in vision+audio and vision-only settings.",
                "position": 1142
            },
            {
                "img": "https://arxiv.org/html/2503.19356/x4.png",
                "caption": "Figure 4:Comparing correctness of selected baseline LMMs across individual categories of Qualcomm IVD.",
                "position": 1149
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19356/extracted/6306849/figures/diversity.png",
                "caption": "Figure A.1:Each image showcases a different video from our collection, demonstrating the substantial variation in visual scenarios captured within the dataset. These examples highlight the diversity of environments (indoor and outdoor settings), participants, objects, actions, lighting conditions, camera angles, and compositional elements present across the dataset.",
                "position": 1827
            }
        ]
    },
    {
        "header": "Appendix BAdditional Experimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19356/x5.png",
                "caption": "Figure B.1:Examples of questions that GPT-4o refused to answer due toResponsibleAIPolicyViolation.",
                "position": 1846
            }
        ]
    },
    {
        "header": "Appendix CGPT-4o Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19356/x6.png",
                "caption": "Figure D.1:Simple daily face-to-face questions that strong baseline LMMs such as GPT-4o, VideoLLaMMA2-72B, and VideoLLaMA2.1-7B-AV fail to answer.",
                "position": 2112
            }
        ]
    },
    {
        "header": "Appendix DFailure Cases",
        "images": []
    }
]