[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13111/extracted/6286766/figures/Teaser.png",
                "caption": "",
                "position": 125
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13111/extracted/6286766/figures/Dataset_Sample.png",
                "caption": "Figure 2:CA-VQA Data Example.Example of a single sample from our dataset. Each reference frame has between 0-4 multi-view support frames. All frames (reference and support) come with three metric depth maps: Ground truth (FARO laser), ARKit Depth (LiDAR-fused) and Monocular (DepthPro). Each support frame contains the relative pose from the reference image, along with camera intrinsics.",
                "position": 531
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Data",
        "images": []
    },
    {
        "header": "4Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13111/extracted/6286766/figures/Depth_Tool-Use.png",
                "caption": "Figure 3:Example of leveraging depth maps via tool-use. The model predicts the objects’ 2D bounding boxes and function calls, receives thetool outputs(which is the median depth value within the box, marked with an×\\mathbf{\\times}×), and finally reasons about the answer.",
                "position": 682
            },
            {
                "img": "https://arxiv.org/html/2503.13111/extracted/6286766/figures/Qualitative_Example.png",
                "caption": "Figure 4:Qualitative Example.We show the predictions of various models on a challenging example from our CA-VQA benchmark. Strong commercial (2a&b) and research models (2c&d) fail. MM-Spatial (1a) is much better, and even more so with CoT enabled (1b), demonstrating our model’s strong object grounding (see predicted 2D boxes in the image), depth estimation, and spatial reasoning ability. Accuracy improves further when leveraging ground-truth depth via tool-use (1c), although our CoT model’s (1b) predictions are very close to that, for both the intermediate depth values and final answer; monocular estimated depth (1d) is less accurate and yields a worse result.",
                "position": 685
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13111/extracted/6286766/figures/QA_Examples.png",
                "caption": "Figure 5:CA-VQA Overview.Example QA pairs from our Cubify Anything VQA (CA-VQA) dataset, aiming to unlock object-centric 3D spatial understanding in MLLMs. Using high-quality 3D ground truth annotations from CA-1M[61], we generate spatial perception questions across a variety of different tasks, e.g., involvingrelative relationships,metric measurements, and3D object bounding boxes.",
                "position": 4054
            }
        ]
    },
    {
        "header": "Appendix AMore Details about the CA-VQA Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13111/extracted/6286766/figures/Spatial_Category_Examples_1.png",
                "caption": "Figure 6:Examples of CA-VQA data samples from the Binary, Counting and Multi-choice categories.",
                "position": 4214
            },
            {
                "img": "https://arxiv.org/html/2503.13111/extracted/6286766/figures/Spatial_Category_Examples_2.png",
                "caption": "Figure 7:Examples of CA-VQA data samples from the Regression (Metric Estimation) and 2D Grounding categories.",
                "position": 4217
            }
        ]
    },
    {
        "header": "Appendix BOptimal Data Mixture for MM-Spatial",
        "images": []
    },
    {
        "header": "Appendix CResults on Further Benchmark Categories",
        "images": []
    },
    {
        "header": "Appendix DAnalysis of Blind Filtering Procedure",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13111/extracted/6286766/figures/AABB_vs_OBB.png",
                "caption": "Figure 8:Comparative visualization of axis-aligned vs. oriented 3D bounding boxes, taken from the SpatialRGPT paper[27, Appendix K, Figure 11]. The object dimensions computed from AABBs can differ substantially from those computed from OBBs, depending on the object’s rotation. For sake of illustration, assume that the sofa is 2m wide and 0.8m deep. We then obtain the following altered object dimensions when using an AABB instead of an OBB, at different yaw rotation angles (i.e., considering 7-DOF bounding boxes that are gravity-aligned / parallel to the ground, as in CA-1M / CA-VQA): width≈\\approx≈2.1m and depth≈\\approx≈1.7m with 30∘rotation; width≈\\approx≈1.7m and depth≈\\approx≈2.1m with 60∘rotation; and width = 0.8m and depth = 2m with 90∘rotation (i.e., ”full” rotation resulting in swapped dimensions).",
                "position": 5180
            }
        ]
    },
    {
        "header": "Appendix EAxis-aligned vs. Oriented 3D Boxes",
        "images": []
    }
]