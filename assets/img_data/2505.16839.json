[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.16839/x1.png",
                "caption": "Figure 1:We propose LaViDa,the first family of diffusion-based discrete VLM models. LaViDaÂ  models achieve competitive performance against AR baselines (LLaVa-1.6, Open-LLaVa-Next) across multiple visual understanding tasks including MMMU (world knowledge), MathVista (reasoning), ChartQA (OCR), and ScienceQA (science).",
                "position": 96
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background and Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.16839/x2.png",
                "caption": "Figure 2:Overall design of LaViDa.LaViDaâ€™s architecture consists of a vision encoder, a diffusion language model, and an MLP vision projector. The bottom half of the figure illustrates the image encoding process, while the top half depicts the diffusion language modeling process. These two pipelines are described in detail in Sec.3.1.",
                "position": 188
            },
            {
                "img": "https://arxiv.org/html/2505.16839/x3.png",
                "caption": "(a)Complementary Masking",
                "position": 207
            },
            {
                "img": "https://arxiv.org/html/2505.16839/x3.png",
                "caption": "(a)Complementary Masking",
                "position": 210
            },
            {
                "img": "https://arxiv.org/html/2505.16839/x4.png",
                "caption": "(b)Attention Mask of Prefix-DLM",
                "position": 215
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.16839/x5.png",
                "caption": "(a)Text-Infilling",
                "position": 641
            },
            {
                "img": "https://arxiv.org/html/2505.16839/x5.png",
                "caption": "(a)Text-Infilling",
                "position": 644
            },
            {
                "img": "https://arxiv.org/html/2505.16839/x6.png",
                "caption": "(b)Speed vs. Quality Tradeoff",
                "position": 649
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Technical Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.16839/x7.png",
                "caption": "(a)Discretized Schedules at DifferentKğ¾Kitalic_K.",
                "position": 2101
            },
            {
                "img": "https://arxiv.org/html/2505.16839/x7.png",
                "caption": "(a)Discretized Schedules at DifferentKğ¾Kitalic_K.",
                "position": 2104
            },
            {
                "img": "https://arxiv.org/html/2505.16839/x8.png",
                "caption": "(b)Different Choice of Continuous Schedulesfğ‘“fitalic_f.",
                "position": 2109
            }
        ]
    },
    {
        "header": "Appendix BAdditional Experiment Details and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.16839/x9.png",
                "caption": "Figure 6:Additional Qualitative Results for Text Infilling.We showcase several useful applications of text-infilling capabilities.",
                "position": 2371
            },
            {
                "img": "https://arxiv.org/html/2505.16839/x10.png",
                "caption": "Figure 7:Visualization of Different Choices of Attention Mask at Inference Time.Iğ¼Iitalic_Irepresents the image embeddings,Pğ‘ƒPitalic_Prepresents the prompt tokens, andXtsubscriptğ‘‹ğ‘¡X_{t}italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTrepresents the partially masked answer tokens. Each row represents a query and each column represents a key. Colored region indicts tokens queries and keys can interact with each other.",
                "position": 2463
            },
            {
                "img": "https://arxiv.org/html/2505.16839/x11.png",
                "caption": "Figure 8:Visualization of Training Strategies for Prefix-DLM.Given each triplet of imageIğ¼Iitalic_I, promptPğ‘ƒPitalic_Pand answerXğ‘‹Xitalic_X, we create two versions of partially masked answerXtsubscriptğ‘‹ğ‘¡X_{t}italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTandXtCsuperscriptsubscriptğ‘‹ğ‘¡ğ¶X_{t}^{C}italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPTwith complementary masking.(Left) By default, we construct two sequence and apply full attention mask. (Middle) Prefix-DLM-FT1 applies prefix attention mask to each copy independently. (Right) Prefix-DLM-FT2 combinesI,P,Xt,XtCğ¼ğ‘ƒsubscriptğ‘‹ğ‘¡superscriptsubscriptğ‘‹ğ‘¡ğ¶I,P,X_{t},X_{t}^{C}italic_I , italic_P , italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPTinto a single sequence and manipulate the attention mask to achieve an equivalent effect to Prefix-DLM-FT1.",
                "position": 2524
            }
        ]
    },
    {
        "header": "Appendix CAdditional Backgrounds",
        "images": []
    },
    {
        "header": "Appendix DLimitations",
        "images": []
    },
    {
        "header": "Appendix EReproducibility Statement",
        "images": []
    },
    {
        "header": "Appendix FBoarder Impacts and Safeguards",
        "images": []
    },
    {
        "header": "Appendix GLLM Usage",
        "images": []
    },
    {
        "header": "Appendix HConcurrent Work",
        "images": []
    }
]