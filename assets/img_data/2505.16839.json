[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.16839/x1.png",
                "caption": "Figure 1:We propose LaViDa,the first family of diffusion-based discrete VLM models. LaViDa  models achieve competitive performance against AR baselines (LLaVa-1.6, Open-LLaVa-Next) across multiple visual understanding tasks including MMMU (world knowledge), MathVista (reasoning), ChartQA (OCR), and ScienceQA (science).",
                "position": 96
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background and Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.16839/x2.png",
                "caption": "Figure 2:Overall design of LaViDa.LaViDa’s architecture consists of a vision encoder, a diffusion language model, and an MLP vision projector. The bottom half of the figure illustrates the image encoding process, while the top half depicts the diffusion language modeling process. These two pipelines are described in detail in Sec.3.1.",
                "position": 188
            },
            {
                "img": "https://arxiv.org/html/2505.16839/x3.png",
                "caption": "(a)Complementary Masking",
                "position": 207
            },
            {
                "img": "https://arxiv.org/html/2505.16839/x3.png",
                "caption": "(a)Complementary Masking",
                "position": 210
            },
            {
                "img": "https://arxiv.org/html/2505.16839/x4.png",
                "caption": "(b)Attention Mask of Prefix-DLM",
                "position": 215
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.16839/x5.png",
                "caption": "(a)Text-Infilling",
                "position": 641
            },
            {
                "img": "https://arxiv.org/html/2505.16839/x5.png",
                "caption": "(a)Text-Infilling",
                "position": 644
            },
            {
                "img": "https://arxiv.org/html/2505.16839/x6.png",
                "caption": "(b)Speed vs. Quality Tradeoff",
                "position": 649
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Technical Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.16839/x7.png",
                "caption": "(a)Discretized Schedules at DifferentK𝐾Kitalic_K.",
                "position": 2101
            },
            {
                "img": "https://arxiv.org/html/2505.16839/x7.png",
                "caption": "(a)Discretized Schedules at DifferentK𝐾Kitalic_K.",
                "position": 2104
            },
            {
                "img": "https://arxiv.org/html/2505.16839/x8.png",
                "caption": "(b)Different Choice of Continuous Schedulesf𝑓fitalic_f.",
                "position": 2109
            }
        ]
    },
    {
        "header": "Appendix BAdditional Experiment Details and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.16839/x9.png",
                "caption": "Figure 6:Additional Qualitative Results for Text Infilling.We showcase several useful applications of text-infilling capabilities.",
                "position": 2371
            },
            {
                "img": "https://arxiv.org/html/2505.16839/x10.png",
                "caption": "Figure 7:Visualization of Different Choices of Attention Mask at Inference Time.I𝐼Iitalic_Irepresents the image embeddings,P𝑃Pitalic_Prepresents the prompt tokens, andXtsubscript𝑋𝑡X_{t}italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTrepresents the partially masked answer tokens. Each row represents a query and each column represents a key. Colored region indicts tokens queries and keys can interact with each other.",
                "position": 2463
            },
            {
                "img": "https://arxiv.org/html/2505.16839/x11.png",
                "caption": "Figure 8:Visualization of Training Strategies for Prefix-DLM.Given each triplet of imageI𝐼Iitalic_I, promptP𝑃Pitalic_Pand answerX𝑋Xitalic_X, we create two versions of partially masked answerXtsubscript𝑋𝑡X_{t}italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTandXtCsuperscriptsubscript𝑋𝑡𝐶X_{t}^{C}italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPTwith complementary masking.(Left) By default, we construct two sequence and apply full attention mask. (Middle) Prefix-DLM-FT1 applies prefix attention mask to each copy independently. (Right) Prefix-DLM-FT2 combinesI,P,Xt,XtC𝐼𝑃subscript𝑋𝑡superscriptsubscript𝑋𝑡𝐶I,P,X_{t},X_{t}^{C}italic_I , italic_P , italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPTinto a single sequence and manipulate the attention mask to achieve an equivalent effect to Prefix-DLM-FT1.",
                "position": 2524
            }
        ]
    },
    {
        "header": "Appendix CAdditional Backgrounds",
        "images": []
    },
    {
        "header": "Appendix DLimitations",
        "images": []
    },
    {
        "header": "Appendix EReproducibility Statement",
        "images": []
    },
    {
        "header": "Appendix FBoarder Impacts and Safeguards",
        "images": []
    },
    {
        "header": "Appendix GLLM Usage",
        "images": []
    },
    {
        "header": "Appendix HConcurrent Work",
        "images": []
    }
]