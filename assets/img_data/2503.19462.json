[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19462/x1.png",
                "caption": "",
                "position": 72
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19462/x2.png",
                "caption": "Figure 2:1D Toy Experiment.We employ Flow Matching objective[31]to train the teacher model, which learns the ODE that maps Gaussian distribution to the data distribution. The data distribution consists of two data points.a)illustrates the knowledge distillation methods, where a student model is trained to mimic the teacher model‚Äôs denoising process.b)highlights the challenges posed by dataset or Gaussian noise mismatching in knowledge distillation, which can lead to unreliable guidance.c)demonstrates the distribution matching methods, which aims to align the output distribution of the student model with that of the teacher model.d)emphasizes the issue in distribution matching, which can result in inaccurate guidance.e)illustrates the frequency of useless data points in relation toMùëÄMitalic_M.f)shows the distillation results at various values ofMùëÄMitalic_M.",
                "position": 188
            },
            {
                "img": "https://arxiv.org/html/2503.19462/x3.png",
                "caption": "Figure 3:Method Overview.(a)Our method first designs a trajectory-based few-step guidance, which utilizes the key data points from the denoising trajectory to enable the student model to mimic the denoising process of the pretrained video diffusion model with fewer steps.(b)To fully exploit the data distribution at each diffusion timestep captured by our synthetic dataset, we propose an adversarial training strategy to align the output distribution of the student model with that captured by our synthetic dataset.",
                "position": 191
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19462/x4.png",
                "caption": "Figure 4:The pipeline of constructing SynVid.",
                "position": 219
            },
            {
                "img": "https://arxiv.org/html/2503.19462/x5.png",
                "caption": "Figure 5:The illustration of features at different layers and diffusion timesteps of our feature extractor. The features within the red box are selected for discrimination.",
                "position": 244
            },
            {
                "img": "https://arxiv.org/html/2503.19462/x6.png",
                "caption": "Figure 6:Qualitative results on text-to-video.Left:a girl raises her left hand to cover her smiling mouth.Right:the camera follows behind a white vintage SUV with a black roof rack as it speeds up a steep dirt road surrounded by pine trees on a steep mountain slope.",
                "position": 484
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19462/x7.png",
                "caption": "Figure 7:Qualitative ablation study results. Please zoom in for details. Prompt:a toy robot wearing purple overalls and cowboy boots taking a pleasant stroll in Johannesburg South Africa during a beautiful sunset.",
                "position": 703
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19462/x8.png",
                "caption": "Figure 8:Model architecture for the 1D toy experiment.",
                "position": 1602
            }
        ]
    },
    {
        "header": "ADetails about the 1D Toy Experiment",
        "images": []
    },
    {
        "header": "BSynVid",
        "images": []
    },
    {
        "header": "CMore Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19462/x9.png",
                "caption": "Figure 9:The length distribution of text prompts.",
                "position": 1631
            },
            {
                "img": "https://arxiv.org/html/2503.19462/x10.png",
                "caption": "Figure 10:The illustration of our synthetic video dataset, SynVid.",
                "position": 1634
            },
            {
                "img": "https://arxiv.org/html/2503.19462/x11.png",
                "caption": "Figure 11:Qualitative results on text-to-video with high resolution,i.e., 720√ó\\times√ó1280√ó\\times√ó129.",
                "position": 2107
            },
            {
                "img": "https://arxiv.org/html/2503.19462/x12.png",
                "caption": "Figure 12:Qualitative results on text-to-video with medium resolution,i.e., 544√ó\\times√ó960√ó\\times√ó93.",
                "position": 2110
            }
        ]
    },
    {
        "header": "DLimitations and Future Work",
        "images": []
    }
]