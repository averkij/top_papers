[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11037/Case.png",
                "caption": "Figure 1:When the agentic search model produces wrong answers, its lengthy and complex reasoning makes it difficult for users to verify. To ensure reliability, the model should explicitly state when information is insufficient and that no answer is available.",
                "position": 170
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Preliminary Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11037/pre_boundary.png",
                "caption": "Figure 2:Evaluation results of accuracy, precision andIDKrate (ρIDK\\rho_{\\textit{IDK}}) of models before and after RL. The sharp drop inρIDK\\rho_{\\textit{IDK}}coupled with the narrowing gap between accuracy and precision, indicates a diminished boundary awareness after RL.",
                "position": 345
            },
            {
                "img": "https://arxiv.org/html/2601.11037/x1.png",
                "caption": "Figure 3:Left:Validation accuracy under different reward settings during RL training.Right:IDKrate (ρIDK\\rho_{\\textit{IDK}}) under the modified reward during RL training.",
                "position": 348
            },
            {
                "img": "https://arxiv.org/html/2601.11037/x2.png",
                "caption": "Figure 4:The overall framework of BAPO. Its training process begins with(a)agentic reasoning,\nwhere the model generates a group of rollouts for each question by interleaving thought processes with search environment interactions. These rollouts are then passed to the(b)reward computationmodule, which is comprised of(b.1)correctness rewardℛCorrect\\mathcal{R}^{\\textit{Correct}}derived from the correctness of format and outcome,(b.2)boundary-aware rewardℛIDK\\mathcal{R}^{\\textit{IDK}}designed to incentivizeIDKresponses when no correct rollout exists within the group, and(b.3) adaptive reward modulatoradaptively disablingℛIDK\\mathcal{R}^{\\textit{IDK}}based onIDKratio at the exploration stage and diversity of rollouts at the plateau stage.",
                "position": 352
            }
        ]
    },
    {
        "header": "4The Framework of BAPO",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11037/reward_trends_14b.png",
                "caption": "Figure 5:Upper:Dynamics of correctness rewardℛCorrect\\mathcal{R}^{\\textit{Correct}}and boundary-aware rewardℛIDK\\mathcal{R}^{\\textit{IDK}}.Bottom:Dynamics ofIDKratioρIDK\\rho_{\\textit{IDK}}during BAPO training on Qwen2.5-14B-Instruct.",
                "position": 908
            },
            {
                "img": "https://arxiv.org/html/2601.11037/reward_trends_14b.png",
                "caption": "",
                "position": 911
            },
            {
                "img": "https://arxiv.org/html/2601.11037/idk_rate_trends_14b.png",
                "caption": "",
                "position": 916
            },
            {
                "img": "https://arxiv.org/html/2601.11037/rejection_success.png",
                "caption": "Figure 6:Rejection success rates calculated on Qwen2.5-Instruct series models.",
                "position": 938
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11037/accuracy_vs_passk.png",
                "caption": "Figure 7:Accuracy of the Pass@KKon Qwen2.5-7B-Instruct for different values of K. The accuracy improves with increasingKKand stabilizes afterK=32K=32.",
                "position": 1822
            },
            {
                "img": "https://arxiv.org/html/2601.11037/expmonitor.png",
                "caption": "Figure 8:Confident and uncertain expressions.",
                "position": 2360
            }
        ]
    },
    {
        "header": "Appendix CPrompt Template",
        "images": []
    },
    {
        "header": "Appendix DDetailed Related Work",
        "images": []
    },
    {
        "header": "Appendix EThe Use of Large Language Models",
        "images": []
    }
]