[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02160/x1.png",
                "caption": "",
                "position": 102
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02160/extracted/6331841/figs/intro_motivation.jpg",
                "caption": "Figure 2:The illustration of our motivation. We propose a novelmodel-data co-evolutionparadigm, where less-controllable preceding models systematically synthesize better customization data for successive more-controllable variants, enabling persistent co-evolution between enhanced model and enriched data.",
                "position": 129
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02160/x2.png",
                "caption": "Figure 3:Illustration of our proposed synthetic data curation framework based on in-context data generation.",
                "position": 185
            },
            {
                "img": "https://arxiv.org/html/2504.02160/x3.png",
                "caption": "Figure 4:Illustration of the training framework ofUNO. It introduces two pivotal enhancements to the model: progressive cross-modal alignment and universal rotary position embedding(UnoPE). The progressive cross-modal alignment is divided into two stages. In the StageII\\mathrm{I}roman_I, we use single-subject in-context generated data to finetune the pretrained T2I model into an S2I model. In the StageIIII\\mathrm{II}roman_II, we continue training on generated multiple-subject data pairs. The UnoPE can effectively equip UNO with the capability of mitigating the attribute confusion issue when scaling visual subject controls.",
                "position": 188
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02160/x4.png",
                "caption": "Figure 5:Model performance on Dreambench[33]. We conduct experiments under different quality score levels.",
                "position": 229
            },
            {
                "img": "https://arxiv.org/html/2504.02160/x5.png",
                "caption": "Figure 6:Qualitative comparison with different methods on single-subject driven generation.",
                "position": 235
            },
            {
                "img": "https://arxiv.org/html/2504.02160/x6.png",
                "caption": "Figure 7:Qualitative comparison with different methods on multi-subject driven generation.",
                "position": 272
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02160/x7.png",
                "caption": "Figure 8:Radar charts of user evaluation of methods for single-subject driven and multi-subject driven generation on different dimensions",
                "position": 567
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02160/x8.png",
                "caption": "Figure 9:Ablation study ofUNO. Zoom in for details.",
                "position": 768
            }
        ]
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Less-to-More Generalization:Unlocking More Controllability by In-Context Generation",
        "images": []
    },
    {
        "header": "Supplementary Material",
        "images": []
    },
    {
        "header": "FIn-Context Data Generation Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02160/x9.png",
                "caption": "Figure 10:Illustration of the taxonomy tree.",
                "position": 1422
            },
            {
                "img": "https://arxiv.org/html/2504.02160/x10.png",
                "caption": "Figure 11:Diptych text template for generating subject-consistent image-pair with FLUX.1[17].",
                "position": 1425
            },
            {
                "img": "https://arxiv.org/html/2504.02160/x11.png",
                "caption": "(a)System prompt of LLM used to generate subject instances in creative type.",
                "position": 1444
            },
            {
                "img": "https://arxiv.org/html/2504.02160/x11.png",
                "caption": "(a)System prompt of LLM used to generate subject instances in creative type.",
                "position": 1447
            },
            {
                "img": "https://arxiv.org/html/2504.02160/x12.png",
                "caption": "(b)System prompt of LLM used to generate subject instances in realistic type.",
                "position": 1452
            },
            {
                "img": "https://arxiv.org/html/2504.02160/x13.png",
                "caption": "(c)System prompt of LLM used to generate subject instances in text-decorated type.",
                "position": 1457
            },
            {
                "img": "https://arxiv.org/html/2504.02160/x14.png",
                "caption": "Figure 13:System prompt of LLM used to generate scene descriptions.",
                "position": 1464
            },
            {
                "img": "https://arxiv.org/html/2504.02160/x15.png",
                "caption": "(a)System prompt of the filter VLM.",
                "position": 1475
            },
            {
                "img": "https://arxiv.org/html/2504.02160/x15.png",
                "caption": "(a)System prompt of the filter VLM.",
                "position": 1478
            },
            {
                "img": "https://arxiv.org/html/2504.02160/x16.png",
                "caption": "(b)Prompt for the first round CoT of the filter VLM.",
                "position": 1483
            },
            {
                "img": "https://arxiv.org/html/2504.02160/x17.png",
                "caption": "(c)Prompt for the second round CoT of the filter VLM.",
                "position": 1489
            },
            {
                "img": "https://arxiv.org/html/2504.02160/x18.png",
                "caption": "(d)Prompt for the third round CoT of the filter VLM.",
                "position": 1494
            },
            {
                "img": "https://arxiv.org/html/2504.02160/x19.png",
                "caption": "Figure 15:Sampled data from different VLM score intervals.",
                "position": 1501
            },
            {
                "img": "https://arxiv.org/html/2504.02160/x20.png",
                "caption": "Figure 16:Sampled data from our final multi-subject in-context data.",
                "position": 1504
            },
            {
                "img": "https://arxiv.org/html/2504.02160/x21.png",
                "caption": "Figure 17:Amount of data in each VLM score interval.",
                "position": 1507
            }
        ]
    },
    {
        "header": "GAnalysis on LoRA Rank",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02160/x22.png",
                "caption": "Figure 18:Analysis of model performance under different LoRA ranks.",
                "position": 1518
            }
        ]
    },
    {
        "header": "HMore Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02160/x23.png",
                "caption": "Figure 19:More comparison with different methods on multi-subject driven generation. Weitalicizethe subject-related editing part of the prompts.",
                "position": 1541
            },
            {
                "img": "https://arxiv.org/html/2504.02160/x24.png",
                "caption": "Figure 20:More multi-subject generation results from our UNO model.",
                "position": 1544
            },
            {
                "img": "https://arxiv.org/html/2504.02160/x25.png",
                "caption": "Figure 21:More virtual try-on results from our UNO model.",
                "position": 1547
            },
            {
                "img": "https://arxiv.org/html/2504.02160/x26.png",
                "caption": "Figure 22:More identity preservation results from our UNO model.",
                "position": 1550
            },
            {
                "img": "https://arxiv.org/html/2504.02160/x27.png",
                "caption": "Figure 23:More stylized generation results from our UNO model.",
                "position": 1553
            }
        ]
    },
    {
        "header": "ILimitation and Discussion",
        "images": []
    }
]