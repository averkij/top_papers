[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18052/x1.png",
                "caption": "Figure 1:Concept unlearning in SAeUron.We localize and remove SAE features corresponding to the unwanted concept (Cartoon) while preserving the overall performance of the diffusion model.",
                "position": 114
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Sparse Autoencoders for Diffusion Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18052/extracted/6169300/figures/schema_method_nums_new.jpg",
                "caption": "Figure 2:Unlearning procedure in SAeUron.(a) Concept-specific features are selected for unlearning according to their importance scores. (b) During inference in the U-Net of the diffusion model, activation between selected cross-attention blocks is passed through a trained SAE. The selected SAE features are then ablated by scaling them with a negative multiplierŒ≥csubscriptùõæùëê\\gamma_{c}italic_Œ≥ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT, removing their influence on the final output. The remaining features are left unchanged, ensuring minimal impact on the overall model performance.",
                "position": 258
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18052/x2.png",
                "caption": "Figure 3:Feature importance scores.Most of the features have near-zero scores, indicating that SAE learns only a few concept-specific features. During the evaluation, we find a threshold based on percentile of scores and block features exceeding it.",
                "position": 319
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18052/x3.png",
                "caption": "Figure 4:Object classification with k-nearest neighbors algorithm based on SAE feature activations.Features selected with our score-based selection approach demonstrate strong discriminative power across timesteps. Even randomly selected features exhibit notably higher accuracy than random guess baseline, proving that SAE learns meaningful visual attributes.",
                "position": 429
            },
            {
                "img": "https://arxiv.org/html/2501.18052/extracted/6169300/figures/style_object_features_2.jpg",
                "caption": "Figure 5:Activations of features selected for unlearning displayed on image patches.(Left) Features corresponding to theBricksstyle strongly activate on patterns characteristic of this style. (Right) Conversely,Butterfly-related features activate successfully on image regions containing the object, regardless of the style.",
                "position": 433
            },
            {
                "img": "https://arxiv.org/html/2501.18052/x4.png",
                "caption": "Figure 6:Evaluation on sequential unlearning of multiple concepts.Results present the average of unlearning accuracy (UA) and retaining accuracy (RA=IRA+CRA2RAIRACRA2\\text{RA}=\\frac{\\text{IRA}+\\text{CRA}}{2}RA = divide start_ARG IRA + CRA end_ARG start_ARG 2 end_ARG). SAeUron achieves superior unlearning effectiveness while retaining the overall model‚Äôs performance. At the same time, we observe a significant drop in retaining the ability of competing approaches.",
                "position": 659
            },
            {
                "img": "https://arxiv.org/html/2501.18052/extracted/6169300/figures/unlearning_vis_new.jpg",
                "caption": "Figure 7:Qualitative evaluation of SAeUron on object and style unlearning.Removing objects and styles effectively preserves other elements of the test prompt while having minimal impact on the generation quality of remaining concepts.",
                "position": 664
            }
        ]
    },
    {
        "header": "6Additional experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18052/x5.png",
                "caption": "Figure 8:Robustness to adversarial prompts crafted using UnlearnDiffAtk method.Our blocking approach demonstrates strong robustness to adversarial prompts, as evidenced by a minimal drop in unlearning accuracy under attack scenarios. In contrast, competing methods exhibit significant vulnerability.",
                "position": 693
            }
        ]
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "8Conclusions",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ABatchTopK SAEs trained for diffusion models",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18052/x6.png",
                "caption": "Figure 9:Number of active features per image sample.We see that SAEs assigns unequal number of active features per sample, signifying that some samples are more important than the others for SAE to obtain good reconstruction error.",
                "position": 1474
            },
            {
                "img": "https://arxiv.org/html/2501.18052/x7.png",
                "caption": "Figure 10:Average number of active features corresponding to image patches.Interestingly we observe that BatchTopK SAEs trained on activations from diffusion model allocate more active features to reconstruct activation vectors corresponding to central image regions.",
                "position": 1479
            }
        ]
    },
    {
        "header": "Appendix BPrompts from a validation set for feature score calculation",
        "images": []
    },
    {
        "header": "Appendix CSelection of cross-attention blocks to apply SAE",
        "images": []
    },
    {
        "header": "Appendix DDetails of sequential unlearning evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18052/x8.png",
                "caption": "Figure 13:Evaluation on unlearning multiple concepts.SAeUron achieves high unlearning accuracy and outperforms other approaches in retaining generative capabilities for non-targeted concepts.",
                "position": 1682
            }
        ]
    },
    {
        "header": "Appendix ESAE trainings details",
        "images": []
    },
    {
        "header": "Appendix FSAE generalization abilities",
        "images": []
    },
    {
        "header": "Appendix GHyperparameters for object unlearning",
        "images": []
    },
    {
        "header": "Appendix HActivation steering on style features",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18052/extracted/6169300/figures/empty_prompt_steering_images/Picasso_ref.jpg",
                "caption": "Figure 16:Steering on unconditional generations with features selected using score-based method.Features associated with specific styles effectively produce generations that visibly reflect those styles.",
                "position": 1946
            },
            {
                "img": "https://arxiv.org/html/2501.18052/extracted/6169300/figures/empty_prompt_steering_images/without_steering.jpg",
                "caption": "",
                "position": 1995
            },
            {
                "img": "https://arxiv.org/html/2501.18052/extracted/6169300/figures/empty_prompt_steering_images/Picasso.jpg",
                "caption": "",
                "position": 2001
            },
            {
                "img": "https://arxiv.org/html/2501.18052/extracted/6169300/figures/empty_prompt_steering_images/Abstractionism_ref.jpg",
                "caption": "",
                "position": 2018
            },
            {
                "img": "https://arxiv.org/html/2501.18052/extracted/6169300/figures/empty_prompt_steering_images/Abstractionism.jpg",
                "caption": "",
                "position": 2030
            },
            {
                "img": "https://arxiv.org/html/2501.18052/extracted/6169300/figures/empty_prompt_steering_images/Superstring_ref.jpg",
                "caption": "",
                "position": 2047
            },
            {
                "img": "https://arxiv.org/html/2501.18052/extracted/6169300/figures/empty_prompt_steering_images/Superstring.jpg",
                "caption": "",
                "position": 2059
            },
            {
                "img": "https://arxiv.org/html/2501.18052/extracted/6169300/figures/empty_prompt_steering_images/Pastel_ref.jpg",
                "caption": "",
                "position": 2076
            },
            {
                "img": "https://arxiv.org/html/2501.18052/extracted/6169300/figures/empty_prompt_steering_images/Pastel.jpg",
                "caption": "",
                "position": 2088
            },
            {
                "img": "https://arxiv.org/html/2501.18052/extracted/6169300/figures/empty_prompt_steering_images/Color_Fantasy_ref.jpg",
                "caption": "",
                "position": 2107
            },
            {
                "img": "https://arxiv.org/html/2501.18052/extracted/6169300/figures/empty_prompt_steering_images/Color_Fantasy.jpg",
                "caption": "",
                "position": 2119
            }
        ]
    },
    {
        "header": "Appendix INumber of score-based selected features for unlearning",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18052/x9.png",
                "caption": "Figure 17:Number of selected features for object unlearning.",
                "position": 2132
            }
        ]
    },
    {
        "header": "Appendix JK-nearest neighbors classification for style features",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18052/x10.png",
                "caption": "Figure 18:Style classification with k-nearest neighbors algorithm based on SAE feature activations.",
                "position": 2145
            }
        ]
    },
    {
        "header": "Appendix KDistribution of feature importance scores across timesteps",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18052/x11.png",
                "caption": "Figure 19:Percentiles of score distribution across denoising timesteps.",
                "position": 2155
            }
        ]
    },
    {
        "header": "Appendix LUnlearnDiffAtk evaluation of object unlearning",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18052/x12.png",
                "caption": "Figure 20:Robustness to adversarial prompts crafted using UnlearnDiffAtk method on object unlearning.",
                "position": 2171
            },
            {
                "img": "https://arxiv.org/html/2501.18052/extracted/6169300/figures/diffatk_object/orig_architectures.png",
                "caption": "Figure 21:Effect of UnlearnDiffAtk on object unlearning with our approach.The left column shows images generated with SAeUron, where the object should be unlearned, while the right column presents results after the adversarial attack. Despite the attack‚Äôs success, the targeted object remains absent from the image.",
                "position": 2175
            },
            {
                "img": "https://arxiv.org/html/2501.18052/extracted/6169300/figures/diffatk_object/success_architectures.png",
                "caption": "",
                "position": 2216
            },
            {
                "img": "https://arxiv.org/html/2501.18052/extracted/6169300/figures/diffatk_object/orig_flame.png",
                "caption": "",
                "position": 2233
            },
            {
                "img": "https://arxiv.org/html/2501.18052/extracted/6169300/figures/diffatk_object/success_flame.png",
                "caption": "",
                "position": 2239
            },
            {
                "img": "https://arxiv.org/html/2501.18052/extracted/6169300/figures/diffatk_object/orig_trees.png",
                "caption": "",
                "position": 2256
            },
            {
                "img": "https://arxiv.org/html/2501.18052/extracted/6169300/figures/diffatk_object/success_trees.png",
                "caption": "",
                "position": 2262
            },
            {
                "img": "https://arxiv.org/html/2501.18052/extracted/6169300/figures/diffatk_object/orig_statues.png",
                "caption": "",
                "position": 2279
            },
            {
                "img": "https://arxiv.org/html/2501.18052/extracted/6169300/figures/diffatk_object/success_statues.png",
                "caption": "",
                "position": 2285
            }
        ]
    },
    {
        "header": "Appendix MPseudocode of SAeUron",
        "images": []
    },
    {
        "header": "Appendix NAuto-interpreting features selected for unlearning",
        "images": []
    }
]