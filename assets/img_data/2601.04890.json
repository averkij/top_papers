[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04890/figs/tii_logo.png",
                "caption": "",
                "position": 107
            },
            {
                "img": "https://arxiv.org/html/2601.04890/figs/hf_logo.png",
                "caption": "",
                "position": 124
            }
        ]
    },
    {
        "header": "1.‚ÄÉ‚ÄäIntroduction",
        "images": []
    },
    {
        "header": "2.‚ÄÉ‚ÄäLearnable Multipliers",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04890/x1.png",
                "caption": "Figure 1:Projector and MLP scale (1) sweep experiments described in section3. Norm scalesS‚Äã(Œ∑P,ŒªP)S(\\eta_{P},\\lambda_{P})andS‚Äã(Œ∑M‚ÄãL‚ÄãP,ŒªM‚ÄãL‚ÄãP)S(\\eta_{MLP},\\lambda_{MLP})use relative values of learning rate and weight decay.(Top left):the final loss of three projector norm configurations.(Top middle and right):trajectories of projector‚ÄñW‚Äñ\\|W\\|and multiplier‚Äñùêú‚Äñ\\|\\mathbf{c}\\|norms during training are compared between the experiment and Adam Brownian Motion simulation.(Bottom left and middle):logits and matrix/multipliers norms for the considered configuration.(Bottom right):the final loss of three MLP experiment configurations.",
                "position": 271
            }
        ]
    },
    {
        "header": "3.‚ÄÉ‚ÄäWhat is learned by the multipliers?",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04890/x2.png",
                "caption": "Figure 2:(Left):a ratio of residual block output norms of the configuration with scalar multipliers to the configuration without.(Middle):values of scalar multipliers at the end of each block.(Right):values of internal scalar multipliers at selected locations of each block.",
                "position": 349
            },
            {
                "img": "https://arxiv.org/html/2601.04890/x3.png",
                "caption": "Figure 3:Distributions of row norms‚ÄñWi‚Å£‚àô‚Äñ\\|W_{i\\,\\bullet}\\|of attention/SSM input and MLP gate projections, which correspond to internal features of these blocks. We collect the norms across all the model layers while normalizing norm values of each layer by their mean to align the scale of different layers and focus on within-layer distribution.",
                "position": 361
            }
        ]
    },
    {
        "header": "4.‚ÄÉ‚ÄäAspects of multiplier training dynamics",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04890/x4.png",
                "caption": "(a)",
                "position": 393
            },
            {
                "img": "https://arxiv.org/html/2601.04890/x4.png",
                "caption": "(a)",
                "position": 396
            },
            {
                "img": "https://arxiv.org/html/2601.04890/x5.png",
                "caption": "(b)",
                "position": 401
            },
            {
                "img": "https://arxiv.org/html/2601.04890/x6.png",
                "caption": "Figure 5:The width scaling of the norms of linear layers matrices, various activations throughout the model, and scalar multipliers attached to the selected model activations. We use geometric average to aggregate the values across the layers. Other experimental details can be found in section4, and we provide time evolution of selected output norms on figure10.",
                "position": 491
            },
            {
                "img": "https://arxiv.org/html/2601.04890/x7.png",
                "caption": "Figure 6:Training dynamics when excluding (blue) vs. including (orange) multiplier gradients from the global clip norm: (Left) initial loss, (Middle) initial gradient norm, and (Right) the long-term loss difference.",
                "position": 505
            }
        ]
    },
    {
        "header": "5.‚ÄÉ‚ÄäResults",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04890/x8.png",
                "caption": "Figure 7:(Left): Loss values after LR decay for 4 multiplier tuning configurations. Each configuration is trained both with standard parametrization and with learnable vector multipliers, marked with+LRM.(Right): loss difference between the runs with and without learnable multipliers.",
                "position": 556
            }
        ]
    },
    {
        "header": "6.‚ÄÉ‚ÄäConclusion and discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperiment settings",
        "images": []
    },
    {
        "header": "Appendix BMLP experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04890/x9.png",
                "caption": "Figure 8:All the subplots, except bottom right, show behavior of different norms as the MLP norm scaleS‚Äã(Œ∑MLP,ŒªMLP)S(\\eta_{\\mathrm{MLP}},\\lambda_{\\mathrm{MLP}})is varied. The rows contain similar types of norms and share y-axis scale for easier comparison. The bottom right subplot duplicates the bottom right subplot of figure1; we add it for convenience as an illustration of performance across the four considered configurations.(Top row):The norms of the output of MLP, attention and SSM residual blocks, as well as the norm of the outputs of embedding layer. The norms are averaged across the tokens in a batch. Then, we apply geometric average to aggregate residual block norms across model layers.(Middle row):The norms of matrix weights:Wup,Wgate,WdownW^{\\mathrm{up}},W^{\\mathrm{gate}},W^{\\mathrm{down}}for MLP (see (6)),WQ‚ÄãK‚ÄãVW^{QKV}111Equal contribution.andWoutW^{\\mathrm{out}}for attention (see (7)),WX‚ÄãZ‚ÄãB‚ÄãC‚Äãd‚ÄãtW^{XZBCdt}andWoutW^{\\mathrm{out}}for SSM (see (8),(9),(10),(11),(12)), and the embeddings matrix. To aggregate the weight norms into a single metric for residual blocks, in addition to geometric averaging across the layers we further apply geometric averaging across the matrix types within the block, for exampleWup,Wgate,WdownW^{\\mathrm{up}},W^{\\mathrm{gate}},W^{\\mathrm{down}}for MLP.(Bottom row):The norms of various non-matrix parameters that are free from the noise-WD equilibrium and can adjust their scale. For the three MLP multipliers we again apply geometric average; Conv1d acts as a row multiplier forWX‚ÄãB‚ÄãCW^{XBC}in the SSM block (see (8),(9));DDparameter scales the skip connection in the recurrent SSM computation, such that larger values ofDDmake SSM block computation closer to gated MLP.",
                "position": 1281
            },
            {
                "img": "https://arxiv.org/html/2601.04890/x10.png",
                "caption": "Figure 9:The behavior the norms of the MLP, attention and SSM residual blocks outputs across the model layers. This figure complements figure1, where the norms were averaged across the layers. Overall, we observe strong layer-specific patterns, suggesting that the layer-averaged metrics in figure1adequately capture the behavior of the norms with respect to MLP scaleS‚Äã(Œ∑M‚ÄãL‚ÄãP,ŒªM‚ÄãL‚ÄãP)S(\\eta_{MLP},\\lambda_{MLP}).",
                "position": 1363
            }
        ]
    },
    {
        "header": "Appendix CMultipliers placement",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04890/x11.png",
                "caption": "Figure 10:Evolution of signal norms during training across different model widths (W‚Äã128W128toW‚Äã4096W4096).(Center)MLP and Attention output norms show consistent norm growth in the later stages of training, which we attribute to drift along the direction of residual normalization symmetry.(Left)Embedding output norms also grow to match residual growth.(Right)Projector output norms also grow with training time but seem to plateau at the same level, presumably corresponding to the reasonable logits scale. The model width affect the projector output norm only in the intermediate stage of training, while for MLP and attention outputs norm are grow with different offset for different widths, reflecting arbitrary scale of residuals due to normalization symmetry.",
                "position": 1551
            },
            {
                "img": "https://arxiv.org/html/2601.04890/x12.png",
                "caption": "Figure 11:The detailed evaluation curves for the long runs reported in table2. The markers correspond to actual evaluation score at a given checkpoint while solid lines denote running window average score over 20 last checkpoints. We evaluated checkpoints every gigatoken to carefully average the benchmarks stochasticity, and started this frequent evaluation only from 100GT due to compute constraints. We perform√ó32\\times 32exponential decay from 160GT to 200GT, which explains the growth of the scores in this time window is thanks to the learning rate decay. After the end of exponential decay, the model was trained for 40 more gigatokens with minimal learning rate to obtain enough evaluation points ensuring well averaged scores reported in table2.",
                "position": 1557
            }
        ]
    },
    {
        "header": "Appendix DAdditional plots",
        "images": []
    }
]