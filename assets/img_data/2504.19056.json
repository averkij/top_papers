[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.19056/extracted/6392204/resources/images/overview.png",
                "caption": "Figure 1:Overview of different components in animated character generation. Each aspect, including face, expression, image, avatar, gesture, motion, object, and texture, enhances realism and expressiveness within digital animation environments. Generative AI techniques, such as transformer-based and diffusion-based models, contribute to these components by improving quality, streamlining content creation, and enabling more sophisticated character animation.\nGenerative AI techniques, such as transformer-based and diffusion-based models, contribute to these components, significantly enhancing quality and streamlining content creation.",
                "position": 389
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Face",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.19056/extracted/6392204/resources/images/DG-face.png",
                "caption": "Figure 3:Overview of the Dual-Generator (DG)[23]network, which consists of two generators: the ID-preserving Shape Generator (IDSG) and the Reenacted Face Generator (RFG).\nGiven a source faceIssubscriptùêºùë†I_{s}italic_I start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPTand a reference faceIrsubscriptùêºùëüI_{r}italic_I start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT, the IDSG transforms the reference‚Äôs actions into landmarksl^tsubscript^ùëôùë°\\hat{l}_{t}over^ start_ARG italic_l end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT.\nUsing these landmarks andIssubscriptùêºùë†I_{s}italic_I start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT, the RFG produces a reenacted faceI^tsubscript^ùêºùë°\\hat{I}_{t}over^ start_ARG italic_I end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTthat matches the pose and expression ofIrsubscriptùêºùëüI_{r}italic_I start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPTwhile preserving the identity ofIssubscriptùêºùë†I_{s}italic_I start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT. Reprinted from[23].",
                "position": 1055
            },
            {
                "img": "https://arxiv.org/html/2504.19056/extracted/6392204/resources/images/In-ViTe.png",
                "caption": "Figure 4:Overview of InViTe[187]in three stages:\n(a) capturing a user‚Äôs face to produce a personalized 3D model,\n(b) generating intermediate outputs for rendering,\nand (c) performing 3D face manipulation (such as makeup style changes) on a mobile device.\nReprinted from[187].",
                "position": 1091
            }
        ]
    },
    {
        "header": "4Expression",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.19056/extracted/6392204/resources/images/adamesh_exp_adapter.png",
                "caption": "Figure 5:Overview of AdaMesh[44]model: (a) The expression adapter integrates MoLoRA[206]parameters (striped patches) into pre-trained encoders and the decoder to enable efficient adaptation for facial expressions. (b) Architecture of the Conformer block[207], showcasing its 1D convolution module and multi-head attention. (c) Illustration of MoLoRA applied to the convolution operator, with low-rank decomposition enhancing adaptation efficiency. MoLoRA is also applied to linear layers in the expression adapter.\nReprinted from[44].",
                "position": 1442
            },
            {
                "img": "https://arxiv.org/html/2504.19056/extracted/6392204/resources/images/nfr.png",
                "caption": "Figure 6:Overview of Neural Face Rigging (NFR)[51]. The model extracts an expression codezesubscriptùëßùëíz_{e}italic_z start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPTfrom an unrigged mesh with an unknown expression (yellow) and an identity codezisubscriptùëßùëñz_{i}italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTfrom a target neutral mesh (cyan). The extracted codes are combined to generate a retargeted mesh (blue) with transferred expressions and identity-preserving deformations. A part ofzesubscriptùëßùëíz_{e}italic_z start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT, denotedzFACSsubscriptùëßFACSz_{\\text{FACS}}italic_z start_POSTSUBSCRIPT FACS end_POSTSUBSCRIPT, enables interpretable rigging parameters, making NFR an artist-friendly tool for automatic rigging and expression retargeting.\nReprinted from[51].",
                "position": 1480
            }
        ]
    },
    {
        "header": "5Image",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.19056/x1.png",
                "caption": "Figure 7:The Cut-Mix-Unmix data augmentation technique for multi-subject generation, implemented in SVDiff[60]. The Cut-Mix-Unmix method is a data augmentation technique designed to train image generation models to handle multiple subjects. By creating composite images and corresponding textual prompts, the method teaches the model to distinguish between different concepts. Through Unmix regularization and applying MSE to the attention maps, the model is encouraged to separate better the subjects (e.g., a dog and a panda).\nReprinted from[60].",
                "position": 1723
            },
            {
                "img": "https://arxiv.org/html/2504.19056/x2.png",
                "caption": "Figure 8:Examples of image editing using the DiffusionDisentanglement model[61]. Each row displays a text description combining style-neutral content and target attribute descriptions (separated by commas). For each attribute section, the first row shows results on optimization images, while the second row demonstrates transfer to unseen images. The left column contains source images, and the right column shows the corresponding modified images.\nReprinted from[61].",
                "position": 1758
            },
            {
                "img": "https://arxiv.org/html/2504.19056/x3.png",
                "caption": "Figure 9:Visual ChatGPT architecture[67]showing the workflow from user query to output. The system processes complex instructions on a flower image through a prompt manager that coordinates visual foundation models (BLIP, Stable Diffusion, ControlNet, etc.). The example demonstrates iterative reasoning through depth estimation and style transfer to transform a yellow flower into a red cartoon version. Reprinted from[67].",
                "position": 1780
            }
        ]
    },
    {
        "header": "6Avatar",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.19056/extracted/6392204/resources/images/dreamavatar.png",
                "caption": "Figure 10:DreamAvatar‚Äôs dual-observation-space architecture shows how text prompts and SMPL parameters drive avatar generation through canonical and posed spaces. The system uses an SMPL-based deformation field (left), shared NeRF module (center), and diffusion model guidance with landmark ControlNet for head refinement (right) to produce high-quality results with consistent facial features.\nReprinted from[88].",
                "position": 2037
            },
            {
                "img": "https://arxiv.org/html/2504.19056/extracted/6392204/resources/images/dreamhuman.png",
                "caption": "Figure 11:Pipeline of DreamHuman[84]: the model takes a text promptpùëùpitalic_p(e.g.,a woman wearing a dress) and generates a 3D animatable avatar using a deformable, pose-conditioned NeRF constrained by the imGHUM[252]body model. The pipeline incorporates semantic zooming for critical regions (face, hands) to improve detail and employs Score Distillation Sampling (SDS)[130]for optimization, producing high-fidelity avatars with pose control.\nReprinted from[84].",
                "position": 2111
            }
        ]
    },
    {
        "header": "7Gesture",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.19056/extracted/6392204/resources/images/gesture.png",
                "caption": "Figure 12:GestureDiffuCLIP[106]integrates a CLIP encoder for semantic alignment and a diffusion process for refining gesture sequences. The model uses multi-head causal and semantic-aware attention mechanisms and adaptive instance normalization (AdaIN) to generate expressive gestures.\nReprinted from[106].",
                "position": 2481
            },
            {
                "img": "https://arxiv.org/html/2504.19056/x4.png",
                "caption": "Figure 13:The architecture ofDiffuseStyleGesture+[111], showcasing its multimodal integration for co-speech gesture generation. The model incorporates speaker IDs, seed gestures, audio, and text through feature extraction and representation modules. A diffusion process is employed to refine noisy gestures into expressive outputs, leveraging self-attention and cross-local attention mechanisms. The system also uses Huber loss[289]for enhanced denoising performance and produces stylistically diverse gesture outputs. Reprinted from[111].",
                "position": 2523
            }
        ]
    },
    {
        "header": "8Motion",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.19056/extracted/6392204/resources/images/tmr.png",
                "caption": "Figure 14:The architecture of the TMR[120]framework shows the use of dual encoders for text and motion, and a joint embedding space for similarity-based retrieval. Reprinted from[120].",
                "position": 2818
            },
            {
                "img": "https://arxiv.org/html/2504.19056/extracted/6392204/resources/images/motiongpt.png",
                "caption": "Figure 15:An overview of MotionGPT[21], which uses a frozen VQ-VAE and LLM, with LoRA[320]applied to fine-tune the LLM for generating motion tokens. Reprinted from[21].",
                "position": 2836
            }
        ]
    },
    {
        "header": "9Object",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.19056/extracted/6392204/resources/images/DreamFusion.png",
                "caption": "Figure 16:DreamFusion[130]generates 3D objects from text prompts like‚Äúa DSLR photo of a peacock on a surfboard.\"It trains a Neural Radiance Field (NeRF) from scratch for each caption, using shading from normals and a frozen Imagen model[57]to guide updates for improved geometry and appearance. Reprinted from[130].",
                "position": 3264
            }
        ]
    },
    {
        "header": "10Texture",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.19056/extracted/6392204/resources/images/Text2tex.png",
                "caption": "Figure 17:Pipeline of Text2Tex[145], featuring a two-stage process for generating high-quality, multi-view consistent textures. Stage I generates textures via depth-to-image diffusion from predefined viewpoints. Stage II refines the results by automatically selecting additional views to correct distortions and artifacts. This approach balances realism, consistency, and efficiency. Reprinted from[145].",
                "position": 3594
            },
            {
                "img": "https://arxiv.org/html/2504.19056/extracted/6392204/resources/images/Meta-3D-TextureGen.png",
                "caption": "Figure 18:Meta 3D TextureGen[153]employs a two-stage architecture: a geometry-aware diffusion process generates multi-view images, followed by UV-space inpainting using incidence-aware blending. This design enables seamless, high-resolution (up to 4K) textures with minimal artifacts. Reprinted from[153].",
                "position": 3628
            }
        ]
    },
    {
        "header": "11Open Problems & Research Directions",
        "images": []
    },
    {
        "header": "12Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.19056/extracted/6392204/resources/images/SMPL.png",
                "caption": "Figure 19:Comparison of SMPL[1], SMPL+H[424], and SMPL-X[305]models: SMPL captures basic body shapes, SMPL+H adds detailed hand poses, and SMPL-X includes body, hands, and facial expressions for a more complete and realistic representation. Reprinted from[305].",
                "position": 7425
            },
            {
                "img": "https://arxiv.org/html/2504.19056/extracted/6392204/resources/images/nerf.png",
                "caption": "Figure 20:An overview of volumetric rendering in NeRF. Reprinted from[9].",
                "position": 7625
            },
            {
                "img": "https://arxiv.org/html/2504.19056/extracted/6392204/resources/images/controlnet.png",
                "caption": "Figure 21:Before ControlNet (left): A neural network block processes inputxùë•xitalic_xto outputyùë¶yitalic_y.\nAfter ControlNet (right): The original block is locked asycsubscriptùë¶ùëêy_{c}italic_y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT, while a trainable copy integrates external inputcùëêcitalic_cvia zero convolutions, enhancing functionality. Reprinted from[8].",
                "position": 7773
            }
        ]
    },
    {
        "header": "Appendix ABackground",
        "images": []
    }
]