[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.03734/extracted/6269393/figures/contrast_results_v2.png",
                "caption": "Figure 1:(Top)Different feature extraction approaches in VLA models. (a) Direct Feature Passing: existing approaches, exemplified by Octo and OpenVLA, extract and pass visual and text tokens independently to the policy network. (b) Text-Aware Feature Extraction: the proposed approach, OTTER, extracts visual tokens that correspond to the text tokens, and then feeds them into the policy.(Bottom)Real-world Robot Experiments: OTTER demonstrates higher success rates on both training and unseen real-world robot pick-and-place tasks compared to Octo and OpenVLA. OTTER exhibits better zero-shot generalization to unseen objects, maintaining strong performance across a variety of novel tasks.",
                "position": 119
            },
            {
                "img": "https://arxiv.org/html/2503.03734/extracted/6269393/figures/model_arc_vertical.png",
                "caption": "Figure 2:OTTER Model architecture.At each timesteptğ‘¡titalic_t, text-aware visual featuresfvâ¢lsubscriptğ‘“ğ‘£ğ‘™f_{vl}italic_f start_POSTSUBSCRIPT italic_v italic_l end_POSTSUBSCRIPTare extracted from a pre-trained CLIP model (seeFigure3). Thenfvâ¢lsubscriptğ‘“ğ‘£ğ‘™f_{vl}italic_f start_POSTSUBSCRIPT italic_v italic_l end_POSTSUBSCRIPTand the text tokensflsubscriptğ‘“ğ‘™f_{l}italic_f start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPTare further compressed through separate attention pooling layers, producing two representationsfvâ¢lâ€²subscriptsuperscriptğ‘“â€²ğ‘£ğ‘™f^{\\prime}_{vl}italic_f start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v italic_l end_POSTSUBSCRIPTandflâ€²subscriptsuperscriptğ‘“â€²ğ‘™f^{\\prime}_{l}italic_f start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT, respectively. The robotâ€™s proprioception is encoded by an MLP layer to generate the embodiment representationfesubscriptğ‘“ğ‘’f_{e}italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT. The tokensflâ€²subscriptsuperscriptğ‘“â€²ğ‘™f^{\\prime}_{l}italic_f start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT,fvâ¢lâ€²subscriptsuperscriptğ‘“â€²ğ‘£ğ‘™f^{\\prime}_{vl}italic_f start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v italic_l end_POSTSUBSCRIPT, andfesubscriptğ‘“ğ‘’f_{e}italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPTare then concatenated to formftsubscriptğ‘“ğ‘¡f_{t}italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, which serves as input to a causal transformer. With a context window ofTğ‘‡Titalic_Tsteps, the model autoregressively predicts the future actions (atsubscriptğ‘ğ‘¡a_{t}italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT) at each step.",
                "position": 129
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.03734/x1.png",
                "caption": "Figure 3:Text-aware Visual Features ExtractionWe calculate the similarity between the visual patch features and per-token language features, then take the softmax over the patch feature dimension. Intuitively, this gives a distribution of semantic similarity over all spatial locations. We then multiply the visual patch features to retrieve the visual semantic features that correspond to each token in the sentence. InAppendixC, we provide more in-depth analysis and visualizations of the proposed method.",
                "position": 242
            },
            {
                "img": "https://arxiv.org/html/2503.03734/x2.png",
                "caption": "Figure 4:Example scenes in the simulation (left) and in the physical environments (right) using a Franka robot.",
                "position": 293
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.03734/x3.png",
                "caption": "Figure 5:We evaluate OTTERâ€™s performance with improved vision language features by scaling CLIP. In particular, we train OTTER with three CLIP variants with increasing FLOPs: ViT-B/32, ViT-B/16, and ViT-L/16. We report the task performance vs. the inference FLOPs per image on training and unseen tasks. The results suggest that the OTTER can benefit from scaling up vision-language model.",
                "position": 759
            }
        ]
    },
    {
        "header": "6Limitations and Conclusions",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "7Potential Broader Impact",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AEnvironment Setup",
        "images": []
    },
    {
        "header": "Appendix BModel and Training Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.03734/x4.png",
                "caption": "Figure 6:Examples of attention maps for CLIP fine-tuned with VLA (left) and frozen CLIPâ€™s output (Xoâ¢uâ¢tsubscriptğ‘‹ğ‘œğ‘¢ğ‘¡X_{out}italic_X start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT) (middle) and frozen CLIPâ€™s attention features (Xaâ¢tâ¢tâ¢nsubscriptğ‘‹ğ‘ğ‘¡ğ‘¡ğ‘›X_{attn}italic_X start_POSTSUBSCRIPT italic_a italic_t italic_t italic_n end_POSTSUBSCRIPT) (right). The first column shows the side view observation and the text query is below each attention map. Fine-tune CLIP pays attention to the background and the frozen CLIPâ€™s output (Xoâ¢uâ¢tsubscriptğ‘‹ğ‘œğ‘¢ğ‘¡X_{out}italic_X start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT) is noisy. In contrast, the frozen CLIP (Xaâ¢tâ¢tâ¢nsubscriptğ‘‹ğ‘ğ‘¡ğ‘¡ğ‘›X_{attn}italic_X start_POSTSUBSCRIPT italic_a italic_t italic_t italic_n end_POSTSUBSCRIPT) pays attention to the correct object associated with the text query. These examples indicate that fine-tuning CLIP on robotic datasets can degrade the performance of the pre-trained CLIP, especially when the robotics dataset is small. It also highlights the benefits of usingXaâ¢tâ¢tâ¢nsubscriptğ‘‹ğ‘ğ‘¡ğ‘¡ğ‘›X_{attn}italic_X start_POSTSUBSCRIPT italic_a italic_t italic_t italic_n end_POSTSUBSCRIPTfor fused vision-language features.",
                "position": 1676
            }
        ]
    },
    {
        "header": "Appendix CVision-Language Attention Visualization",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.03734/extracted/6269393/figures/attention_oxe.jpg",
                "caption": "Figure 7:Examples of attention maps of frozen CLIPâ€™s attention features (Xattn) on Open-X dataset. The bottom texts are the corresponding text tokens.",
                "position": 1724
            }
        ]
    },
    {
        "header": "Appendix DMore Ablations",
        "images": []
    }
]