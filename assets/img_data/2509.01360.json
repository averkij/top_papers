[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01360/x1.png",
                "caption": "Figure 1:Recall@5 on zero-shot image-to-image retrieval across four medical datasets.The proposed M3Ret, pretrained respectively with generative (MAE[1]) and contrastive (SimDINO[2]) self-supervision, achieves superior or comparable performance on ChestXray14 (X-ray)[3], Fetal Planes (Ultrasound)[4], Hyper Kvasir (Endoscopy)[5], and CT-RATE (CT)[6], outperforming language-supervised (BMC-CLIP[7]) and visual SSL (UniMiSS+[8]) baselines.",
                "position": 130
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01360/x2.png",
                "caption": "Figure 2:Comparison of visual representation learning strategies in medical imaging.(a)BMC-CLIP[7]relies on 2D image-text pairs (e.g., X-rays and reports), limiting generalization beyond paired data and modalities.(b)VoCo[35]learns from 3D sub-volumes with random cropping via contrastive learning, but neglects global context.(c)Our proposed approach learns unified visual representations from heterogeneous medical imaging modalities (2D, 3D, and videos) using purely visual SSL (e.g., MAE[1], SimDINO[2]), without any extra supervision signals such as text or modality-specific design.",
                "position": 173
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Zero-shot Medical Image Retrieval",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01360/x3.png",
                "caption": "Figure 3:Recall@5 for ChestXray14 and CT-RATE datasets.(a)Performance under varying numbers of local crops.(b)Effect of embedding strategies.(c)Comparison of different patch sizes. M3Ret (SimDINO) shows stable performance across local views and embedding choices, while smaller patch size offers better granularity and improves representation learning capacity.",
                "position": 1339
            },
            {
                "img": "https://arxiv.org/html/2509.01360/x4.png",
                "caption": "Figure 4:Recall@5 across four downstream tasks.Top: effect of model size scaling using ViT-T, ViT-S, and ViT-B.Bottom: effect of increasing pretraining data ratio (20%, 60%, 100%). M3Ret consistently benefits from larger models and more data, demonstrating its scalability. Red dashed lines show the fitted power laws.",
                "position": 1355
            },
            {
                "img": "https://arxiv.org/html/2509.01360/x5.png",
                "caption": "Figure 5:Qualitative comparison of top 3 retrieval examples.The top two rows show two X-ray query images presenting pleural effusion and pneumothorax, respectively. The bottom two rows display two ultrasound query images capturing the fetal brain and thorax, respectively. The top 3 retrieved results based on embedding similarity are listed in each row.",
                "position": 1378
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "Appendix APretraining Data Distribution",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01360/x6.png",
                "caption": "Figure 6:Distribution of body parts in the pretraining dataset.Top Left:X-ray.Top Right:Ultrasound.Bottom Left:Endoscopy.Bottom Right:CT.",
                "position": 1992
            }
        ]
    },
    {
        "header": "Appendix BDetails of Data Augmentation in SimDINO",
        "images": []
    },
    {
        "header": "Appendix CDownstream Task Configuration",
        "images": []
    },
    {
        "header": "Appendix DDownstream Classification Performance",
        "images": []
    },
    {
        "header": "Appendix ELimitations and Future Work",
        "images": []
    },
    {
        "header": "Appendix FBroader Impact",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01360/x7.png",
                "caption": "Figure 7:Qualitative results of top 3 retrieval examples in regional abnormality retrieval.",
                "position": 2799
            },
            {
                "img": "https://arxiv.org/html/2509.01360/x8.png",
                "caption": "Figure 8:Qualitative results of top 3 retrieval examples in CT→\\rightarrowX-ray Task.",
                "position": 2802
            },
            {
                "img": "https://arxiv.org/html/2509.01360/x9.png",
                "caption": "Figure 9:Qualitative results of top 3 retrieval examples in X-ray→\\rightarrowCT Task.",
                "position": 2805
            },
            {
                "img": "https://arxiv.org/html/2509.01360/x10.png",
                "caption": "Figure 10:Qualitative results of top 3 retrieval examples in CT→\\rightarrowMRI Task.",
                "position": 2808
            },
            {
                "img": "https://arxiv.org/html/2509.01360/x11.png",
                "caption": "Figure 11:Qualitative results of top 3 retrieval examples in MRI→\\rightarrowCT Task.",
                "position": 2811
            },
            {
                "img": "https://arxiv.org/html/2509.01360/x12.png",
                "caption": "Figure 12:Qualitative results of top 3 retrieval examples in X-ray→\\rightarrowMRI Task.",
                "position": 2814
            },
            {
                "img": "https://arxiv.org/html/2509.01360/x13.png",
                "caption": "Figure 13:Qualitative results of top 3 retrieval examples in MRI→\\rightarrowX-ray Task.",
                "position": 2817
            }
        ]
    },
    {
        "header": "Appendix GMore Visualization Results",
        "images": []
    }
]