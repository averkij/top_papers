[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02444/figures/new_teaser.png",
                "caption": "Figure 1:RankVideojudges the relevance between a query-video pair, dynamically reasoning or answering depending on the difficulty of the query-video pair.",
                "position": 123
            }
        ]
    },
    {
        "header": "2Related",
        "images": []
    },
    {
        "header": "3Data Synthesis",
        "images": []
    },
    {
        "header": "4RankVideoTwo-Stage Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02444/figures/figure_2.png",
                "caption": "Figure 2:RankVideois trained with a two-stage process. Stage 1 uses a perception-grounded supervised finetuning, where the model learns to generate captions grounded in video content. In Stage 2, for each text query, we sample a query grouped batch containing one positive (relevant) video and one or more negatives (not relevant), and score each candidate using the difference between the logits for yes and no. The model is optimized with a combined objective: (1) teacher-probability distillation towardpy​e​sp_{yes}, (2) a pointwise loss for stable binary calibration, and (3) pairwise ranking loss that pushes the positive to the top within the query batch.",
                "position": 198
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02444/figures/ecdf_bylabel.png",
                "caption": "Figure 3:Training Stage-2 increases score separation in the reranking regime. Empirical CDF of the reranker scoresθ​(q,v)=ℓθ​(yes|q,v)−ℓθ​(no|q,v)s_{\\theta}(q,v)=\\ell_{\\theta}(\\textbf{yes}|q,v)-\\ell_{\\theta}(\\textbf{no}|q,v)for relevant and non relevant query video pairs. Stage 2 shifts relevant pairs towards larger positive margins and suppresses non relevant candidates towards more negative margins, reducing overlaps in the score distributions within reranking candidate pools.",
                "position": 551
            },
            {
                "img": "https://arxiv.org/html/2602.02444/figures/latency_fig.png",
                "caption": "Figure 4:Median query latency for Qwen3VL Instruct/Thinking (QVL-I/T) andRankVideostages 1 and 2. Latency is computed as the mean for 100 query-video pairs with a batch size of 1. All evaluations are run with batch size 1 as larger batches exceed GPU memory for VLMs.",
                "position": 716
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARankVideoDetails",
        "images": []
    },
    {
        "header": "Appendix BTraining Loss Ablation",
        "images": []
    },
    {
        "header": "Appendix CRankVideoPerformance Across Query Types",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02444/figures/event_type_fig.png",
                "caption": "Figure 5:RankVideonDCG@10 by query event type. Multi-word categories are abbreviated as acronyms in the plot (e.g., PD=Political Development, LD=Launch/Discovery, SE=Social Events). Only attributes with≥\\geq30 test queries are included.",
                "position": 1842
            },
            {
                "img": "https://arxiv.org/html/2602.02444/figures/lang_fig.png",
                "caption": "Figure 6:RankVideonDCG@10 by video language (en=English, es=Spanish, ar=Arabic, zh=Chinese, ru=Russian, ko=Korean). Only attributes with≥\\geq30 test queries are included.",
                "position": 1845
            },
            {
                "img": "https://arxiv.org/html/2602.02444/figures/vid_type_fig.png",
                "caption": "Figure 7:RankVideonDCG@10 by video type. Professional: e.g., news broadcasts with reports; Edited: e.g., videos with multiple spliced clips and visual effects; Diet Raw (DR): single-stream videos with minimal text/speech overlays; Raw: continuous, unedited streams. Only attributes with≥\\geq30 test queries are included.",
                "position": 1848
            },
            {
                "img": "https://arxiv.org/html/2602.02444/figures/modality_fig.png",
                "caption": "Figure 8:RankVideonDCG@10 by query/video modality. OCR=queries written from text visible in video frames; Text=queries written using only the YouTube description; Base=Wikipedia-title-style queries; Speech=queries written from spoken content; Specific=queries targeting fine-grained aspects of events. Only attributes with≥\\geq30 test queries are included.",
                "position": 1851
            }
        ]
    },
    {
        "header": "Appendix DDisconnect Between Binary Classification and Reranking",
        "images": []
    },
    {
        "header": "Appendix EReasoning Examples",
        "images": []
    },
    {
        "header": "Appendix FUse of AI Assistants",
        "images": []
    }
]