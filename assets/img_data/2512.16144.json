[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16144/figures/prime-intellect-butterfly.png",
                "caption": "",
                "position": 111
            },
            {
                "img": "https://arxiv.org/html/2512.16144/x1.png",
                "caption": "Figure 1:INTELLECT-3Evaluation Results.222All the models above were evaluated using our public, reproducible Environments Hub implementations. To ensure the best results, we use APIs provided directly by the model creators whenever available to avoid any performance loss from quantization or other inference optimizations.",
                "position": 133
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16144/logos/hf.png",
                "caption": "",
                "position": 257
            },
            {
                "img": "https://arxiv.org/html/2512.16144/logos/github.png",
                "caption": "",
                "position": 257
            },
            {
                "img": "https://arxiv.org/html/2512.16144/logos/prime.jpg",
                "caption": "",
                "position": 257
            }
        ]
    },
    {
        "header": "2Training Infrastructure",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16144/x2.png",
                "caption": "Figure 2:Architecture.A RL training run involves the coordination of atrainer,orchestratorand aninferenceservice. The FSDP trainer and vLLM inference run disaggregated, and can be individually deployed across multiple nodes.",
                "position": 303
            },
            {
                "img": "https://arxiv.org/html/2512.16144/x3.png",
                "caption": "Figure 3:Asynchronous Off-Policy Training.We show the execution graph of one-step off-policy training in an idealized setting where the trainer step time equals the inference step time. At stepnn, the inference engine uses a policy no older thanθmin⁡(0,n−1)\\theta_{\\min{(0,n-1)}}.",
                "position": 328
            },
            {
                "img": "https://arxiv.org/html/2512.16144/x4.png",
                "caption": "Figure 4:Continuous Batching & In-Flight Weight Updates.Continuous batching maintains a constant inference load because finished rollout are immediately replaced with new rollout requests. The policy used to generate rollouts is updated in-flight as soon as it becomes available, allowing rollouts to be generated by multiple policies.",
                "position": 351
            },
            {
                "img": "https://arxiv.org/html/2512.16144/x5.png",
                "caption": "Figure 5:Execution time and TFLOPS oftorch._grouped_mmwith hidden dim40964096and MoE dim14081408on H200 SXM at different sequence lengths and number of experts. We assume that the input is perfectly balanced between the experts and thus an increase in experts leads to an inversely proportional decrease in the number of tokens and work per expert, eventually causing lower TFLOPS as the work per expert is no longer able to saturate the kernel. At sequence lengths (N)32,76832,768and65,53665,536, the TFLOPS remains in the saturated regime up to128128experts. We thus do not gain significant throughput from using expert parallel given our training parameters.",
                "position": 425
            }
        ]
    },
    {
        "header": "3INTELLECT-3Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16144/figures/deepdive-reward.png",
                "caption": "Figure 7:Mean reward ofQwen/Qwen3-4B-Instruct-2507qwen3technicalreportover RL training steps on DeepDive after a short SFT phase.",
                "position": 705
            },
            {
                "img": "https://arxiv.org/html/2512.16144/x6.png",
                "caption": "(a)Stage 1",
                "position": 810
            },
            {
                "img": "https://arxiv.org/html/2512.16144/x6.png",
                "caption": "(a)Stage 1",
                "position": 813
            },
            {
                "img": "https://arxiv.org/html/2512.16144/x7.png",
                "caption": "(b)Stage 2",
                "position": 818
            },
            {
                "img": "https://arxiv.org/html/2512.16144/x8.png",
                "caption": "Figure 9:Reinforcement Learning.Reasoning benchmark scores as training progresses. The benchmarks scores generally trend up and do not appear to have reached a plateau.",
                "position": 880
            },
            {
                "img": "https://arxiv.org/html/2512.16144/x9.png",
                "caption": "Figure 10:Training Stability.Early ablations of GSPO against CISPO (our algorithm at the time). We use async-8 as a testbed for algorithms to test if they would handle high levels of off-policyness. We observe strange reward (and all other metrics) collapse with GSPO also reported inkhatri2025artscalingreinforcementlearningqi2025defeatingtraininginferencemismatchfp16.",
                "position": 888
            }
        ]
    },
    {
        "header": "4Evaluations",
        "images": []
    },
    {
        "header": "5Conclusion & Future Work",
        "images": []
    },
    {
        "header": "Appendix AReproducing Evaluations",
        "images": []
    }
]