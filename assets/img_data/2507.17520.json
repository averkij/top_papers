[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.17520/x1.png",
                "caption": "Figure 1:Method overview.InstructVLA integrates robust multimodal understanding with precise instruction-driven robotic control, leveraging the world knowledge of VLMs. The core training strategy, vision-language-action instruction tuning, enhances manipulation by enabling the model to perform vision language reasoning before generating actions.",
                "position": 131
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3InstructVLA",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.17520/x2.png",
                "caption": "Figure 2:Overview of the InstructVLA.InstructVLA integrates the multimodal reasoning capabilities of a vision-language model with robotic manipulation. Generation consists of three steps: (1) asynchronous auto-regressive reasoning by the VLM, (2) latent action generation, and (3) action decoding. A MoE adaptation enables the VLM to alternate between reasoning and latent action prediction. The flow matching action expert decodes the final actions, conditioned on latent actions.",
                "position": 189
            }
        ]
    },
    {
        "header": "4VLA Dataset and Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.17520/x3.png",
                "caption": "Figure 3:Vision-language-action instruction tuning data examples.Annotations focus on:\n(1) improving scene understanding and (2) learning instruction following and planning.",
                "position": 224
            },
            {
                "img": "https://arxiv.org/html/2507.17520/x4.png",
                "caption": "Figure 4:Simpler-Instruct.We visualize six representative test cases, with instructions and responses from InstructVLA during evaluation. Top four failure modes of other VLAs are listed.",
                "position": 245
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.17520/x5.png",
                "caption": "Figure 5:Real-world experiments.“Atomic” refers to atomic instructions, while “Reasoning” denotes situated reasoning. For the Bridge settings, InstructVLA’s responses are presented.",
                "position": 913
            },
            {
                "img": "https://arxiv.org/html/2507.17520/x6.png",
                "caption": "Figure 6:Ablation Studies.Ablation studies are grouped into two perspectives:(a-d) Action ability integration: analysis of how design choices in data (language motion), representation (latent action tokens), vision encoders, and finetuning strategies influence manipulation performance.(e-g) Multimodal ability transfer: analysis of how vision-language understanding contributes to manipulation bt VL-to-action learning, instruction data scaling, and inference time thinking.",
                "position": 923
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix AMore Experiments and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.17520/x7.png",
                "caption": "Figure 7:Magma results.Magma’s responses collapse when given instructions resembling those in its manipulation tasks, possibly due to learned actions interfering with its language latent space.",
                "position": 1348
            },
            {
                "img": "https://arxiv.org/html/2507.17520/x8.png",
                "caption": "Figure 8:Comparison with GPT-4o.We visualize three examples from the VLA-IT language validation set. Each example includes a scenario caption (top), instruction response (middle), and question answering (bottom). The GPT-4o column displaysresponses only, as the instructions are identical across models.",
                "position": 1351
            },
            {
                "img": "https://arxiv.org/html/2507.17520/x9.png",
                "caption": "Figure 9:Test-time tinking and dual-frequency evaluation.“Expert” refers to the model after action pretraining, while “Generalist” denotes the model after VLA-IT tuning. For dual-frequency evaluation, the horizontal axis represents the ratio of VLM executions to expert model executions.",
                "position": 1407
            },
            {
                "img": "https://arxiv.org/html/2507.17520/x10.png",
                "caption": "Figure 10:Performance visualizationof 30 situated reasoning tasks with and without reasoning enabled. Activating reasoning in our generalist model generally improves performance. For clarity, tasks are grouped into three categories:Subtask, involving subtask identification;Commonsense Reasoning, requiring broad world knowledge; andCommonsense for Tool Use, focusing on tool-related reasoning.",
                "position": 1410
            },
            {
                "img": "https://arxiv.org/html/2507.17520/x11.png",
                "caption": "Figure 11:Case study on cross-embodiment.Top left: rollouts on SimplerEnv-Instruct. Top right: similar scenarios from the Bridge dataset with corresponding instructions. Bottom left: zero-shot results trained only on Bridge instructions. Bottom right: rollouts from the fine-tuned model.",
                "position": 1471
            },
            {
                "img": "https://arxiv.org/html/2507.17520/x12.png",
                "caption": "Figure 12:Case study on multimodal capabilities.OCR represents a unique multimodal skill of VLMs that is absent from typical manipulation datasets. We evaluate two tasks from the Instruction Aggregation set in SimplerEnv-Instruct, involving moving one letter to another (seeFigure13(1)). By comparing different finetuning paradigms, we assess how effectively multimodal capabilities are integrated into VLA models.",
                "position": 1477
            }
        ]
    },
    {
        "header": "Appendix BCase Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.17520/x13.png",
                "caption": "Figure 13:Reasoning cases in SimplerEnv-Instruct.Three cases of the VL fine-tuned OpenVLA and InstructVLA-Generalist. “SR” denotes success rate.",
                "position": 1529
            },
            {
                "img": "https://arxiv.org/html/2507.17520/x14.png",
                "caption": "Figure 14:Failure case 1 of InstructVLA.The model receives only a third-person view image as visual input, making it difficult to estimate depth or the gripper’s relative position to the object. Consequently, it fails to grasp the object accurately, despite the gripper appearing aligned with the target in the image.",
                "position": 1541
            },
            {
                "img": "https://arxiv.org/html/2507.17520/x15.png",
                "caption": "Figure 15:Failure case 2 of InstructVLA.The model fails to accurately estimate depth due to the real-to-sim gap, specifically the absence of arm reflection on the table, which causes the robot to become stuck in an out-of-distribution position.",
                "position": 1544
            },
            {
                "img": "https://arxiv.org/html/2507.17520/x16.png",
                "caption": "Figure 16:GPT-4o as the auxiliary system 2.We prompt GPT-4o with the first image from the environment along with the instruction, asking it to rewrite the prompt in a simple and clear format.",
                "position": 1556
            }
        ]
    },
    {
        "header": "Appendix CData Annotation Details and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.17520/x17.png",
                "caption": "Figure 17:Data analysis.Left: We manually identify common task categories and calculate the distribution. The proportion of direct prompts is reduced in favor of more diverse, free-form expressions. Right: Word cloud and verb-noun analyses compare the original Fractal instructions with the VLA-IT corpus.",
                "position": 1692
            },
            {
                "img": "https://arxiv.org/html/2507.17520/x17.png",
                "caption": "Figure 17:Data analysis.Left: We manually identify common task categories and calculate the distribution. The proportion of direct prompts is reduced in favor of more diverse, free-form expressions. Right: Word cloud and verb-noun analyses compare the original Fractal instructions with the VLA-IT corpus.",
                "position": 1695
            },
            {
                "img": "https://arxiv.org/html/2507.17520/x18.png",
                "caption": "Figure 18:More VLA instructions on Fractal dataset.",
                "position": 1701
            },
            {
                "img": "https://arxiv.org/html/2507.17520/x19.png",
                "caption": "Figure 19:More VLA instructions on Bridge dataset.",
                "position": 1707
            },
            {
                "img": "https://arxiv.org/html/2507.17520/x20.png",
                "caption": "Figure 20:Language motion examples",
                "position": 1892
            },
            {
                "img": "https://arxiv.org/html/2507.17520/x21.png",
                "caption": "Figure 21:Comparison of GPT annotations with and without ground truth instruction.Errors are highlighted in red.",
                "position": 1897
            },
            {
                "img": "https://arxiv.org/html/2507.17520/x22.png",
                "caption": "Figure 22:Comparison of GPT annotations with and without ground truth instruction.Errors are highlighted in red. In this case, GPT-4o incorrectly infers the temporal sequence of actions without access to the instruction.",
                "position": 1900
            }
        ]
    },
    {
        "header": "Appendix DBenchmark Visualization",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.17520/x23.png",
                "caption": "Figure 23:Examples of Instruction Aggregation in SimplerEnv-Instruct.We list ten examples with corresponding instructions and responses. Notably, InstructVLA shows the strongzero-shotability to interpret multilingual instructions, recognize novel objects, and leverage OCR capabilities.",
                "position": 1935
            },
            {
                "img": "https://arxiv.org/html/2507.17520/x24.png",
                "caption": "Figure 24:Examples of Situated Reasoning in SimplerEnv-Instruct.The second example’s responses is recorded before and after the drawer is open.",
                "position": 1938
            }
        ]
    },
    {
        "header": "Appendix EModel Design and Training Details",
        "images": []
    },
    {
        "header": "Appendix FMultimodal Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.17520/x25.png",
                "caption": "Figure 25:Zero-shot multimodal question answering.Four commonsense and four embodied examples are selected.",
                "position": 2330
            }
        ]
    },
    {
        "header": "Appendix GReal-world Experiments Setup and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.17520/x26.png",
                "caption": "Figure 26:Real-world dataset examples.Four examples from the few-shot training set, illustrating cluster classification tasks (left) and rack pick-and-place tasks (right).",
                "position": 2342
            },
            {
                "img": "https://arxiv.org/html/2507.17520/x27.png",
                "caption": "Figure 27:Zero-shot grounding.In a clustered pick-and-place setting, InstructVLA accurately places the blue cube by semantically grounding the reference to the celebrity.",
                "position": 2345
            },
            {
                "img": "https://arxiv.org/html/2507.17520/x28.png",
                "caption": "Figure 28:Light distraction.Stable visual features from DINO and SigLIP enable the model to operate robustly under extreme out-of-distribution lighting conditions.",
                "position": 2348
            },
            {
                "img": "https://arxiv.org/html/2507.17520/x29.png",
                "caption": "Figure 29:Zero-shot evaluation.We perform zero-shot evaluation in the Bridge kitchen environment with augmented background and novel objects. The instruction and model response are visualized in the first image.",
                "position": 2351
            },
            {
                "img": "https://arxiv.org/html/2507.17520/x30.png",
                "caption": "Figure 30:Real-world settings.A third-person view is captured using an Intel D435i camera for the Franka (few-shot) and WidowX (zero-shot) settings.",
                "position": 2360
            }
        ]
    },
    {
        "header": "Appendix HBroader Impacts and Future Work",
        "images": []
    }
]