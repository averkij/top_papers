[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.17612/x1.png",
                "caption": "Figure 1:JarvisArt supports multi-granularity retouching goals, ranging from scene-level adjustments to region-specific refinements. Users can perform intuitive, free-form edits through natural inputs such as text prompts, bounding boxes, or brushstrokes. Furthermore, users can edit any-resolution images with JarvisArt.Purple: multi-modal context understanding.Green: retouching strategy reasoning.Orange: decision-making in tool orchestration.",
                "position": 121
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.17612/x2.png",
                "caption": "Figure 2:The data generation pipeline comprises three main stages: 1) Curation of diverse source–target examples covering varied scenes and styles with corresponding Lightroom configurations; 2) Generation of diverse user instrcutions that reflects different creative intents; 3) Production of Chain-of-Thought traces that simulate a human artist reasoning process.",
                "position": 197
            },
            {
                "img": "https://arxiv.org/html/2506.17612/x3.png",
                "caption": "Figure 3:Overview of the two-stag post-training framework. Initially, JarvisArt undergoes supervised fine-tuning (SFT) on CoT-annotated data to develop foundational artistic reasoning and tool-use skills. Following this, we apply the Group Relative Policy Optimization for Retouching (GRPO-R) algorithm to further enhance the JarvisArt’s reasoning, tool proficiency, and generalization.",
                "position": 227
            },
            {
                "img": "https://arxiv.org/html/2506.17612/x4.png",
                "caption": "Figure 4:Agent-to-Lightroom protocol.",
                "position": 319
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.17612/extracted/6559615/figs/results1.png",
                "caption": "Figure 5:Visual comparison of different methods on MMArt-Bench.",
                "position": 556
            },
            {
                "img": "https://arxiv.org/html/2506.17612/x5.png",
                "caption": "Figure 6:User preference study.",
                "position": 575
            },
            {
                "img": "https://arxiv.org/html/2506.17612/x6.png",
                "caption": "Figure 7:Questionnaire results and user ratings comparing JarvisArt with the commercial Adobe Lightroom system. Ratings are on a 5-point Likert scale (1=strongly disagree, 5=strongly agree).",
                "position": 578
            },
            {
                "img": "https://arxiv.org/html/2506.17612/x7.png",
                "caption": "Figure 8:Visualization of the reward trends across training steps of for JarvisArt.",
                "position": 626
            }
        ]
    },
    {
        "header": "5Ablation Study",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendices",
        "images": []
    },
    {
        "header": "Appendix ADetails of the MMArt dataset.",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.17612/x8.png",
                "caption": "Figure 9:Statistics of the MMArt dataset. (a) The dataset is divided into four primary scenarios: portrait, landscape, street scenes, and still life, each containing a variety of subcategories. (b) A word cloud illustrates the rich linguistic diversity found in user instructions.",
                "position": 967
            },
            {
                "img": "https://arxiv.org/html/2506.17612/x9.png",
                "caption": "Figure 10:Visual examples to demonstrate the diversity of the proposed dataset.",
                "position": 970
            }
        ]
    },
    {
        "header": "Appendix BAdditional Method Details",
        "images": []
    },
    {
        "header": "Appendix CAdditional Experimental Details",
        "images": []
    },
    {
        "header": "Appendix DAdditional Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.17612/x10.png",
                "caption": "Figure 11:Examples of MMArt data annotated with Chain-of-Thought (CoT) reasoning.",
                "position": 1521
            },
            {
                "img": "https://arxiv.org/html/2506.17612/x11.png",
                "caption": "Figure 12:Data samples from MMArt with standard instructions.",
                "position": 1524
            },
            {
                "img": "https://arxiv.org/html/2506.17612/x12.png",
                "caption": "Figure 13:An example of JarvisArt empowering users to achieve interactive and interpretable editing, transforming their ambiguous intentions into artistic visual outcomes.",
                "position": 1527
            },
            {
                "img": "https://arxiv.org/html/2506.17612/x13.png",
                "caption": "Figure 14:An example of JarvisArt empowering users to achieve interactive and interpretable editing, transforming their ambiguous intentions into artistic visual outcomes.",
                "position": 1530
            },
            {
                "img": "https://arxiv.org/html/2506.17612/x14.png",
                "caption": "Figure 15:Editing results with JarvisArt are generated under complex prompts, with all retouching operations performed in a Lightroom environment, allowing for iterative adjustments.",
                "position": 1533
            },
            {
                "img": "https://arxiv.org/html/2506.17612/x15.png",
                "caption": "Figure 16:Editing results with JarvisArt are generated under complex prompts, with all retouching operations performed in a Lightroom environment, allowing for iterative adjustments.",
                "position": 1536
            },
            {
                "img": "https://arxiv.org/html/2506.17612/extracted/6559615/figs_Appendix/app_viusal_reuslts1.jpg",
                "caption": "Figure 17:Visual comparisons of all state-of-the-art editing methods alongside two automatic retouching modes from commercial software.",
                "position": 1539
            },
            {
                "img": "https://arxiv.org/html/2506.17612/extracted/6559615/figs_Appendix/app_viusal_reuslts2.jpg",
                "caption": "Figure 18:Visual comparisons of all state-of-the-art editing methods alongside two automatic retouching modes from commercial software.",
                "position": 1542
            },
            {
                "img": "https://arxiv.org/html/2506.17612/x16.png",
                "caption": "Figure 19:Visual comparisons of all instruction-based editing methods on MIT-FiveKfivek.",
                "position": 1545
            },
            {
                "img": "https://arxiv.org/html/2506.17612/x17.png",
                "caption": "Figure 20:Visual comparisons of all instruction-based editing methods on MIT-FiveKfivek.",
                "position": 1548
            },
            {
                "img": "https://arxiv.org/html/2506.17612/x18.png",
                "caption": "Figure 21:Prompt for MLLM-based metrics (SC, PQ) from scene-level and region-level.",
                "position": 1551
            },
            {
                "img": "https://arxiv.org/html/2506.17612/x19.png",
                "caption": "Figure 22:Role-playing prompt for preset recommendation.",
                "position": 1555
            },
            {
                "img": "https://arxiv.org/html/2506.17612/x20.png",
                "caption": "Figure 23:Prompt for simulating the professional user instructions",
                "position": 1559
            },
            {
                "img": "https://arxiv.org/html/2506.17612/x21.png",
                "caption": "Figure 24:Prompt for simulating the casual user instructions.",
                "position": 1563
            },
            {
                "img": "https://arxiv.org/html/2506.17612/x22.png",
                "caption": "Figure 25:Prompt for generating the initial Chain-of-Thought (COT) annotations.",
                "position": 1567
            },
            {
                "img": "https://arxiv.org/html/2506.17612/x23.png",
                "caption": "Figure 26:Prompt for generating the refined Chain-of-Thought (COT) annotations.",
                "position": 1571
            }
        ]
    },
    {
        "header": "Appendix EDetails of Retouching Tools in Lightroom",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]