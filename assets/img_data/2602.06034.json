[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06034/x1.png",
                "caption": "",
                "position": 107
            },
            {
                "img": "https://arxiv.org/html/2602.06034/x2.png",
                "caption": "",
                "position": 109
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06034/x3.png",
                "caption": "Figure 1:Comparison between text-based CoT (left) and multimodal interleaved CoT (right) for multimodal retrieval. Text-based CoT relies on language-driven inference over static visual representations, often failing to resolve fine-grained differences. In contrast, V-Retrver performs multimodal interleaved CoT reasoning by invoking visual tools to inspect candidate images, enabling grounded reasoning and more reliable ranking decisions.",
                "position": 114
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06034/x4.png",
                "caption": "Figure 2:Overview of the V-Retrver framework. The left panel illustrates the inference pipeline, featuring a coarse-to-fine process with embedding-based retrieval and agentic reranking. The right panel details the three training stages we proposed, including Cold Start, Rejection sampling Fine-Tuning, and EAPO.",
                "position": 178
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06034/x5.png",
                "caption": "(a)Rank Reward",
                "position": 1044
            },
            {
                "img": "https://arxiv.org/html/2602.06034/x5.png",
                "caption": "(a)Rank Reward",
                "position": 1047
            },
            {
                "img": "https://arxiv.org/html/2602.06034/x6.png",
                "caption": "(b)Response Length",
                "position": 1052
            },
            {
                "img": "https://arxiv.org/html/2602.06034/x7.png",
                "caption": "(c)Tool Calls",
                "position": 1057
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APrompt Template",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06034/x8.png",
                "caption": "Figure 4:System Prompt template for training and inference.",
                "position": 2153
            },
            {
                "img": "https://arxiv.org/html/2602.06034/x9.png",
                "caption": "Figure 5:User Prompt template for training and inference.",
                "position": 2164
            },
            {
                "img": "https://arxiv.org/html/2602.06034/x10.png",
                "caption": "Figure 6:Annotation Prompt template.",
                "position": 2175
            }
        ]
    },
    {
        "header": "Appendix BDetails about M-BEIR Dataset",
        "images": []
    },
    {
        "header": "Appendix CDetails about Unseen Dataset",
        "images": []
    },
    {
        "header": "Appendix DExploration of RAG Applications",
        "images": []
    },
    {
        "header": "Appendix EAlgorithms and Detailed Analysis",
        "images": []
    },
    {
        "header": "Appendix FQualitative Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06034/x11.png",
                "caption": "Figure 7:A qualitative example of the retrieval result generated from V-Retrver.",
                "position": 2663
            },
            {
                "img": "https://arxiv.org/html/2602.06034/x12.png",
                "caption": "Figure 8:A qualitative example of the retrieval result generated from V-Retrver.",
                "position": 2666
            },
            {
                "img": "https://arxiv.org/html/2602.06034/x13.png",
                "caption": "Figure 9:A qualitative example of the retrieval result generated from V-Retrver.",
                "position": 2669
            },
            {
                "img": "https://arxiv.org/html/2602.06034/x14.png",
                "caption": "Figure 10:A qualitative example of the retrieval result generated from V-Retrver.",
                "position": 2672
            },
            {
                "img": "https://arxiv.org/html/2602.06034/x15.png",
                "caption": "Figure 11:A qualitative example of the retrieval result generated from V-Retrver.",
                "position": 2675
            }
        ]
    },
    {
        "header": "Appendix GLimitations and Future Works",
        "images": []
    }
]