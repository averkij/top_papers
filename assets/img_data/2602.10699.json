[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10699/figure/introduction.png",
                "caption": "Figure 1.Comparison of Beams Search and V-STAR. Left: probability-based pruning removes high-reward items and produces homogeneous candidates. Right: V-STAR expands high-value prefixes under a fixed budget and strengthens within-group learning with Sibling-GRPO.",
                "position": 86
            }
        ]
    },
    {
        "header": "2.Related Work",
        "images": []
    },
    {
        "header": "3.Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10699/figure/backbone.png",
                "caption": "Figure 2.Overview of the proposed self-evolving decoding-and-learning framework V-STAR.",
                "position": 238
            }
        ]
    },
    {
        "header": "4.Methodology",
        "images": []
    },
    {
        "header": "5.Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10699/figure/prob_reward_mismatch7.png",
                "caption": "Figure 3.Spearman CorrelationœÅ\\rhoof Probability(P) and Value (V) Signals with Ground-truth Rewards.",
                "position": 1006
            },
            {
                "img": "https://arxiv.org/html/2602.10699/figure/scale4.png",
                "caption": "Figure 4.Training time scaling with decoding token budget.",
                "position": 1009
            }
        ]
    },
    {
        "header": "6.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]