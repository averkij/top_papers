[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.05489/x1.png",
                "caption": "Figure 1:(a) Different paradigms of temporal search.Previous works such as VideoAgent(wang2024videoagent)and T*(ye2025rethinking)predominantly rely on handcrafted workflows, resulting in suboptimal strategies. Our approach adopts end-to-end reinforcement learning, enabling the model to learn optimal search strategies directly from data.(b) Interleaved text-video thinking process.We reformulate the temporal search task as an interleaved text-video thinking process, where the temporal search is seamlessly interleaved into the reasoning process.",
                "position": 201
            },
            {
                "img": "https://arxiv.org/html/2511.05489/x2.png",
                "caption": "Figure 2:Two failure modes with the original GRPO reward.Left:Insufficient temporal exploration.The model misses critical frames required to correctly answer the question. Right:Inconsistent logical reasoning.The intermediate reasoning process contradicts the final answer.",
                "position": 210
            }
        ]
    },
    {
        "header": "2Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.05489/x3.png",
                "caption": "Figure 3:Overall pipeline of GRPO-CSV.Building upon the original GRPO, CSV extracts a dynamic frame set from the multi-modal CoT and constructs a vision-only CoT for re-answering. This design verifies that the searched dynamic frames provide sufficient evidence for correct reasoning, ensuring completeness and consistency without requiring explicit frame-level supervision.",
                "position": 291
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.05489/x4.png",
                "caption": "Table 1:Temporal search performance.We report temporal similarity, visual similarity, and question-answering (QA) accuracy on Haystack-LVBench, as well as QA accuracy on Haystack-Ego4D test-tiny subset. Baseline results are directly cited fromye2025rethinking.†indicates the average number of keyframes determined by the model adaptively.",
                "position": 371
            },
            {
                "img": "https://arxiv.org/html/2511.05489/x4.png",
                "caption": "Table 2:Video understanding performance.E2E stands for end-to-end optimization. # Frame represents the number of input frames.†indicates the keyframes produced by temporal search.",
                "position": 564
            },
            {
                "img": "https://arxiv.org/html/2511.05489/x4.png",
                "caption": "(a)Ablation Results",
                "position": 764
            },
            {
                "img": "https://arxiv.org/html/2511.05489/x4.png",
                "caption": "(b)Training Dynamics",
                "position": 854
            },
            {
                "img": "https://arxiv.org/html/2511.05489/x5.png",
                "caption": "Figure 5:Hypothesis-driven search.Given the context that dogs are lying in a row across multiple scenes and remain still, the model hypothesizes that they are waiting to be photographed. It then searches for the person taking a photo to gather supporting evidence and provides the final answer.",
                "position": 959
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix ASearch Function",
        "images": []
    },
    {
        "header": "Appendix BDataset Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.05489/x6.png",
                "caption": "Figure 6:Illustration of the proposed two-stage data filtering pipeline.",
                "position": 1139
            },
            {
                "img": "https://arxiv.org/html/2511.05489/x7.png",
                "caption": "Figure 7:Dataset analysis.(1) The training set is mainly composed of long videos. The average length is 1659 seconds, and the maximum length exceeds 10,000 seconds. (2) Egocentric QA pairs come from Haystack-Ego4D, and Exocentric QA data mainly from VideoMarathon and Cinepile, where VideoMarathon employs Panda-70M as the video source. (3) Question types include multiple-choice and open-ended questions. To obtain open-ended QA pairs, we convert some multiple-choice tasks into open-ended questions.",
                "position": 1169
            }
        ]
    },
    {
        "header": "Appendix CPrompt Design",
        "images": []
    },
    {
        "header": "Appendix DEvaluation Metrics",
        "images": []
    },
    {
        "header": "Appendix EEfficiency Analysis",
        "images": []
    },
    {
        "header": "Appendix FTraining Details",
        "images": []
    },
    {
        "header": "Appendix GMore Case Studies",
        "images": []
    },
    {
        "header": "Appendix HBroader Impacts",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.05489/x8.png",
                "caption": "Figure 13:Search pattern: search confirmation.",
                "position": 1630
            },
            {
                "img": "https://arxiv.org/html/2511.05489/x9.png",
                "caption": "Figure 14:Search pattern: elimination method.",
                "position": 1633
            },
            {
                "img": "https://arxiv.org/html/2511.05489/x10.png",
                "caption": "Figure 15:Search pattern: sequential search.",
                "position": 1636
            },
            {
                "img": "https://arxiv.org/html/2511.05489/x11.png",
                "caption": "Figure 16:Failure case: insufficient search.There were 4 options in total, but only 2 were reviewed before the search was terminated.",
                "position": 1639
            },
            {
                "img": "https://arxiv.org/html/2511.05489/x12.png",
                "caption": "Figure 17:Failure case: visual hallucination.No information related to riding was found in the search results.",
                "position": 1642
            }
        ]
    },
    {
        "header": "Appendix IThe Use of Large Language Models",
        "images": []
    }
]