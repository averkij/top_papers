[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "",
        "images": []
    },
    {
        "header": "Memory, Benchmark & Robots: A Benchmark for Solving Complex Tasks with Reinforcement Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.10550/x1.png",
                "caption": "Figure 1:Systematic classification of problems with memory in RL reveals distinct memory utilization patterns and enables objective evaluation of memory mechanisms across different agents.",
                "position": 262
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x2.png",
                "caption": "Figure 2:Illustration of demonstrative memory-intensive tasks execution from the proposed MIKASA-Robo benchmark. TheShellGameTouch-v0task requires the agent to memorize the ball’s location under mugs and touch the correct one. InRememberColor9-v0, the agent must memorize a cube’s color and later select the matching one. InRotateLenientPos-v0, the agent must rotate a peg while keeping track of its previous rotations.",
                "position": 739
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x3.png",
                "caption": "Figure 3:MIKASA bridges the gap between human-like memory complexity and RL agents requirements. While agents tasks don’t require the full spectrum of human memory capabilities, they can’t be reduced to simple spatio-temporal dependencies. MIKASA provides a balanced framework that captures essential memory aspects for agents tasks while maintaining practical simplicity.",
                "position": 1519
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x4.png",
                "caption": "Figure 4:Performance of PPO-MLP trained instatemode, i.e., in MDP mode without the need for memory. These results suggest that the proposed tasks are inherently solvable with a success rate of 100%percent\\%%.",
                "position": 1779
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x4.png",
                "caption": "Figure 4:Performance of PPO-MLP trained instatemode, i.e., in MDP mode without the need for memory. These results suggest that the proposed tasks are inherently solvable with a success rate of 100%percent\\%%.",
                "position": 1782
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x5.png",
                "caption": "Figure 5:PPO with MLP and LSTM backbones trained inRGB+jointsmode on theRememberColor-v0environment with dense rewards. Both architectures fail to solve medium and high complexity tasks.",
                "position": 1787
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x6.png",
                "caption": "Figure 6:Demonstration of PPO-MLP performance on MIKASA-Robo benchmark when trained with oracle-levelstateinformation. In this learning mode, MDP problem formulation is considered, i.e. memory is not required for successful problem solving. At the same time, the obtained results show that it is possible to solve these problems and obtain 100% Success Rate.",
                "position": 3337
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x6.png",
                "caption": "",
                "position": 3340
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x7.png",
                "caption": "",
                "position": 3344
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x8.png",
                "caption": "",
                "position": 3348
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x9.png",
                "caption": "",
                "position": 3352
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x10.png",
                "caption": "",
                "position": 3357
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x11.png",
                "caption": "",
                "position": 3361
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x12.png",
                "caption": "",
                "position": 3365
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x13.png",
                "caption": "",
                "position": 3369
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x14.png",
                "caption": "",
                "position": 3374
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x15.png",
                "caption": "",
                "position": 3378
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x16.png",
                "caption": "",
                "position": 3382
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x17.png",
                "caption": "",
                "position": 3386
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x18.png",
                "caption": "",
                "position": 3391
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x19.png",
                "caption": "",
                "position": 3395
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x20.png",
                "caption": "",
                "position": 3399
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x21.png",
                "caption": "",
                "position": 3403
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x22.png",
                "caption": "",
                "position": 3408
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x23.png",
                "caption": "",
                "position": 3412
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x24.png",
                "caption": "",
                "position": 3416
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x25.png",
                "caption": "",
                "position": 3420
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x26.png",
                "caption": "",
                "position": 3425
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x27.png",
                "caption": "",
                "position": 3429
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x28.png",
                "caption": "",
                "position": 3433
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x29.png",
                "caption": "Figure 7:Demonstration of PPO-MLP performance on MIKASA-Robo benchmark when trained with oracle-levelstateinformation. Results are shown for memory capacity (SeqOfColors[3,5,7]-v0,BunchOfColors[3,5,7]-v0) and sequential memory (ChainOfColors[3,5,7]-v0).",
                "position": 3440
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x29.png",
                "caption": "",
                "position": 3443
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x30.png",
                "caption": "",
                "position": 3447
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x31.png",
                "caption": "",
                "position": 3451
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x32.png",
                "caption": "",
                "position": 3455
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x33.png",
                "caption": "",
                "position": 3460
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x34.png",
                "caption": "",
                "position": 3464
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x35.png",
                "caption": "",
                "position": 3468
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x36.png",
                "caption": "",
                "position": 3472
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x37.png",
                "caption": "",
                "position": 3477
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x38.png",
                "caption": "Figure 8:Performance evaluation of PPO-MLP and PPO-LSTM on the MIKASA-Robo benchmark using the “RGB+joints” training mode with dense reward function, where the agent only receives images from the camera (from above and from the gripper) and information about the state of the joints (position and velocity). The results demonstrate that numerous tasks pose significant challenges even for PPO-LSTM agents with memory, establishing these environments as effective benchmarks for evaluating advanced memory-enhanced architectures.",
                "position": 3483
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x38.png",
                "caption": "",
                "position": 3486
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x39.png",
                "caption": "",
                "position": 3490
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x40.png",
                "caption": "",
                "position": 3494
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x41.png",
                "caption": "",
                "position": 3498
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x42.png",
                "caption": "",
                "position": 3503
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x43.png",
                "caption": "",
                "position": 3507
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x44.png",
                "caption": "",
                "position": 3511
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x45.png",
                "caption": "",
                "position": 3515
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x46.png",
                "caption": "",
                "position": 3520
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x47.png",
                "caption": "",
                "position": 3524
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x48.png",
                "caption": "",
                "position": 3528
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x49.png",
                "caption": "",
                "position": 3532
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x50.png",
                "caption": "Figure 9:Performance evaluation of PPO-MLP and PPO-LSTM on the MIKASA-Robo benchmark using the “RGB+joints” with sparse reward function training mode, where the agent only receives images from the camera (from above and from the gripper) and information about the state of the joints (position and velocity). This training mode with sparse reward function causes even more difficulty for the agent to learn, making this mode even more challenging for memory-enhanced agents.",
                "position": 3538
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x50.png",
                "caption": "",
                "position": 3541
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x51.png",
                "caption": "",
                "position": 3545
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x52.png",
                "caption": "",
                "position": 3549
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x53.png",
                "caption": "",
                "position": 3553
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x54.png",
                "caption": "",
                "position": 3558
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x55.png",
                "caption": "",
                "position": 3562
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x56.png",
                "caption": "",
                "position": 3566
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x57.png",
                "caption": "",
                "position": 3570
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x58.png",
                "caption": "Figure 10:ShellGameTouch-v0: The robot observes a ball in front of it. next, this ball is covered by a mug and then the robot has to touch the mug with the ball underneath.",
                "position": 3611
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x59.png",
                "caption": "Figure 11:RememberColor9-v0: The robot observes a colored cube in front of it, then this cube disappears and an empty table is shown. Then 9 cubes appear on the table, and the agent must touch a cube of the same color as the one it observed at the beginning of the episode.",
                "position": 3738
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x60.png",
                "caption": "Figure 12:RememberShape9-v0: The robot observes an object with specific shape in front of it, then the object disappears and an empty table appears. Then 9 objects of different shapes appear on the table, and the agent must touch an object of the same shape as the one it observed at the beginning of the episode.",
                "position": 3867
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x61.png",
                "caption": "Figure 13:RememberShapeAndColor5x3-v0: An object of a certain shape and color appears in front of the agent. Then the object disappears and the agent sees an empty table. Then objects of 5 different shapes and 3 different colors appear on the table and the agent has to touch what it observed at the beginning of the episode.",
                "position": 3996
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x62.png",
                "caption": "Figure 14:InterceptMedium-v0: A ball rolls on the table in front of the agent with a random initial velocity, and the agent’s task is to intercept this ball and direct it at the target zone.",
                "position": 4125
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x63.png",
                "caption": "Figure 15:InterceptGrabMedium-v0: A ball rolls on the table in front of the agent with a random initial velocity, and the agent’s task is to intercept this ball with a gripper and lift it up.",
                "position": 4254
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x64.png",
                "caption": "Figure 16:RotateLenientPos-v0: A randomly oriented peg is placed in front of the agent. The agent’s task is to rotate this peg by a certain angle (the center of the peg can be shifted).",
                "position": 4401
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x65.png",
                "caption": "Figure 17:RotateStrictPos-v0: A randomly oriented peg is placed in front of the agent. The agent’s task is to rotate this peg by a certain angle (it is not allowed to move the center of the peg)",
                "position": 4525
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x66.png",
                "caption": "Figure 18:TakeItBack-v0: The agent observes a green cube in front of him. The agent’s task is to move the green cube to the red target, and as soon as it lights up violet, return the cube to its original position (the agent does not observes the original position of the cube).",
                "position": 4660
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x67.png",
                "caption": "Figure 19:SeqOfColors7-v0: In front of the agent, 7 cubes of different colors appear sequentially. After the last cube is shown, the agent observes an empty table. Then 9 cubes of different colors appear on the table and the agent has to touch the cubes that were shown at the beginning of the episode in any order.",
                "position": 4775
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x68.png",
                "caption": "Figure 20:BunchOfColors7-v0: 7 cubes of different colors appear simultaneously in front of the agent. After the agent observes an empty table. Then, 9 cubes of different colors appear on the table and the agent has to touch the cubes that were shown at the beginning of the episode in any order.",
                "position": 4922
            },
            {
                "img": "https://arxiv.org/html/2502.10550/x69.png",
                "caption": "Figure 21:ChainOfColors7-v0: In front of the agent, 7 cubes of different colors appear sequentially. After the last cube is shown, the agent sees an empty table. Then 9 cubes of different colors appear on the table and the agent must unmistakably touch the cubes that were shown at the beginning of the episode, in the same strict order.",
                "position": 5069
            }
        ]
    },
    {
        "header": "",
        "images": []
    }
]