[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11868/x1.png",
                "caption": "Figure 1:Task resolution rate per model on Terminal-Bench 2.0. The error bars correspond to a 95% confidence interval. The agent scaffold used to report each model was chosen to maximize performance. Results for all agents and models evaluated are inAppendix A.",
                "position": 643
            }
        ]
    },
    {
        "header": "2Terminal-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11868/x2.png",
                "caption": "Figure 2:A Terminal-Bench task is composed of an instruction, a Dockerfile, a set of tests, and an oracle solution. Agents run inside a container into which the tests are copied and executed.",
                "position": 670
            },
            {
                "img": "https://arxiv.org/html/2601.11868/x3.png",
                "caption": "Figure 3:Our task audit process consists of multiple rounds of manual review of each task by several project contributors to audit for common mistakes. Between the three reviews, the average task received approximately three hours of combined reviewer attention, implying multiple hundreds of person-hours went into reviewing alone, excluding the time spent creating the tasks.",
                "position": 689
            },
            {
                "img": "https://arxiv.org/html/2601.11868/x4.png",
                "caption": "Figure 4:Tasks per category in Terminal-Bench 2.0. Categories were assigned by the task author. Software engineering is the largest category, although no single category represents the majority of tasks. Terminal-Bench 2.0 has representation across a variety of categories, including non-engineering-specific categories such as “personal assistant” and “video processing”.",
                "position": 782
            }
        ]
    },
    {
        "header": "3Experimental Setup",
        "images": []
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11868/x5.png",
                "caption": "Figure 5:The Pareto frontier of agent performance showing the tradeoff between performance and cost (log scale) on Terminal-Bench 2.0.",
                "position": 844
            },
            {
                "img": "https://arxiv.org/html/2601.11868/x6.png",
                "caption": "Figure 6:Performance of each model with its best agent harness as a function of release date. New models show increased capabilities, pointing towards a future where models may be able to automate the work captured by Terminal-Bench 2.0",
                "position": 847
            },
            {
                "img": "https://arxiv.org/html/2601.11868/x7.png",
                "caption": "Figure 7:Distribution of empirical difficulty within each human-predicted category (rows sum to 100%). For example, of all tasks rated hard by humans, 93.3% were also empirically hard for models, 3.3% were medium, and 3.3% were easy. Overall correlation: r=0.436, p<<0.001.",
                "position": 870
            },
            {
                "img": "https://arxiv.org/html/2601.11868/x8.png",
                "caption": "Figure 8:LLM Judge evaluation of failure modes across models (Terminus 2 scaffold).Execution errors dominate, while coherence and verification failures occur at comparable rates.",
                "position": 895
            },
            {
                "img": "https://arxiv.org/html/2601.11868/x9.png",
                "caption": "Figure 9:Terminus 2 command failures across all models and tasks mapped to the failure mode taxonomy. Categories (inner ring) that represent less than5%5\\%of all failures have been grouped under the “Other”category. Subcategories (outer ring) representing less than3%3\\%of failures have been similarly grouped. Labels are shortened versions of those shown in the taxonomy in AppendixE.2.",
                "position": 908
            }
        ]
    },
    {
        "header": "5Limitations",
        "images": []
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix Table of Contents",
        "images": []
    },
    {
        "header": "Appendix ADetailed Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11868/x10.png",
                "caption": "Figure 10:Distributions of agent execution time, number of model calls (Terminus 2 only), and total token usage across all trials. Most trials are completed in under 20 minutes, using fewer than 25 model calls and 10 million tokens. In extreme cases, agents ran for up to 2 hours, making hundreds of model calls and using nearly 100 million tokens.",
                "position": 2098
            },
            {
                "img": "https://arxiv.org/html/2601.11868/x11.png",
                "caption": "Figure 11:Task resolution rate per model using Terminus 2. The grid reveals a common set offrontier tasks, such as complex system configuration, kernel driver compilation, and database migration, that remain unsolved by any model in our evaluation. Tasks appearing on the rough diagonal of the grid imply a set of tasks that transition from unsolvable to solvable as model ability improves and are interesting candidates for further study.",
                "position": 2108
            },
            {
                "img": "https://arxiv.org/html/2601.11868/x12.png",
                "caption": "Figure 12:Timeout rate per model using Terminus 2. Tasks are sorted by model performance, similar toFigure 11.",
                "position": 2111
            },
            {
                "img": "https://arxiv.org/html/2601.11868/x13.png",
                "caption": "Figure 13:Average task token usage per model using Terminus 2. Tasks are sorted by model performance, similar toFigure 11.",
                "position": 2114
            }
        ]
    },
    {
        "header": "Appendix BQuality Control",
        "images": []
    },
    {
        "header": "Appendix CTrace Failure Description and Examples",
        "images": []
    },
    {
        "header": "Appendix DAdapters",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11868/x14.png",
                "caption": "Figure 14:The adapter architecture in Terminal-Bench. External benchmarks are transformed into a standardized format through adapters, validated via parity experiments, and integrated into the evaluation harness. This design enables any agent to be evaluated on any adapted benchmark without modification.",
                "position": 4430
            }
        ]
    },
    {
        "header": "Appendix EErrors Analysis - Command Failures",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11868/x15.png",
                "caption": "Figure 15:Terminus 2 command failures with GPT-5.2.",
                "position": 4932
            },
            {
                "img": "https://arxiv.org/html/2601.11868/x16.png",
                "caption": "Figure 16:Terminus 2 command failures with GPT-5.",
                "position": 4935
            },
            {
                "img": "https://arxiv.org/html/2601.11868/x17.png",
                "caption": "Figure 17:Terminus 2 command failures with GPT-5 Mini.",
                "position": 4938
            },
            {
                "img": "https://arxiv.org/html/2601.11868/x18.png",
                "caption": "Figure 18:Terminus 2 command failures with GPT-5 Nano.",
                "position": 4941
            },
            {
                "img": "https://arxiv.org/html/2601.11868/x19.png",
                "caption": "Figure 19:Terminus 2 command failures with GPT-5-Codex.",
                "position": 4944
            },
            {
                "img": "https://arxiv.org/html/2601.11868/x20.png",
                "caption": "Figure 20:Terminus 2 command failures with GPT-OSS-120B.",
                "position": 4947
            },
            {
                "img": "https://arxiv.org/html/2601.11868/x21.png",
                "caption": "Figure 21:Terminus 2 command failures with GPT-OSS-20B.",
                "position": 4950
            },
            {
                "img": "https://arxiv.org/html/2601.11868/x22.png",
                "caption": "Figure 22:Terminus 2 command failures with Claude Opus 4.5.",
                "position": 4953
            },
            {
                "img": "https://arxiv.org/html/2601.11868/x23.png",
                "caption": "Figure 23:Terminus 2 command failures with Claude Opus 4.1.",
                "position": 4956
            },
            {
                "img": "https://arxiv.org/html/2601.11868/x24.png",
                "caption": "Figure 24:Terminus 2 command failures with Claude Sonnet 4.5.",
                "position": 4959
            },
            {
                "img": "https://arxiv.org/html/2601.11868/x25.png",
                "caption": "Figure 25:Terminus 2 command failures with Claude Haiku 4.5.",
                "position": 4962
            },
            {
                "img": "https://arxiv.org/html/2601.11868/x26.png",
                "caption": "Figure 26:Terminus 2 command failures with Gemini 2.5 Pro.",
                "position": 4965
            },
            {
                "img": "https://arxiv.org/html/2601.11868/x27.png",
                "caption": "Figure 27:Terminus 2 command failures with Gemini 2.5 Flash.",
                "position": 4968
            },
            {
                "img": "https://arxiv.org/html/2601.11868/x28.png",
                "caption": "Figure 28:Terminus 2 command failures with Grok 4.",
                "position": 4971
            },
            {
                "img": "https://arxiv.org/html/2601.11868/x29.png",
                "caption": "Figure 29:Terminus 2 command failures with Grok Code Fast 1.",
                "position": 4974
            },
            {
                "img": "https://arxiv.org/html/2601.11868/x30.png",
                "caption": "Figure 30:Terminus 2 command failures with GLM 4.6.",
                "position": 4977
            },
            {
                "img": "https://arxiv.org/html/2601.11868/x31.png",
                "caption": "Figure 31:Terminus 2 command failures with MiniMax M2.",
                "position": 4980
            },
            {
                "img": "https://arxiv.org/html/2601.11868/x32.png",
                "caption": "Figure 32:Terminus 2 command failures with Kimi K2 Instruct.",
                "position": 4983
            },
            {
                "img": "https://arxiv.org/html/2601.11868/x33.png",
                "caption": "Figure 33:Terminus 2 command failures with Qwen3 Coder 480B A35B Instruct (FP8).",
                "position": 4986
            }
        ]
    },
    {
        "header": "Appendix FTerminus 2",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11868/x34.png",
                "caption": "Figure 34:Terminus 2, our minimal agent for comparing language models on Terminal-Bench",
                "position": 5713
            }
        ]
    },
    {
        "header": "Appendix GModel Performance: Output Tokens and Episode Count Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11868/x35.png",
                "caption": "Figure 35:Relationship between total average episodes and success rate across models in Terminus 2. The negligible correlation demonstrates that the number of interaction rounds has no meaningful relationship with task success.",
                "position": 5729
            },
            {
                "img": "https://arxiv.org/html/2601.11868/x36.png",
                "caption": "Figure 36:Relationship between average output tokens and success rate across models in Terminus 2. The weak negative correlation (r=-0.170, p=0.515) indicates that verbosity does not significantly predict task success.",
                "position": 5742
            }
        ]
    },
    {
        "header": "Appendix HList of Tasks in Terminal-Bench 2.0",
        "images": []
    },
    {
        "header": "Appendix ILLM Usage",
        "images": []
    }
]