[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.01163/figures/logos/homepage.png",
                "caption": "",
                "position": 129
            },
            {
                "img": "https://arxiv.org/html/2511.01163/figures/logos/huggingface.png",
                "caption": "",
                "position": 130
            },
            {
                "img": "https://arxiv.org/html/2511.01163/x1.png",
                "caption": "",
                "position": 134
            },
            {
                "img": "https://arxiv.org/html/2511.01163/x2.png",
                "caption": "Figure 1:The\\namebenchmark.\\nameevaluates UMMs through reciprocal cross-modal reasoning:\\ourvg(left) requires generating images with language-augmented reasoning, while\\ourir(right) requires generating text answers with visually-augmented reasoning.",
                "position": 138
            },
            {
                "img": "https://arxiv.org/html/2511.01163/x2.png",
                "caption": "",
                "position": 141
            },
            {
                "img": "https://arxiv.org/html/2511.01163/x3.png",
                "caption": "",
                "position": 145
            },
            {
                "img": "https://arxiv.org/html/2511.01163/x4.png",
                "caption": "",
                "position": 149
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3\\nameBenchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.01163/x5.png",
                "caption": "Figure 2:Overview of\\ourvg, the benchmark for evaluating how unified multimodal models generate images under intensive verbal reasoning. The benchmark spans44domains (natural science, culture and art, common sense, and logic), instantiated across77reasoning subtasks.",
                "position": 388
            },
            {
                "img": "https://arxiv.org/html/2511.01163/x6.png",
                "caption": "Figure 3:Overview of\\ourir, the benchmark for evaluating visually-augmented reasoning in verbal generation. The benchmark spans33scenarios and66subtasks: physical world modeling, logical assistance, and visual perception enhancement.",
                "position": 397
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.01163/x7.png",
                "caption": "Figure 4:Example outputs on\\ourir. Each row corresponds to one reasoning scenario, with the input on the left and outputs from representative unified models shown across columns.",
                "position": 2594
            },
            {
                "img": "https://arxiv.org/html/2511.01163/x8.png",
                "caption": "Figure 5:Cascade reasoning evaluationacross EditWorld and\\namebenchmarks. We compare cascade approaches (FLUX+GPT with GPT-4o prompt refinement) against UMMs.",
                "position": 3456
            },
            {
                "img": "https://arxiv.org/html/2511.01163/x9.png",
                "caption": "Figure 6:Visual reasoning augmentation evaluationacross three problem domains. We compare VLM performance w/ and w/o visual reasoning artifacts from UMMs.",
                "position": 3467
            },
            {
                "img": "https://arxiv.org/html/2511.01163/x10.png",
                "caption": "(a)Reasoning subtask performances.",
                "position": 3482
            },
            {
                "img": "https://arxiv.org/html/2511.01163/x10.png",
                "caption": "(a)Reasoning subtask performances.",
                "position": 3485
            },
            {
                "img": "https://arxiv.org/html/2511.01163/x11.png",
                "caption": "(b)Reasoning subtask correlation matrix.",
                "position": 3490
            },
            {
                "img": "https://arxiv.org/html/2511.01163/x12.png",
                "caption": "Figure 8:Evaluation reliability of GPT-4.1 across five assessment dimensions. Left: Pearson correlation coefficients between GPT-4.1 and human experts (green) versus GPT-4.1 and Gemini-2.5-Pro (purple). Right: Mean Absolute Error for the same comparisons.",
                "position": 3497
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExtended Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.01163/x13.png",
                "caption": "Figure 9:Prompt used for evaluating the reasoning process of temporal (RP).",
                "position": 4435
            },
            {
                "img": "https://arxiv.org/html/2511.01163/x14.png",
                "caption": "Figure 10:Prompt template for evaluating visual-temporal reasoning capabilities (RV). (Continued in Figure11)",
                "position": 4438
            },
            {
                "img": "https://arxiv.org/html/2511.01163/x15.png",
                "caption": "Figure 11:Prompt template for evaluating visual-temporal reasoning capabilities (RV). (Continued from Figure10)",
                "position": 4441
            },
            {
                "img": "https://arxiv.org/html/2511.01163/x16.png",
                "caption": "Figure 12:Prompt template for evaluating process of causal reasoning capabilities (RP).",
                "position": 4444
            },
            {
                "img": "https://arxiv.org/html/2511.01163/x17.png",
                "caption": "Figure 13:Prompt template for evaluating visual causal reasoning capabilities (RV). (Continued in Figure14)",
                "position": 4447
            },
            {
                "img": "https://arxiv.org/html/2511.01163/x18.png",
                "caption": "Figure 14:Prompt template for evaluating visual causal reasoning capabilities (RV). (Continued from Figure13)",
                "position": 4450
            },
            {
                "img": "https://arxiv.org/html/2511.01163/x19.png",
                "caption": "Figure 15:Prompt template for evaluating reasoning alignment capabilities (Align.). (Continued in Figure16)",
                "position": 4453
            },
            {
                "img": "https://arxiv.org/html/2511.01163/x20.png",
                "caption": "Figure 16:Prompt template for evaluating reasoning alignment capabilities (Align.). (Continued from Figure15)",
                "position": 4456
            },
            {
                "img": "https://arxiv.org/html/2511.01163/x21.png",
                "caption": "Figure 17:Prompt template for evaluating visual consistency (VC.). (Continued from Figure17)",
                "position": 4459
            },
            {
                "img": "https://arxiv.org/html/2511.01163/x22.png",
                "caption": "Figure 18:Prompt template for evaluating image quality (IQ.). (Continued from Figure18)",
                "position": 4462
            }
        ]
    },
    {
        "header": "Appendix BExperiment Details",
        "images": []
    }
]