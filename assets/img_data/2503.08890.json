[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08890/extracted/6272242/pics/system.jpg",
                "caption": "Figure 1:Illustration of ourPlainQAFactframework. The inputs are PLSs and scientific abstracts. Our evaluation framework consists of a pre-trained classifier for factuality type categorization, retrieval of external domain knowledge for elaborative explanation sentences, and a local LLM-based question-answering system.",
                "position": 136
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3PlainFactBenchmark",
        "images": []
    },
    {
        "header": "4PlainQAFactFramework",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08890/extracted/6272242/pics/main_baseline_comparison.png",
                "caption": "Figure 2:Scaled factuality accuracy onPlainFactand the CELLS test set.PlainQAFactsignificantly outperforms all other metrics on both datasets, except for AlignScore onPlainFact(p<0.05ð‘0.05p<0.05italic_p < 0.05, paired t-test). We compute the std. by running the experiment five times for both our method andLlama 3.1.",
                "position": 597
            },
            {
                "img": "https://arxiv.org/html/2503.08890/extracted/6272242/pics/external_only.png",
                "caption": "Figure 3:Scaled factuality accuracy on human-annotated elaborative explanation sentences fromPlainFact.PlainQAFactsignificantly outperforms all other metrics on both evaluation levels (p<0.05ð‘0.05p<0.05italic_p < 0.05, paired t-test). We compute the std. by running the experiment five times for both our method andLlama 3.1.",
                "position": 611
            },
            {
                "img": "https://arxiv.org/html/2503.08890/extracted/6272242/pics/running_time.png",
                "caption": "Figure 4:Evaluation time comparison ofPlainQAFact. We compare both sentence- and summary-level evaluation time over four settings: without information retrieval,PlainQAFact, and using GPT-4o as the classifier and answer extractor. We report the percentage reduction in time for summary-level evaluation given the sentence-level running time.",
                "position": 743
            }
        ]
    },
    {
        "header": "7Discussion and Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADataset Annotation Protocol",
        "images": []
    },
    {
        "header": "Appendix BDataset Examples",
        "images": []
    },
    {
        "header": "Appendix CLLM Prompts",
        "images": []
    },
    {
        "header": "Appendix DDetailed Experiment Settings",
        "images": []
    },
    {
        "header": "Appendix ESummary-Level Ablation Study ofPlainQAFact",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08890/extracted/6272242/pics/external_removal.png",
                "caption": "Figure 5:Evaluation performance across metrics for explanation sentences underPlainFact. We select only those summaries containing at least five explanation sentences, resulting in a total of 116 summaries. Note that this experiment is conducted at the summary level. A higher change percentage indicates a metric is more sensitive to the explanation information, implying that such metrics may not effectively verify the factuality of PLS. Ideally, we expect the score change percentage to increase slowly. Among the metrics evaluated, only AlignScore, SummaC, and our method show this expected pattern.",
                "position": 1965
            }
        ]
    },
    {
        "header": "Appendix FExplanation Sentence Removal",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08890/extracted/6272242/pics/new_factpico_RQ1.png",
                "caption": "Figure 6:Score change percentage from baselines over three metrics on the FactPICO dataset in removing factual added information (88 pairs). We expect each model get higher performance when more external information is removed. The evaluation dataset contains 88 valid summary-abstract pairs.",
                "position": 1995
            },
            {
                "img": "https://arxiv.org/html/2503.08890/extracted/6272242/pics/new_factpico_RQ2.png",
                "caption": "Figure 7:Score change percentage from baselines over three metrics on the FactPICO dataset in removing non-factual added information (60 pairs). We expect each metricâ€™s score increases when more non-factual information is removed.",
                "position": 2003
            }
        ]
    },
    {
        "header": "Appendix GPilot Study on FactPICO",
        "images": []
    }
]