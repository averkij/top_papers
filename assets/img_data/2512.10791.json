[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2The FACTS Leaderboard",
        "images": []
    },
    {
        "header": "3FACTS Multimodal",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.10791/assets/mm/mm_train.jpg",
                "caption": "Table 2:Examples from the FACTS Multimodal benchmark, illustrating the rubric-based evaluation. The autorater uses the rubric to generate two distinct verdicts for factuality and completeness.",
                "position": 668
            },
            {
                "img": "https://arxiv.org/html/2512.10791/assets/mm/mm_racta.jpg",
                "caption": "",
                "position": 777
            },
            {
                "img": "https://arxiv.org/html/2512.10791/assets/mm/mm_abc.png",
                "caption": "",
                "position": 855
            },
            {
                "img": "https://arxiv.org/html/2512.10791/x1.png",
                "caption": "Figure 1:Distributions of image and question categories in the FACTS Multimodal benchmark.",
                "position": 936
            }
        ]
    },
    {
        "header": "4FACTS Parametric",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.10791/x2.png",
                "caption": "Figure 2:Example of the sentence-level annotation interface. Annotators are shown the image, the full model-generated paragraph, and a highlighted sentence. They assess its factuality by selecting a label (here, ‘Contradiction’) and providing a textual explanation for any inaccuracies observed.",
                "position": 1157
            },
            {
                "img": "https://arxiv.org/html/2512.10791/assets/parametric/domain_topic_distribution.png",
                "caption": "Figure 3:Distributions of context domain and of answer type as a percent of the total set of questions in the FACTS Parametric benchmark.",
                "position": 1378
            },
            {
                "img": "https://arxiv.org/html/2512.10791/assets/parametric/answer_type.png",
                "caption": "",
                "position": 1387
            }
        ]
    },
    {
        "header": "5FACTS Search",
        "images": []
    },
    {
        "header": "6FACTS Grounding v2",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.10791/assets/grounding/domain_distribution.png",
                "caption": "Figure 4:Distributions of context domain and of task requested by the user as a percent of the total set of prompts in the benchmark.",
                "position": 2057
            },
            {
                "img": "https://arxiv.org/html/2512.10791/assets/grounding/task_distribution.png",
                "caption": "",
                "position": 2066
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Contributions and Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]