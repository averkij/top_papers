[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.03884/x1.png",
                "caption": "Figure 1:Training loss, validation perplexity (PPL), and downstream performance of 1B dense models. We compare models employing different activation functions, including SwiGLU, GELU, ReLU, PolyReLU, and PolyNorm. It indicates that models using PolyReLU and PolyNorm exhibit lower training loss and validation PPL, alongside better downstream performance.",
                "position": 85
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Polynomial Composition Activation Function",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.03884/x2.png",
                "caption": "Figure 2:Block diagrams of Transformer MLP blocks utilizing ReLU/GELU, SwiGLU, PolyReLU and PolyNorm. ‚ÄúFC‚Äù stands for Fully Connected layer. ‚Äúxisuperscriptùë•ùëñx^{i}italic_x start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT‚Äù represents theiùëñiitalic_i-th power of the input tensorxùë•xitalic_x, while ‚Äúajsubscriptùëéùëóa_{j}italic_a start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT‚Äù denotes thejùëójitalic_j-th element of the learnable weight vectoraùëéaitalic_a. ‚ÄúN‚Äù indicates a normalization operation.",
                "position": 176
            }
        ]
    },
    {
        "header": "3Theoretical Analysis",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.03884/x3.png",
                "caption": "Figure 3:Traning and validation loss on C4 and Wikipedia for MoE models with 200 billion training tokens. We compare models using SwiGLU and PolyNorm activation functions. PolyNorm demonstrates lower training and validation losses, indicating faster convergence.",
                "position": 555
            },
            {
                "img": "https://arxiv.org/html/2411.03884/x4.png",
                "caption": "Figure 4:Downstrean performance dynamics on HellaSwag, MMLU Var, ARC-Challenge, and SciQ for MoE models with 200 billion training tokens. Models with PolyNorm significantly outperform those with SwiGLU on downstream tasks.",
                "position": 559
            },
            {
                "img": "https://arxiv.org/html/2411.03884/x5.png",
                "caption": "((a))Different orders of PolyReLU",
                "position": 734
            },
            {
                "img": "https://arxiv.org/html/2411.03884/x5.png",
                "caption": "((a))Different orders of PolyReLU",
                "position": 737
            },
            {
                "img": "https://arxiv.org/html/2411.03884/x6.png",
                "caption": "((b))Different compositions",
                "position": 745
            },
            {
                "img": "https://arxiv.org/html/2411.03884/x7.png",
                "caption": "((c))ReLU variants",
                "position": 753
            },
            {
                "img": "https://arxiv.org/html/2411.03884/x8.png",
                "caption": "((a))Rank ofWupsubscriptùëäupW_{\\text{up}}italic_W start_POSTSUBSCRIPT up end_POSTSUBSCRIPT, dense",
                "position": 767
            },
            {
                "img": "https://arxiv.org/html/2411.03884/x8.png",
                "caption": "((a))Rank ofWupsubscriptùëäupW_{\\text{up}}italic_W start_POSTSUBSCRIPT up end_POSTSUBSCRIPT, dense",
                "position": 770
            },
            {
                "img": "https://arxiv.org/html/2411.03884/x9.png",
                "caption": "((b))Rank ofWdownsubscriptùëädownW_{\\text{down}}italic_W start_POSTSUBSCRIPT down end_POSTSUBSCRIPT, dense",
                "position": 778
            },
            {
                "img": "https://arxiv.org/html/2411.03884/x10.png",
                "caption": "((c))Rank ofWupsubscriptùëäupW_{\\text{up}}italic_W start_POSTSUBSCRIPT up end_POSTSUBSCRIPT, MoE",
                "position": 786
            },
            {
                "img": "https://arxiv.org/html/2411.03884/x11.png",
                "caption": "((d))Rank ofWdownsubscriptùëädownW_{\\text{down}}italic_W start_POSTSUBSCRIPT down end_POSTSUBSCRIPT, MoE",
                "position": 794
            },
            {
                "img": "https://arxiv.org/html/2411.03884/x12.png",
                "caption": "((a))SwiGLU, dense",
                "position": 806
            },
            {
                "img": "https://arxiv.org/html/2411.03884/x12.png",
                "caption": "((a))SwiGLU, dense",
                "position": 809
            },
            {
                "img": "https://arxiv.org/html/2411.03884/x13.png",
                "caption": "((b))PolyNorm, dense",
                "position": 817
            },
            {
                "img": "https://arxiv.org/html/2411.03884/x14.png",
                "caption": "((c))SwiGLU, MoE",
                "position": 825
            },
            {
                "img": "https://arxiv.org/html/2411.03884/x15.png",
                "caption": "((d))PolyNorm, MoE",
                "position": 833
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AOmitted Proofs",
        "images": []
    },
    {
        "header": "Appendix BActivation Functions",
        "images": []
    },
    {
        "header": "Appendix CExperimental Details",
        "images": []
    },
    {
        "header": "Appendix DAdditional Results on Dense Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.03884/x16.png",
                "caption": "Figure 8:Traning loss, validation loss, and validation perplexity (PPL) for the 1B dense model with different orders of PolyReLU activation functions.",
                "position": 2410
            },
            {
                "img": "https://arxiv.org/html/2411.03884/x17.png",
                "caption": "Figure 9:Traning loss, validation loss, and validation perplexity (PPL) for the 1B dense model with different polynomial compositions.",
                "position": 2414
            },
            {
                "img": "https://arxiv.org/html/2411.03884/x18.png",
                "caption": "Figure 10:Traning loss, validation loss, and validation perplexity (PPL) for the 1B dense model with different variants of ReLU activation functions.",
                "position": 2418
            },
            {
                "img": "https://arxiv.org/html/2411.03884/x19.png",
                "caption": "Figure 11:Validation loss and downstream evaluations for MoE models with 200 billion training tokens, comparing SwiGLU and PolyNorm activation functions. PolyNorm shows superior performance in terms of lower loss and better downstream results.",
                "position": 2429
            }
        ]
    },
    {
        "header": "Appendix EAdditional Results on MoE model",
        "images": []
    }
]