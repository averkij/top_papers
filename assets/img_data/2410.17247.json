[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17247/x1.png",
                "caption": "Figure 1:Observatioins about visual redundancy acoross layers. Left: TextVQA performance of LLaVA-1.5 with varying ratio of retained image tokens at different layer. The preserved image tokens are those that receive the highest attention from the text tokens. Right: Visualization of attention map in shallow and deep layers.",
                "position": 138
            },
            {
                "img": "https://arxiv.org/html/2410.17247/x2.png",
                "caption": "Figure 2:Overview of PyramidDrop. We divide the forward pass of the LLM into multiple stages, and drop part of the image tokens at the end of each stage with a pre-defined ratio. The dropping is based on a lightweight attention calculation with a negligible time overhead, and according to this criterion, the LLM accurately selects important image tokens related to instruction. Due to the efficient redundancy reduction strategy, the average sequence length decreases rapidly.",
                "position": 161
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17247/x3.png",
                "caption": "Figure 3:We compare the performance of the original LLaVA-1.5 and LLaVA-1.5 trained using PDrop, where we preserve different ratios of image tokens at layer 2, 8, 16, and 24, respectively. The horizontal axis represents the proportion of retained image tokens according to attention score.",
                "position": 603
            },
            {
                "img": "https://arxiv.org/html/2410.17247/x4.png",
                "caption": "Figure 4:The performance of LLaVA-NeXT-7B with different inference acceleration strategies. PDrop (without training) outperforms FastV on DocVQA, ChartQA, and GQA with across various inference cost budgets.",
                "position": 1065
            },
            {
                "img": "https://arxiv.org/html/2410.17247/x5.png",
                "caption": "Figure 5:Visualization of token dropping in LLM of LLaVA\n-1.5. We compute the attention score of image tokens received from the last instruction token as the ranking criterion, and find LLM accurately retain image tokens according to instruction.",
                "position": 1068
            }
        ]
    },
    {
        "header": "5CONCLUSION",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]