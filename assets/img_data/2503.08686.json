[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08686/x1.png",
                "caption": "",
                "position": 117
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08686/x2.png",
                "caption": "Figure 2:Architecture of the proposed OmniMamba“MMU” refers to multimodal understanding, while “T2I” refers to text-to-image generation. OmniMamba employs a next-token prediction paradigm for both multimodal understanding and visual generation tasks. To address the distinct requirements of each task—semantic information extraction for multimodal understanding and high-fidelity image compression for visual generation, we utilize separate encoders and heads. Furthermore, we purpose decoupled vocabularies to guide modality-specific generation and task-specific LoRA for parameter-efficient adaptation.",
                "position": 179
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08686/x3.png",
                "caption": "Figure 3:The Mamba-2 block with task-specific LoRA. It is worth noting that while the Mamba-2 Block in the Mamba-2 paper has two input projectors, the actual code implementation separates the feature dimensions from a single projector output. For simplicity, we depict only one input projector in our illustration. Our task-specific LoRA is applied to this entire input projector.",
                "position": 223
            },
            {
                "img": "https://arxiv.org/html/2503.08686/x4.png",
                "caption": "Figure 4:Training strategy of OmniMamba. The trainable components are indicated by a flame symbol, while the frozen ones are represented by snowflakes. The dashed arrows indicate that this route is temporarily dropped and does not participate in model training.",
                "position": 246
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08686/x5.png",
                "caption": "Figure 5:Qualitative results of OmniMamba on multimodal understanding and visual generation.",
                "position": 744
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ATraining Details",
        "images": []
    },
    {
        "header": "Appendix BLimitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08686/extracted/6270528/figure/dog_bicycle.jpg",
                "caption": "Table 6:OmniMamba can accurately determine spatial positions, JanusFlow made the correct judgment, but its response was overly concise, while Show-o lacks spatial reasoning capabilities, as indicated by theredtext.",
                "position": 1943
            },
            {
                "img": "https://arxiv.org/html/2503.08686/extracted/6270528/figure/dog_bicycle.jpg",
                "caption": "Table 6:OmniMamba can accurately determine spatial positions, JanusFlow made the correct judgment, but its response was overly concise, while Show-o lacks spatial reasoning capabilities, as indicated by theredtext.",
                "position": 1944
            },
            {
                "img": "https://arxiv.org/html/2503.08686/extracted/6270528/figure/sheep.jpg",
                "caption": "Table 7:OmniMamba correctly outputs the quantity while avoiding the user’s trap question, whereas Show-o, despite counting correctly, was misled by the question, as indicated by theredtext. On the other hand, JanusFlow’s response was not accurate enough.",
                "position": 2007
            },
            {
                "img": "https://arxiv.org/html/2503.08686/extracted/6270528/figure/sheep.jpg",
                "caption": "Table 7:OmniMamba correctly outputs the quantity while avoiding the user’s trap question, whereas Show-o, despite counting correctly, was misled by the question, as indicated by theredtext. On the other hand, JanusFlow’s response was not accurate enough.",
                "position": 2008
            },
            {
                "img": "https://arxiv.org/html/2503.08686/extracted/6270528/figure/dog_and_cat.png",
                "caption": "Table 8:OmniMamba can accurately describe the information in the scene, whereas Show-o made a mistake about the color of the cat and misidentified the dog as a teddy bear, as indicated by theredtext.",
                "position": 2071
            },
            {
                "img": "https://arxiv.org/html/2503.08686/extracted/6270528/figure/dog_and_cat.png",
                "caption": "Table 8:OmniMamba can accurately describe the information in the scene, whereas Show-o made a mistake about the color of the cat and misidentified the dog as a teddy bear, as indicated by theredtext.",
                "position": 2072
            },
            {
                "img": "https://arxiv.org/html/2503.08686/x6.png",
                "caption": "Figure 6:Qualitative results of OmniMamba visual generation. Prompts are randomly drawn from the MS-COCO validation set.",
                "position": 2134
            }
        ]
    },
    {
        "header": "Appendix CAdditional Qualitative Results",
        "images": []
    }
]