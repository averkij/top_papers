[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19304/x1.png",
                "caption": "Figure 1:Conceptual comparison between learning in a single environment (left) and cross-environment learning (right). In the single-environment case, trajectories from one environment are used to update the agentâ€™s internal components (model and agent code). In the cross-environment case, trajectories collected across many environments update both the agent components and a shared learning procedure, so that the agent learns how to learn from diverse environments.",
                "position": 192
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Automated Environment Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19304/x2.png",
                "caption": "Figure 2:Overview of theAutoEnvenvironment generation pipeline.\nThe left panel shows an input environment theme and its DSL design in YAML form.\nThe middle panel illustrates how coding agents instantiate basic code from the DSL, then run a self-repair loop followed by three verification stages, execution testing, level generation, and reliability checking with differential models.\nThe right panel presents the core code structure for the three abstraction layers, and an example SkinEnv that renders both text descriptions and an image view.",
                "position": 264
            }
        ]
    },
    {
        "header": "4Learning",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19304/x3.png",
                "caption": "Figure 3:Impact of environment diversity on learning performance across 36 environments.\nLeft: Performance gain of individual learning methods decreases as more environments are included (top-k by gain).\nRight: Expanding learning method space (M=1 to M=4) improves the upper bound with diminishing marginal returns. For each M, we evaluate all(4M)\\binom{4}{M}method subsets by taking the per-environment maximum over methods in each subset, averaging across environments, then averaging over subsets. This shows the performance upper bound achievable with M method choices.",
                "position": 1008
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AEnvironment Abstraction",
        "images": []
    },
    {
        "header": "Appendix BEnvironment Generation Details",
        "images": []
    },
    {
        "header": "Appendix CEnvironment Evalution Details",
        "images": []
    },
    {
        "header": "Appendix DLearning Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19304/x4.png",
                "caption": "Figure A1:Generated multimodal skin for partial environments inAutoEnv-36.",
                "position": 5002
            },
            {
                "img": "https://arxiv.org/html/2511.19304/x5.png",
                "caption": "Figure A2:Generated multimodal skins for the same base environment, showing how rules can be decoupled from observations.",
                "position": 5005
            }
        ]
    },
    {
        "header": "Appendix EAddtional Experiments",
        "images": []
    }
]