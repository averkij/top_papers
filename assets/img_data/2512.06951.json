[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1‚ÄÉ‚ÄäIntroduction",
        "images": []
    },
    {
        "header": "2‚ÄÉ‚ÄäRelated Work",
        "images": []
    },
    {
        "header": "3‚ÄÉ‚ÄäMethod Overview",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06951/media/Model_schema.png",
                "caption": "Figure 1:Overall architecture of our system. The vision backbone (SigLIP) processes images from three cameras, PaliGemma VLM processes visual tokens along with task embeddings and robot state, and the Action Expert predicts actions via flow matching. Task embeddings replace language processing, and System 2 provides stage information for non-Markovian context.",
                "position": 488
            },
            {
                "img": "https://arxiv.org/html/2512.06951/media/radio1.png",
                "caption": "(a)Lifting radio",
                "position": 698
            },
            {
                "img": "https://arxiv.org/html/2512.06951/media/radio1.png",
                "caption": "(a)Lifting radio",
                "position": 701
            },
            {
                "img": "https://arxiv.org/html/2512.06951/media/radio2.png",
                "caption": "(b)Placing radio back",
                "position": 706
            },
            {
                "img": "https://arxiv.org/html/2512.06951/media/popcorn1.png",
                "caption": "(c)Opening microwave",
                "position": 711
            },
            {
                "img": "https://arxiv.org/html/2512.06951/media/popcorn2.png",
                "caption": "(d)Pressing button",
                "position": 716
            }
        ]
    },
    {
        "header": "4‚ÄÉ‚ÄäModel Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06951/media/k_coeffs_deviation_heatmap.png",
                "caption": "(a)Key coefficients deviation from identity",
                "position": 802
            },
            {
                "img": "https://arxiv.org/html/2512.06951/media/k_coeffs_deviation_heatmap.png",
                "caption": "(a)Key coefficients deviation from identity",
                "position": 805
            },
            {
                "img": "https://arxiv.org/html/2512.06951/media/v_coeffs_deviation_heatmap.png",
                "caption": "(b)Value coefficients deviation from identity",
                "position": 810
            },
            {
                "img": "https://arxiv.org/html/2512.06951/media/attention_mask_training.png",
                "caption": "Figure 4:Attention mask structure during training. Blue indicates bidirectional attention, white indicates no attention, and diagonal patterns indicate causal attention.",
                "position": 865
            }
        ]
    },
    {
        "header": "5‚ÄÉ‚ÄäCorrelated Noise for Flow Matching",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06951/media/actions_correlation_matrix.png",
                "caption": "Figure 5:Action correlation matrixùö∫‚àà‚Ñù690√ó690\\boldsymbol{\\Sigma}\\in\\mathbb{R}^{690\\times 690}computed from training data (30 timesteps√ó\\times23 action dimensions = 690). The matrix shows strong block-diagonal structure indicating high temporal correlation and significant cross-dimensional correlation.",
                "position": 1025
            }
        ]
    },
    {
        "header": "6‚ÄÉ‚ÄäTraining",
        "images": []
    },
    {
        "header": "7‚ÄÉ‚ÄäInference Optimizations",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06951/media/inpainting_diagram.png",
                "caption": "Figure 6:Rolling window soft inpainting across two consecutive predictions. The first prediction generates 30 actions (blue circles), executes 26, and saves the last 4 (red circles) for inpainting. The second prediction (green squares) starts close to the saved actions through soft inpainting.",
                "position": 1244
            },
            {
                "img": "https://arxiv.org/html/2512.06951/media/compression_diagram.png",
                "caption": "Figure 7:Action compression via cubic spline interpolation. The original 26 actions (blue circles) are compressed to 20 steps (purple crosses), achieving 1.3√ó\\timesspeedup.",
                "position": 1247
            },
            {
                "img": "https://arxiv.org/html/2512.06951/media/stage_jump.png",
                "caption": "Figure 8:Stage tracking with voting logic resolves non-Markovian states.Top: Current stage after voting shows smooth, stable progression.Bottom: Raw model predictions are noisy and sometimes jump ahead (e.g., predicting stage 9 when still in early stages). In the radio task example, the robot grasping the radio at the beginning (stage 1-2) looks visually identical to placing it back later (stage 9), but System 2 correctly maintains stage 2 context despite model confusion.",
                "position": 1391
            }
        ]
    },
    {
        "header": "8‚ÄÉ‚ÄäEvaluation",
        "images": []
    },
    {
        "header": "9‚ÄÉ‚ÄäResults",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06951/media/per_task_and_eps_score.png",
                "caption": "Figure 9:Per-episode scores on public evaluation. Each row is a task (sorted by average duration in demo dataset), each column an episode (sorted by score). Dark green indicates success, light green - partial success, red indicates failure.",
                "position": 1693
            },
            {
                "img": "https://arxiv.org/html/2512.06951/media/reasons.png",
                "caption": "Figure 10:Distribution of failure reasons across labeled evaluation episodes. Dexterity issues (clumsiness in grasping/releasing) dominate, followed by progress/order errors and confusion from out-of-distribution states.",
                "position": 1729
            }
        ]
    },
    {
        "header": "10‚ÄÉ‚ÄäDiscussion and Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AStage-Task Fusion Details",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    },
    {
        "header": "Appendix CAction Expert Architecture",
        "images": []
    },
    {
        "header": "Appendix DFAST Auxiliary Training",
        "images": []
    }
]