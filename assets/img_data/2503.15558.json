[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15558/x1.png",
                "caption": "Figure 1:An overview of Cosmos-Reason1. Cosmos-Reason1 contains two multimodal large language models of 8B and 56B, trained in four stages, including vision pre-training, general SFT, Physical AI SFT, and Physical AI RL. We also define two ontologies for physical common sense and embodied reasoning, and build three benchmarks to evaluate models’ Physical AI reasoning capabilities.",
                "position": 146
            }
        ]
    },
    {
        "header": "2Physical AI Reasoning",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15558/x2.png",
                "caption": "Figure 2:A pie chart showing our physical common sense ontology. The ontology has three categories (Space, Time, and Fundamental Physics) and 16 fine-grained subcategories.",
                "position": 189
            }
        ]
    },
    {
        "header": "3Cosmos-Reason1",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15558/x3.png",
                "caption": "Figure 3:An illustration of our multimodal large language model architecture. Given an input video and an input text prompt, the video is projected into the LLM’s token embedding space as video tokens by a vision encoder followed by a projector. The text tokens are concatenated with the video tokens and fed into the LLM backbone, a hybrid Mamba-MLP-Transformer architecture. Our model can output responses with long chain-of-thought reasoning processes.",
                "position": 586
            },
            {
                "img": "https://arxiv.org/html/2503.15558/x4.png",
                "caption": "Figure 4:An illustration of our hybrid Mamba-MLP-Transformer backbone architecture. The middle sub-figure represents the 8B LLM backbone, and the bottom sub-figure depicts the 56B LLM backbone. A Transformer block consists of a self-attention layer and an MLP layer. We also show an example of Alternating Mamba-MLP module on top of the figure.",
                "position": 706
            }
        ]
    },
    {
        "header": "4Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15558/extracted/6291482/images/4_data/common_sense.jpg",
                "caption": "(a)",
                "position": 802
            },
            {
                "img": "https://arxiv.org/html/2503.15558/extracted/6291482/images/4_data/common_sense.jpg",
                "caption": "(a)",
                "position": 805
            },
            {
                "img": "https://arxiv.org/html/2503.15558/extracted/6291482/images/4_data/bridgev2.jpg",
                "caption": "(b)",
                "position": 811
            },
            {
                "img": "https://arxiv.org/html/2503.15558/extracted/6291482/images/4_data/robovqa.jpg",
                "caption": "(c)",
                "position": 817
            },
            {
                "img": "https://arxiv.org/html/2503.15558/extracted/6291482/images/4_data/agibot.jpg",
                "caption": "(d)",
                "position": 823
            },
            {
                "img": "https://arxiv.org/html/2503.15558/extracted/6291482/images/4_data/holoassist.jpg",
                "caption": "(e)",
                "position": 829
            },
            {
                "img": "https://arxiv.org/html/2503.15558/extracted/6291482/images/4_data/av.jpg",
                "caption": "(f)",
                "position": 835
            },
            {
                "img": "https://arxiv.org/html/2503.15558/x5.png",
                "caption": "Figure 6:Embodied reasoning SFT data curation pipeline. We demonstrate an illustrative example for AgiBot, where we (1) extract short horizon segments corresponding to the subtask, (2) caption the extracted clip to obtain state-action context, (3) curate QA pairs for “next plausible subtask prediction”, (4) prompt R1 with the question and caption to elicit reasoning, (5) clean and rewrite the reasoning trace to obtain valid SFT samples.",
                "position": 893
            }
        ]
    },
    {
        "header": "5Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15558/x6.png",
                "caption": "Figure 7:An illustration showing the categorical distribution of initial questions collected for physical common sense benchmark according to the ontology defined inTab.1. We select a subset of 604 questions as our evaluation benchmark.",
                "position": 1108
            }
        ]
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15558/extracted/6291482/images/6_3_rl/frame_0058.png",
                "caption": "Figure 8:Intriguing results before and after RL. When presented with an ambiguous question, we observe that after RL, our model learns to reject all provided options based on its knowledge.",
                "position": 1613
            },
            {
                "img": "https://arxiv.org/html/2503.15558/extracted/6291482/images/6_3_rl/frame_0072.png",
                "caption": "",
                "position": 1623
            },
            {
                "img": "https://arxiv.org/html/2503.15558/extracted/6291482/images/6_3_rl/frame_0091.png",
                "caption": "",
                "position": 1624
            },
            {
                "img": "https://arxiv.org/html/2503.15558/extracted/6291482/images/6_3_rl/aot_frame_1.png",
                "caption": "Figure 9:While the model before RL struggles to understand and associate perception with reverse actions, RL enables the model to reason through time while avoiding distractions, such as stationary text.",
                "position": 1670
            },
            {
                "img": "https://arxiv.org/html/2503.15558/extracted/6291482/images/6_3_rl/aot_frame_14.png",
                "caption": "",
                "position": 1680
            },
            {
                "img": "https://arxiv.org/html/2503.15558/extracted/6291482/images/6_3_rl/aot_frame_16.png",
                "caption": "",
                "position": 1681
            },
            {
                "img": "https://arxiv.org/html/2503.15558/extracted/6291482/images/6_3_rl/aot_frame_29.png",
                "caption": "",
                "position": 1682
            }
        ]
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Appendix APrompts Used for Data Curation",
        "images": []
    },
    {
        "header": "Appendix BAdditional results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15558/extracted/6291482/images/6_3_rl/puzzle_frame_0001.jpg",
                "caption": "Figure 10:While the model before RL mistakenly associates spatial questions with temporal reasoning, RL enables the model to identify key features of the first frame and compare them with subsequent frames.",
                "position": 1981
            },
            {
                "img": "https://arxiv.org/html/2503.15558/extracted/6291482/images/6_3_rl/puzzle_frame_0002.png",
                "caption": "",
                "position": 1996
            },
            {
                "img": "https://arxiv.org/html/2503.15558/extracted/6291482/images/6_3_rl/puzzle_frame_0009.png",
                "caption": "",
                "position": 1997
            },
            {
                "img": "https://arxiv.org/html/2503.15558/extracted/6291482/images/6_3_rl/puzzle_frame_0011.jpg",
                "caption": "",
                "position": 1998
            },
            {
                "img": "https://arxiv.org/html/2503.15558/extracted/6291482/images/6_3_rl/puzzle_frame_0014.png",
                "caption": "",
                "position": 2005
            },
            {
                "img": "https://arxiv.org/html/2503.15558/extracted/6291482/images/6_3_rl/puzzle_frame_0017.png",
                "caption": "",
                "position": 2006
            },
            {
                "img": "https://arxiv.org/html/2503.15558/extracted/6291482/images/6_3_rl/puzzle_frame_0024.png",
                "caption": "",
                "position": 2007
            },
            {
                "img": "https://arxiv.org/html/2503.15558/extracted/6291482/images/6_3_rl/puzzle_frame_0030.png",
                "caption": "",
                "position": 2008
            },
            {
                "img": "https://arxiv.org/html/2503.15558/extracted/6291482/images/6_3_rl/object_frame_0001.png",
                "caption": "Figure 11:While the model before RL fails to reason about object permanence through long CoT, RL enables the model to correctly infer that the object’s disappearance is not due to camera movement, using concise and direct reasoning.",
                "position": 2046
            },
            {
                "img": "https://arxiv.org/html/2503.15558/extracted/6291482/images/6_3_rl/object_frame_0006.png",
                "caption": "",
                "position": 2056
            },
            {
                "img": "https://arxiv.org/html/2503.15558/extracted/6291482/images/6_3_rl/object_frame_0008.png",
                "caption": "",
                "position": 2057
            },
            {
                "img": "https://arxiv.org/html/2503.15558/extracted/6291482/images/6_3_rl/object_frame_0011.png",
                "caption": "",
                "position": 2058
            },
            {
                "img": "https://arxiv.org/html/2503.15558/extracted/6291482/images/6_3_rl/object_frame_0015.png",
                "caption": "",
                "position": 2059
            },
            {
                "img": "https://arxiv.org/html/2503.15558/extracted/6291482/images/6_3_rl/object_frame_0020.png",
                "caption": "",
                "position": 2060
            },
            {
                "img": "https://arxiv.org/html/2503.15558/extracted/6291482/images/6_3_rl/object_frame_0024.png",
                "caption": "",
                "position": 2061
            },
            {
                "img": "https://arxiv.org/html/2503.15558/extracted/6291482/images/6_3_rl/object_frame_0030.png",
                "caption": "",
                "position": 2062
            }
        ]
    },
    {
        "header": "Appendix CContributors and Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]