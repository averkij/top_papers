[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.03983/",
                "caption": "",
                "position": 61
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.03983/x2.png",
                "caption": "Figure 1:Audio Flamingo 2 versus previous SOTA ALMs on audio understanding and reasoning benchmarks (values normalized). AF2 outperforms all baselines and has smaller model footprints.",
                "position": 104
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.03983/x3.png",
                "caption": "Figure 2:Overview ofAudio Flamingo 2’s cross-attention architecture and three-stage curriculum training.",
                "position": 157
            }
        ]
    },
    {
        "header": "3Audio Flamingo 2 Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.03983/x4.png",
                "caption": "Figure 3:Illustration of AF-CLAP training process. We collect long and short videos, segment it into 10-second clips, caption it, and prompt an LLM to generate audio captions. The data is then used to train CLAP with a modified contrastive loss.",
                "position": 217
            },
            {
                "img": "https://arxiv.org/html/2503.03983/x5.png",
                "caption": "Figure 4:Examples fromAudioSkills. Compared to other AQA datasets, questions in AudioSkills require deliberate reasoning.",
                "position": 277
            }
        ]
    },
    {
        "header": "4Audio Flamingo 2 Training Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.03983/x6.png",
                "caption": "Figure 5:The proportion of video categories (for sourcing audios) (left), question categories (middle), and distribution of durations (right) for the LongAudio dataset with 262,928 unique AQA and 80k unique audios. We target captioning and reasoning tasks.",
                "position": 344
            },
            {
                "img": "https://arxiv.org/html/2503.03983/x7.png",
                "caption": "Figure 6:The pipeline for generatingLongAudio. The process begins by segmenting the long video into short video and audio clips, each≈\\approx≈10 seconds. These\nclips are individually annotated with captions. Subsequently, an LLM\nis employed to generate question-and-answer pairs based on the captions of these clips. A subset of the data goes through expert review to constructLongAudioBench.",
                "position": 347
            }
        ]
    },
    {
        "header": "5Audio Flamingo 2 Training Strategy",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.03983/x8.png",
                "caption": "Figure 7:Performance comparison of AF2 on different LLM sizes, w/ and w/o AudioSkills. More results in TableLABEL:tab:8col_4rows.",
                "position": 798
            }
        ]
    },
    {
        "header": "6Experiments",
        "images": []
    }
]