[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13726/x1.png",
                "caption": "Figure 1:The pipeline of DAWN. First, we train the Latent Flow Generator (LFG) in (a) to extract the motion representation from the video. Then the Pose and Blink generation Network (PBNet) in (b) is utilized to generate the head pose and blink sequences of the avatar. Subsequently, the Audio-to-Video Flow Diffusion Model (A2V-FDM) in (c) generates the talking head video from the source image conditioned by the audio and pose/blink sequences provided by the PBNet.",
                "position": 158
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13726/x2.png",
                "caption": "Figure 2:Qualitative comparison with several state-of-the-art methods methods on HDTF(Zhang et al.,2021)and CREMA(Cao et al.,2014)datasets. Our method produces higher-quality results in video quality, lip-sync consistency, identity preservation, and head motions.",
                "position": 514
            },
            {
                "img": "https://arxiv.org/html/2410.13726/x3.png",
                "caption": "Figure 3:Visualization of cross-identity reenactment. We extract the audio, head pose, and blink signals from the video in the first row, and use them to drive the source image, generating the talking head video in the second row.",
                "position": 735
            }
        ]
    },
    {
        "header": "5conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13726/x4.png",
                "caption": "Figure 4:The qualitative study on higher resolution (256×256256256256\\times 256256 × 256) and different portrait styles.",
                "position": 1594
            },
            {
                "img": "https://arxiv.org/html/2410.13726/extracted/5936384/time_cost_2.png",
                "caption": "Figure 5:The comparison experiment on generation time cost. The “*\" refers to diffusion-based methods.",
                "position": 1614
            },
            {
                "img": "https://arxiv.org/html/2410.13726/x5.png",
                "caption": "Figure 6:The visualization of the ablation study on PBNet demonstrates different methodologies. The term “w/o PBNet\" indicates\n“without PBNet\", whereby the A2V-FDM is utilized to infer pose and blink movements. Conversely, “w PBNet\" signifies the “with PBNet\", which directly generates explicit pose and blink signals to control the generation of A2V-FDM.",
                "position": 1694
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]