[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background and Motivation",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13525/x1.png",
                "caption": "Figure 1:Two distributions of expert activation (left) and latency of an MoE layer under the activation patterns (right).",
                "position": 214
            },
            {
                "img": "https://arxiv.org/html/2512.13525/x2.png",
                "caption": "Figure 2:Performance of MoE and attention. Latency of an MoE layer under various number of activated experts (left) and the comparison between MoE and attention layers (right).",
                "position": 252
            }
        ]
    },
    {
        "header": "3System Design",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13525/x3.png",
                "caption": "Figure 3:Architecture overview ofJanus.",
                "position": 337
            },
            {
                "img": "https://arxiv.org/html/2512.13525/x4.png",
                "caption": "Figure 4:Comparison between a strawman solution (left) and adaptive two-phase communication (middle and right).",
                "position": 370
            },
            {
                "img": "https://arxiv.org/html/2512.13525/x5.png",
                "caption": "Figure 5:Scheduling workflow ofJanus.",
                "position": 406
            }
        ]
    },
    {
        "header": "4Implementation",
        "images": []
    },
    {
        "header": "5Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13525/x6.png",
                "caption": "Figure 6:Latency of an attention (top) and MoE layer (bottom) under varying batch sizes and degree of parallelism.",
                "position": 759
            },
            {
                "img": "https://arxiv.org/html/2512.13525/x7.png",
                "caption": "Figure 7:TPOT and per-GPU throughput of DeepSeek-V2 under different batch sizes. The TPOT SLO is fixed at 200 ms, requiring all systems to scale as load increases. Annotations (e.g., 1A6E) indicate the optimal configuration of attention (A) and expert (E) instances selected byJanus.",
                "position": 777
            },
            {
                "img": "https://arxiv.org/html/2512.13525/x8.png",
                "caption": "Figure 8:Normalized TPOT under various model variants and batch sizes.",
                "position": 809
            },
            {
                "img": "https://arxiv.org/html/2512.13525/x9.png",
                "caption": "Figure 9:Simulation of scaling decisions forJanusand SGLang under real-world workloads.",
                "position": 827
            },
            {
                "img": "https://arxiv.org/html/2512.13525/x10.png",
                "caption": "Figure 10:Performance breakdown ofJanus’s key designs.",
                "position": 834
            },
            {
                "img": "https://arxiv.org/html/2512.13525/x11.png",
                "caption": "Figure 11:Difference in the number of activated experts between the most- and least-activated instances.",
                "position": 837
            },
            {
                "img": "https://arxiv.org/html/2512.13525/x12.png",
                "caption": "Figure 12:Overhead ofJanus’s scheduling.",
                "position": 863
            }
        ]
    },
    {
        "header": "6Discussion and Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    }
]