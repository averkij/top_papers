[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23709/x1.png",
                "caption": "",
                "position": 116
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23709/x2.png",
                "caption": "Figure 2:Overview of Auto-regressive Temporal-aware Decoder.Given the denoised latent and warped previous frame, our decoder enhances temporal consistency using temporal processor modules. This module aligns and fuses these features via interpolation, convolution, and weighted fusion, effectively stabilizing detail reconstruction when decoding into the final RGB frame.",
                "position": 346
            },
            {
                "img": "https://arxiv.org/html/2512.23709/x3.png",
                "caption": "Figure 3:Training pipeline of Stream-DiffVSR.The training process consists of three sequential stages: (1) Distilling the denoising U-Net to reduce diffusion steps while maintaining perceptual quality with training objective (1); (2) Training the Temporal Processor Module (TPM) within the decoder to enhance temporal consistency at the RGB level with training objective (3); (3) Training the Auto-Regressive Temporal Guidance (ARTG) module to leverage previously restored high-quality frames for improved temporal coherence with training objective (6). Each module is trained separately before integrating them into the final framework.",
                "position": 398
            },
            {
                "img": "https://arxiv.org/html/2512.23709/x4.png",
                "caption": "Figure 4:Overview of our pipeline.Given a low-quality (LQ) input frame, we first initialize its latent representation and employ an autoregressive diffusion model composed of a distilled denoising U-Net, autoregressive temporal Guidance, and an autoregressive temporal Decoder. Temporal guidance utilizes flow-warped high-quality (HQ) results from the previous frame to condition the current frame’s latent denoising and decoding processes, significantly improving perceptual quality and temporal consistency in an efficient, online manner.",
                "position": 401
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23709/x5.png",
                "caption": "Figure 5:Qualitative comparison on REDS4 and Vimeo-90K-T datasets.Our method demonstrates superior visual quality with sharper details compared to unidirectional methods (TMP[99], RealViformer[98]) and competitive performance against bidirectional methods (StableVSR[58], MGLD-VSR[89], RVRT[37], BasicVSR++[5], RealBasicVSR[6]). Improvements include reduced artifacts and enhanced temporal stability (see zoomed patches).",
                "position": 1201
            },
            {
                "img": "https://arxiv.org/html/2512.23709/x6.png",
                "caption": "Figure 6:Ablation study on the Temporal Processor Module (TPM).Integrating TPM improves motion stability and reduces temporal artifacts by leveraging warped previous-frame features, enhancing temporal consistency in video super-resolution.",
                "position": 1523
            },
            {
                "img": "https://arxiv.org/html/2512.23709/x6.png",
                "caption": "Figure 6:Ablation study on the Temporal Processor Module (TPM).Integrating TPM improves motion stability and reduces temporal artifacts by leveraging warped previous-frame features, enhancing temporal consistency in video super-resolution.",
                "position": 1526
            },
            {
                "img": "https://arxiv.org/html/2512.23709/x7.png",
                "caption": "Figure 7:Ablation study on inference steps.The 4-step model yields the best quality–efficiency trade-off, validating our distillation strategy.",
                "position": 1532
            },
            {
                "img": "https://arxiv.org/html/2512.23709/x7.png",
                "caption": "Figure 7:Ablation study on inference steps.The 4-step model yields the best quality–efficiency trade-off, validating our distillation strategy.",
                "position": 1535
            },
            {
                "img": "https://arxiv.org/html/2512.23709/x8.png",
                "caption": "Figure 8:Ablation study on Auto-regressive Temporal Guidance (ARTG).ARTG enhances temporal consistency and perceptual quality by leveraging warped previous frames, reducing flickering, and improving structural coherence.",
                "position": 1542
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Overview",
        "images": []
    },
    {
        "header": "Appendix AExperimental Setup",
        "images": []
    },
    {
        "header": "Appendix BAdditional Implementation Details",
        "images": []
    },
    {
        "header": "Appendix CAdditional Training Detials",
        "images": []
    },
    {
        "header": "Appendix DAdditional Quantitative comparison.",
        "images": []
    },
    {
        "header": "Appendix EAdditional Visual Result",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23709/x9.png",
                "caption": "Figure 9:Qualitative comparison with Upscale-A-Video (UAV).Due to GPU memory limitations (OOM on an RTX 4090), we use UAV results extracted from its official project video for qualitative comparison. Despite this constraint, our Stream-DiffVSR exhibits superior visual fidelity and temporal consistency across frames.",
                "position": 4423
            },
            {
                "img": "https://arxiv.org/html/2512.23709/x10.png",
                "caption": "Figure 10:Additional visual results.",
                "position": 4426
            },
            {
                "img": "https://arxiv.org/html/2512.23709/x11.png",
                "caption": "Figure 11:Additional visual results.",
                "position": 4429
            },
            {
                "img": "https://arxiv.org/html/2512.23709/x12.png",
                "caption": "Figure 12:Additional visual results.",
                "position": 4432
            },
            {
                "img": "https://arxiv.org/html/2512.23709/x13.png",
                "caption": "Figure 13:Temporal consistency comparison.Qualitative comparison of temporal consistency across consecutive frames. Our proposed Stream-DiffVSR effectively mitigates flickering artifacts and maintains stable texture reconstruction, demonstrating superior temporal coherence compared to existing VSR methods.",
                "position": 4435
            },
            {
                "img": "https://arxiv.org/html/2512.23709/x14.png",
                "caption": "Figure 14:Optical flow visualization comparison.Visualization of optical flow consistency across different VSR methods. Our proposed Stream-DiffVSR produces smoother and more temporally coherent flow fields, indicating improved motion consistency and reduced temporal artifacts compared to competing approaches.",
                "position": 4438
            },
            {
                "img": "https://arxiv.org/html/2512.23709/x15.png",
                "caption": "Figure 15:Limitation on the first frame without temporal context.Our method may underperform on the first frame of a video sequence due to the absence of prior temporal information. This limitation is inherent to online VSR settings, where no past frames are available for guidance.",
                "position": 4457
            }
        ]
    },
    {
        "header": "Appendix FFailure cases",
        "images": []
    }
]