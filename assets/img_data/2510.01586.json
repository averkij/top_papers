[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary",
        "images": []
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01586/x1.png",
                "caption": "Figure 1:Framework. AdvEvo-MARL begins by warming up attacker agents through supervised fine-tuning to embed prior knowledge of jailbreak behaviors. Then, attackers and defenders learn to co-evolve via adversarial multi-agent reinforcement learning. During policy updates, agents within the same functional group (i.e., attackers or defenders) leverage a public baseline which is computed as the mean return of their respective group to estimate their individual advantages for optimization.",
                "position": 187
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01586/x2.png",
                "caption": "Figure 2:Task benchmark performance. AdvEvo-MARL exhibits minimal degradation and even improved results.",
                "position": 906
            },
            {
                "img": "https://arxiv.org/html/2510.01586/x3.png",
                "caption": "Figure 3:Performance variations under different training configurations.Left:robustness performance, AdvEvo-MARL consistently maintains the lowest ASR,Right:task performance, AdvEvo-MARL improves task utility across all settings, reaching a maximum 4% gain on LiveCodeBench.",
                "position": 917
            },
            {
                "img": "https://arxiv.org/html/2510.01586/x4.png",
                "caption": "Figure 4:Attacker-generated prompts Diversity.",
                "position": 934
            },
            {
                "img": "https://arxiv.org/html/2510.01586/charts/figs/accuracy_smoothed.png",
                "caption": "((a))Evaluation accuracy.",
                "position": 944
            },
            {
                "img": "https://arxiv.org/html/2510.01586/charts/figs/accuracy_smoothed.png",
                "caption": "((a))Evaluation accuracy.",
                "position": 947
            },
            {
                "img": "https://arxiv.org/html/2510.01586/charts/figs/asr_smoothed.png",
                "caption": "((b))Attack success rate.",
                "position": 952
            },
            {
                "img": "https://arxiv.org/html/2510.01586/charts/figs/attacker_reward_smoothed.png",
                "caption": "((c))Attacker Reward.",
                "position": 957
            },
            {
                "img": "https://arxiv.org/html/2510.01586/charts/figs/defender_reward_smoothed.png",
                "caption": "((d))Defender Reward.",
                "position": 963
            },
            {
                "img": "https://arxiv.org/html/2510.01586/charts/figs/response_length_smoothed.png",
                "caption": "((e))Response Length.",
                "position": 968
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]