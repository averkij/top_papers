[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.10443/x1.png",
                "caption": "Figure 1:Comparison of Visual and Audio Encoding in Video Modeling.(a) Existing methods encode each modality separately and then concatenate them, leading to inconsistencies and difficulties in handling long videos. (b) We propose Temporal Dynamic Context (TDC) compression, which incorporates both static visual features and dynamic video context to represent videos more effectively. This approach enables better multimodal integration and efficient compression for long videos.",
                "position": 76
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.10443/x2.png",
                "caption": "Figure 2:Architecture of Our Multimodal Video Encoder.We first extract features for each second of the video, including both visual and corresponding audio tokens. The first frame is selected as the static frame, and a Q-Former is used to perform Temporal Dynamic Context compression based on its relationship with subsequent frames, resulting inKùêæKitalic_Kcompressed tokens per frame. The final video representation consists of all static frame tokens and multimodal video context.",
                "position": 129
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.10443/x3.png",
                "caption": "Figure 3:Qualitative Demonstrations of Our 7B Model.(a) Our model can uniformly comprehend both audio and visual information, demonstrating strong performance in audio-visual dialogue tasks. (b) In movie description tasks, it can generate detailed descriptions of both the plot and visual elements. For extremely long videos, our LVCoT processes them segment by segment. The generated segment information, along with the timeline, serves as part of the reasoning process, enriching the final output with more details.",
                "position": 889
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix AAppendix Overview",
        "images": []
    },
    {
        "header": "Appendix BAdditional evaluations",
        "images": []
    },
    {
        "header": "Appendix CExperimental details",
        "images": []
    },
    {
        "header": "Appendix DLimitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]