[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.19652/figures/abstract.drawio-1.png",
                "caption": "",
                "position": 145
            },
            {
                "img": "https://arxiv.org/html/2508.19652/figures/github.png",
                "caption": "",
                "position": 148
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.19652/x1.png",
                "caption": "Figure 1:Overall framework ofVision-SR1. During RL training, the VLM performs two rollouts. In the first pass, the model takes an imageâ€“query pair and generates a structured output (visual perception, CoT reasoning, and answer), with answer reward computed against the ground truth. In the second pass, the model is re-prompted to answer using only query and its generated visual perception. If the correct answer is derived, a self-visual reward is assigned.",
                "position": 341
            },
            {
                "img": "https://arxiv.org/html/2508.19652/figures/data_curation.drawio-1.png",
                "caption": "Figure 2:We prompt Qwen-2.5-VL-7B to create the SFT cold-start dataset to learn the ideal format to get a quick start in the RL stage to reduce training steps and training time. Our cold start SFT dataset is the intersection of three source datasets, creating a final set of 9K examples. Our filtration process in Sec2.4guarantees zero false positives across text-only, caption-based, and direct CoT reasoning.",
                "position": 461
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": []
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperiment Details",
        "images": []
    }
]