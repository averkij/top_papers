[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": []
    },
    {
        "header": "IIRelated Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13579/x1.png",
                "caption": "Figure 2:Tactile Alignment Overview.Our method consists of two stages: self-supervised representation learning and cross-embodiment alignment via pseudo-pairs. We use a learnable length-1 query between the encoder and decoder to produce a fixed-dimensional latent representation via cross-attention pooling. A learnable length 1 query is implemented between the encoder and decoder to output a fixed-dimensional latent representations after the cross-attention module. In step2, we aggregate the learned latents from both domains to construct pseudo-pairs(h∗,r∗)(h^{*},r^{*}), and learn a velocity fieldvθv_{\\theta}that transports the glove latent distribution to the robot latent distribution.",
                "position": 223
            }
        ]
    },
    {
        "header": "IIIMethodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13579/x2.png",
                "caption": "Figure 3:Red and blue indicate two subsets of the source distribution; training uses the provided pairs (lines), with colors preserved atα=0.2\\alpha=0.2for the target samples associated with each pair (left of each panel) and their transformed targets (right of each panel).First:Standard rectified flow[23]learns a low-cost transport between two distributions by training on randomly.Second:We propose using pseudo-pairs to the rectified flow for guiding the velocity field toward desired correspondences between the source and target distributions.Third:Despite noise in the pseudo-pairs, the learned rectified flow remains robust and converges to an efficient transport map between the two distributions.",
                "position": 284
            },
            {
                "img": "https://arxiv.org/html/2602.13579/x3.png",
                "caption": "Figure 4:H2R Action Policy.Given either human or robot inputs, the shared policy follows a color-coded structure, representingrobot,human, andsharedmodules. Human glove latent features are passed into an ODE solver via a learned velocity field. The proprioceptive encoder takes fingertip locations inyellow dotsand wrist orientation. Only theyellowmodules are trained; all others are frozen.",
                "position": 375
            }
        ]
    },
    {
        "header": "IVExperiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13579/x4.png",
                "caption": "Figure 5:Tactile Features UMAP Projections.First:Rectified flow maps the glove latent distribution to overlap with the robot distribution.Second & Third:Colors denote normalized raw tactile magnitude (0: no contact, 1: highest force/shear), computed separately for glove and robot data. As indicated by the arrows, the alignment exhibits a consistent cross-domain trend in contact force magnitudes, even though force is not used during training.",
                "position": 448
            },
            {
                "img": "https://arxiv.org/html/2602.13579/x5.png",
                "caption": "Figure 6:Pivoting Task.The task begins in a non-contact state and transitions to pivoting upon contact detection via tactile feedback, with the goal of maintaining contact without dropping the object.Top: training demonstrations.Bottom: robot policy rollouts.",
                "position": 451
            },
            {
                "img": "https://arxiv.org/html/2602.13579/x6.png",
                "caption": "Figure 7:Insertion Task.With randomized grasps, the policy leverages tactile feedback to perform search, alignment, and insertion of the adapter into the outlet. We show human demonstrations helps generalization to unseen adapters across variations in geometry, size, and mass.",
                "position": 902
            },
            {
                "img": "https://arxiv.org/html/2602.13579/media/lid_closing.png",
                "caption": "Figure 8:Lid Closing Task.With randomized grasps, the policy uses touch to perform search, alignment, and closing between the lid and the bottle. We show human data improves generalization to unseen objects with varying lid and bottle geometries, sizes, and masses.",
                "position": 905
            },
            {
                "img": "https://arxiv.org/html/2602.13579/x7.png",
                "caption": "Figure 9:Light Bulb Screwing Task.TactAlign learns a dexterous tactile policy using only human demonstrations, with zero robot data.",
                "position": 914
            },
            {
                "img": "https://arxiv.org/html/2602.13579/x8.png",
                "caption": "Figure 10:ℓ1\\ell_{1}force prediction error (mean±\\pmstd) along each axis, averaged over five evaluations.G→RG\\rightarrow Revaluates force prediction on the robot using human tactile signals, with (blue) and without alignment (red) alignment. With alignment reduces the force prediction error by 96.75% across the three force axes.R→RR\\rightarrow Rdenotes the robot-to-robot baseline (yellow), representing a best-case scenario.",
                "position": 946
            }
        ]
    },
    {
        "header": "VLimitation",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13579/x9.png",
                "caption": "Figure 11:Our human dataset leverages OSMO[45]tactile glove, providing the human demonstrator with full dexterity while capturing both shear and normal tactile signals.",
                "position": 1607
            },
            {
                "img": "https://arxiv.org/html/2602.13579/media/force_data_collection.png",
                "caption": "Figure 12:Robot and human force label collection using an F/T sensor (ATI Gamma) fixed beneath the table.",
                "position": 1626
            },
            {
                "img": "https://arxiv.org/html/2602.13579/media/appendix/force_dataset.png",
                "caption": "Figure 13:Force dataset distribution",
                "position": 1634
            },
            {
                "img": "https://arxiv.org/html/2602.13579/x10.png",
                "caption": "Figure 14:Red boxes denote ‘Seen-by-all’ objects, blue boxes denote ‘Human-only’ objects, and green boxes denote ‘Held-out’ objects for each task in Tab.II.",
                "position": 1640
            }
        ]
    },
    {
        "header": "Appendix BPost Processing",
        "images": []
    },
    {
        "header": "Appendix CPseudo-pairs",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13579/media/norm_hist.png",
                "caption": "Figure 15:Norm of raw sensor signal observations from the human glove[45]and the robot Xela sensor. The red dotted vertical line indicates the chosen non-contact threshold.",
                "position": 1892
            }
        ]
    },
    {
        "header": "Appendix DTraining and Architecture Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13579/x11.png",
                "caption": "Figure 16:How the tactile latent distribution of human’s evolve with time via rectified flow.",
                "position": 1955
            }
        ]
    },
    {
        "header": "Appendix EInference",
        "images": []
    }
]