[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11096/x1.png",
                "caption": "Figure 1:Multi-subject animations generated byCoDance.Given a single (potentially misaligned) driving pose sequence and one multi-subject reference image,CoDanceproduces coordinated, pose-controllablegroup danceswithout per-subject spatial alignment.",
                "position": 78
            },
            {
                "img": "https://arxiv.org/html/2601.11096/x2.png",
                "caption": "Figure 2:The illustration ofCoDancemotivation.Although excelling at single-person animation, prior methods fail when handling multiple subjects due to a rigid binding between the reference and target pose, which results in mismatched outputs. By contrast, our Unbind-Rebind method successfully decouples motion from appearance, yielding compelling results.",
                "position": 81
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11096/x3.png",
                "caption": "Figure 3:The pipeline ofCoDance.Given a reference imageIrI^{r}, a driving pose sequenceI1:FpI^{p}_{1:F}, a text promptùíØ\\mathcal{T}, and a subject mask‚Ñ≥\\mathcal{M}, our model generates an animation videoI1:FgI^{g}_{1:F}.\nA VAE encoder extracts the latent featureferf^{r}_{e}fromIrI^{r}.\nThe Unbind module, implemented as aPose Shift Encoder, processesI1:FpI^{p}_{1:F}to produce pose features.\nThese are concatenated with patchified tokens from the noisy latent input for the DiT backbone.\nThe Rebind module provides dual guidance:semantic featuresfrom a umT5 text encoder are injected via cross-attention, whilespatial featuresfrom a Mask Encoder are added element-wise to the noisy latent. To bolster the model‚Äôs semantic comprehension, the training process alternates between animation data (with probabilitypanip_{\\text{ani}}) and a diverse text-to-video dataset (with probability1‚àípani1-p_{\\text{ani}}).\nThe DiT is initialized from a pretrained T2V model and fine-tuned using LoRA.\nFinally, a VAE decoder reconstructs the video. Note that the Unbind module and mixed-data training are applied exclusively during the training phase.",
                "position": 152
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11096/x4.png",
                "caption": "Figure 4:Qualitative comparisons with SOTA methods.",
                "position": 484
            },
            {
                "img": "https://arxiv.org/html/2601.11096/x5.png",
                "caption": "Figure 5:Qualitative results of ablation study.",
                "position": 575
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]