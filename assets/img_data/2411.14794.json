[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14794/x1.png",
                "caption": "",
                "position": 62
            },
            {
                "img": "https://arxiv.org/html/2411.14794/x2.png",
                "caption": "",
                "position": 98
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14794/x3.png",
                "caption": "Figure 2:The automatic generation pipeline ofVideoEspresso.(i)Question-Answer Pair Construction: We use video frame-leveled captions to extract the key frames of the video and group descriptions of these frames. Then, we prompt GPT-4 to design questions for each group of video frames. (ii)Multimodal Chain-of-Thought Annotation: We extract key evidence text and generate captions with the highest relevance to the question with GPT-4o. Additionally, we annotate spatial and temporal information for key items, which results in multimodal Chain of Thought data pairs grounded in both temporal and spatial dimensions.",
                "position": 127
            }
        ]
    },
    {
        "header": "3VideoEspresso",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14794/x4.png",
                "caption": "Figure 3:The statistical analysis of ourVideoEspressodataset.",
                "position": 161
            },
            {
                "img": "https://arxiv.org/html/2411.14794/x5.png",
                "caption": "Figure 4:The dataset attributes comparison between ourVideoEspressoand MVbench.",
                "position": 197
            },
            {
                "img": "https://arxiv.org/html/2411.14794/x6.png",
                "caption": "Figure 5:Two-Stage Video Evidence of Thought Training Procedure.The Frame Selector comprises a tiny LVLM and a tiny LLM, tasked with generating captions for videos and selecting the most relevant frame to as core video token for large reasoning model. A two-stage supervised fine-tuning technique is employed. During stage-1, a set of cue prompts is introduced to guide the model in producing evidence, while in stage-2, the evidence generated from stage-1 is concatenated and used directly to guide the answer generation.",
                "position": 205
            }
        ]
    },
    {
        "header": "4Hybrid LVLMs Collaboration for VideoQA",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails of VideoEspresso",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14794/x7.png",
                "caption": "Figure 6:ComparisonbetweenVideoEspressoand other VideoQA dataset.",
                "position": 1926
            }
        ]
    },
    {
        "header": "Appendix BTraining Implementation",
        "images": []
    },
    {
        "header": "Appendix CPrompt Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14794/x8.png",
                "caption": "Figure 7:QA-Construction Prompt.",
                "position": 1943
            },
            {
                "img": "https://arxiv.org/html/2411.14794/x9.png",
                "caption": "Figure 8:QA-Filter Prompt.",
                "position": 1946
            },
            {
                "img": "https://arxiv.org/html/2411.14794/x10.png",
                "caption": "Figure 9:CoT-Evidence Construction Prompt.",
                "position": 1949
            },
            {
                "img": "https://arxiv.org/html/2411.14794/x11.png",
                "caption": "Figure 10:Subjective Evaluation Prompt.",
                "position": 1952
            }
        ]
    },
    {
        "header": "Appendix DEvaluation Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14794/x12.png",
                "caption": "Figure 11:Example of test set.RùëÖRitalic_Rrepresent the Reference Answer, whileDisubscriptùê∑ùëñD_{i}italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTstand for theiùëñiitalic_i-th Distractor.",
                "position": 2022
            },
            {
                "img": "https://arxiv.org/html/2411.14794/extracted/6017056/pic/Supp_opt_len.png",
                "caption": "Figure 12:The Distributionof token length disparities between reference answers and the longest distractor option.",
                "position": 2025
            },
            {
                "img": "https://arxiv.org/html/2411.14794/x13.png",
                "caption": "Figure 13:Example of over-analysis with GPT-4o.",
                "position": 2035
            },
            {
                "img": "https://arxiv.org/html/2411.14794/x14.png",
                "caption": "Figure 14:Example of Non-factual response with GPT-4o.",
                "position": 2038
            }
        ]
    },
    {
        "header": "Appendix ECase Study",
        "images": []
    }
]