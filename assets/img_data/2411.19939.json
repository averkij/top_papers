[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19939/x1.png",
                "caption": "Figure 1:Overview of our work. We have discovered a problem in current multimodal safety data samples, which says visual safety information leakage (VSIL). Based on this leakage, we further find it leads to a counter-intuitive problem, that simpler SFT-based alignment methods can perform nearly the same high safety rate. Thus, we construct VLSBench, preventing visual leakage. This newly proposed task discourages textual alignment and motivates more dedicated multimodal alignment methods to better solve this challenging task. The red bar shows evaluation results separately on the raw and jailbreak set of JailbreakV[34], a typical dataset with VSIL. The green bar shows evaluation results on our VLSBench.",
                "position": 156
            }
        ]
    },
    {
        "header": "2Visual Leakage in Multimodal Safety",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19939/x2.png",
                "caption": "Figure 2:Four examples in current benchmarks to showcase the problem of visual safety information leakage. The leakage information from visual to textual is marked as red.",
                "position": 277
            },
            {
                "img": "https://arxiv.org/html/2411.19939/x3.png",
                "caption": "Figure 3:Safety taxonomy of our VLSBench.",
                "position": 504
            },
            {
                "img": "https://arxiv.org/html/2411.19939/x3.png",
                "caption": "Figure 3:Safety taxonomy of our VLSBench.",
                "position": 507
            }
        ]
    },
    {
        "header": "3VLSBench Construction Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19939/x4.png",
                "caption": "Figure 5:Overview of VLSBench construction pipeline. Our pipeline features prevent visual leakage. This pipeline includes four steps:(a)Harmful query and image description generation.(b)M: Mitigating visual leakage from textual harmful query.(c)Iterative image generation from image description.(d)Final filtration ensuring image-text pairs are matched and harmful.",
                "position": 753
            }
        ]
    },
    {
        "header": "4Benchmark Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19939/x5.png",
                "caption": "Figure 6:Examples of our dataset across our three evaluation labels. We give an image-text pair and corresponding response evaluated asSafe with Refuse,Safe with WarningandUnsafe.",
                "position": 904
            },
            {
                "img": "https://arxiv.org/html/2411.19939/x6.png",
                "caption": "Figure 7:Textual SFT compared with Multimodal SFT on our VSLBench. Dash lines mean the average safety rate on the three base models.",
                "position": 907
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "AVSIL Examples from Existing Datasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19939/x7.png",
                "caption": "Figure 8:Selected examples with VSIL:(a)-(e)is from JailbreakV[34],(f)is from FigStep[16],(g)-(i)is from Ch3ef[46]and(j)-(l)is from Harmbench[35].",
                "position": 1891
            }
        ]
    },
    {
        "header": "BExperimental Details on Multimodal Safety Dataset with VSIL",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19939/x8.png",
                "caption": "Figure 9:Words cloud of our VLSBench’s textual queries.",
                "position": 2136
            }
        ]
    },
    {
        "header": "CVLSBench Data Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19939/x9.png",
                "caption": "Figure 10:The harmful elements: sensitive objects and risky scenarios examples, used to generate our harmful queries and image captions.",
                "position": 2202
            },
            {
                "img": "https://arxiv.org/html/2411.19939/x10.png",
                "caption": "Figure 11:Prompt used to categorize our image-text pairs in VLSBench.",
                "position": 2232
            }
        ]
    },
    {
        "header": "DMore Examples of VLSBench",
        "images": []
    },
    {
        "header": "ELimitations",
        "images": []
    },
    {
        "header": "FEvaluation Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19939/x11.png",
                "caption": "Figure 12:Evaluation method analysis. The LlamaGuard model is not able to perceive the image and identify the risky content in image-text pairs which hinders its evaluation. On the other hand, GPT-based methods is able to correctly evaluate on our VLSBench.",
                "position": 2263
            },
            {
                "img": "https://arxiv.org/html/2411.19939/x12.png",
                "caption": "Figure 13:Our GPT-4o evaluation prompt for our VLSBench.",
                "position": 2266
            }
        ]
    },
    {
        "header": "GData Construction Prompt Templates",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19939/x13.png",
                "caption": "Figure 14:Selected models’ results on our VLSBench across 6 categories. Reported results include safe with refuse rate, safe with warning rate, and total safe rate.",
                "position": 2603
            },
            {
                "img": "https://arxiv.org/html/2411.19939/x14.png",
                "caption": "Figure 15:",
                "position": 2606
            },
            {
                "img": "https://arxiv.org/html/2411.19939/x15.png",
                "caption": "Figure 16:",
                "position": 2609
            },
            {
                "img": "https://arxiv.org/html/2411.19939/x16.png",
                "caption": "Figure 17:",
                "position": 2612
            },
            {
                "img": "https://arxiv.org/html/2411.19939/x17.png",
                "caption": "Figure 18:Used prompt for GPT-4o to generate harmful queries and image descriptions with safety-related information from harmful elements, like risky objects and scenarios.",
                "position": 2615
            },
            {
                "img": "https://arxiv.org/html/2411.19939/x18.png",
                "caption": "Figure 19:Used prompt for GPT-4o to make the harmful query less obvious and less harmful, thus hiding the safety information from the image, preventing VSIL problem.",
                "position": 2618
            },
            {
                "img": "https://arxiv.org/html/2411.19939/x19.png",
                "caption": "Figure 20:Used prompt for GPT-4o to generate harmful queries with safety-related information from existing images.",
                "position": 2621
            },
            {
                "img": "https://arxiv.org/html/2411.19939/x20.png",
                "caption": "Figure 21:Used prompt for GPT-4o to filter the unsuccessful revised query, which is still harmful or loses original meanings.",
                "position": 2624
            },
            {
                "img": "https://arxiv.org/html/2411.19939/x21.png",
                "caption": "Figure 22:Used prompt for Qwen2-VL-72B for iterative image generation.",
                "position": 2627
            },
            {
                "img": "https://arxiv.org/html/2411.19939/x22.png",
                "caption": "Figure 23:Used prompt for GPT-4o to the final filtration of image-text pair to filter out pairs that do not match meaningfully and are not harmful.",
                "position": 2630
            }
        ]
    },
    {
        "header": "HEthics Statement",
        "images": []
    }
]