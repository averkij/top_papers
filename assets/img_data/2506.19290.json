[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19290/extracted/6565647/figures/swe-logo.png",
                "caption": "",
                "position": 91
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19290/x1.png",
                "caption": "Figure 1:(Top)The Skywork-SWE model achieves 38.0% pass@1 accuracy on the SWE-bench Verified benchmark, outperforming previous open-source SoTA Qwen2.5-Coder-32B-based LLMs built on the OpenHands agent framework. Moreover, Skywork-SWE clearly demonstrates the data scaling law phenomenon for software engineering capabilities in LLMs, with no signs of saturation at8,20982098,2098 , 209collected training trajectories.All evaluations are conducted in-house using a single attempt per instance, without verifiers or multiple rollouts, based on the OpenHands v0.32.0 framework.(Bottom)Performance comparison among recent advanced approaches using OpenHands on SWE-bench Verified. With the incorporation of test-time scaling¬†(TTS) techniques, Skywork-SWE achieves a further improvement to 47.0% accuracy, surpassing the latest Qwen and DeepSeek model series.",
                "position": 183
            },
            {
                "img": "https://arxiv.org/html/2506.19290/x1.png",
                "caption": "",
                "position": 186
            },
            {
                "img": "https://arxiv.org/html/2506.19290/x2.png",
                "caption": "",
                "position": 191
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19290/x3.png",
                "caption": "Figure 2:Overview of our three-stage Skywork-SWE data collection pipeline. (1)Stage A.Data Collection and Pre-filtering: Step A.1 scrapes GitHub repo metadata. Step A.2 collects and filters relevant pull requests¬†(PRs). Step A.3 validates PRs via installation checks. (2)Stage B.Execution-based Validation: Step B.1 configures unified execution commands. Step B.2 builds Docker-based runtime environments. Step B.3 validates task instances by executing unit tests. (3)Stage C.Agent Trajectory Generation: Step C.1 generates agent trajectories. Step C.2 validates trajectories via patch testing. Step C.3 collects validated trajectories for training.",
                "position": 376
            },
            {
                "img": "https://arxiv.org/html/2506.19290/x4.png",
                "caption": "Figure 3:Visualization of data flow across four key hierarchical filtering steps in our data collection pipeline. The first three steps belong to the Data Collection & Pre-filtering stage in Fig.2, while the last step corresponds to the Environment Setup & Execution-based Validation stage. The text inBlueandgreenindicates the number of repositories and task instances, respectively.",
                "position": 385
            },
            {
                "img": "https://arxiv.org/html/2506.19290/extracted/6565647/figures/wordcloud-sp.png",
                "caption": "Table 2:Dataset statistics for Skywork-SWE, SWE-Gym Lite(Pan et¬†al.,2024), and SWE-bench Verified(OpenAI,2024e).\nAll metrics are reported as per-instance averages, except for those under the Size category and the #Hints metric.",
                "position": 599
            },
            {
                "img": "https://arxiv.org/html/2506.19290/extracted/6565647/figures/wordcloud-sp.png",
                "caption": "Figure 6:Word cloud of repository names in the Skywork-SWE dataset. Font size is proportional to the number of instances in each repository. The Skywork-SWE dataset demonstrates high diversity across the collected GitHub repositories.",
                "position": 859
            },
            {
                "img": "https://arxiv.org/html/2506.19290/x5.png",
                "caption": "Figure 7:Repository- and year-wise histograms on the Skywork-SWE dataset. (a) The x-axis denotes the number of instances per repository, with every 150 repositories grouped into a single bin for clarity. (b) The x-axis indicates the year in which each issue was created, reflecting the temporal distribution of instance counts. The Skywork-SWE dataset reflects substantial diversity in both temporal coverage and repository sources.",
                "position": 872
            },
            {
                "img": "https://arxiv.org/html/2506.19290/x6.png",
                "caption": "Figure 8:Statistical analysis of instance-level edits on the Skywork-SWE dataset. (a) Histogram of the number of edited files per instance. (b) Histogram of the number of edited functions per instance. (c) Histogram of the number of edited code hunks per instance. (d) Histogram of the number of edited code lines per instance.",
                "position": 881
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19290/x7.png",
                "caption": "Figure 9:Resolve rate (%) of Skywork-SWE-32B under different test-time scaling strategies.\n(a) Effect of Best-of-N Sampling withNùëÅNitalic_Nranging from 1 to 8.\n(b) Effect of maximum rollout turns varying from 10 to 100.",
                "position": 1386
            }
        ]
    },
    {
        "header": "5Discussions",
        "images": []
    },
    {
        "header": "6Conclusion and Future Directions",
        "images": []
    },
    {
        "header": "7Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]