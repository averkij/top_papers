[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.09082/x1.png",
                "caption": "Figure 1:We introduce a zero-shot test-time inference procedure calledKL-tracing, which extracts robust optical flow and point tracking from a generative world model on challengingin-the-wildvideos.In every column, the green line links the query location in the first frame (top) to the position predicted by our method in the second frame (bottom).\nAll clips are real-world internet videos and contain phenomena that classical, appearance-based optical flow methods find challenging:\n(A) Newtonâ€™s cradle, where both frames have four balls in the middle, but the balls are different; the example involves physical reasoning.\n(B) Globe has challenging in-place object rotation and the query point is in the textureless ocean.\n(C) Dog weaving through occluding poles with large, rapid motion, including depth changes and motion blur.\n(D) Soccer tackle with fast, diagonal motion with motion blur and partial occlusion.\n(E) Windmill rotation where the repetitive blades and uniform sky make local matching challenging.\nThese examples highlight the benefits of leveraging a powerful world model to extract optical flow for challenging real-world scene dynamics.",
                "position": 61
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Test-time inference procedure and evaluation setup for optical flow",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.09082/extracted/6616316/figures/fig_general_method.png",
                "caption": "Figure 2:Test-time inference procedure for extracting flow from a pre-trained, frozen, generative video model, based on the Counterfactual World Model (CWM) paper[3].This involves three steps: (1) Perturbation: add a small, white-colored 2D Gaussian dot perturbation to frame 1 at the location of the point we wish to track. (2) Generate model predictions conditioned on the two frames. For CWM, Cosmos, and LRAS, we provide frame 1 and masked patches of frame 2 (Sections4.1,4.3,4.4). For Stable Video Diffusion, we provide the noised latents of both frames (Section4.2). (3) Estimate optical flow by computing the RGB difference between the clean and perturbed predictions.",
                "position": 124
            }
        ]
    },
    {
        "header": "4Model evaluations",
        "images": []
    },
    {
        "header": "5KL-tracing bypasses sampling randomness",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.09082/extracted/6616316/figures/fig_kl_method.png",
                "caption": "Figure 3:KL-tracing, our novel yet simple test-time inference procedure for extracting optical flow from controllable generative models such as LRAS.We follow the same steps for perturbation and conditioned prediction as in Figure2, but estimate optical flow by computing the KL divergence between the clean and perturbed prediction logits.",
                "position": 267
            },
            {
                "img": "https://arxiv.org/html/2507.09082/extracted/6616316/figures/fig_shortcomings_world_models.png",
                "caption": "Figure 4:Our method, KL-tracing using LRAS extracts better flow than other generative video models.(A) Deterministic models, such as CWM[3], often produce blurry predictions as they model a single, average future state.\n(B) Stable Video Diffusion lacks fine-grained controllability due to its coarse global latent code. Its clean and perturbed predictions differ in locations where the perturbation isnotsupposed to be carried to.\n(C) The Cosmos autoregressive world model lacks fine-grained controllability as it does not utilize pointers to denote the position of each token, making it challenging to prompt for flow extraction.\n(D) The LRAS model is highly controllable and has minimal differences between the clean and perturbed predictions. We use KL-tracing to compute the difference in logit instead of RGB space, obtaining sharp flow extractions.",
                "position": 283
            },
            {
                "img": "https://arxiv.org/html/2507.09082/x2.png",
                "caption": "Figure 5:KL-divergence of prediction distributions bypasses noisy RGB differences resulting from sampling randomness.Computing the KL divergence of the clean and perturbed prediction logits (last column) is more efficient yet functionally similar to computing the average RGB difference over many samples (second last column).",
                "position": 292
            },
            {
                "img": "https://arxiv.org/html/2507.09082/x3.png",
                "caption": "Figure 6:Large world models such as LRAS capture aspects of real-world dynamics that are challenging for specialized flow models relying on visual similarity or photometric loss. Specialized optical flow baselines, both supervised and self-supervised, struggle on complex, real-world scenes that are not fully captured by their training heuristics (Section6). A deeper understanding of dynamics beyond feature/photometric similarity allows large world models such as LRAS to resolve motion in the presence of homogeneous objects, motion blur, and partial occlusion, among others.",
                "position": 303
            }
        ]
    },
    {
        "header": "6Large video world models capture aspects of real-world dynamics that are challenging for specialized flow models",
        "images": []
    },
    {
        "header": "7Discussion",
        "images": []
    },
    {
        "header": "8Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional details on methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.09082/extracted/6616316/figures/fig_cosmos_three_settings.png",
                "caption": "Figure 7:All evaluation settings for Cosmos[29]) result in poor flow extractions.See Section4.3for a more detailed explanation of each result.",
                "position": 1162
            }
        ]
    },
    {
        "header": "Appendix BAdditional results",
        "images": []
    }
]