[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14171/x1.png",
                "caption": "",
                "position": 100
            },
            {
                "img": "https://arxiv.org/html/2412.14171/x2.png",
                "caption": "",
                "position": 100
            },
            {
                "img": "https://arxiv.org/html/2412.14171/x3.png",
                "caption": "",
                "position": 101
            },
            {
                "img": "https://arxiv.org/html/2412.14171/x4.png",
                "caption": "",
                "position": 112
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14171/x5.png",
                "caption": "Figure 2:A taxonomy ofvisual-spatial intelligencecapabilities.",
                "position": 133
            }
        ]
    },
    {
        "header": "2Visual-Spatial Intelligence",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14171/x6.png",
                "caption": "Figure 3:Tasks demonstration ofVSI-Bench. Note: the questions above are simplified slightly for clarity and brevity.",
                "position": 158
            }
        ]
    },
    {
        "header": "3VSI-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14171/x7.png",
                "caption": "Figure 4:Benchmark curation pipeline.The pipeline first unifies diverse datasets into a standardized format and semantic space for consistent processing. QA pairs are then generated through both human annotation and question templates. To ensure quality, human verification is implemented at all key stages for filtering low-quality videos, annotations, and ambiguous QA pairs.",
                "position": 181
            },
            {
                "img": "https://arxiv.org/html/2412.14171/x8.png",
                "caption": "Figure 5:Benchmark Statistics.Top: The distribution of tasks across three main categories.Bottom: The video length statistic.",
                "position": 184
            },
            {
                "img": "https://arxiv.org/html/2412.14171/x9.png",
                "caption": "Figure 6:Evaluation onVSI-Bench.Left:Dark grayindicates the best result among all models andlight grayindicates the best result among open-source models.†indicates results onVSI-Bench(tiny) set.Right:Results including the top-3 open-source models.",
                "position": 207
            },
            {
                "img": "https://arxiv.org/html/2412.14171/x10.png",
                "caption": "Figure 7:Performance comparisons between Vision Enabled (w/ video), Vision Disabled (w/o video) and Chance Level (Freq.).Enabled−--Disabled indicates the gap between Vision Enabled and Vision Disabled, and Disabled−--Chance betokens the gap between Vision Disabled and Chance Level (Freq.). Tasks are sorted by Enable−--Disable for better understanding.",
                "position": 605
            }
        ]
    },
    {
        "header": "4Evaluation onVSI-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14171/x11.png",
                "caption": "Figure 8:Examples of how a MLLM thinks as seen in self-explanations. While a MLLM exhibits strong video understanding and linguistic reasoning capabilities, its spatial reasoning capabilities are still developing.",
                "position": 665
            }
        ]
    },
    {
        "header": "5How MLLMsThink in SpaceLinguistically",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14171/x12.png",
                "caption": "Figure 9:Human-conducted analysis of errors by type. Over 70% of errors stem from faulty spatial reasoning capabilities.",
                "position": 719
            },
            {
                "img": "https://arxiv.org/html/2412.14171/x13.png",
                "caption": "Figure 10:Relative improvements ofCoT,self-consistencyandTree-of-Thoughtcompared to the baseline. All three prevailing prompting techniques fail on average on our benchmark, and, in some cases, task performance becomesmuch worseafter applying them.\nThis implies thatVSI-Benchcannot be solved by solely improving linguistic capabilities.",
                "position": 793
            }
        ]
    },
    {
        "header": "6How MLLMsThink in SpaceVisually",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14171/x14.png",
                "caption": "Figure 11:Visualization of cognitive maps from MLLM and GT.",
                "position": 849
            }
        ]
    },
    {
        "header": "7Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14171/x15.png",
                "caption": "Figure 12:Locality of the MLLM’s predicted cognitive maps.The MLLM’s map-distance accuracy decreases dramatically with increasing object distance.",
                "position": 954
            }
        ]
    },
    {
        "header": "8Discussion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix Outline",
        "images": []
    },
    {
        "header": "Appendix BTechnical Details forVSI-BenchConstruction and Analysis",
        "images": []
    },
    {
        "header": "Appendix CEvaluation Details",
        "images": []
    },
    {
        "header": "Appendix DInput Sequencing and Repetition Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14171/x16.png",
                "caption": "Figure 13:VSI-BenchExamples (Part 1).",
                "position": 3608
            },
            {
                "img": "https://arxiv.org/html/2412.14171/x17.png",
                "caption": "Figure 14:VSI-BenchExamples (Part 2).",
                "position": 3611
            },
            {
                "img": "https://arxiv.org/html/2412.14171/x18.png",
                "caption": "Figure 15:Additional Error Analysis Examples.",
                "position": 3614
            },
            {
                "img": "https://arxiv.org/html/2412.14171/x19.png",
                "caption": "Figure 16:Zero-Shot CoT Examples.",
                "position": 3617
            },
            {
                "img": "https://arxiv.org/html/2412.14171/x20.png",
                "caption": "Figure 17:Self-Consistency w/ CoT Examples.",
                "position": 3620
            },
            {
                "img": "https://arxiv.org/html/2412.14171/x21.png",
                "caption": "Figure 18:Tree-of-Thought Examples.",
                "position": 3623
            },
            {
                "img": "https://arxiv.org/html/2412.14171/x22.png",
                "caption": "Figure 19:Additional predicted cognitive map examples.",
                "position": 3626
            }
        ]
    },
    {
        "header": "Appendix EVisualization Results",
        "images": []
    }
]