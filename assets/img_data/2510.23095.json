[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23095/figures/qwen.jpg",
                "caption": "",
                "position": 89
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Analysis of Multimodal Rotary Position Embedding",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23095/x1.png",
                "caption": "(a)Vanilla RoPE / V2PE",
                "position": 268
            },
            {
                "img": "https://arxiv.org/html/2510.23095/x1.png",
                "caption": "(a)Vanilla RoPE / V2PE",
                "position": 271
            },
            {
                "img": "https://arxiv.org/html/2510.23095/x2.png",
                "caption": "(b)MRoPE",
                "position": 276
            },
            {
                "img": "https://arxiv.org/html/2510.23095/x3.png",
                "caption": "(c)VideoRoPE / HoPE",
                "position": 281
            },
            {
                "img": "https://arxiv.org/html/2510.23095/x4.png",
                "caption": "(d)CircleRoPE",
                "position": 287
            },
            {
                "img": "https://arxiv.org/html/2510.23095/x5.png",
                "caption": "(e)IL-RoPE / Omni-RoPE",
                "position": 292
            },
            {
                "img": "https://arxiv.org/html/2510.23095/x6.png",
                "caption": "(f)MHRoPE / MRoPE-I",
                "position": 297
            },
            {
                "img": "https://arxiv.org/html/2510.23095/x7.png",
                "caption": "Figure 2:Visual attention sink in MRoPE.\nAverage attention scores to input sequence from the ChartQA (Left) and VideoMME-short (Right).",
                "position": 371
            },
            {
                "img": "https://arxiv.org/html/2510.23095/x7.png",
                "caption": "",
                "position": 374
            },
            {
                "img": "https://arxiv.org/html/2510.23095/x8.png",
                "caption": "",
                "position": 378
            },
            {
                "img": "https://arxiv.org/html/2510.23095/x9.png",
                "caption": "Figure 3:Frequence allocation of different multimodal RoPEs.",
                "position": 410
            },
            {
                "img": "https://arxiv.org/html/2510.23095/x10.png",
                "caption": "(a)MRoPE",
                "position": 436
            },
            {
                "img": "https://arxiv.org/html/2510.23095/x10.png",
                "caption": "(a)MRoPE",
                "position": 439
            },
            {
                "img": "https://arxiv.org/html/2510.23095/x11.png",
                "caption": "(b)MHRoPE / MRoPE-I",
                "position": 444
            }
        ]
    },
    {
        "header": "3Experiment",
        "images": []
    },
    {
        "header": "4Conclusion",
        "images": []
    },
    {
        "header": "Appendix AEthics Statement",
        "images": []
    },
    {
        "header": "Appendix BReproducibility Statement",
        "images": []
    },
    {
        "header": "Appendix CLLM Usage",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23095/x12.png",
                "caption": "Figure 5:Video extrapolation performance. Models are trained with a context length of 32k (256 frames) and extrapolated to 64k (512 frames), 128k (1024 frames), and 256k (2048 frames).",
                "position": 1107
            }
        ]
    },
    {
        "header": "Appendix DAppendix",
        "images": []
    }
]