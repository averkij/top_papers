[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16801/x1.png",
                "caption": "",
                "position": 96
            },
            {
                "img": "https://arxiv.org/html/2411.16801/x2.png",
                "caption": "Figure 2:Limitations of previous data curation approaches used in controllable generation.Previous approaches on controllable generation often use a paired dataset consisting of low-quality segmented garments and human images for training. It leads to several undesirable artifacts as shown in right (generated with baselines).\nFor example, garments are directly replicated from the reference images in (a), shirts and skirts are blended together in (b), and generated skirts fail to resemble the reference in (c).",
                "position": 101
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16801/x3.png",
                "caption": "Figure 3:Overview of BootComp.We propose a two-stage framework: synthetic data generation and composition module training for controllable human image generation.\n(a) We train a decomposition network that maps from a segmented garment image to a product garment image. (b) We bootstrap synthetic paired data of human and multiple garment images. (c) We finally train our composition module with the synthetic paired dataset enabling it to generate human images with multiple reference garment images.",
                "position": 123
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16801/x4.png",
                "caption": "Figure 4:Extended self-attention architecture.In a extended self-attention layer,reference hidden statesare concatenated with thetarget hidden statesin the key and value matrices. This architecture enables injecting reference image features within the target image. Note that decomposition module also uses same structure but works within a single network.",
                "position": 241
            },
            {
                "img": "https://arxiv.org/html/2411.16801/x5.png",
                "caption": "Figure 5:Examples of high&low-quality generated garments.When human parsing results are not precise, the decomposition network struggles to generate product garment images accurately, resulting in low-quality garment images. We filter out these cases.",
                "position": 261
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16801/x6.png",
                "caption": "Figure 6:Qualitative comparison of human image generation with multiple garments.BootComp¬†generates realistic human images with multiple reference garments even with non-straightforward combinations of garments without losing details of each reference. For example, Parts2Whole replaces reference soccer cleats with stilettos, while ours accurately generates each reference (left, middle row).",
                "position": 313
            },
            {
                "img": "https://arxiv.org/html/2411.16801/x7.png",
                "caption": "Figure 7:More applications of BootComp.We showcase the extensive applications of our method, BootComp. BootComp¬†creates human images by controlling the (a) poses and (b) styles of the generated human images. BootComp¬†also enables (c) personalized human image generation by taking user‚Äôs images as conditions (e.g., face, full body).",
                "position": 417
            },
            {
                "img": "https://arxiv.org/html/2411.16801/x8.png",
                "caption": "Figure 8:Visualization of segmented paired data and our synthetic paired data.We provide a visual comparison between segmented and synthetic pairs. Given a single garment and a human image pair, we segment out other garments from the human image in the segmented paired data.",
                "position": 434
            },
            {
                "img": "https://arxiv.org/html/2411.16801/x9.png",
                "caption": "Figure 9:Visual comparison on data construction methods.Visual comparison between generated human images where each model is trained on segmented and synthetic pairs. The model trained on segmented pair data struggles to generate naturally harmonized human images (red).",
                "position": 482
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16801/x10.png",
                "caption": "Figure 10:Examples of training data for decomposition module.We collect pairs of a human image and a single reference garment image from public datasets including VITON-HD, DressCode, and LAION-Fashion. It consists of various garments in different categories,e.g., shirts, pants, shoes and bagsetc.",
                "position": 1317
            }
        ]
    },
    {
        "header": "Appendix BSynthetic Dataset Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16801/x11.png",
                "caption": "Figure 11:Examples of pairs filtered out by different similarity metrics.We present examples of generated garment images and their corresponding human images that were excluded based on various image similarity metrics. Using LPIPS, garments with complicated patterns are filtered out, and using CLIP score, inner layer garments are filtered out even when they are considered identical in human perception. In contrast, DreamSim captures the distance between images in a way aligned with human perception, filtering out undesirable pairs.",
                "position": 1373
            },
            {
                "img": "https://arxiv.org/html/2411.16801/x12.png",
                "caption": "Figure 12:Examples of generated garment images with different image distance values.We provide examples of generated garment images and corresponding human images, varying the distance values measured by DreamSim. With the distance valued‚â•0.4ùëë0.4d\\geq 0.4italic_d ‚â• 0.4, generated garments are inconsistent with the actual garment, while ford<0.4ùëë0.4d<0.4italic_d < 0.4, the generated garments closely resemble the actual garment.",
                "position": 1378
            },
            {
                "img": "https://arxiv.org/html/2411.16801/x13.png",
                "caption": "Figure 13:Examples of our synthetic paired data.We visualize our synthetic pairs of a human image and multiple garment images. Our decomposition module generates high-quality garment images in product view on different categories including shirts, pants, shoes and bags.",
                "position": 1392
            },
            {
                "img": "https://arxiv.org/html/2411.16801/x14.png",
                "caption": "Figure 14:Examples of synthetic paired data generated by the decomposition module trained on MVImgNet[51].We show the potential extension of our decomposition module to the general domain. Given an image containing common objects such as cups, chairs, and broccoli, the decomposition module generates each object in a different view, constructing paired data. Reference images are obtained from COCO[26].",
                "position": 1404
            }
        ]
    },
    {
        "header": "Appendix CApplications of Decomposition module",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16801/x15.png",
                "caption": "Figure 15:Examples of generated subjects in multi-view by the decomposition module trained on MVImgNet.The decomposition module can serve as a multi-view generator for single-subject images. Subject images are from DreamBooth[40].",
                "position": 1427
            }
        ]
    },
    {
        "header": "Appendix DAdditional Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16801/x16.png",
                "caption": "Figure 16:Limitations of BootComp.BootComp¬†struggles on naturally dressing hats and preserving tiny details like letters.",
                "position": 1451
            },
            {
                "img": "https://arxiv.org/html/2411.16801/x17.png",
                "caption": "Figure 17:More qualitative comparisons.BootComp¬†generates realistic human images wearing multiple reference garments, faithfully preserving fine-details of each garment, while baselines often generate inconsistent garment images and blend reference garments.",
                "position": 1456
            },
            {
                "img": "https://arxiv.org/html/2411.16801/x18.png",
                "caption": "Figure 18:Generated human images by BootComp.BootComp¬†can realistically dress humans with diverse categories of garments, including bags and shoes, which are not available for previous approaches. BootComp¬†is capable of dressing complex combinations such as jackets and inner layers (First row, second column) and less common garments such as overalls (Second row, third column).\nAlso, BootComp¬†can address challenging garments such as asymmetric-length garments and sandals (Third row, second column), and garments with unique details (Last row, third column).",
                "position": 1459
            },
            {
                "img": "https://arxiv.org/html/2411.16801/x19.png",
                "caption": "Figure 19:Application results by BootComp.BootComp¬†is capable of generating human images with various conditions. By using structural conditions, it can control poses in the generated images. With text prompts, BootComp¬†can manipulate the backgrounds of images. Additionally, it supports personalized generation through virtual try-on and face-based generations.",
                "position": 1463
            }
        ]
    },
    {
        "header": "Appendix ELimitations",
        "images": []
    }
]