[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06782/x1.png",
                "caption": "Figure 1:Policy Comparison.The representative performance among Behavior Transformer[52]served as an autoregressive policy, Diffusion Policy[11], and our approach in the state-based Robomimic square task experiment. CARP achieves a superior balance of performance and efficiency.",
                "position": 97
            },
            {
                "img": "https://arxiv.org/html/2412.06782/x2.png",
                "caption": "(a)Autoregressive Policy",
                "position": 102
            },
            {
                "img": "https://arxiv.org/html/2412.06782/x2.png",
                "caption": "(a)Autoregressive Policy",
                "position": 105
            },
            {
                "img": "https://arxiv.org/html/2412.06782/x3.png",
                "caption": "(b)Diffusion Policy",
                "position": 110
            },
            {
                "img": "https://arxiv.org/html/2412.06782/x4.png",
                "caption": "(c)CARP (Ours)",
                "position": 115
            },
            {
                "img": "https://arxiv.org/html/2412.06782/x5.png",
                "caption": "(a)Multi-Scale Action Tokenization",
                "position": 166
            },
            {
                "img": "https://arxiv.org/html/2412.06782/x5.png",
                "caption": "(a)Multi-Scale Action Tokenization",
                "position": 170
            },
            {
                "img": "https://arxiv.org/html/2412.06782/x6.png",
                "caption": "(b)Coarse-to-Fine Autoregressive Prediction",
                "position": 176
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06782/x7.png",
                "caption": "Figure 4:Single-Task Simulation Setup.We evaluate three tasks from the Robomimic[38]benchmark‚ÄîLift, Can, and Square‚Äîordered by increasing difficulty, along with a Kitchen task[17]on the far left.",
                "position": 484
            },
            {
                "img": "https://arxiv.org/html/2412.06782/x8.png",
                "caption": "Figure 5:Visualization of the Trajectory and Refining Process.The left panel shows the final predicted trajectories for each task, with CARP producing smoother and more consistent paths than Diffusion Policy (DP).\nThe right panel visualizes intermediate trajectories during the refinement process for CARP (top-right) and DP (bottom-right).\nDP displays considerable redundancy, resulting in slower processing and unstable training, as illustrated by 6 selected steps among 100 denoising steps.\nIn contrast, CARP achieves efficient trajectory refinement across all 4 scales, with each step contributing meaningful updates.",
                "position": 714
            },
            {
                "img": "https://arxiv.org/html/2412.06782/x9.png",
                "caption": "Figure 6:Multi-Task Simulation Setup.We evaluate eight tasks from the MimicGen[39]benchmark: Coffee, Hammer Cleanup, Mug Cleanup, Nut Assembly, Square, Stack, Stack Three, and Threading, listed left-to-right and top-to-bottom.",
                "position": 749
            },
            {
                "img": "https://arxiv.org/html/2412.06782/x10.png",
                "caption": "Figure 7:Real-World Setup.The left panel shows the environment used for the experiment and demonstration collection. The right panel shows the trajectory from the Cup and Bowl datasets.",
                "position": 862
            },
            {
                "img": "https://arxiv.org/html/2412.06782/x11.png",
                "caption": "Figure 8:Real-World Results (Visual Policy).We report the average success rate across 20 trials and the inference speed as action prediction frequency.\nCARP achieves competitive success rates with significantly faster inference compared to DP.",
                "position": 892
            },
            {
                "img": "https://arxiv.org/html/2412.06782/x12.png",
                "caption": "Figure 9:Visualization of CAPR on Real-World Tasks.CARP generates smooth and successful trajectories for the Cup and Bowl tasks, with temporal progression from left to right.",
                "position": 912
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Supplementary Material",
        "images": []
    },
    {
        "header": "Appendix ALimitations and Future Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06782/x13.png",
                "caption": "Figure 10:Coarse-to-Fine Autoregressive Inference.During inference, we leverage kv-caching to enablecoarse-to-fineprediction without the need for causal masks.\nThe finest-scale token mapùíìKsubscriptùíìùêæ\\boldsymbol{r}_{K}bold_italic_r start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPTis subsequently decoded by the action VQVAE decoder into executable actions for the robotic arm.",
                "position": 1930
            }
        ]
    },
    {
        "header": "Appendix BDefinition of Autoregressive Policy",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06782/x14.png",
                "caption": "Figure 11:Conventional Autoregressive Policy.In reinforcement learning, conventional autoregressive policies generate action tokens sequentially, where each token is predicted based on the previously generated tokens. This differs from the action chunking prediction shown inFig.2(a).",
                "position": 1941
            },
            {
                "img": "https://arxiv.org/html/2412.06782/x15.png",
                "caption": "Figure 12:Visualization of Tasks in Single-Task Experiment.",
                "position": 1991
            },
            {
                "img": "https://arxiv.org/html/2412.06782/x16.png",
                "caption": "Figure 13:Visualization of Tasks in Multi-Task Experiment.",
                "position": 1996
            },
            {
                "img": "https://arxiv.org/html/2412.06782/x17.png",
                "caption": "Figure 14:Visualization of All Interaction Tasks in Kitchen Experiment.",
                "position": 2001
            },
            {
                "img": "https://arxiv.org/html/2412.06782/x18.png",
                "caption": "Figure 15:Visualization of Tasks in Real-World Setup.",
                "position": 2006
            }
        ]
    },
    {
        "header": "Appendix CImplementation Consistent with Diffusion Policy",
        "images": []
    }
]