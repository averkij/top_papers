[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01584/x1.png",
                "caption": "",
                "position": 80
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01584/x2.png",
                "caption": "Figure 2:ViSTA-SLAM Overview.Given sequential video frames without intrinsics as the input, our frontend model takes in view pairs and predicts local pointmaps and relative poses within each pair.\nWe then use the pair-wise predictions to construct aSim​(3)\\mathrm{Sim(3)}pose graph with loop closure and optimize it via Levenberg–Marquardt algorithm. The frontend model employs a fully symmetric design, making the model lightweight and supporting more flexible pose graph optimization. The blue edges in the pose graph and final results represent connections between neighboring nodes (views), while the orange edges correspond to loop closures.",
                "position": 133
            }
        ]
    },
    {
        "header": "2ViSTA-SLAM Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01584/x3.png",
                "caption": "Figure 3:Asymmetric vs. Symmetric Architectures.Asymmetric architectures[53,23]use two decoders to regress point maps in a shared coordinate space. our symmetric formulation regresses relative pose and local point maps with only a single decoder, reducing over 36% of the parameters (∼\\sim0.4 vs. 0.7 billion), while achieving higher accuracy and enabling pose graph optimization in the backend.",
                "position": 327
            },
            {
                "img": "https://arxiv.org/html/2509.01584/x4.png",
                "caption": "",
                "position": 333
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01584/x5.png",
                "caption": "Figure 5:Trajectory estimation results on 7-Scenesoffice(top) and TUM-RGBDroom(bottom).Estimated camera trajectories are projected onto thexx–yyplane, with ground-truth shown as dashed lines. The trajectory color encodes ATE RMSE: higher errors in red, lower in blue. For MASt3R-SLAM[36]and VGGT-SLAM[28], only the poses of their selected keyframes are estimated.",
                "position": 749
            },
            {
                "img": "https://arxiv.org/html/2509.01584/x6.png",
                "caption": "",
                "position": 755
            },
            {
                "img": "https://arxiv.org/html/2509.01584/figs/recon/7scenes_redkitchen_cut3r_screenshot_000.jpg",
                "caption": "Figure 6:Reconstruction results on 7-Scenesredkitchen(left), TUM-RGBDroom(middle), and BundleFusionapt1(right).Purple boxes highlight reconstruction artifacts near the edges (background points wrongly mapped to the edge of the foreground). Red boxes indicate misalignments. Green boxes highlights ViSTA-SLAM’s competitive results.\nVGGT-SLAM fails to complete reconstruction onapt1due to divergence in pose graph optimization.",
                "position": 941
            },
            {
                "img": "https://arxiv.org/html/2509.01584/figs/recon/tumrgbd_room_cut3r_screenshot_000.jpg",
                "caption": "",
                "position": 954
            },
            {
                "img": "https://arxiv.org/html/2509.01584/figs/recon/bf_apt1_cut3r_screenshot_000.jpg",
                "caption": "",
                "position": 955
            },
            {
                "img": "https://arxiv.org/html/2509.01584/figs/recon/7scenes_slam3r.jpg",
                "caption": "",
                "position": 967
            },
            {
                "img": "https://arxiv.org/html/2509.01584/figs/recon/tumrgbd_room_slam3r_screenshot_000.jpg",
                "caption": "",
                "position": 968
            },
            {
                "img": "https://arxiv.org/html/2509.01584/figs/recon/bf_slam3r.jpg",
                "caption": "",
                "position": 969
            },
            {
                "img": "https://arxiv.org/html/2509.01584/figs/recon/7scenes_mast3rslam.jpg",
                "caption": "",
                "position": 983
            },
            {
                "img": "https://arxiv.org/html/2509.01584/figs/recon/tum_mast3rslam.jpg",
                "caption": "",
                "position": 984
            },
            {
                "img": "https://arxiv.org/html/2509.01584/figs/recon/bf_mast3rslam.jpg",
                "caption": "",
                "position": 985
            },
            {
                "img": "https://arxiv.org/html/2509.01584/figs/recon/7scenes_vggtslam.jpg",
                "caption": "",
                "position": 999
            },
            {
                "img": "https://arxiv.org/html/2509.01584/figs/recon/tum_vggtslam.jpg",
                "caption": "",
                "position": 1000
            },
            {
                "img": "https://arxiv.org/html/2509.01584/figs/recon/bf_vggtslam_fail.jpg",
                "caption": "",
                "position": 1001
            },
            {
                "img": "https://arxiv.org/html/2509.01584/figs/recon/7scenes_ours.jpg",
                "caption": "",
                "position": 1013
            },
            {
                "img": "https://arxiv.org/html/2509.01584/figs/recon/tum_ours.jpg",
                "caption": "",
                "position": 1014
            },
            {
                "img": "https://arxiv.org/html/2509.01584/figs/recon/bf_ours.jpg",
                "caption": "",
                "position": 1015
            }
        ]
    },
    {
        "header": "4Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "5Relative Scale in Pose Graph",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01584/figs/supp/scannet00_nopgo.jpg",
                "caption": "Figure 7:Qualitative Comparison for Pose Graph Optimization.Red boxes highlight regions with misalignments, while green boxes indicate areas where these misalignments have been corrected after pose graph optimization.",
                "position": 2261
            },
            {
                "img": "https://arxiv.org/html/2509.01584/figs/supp/scannet00.jpg",
                "caption": "",
                "position": 2267
            },
            {
                "img": "https://arxiv.org/html/2509.01584/figs/supp/wrong_loop.jpg",
                "caption": "",
                "position": 2283
            }
        ]
    },
    {
        "header": "6Per Scene Evaluation Results on 7-Scenes",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01584/figs/supp/bf_apt2.png",
                "caption": "Figure 9:More Qualitative Results.Reconstructions and camera trajectories from different datasets.",
                "position": 2318
            },
            {
                "img": "https://arxiv.org/html/2509.01584/figs/supp/tum_floor.png",
                "caption": "",
                "position": 2333
            },
            {
                "img": "https://arxiv.org/html/2509.01584/figs/supp/bf_apt0.png",
                "caption": "",
                "position": 2346
            },
            {
                "img": "https://arxiv.org/html/2509.01584/figs/supp/7scenes_redkitchen.png",
                "caption": "",
                "position": 2349
            },
            {
                "img": "https://arxiv.org/html/2509.01584/figs/supp/m2dgr_room03.png",
                "caption": "",
                "position": 2362
            },
            {
                "img": "https://arxiv.org/html/2509.01584/figs/supp/m2dgr_room01.png",
                "caption": "",
                "position": 2365
            }
        ]
    },
    {
        "header": "7More Qualitative Results",
        "images": []
    }
]