[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20426/extracted/6481075/fig/logo.png",
                "caption": "",
                "position": 67
            },
            {
                "img": "https://arxiv.org/html/2505.20426/extracted/6481075/fig/mmperspective.png",
                "caption": "Figure 1:MMPerspective benchmark overview.We introduce 10 tasks spanning 3 complementary dimensions of perspective understanding: PerspectivePerception,Reasoning, andRobustness.",
                "position": 92
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20426/extracted/6481075/fig/radar.png",
                "caption": "Figure 2:Left: MMPerspective benchmark consists of 2,711 instances and 5,083 QA pairs, hierarchically organized into 3 core categories (PerspectivePerception,Reasoning, andRobustness).Right: The accuracy of 8 representative MLLMs on 10 tasks of MMPerspective across the 3 categories.",
                "position": 125
            }
        ]
    },
    {
        "header": "2MMPerspective",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20426/extracted/6481075/fig/pre.png",
                "caption": "Figure 3:Perspective illustration with terminology. The figure is adapted from(Robertson and Bertling,2013).",
                "position": 158
            },
            {
                "img": "https://arxiv.org/html/2505.20426/extracted/6481075/fig/data_pipeline.png",
                "caption": "Figure 4:Data Curation Pipeline for MMPerspective.",
                "position": 195
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20426/x1.png",
                "caption": "Figure 5:Heatmaps illustrating the relationship between model size and performance, measured by P&R Overall Accuracy and Robustness. Darker colors indicate higher performance. Each line represents a model family, with sizes increasing from left to right.",
                "position": 1195
            },
            {
                "img": "https://arxiv.org/html/2505.20426/x2.png",
                "caption": "Figure 6:Correlation analysis between performance and size across MLLM families: (a) Overall accuracy vs. model size (r=0.81ùëü0.81r=0.81italic_r = 0.81), (b) Robustness vs. model size (r=0.34ùëü0.34r=0.34italic_r = 0.34), (c) Overall accuracy vs. encoder size (r=0.51ùëü0.51r=0.51italic_r = 0.51), (d) Robustness vs. encoder size (r=0.15ùëü0.15r=0.15italic_r = 0.15). Total model scaling strongly impacts perspective understanding, while vision encoder size has a limited influence on robustness.",
                "position": 1198
            },
            {
                "img": "https://arxiv.org/html/2505.20426/x3.png",
                "caption": "Figure 7:Error pattern analysis across model families: (a) Cumulative distribution of phi coefficients shows significantly higher correlations within families than across families (Cohen‚Äôsd=0.33ùëë0.33d=0.33italic_d = 0.33,p<0.001ùëù0.001p<0.001italic_p < 0.001). (b) Task-wise breakdown reveals perception tasks (VAP,CLP) exhibit the strongest family-specific patterns, while reasoning tasks (VPC,LRR) show weaker family effects.",
                "position": 1201
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ATask Definitions",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20426/extracted/6481075/fig/samples/VPP.png",
                "caption": "Figure 8:Examples of Vanishing Point Perception.",
                "position": 2627
            },
            {
                "img": "https://arxiv.org/html/2505.20426/extracted/6481075/fig/samples/CLP.png",
                "caption": "Figure 9:Examples of Critical Line Perception.",
                "position": 2630
            },
            {
                "img": "https://arxiv.org/html/2505.20426/extracted/6481075/fig/samples/VAP.png",
                "caption": "Figure 10:Examples of View Angle Perception.",
                "position": 2633
            },
            {
                "img": "https://arxiv.org/html/2505.20426/extracted/6481075/fig/samples/LDP.png",
                "caption": "Figure 11:Examples of Line Relationship Reasoning.",
                "position": 2636
            },
            {
                "img": "https://arxiv.org/html/2505.20426/extracted/6481075/fig/samples/PTR.png",
                "caption": "Figure 12:Examples of Perspective Type Reasoning.",
                "position": 2639
            },
            {
                "img": "https://arxiv.org/html/2505.20426/extracted/6481075/fig/samples/LRR.png",
                "caption": "Figure 13:Examples of Line Relationship Reasoning.",
                "position": 2642
            },
            {
                "img": "https://arxiv.org/html/2505.20426/extracted/6481075/fig/samples/PTS.png",
                "caption": "Figure 14:Examples of Perspective Transformation Spotting.",
                "position": 2645
            },
            {
                "img": "https://arxiv.org/html/2505.20426/extracted/6481075/fig/samples/VPC.png",
                "caption": "Figure 15:Examples of Vanishing Point Counting.",
                "position": 2648
            },
            {
                "img": "https://arxiv.org/html/2505.20426/extracted/6481075/fig/samples/OVR.png",
                "caption": "Figure 16:Examples of Out-of-View Reasoning.",
                "position": 2651
            },
            {
                "img": "https://arxiv.org/html/2505.20426/extracted/6481075/fig/robustness.png",
                "caption": "Figure 17:Examples of Perspective-Invariant Image Operations for Robustness Evaluation.",
                "position": 2654
            }
        ]
    },
    {
        "header": "Appendix BMore Terminology of Perspective",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20426/extracted/6481075/fig/pre_supp.png",
                "caption": "Figure 18:The relationship between Station Point (SP), Picture Plane (PP), Line of Sight (LS), and Horizon Line (HL) in perspective drawing. They demonstrate how viewing objects from different heights and angles affects spatial representation, emphasizing the critical distinction between LS and HL for accurate perspective construction. Figures are adapted from[Robertson and Bertling,2013].",
                "position": 2666
            },
            {
                "img": "https://arxiv.org/html/2505.20426/extracted/6481075/fig/wordcloud.png",
                "caption": "Figure 19:Word clouds of questions (left) and answer choices (right) in the MMPerspective Benchmark, illustrating the distribution of key terms related to perspective understanding.",
                "position": 2669
            }
        ]
    },
    {
        "header": "Appendix CMore Visualization",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20426/x4.png",
                "caption": "Figure 20:The heatmap for Vanishing Point Perception.",
                "position": 2683
            },
            {
                "img": "https://arxiv.org/html/2505.20426/x4.png",
                "caption": "Figure 20:The heatmap for Vanishing Point Perception.",
                "position": 2686
            },
            {
                "img": "https://arxiv.org/html/2505.20426/x5.png",
                "caption": "Figure 21:The heatmap for Critical Line Perception.",
                "position": 2691
            },
            {
                "img": "https://arxiv.org/html/2505.20426/x6.png",
                "caption": "Figure 22:The heatmap for View Angle Perception.",
                "position": 2696
            },
            {
                "img": "https://arxiv.org/html/2505.20426/x7.png",
                "caption": "Figure 23:The heatmap for Lens Distortion Perception.",
                "position": 2702
            },
            {
                "img": "https://arxiv.org/html/2505.20426/x7.png",
                "caption": "Figure 23:The heatmap for Lens Distortion Perception.",
                "position": 2705
            },
            {
                "img": "https://arxiv.org/html/2505.20426/x8.png",
                "caption": "Figure 24:The heatmap for Perspective Type Reasoning.",
                "position": 2710
            },
            {
                "img": "https://arxiv.org/html/2505.20426/x9.png",
                "caption": "Figure 25:The heatmap for Line Relationship Reasoning.",
                "position": 2715
            },
            {
                "img": "https://arxiv.org/html/2505.20426/x10.png",
                "caption": "Figure 26:The heatmap for Out of View Reasoning.",
                "position": 2721
            },
            {
                "img": "https://arxiv.org/html/2505.20426/x10.png",
                "caption": "Figure 26:The heatmap for Out of View Reasoning.",
                "position": 2724
            },
            {
                "img": "https://arxiv.org/html/2505.20426/x11.png",
                "caption": "Figure 27:The heatmap for Perspective Transformation Spotting.",
                "position": 2729
            },
            {
                "img": "https://arxiv.org/html/2505.20426/x12.png",
                "caption": "Figure 28:The heatmap for Vanishing Point Counting.",
                "position": 2734
            },
            {
                "img": "https://arxiv.org/html/2505.20426/extracted/6481075/fig/CoT/cot_gpt_1.png",
                "caption": "Figure 29:Examples of Chain-of-Thought Reasoning.",
                "position": 2747
            },
            {
                "img": "https://arxiv.org/html/2505.20426/extracted/6481075/fig/CoT/cot_gemini_1.png",
                "caption": "Figure 30:Examples of Chain-of-Thought Reasoning.",
                "position": 2750
            },
            {
                "img": "https://arxiv.org/html/2505.20426/extracted/6481075/fig/CoT/cot_gpt_2.png",
                "caption": "Figure 31:Examples of Chain-of-Thought Reasoning.",
                "position": 2753
            },
            {
                "img": "https://arxiv.org/html/2505.20426/extracted/6481075/fig/CoT/cot_gemini_2.png",
                "caption": "Figure 32:Examples of Chain-of-Thought Reasoning.",
                "position": 2756
            },
            {
                "img": "https://arxiv.org/html/2505.20426/x13.png",
                "caption": "Figure 33:Examples of GPT-4o with Chain-of-Thought Reasoning in Perspective Type Reasoning.",
                "position": 2762
            },
            {
                "img": "https://arxiv.org/html/2505.20426/x14.png",
                "caption": "Figure 34:Examples of Gemini-2-flash with Chain-of-Thought Reasoning in Perspective Transformation Spotting.",
                "position": 2765
            },
            {
                "img": "https://arxiv.org/html/2505.20426/x15.png",
                "caption": "Figure 35:Examples of Gemini-2-flash with Chain-of-Thought Reasoning in Line Relationship Reasoning.",
                "position": 2768
            },
            {
                "img": "https://arxiv.org/html/2505.20426/extracted/6481075/fig/radar/radar_1.png",
                "caption": "Figure 36:Task performance of models within each family (part 1).",
                "position": 2778
            },
            {
                "img": "https://arxiv.org/html/2505.20426/extracted/6481075/fig/radar/radar_2.png",
                "caption": "Figure 37:Task performance of models within each family (part 2).",
                "position": 2781
            },
            {
                "img": "https://arxiv.org/html/2505.20426/extracted/6481075/fig/diff.png",
                "caption": "Figure 38:Distribution of question difficulty across task types.",
                "position": 2792
            }
        ]
    },
    {
        "header": "Appendix DAnnotation Tool",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20426/extracted/6481075/fig/anno_tool.png",
                "caption": "Figure 39:Annotation interface developed for constructing perspective-based multiple-choice questions. The tool integrates geometric drawing utilities, structured answer selection, and image navigation to support precise and consistent labeling.",
                "position": 2805
            }
        ]
    },
    {
        "header": "Appendix ELimitations",
        "images": []
    }
]