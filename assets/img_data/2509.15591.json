[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15591/x1.png",
                "caption": "Figure 1:Latent Zoning Network(LZN) connects multiple encoders and decoders through a shared latent space, enabling a wide range of ML tasks via different encoder-decoder combinations or standalone encoders/decoders. The figure illustrates eight example tasks, but more could be supported. Only tasks 1-4 are evaluated in this paper, while the rest are for illustration.",
                "position": 120
            },
            {
                "img": "https://arxiv.org/html/2509.15591/x2.png",
                "caption": "Figure 2:The latent space ofLZNhas two key properties:(1) Generative:It follows a simple Gaussian prior, allowing easy sampling for generation tasks.(2) Unified:It serves as a shared representation across alldata types(e.g., image, text, label).\nEach data type induces a distinct partitioning of the latent space intolatent zones, where each zone corresponds to a specific sample (e.g., an individual image or label). The latent space is shown as a closed circle for illustration, but it is unbounded in practice.",
                "position": 123
            },
            {
                "img": "https://arxiv.org/html/2509.15591/x2.png",
                "caption": "Figure 2:The latent space ofLZNhas two key properties:(1) Generative:It follows a simple Gaussian prior, allowing easy sampling for generation tasks.(2) Unified:It serves as a shared representation across alldata types(e.g., image, text, label).\nEach data type induces a distinct partitioning of the latent space intolatent zones, where each zone corresponds to a specific sample (e.g., an individual image or label). The latent space is shown as a closed circle for illustration, but it is unbounded in practice.",
                "position": 125
            },
            {
                "img": "https://arxiv.org/html/2509.15591/x3.png",
                "caption": "Figure 3:Training and inference inLZNrely on two atomic operations:(1) Latent computation (§˜2.2.1):Computes latent zones for a data type by encoding samples intoanchor pointsand using flow matching (FM)liu2022flow;lipman2022flowto partition the latent space. Conversely, any latent point can be mapped to a sample via the decoder (not shown).(2) Latent alignment (§˜2.2.2):Aligns latent zones across data types by matching their FM processes.This figure also illustrates the approach forLZNin joint conditional generative modeling and classification (§˜5).",
                "position": 132
            }
        ]
    },
    {
        "header": "2Latent Zoning Network(LZN)",
        "images": []
    },
    {
        "header": "3Case Study 1: Unconditional Generative Modeling",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15591/x4.png",
                "caption": "Figure 4:LZNfor unconditional generative modeling (§˜3). During training, theLZNlatent of each target image is fed as an extra condition to the rectified flow (RF) modelliu2022flow, making the RF learnconditionalflows based onLZNlatents. The objective remains the standard RF loss, and theLZNencoder is trained end-to-end within it. During generation, we sampleLZNlatents from a standard Gaussian and use them as the extra condition. We illustrate the approach with RF, but sinceLZNlatents require no supervision and are differentiable, the method could apply to other tasks by adding a condition input forLZNlatents to the task network.",
                "position": 431
            },
            {
                "img": "https://arxiv.org/html/2509.15591/fig/plots/8_crop_gen_images/afhqcat_lzn.jpg",
                "caption": "Figure 5:Generated images of RF+LZNonAFHQ-Cat,CelebA-HQ,LSUN-Bedroom. More inApp.˜C.",
                "position": 634
            },
            {
                "img": "https://arxiv.org/html/2509.15591/fig/plots/8_crop_gen_images/celebahq_lzn.jpg",
                "caption": "",
                "position": 637
            },
            {
                "img": "https://arxiv.org/html/2509.15591/fig/plots/8_crop_gen_images/lsunbedroom_lzn.jpg",
                "caption": "",
                "position": 638
            }
        ]
    },
    {
        "header": "4Case Study 2: Unsupervised\nRepresentation Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15591/x5.png",
                "caption": "Figure 6:LZNfor unsupervised representation learning (§˜4). During training, each image batch undergoes two sets of data augmentations, and latent zones for each set are computedusing the same encoder. We then apply latent alignment (§˜2.2.2) to train the encoder. At inference, we can use theLZNlatents, the encoder outputs (i.e., anchor points), or intermediate encoder outputs (§˜D.4). The latter two options avoid the costly latent computation process.",
                "position": 657
            }
        ]
    },
    {
        "header": "5Case Study 3: Conditional Generative Modeling and Classification",
        "images": []
    },
    {
        "header": "6Limitations and Future Work",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix Contents",
        "images": []
    },
    {
        "header": "Appendix AMore Details on Latent Computation (§˜2.2.1)",
        "images": []
    },
    {
        "header": "Appendix BMore Details on Latent Alignment (§˜2.2.2)",
        "images": []
    },
    {
        "header": "Appendix CMore Details and Results on Case Study 1",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15591/fig/plots/7_gen_images_FOR_6047_6048/cifar10_rf.jpg",
                "caption": "Figure 9:Generated images of RF onCIFAR10.",
                "position": 3195
            },
            {
                "img": "https://arxiv.org/html/2509.15591/fig/plots/7_gen_images_FOR_6047_6048/cifar10_lzn.jpg",
                "caption": "Figure 10:Generated images of RF+LZNonCIFAR10.",
                "position": 3198
            },
            {
                "img": "https://arxiv.org/html/2509.15591/fig/plots/7_gen_images_FOR_6047_6048/afhqcat_rf.jpg",
                "caption": "Figure 11:Generated images of RF onAFHQ-Cat.",
                "position": 3201
            },
            {
                "img": "https://arxiv.org/html/2509.15591/fig/plots/7_gen_images_FOR_6047_6048/afhqcat_lzn.jpg",
                "caption": "Figure 12:Generated images of RF+LZNonAFHQ-Cat.",
                "position": 3204
            },
            {
                "img": "https://arxiv.org/html/2509.15591/fig/plots/7_gen_images_FOR_6047_6048/celebahq_rf.jpg",
                "caption": "Figure 13:Generated images of RF onCelebA-HQ.",
                "position": 3207
            },
            {
                "img": "https://arxiv.org/html/2509.15591/fig/plots/7_gen_images_FOR_6047_6048/celebahq_lzn.jpg",
                "caption": "Figure 14:Generated images of RF+LZNonCelebA-HQ.",
                "position": 3210
            },
            {
                "img": "https://arxiv.org/html/2509.15591/fig/plots/7_gen_images_FOR_6047_6048/lsunbedroom_rf.jpg",
                "caption": "Figure 15:Generated images of RF onLSUN-Bedroom.",
                "position": 3213
            },
            {
                "img": "https://arxiv.org/html/2509.15591/fig/plots/7_gen_images_FOR_6047_6048/lsunbedroom_lzn.jpg",
                "caption": "Figure 16:Generated images of RF+LZNonLSUN-Bedroom.",
                "position": 3216
            },
            {
                "img": "https://arxiv.org/html/2509.15591/x6.png",
                "caption": "(a)CIFAR10.",
                "position": 3332
            },
            {
                "img": "https://arxiv.org/html/2509.15591/x6.png",
                "caption": "(a)CIFAR10.",
                "position": 3335
            },
            {
                "img": "https://arxiv.org/html/2509.15591/x7.png",
                "caption": "(b)AFHQ-Cat.",
                "position": 3340
            },
            {
                "img": "https://arxiv.org/html/2509.15591/x8.png",
                "caption": "(c)CelebA-HQ.",
                "position": 3346
            },
            {
                "img": "https://arxiv.org/html/2509.15591/x9.png",
                "caption": "(d)LSUN-Bedroom.",
                "position": 3351
            }
        ]
    },
    {
        "header": "Appendix DMore Details and Results on Case Study 2",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15591/x10.png",
                "caption": "Figure 18:t-SNE visualization ofLZNrepresentations projected into 2D for 20 randomly selected ImageNet validation classes. Images from the same class form distinct clusters, indicating thatLZNlearns meaningful image representations.",
                "position": 3761
            },
            {
                "img": "https://arxiv.org/html/2509.15591/x11.png",
                "caption": "Figure 19:LZN’s linear classification accuracy with different feature extraction methods. Note that this experiment uses fewer iterations (1060000) than the main experiment (5000000) and omits data augmentation when training the linear classifier (used in the main experiment), so the accuracies are lower than the main experiment.",
                "position": 3826
            },
            {
                "img": "https://arxiv.org/html/2509.15591/x12.png",
                "caption": "Figure 20:LZN’s linear classification accuracy vs. training iteration. The accuracy is still improving at a fast rate at the end of training. More training might further improve the result.",
                "position": 3834
            }
        ]
    },
    {
        "header": "Appendix EMore Details and Results on Case Study 3",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15591/fig/plots/7_gen_images_FOR_6047_6048/cifar10_cond_rf.jpg",
                "caption": "Figure 22:Generated images of RF onCIFAR10(conditional generation). Every 5 rows corresponds to one class inCIFAR10.",
                "position": 4163
            },
            {
                "img": "https://arxiv.org/html/2509.15591/fig/plots/7_gen_images_FOR_6047_6048/cifar10_cond_lzn.jpg",
                "caption": "Figure 23:Generated images of RF+LZNonCIFAR10(conditional generation). Every 5 rows corresponds to one class inCIFAR10.",
                "position": 4166
            },
            {
                "img": "https://arxiv.org/html/2509.15591/x13.png",
                "caption": "Figure 24:FID vs. number of sampling steps in the Euler sampler onCIFAR10(conditional generation). RF+LZNoutperforms RF in most cases.",
                "position": 4210
            },
            {
                "img": "https://arxiv.org/html/2509.15591/x14.png",
                "caption": "Figure 25:Classification accuracy with different hyperparameters onCIFAR10. Generally, increasing the batch size and decreasingα\\alphaimprove the classification accuracy.",
                "position": 4234
            }
        ]
    },
    {
        "header": "Appendix FExtended Discussions on Related Work",
        "images": []
    },
    {
        "header": "Appendix GExtended Discussions on Limitations and Future Work",
        "images": []
    }
]