[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20396/x1.png",
                "caption": "Figure 1:A sim-to-real RL recipe for vision-based dexterous manipulation.We close the environment modeling gap between simulation and the real world through an automated real-to-sim tuning module, design generalizable task rewards by disentangling each manipulation task into contact states and object states, improve sample efficiency of dexterous manipulation policy training by using task-aware hand poses and divide-and-conquer distillation, and transfer vision-based policies to the real world with a mixture of sparse and dense object representations.",
                "position": 105
            }
        ]
    },
    {
        "header": "IIBackground",
        "images": []
    },
    {
        "header": "IIIChallenges and Approaches",
        "images": []
    },
    {
        "header": "IVExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20396/x2.png",
                "caption": "Figure 2:Policies learned in simulation.Left: grasp; middle: box lift; right: bimanual handover (right-to-left, left-to-right).",
                "position": 354
            },
            {
                "img": "https://arxiv.org/html/2502.20396/extracted/6236542/figures/objexp.png",
                "caption": "Figure 3:Traininggrasp-and-reachpolicy with different object sets.Each curve is computed from the statistics of 10 training runs with different random seeds. Left: training with complex objects v.s. simple geometric primitive objects. Right: training with differently grouped geometric objects.",
                "position": 399
            },
            {
                "img": "https://arxiv.org/html/2502.20396/x3.png",
                "caption": "Figure 4:Different contact patterns emerge from different placements of contact markers.Top: contact markers on the left and right side centers; middle: markers on the top and bottom side centers; bottom: markers on the bottom side edges.",
                "position": 413
            },
            {
                "img": "https://arxiv.org/html/2502.20396/x4.png",
                "caption": "Figure 5:Policy robustness.Our learned policies remain robust under different force perturbations, including knock (top left), pull (top right), push (bottom left), and drag (bottom right).",
                "position": 522
            }
        ]
    },
    {
        "header": "VLimitations",
        "images": []
    },
    {
        "header": "VIConclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]