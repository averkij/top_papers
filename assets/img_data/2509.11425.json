[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Proposed Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.11425/figures/pipelines_v5.png",
                "caption": "Figure 1:Overview of the FuseCodec speech tokenization framework. Input speech𝐱\\mathbf{x}is encoded into latent features𝐙\\mathbf{Z}, then quantized into discrete tokens𝐐(1:K)\\mathbf{Q}^{(1:K)}via residual vector quantization (RVQ). To enrich these tokens, we incorporate semantic (𝐒i,𝐒^\\mathbf{S}_{i},\\hat{\\mathbf{S}}) and contextual (𝐂i,𝐂^,𝐂∗\\mathbf{C}_{i},\\hat{\\mathbf{C}},\\mathbf{C}^{*}) representations from frozen pre-trained models. Global vectors𝐒^\\hat{\\mathbf{S}}and𝐂^\\hat{\\mathbf{C}}are formed via mean pooling and[CLS]selection, respectively. We propose three strategies: (i) Latent Representation Fusion, injecting global vectors𝐒^,𝐂^\\hat{\\mathbf{S}},\\hat{\\mathbf{C}}with𝐙\\mathbf{Z}to yield fused latent𝐙′\\mathbf{Z}^{\\prime}; (ii) Global Semantic-Contextual Supervision, supervising𝐐(1)\\mathbf{Q}^{(1)}with global vectors; and (iii) Temporally Aligned Contextual Supervision, aligning full contextual embeddings{𝐂i}\\{\\mathbf{C}_{i}\\}to RVQ outputs via a windowed matching algorithm to form𝐂∗\\mathbf{C}^{*}.",
                "position": 199
            }
        ]
    },
    {
        "header": "4Tokenizer Design and Loss Functions",
        "images": []
    },
    {
        "header": "5Experimental Setup",
        "images": []
    },
    {
        "header": "6Experimental Results and Discussion",
        "images": []
    },
    {
        "header": "7Ablation Studies",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]