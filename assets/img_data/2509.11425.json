[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Proposed Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.11425/figures/pipelines_v5.png",
                "caption": "Figure 1:Overview of the FuseCodec speech tokenization framework. Input speechğ±\\mathbf{x}is encoded into latent featuresğ™\\mathbf{Z}, then quantized into discrete tokensğ(1:K)\\mathbf{Q}^{(1:K)}via residual vector quantization (RVQ). To enrich these tokens, we incorporate semantic (ğ’i,ğ’^\\mathbf{S}_{i},\\hat{\\mathbf{S}}) and contextual (ğ‚i,ğ‚^,ğ‚âˆ—\\mathbf{C}_{i},\\hat{\\mathbf{C}},\\mathbf{C}^{*}) representations from frozen pre-trained models. Global vectorsğ’^\\hat{\\mathbf{S}}andğ‚^\\hat{\\mathbf{C}}are formed via mean pooling and[CLS]selection, respectively. We propose three strategies: (i) Latent Representation Fusion, injecting global vectorsğ’^,ğ‚^\\hat{\\mathbf{S}},\\hat{\\mathbf{C}}withğ™\\mathbf{Z}to yield fused latentğ™â€²\\mathbf{Z}^{\\prime}; (ii) Global Semantic-Contextual Supervision, supervisingğ(1)\\mathbf{Q}^{(1)}with global vectors; and (iii) Temporally Aligned Contextual Supervision, aligning full contextual embeddings{ğ‚i}\\{\\mathbf{C}_{i}\\}to RVQ outputs via a windowed matching algorithm to formğ‚âˆ—\\mathbf{C}^{*}.",
                "position": 199
            }
        ]
    },
    {
        "header": "4Tokenizer Design and Loss Functions",
        "images": []
    },
    {
        "header": "5Experimental Setup",
        "images": []
    },
    {
        "header": "6Experimental Results and Discussion",
        "images": []
    },
    {
        "header": "7Ablation Studies",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]