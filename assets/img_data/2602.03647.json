[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03647/x1.png",
                "caption": "Figure 1:Demonstration of Search-R1 and Search-R2.While Search-R1 (Left) is disrupted by retrieval noise and falls into an error propagation loop, Search-R2 (Right) utilizes an Actor-Refiner collaboration. The Meta-Refiner identifies the deviation and applies a \"cut-and-regenerate\" mechanism to surgically repair the reasoning chain at the point of error, successfully redirecting focus from the incorrect entity (Aguinaldo) to the correct one (Quezon).",
                "position": 108
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03647/x2.png",
                "caption": "Figure 2:Overview of the Search-R2 framework. The Actor generates initial reasoning trajectories with search queries. The Meta-Refiner employs a Discriminator to detect errors and a Trimmer to identify the exact step of failure. Upon rejection, the trajectory is truncated and regenerated from the error point. The system is jointly optimized via GRPO using a hybrid reward.",
                "position": 156
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Formalization",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03647/Figures/rollout_samples.png",
                "caption": "Figure 3:The total rollout numbers after revision (initial rollout numbers + refined rollout numbers) corresponding to different max revision time settings.",
                "position": 835
            },
            {
                "img": "https://arxiv.org/html/2602.03647/Figures/trajectory_comparison_new.png",
                "caption": "Figure 4:Average counts of Search-R2 winning and failing against Search-R1 across all seven datasets for each rubric.",
                "position": 917
            }
        ]
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ANotation Table",
        "images": []
    },
    {
        "header": "Appendix BProof for Performance Decomposition of Meta-Refiner",
        "images": []
    },
    {
        "header": "Appendix CProof for Decomposition of Trimming Strategy",
        "images": []
    },
    {
        "header": "Appendix DDrivers of Performance Gain",
        "images": []
    },
    {
        "header": "Appendix ESupplementary Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03647/Figures/training_dynamics_tasks_new.png",
                "caption": "Figure 5:Detailed training dynamics of Search-R2 with different base models across all seven datasets.",
                "position": 1873
            }
        ]
    },
    {
        "header": "Appendix FTraining Dynamics",
        "images": []
    },
    {
        "header": "Appendix GComparison against Search-R1 with Double Rollout Numbers",
        "images": []
    },
    {
        "header": "Appendix HDetailed Ablation Study Results",
        "images": []
    },
    {
        "header": "Appendix ISupplementary Introduction to Trajectory Quality Analysis",
        "images": []
    },
    {
        "header": "Appendix JPseudocode for LLM Response Rollout with Multi-Turn Search",
        "images": []
    },
    {
        "header": "Appendix KLocal Process Reward Implementation Details",
        "images": []
    },
    {
        "header": "Appendix LMeta-Refiner Prompt",
        "images": []
    }
]