[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18795/x1.png",
                "caption": "",
                "position": 52
            },
            {
                "img": "https://arxiv.org/html/2510.18795/x2.png",
                "caption": "",
                "position": 62
            },
            {
                "img": "https://arxiv.org/html/2510.18795/Figure/logo_hf.png",
                "caption": "",
                "position": 63
            }
        ]
    },
    {
        "header": "1introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18795/x3.png",
                "caption": "Figure 1:Illustration of research gap. Previous work directly aligns the LLM-based embedder with the CLIP image encoder, disregarding the valuable knowledge embedded in the pre-trained CLIP model. In contrast, ProCLIP first transfers knowledge from CLIPâ€™s text encoder to the LLM embedder via distillation, establishing an initial alignment. It then refines the alignment between the CLIP image encoder and the LLM-based embedder through image-text contrastive learning with self-distillation regularization.",
                "position": 105
            }
        ]
    },
    {
        "header": "2related work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18795/x4.png",
                "caption": "Figure 2:The training pipeline of our proposed ProCLIP. It consists of representation inheritance via cross-architecture distillation and contrastive tuning integrated with self-distillation regularization.",
                "position": 127
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18795/x5.png",
                "caption": "Figure 3:Per-language image-text retrieval performance on the XM3600 benchmark.",
                "position": 760
            },
            {
                "img": "https://arxiv.org/html/2510.18795/x6.png",
                "caption": "",
                "position": 764
            },
            {
                "img": "https://arxiv.org/html/2510.18795/x7.png",
                "caption": "Figure 4:MMVP performance comparison. ProCLIP presents excellent performance.",
                "position": 1402
            },
            {
                "img": "https://arxiv.org/html/2510.18795/x7.png",
                "caption": "Figure 4:MMVP performance comparison. ProCLIP presents excellent performance.",
                "position": 1405
            },
            {
                "img": "https://arxiv.org/html/2510.18795/x8.png",
                "caption": "Figure 6:Ablation on different LLM-based embedders.",
                "position": 1571
            },
            {
                "img": "https://arxiv.org/html/2510.18795/x8.png",
                "caption": "Figure 6:Ablation on different LLM-based embedders.",
                "position": 1574
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]