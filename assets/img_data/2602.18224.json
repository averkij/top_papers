[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.18224/real1.jpg",
                "caption": "Figure 1:Out-of-box real-robot task examples.We deploy SimVLA without any additional fine-tuning on our held-out scenes and evaluate it on a set of multi-stage tasks that require both dexterous manipulation and semantic understanding.",
                "position": 109
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3SimVLA: A Simple VLA Baseline",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.18224/fig1.png",
                "caption": "Figure 2:SimVLA overview.SimVLA is a minimal baseline: a VLM encoder produces fused vision-language tokens once per control step, and a lightweight action transformer performs flow-matching denoising to generate a continuous action chunk.",
                "position": 296
            }
        ]
    },
    {
        "header": "4Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.18224/galaxea_zero_shot.png",
                "caption": "Figure 3:Real-robot zero-shot results on Galaxea R1 Lite.",
                "position": 1424
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADatasets and Experimental Details",
        "images": []
    }
]