[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17793/figures/huggingface.png",
                "caption": "",
                "position": 92
            },
            {
                "img": "https://arxiv.org/html/2510.17793/x1.png",
                "caption": "Figure 1:Overview of our work. We curate 2.5M multi-task, multi-domain training samples (left) and use large-scale iterative rejection sampling SFT to trainFARE, a family of automatic evaluators (top right). We evaluate FARE on static benchmarks and on various real-world downstream tasks.",
                "position": 95
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background and Related Work",
        "images": []
    },
    {
        "header": "3FARE: data and training recipe",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17793/x2.png",
                "caption": "Figure 2:Breakdown of our curated training dataset of 2.5M samples by task (left), domain (center), and curation approach (right). Domain breakdown excludes step-level data, which is entirely math.",
                "position": 204
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17793/x3.png",
                "caption": "Figure 3:Best-of-10 performance for Llama-3.1-8B generator across 4 challenging benchmarks with baseline (red line) and oracle (green line) performance with FARE and SFR-Judge (SFR), Skywork Critic (SC), Compass-Judger (CJ), and Self-Taught Evaluator (STE) as baselines. FARE are the best small (≤\\leq14B) and large (≥\\geq20B) reranking models: FARE-20B achieves near oracle re-ranking performance on MATH, while FARE-8B matches 70B judges.",
                "position": 904
            },
            {
                "img": "https://arxiv.org/html/2510.17793/x4.png",
                "caption": "Figure 4:Performance of downstream GRPO-trained model with FARE as reference-based verifiers. Moving from string based output matching to our models as generative verifiers brings tangible performance gains across both natural-language based (e.g., GPQA) and math settings.",
                "position": 916
            },
            {
                "img": "https://arxiv.org/html/2510.17793/x5.png",
                "caption": "Figure 5:Continual training of FARE-20B for code evaluation with only 15K samples yields larger gains over gpt-oss-20B/120B.",
                "position": 924
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AExtended Related Work",
        "images": []
    },
    {
        "header": "Appendix BData and Training Details",
        "images": []
    },
    {
        "header": "Appendix CBenchmark and Baseline Details",
        "images": []
    },
    {
        "header": "Appendix DAblations, analysis, and additional results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17793/x6.png",
                "caption": "Figure 6:Pairwise positional robustness emerges as at large data scales, as shown with training FARE-8B and an earlier development run initialized with Qwen2.5-7B-Instruct.",
                "position": 3226
            }
        ]
    },
    {
        "header": "Appendix EPrompts and Examples",
        "images": []
    }
]