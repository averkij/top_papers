[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1.Introduction",
        "images": []
    },
    {
        "header": "2.Related Work",
        "images": []
    },
    {
        "header": "3.Recommendation Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.20859/extracted/6397911/x-cross.png",
                "caption": "Figure 2.X-Cross model architecture. Each source domain language model is implemented with several Transformer (vertical) layers. On the left side: at each layer, the “hot”-trainable integrator receives activations from the “frozen” layers and then passes the integrated representations to the next layer. On the right side: a “zoom-in” into an X-Cross integrator located at one of the network layers.",
                "position": 312
            }
        ]
    },
    {
        "header": "4.Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.20859/extracted/6397911/sen.png",
                "caption": "Figure 3.Accuracy (Hit@1) comparison across datasets for X-Cross and LoRA. The dashed red-line denotes the performance of the reference model.",
                "position": 979
            },
            {
                "img": "https://arxiv.org/html/2504.20859/extracted/6397911/layers.png",
                "caption": "Figure 4.Accuracy (Hit@1) vs number of layers.",
                "position": 1200
            }
        ]
    },
    {
        "header": "5.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]