[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22322/fig/androidlab.png",
                "caption": "Figure 1:Success rate on AndroidLab[1]across model families and scales.\nCompared with the vanilla prompting (PT),\nour SmartSnap brings significant gains via fine-tuning (FT) and reinforcement learning (RL) without relying on sophisticated rule-based verifiers and task-specific reward models.\nThe developed self-verifying agents learn to complete tasks and curate snapshot evidences in a complementary manner,\nachieving competitive performance with larger LLMs.",
                "position": 115
            },
            {
                "img": "https://arxiv.org/html/2512.22322/x1.png",
                "caption": "Figure 2:Three strategies for agent verification distinguished by their inputs to the verifier:\n(a) the task-specific script accessing theground-truth state;\n(b) thefull trajectory with noisy context;\nand (c) theagent-curated evidence set.",
                "position": 129
            }
        ]
    },
    {
        "header": "2Related Works and Preliminaries",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22322/x2.png",
                "caption": "Figure 3:An example of self-verification for evidence curation.\nThe agent decomposes the task into an actionable checklist where the date, the amount, and the category tag are to be confirmed during stepwise task completion.\nThe proactive step of taking snapshots that list the target transaction provides a definitive evidence for task completion.",
                "position": 286
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22322/fig/rollout_num_evidence.jpg",
                "caption": "(a)Number of Evidence (Training).",
                "position": 1097
            },
            {
                "img": "https://arxiv.org/html/2512.22322/fig/rollout_num_evidence.jpg",
                "caption": "(a)Number of Evidence (Training).",
                "position": 1100
            },
            {
                "img": "https://arxiv.org/html/2512.22322/fig/validation_num_evidence.jpg",
                "caption": "(b)Number of Evidence (Validation).",
                "position": 1105
            },
            {
                "img": "https://arxiv.org/html/2512.22322/fig/rollout_num_agent_turns.jpg",
                "caption": "(c)Number of Agent Turns (Training).",
                "position": 1110
            },
            {
                "img": "https://arxiv.org/html/2512.22322/fig/validation_num_agent_turns.jpg",
                "caption": "(d)Number of Agent Turns (Validation).",
                "position": 1116
            },
            {
                "img": "https://arxiv.org/html/2512.22322/fig/rollout_response_length.jpg",
                "caption": "(e)Response Length (Training).",
                "position": 1121
            },
            {
                "img": "https://arxiv.org/html/2512.22322/fig/validation_response_length.jpg",
                "caption": "(f)Response Length (Validation).",
                "position": 1126
            },
            {
                "img": "https://arxiv.org/html/2512.22322/fig/rollout_critic-reward.jpg",
                "caption": "(a)Training Reward (mean).",
                "position": 1133
            },
            {
                "img": "https://arxiv.org/html/2512.22322/fig/rollout_critic-reward.jpg",
                "caption": "(a)Training Reward (mean).",
                "position": 1136
            },
            {
                "img": "https://arxiv.org/html/2512.22322/fig/rollout_critic-reward-std.jpg",
                "caption": "(b)Training Reward (std.).",
                "position": 1141
            },
            {
                "img": "https://arxiv.org/html/2512.22322/fig/validation_bluecoins.jpg",
                "caption": "(c)Validation on Bluecoins.",
                "position": 1146
            },
            {
                "img": "https://arxiv.org/html/2512.22322/fig/validation_calendar.jpg",
                "caption": "(d)Validation on Calendar.",
                "position": 1152
            },
            {
                "img": "https://arxiv.org/html/2512.22322/fig/validation_cantook.jpg",
                "caption": "(e)Validation on Cantook.",
                "position": 1157
            },
            {
                "img": "https://arxiv.org/html/2512.22322/fig/validation_clock.jpg",
                "caption": "(f)Validation on Clock.",
                "position": 1162
            },
            {
                "img": "https://arxiv.org/html/2512.22322/fig/validation_contacts.jpg",
                "caption": "(g)Validation on Contacts.",
                "position": 1168
            },
            {
                "img": "https://arxiv.org/html/2512.22322/fig/validation_mapsme.jpg",
                "caption": "(h)Validation on Maps.me.",
                "position": 1173
            },
            {
                "img": "https://arxiv.org/html/2512.22322/fig/validation_pimusic.jpg",
                "caption": "(i)Validation on Pi Music.",
                "position": 1178
            },
            {
                "img": "https://arxiv.org/html/2512.22322/fig/validation_settings.jpg",
                "caption": "(j)Validation on Settings.",
                "position": 1184
            },
            {
                "img": "https://arxiv.org/html/2512.22322/fig/validation_zoom.jpg",
                "caption": "(k)Validation on Zoom.",
                "position": 1189
            },
            {
                "img": "https://arxiv.org/html/2512.22322/fig/bluecoins-diff.png",
                "caption": "Figure 6:The agent trajectories before and after SmartSnap RL of one testing example of the app Bluecoins (Qwen3-8B).\nInitially,\nthe agent fails to comprehend the instruction properly and consistently taps the top-leftok/cancelbutton to find the entrance towards creating a new expense note.\nAfter SmartSnap RL,\nthe agent learns to utilize the floating button for creating new notes with correct in-filling of amount and date.\nIn addition,\nit grasps the evidence-based self-verification during stepwise reasoning,\nwhere snapshots that respectively prove the completion of filling amount, date, and expense type are submitted for verification.",
                "position": 1249
            },
            {
                "img": "https://arxiv.org/html/2512.22322/fig/settings-diff.png",
                "caption": "Figure 7:The agent trajectories before and after SmartSnap RL of one testing example of the app Settings (Qwen3-8B).\nInitially,\nthe agent fails to act according to its thought process,\nentering theNetwork & Internetby mistake rather than theDisplay.\nIt fails to jump out of the loop and keeps repeating the erroneous actions.\nAfter SmartSnap RL,\nthe agent learns to utilize thesearchtool in the app and quickly pinpoints the correct page for the theme switch.\nIn addition,\nit grasps the evidence-based self-verification during stepwise reasoning,\nwhere the exact snapshot with the dark theme switch turned on is submitted for verification.",
                "position": 1261
            }
        ]
    },
    {
        "header": "5Conslusion",
        "images": []
    },
    {
        "header": "Contributions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APrompts",
        "images": []
    }
]