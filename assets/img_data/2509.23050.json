[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23050/x1.png",
                "caption": "Figure 1:Framework Overview.For data from two distributionsùí´VT\\mathcal{P}_{\\text{VT}}(vision-dependent) andùí´t\\mathcal{P}_{\\text{t}}(vision-independent), we extract chain-of-embedding for two queries w/ and w/o visual input, and use the expected representation distance to spotvisual integration pointl‚àól^{*}. Then, estimatingtotal visual integrationbased onl‚àól^{*}allows us to quantify LP of an LVLM per sample.",
                "position": 148
            }
        ]
    },
    {
        "header": "2Problem Statement",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23050/x2.png",
                "caption": "Figure 2:Visual Integration Point.We consistently observe that there is a specific layerl‚àól^{*}that clearly distinguish the distance betweenZvislZ^{l}_{\\text{vis}}andZblindlZ^{l}_{\\text{blind}}across two groupsùíüVT\\mathcal{D}_{\\text{VT}}andùíüT\\mathcal{D}_{\\text{T}}.",
                "position": 274
            }
        ]
    },
    {
        "header": "4Extended Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23050/x3.png",
                "caption": "Figure 3:VIPs of different models observed across different datasets.Our novel framework, fueled by contrasting chain-of-embedding, allows us to consistently observe VIP across multiple models and datasets, and further enables us to estimate TVI to measure language prior.",
                "position": 399
            },
            {
                "img": "https://arxiv.org/html/2509.23050/x4.png",
                "caption": "Figure 4:TVI under language priors of different strengths.We see that TVI effectively discerns the differences in strength of LP, thereby standing for a reliable measure for LP.",
                "position": 410
            },
            {
                "img": "https://arxiv.org/html/2509.23050/x5.png",
                "caption": "Figure 5:Ablations on model scales.VIP and the dimension-normalized TVI analysis results for three variants ofGemma-3model family.",
                "position": 627
            }
        ]
    },
    {
        "header": "5Theoretical Analysis",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23050/x6.png",
                "caption": "Figure 6:Complete experimental results.(Part 1)",
                "position": 1546
            },
            {
                "img": "https://arxiv.org/html/2509.23050/x7.png",
                "caption": "Figure 7:Complete experimental results.(Part 2)",
                "position": 1550
            },
            {
                "img": "https://arxiv.org/html/2509.23050/x8.png",
                "caption": "Figure 8:Complete experimental results.(Part 3)",
                "position": 1554
            }
        ]
    },
    {
        "header": "Appendix ARelated Work",
        "images": []
    },
    {
        "header": "Appendix BDiscussion on Visual Integration Point",
        "images": []
    },
    {
        "header": "Appendix CImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23050/x9.png",
                "caption": "Figure 9:Experimental results under controlled settings.We use a synthetic baseline constructed from CommonsenseQA questions paired with irrelevant images asùíüT\\mathcal{D}_{\\text{T}}(vision-independent), while standard VQA benchmarks (MME, MMBench, and VLind-Bench) are used asùíüVT\\mathcal{D}_{\\text{VT}}(vision-dependent).",
                "position": 1657
            }
        ]
    },
    {
        "header": "Appendix DAdditional Experimental Results",
        "images": []
    },
    {
        "header": "Appendix EFurther Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23050/x10.png",
                "caption": "Figure 10:Visualization of visual attention maps under weak and strong language priors.",
                "position": 1748
            },
            {
                "img": "https://arxiv.org/html/2509.23050/x11.png",
                "caption": "Figure 11:Layer-wise representation distances in latent space vs. output space.We apply the logit-lens to project hidden states at each layer into the output space. In (a), distances are computed over the entire output vector, while in (b), they are restricted to the top-kktoken positions corresponding to candidate answer options.",
                "position": 1754
            }
        ]
    },
    {
        "header": "Appendix FDetails on Theoretical Analysis and Proofs",
        "images": []
    },
    {
        "header": "Appendix GImpact Statement",
        "images": []
    },
    {
        "header": "Appendix HLimitations",
        "images": []
    },
    {
        "header": "Appendix IDisclosure of LLM Usage",
        "images": []
    }
]