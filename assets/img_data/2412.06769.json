[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06769/extracted/6056756/figures/figure_1_meta_3.png",
                "caption": "Figure 1:A comparison of Chain of Continuous Thought (Coconut) with Chain-of-Thought (CoT). In CoT, the model generates the reasoning process as a word token sequence (e.g.,[xi,xi+1,‚Ä¶,xi+j]subscriptùë•ùëñsubscriptùë•ùëñ1‚Ä¶subscriptùë•ùëñùëó[x_{i},x_{i+1},...,x_{i+j}][ italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT , ‚Ä¶ , italic_x start_POSTSUBSCRIPT italic_i + italic_j end_POSTSUBSCRIPT ]in the figure).Coconutregards the last hidden state as a representation of the reasoning state (termed ‚Äúcontinuous thought‚Äù), and directly uses it as the next input embedding. This allows the LLM to reason in an unrestricted latent space instead of a language space.",
                "position": 107
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Coconut: Chain of Continuous Thought",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06769/extracted/6056756/figures/figure_2_meta_5.png",
                "caption": "Figure 2:Training procedure of Chain of Continuous Thought (Coconut).\nGiven training data with language reasoning steps, at each training stage we integratecùëêcitalic_cadditional continuous thoughts (c=1ùëê1c=1italic_c = 1in this example), and remove one language reasoning step. The cross-entropy loss is then used on the remaining tokens after continuous thoughts.",
                "position": 171
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06769/x1.png",
                "caption": "Figure 3:Accuracy on GSM8k with different number of continuous thoughts.",
                "position": 360
            },
            {
                "img": "https://arxiv.org/html/2412.06769/extracted/6056756/figures/figure_4_meta.png",
                "caption": "Figure 4:A case study where we decode the continuous thought into language tokens.",
                "position": 383
            }
        ]
    },
    {
        "header": "5Understanding the Latent Reasoning inCoconut",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06769/extracted/6056756/figures/figure_5_revised_1111.png",
                "caption": "Figure 5:The accuracy of final answer (left) and reasoning process (right) of multiple variants ofCoconutand baselines on ProsQA.",
                "position": 416
            },
            {
                "img": "https://arxiv.org/html/2412.06769/extracted/6056756/figures/figure_6_meta_3.png",
                "caption": "Figure 6:A case study of ProsQA. The model trained withCoThallucinates an edge (Every yumpus is a rempus) after getting stuck in a dead end.Coconut(k=1) outputs a path that ends with an irrelevant node.Coconut(k=2) solves the problem correctly.",
                "position": 419
            },
            {
                "img": "https://arxiv.org/html/2412.06769/extracted/6056756/figures/figure_7_meta_4.png",
                "caption": "",
                "position": 425
            },
            {
                "img": "https://arxiv.org/html/2412.06769/extracted/6056756/figures/percentile.png",
                "caption": "Figure 8:Analysis of parallelism in latent tree search. The left plot depicts the cumulative value of the top-1, top-2, and top-3 candidate nodes for the first thoughts, calculated across test cases and ranked by percentile. The significant gaps between the lines reflect the model‚Äôs ability to explore alternative latent thoughts in parallel. The right plot shows the corresponding analysis for the second thoughts, where the gaps between lines are narrower, indicating reduced parallelism and increased certainty in reasoning as the search tree develops. This shift highlights the model‚Äôs transition toward more focused exploration in later stages.",
                "position": 452
            },
            {
                "img": "https://arxiv.org/html/2412.06769/extracted/6056756/figures/value_stats_meta_2.png",
                "caption": "Figure 9:The correlation between prediction probability of concepts and their heights.",
                "position": 465
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Datasets",
        "images": []
    },
    {
        "header": "8Clock-Time Reasoning Efficiency Metric",
        "images": []
    },
    {
        "header": "9Using More Continuous Thoughts",
        "images": []
    }
]