[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15517/x1.png",
                "caption": "Figure 1:Robo2VLM-1 dataset overview. The middle colorbar traces a typical manipulation episodeâ€”from pre-grasp through immobilization, contact, detach, and into post-grasp. Surrounding panels give example questions for each VQA category. Dashed arrows connect every category to the phase(s) in which its questions are sampled. Icons beneath each panel list the key sensing modalities (RGB, stereo depth, wrist/side cameras, gripper state, end-effector pose, language instructions) needed to derive ground-truth answers.",
                "position": 118
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Robo2VLM",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15517/x2.png",
                "caption": "Figure 2:Robo2VLM framework.Robo2VLM generates multi-modal real-world robot trajectories through (1) manipulation phase classification, (2) keyframe selection guided by scene and interaction cues, and (3) structured VQA question prototype.",
                "position": 194
            }
        ]
    },
    {
        "header": "4Robo2VLM-1 Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15517/x3.png",
                "caption": "Figure 3:Distribution and key statistics of Robo2VLM-1 dataset.(Left) Robo2VLM-1 covers diverse scenes with the most frequent scenes in office (33.6%), lab (25.3%), and kitchen (16.9%). (Middle) Robo2VLM-1 covers tasks including common manipulation actions include pick (21.5%), put (20.6%), and move (9.9%). (Right) The table summarizes key dataset statistics including question characteristics, answer choices, and image resolutions.",
                "position": 579
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15517/x4.png",
                "caption": "Figure 4:Fine-tuning LLaVA 1.6 with increasing training data of Robo2VLM-1from 10k to 50k VQA items. Accuracy improvements almost all categories compared to no fine-tuning.",
                "position": 1056
            },
            {
                "img": "https://arxiv.org/html/2505.15517/x5.png",
                "caption": "Figure 5:Comparison of human performance to different multimodal foundation models.",
                "position": 1069
            }
        ]
    },
    {
        "header": "6Conclusion and Discussion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]