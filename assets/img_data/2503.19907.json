[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19907/x1.png",
                "caption": "",
                "position": 73
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19907/x2.png",
                "caption": "Figure 2:Overview ofFullDiTarchitecture and comparison with adapter-based models.We present the diffusion process of the multi-task video generative model on the left. For research purposes, this paper shows input conditions consisting oftemporal-onlycameras,spatial-onlyidentities, andtemporal-spatialdepth video. Additional conditions can be incorporated into this model architecture for broader applications. Shown in (a),FullDiTunifies various inputs with procedures: (1) patchify and tokenize the input condition to a unified sequence representation, (2) concat all sequences together to a longer one, and (3) learn condition dynamics with full self-attention.\nBy comparison, earlier adapter-based approaches (shown in (b)) use distinct adapter designs that operate independently to process various inputs, leading to branch conflicts, parameter redundancy, and suboptimal performance.\nEach blockâ€™s subscript indicates its layer index.",
                "position": 86
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19907/x3.png",
                "caption": "Figure 3:Illustration of the condition training order.We useredto indicate the training data volume. M is for million.",
                "position": 304
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19907/x4.png",
                "caption": "Figure 4:Examples of two types of identity images.",
                "position": 325
            },
            {
                "img": "https://arxiv.org/html/2503.19907/x5.png",
                "caption": "Figure 5:Qualitative comparison ofFullDiTand previous single control video generation methods.We present identity-to-video results compared with ConceptMaster[23], depth-to-video results compared with Ctrl-Adapter[33]and ControlVideo[66], and camera-to-video results compared with MotionCtrl[54], CamI2V[68], and CameraCtrl[18]. Results denoted with * are image-to-video methods.",
                "position": 523
            },
            {
                "img": "https://arxiv.org/html/2503.19907/x6.png",
                "caption": "Figure 6:Qualitative results ofFullDiTwith multiple control signals.We show camera+identity+depth-to-video in (a) and (b), camera+identity-to-video in (c), identity+depth-to-video in (d), and camera+depth-to-video in (e).",
                "position": 555
            },
            {
                "img": "https://arxiv.org/html/2503.19907/x7.png",
                "caption": "Figure 7:Camera-to-Video Performance with the Increase of Training Data Volume.We also show the data volume and performance of MotionCtrl[54]and CamI2V[68]for comparison.",
                "position": 563
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]