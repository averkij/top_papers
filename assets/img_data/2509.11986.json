[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.11986/x1.png",
                "caption": "(a)Input image with red answer mask",
                "position": 122
            },
            {
                "img": "https://arxiv.org/html/2509.11986/x1.png",
                "caption": "(a)Input image with red answer mask",
                "position": 125
            },
            {
                "img": "https://arxiv.org/html/2509.11986/x2.png",
                "caption": "(b)Embedding norm signed difference",
                "position": 130
            },
            {
                "img": "https://arxiv.org/html/2509.11986/x3.png",
                "caption": "(c)Image overlay with norm difference",
                "position": 135
            }
        ]
    },
    {
        "header": "2VLMs and Connectors",
        "images": []
    },
    {
        "header": "3Quantifying Information Loss",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.11986/x4.png",
                "caption": "Figure 2:Thekk-nearest neighbors overlap ratio measures the overlap of an image’s neighbors before and after projection. In this example, withk=3k=3, the overlap ratio is 0.67 because two out of the three nearest neighbors are identical in both representation spaces.",
                "position": 240
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.11986/x5.png",
                "caption": "Figure 3:Neighborhood overlap ratios across three datasets: SeedBench validation, a 10,000-sample subset of VQAv2 validation, and Vizwiz grounding VQA validation. Analysis using 10, 50, and 100 nearest neighbors shows overlap ratios below 0.62 for all models, suggesting connectors poorly preserve geometric relationships and neighbor rankings for the visual representations.",
                "position": 371
            }
        ]
    },
    {
        "header": "5Neighbor Rankings and Structural Information are Not Preserved",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.11986/x6.png",
                "caption": "(a)Five nearest neighbors of LLaVA image embeddings",
                "position": 445
            },
            {
                "img": "https://arxiv.org/html/2509.11986/x6.png",
                "caption": "(a)Five nearest neighbors of LLaVA image embeddings",
                "position": 448
            },
            {
                "img": "https://arxiv.org/html/2509.11986/x7.png",
                "caption": "(b)Five nearest neighbors of Idefics2 image embeddings",
                "position": 454
            },
            {
                "img": "https://arxiv.org/html/2509.11986/x8.png",
                "caption": "(c)Five nearest neighbors of Qwen2.5-VL image embeddings",
                "position": 460
            }
        ]
    },
    {
        "header": "6Reconstruction and Model Behavior",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.11986/x9.png",
                "caption": "Figure 5:Correlation between reconstruction loss and question-answering accuracy on the VizWiz grounding VQA task. For LLaVA and Idefics2, all correlations have app-value <5​e​−5510-5, indicating statistically significant relationships, whereas no clear correlation is observed for Qwen2.5-VL. The reconstruction loss occurs in both answer-relevant and irrelevant patches. Loss in relevant patches negatively affects performance of LLaVA and Idefics2. “Norm” represents differences between theL2L^{2}norm of the embeddings.",
                "position": 547
            }
        ]
    },
    {
        "header": "7Analysis",
        "images": []
    },
    {
        "header": "8Related Work",
        "images": []
    },
    {
        "header": "9Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AConnectors in Autoregressive Vision-Language Models",
        "images": []
    },
    {
        "header": "Appendix BAblation on Index Method forkk-NN Overlap Ratio",
        "images": []
    },
    {
        "header": "Appendix CAdditional Evaluation Results",
        "images": []
    },
    {
        "header": "Appendix DVisualization",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.11986/x10.png",
                "caption": "Figure 6:Alignment visualization for LLaVA pre- and post-projection embeddings through PCA.",
                "position": 1512
            },
            {
                "img": "https://arxiv.org/html/2509.11986/x11.png",
                "caption": "Figure 7:Additional visualization of high reconstruction loss patches that contributes to model’s failure on answering questions that requires recognizing text in the objects. Left: input images with answer-relevant regions in red masks. Middle: signed difference between post-projection embeddings norms and pre-projection embedding norms. Right: normalized norm differences overlay with the input image, with highest loss patches marked in yellow.",
                "position": 1524
            },
            {
                "img": "https://arxiv.org/html/2509.11986/x11.png",
                "caption": "",
                "position": 1527
            },
            {
                "img": "https://arxiv.org/html/2509.11986/x12.png",
                "caption": "",
                "position": 1531
            },
            {
                "img": "https://arxiv.org/html/2509.11986/x13.png",
                "caption": "",
                "position": 1535
            },
            {
                "img": "https://arxiv.org/html/2509.11986/x14.png",
                "caption": "",
                "position": 1540
            },
            {
                "img": "https://arxiv.org/html/2509.11986/x15.png",
                "caption": "",
                "position": 1544
            },
            {
                "img": "https://arxiv.org/html/2509.11986/x16.png",
                "caption": "",
                "position": 1548
            },
            {
                "img": "https://arxiv.org/html/2509.11986/x17.png",
                "caption": "",
                "position": 1553
            },
            {
                "img": "https://arxiv.org/html/2509.11986/x18.png",
                "caption": "",
                "position": 1557
            },
            {
                "img": "https://arxiv.org/html/2509.11986/x19.png",
                "caption": "",
                "position": 1561
            },
            {
                "img": "https://arxiv.org/html/2509.11986/x20.png",
                "caption": "Figure 8:Idefics highkkNN overlap ratio example, where we can observe the reordering among semantically similar vision embeddings.",
                "position": 1567
            },
            {
                "img": "https://arxiv.org/html/2509.11986/x21.png",
                "caption": "",
                "position": 1571
            },
            {
                "img": "https://arxiv.org/html/2509.11986/x22.png",
                "caption": "",
                "position": 1574
            },
            {
                "img": "https://arxiv.org/html/2509.11986/x23.png",
                "caption": "",
                "position": 1576
            },
            {
                "img": "https://arxiv.org/html/2509.11986/x24.png",
                "caption": "",
                "position": 1579
            },
            {
                "img": "https://arxiv.org/html/2509.11986/x25.png",
                "caption": "",
                "position": 1581
            },
            {
                "img": "https://arxiv.org/html/2509.11986/x26.png",
                "caption": "(a)Top five retrieved images of LLaVA image embeddings",
                "position": 1592
            },
            {
                "img": "https://arxiv.org/html/2509.11986/x26.png",
                "caption": "(a)Top five retrieved images of LLaVA image embeddings",
                "position": 1595
            },
            {
                "img": "https://arxiv.org/html/2509.11986/x27.png",
                "caption": "(b)Top five retrieved images of Idefics2 image embeddings",
                "position": 1601
            },
            {
                "img": "https://arxiv.org/html/2509.11986/x28.png",
                "caption": "(c)Top five retrieved images of Qwen2.5-VL image embeddings",
                "position": 1607
            },
            {
                "img": "https://arxiv.org/html/2509.11986/x29.png",
                "caption": "Figure 12:Visualization of low CIDEr score captioning samples and the reconstruction loss overlay with the input image. We can observe that details regarding the high loss patches are missing from the generated captions. High loss patches are marked in yellow squares.",
                "position": 1618
            },
            {
                "img": "https://arxiv.org/html/2509.11986/x30.png",
                "caption": "",
                "position": 1622
            },
            {
                "img": "https://arxiv.org/html/2509.11986/x31.png",
                "caption": "Figure 13:Image reconstruction with LLaVA pre-and post-projection embeddings on out-of-distribution (top) and in-distribution (bottom) examples.",
                "position": 1651
            }
        ]
    },
    {
        "header": "Appendix EImage Reconstruction with Different Embeddings",
        "images": []
    }
]