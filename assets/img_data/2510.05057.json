[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05057/stamo/Figures/icon2.png",
                "caption": "",
                "position": 57
            },
            {
                "img": "https://arxiv.org/html/2510.05057/x1.png",
                "caption": "Figure 1:An overview of ourStaMoframework.Our method efficiently compresses and encodes robotic visual representations, enabling the learning of a compact state representation. Motion naturally emerges as the difference between these states in the highly compressed token space. This approach facilitates efficient world modeling and demonstrates strong generalization, with the potential to scale up with more data.",
                "position": 91
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05057/x2.png",
                "caption": "Figure 2:Where isStaMo?This figure visualizes how different robotic representations fall on the spectrum of expressiveness versus compactness. StaMo uniquely occupies the ideal position, offering both a rich, expressive state representation and the ability to model motion from a highly compact space.",
                "position": 168
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05057/x3.png",
                "caption": "Figure 3:Reconstruct images using ourStaMoencoder with as few as two 1024-dimensional tokens.The first row shows the ground truth, and the second row shows the predicted results, with corresponding PSNR and SSIM metrics listed below. The results demonstrate thatStaMocan preserve high image fidelity and structural similarity even under extremely compressed state representations.",
                "position": 307
            },
            {
                "img": "https://arxiv.org/html/2510.05057/stamo/Figures/LP-MSE_Academic_Full_Borders.png",
                "caption": "Figure 4:Linear Probing MSE results.We compare our method against three baselines. Our method consistently achieves thelowestMSE across all horizons.",
                "position": 525
            },
            {
                "img": "https://arxiv.org/html/2510.05057/stamo/Figures/LP-MSE_Academic_Full_Borders.png",
                "caption": "Figure 4:Linear Probing MSE results.We compare our method against three baselines. Our method consistently achieves thelowestMSE across all horizons.",
                "position": 528
            },
            {
                "img": "https://arxiv.org/html/2510.05057/stamo/Figures/scaling_performance_compact_x.png",
                "caption": "Figure 5:Scaling Performance.The Performace of our model can be steadly scaling with more data, including human ego-centric data.",
                "position": 533
            },
            {
                "img": "https://arxiv.org/html/2510.05057/x4.png",
                "caption": "Figure 6:Real World Setting and Tasks.We designed a benchmark of six real-world robotics tasks, spanning both short- and long-horizon challenges, to evaluate the effectiveness of our representation for learning world models.",
                "position": 552
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05057/x5.png",
                "caption": "Figure 7:Visual reconstruction results from the same episode.The first and last frames are reconstructed from ground-truth images using theStaMoencoder, while the intermediate frames are generated by linearly interpolating between the latent state tokens of the two endpoints. The transitions show that both the robotic arm and the objects move in a continuous and smooth manner.",
                "position": 1355
            },
            {
                "img": "https://arxiv.org/html/2510.05057/x6.png",
                "caption": "Figure 8:Transfer linear interpolation experiment with theStaMoencoder.The left and right panels illustrate different task scenarios, where reconstructions are obtained by tokens(3) + tokens(2) â€“ tokens(1), demonstrating the linear interpolation property of latent representations during transfer.",
                "position": 1358
            }
        ]
    },
    {
        "header": "Appendix BAction Linear Probing Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05057/x7.png",
                "caption": "Figure 9:MLP-predicted actions replay.Actions are predicted by an MLP probe from ourStaMolatent action representation and replayed in LIBERO-10 tasks. The results show that our latent actions are remarkably effective, encoding functionally coherent and executable motions.",
                "position": 1451
            }
        ]
    },
    {
        "header": "Appendix CReal World Setting",
        "images": []
    }
]