[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12963/x1.png",
                "caption": "Figure 1:The proposed KDTalker: A keypoint-based spatiotemporal diffusion framework that generates synchronized, high-fidelity talking videos from audio and a single image, enhancing pose diversity and expression detail with realistic, temporally consistent animations.",
                "position": 185
            },
            {
                "img": "https://arxiv.org/html/2503.12963/extracted/6286149/fig/Inference_Time_vs_Head_Diversity_LSE-D.png",
                "caption": "Figure 2:Inference Time vs Head Diversity&\\&&LSE-D. The value of LSE-D (Lip Sync Error Distance), a metric quantifying the alignment between lip movements and audio, is represented by the size of the circle. A smaller circle indicates a lower LSE-D value, reflecting better lip sync performance.",
                "position": 222
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12963/x2.png",
                "caption": "Figure 3:Overview of the proposed KDTalker for talking portrait synthesis.",
                "position": 258
            },
            {
                "img": "https://arxiv.org/html/2503.12963/x3.png",
                "caption": "Figure 4:Reference-Guided Priors.",
                "position": 281
            },
            {
                "img": "https://arxiv.org/html/2503.12963/x4.png",
                "caption": "Figure 5:Spatiotemporal-Aware Attention Network.",
                "position": 360
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12963/x5.png",
                "caption": "Figure 6:Qualitative comparison with the state-of-the-art methods on HDTF dataset.",
                "position": 533
            },
            {
                "img": "https://arxiv.org/html/2503.12963/x6.png",
                "caption": "Figure 7:Qualitative comparison of head motion diversity between existing methods and our proposed approach.",
                "position": 536
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]