[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.13348/x1.png",
                "caption": "Figure 1:Our key observations and VisionThink performance and efficiency.Left: We find that in most general scenarios, even reducing visual tokens by a factor of four results in only minimal performance drop. However, token compression leads to a significant performance drop on strong OCR-related benchmarks.Right: Our VisionThink significantly outperforms previous work in both performance and efficiency.",
                "position": 133
            }
        ]
    },
    {
        "header": "2Preliminary",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.13348/x2.png",
                "caption": "Figure 2:Framework of VisionThink.(a) The left image illustrates VisionThink processing an image with resolution reduced by a factor of four, where the VLM directly provides an answer. (b) The right image shows a case where the model detects insufficient information and requests a high-resolution image to answer the question.",
                "position": 243
            },
            {
                "img": "https://arxiv.org/html/2507.13348/x3.png",
                "caption": "Figure 3:(a) Impact of the Penalty Ratio. Applying a penalty to all resize image requests or removing the penalty entirely will both lead to model collapse.\n(b) VisionThink correctly solves OCR-related problems by autonomously requesting high-resolution images.",
                "position": 332
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.13348/x4.png",
                "caption": "Figure 4:Inference Time Cost and Benchmark Performance Comparison for Reasoning Model. Qwen-RL and Qwen-RLÂ (1/4) represent leveraging the LLM-as-Judge on the Qwen2.5-VL-Instruct Model and inference on full resolution image and 1/4 resolution image, respectively.",
                "position": 638
            },
            {
                "img": "https://arxiv.org/html/2507.13348/x5.png",
                "caption": "Figure 5:VisionThink smartly determine the high-resolution image ratio.Apply Resize indicates that the model autonomously requests to view the original high-resolution image, while Direct Answer indicates that the model is able to answer the question using only the 1/4-sized image.",
                "position": 882
            }
        ]
    },
    {
        "header": "5Related Works",
        "images": []
    },
    {
        "header": "6Concluding Remarks",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelated Works",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.13348/x6.png",
                "caption": "Figure 6:An example illustrating the original evaluation method used in ChartQA.",
                "position": 2614
            }
        ]
    },
    {
        "header": "Appendix CFurther Discussions",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.13348/x7.png",
                "caption": "Figure 7:Impact of Prompt Choice.Prompts lead to substantial variation in image resize call ratios, with the Qwen official agent prompt demonstrating the most effective performance.",
                "position": 3549
            },
            {
                "img": "https://arxiv.org/html/2507.13348/x8.png",
                "caption": "Figure 8:Ablation Study on Penalty Ratio Threshold.As the threshold increases, the model progressively favors requesting image resizing instead of providing direct answers.",
                "position": 3638
            }
        ]
    },
    {
        "header": "Appendix DQualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.13348/x9.png",
                "caption": "",
                "position": 3671
            },
            {
                "img": "https://arxiv.org/html/2507.13348/x10.png",
                "caption": "",
                "position": 3673
            },
            {
                "img": "https://arxiv.org/html/2507.13348/x11.png",
                "caption": "",
                "position": 3675
            }
        ]
    },
    {
        "header": "Appendix EBroader Impact Statement",
        "images": []
    }
]