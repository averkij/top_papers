[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10061/x1.png",
                "caption": "",
                "position": 87
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10061/x2.png",
                "caption": "Figure 1:Comparison of Inference-time Reasoning Models.(a) Equipping image models with external verifier. (b) Interleaving textual planning within unified multimodal large language models. (c)CoF-T2I: Our proposed video-based CoF reasoning model.",
                "position": 119
            },
            {
                "img": "https://arxiv.org/html/2601.10061/x3.png",
                "caption": "Figure 2:Visualizations of CoF-T2I output.We visualize the reasoning trajectories generated by CoF-T2I. For each example, the final output is shown in large, and the intermediate latent frames are shown in small.",
                "position": 122
            },
            {
                "img": "https://arxiv.org/html/2601.10061/x4.png",
                "caption": "Figure 3:Overview of CoF-T2I.CoF-T2I builds on a video generation backbone, reframing inference-time reasoning for T2I generation as a CoF refinement process.Training.Given a CoF trajectory, we employ a video VAE to encode each frame, and optimize a vanilla flow matching objective.Inference.Starting from noisy initialization, the model denoises to sample a progressively refined reasoning trajectory internalized during training, only the final-frame latent is fully decoded and taken as the output image.Quality assessment.Along the CoF trajectory, text-image alignment and aesthetic quality continue to improve.",
                "position": 164
            }
        ]
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10061/x5.png",
                "caption": "Figure 4:Curation Pipeline for CoF-Evol-Instruct.Aquality-aware construction pipelineto curate reasoning data. We generate an initial pool of images across diverse distributions and dynamically route valid samples. These images are then expanded into complete CoF sequences through targeted construction strategies. Our pipeline ensures both sample-level diversity and frame-wise consistency.",
                "position": 214
            },
            {
                "img": "https://arxiv.org/html/2601.10061/x6.png",
                "caption": "Figure 5:Visualization ofCoF-Evol-InstructDataset.We showcase the prompt and correspondingCoFtrajectories in our data, includingfivecategories:Attribute Binding,Object Combination,Spatial Arrangement,Context Manipulation, andQuantity Control.",
                "position": 251
            },
            {
                "img": "https://arxiv.org/html/2601.10061/x7.png",
                "caption": "Table 1:Performance comparison on GenEval.The best and the second bestOverallscores are inboldandunderlined, respectively.",
                "position": 288
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10061/x7.png",
                "caption": "Table 2:Performance comparison on Imagine-Bench.The best and the second best scores are inboldandunderlined, respectively.",
                "position": 861
            },
            {
                "img": "https://arxiv.org/html/2601.10061/x7.png",
                "caption": "Figure 6:Evolution of generation quality across reasoning steps.We visualize the progressive improvement on both GenEval (left) and Imagine-Bench (right). The results exhibit a general ascending trend in performance scores across the inference steps.",
                "position": 1437
            }
        ]
    },
    {
        "header": "4Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix Overview",
        "images": []
    },
    {
        "header": "Appendix ARelated Works",
        "images": []
    },
    {
        "header": "Appendix BMore Dataset Details",
        "images": []
    },
    {
        "header": "Appendix CMore Experiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10061/x8.png",
                "caption": "Figure 7:Qualitative Analysis: Reasoning Trajectories and Comparisons forCoF-T2I",
                "position": 2478
            }
        ]
    },
    {
        "header": "Appendix DLimitations and Future Work",
        "images": []
    },
    {
        "header": "Appendix EQualitative Examples",
        "images": []
    }
]