[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14802/x1.png",
                "caption": "Figure 1:Evaluation of continual learning capabilities across three key dimensions: factual memory (NaturalQuestions, PopQA), sense-making (NarrativeQA), and associativity (MuSiQue, 2Wiki, HotpotQA, and LV-Eval).\nHippoRAG 2 surpasses other methods across all benchmark categories, bringing it one step closer to a true long-term memory system.",
                "position": 136
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14802/x2.png",
                "caption": "Figure 2:HippoRAG 2 methodology.\nFor offline indexing, we use an LLM to extract open KG triples from passages, with synonym detection applied to phrase nodes. Together, these phrases and passages form the open KG.\nFor online retrieval, an embedding model scores both the passages and triples to identify the seed nodes of both types for the Personalized PageRank (PPR) algorithm. Recognition memory filters the top triples using an LLM. The PPR algorithm then performs context-based retrieval on the KG to provide the most relevant passages for the final QA task. The different colors shown in the KG nodes above reflect their probability mass; darker shades indicate higher probabilities induced by the PPR process.",
                "position": 200
            }
        ]
    },
    {
        "header": "3HippoRAG 2",
        "images": []
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Results",
        "images": []
    },
    {
        "header": "6Discussions",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendices",
        "images": []
    },
    {
        "header": "Appendix ALLM Prompts",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14802/x3.png",
                "caption": "Figure 3:LLM prompts for triple filtering (recognition memory).",
                "position": 1780
            }
        ]
    },
    {
        "header": "Appendix BPipeline Example",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14802/x4.png",
                "caption": "Figure 4:An example of HippoRAG 2 pipeline.",
                "position": 1790
            }
        ]
    },
    {
        "header": "Appendix CDetailed Experimental Results",
        "images": []
    },
    {
        "header": "Appendix DGraph Statistics",
        "images": []
    },
    {
        "header": "Appendix EError Analysis",
        "images": []
    },
    {
        "header": "Appendix FCost and Efficiency",
        "images": []
    },
    {
        "header": "Appendix GImplementation Details and Hyperparameters",
        "images": []
    }
]