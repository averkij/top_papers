[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06938/plots/figure1.png",
                "caption": "Figure 1:Workflow to examine hallucination risk in each layer of the transformer.To assess the extent to which transformer models infer meaningful concepts from semantically void inputs, we study the semanticity of concepts in SAEs trained on residual stream activations of 1.3 million Gaussian noise samples from a pre-trained CLIP vision transformer at each layer by probing the concepts with natural images from the ImageNet-1k validation set. Many concepts are highly interpretable and consistent, despite the SAEs only ever being exposed to pure noise residual stream activations during training. While this noise training setup is specific to our first experiment, subsequent experiments adopt the same concept evaluation approach with alternative transformer models and input modalities. See Preliminaries and Appendix A for further experimental details.",
                "position": 100
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06938/x1.png",
                "caption": "Figure 2:Semantic concepts are reliably invoked by transformer layer 9 activations from pure noise.Each panel shows the top 4 images among 50,000 candidate images from the ImageNet-1k validation set that most activated a particular semantic concept. These semantic concepts are defined by an SAE trained on pure noise activations from the residual stream at layer 9 of a CLIP vision transformer. Above each image we report the corresponding semantic label for that image. Patch colors indicate the individual patch activation strengths within that image for a given semantic concept (yellow = more activation of the concept). See Appendix B for additional concept examples.",
                "position": 200
            }
        ]
    },
    {
        "header": "3Transformers represent semantic concepts for pure noise inputs",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06938/x2.png",
                "caption": "Figure 3:Transformers fed with noise inputs lead to neuron activations with detectable and controllable semantic structure in many layers.We measure the interpretability of a concept by the semantic similarity of the labels of the top 16 images that maximally activate that concept. As a more stringent test, we also measure a concept’s steerability by the ability of that concept to causally induce its own class label when added to the residual stream of neutral input images. We find that a very large portion of our noise-derived semantic concepts are highly interpretable, and a non-negligible number of these concepts are even steerable, particularly in the early and middle layers. We report the percentage of unique concepts across our noise-trained SAE meeting the aforementioned interpretability and steerability thresholds.",
                "position": 217
            },
            {
                "img": "https://arxiv.org/html/2509.06938/x3.png",
                "caption": "Figure 4:Increasing uncertainty in vision or text inputs elicits more semantic structure in mid-layers of transformers.The average number of SAE concepts identified (L0) increases dramatically with increasing input perturbation. We report the average change in L0 from baseline, corresponding to the number of SAE concepts with non-zero activations, across (a) patch-shuffled image activations and (b) n-gram-shuffled text activations for each transformer layer. Error bars show one standard deviation. Smaller patches and lower n-gram count induce greater input uncertainty for images and text, respectively. For both modalities, the L0 difference between natural inputs and perturbed inputs peaks in the middle layers, and increases with increasing levels of deliberate scrambling of transformer input information.",
                "position": 226
            }
        ]
    },
    {
        "header": "4More unstructured transformer inputs invoke more semantic concepts",
        "images": []
    },
    {
        "header": "5Input-related internal concept activations predict output hallucinations",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06938/x4.png",
                "caption": "Figure 5:Transformer layer activations can be used to directly predict risk of hallucinated model output.(a) Hallucination score prediction for the task of faithful summarization of 1,006 source articles. We compare Gemma 2B-IT-generated summaries against the ground truth source articles. We use the sparse SAE concept activations, derived from Gemma 2B-IT residual stream activations, as input to a PLS regression model, predicting the hallucination score for each example. We report 10-fold cross-validated coefficient of determination (R2R^{2}) on unseen examples, with error bars showing one standard deviation. (b) Suppressing the top 10 SAE concepts in Layer 11’s residual stream, identified by the PLS model to be the primary drivers of hallucination, significantly reduces mean hallucination scores across the top quartile most hallucinated examples (n=252n=252). We show a histogram of hallucination scores before (grey) and after (blue) suppression: many examples report significant reductions in hallucination, with a mean score drop of 0.19 in this subset (dashed lines).",
                "position": 249
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASAE hyperparameters and training metrics",
        "images": []
    },
    {
        "header": "Appendix BAdditional noise trained SAE concept examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06938/x5.png",
                "caption": "Figure 6:Maximally activating images for three features from an SAE trained on Layer 3 activations of a ViT with Gaussian noise input. These features, also representative of early transformer layers (Layers 1 and 2), include: (i) a positional feature that consistently activates for the final patch token across rows, (ii) a texture-sensitive feature that activates for highly repetitive, noise-like patterns resembling dishrags, and (iii) a feature capturing water-like backgrounds with birds in the foreground.",
                "position": 1444
            },
            {
                "img": "https://arxiv.org/html/2509.06938/x6.png",
                "caption": "Figure 7:Maximally activating images for three features from an SAE trained on Layer 4 activations of a ViT with Gaussian noise input. These features remain grounded in low-level visual patterns but display more variation compared to earlier layers, without forming coherent semantic concepts:\ni) a feature that activates for vertically folded fabrics such as theater curtains and mosquito nets, likely driven by repetitive linear structures;\nii) a feature that responds to mesh-covered textures, such as window shades with overlaid nets, producing a noisy visual appearance;\niii) a feature that activates for grassy textures, possibly due to fine-grained spatial repetition.",
                "position": 1448
            },
            {
                "img": "https://arxiv.org/html/2509.06938/x7.png",
                "caption": "Figure 8:Maximally activating images for three features from an SAE trained on Layer 5 activations of a ViT with Gaussian noise input. At this layer, semantic structure begins to emerge, marking a departure from purely textural or position-based features:\ni) a feature that activates for natural scenes containing clouds and water bodies, indicating sensitivity to environmental layouts;\nii) a feature that responds to food items with soft, clumped textures—such as mashed potatoes, ice cream, and cauliflower—highlighting texture-based abstraction;\niii) a feature that activates for the presence of printed text, capturing more symbolic and structured content.",
                "position": 1455
            },
            {
                "img": "https://arxiv.org/html/2509.06938/x8.png",
                "caption": "Figure 9:Maximally activating images for three features from an SAE trained on Layer 6 activations of a ViT with Gaussian noise input. At this layer, we have features with clear semantics:\ni) a feature that activates for furry dogs;\nii) a feature that activates for food items such as cheeseburgers;\niii) a feature that activates for the watermarks on images.",
                "position": 1462
            },
            {
                "img": "https://arxiv.org/html/2509.06938/x9.png",
                "caption": "Figure 10:Maximally activating images for three features from an SAE trained on Layer 7 activations of a ViT with Gaussian noise input. The features at this depth increasingly correspond to distinct semantic categories and complex scene elements:\ni) a feature that activates for dogs with upright posture and distinct facial structure, reflecting refined animal categorization;\nii) a feature that responds to natural landscapes featuring prominent vertical structures such as mountain peaks, suggesting emerging scene-level abstraction;\niii) a feature that activates for human crowds or densely populated scenes, particularly those with small, background-scale human figures.",
                "position": 1469
            },
            {
                "img": "https://arxiv.org/html/2509.06938/x10.png",
                "caption": "Figure 11:Maximally activating images for three features from an SAE trained on Layer 8 activations of a ViT with Gaussian noise input. At this stage, features begin to consolidate around coherent semantic clusters that span both foreground entities and background textures:\ni) a feature that activates for human fingers, in the context of them being used to hold some organic objects;\nii) a feature that activates for dog ears, showing refined selectivity for specific facial features;\niii) a feature that strongly activates for odometers.",
                "position": 1476
            },
            {
                "img": "https://arxiv.org/html/2509.06938/x11.png",
                "caption": "Figure 12:Maximally activating images for three features from an SAE trained on Layer 9 activations of a ViT with Gaussian noise input. These features reveal increasingly abstracted and high-level regularities in structured visual scenes:\ni) a feature that activates for glossy or metallic surfaces with high reflectance, capturing regularities in material properties and lighting conditions;\nii) a feature that activates for scenes with snow-covered elements;\niii) a feature that activates for bookshelves or library aisles.",
                "position": 1483
            },
            {
                "img": "https://arxiv.org/html/2509.06938/x12.png",
                "caption": "Figure 13:Maximally activating images for three features from an SAE trained on Layer 10 activations of a ViT with Gaussian noise input. These features demonstrate increasing semantic clarity and category specificity:\ni) a feature that activates for close-up views of birds of prey and owls, capturing high-level animal features such as eyes and beaks;\nii) a feature that activates for elevated natural landscapes, including snowy peaks, alpine valleys, and volcanoes, emphasizing large-scale geographic structures;\niii) a feature that activates for floral structures—especially daisy-like flowers.",
                "position": 1490
            },
            {
                "img": "https://arxiv.org/html/2509.06938/x13.png",
                "caption": "Figure 14:Maximally activating images for three features from an SAE trained on Layer 11 activations of a ViT with Gaussian noise input. These features exhibit advanced semantic grouping and fine-grained category emergence:\ni) a feature that activates for sharks, in different perspectives underwater;\nii) a feature that activates for historical architecture and vaulted interiors;\niii) a feature that activates for small to medium-sized furry dog breeds.",
                "position": 1498
            },
            {
                "img": "https://arxiv.org/html/2509.06938/x14.png",
                "caption": "Figure 15:Maximally activating images for three features from an SAE trained on Layer 12 activations of a transformer with Gaussian noise input. At this deepest layer, we observe strong category-level alignment and rich semantic specificity:\ni) a feature that activates for jack-o’-lanterns, capturing lighting, shape, and facial carve patterns across varied contexts;\nii) a feature that consistently activates for elephants, invariant to pose and background, indicating robust object identity abstraction;\niii) a feature that activates for cat faces, spanning multiple breeds and subspecies, reflecting high-resolution encoding of facial features and textures.",
                "position": 1505
            }
        ]
    },
    {
        "header": "Appendix CNoise trained SAE concept overlap with different initializations",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06938/x15.png",
                "caption": "Figure 16:Conceptual overlap dips in the middle layers of the vision transformer. We plot the Jaccard index between the concepts identified by SAEs trained on Gaussian noise for two different random initializations (left Y-axis). We have also super-imposed the change in L0 plot for the 28x28 patch shuffled images from Figure 4a for reference (right Y-axis). There is significantly more concept overlap in the early and later layers, which declines in the middle layers. This is an inverse relationship to the increase in L0 observed for patch shuffled images.",
                "position": 1518
            }
        ]
    },
    {
        "header": "Appendix DLayer-wise L0 for unstructured inputs",
        "images": []
    },
    {
        "header": "Appendix EHallucination task specifications and additional results",
        "images": []
    },
    {
        "header": "Appendix FImpact statement",
        "images": []
    },
    {
        "header": "NeurIPS Paper Checklist",
        "images": []
    }
]