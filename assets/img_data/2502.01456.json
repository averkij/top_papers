[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01456/x1.png",
                "caption": "Figure 1:Overall math performance. Eurus-2-7B-PRIME excels at competition-level mathematics benchmarks, outperforming advanced math models and larger models. Notably, PRIME brings substantial performance gain (+16.7%) over Eurus-2-7B-SFT.",
                "position": 103
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Reinforcement Learning for LLMs and the Challenges of Incoporating Dense Rewards",
        "images": []
    },
    {
        "header": "3PRIME",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01456/x2.png",
                "caption": "Figure 2:Illustration of PRIME. PRIME follows that (1) initialize policy model and the Implicit PRM both with the reference model; (2) sample multiple responses for each prompt and filter with output accuracy; (3) obtain implicit process rewards by the Implicit PRM and update it using cross-entropy (CE) loss; (4) compute advantage and policy loss then update the policy model.",
                "position": 375
            },
            {
                "img": "https://arxiv.org/html/2502.01456/x3.png",
                "caption": "Figure 3:Impact of online prompt filtering on training rewards.",
                "position": 461
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01456/x4.png",
                "caption": "(a)Outcome training rewards (10-step moving).",
                "position": 669
            },
            {
                "img": "https://arxiv.org/html/2502.01456/x4.png",
                "caption": "(a)Outcome training rewards (10-step moving).",
                "position": 672
            },
            {
                "img": "https://arxiv.org/html/2502.01456/x5.png",
                "caption": "(b)Test accuracy across different gradient steps.",
                "position": 677
            }
        ]
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01456/x6.png",
                "caption": "(a)Outcome training rewards (10-step moving).",
                "position": 705
            },
            {
                "img": "https://arxiv.org/html/2502.01456/x6.png",
                "caption": "(a)Outcome training rewards (10-step moving).",
                "position": 708
            },
            {
                "img": "https://arxiv.org/html/2502.01456/x7.png",
                "caption": "(b)Test accuracy across different gradient steps.",
                "position": 713
            },
            {
                "img": "https://arxiv.org/html/2502.01456/x8.png",
                "caption": "Figure 6:Impact of PRM online update.The offline PRM is gradully been overoptimized while online PRMs achieve higher accuracy throughout training.",
                "position": 731
            },
            {
                "img": "https://arxiv.org/html/2502.01456/extracted/6173597/figures/images/policy_ref.png",
                "caption": "(a)Policy ref: We use the policy logprob asœÄrefsubscriptùúãref\\pi_{\\text{ref}}italic_œÄ start_POSTSUBSCRIPT ref end_POSTSUBSCRIPTfor PRM.",
                "position": 751
            },
            {
                "img": "https://arxiv.org/html/2502.01456/extracted/6173597/figures/images/policy_ref.png",
                "caption": "(a)Policy ref: We use the policy logprob asœÄrefsubscriptùúãref\\pi_{\\text{ref}}italic_œÄ start_POSTSUBSCRIPT ref end_POSTSUBSCRIPTfor PRM.",
                "position": 754
            },
            {
                "img": "https://arxiv.org/html/2502.01456/extracted/6173597/figures/images/sfr_ref.png",
                "caption": "(b)SFT ref: We retain the initial policy to provideœÄrefsubscriptùúãref\\pi_{\\text{ref}}italic_œÄ start_POSTSUBSCRIPT ref end_POSTSUBSCRIPTfor PRM and KL.",
                "position": 759
            },
            {
                "img": "https://arxiv.org/html/2502.01456/x9.png",
                "caption": "Figure 8:Different reference model for PRM.We compare two reference model selection strategies for PRIME. Using the policy model as reference and using the initial SFT model as reference. Their rewards are similar.",
                "position": 766
            },
            {
                "img": "https://arxiv.org/html/2502.01456/x10.png",
                "caption": "(a)PRM classification accuracy on training samples.",
                "position": 776
            },
            {
                "img": "https://arxiv.org/html/2502.01456/x10.png",
                "caption": "(a)PRM classification accuracy on training samples.",
                "position": 779
            },
            {
                "img": "https://arxiv.org/html/2502.01456/x11.png",
                "caption": "(b)Training outcome rewards.",
                "position": 784
            },
            {
                "img": "https://arxiv.org/html/2502.01456/x12.png",
                "caption": "Figure 10:PRIME also benefits REINFORCE, GRPO, and PPO, achieving similar improvement as RLOO.",
                "position": 981
            },
            {
                "img": "https://arxiv.org/html/2502.01456/x12.png",
                "caption": "Figure 10:PRIME also benefits REINFORCE, GRPO, and PPO, achieving similar improvement as RLOO.",
                "position": 984
            },
            {
                "img": "https://arxiv.org/html/2502.01456/x13.png",
                "caption": "Figure 11:Comparison of value models and reward models. We show that value models, either the original PPO one or Implicit PRM, is substaintially worse than reward models.",
                "position": 989
            },
            {
                "img": "https://arxiv.org/html/2502.01456/x14.png",
                "caption": "(a)Outcome training rewards (10-step moving).",
                "position": 1027
            },
            {
                "img": "https://arxiv.org/html/2502.01456/x14.png",
                "caption": "(a)Outcome training rewards (10-step moving).",
                "position": 1030
            },
            {
                "img": "https://arxiv.org/html/2502.01456/x15.png",
                "caption": "(b)Math test accuracy across different gradient steps.",
                "position": 1035
            },
            {
                "img": "https://arxiv.org/html/2502.01456/x16.png",
                "caption": "(a)Outcome training rewards (10-step moving).",
                "position": 1042
            },
            {
                "img": "https://arxiv.org/html/2502.01456/x16.png",
                "caption": "(a)Outcome training rewards (10-step moving).",
                "position": 1045
            },
            {
                "img": "https://arxiv.org/html/2502.01456/x17.png",
                "caption": "(b)Math test accuracy across different gradient steps.",
                "position": 1050
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASFT Data & Training Details",
        "images": []
    },
    {
        "header": "Appendix BRL Data Preprocessing",
        "images": []
    }
]