[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Experimental Setup",
        "images": []
    },
    {
        "header": "4Latent Reasoning Identification",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02996/x1.png",
                "caption": "Figure 1:Pass@kkaccuracy (k=1,5,10k=1,5,10) and gold-in-trace rate under reasoning-trace truncation for R1-Qwen-32B.\nHigh accuracy with a low gold-in-trace rate indicates latent reasoning.\nThe model shows strong evidence of latent reasoning in high-resource languages (e.g., English) on MGSM, but it is less detectable on\nMultilingual AIME.",
                "position": 289
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x2.png",
                "caption": "",
                "position": 293
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x3.png",
                "caption": "",
                "position": 295
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x4.png",
                "caption": "",
                "position": 297
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x5.png",
                "caption": "",
                "position": 299
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x6.png",
                "caption": "",
                "position": 301
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x7.png",
                "caption": "",
                "position": 303
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x8.png",
                "caption": "",
                "position": 305
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x9.png",
                "caption": "",
                "position": 307
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x10.png",
                "caption": "",
                "position": 309
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x11.png",
                "caption": "Figure 2:Causal decomposition of newly correct predictions across truncation intervals.\nEach bar partitions gains into three cases: (i) the gold answer is first articulated in the newly added reasoning steps, (ii) it was already articulated in earlier steps, or (iii) it has not yet appeared in the visible truncated trace.\nOn MGSM, performance improvements at early and intermediate truncation ratios are dominated by case (iii), indicating that many gains arise from latent reasoning.",
                "position": 316
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x12.png",
                "caption": "",
                "position": 319
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x13.png",
                "caption": "",
                "position": 320
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x14.png",
                "caption": "",
                "position": 322
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x15.png",
                "caption": "",
                "position": 323
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x16.png",
                "caption": "",
                "position": 324
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x17.png",
                "caption": "Figure 3:Layer-wise rank of the gold answer obtained via\nlogit lens across languages on MGSM (left three\npanels) and Multilingual AIME (right three panels).\nRank trajectories exhibit highly similar trends across languages, suggesting that latent reasoning progresses through comparable layer-wise transformations regardless of language.",
                "position": 737
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x18.png",
                "caption": "",
                "position": 740
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x19.png",
                "caption": "",
                "position": 741
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x20.png",
                "caption": "",
                "position": 742
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x21.png",
                "caption": "",
                "position": 743
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x22.png",
                "caption": "",
                "position": 744
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x23.png",
                "caption": "Figure 4:Aggregated cosine similarity between hidden states in each language and English (reference), averaged over both reasoning steps and layers, for R1-Qwen-32B.\nHigh-resource languages show consistently higher similarity to English, suggesting convergence toward an English-centered latent reasoning pathway.",
                "position": 751
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x24.png",
                "caption": "",
                "position": 754
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x25.png",
                "caption": "",
                "position": 755
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x26.png",
                "caption": "",
                "position": 756
            }
        ]
    },
    {
        "header": "5Latent State Dynamics",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02996/x27.png",
                "caption": "Figure 5:Comparison of cosine similarity with English versus average similarity with other languages, shown separately for correctly and incorrectly solved examples.\nHigh-resource languages show stronger alignment with English, whereas low- and mid-resource languages show weaker or correctness-dependent alignment.",
                "position": 816
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x28.png",
                "caption": "",
                "position": 819
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x29.png",
                "caption": "",
                "position": 820
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x30.png",
                "caption": "",
                "position": 821
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x31.png",
                "caption": "",
                "position": 822
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x32.png",
                "caption": "",
                "position": 824
            }
        ]
    },
    {
        "header": "6Complementary Analysis: Memorization or Latent Reasoning",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethical Considerations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AReasoning Trace Statistics",
        "images": []
    },
    {
        "header": "Appendix BComplete Truncation Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02996/x33.png",
                "caption": "Figure 6:Pass@kkaccuracy (k=1,5,10k=1,5,10) and gold-in-trace rate under reasoning-trace truncation forR1-Qwen-7BonMGSM. The model shows stronger latent reasoning in high-resource languages (e.g., English).",
                "position": 2327
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x34.png",
                "caption": "",
                "position": 2330
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x35.png",
                "caption": "",
                "position": 2331
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x36.png",
                "caption": "",
                "position": 2332
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x37.png",
                "caption": "",
                "position": 2334
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x38.png",
                "caption": "",
                "position": 2335
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x39.png",
                "caption": "",
                "position": 2336
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x40.png",
                "caption": "",
                "position": 2337
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x41.png",
                "caption": "",
                "position": 2339
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x42.png",
                "caption": "",
                "position": 2340
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x43.png",
                "caption": "",
                "position": 2341
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x44.png",
                "caption": "Figure 7:Pass@kkaccuracy (k=1,5,10k=1,5,10) and gold-in-trace rate under reasoning-trace truncation forR1-Qwen-14BonMGSM. The model shows stronger latent reasoning in high-resource languages (e.g., English).",
                "position": 2347
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x45.png",
                "caption": "",
                "position": 2350
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x46.png",
                "caption": "",
                "position": 2351
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x47.png",
                "caption": "",
                "position": 2352
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x48.png",
                "caption": "",
                "position": 2354
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x49.png",
                "caption": "",
                "position": 2355
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x50.png",
                "caption": "",
                "position": 2356
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x51.png",
                "caption": "",
                "position": 2357
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x52.png",
                "caption": "",
                "position": 2359
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x53.png",
                "caption": "",
                "position": 2360
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x54.png",
                "caption": "",
                "position": 2361
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x55.png",
                "caption": "Figure 8:Pass@kkaccuracy (k=1,5,10k=1,5,10) and gold-in-trace rate under reasoning-trace truncation forR1-Qwen-32BonMGSM. The model shows stronger latent reasoning in high-resource languages (e.g., English).",
                "position": 2365
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x56.png",
                "caption": "",
                "position": 2368
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x57.png",
                "caption": "",
                "position": 2369
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x58.png",
                "caption": "",
                "position": 2370
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x59.png",
                "caption": "",
                "position": 2372
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x60.png",
                "caption": "",
                "position": 2373
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x61.png",
                "caption": "",
                "position": 2374
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x62.png",
                "caption": "",
                "position": 2375
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x63.png",
                "caption": "",
                "position": 2377
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x64.png",
                "caption": "",
                "position": 2378
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x65.png",
                "caption": "",
                "position": 2379
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x66.png",
                "caption": "Figure 9:Pass@kkaccuracy (k=1,5,10k=1,5,10) and gold-in-trace rate under reasoning-trace truncation forR1-Qwen-7BonMultilingual AIME. Latent reasoning is less pronounced compared to MGSM.",
                "position": 2383
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x67.png",
                "caption": "",
                "position": 2386
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x68.png",
                "caption": "",
                "position": 2387
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x69.png",
                "caption": "",
                "position": 2388
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x70.png",
                "caption": "",
                "position": 2390
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x71.png",
                "caption": "",
                "position": 2391
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x72.png",
                "caption": "",
                "position": 2392
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x73.png",
                "caption": "",
                "position": 2393
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x74.png",
                "caption": "",
                "position": 2395
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x75.png",
                "caption": "",
                "position": 2396
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x76.png",
                "caption": "",
                "position": 2397
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x77.png",
                "caption": "Figure 10:Pass@kkaccuracy (k=1,5,10k=1,5,10) and gold-in-trace rate under reasoning-trace truncation forR1-Qwen-14BonMultilingual AIME. Latent reasoning is less pronounced compared to MGSM.",
                "position": 2403
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x78.png",
                "caption": "",
                "position": 2406
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x79.png",
                "caption": "",
                "position": 2407
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x80.png",
                "caption": "",
                "position": 2408
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x81.png",
                "caption": "",
                "position": 2410
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x82.png",
                "caption": "",
                "position": 2411
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x83.png",
                "caption": "",
                "position": 2412
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x84.png",
                "caption": "",
                "position": 2413
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x85.png",
                "caption": "",
                "position": 2415
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x86.png",
                "caption": "",
                "position": 2416
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x87.png",
                "caption": "",
                "position": 2417
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x88.png",
                "caption": "Figure 11:Pass@kkaccuracy (k=1,5,10k=1,5,10) and gold-in-trace rate under reasoning-trace truncation forR1-Qwen-32BonMultilingual AIME. Latent reasoning is less pronounced compared to MGSM.",
                "position": 2423
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x89.png",
                "caption": "",
                "position": 2426
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x90.png",
                "caption": "",
                "position": 2427
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x91.png",
                "caption": "",
                "position": 2428
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x92.png",
                "caption": "",
                "position": 2430
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x93.png",
                "caption": "",
                "position": 2431
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x94.png",
                "caption": "",
                "position": 2432
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x95.png",
                "caption": "",
                "position": 2433
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x96.png",
                "caption": "",
                "position": 2435
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x97.png",
                "caption": "",
                "position": 2436
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x98.png",
                "caption": "",
                "position": 2437
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x99.png",
                "caption": "Figure 12:Causal decomposition of newly correct predictions across truncation intervals onMGSMwithR1-Qwen-7B.\nEach bar partitions gains into three cases: (i) the gold answer is first articulated in the newly added reasoning steps,\n(ii) it was already articulated in earlier steps, or\n(iii) it has not yet appeared in the visible truncated trace.\nEarly and intermediate gains are dominated by case (iii), indicating latent reasoning.",
                "position": 2456
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x100.png",
                "caption": "",
                "position": 2459
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x101.png",
                "caption": "",
                "position": 2460
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x102.png",
                "caption": "",
                "position": 2461
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x103.png",
                "caption": "",
                "position": 2463
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x104.png",
                "caption": "",
                "position": 2464
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x105.png",
                "caption": "",
                "position": 2465
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x106.png",
                "caption": "",
                "position": 2466
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x107.png",
                "caption": "",
                "position": 2468
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x108.png",
                "caption": "",
                "position": 2469
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x109.png",
                "caption": "",
                "position": 2470
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x110.png",
                "caption": "Figure 13:Causal decomposition of newly correct predictions across truncation intervals onMGSMwithR1-Qwen-14B.\nEach bar partitions gains into three cases: (i) the gold answer is first articulated in the newly added reasoning steps,\n(ii) it was already articulated in earlier steps, or\n(iii) it has not yet appeared in the visible truncated trace.\nEarly and intermediate gains are dominated by case (iii), indicating latent reasoning.",
                "position": 2480
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x111.png",
                "caption": "",
                "position": 2483
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x112.png",
                "caption": "",
                "position": 2484
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x113.png",
                "caption": "",
                "position": 2485
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x114.png",
                "caption": "",
                "position": 2487
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x115.png",
                "caption": "",
                "position": 2488
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x116.png",
                "caption": "",
                "position": 2489
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x117.png",
                "caption": "",
                "position": 2490
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x118.png",
                "caption": "",
                "position": 2492
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x119.png",
                "caption": "",
                "position": 2493
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x120.png",
                "caption": "",
                "position": 2494
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x121.png",
                "caption": "Figure 14:Causal decomposition of newly correct predictions across truncation intervals onMGSMwithR1-Qwen-32B.\nEach bar partitions gains into three cases: (i) the gold answer is first articulated in the newly added reasoning steps,\n(ii) it was already articulated in earlier steps, or\n(iii) it has not yet appeared in the visible truncated trace.\nEarly and intermediate gains are dominated by case (iii), indicating latent reasoning.",
                "position": 2504
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x122.png",
                "caption": "",
                "position": 2507
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x123.png",
                "caption": "",
                "position": 2508
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x124.png",
                "caption": "",
                "position": 2509
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x125.png",
                "caption": "",
                "position": 2511
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x126.png",
                "caption": "",
                "position": 2512
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x127.png",
                "caption": "",
                "position": 2513
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x128.png",
                "caption": "",
                "position": 2514
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x129.png",
                "caption": "",
                "position": 2516
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x130.png",
                "caption": "",
                "position": 2517
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x131.png",
                "caption": "",
                "position": 2518
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x132.png",
                "caption": "Figure 15:Causal decomposition of newly correct predictions across truncation intervals onMultilingual AIMEwithR1-Qwen-7B.\nEach bar partitions gains into three cases: (i) the gold answer is first articulated in the newly added reasoning steps,\n(ii) it was already articulated in earlier steps, or\n(iii) it has not yet appeared in the visible truncated trace.\nCompared to MGSM, gains are sparser and less dominated by latent reasoning.",
                "position": 2528
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x133.png",
                "caption": "",
                "position": 2531
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x134.png",
                "caption": "",
                "position": 2532
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x135.png",
                "caption": "",
                "position": 2533
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x136.png",
                "caption": "",
                "position": 2535
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x137.png",
                "caption": "",
                "position": 2536
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x138.png",
                "caption": "",
                "position": 2537
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x139.png",
                "caption": "",
                "position": 2538
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x140.png",
                "caption": "",
                "position": 2540
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x141.png",
                "caption": "",
                "position": 2541
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x142.png",
                "caption": "",
                "position": 2542
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x143.png",
                "caption": "Figure 16:Causal decomposition of newly correct predictions across truncation intervals onMultilingual AIMEwithR1-Qwen-14B.\nEach bar partitions gains into three cases: (i) the gold answer is first articulated in the newly added reasoning steps,\n(ii) it was already articulated in earlier steps, or\n(iii) it has not yet appeared in the visible truncated trace.\nCompared to MGSM, gains are sparser and less dominated by latent reasoning.",
                "position": 2552
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x144.png",
                "caption": "",
                "position": 2555
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x145.png",
                "caption": "",
                "position": 2556
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x146.png",
                "caption": "",
                "position": 2557
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x147.png",
                "caption": "",
                "position": 2559
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x148.png",
                "caption": "",
                "position": 2560
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x149.png",
                "caption": "",
                "position": 2561
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x150.png",
                "caption": "",
                "position": 2562
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x151.png",
                "caption": "",
                "position": 2564
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x152.png",
                "caption": "",
                "position": 2565
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x153.png",
                "caption": "",
                "position": 2566
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x154.png",
                "caption": "Figure 17:Causal decomposition of newly correct predictions across truncation intervals onMultilingual AIMEwithR1-Qwen-32B.\nEach bar partitions gains into three cases: (i) the gold answer is first articulated in the newly added reasoning steps,\n(ii) it was already articulated in earlier steps, or\n(iii) it has not yet appeared in the visible truncated trace.\nCompared to MGSM, gains are sparser and less dominated by latent reasoning.",
                "position": 2576
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x155.png",
                "caption": "",
                "position": 2579
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x156.png",
                "caption": "",
                "position": 2580
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x157.png",
                "caption": "",
                "position": 2581
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x158.png",
                "caption": "",
                "position": 2583
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x159.png",
                "caption": "",
                "position": 2584
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x160.png",
                "caption": "",
                "position": 2585
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x161.png",
                "caption": "",
                "position": 2586
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x162.png",
                "caption": "",
                "position": 2588
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x163.png",
                "caption": "",
                "position": 2589
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x164.png",
                "caption": "",
                "position": 2590
            }
        ]
    },
    {
        "header": "Appendix CComplete Similarity results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02996/x165.png",
                "caption": "Figure 18:Aggregated cosine similarity onMGSMbetween hidden states in each language and English (reference), averaged over both reasoning steps and layers.\nHigh-resource languages show consistently higher similarity to English, suggesting convergence toward an English-centered latent reasoning pathway.",
                "position": 2606
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x166.png",
                "caption": "",
                "position": 2609
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x167.png",
                "caption": "",
                "position": 2610
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x168.png",
                "caption": "",
                "position": 2612
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x169.png",
                "caption": "",
                "position": 2613
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x170.png",
                "caption": "",
                "position": 2614
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x171.png",
                "caption": "Figure 19:Aggregated cosine similarity onMultilingual AIMEbetween hidden states in each language and English (reference), averaged over both reasoning steps and layers.\nHigh-resource languages show consistently higher similarity to English, suggesting convergence toward an English-centered latent reasoning pathway.",
                "position": 2619
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x172.png",
                "caption": "",
                "position": 2622
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x173.png",
                "caption": "",
                "position": 2623
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x174.png",
                "caption": "",
                "position": 2625
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x175.png",
                "caption": "",
                "position": 2626
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x176.png",
                "caption": "",
                "position": 2627
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x177.png",
                "caption": "Figure 20:Comparison of cosine similarity with English versus average similarity with other languages, shown separately for correctly and incorrectly solved examples inMGSMwithR1-Qwen-7B.",
                "position": 2648
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x178.png",
                "caption": "",
                "position": 2651
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x179.png",
                "caption": "",
                "position": 2652
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x180.png",
                "caption": "",
                "position": 2653
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x181.png",
                "caption": "",
                "position": 2655
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x182.png",
                "caption": "",
                "position": 2656
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x183.png",
                "caption": "",
                "position": 2657
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x184.png",
                "caption": "",
                "position": 2658
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x185.png",
                "caption": "",
                "position": 2660
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x186.png",
                "caption": "",
                "position": 2661
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x187.png",
                "caption": "",
                "position": 2662
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x188.png",
                "caption": "",
                "position": 2663
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x189.png",
                "caption": "",
                "position": 2665
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x190.png",
                "caption": "",
                "position": 2666
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x191.png",
                "caption": "",
                "position": 2667
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x192.png",
                "caption": "",
                "position": 2668
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x193.png",
                "caption": "",
                "position": 2670
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x194.png",
                "caption": "",
                "position": 2671
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x195.png",
                "caption": "",
                "position": 2672
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x196.png",
                "caption": "",
                "position": 2673
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x197.png",
                "caption": "Figure 21:Comparison of cosine similarity with English versus average similarity with other languages, shown separately for correctly and incorrectly solved examples inMGSMwithR1-Qwen-14B.",
                "position": 2677
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x198.png",
                "caption": "",
                "position": 2680
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x199.png",
                "caption": "",
                "position": 2681
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x200.png",
                "caption": "",
                "position": 2682
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x201.png",
                "caption": "",
                "position": 2684
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x202.png",
                "caption": "",
                "position": 2685
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x203.png",
                "caption": "",
                "position": 2686
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x204.png",
                "caption": "",
                "position": 2687
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x205.png",
                "caption": "",
                "position": 2689
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x206.png",
                "caption": "",
                "position": 2690
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x207.png",
                "caption": "",
                "position": 2691
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x208.png",
                "caption": "",
                "position": 2692
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x209.png",
                "caption": "",
                "position": 2694
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x210.png",
                "caption": "",
                "position": 2695
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x211.png",
                "caption": "",
                "position": 2696
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x212.png",
                "caption": "",
                "position": 2697
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x213.png",
                "caption": "",
                "position": 2699
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x214.png",
                "caption": "",
                "position": 2700
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x215.png",
                "caption": "",
                "position": 2701
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x216.png",
                "caption": "",
                "position": 2702
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x217.png",
                "caption": "Figure 22:Comparison of cosine similarity with English versus average similarity with other languages, shown separately for correctly and incorrectly solved examples inMGSMwithR1-Qwen-32B.",
                "position": 2706
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x218.png",
                "caption": "",
                "position": 2709
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x219.png",
                "caption": "",
                "position": 2710
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x220.png",
                "caption": "",
                "position": 2711
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x221.png",
                "caption": "",
                "position": 2713
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x222.png",
                "caption": "",
                "position": 2714
            },
            {
                "img": "https://arxiv.org/html/2601.02996/",
                "caption": "",
                "position": 2715
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x224.png",
                "caption": "",
                "position": 2716
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x225.png",
                "caption": "",
                "position": 2718
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x226.png",
                "caption": "",
                "position": 2719
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x227.png",
                "caption": "",
                "position": 2720
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x228.png",
                "caption": "",
                "position": 2721
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x229.png",
                "caption": "",
                "position": 2723
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x230.png",
                "caption": "",
                "position": 2724
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x231.png",
                "caption": "",
                "position": 2725
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x232.png",
                "caption": "",
                "position": 2726
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x233.png",
                "caption": "",
                "position": 2728
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x234.png",
                "caption": "",
                "position": 2729
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x235.png",
                "caption": "",
                "position": 2730
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x236.png",
                "caption": "",
                "position": 2731
            }
        ]
    },
    {
        "header": "Appendix DPerturbation Details",
        "images": []
    },
    {
        "header": "Appendix EExperimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02996/x237.png",
                "caption": "Figure 25:Language-specific prompt templates (containing the explicit language instruction) used for controlling the reasoning language.",
                "position": 3170
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x238.png",
                "caption": "Figure 26:Language-specific prompt-hacking prefixes used to reinforce language control. Each prefix is inserted immediately after the<think>tag to steer the modelâ€™s explicit reasoning trace to match the prompt language.",
                "position": 3173
            },
            {
                "img": "https://arxiv.org/html/2601.02996/x239.png",
                "caption": "Figure 27:Language-specific answer-elicitation prefixes used to directly prompt the model for a final numerical answer. Each prefix is appended immediately after the</think>tag to elicit the prediction.",
                "position": 3176
            }
        ]
    },
    {
        "header": "Appendix FEnvironment and Hyperparameters",
        "images": []
    }
]