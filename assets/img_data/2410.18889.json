[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18889/extracted/5951932/graphics/Flow.png",
                "caption": "Figure 1:An illustration of our approach for detecting and addressing mislabeled data: (1) Re-label examples from existing datasets using an ensemble of LLMs. (2) Identify strong disagreements between the LLM‚Äôs predictions and the original labels (i.e., high confidence in a different label), flagging examples based on confidence levels. Our findings show that LLMs detect between 6% and 21% of label errors, and higher LLM confidence is strongly associated with improved precision in error detection. (3) In the training set, we either filter or flip flagged examples to improve model performance, leading to an increase of up to 4%. For the test set, flagged examples are re-annotated by experts to make sure the evaluation is accurate. We found that under accurate evaluation, the performance of LLMs is up to 15% higher than the original mislabeled data.",
                "position": 160
            }
        ]
    },
    {
        "header": "2Data Annotation Approaches",
        "images": []
    },
    {
        "header": "3Experimental Setup",
        "images": []
    },
    {
        "header": "4Label Error Analysis and the Role of LLMs in Error Detection",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18889/x1.png",
                "caption": "Figure 2:When LLMs disagrees with original labels - who is correct?. As the LLM‚Äôs confidence grows, so does the precision of identifying an error in the original labels.",
                "position": 448
            },
            {
                "img": "https://arxiv.org/html/2410.18889/x2.png",
                "caption": "Figure 3:LLM Ensemble of different sizes (random subsets).(Left)presents the performance of the ensemble in terms of ROC AUC compared to the gold labels.(Right)presents the increasing ability to detect label errors. F1 is computed overError / Not Errorpredictions.",
                "position": 461
            }
        ]
    },
    {
        "header": "5Comparing Annotation Approaches",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18889/extracted/5951932/graphics/Compare-dense__no-frame.png",
                "caption": "Figure 4:Annotation approaches comparison.",
                "position": 478
            },
            {
                "img": "https://arxiv.org/html/2410.18889/x3.png",
                "caption": "Figure 5:Comparison between all annotation methods, measured by the weighted-F1-score. Rows represent the‚Äùtrue‚Äùlabel and columns represent the‚Äùprediction‚Äù. For instance, the score ofLLMscompared to theOriginallabel is 0.72.",
                "position": 495
            },
            {
                "img": "https://arxiv.org/html/2410.18889/x3.png",
                "caption": "Figure 5:Comparison between all annotation methods, measured by the weighted-F1-score. Rows represent the‚Äùtrue‚Äùlabel and columns represent the‚Äùprediction‚Äù. For instance, the score ofLLMscompared to theOriginallabel is 0.72.",
                "position": 498
            },
            {
                "img": "https://arxiv.org/html/2410.18889/x4.png",
                "caption": "Figure 6:(x-axis)at listxùë•xitalic_xannotations per annotator.(Right y-axis)The number of annotators with at leastxùë•xitalic_xannotations (bins).(Left y-axis)the average F1-score or accuracy for all user annotations with at leastxùë•xitalic_xannotations.",
                "position": 503
            }
        ]
    },
    {
        "header": "6Implications of Mislabeled Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18889/x5.png",
                "caption": "Figure 7:Fine-tuning a model on a transformed dataset. The gray bar is the original dataset - without any changes. The green bars present results for label flipping for a subset of examples, determined by LLMs-confidence (plain), or at random (dotted). The blue bars represent filtering of these examples.",
                "position": 686
            },
            {
                "img": "https://arxiv.org/html/2410.18889/x6.png",
                "caption": "",
                "position": 690
            }
        ]
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Discussion",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AAnnotation Approaches",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18889/x7.png",
                "caption": "Figure 8:Platform for crowd-sourcing annotation in Amazon Mechanical Turk (MTurk).(Top)Guidelines for the task and definitions.(Bottom)Annotation layout for a single instance.",
                "position": 2314
            },
            {
                "img": "https://arxiv.org/html/2410.18889/x8.png",
                "caption": "",
                "position": 2318
            },
            {
                "img": "https://arxiv.org/html/2410.18889/x9.png",
                "caption": "Table 5:Distribution of crowd-source annotators. Each example was annotated by 3 workers. Plain segments are unanimous annotation, while dotted segments indicate examples where some annotators labeled asinconsistent, and other asconsistent. For example, 19.8% of the examples had twoinconsistentannotation, and oneconsistentannotation.",
                "position": 2326
            },
            {
                "img": "https://arxiv.org/html/2410.18889/x9.png",
                "caption": "Table 5:Distribution of crowd-source annotators. Each example was annotated by 3 workers. Plain segments are unanimous annotation, while dotted segments indicate examples where some annotators labeled asinconsistent, and other asconsistent. For example, 19.8% of the examples had twoinconsistentannotation, and oneconsistentannotation.",
                "position": 2329
            },
            {
                "img": "https://arxiv.org/html/2410.18889/x10.png",
                "caption": "Table 6:How experts annotations have changed after the reconciliation phase. Most changes occur from 1 (consistent) to 0 (inconsistent).",
                "position": 2334
            },
            {
                "img": "https://arxiv.org/html/2410.18889/x11.png",
                "caption": "Figure 9:Annotation platform on Label-Studio for experts",
                "position": 2354
            },
            {
                "img": "https://arxiv.org/html/2410.18889/x12.png",
                "caption": "Figure 10:Four different prompt input templates to LLMs for obtaining binary labels",
                "position": 2371
            }
        ]
    },
    {
        "header": "Appendix BCost and Scalability",
        "images": []
    },
    {
        "header": "Appendix CMislabeled Data Implications",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18889/x13.png",
                "caption": "Figure 11:Similar experiments to the one inFigure¬†7, with small alterations.(Left)Starting from a different base model - pre-trained DeBERTa-v3-base.(Right)Dashed columns present results for when flipping or filtering methods were applied only on the training set, but not the validation.",
                "position": 2409
            },
            {
                "img": "https://arxiv.org/html/2410.18889/x14.png",
                "caption": "",
                "position": 2412
            }
        ]
    },
    {
        "header": "Appendix DStatistical Analysis",
        "images": []
    },
    {
        "header": "Appendix ELabel Errors",
        "images": []
    }
]