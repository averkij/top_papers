[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3NeoBERT",
        "images": []
    },
    {
        "header": "4Effect of Design Choices",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.19587/x1.png",
                "caption": "Figure 1:GLUE ablation scores on the development set. Modifications in grey are not included in the subsequent models. Increasing data size and diversity leads to the highest relative improvement (M‚Å¢2ùëÄ2M2italic_M 2,+3.6%percent3.6+3.6\\%+ 3.6 %), followed by the model size (M‚Å¢7ùëÄ7M7italic_M 7,+2.9%percent2.9+2.9\\%+ 2.9 %). Packing the sequences and using the LLaMA 2 tokenizer cause the largest relative drops (M‚Å¢6ùëÄ6M6italic_M 6,‚àí2.9%percent2.9-2.9\\%- 2.9 %,M‚Å¢3ùëÄ3M3italic_M 3,‚àí2.1%percent2.1-2.1\\%- 2.1 %).",
                "position": 592
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.19587/x2.png",
                "caption": "Figure 2:Pseudo-Perplexity in function of the sequence length for NeoBERT1024(left)and NeoBERT4096(right). This validates the effectiveness of the final pre-training stage on NeoBERT‚Äôs ability to model long sequences.",
                "position": 1004
            },
            {
                "img": "https://arxiv.org/html/2502.19587/x3.png",
                "caption": "",
                "position": 1014
            },
            {
                "img": "https://arxiv.org/html/2502.19587/x4.png",
                "caption": "Figure 3:Model throughput (tokens per second) as a function of sequence length (‚Üë‚Üë\\uparrow‚Üëis better). Above1,02410241,0241 , 024in sequence length, NeoBERT surpasses ModernBERTbasedespite having100‚Å¢M100ùëÄ100M100 italic_Mmore parameters.",
                "position": 1032
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining details",
        "images": []
    },
    {
        "header": "Appendix BAblations",
        "images": []
    },
    {
        "header": "Appendix CGLUE",
        "images": []
    },
    {
        "header": "Appendix DMTEB",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.19587/extracted/6236292/figures/mteb_pretrained.png",
                "caption": "Figure 4:Zero-shot evaluation of BERT and RoBERTa on the English subset of MTEB.",
                "position": 1972
            },
            {
                "img": "https://arxiv.org/html/2502.19587/x5.png",
                "caption": "Figure 5:Average MTEB scores of fine-tuned encoders grouped by task type. The average score is computed across the 56 tasks of MTEB-English. NeoBERT is the best model on five out of seven task types and the best model overall. SeeTable¬†4for complete scores.",
                "position": 1999
            }
        ]
    },
    {
        "header": "Appendix EEfficiency",
        "images": []
    }
]