[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.12040/x1.png",
                "caption": "Figure 1:The difference between our proposed FineCE and existing confidence estimation methods.(a):LLMs either generate an answer when the query is within their knowledge scope or refuse to answer if it falls beyond their capabilities.(b):The model assigns a single confidence score after the entire answer is generated.(c):Our proposed method, FineCE, provides the fine-grained confidence scores for any given text sequence throughout the generation process.",
                "position": 154
            }
        ]
    },
    {
        "header": "2Task Formalization",
        "images": []
    },
    {
        "header": "3FineCE: Fine-grained Confidence Estimation",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.12040/x2.png",
                "caption": "Figure 2:The construction process of the training dataset. It illustrates the confidence scoring procedures forQuestionand Question with Partial Answerusing Monte Carlo sampling. ForQuestion with Answer, the confidence score is determined based on the correctness of the answer. The complete data construction procedure is detailed in Algorithm1.",
                "position": 254
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.12040/x3.png",
                "caption": "Figure 3:(Left:)Comparison of accuracy between the original model predictions and those selectively accepted by FineCE when the output confidence exceeds 0.8. The backbone used is Llama2-13B.(Right:)Effect of fusion depth (1) and fusion width (2) in FineCE on confidence estimation performance, evaluated with Llama-7B and Llama-13B on the GSM8K and CSQA datasets.",
                "position": 1203
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.12040/x4.png",
                "caption": "Figure 4:The three types of training data format.",
                "position": 2245
            },
            {
                "img": "https://arxiv.org/html/2508.12040/x5.png",
                "caption": "Figure 5:The Zero-shot performance on OpenBookQA dataset. From left to right, the figures show the confidence estimation performance of FineCE for the question, partial answer, and complete answer. The x-axis represents the confidence scores (%), and the y-axis represents the ratio of quantities. The top area contains the detailed values of ECE and AUROC.",
                "position": 2406
            },
            {
                "img": "https://arxiv.org/html/2508.12040/x6.png",
                "caption": "Figure 6:On GSM8K(left) and CSQA(right) dataset, the performance confidence estimation for the two different families models using datasets from different sources.The horizontal axis represents the base models.",
                "position": 2409
            },
            {
                "img": "https://arxiv.org/html/2508.12040/x7.png",
                "caption": "Figure 7:The performance comparison using different training technical. The backbone model is LLaMA2-13B.",
                "position": 2412
            },
            {
                "img": "https://arxiv.org/html/2508.12040/x8.png",
                "caption": "Figure 8:The performance confidence estimation for two base models using training datasets from different sources. The horizontal axis represents the base models.",
                "position": 2470
            },
            {
                "img": "https://arxiv.org/html/2508.12040/x9.png",
                "caption": "Figure 9:The prompts used in the baselines.",
                "position": 2473
            }
        ]
    },
    {
        "header": "Appendix BLimitations",
        "images": []
    }
]