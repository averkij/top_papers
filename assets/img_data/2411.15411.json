[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15411/x1.png",
                "caption": "",
                "position": 87
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3TheCompositionCapDataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15411/extracted/6019361/images/resolution_distribution.png",
                "caption": "Figure 2:The image resolution distribution across data points inCompositionCapreflects the high overall quality of images.",
                "position": 267
            },
            {
                "img": "https://arxiv.org/html/2411.15411/extracted/6019361/images/proportion.png",
                "caption": "Figure 3:Distribution of attributes inCompositionCap.",
                "position": 270
            }
        ]
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15411/x2.png",
                "caption": "Figure 4:Overview ofFineCaption: The model incorporates a mask-aware visual encoder and two high-resolution encoders (ConvNext and SAM), enabling precise recognition of mask references and the perception of detailed compositional and spatial information for images.",
                "position": 277
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15411/x3.png",
                "caption": "Figure 5:FineCaptionprovides accurate and concise descriptions focused on specified attributes and regions, while GPT-4o often misses fine-grained references and includes irrelevant information.",
                "position": 1010
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "8More Quantitative Analysis ofCompositionCap",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15411/extracted/6019361/images/wordcloud.png",
                "caption": "Figure 6:Word cloud of key terms from the captions inCompositionCap, illustrating the diverse compositional region description of images.",
                "position": 1773
            },
            {
                "img": "https://arxiv.org/html/2411.15411/extracted/6019361/images/entity.png",
                "caption": "Figure 7:Distribution of entities inCompositionCap.",
                "position": 1776
            },
            {
                "img": "https://arxiv.org/html/2411.15411/extracted/6019361/images/ratio.png",
                "caption": "Figure 8:Distribution of mask ratio inCompositionCap.",
                "position": 1779
            }
        ]
    },
    {
        "header": "9Explanation of Attributes",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15411/x4.png",
                "caption": "Figure 9:Example for Attribute-Aware Regional Captioning task inCompositionCap.",
                "position": 2059
            },
            {
                "img": "https://arxiv.org/html/2411.15411/x5.png",
                "caption": "Figure 10:Example for Attribute-Aware Regional Captioning task inCompositionCap.",
                "position": 2062
            },
            {
                "img": "https://arxiv.org/html/2411.15411/x6.png",
                "caption": "Figure 11:Distribution of attributes inCompositionCap.",
                "position": 2065
            },
            {
                "img": "https://arxiv.org/html/2411.15411/x7.png",
                "caption": "Figure 12:Example for Attribute-Aware Regional Captioning task inCompositionCap.",
                "position": 2068
            },
            {
                "img": "https://arxiv.org/html/2411.15411/x8.png",
                "caption": "Figure 13:Case study for Regional Dense Captioning task forFineCaption.",
                "position": 2071
            },
            {
                "img": "https://arxiv.org/html/2411.15411/x9.png",
                "caption": "Figure 14:Case study for Regional Dense Captioning task forFineCaption.",
                "position": 2074
            },
            {
                "img": "https://arxiv.org/html/2411.15411/x10.png",
                "caption": "Figure 15:Case study for Regional Dense Captioning task forFineCaption.",
                "position": 2077
            },
            {
                "img": "https://arxiv.org/html/2411.15411/x11.png",
                "caption": "Figure 16:Case study for Regional Dense Captioning task forFineCaption.",
                "position": 2080
            }
        ]
    },
    {
        "header": "10More Details and Cases",
        "images": []
    }
]