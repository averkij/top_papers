[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.20422/x1.png",
                "caption": "",
                "position": 117
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.20422/x2.png",
                "caption": "Figure 2:Workflow of our 3to4D approach, designed to optimize a 4D radiance field using a neural representation that captures both static and dynamic elements.\nFirst, a 4D NeRF is trained to represent the static object (plant, left), having the same input structure at each time step.\nThen, we introduce dynamics to the 4D NeRF by distilling the prior from a pre-trained image-to-video model. At each SDS step, we select a viewpoint and render both the input object and the 4D NeRF from the same selected viewpoint. These renders, along with the textual prompts, are then fed into the image-to-video model, and the SDS loss is calculated to guide the generation of motion while preserving the object’s identity.\nThe attention-masked SDS, focuses learning on the relevant parts of the object, improving identity preservation.",
                "position": 188
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.20422/x3.png",
                "caption": "Figure 3:(click-to-view-online)3to4Dbrings various objects to life. On the left, we display the input object along with a textual prompt describing the desired action. On the right, we present four frames from the generated object, viewed from the front. Each 3D frame is split into an RGB image and its corresponding depth map, shown in the top right corner.",
                "position": 406
            }
        ]
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.20422/x4.png",
                "caption": "Figure 4:Comparison between our method and baselines, across all objects tested. We consistently achieve better LPIPS scores across all objects.",
                "position": 579
            },
            {
                "img": "https://arxiv.org/html/2412.20422/x5.png",
                "caption": "",
                "position": 583
            },
            {
                "img": "https://arxiv.org/html/2412.20422/x6.png",
                "caption": "Figure 5:Qualitative comparison with competing methods. A rendered image of the input object is shown on the left, alongside rendered images from our and other methods. While our method preserves the identity of the input object, all other baselines generate different objects.   .",
                "position": 687
            },
            {
                "img": "https://arxiv.org/html/2412.20422/x7.png",
                "caption": "Figure 6:(Click to view video online). Different prompts generated different 4D, matching the movement description. The object in question is a Mario figure (on the left), and we provide three distinct prompts that describe three different dynamics of the figure. On the right, the generated 4D illustrates the corresponding movements based on these prompts.",
                "position": 691
            },
            {
                "img": "https://arxiv.org/html/2412.20422/x8.png",
                "caption": "Figure 7:Qualitative ablation result demonstrates the contribution of each part of 3to4D. Without our viewpoint selector (described in Sec.3.2) the plant does not “bloom”. Without our attention-masked SDS, the plant is less rich in detail.",
                "position": 696
            }
        ]
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Appendix AAppendix: Camera Viewpoint Sampling",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]