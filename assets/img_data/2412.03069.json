[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03069/x1.png",
                "caption": "Figure 1:Multimodal Understanding Results with TokenFlow. We demonstrate for the first time thatdiscrete visual inputcan surpass LLaVA-1.5 13B in understanding performance, achieving a 7.2% average improvement.",
                "position": 89
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03069/x2.png",
                "caption": "Figure 2:Visual Generation Results with TokenFlow. We present diverse 256√ó256 results across various styles, subjects, and scenarios.",
                "position": 96
            },
            {
                "img": "https://arxiv.org/html/2412.03069/x3.png",
                "caption": "Figure 3:Overview of TokenFlow. We incorporate dual encoders and codebooks with a shared mapping, enabling the joint optimization of high-level semantics and low-level pixel details. For a given input image, distancesdsemsubscriptùëësemd_{\\text{sem}}italic_d start_POSTSUBSCRIPT sem end_POSTSUBSCRIPTanddpixsubscriptùëëpixd_{\\text{pix}}italic_d start_POSTSUBSCRIPT pix end_POSTSUBSCRIPTare calculated from the pixel-level and semantic-level codebooks, respectively, with the final codebook index and features determined by minimizing the weighted sumdsem+wdis‚ãÖdpixsubscriptùëësem‚ãÖsubscriptùë§dissubscriptùëëpixd_{\\text{sem}}+w_{\\text{dis}}\\cdot d_{\\text{pix}}italic_d start_POSTSUBSCRIPT sem end_POSTSUBSCRIPT + italic_w start_POSTSUBSCRIPT dis end_POSTSUBSCRIPT ‚ãÖ italic_d start_POSTSUBSCRIPT pix end_POSTSUBSCRIPT. The resulting quantized features are independently decoded for both semantic alignment and image reconstruction training, and then concatenated to provide a unified representation for downstream tasks in understanding and generation.",
                "position": 125
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03069/x4.png",
                "caption": "Figure 4:Visualization of images clustered by (a) VQKD[35], (b) VQGAN[13], and (c) Our TokenFlow. VQKD clusters exhibit semantic similarity, while VQGAN clusters exhibit low-level similarity (i.e.color). Our TokenFlow can successfully combine both semantic and low-level similarity. Implementation details of image clustering can be found inSec.A.1.",
                "position": 236
            },
            {
                "img": "https://arxiv.org/html/2412.03069/x5.png",
                "caption": "Figure 5:Qualitative comparison of different sampling strategies in our framework. (a) Single-pass top-kùëòkitalic_k(kùëòkitalic_k=1200) and top-pùëùpitalic_p(pùëùpitalic_p=0.8) sampling exhibits inconsistent patterns and artifacts. (b) Our proposed multi-step sampling strategy produces more coherent and visually appealing results. Best zoomed in for details.",
                "position": 348
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03069/x6.png",
                "caption": "Figure 6:Impact of codebook size on reconstruction quality, class-conditional generation, and multimodal understanding benchmarks. MME is divide by 28 to have the same scale.",
                "position": 1130
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03069/x7.png",
                "caption": "Figure 7:Comparison of cluster size distributions between VQKD[35], VQGAN[13], and TokenFlow (ours), with a fixed codebook size of 8,192. Analysis performed on 50,000 images from the ImageNet-1k validation set. TokenFlow exhibits significantly smoother distribution compared to others, attributed to our shared mapping design that learns joint distributions of semantic and pixel-level features. This joint learning approach helps maintain high codebook utilization (95%+) even with large-scale codebooks containing over 131K entries.",
                "position": 2243
            },
            {
                "img": "https://arxiv.org/html/2412.03069/x8.png",
                "caption": "Figure 8:Comparison of original images and their reconstructions from quantized semantic features extracted by VQKD[35]. The reconstructed images preserve the semantic content but exhibit significant loss of high-frequency details.",
                "position": 2253
            }
        ]
    },
    {
        "header": "Appendix BAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03069/x9.png",
                "caption": "Figure 9:Qualitative comparison of visual generation capabilities between 1B and 7B models. Prompts (from left to right): (1) ‚ÄùA pizza sitting on top of a wooden cutting board‚Äù, (2) ‚ÄùTelevision set being held by a hand‚Äù, (3) ‚ÄùThe guy is nicely dressed in a suit and tie‚Äù, and (4) ‚ÄùA sailing ship rests on waters‚Äù. The 7B model demonstrates enhanced quality compared to its 1B counterpart.",
                "position": 2412
            },
            {
                "img": "https://arxiv.org/html/2412.03069/x10.png",
                "caption": "Figure 10:Comparison of image reconstruction quality. (a) Original images. (b) Reconstructions using the base pixel decoder. (c) Reconstructions using the enhanced (2√ó2\\times2 √ócapacity) decoder. The enhanced decoder demonstrates superior preservation of fine-grained details, particularly in facial details and textual elements.",
                "position": 2425
            },
            {
                "img": "https://arxiv.org/html/2412.03069/x11.png",
                "caption": "Figure 11:Qualitative comparison of images clustered by VQKD[35], VQGAN[13]and our TokenFlow. VQKD clusters exhibit semantic similarity, while VQGAN clusters exhibit low-level similarity (i.e.color and texture). Our TokenFlow can successfully combine both semantic and low-level similarity (e.g.birds with different background can be mapped into two different index).",
                "position": 2940
            },
            {
                "img": "https://arxiv.org/html/2412.03069/x12.png",
                "caption": "Figure 12:More Visual Generation Results with TokenFlow. We present diverse 256√ó256 results across various styles, subjects, and scenarios.",
                "position": 2945
            }
        ]
    },
    {
        "header": "Appendix CLimitation and Future Work",
        "images": []
    }
]