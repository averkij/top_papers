[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07055/Arxiv/figures/icon.png",
                "caption": "",
                "position": 81
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07055/x1.png",
                "caption": "Figure 1:Theory of Space: active exploration, probed belief, and evaluation.Left: a top-down view of agent trajectory under partial observability in multiple-room scenes.\nMiddle: the agent’s action loop of moving, rotating, and observing in text- or vision-based environments, receiving egocentric observations and updating an internal belief.\nRight: evaluation through exploitation of the belief in spatial tasks and direct probing via probed cognitive maps.",
                "position": 155
            },
            {
                "img": "https://arxiv.org/html/2602.07055/x2.png",
                "caption": "Figure 2:Evaluation accuracy vs. exploration cost for active exploration in vision-world.Faded icons mark the passive setting, where the agent gets a pre-generated exploration history and only reasons.",
                "position": 184
            }
        ]
    },
    {
        "header": "2Theory of Space",
        "images": []
    },
    {
        "header": "3Benchmarking Theory of Space Ability for Foundation Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07055/x3.png",
                "caption": "Figure 3:Theory of Spaceexploitation task suite:it coversroute-level egocentric reasoning andsurvey-level allocentric mapping. Route tasks evaluate path-based inference and egocentric observations. Survey tasks test global mapping, geometric transformation, and perspective conversion. Together they cover both local navigation reasoning and global spatial abstraction.",
                "position": 314
            }
        ]
    },
    {
        "header": "4Evaluation and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07055/x4.png",
                "caption": "",
                "position": 2493
            }
        ]
    },
    {
        "header": "5How do Foundation Models Manage Internal Spatial Belief?",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07055/x5.png",
                "caption": "Figure 5:Internal Spatial Belief Probing.At each step, the agent executes an action, receives an observation, and updates its spatial belief. We probe this belief by prompting the agent to (i) output a JSON-structured cognitive map of all observed objects and (ii) select the next unexplored position from a top-down view given a set of labeled candidate points. For clarity, the figure shows the probing process for a single step.",
                "position": 2537
            },
            {
                "img": "https://arxiv.org/html/2602.07055/x6.png",
                "caption": "Figure 6:Accumulated Information Gain and Cognitive Map Correctness over steps.",
                "position": 3089
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ATechnical Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07055/Arxiv/appendix_figures/field_of_view.png",
                "caption": "(a)Field of view (FOV) specification for the agent in our tasks.The FOV spans 90° in front of the agent and is divided into angular bins (e.g., front, front-slight left, front-left) and distance ranges (near [0,2], mid [2,5], far [5,10]). This egocentric perception defines how spatial relations are observed and reported.",
                "position": 4184
            },
            {
                "img": "https://arxiv.org/html/2602.07055/Arxiv/appendix_figures/field_of_view.png",
                "caption": "(a)Field of view (FOV) specification for the agent in our tasks.The FOV spans 90° in front of the agent and is divided into angular bins (e.g., front, front-slight left, front-left) and distance ranges (near [0,2], mid [2,5], far [5,10]). This egocentric perception defines how spatial relations are observed and reported.",
                "position": 4187
            },
            {
                "img": "https://arxiv.org/html/2602.07055/x7.png",
                "caption": "(b)Distribution of all 3D models used in our vision tasks.",
                "position": 4192
            },
            {
                "img": "https://arxiv.org/html/2602.07055/Arxiv/appendix_figures/vision_distance_instruction.png",
                "caption": "Figure 8:Example of distance cues in the vision prompt.The colored cylinders illustrate objects placed at different distances from the agent: yellow at 2 m, blue at 1 m, red at 2 m, and green at 3 m, providing calibration for mapping visual observations to discretized distance bins.",
                "position": 4202
            },
            {
                "img": "https://arxiv.org/html/2602.07055/Arxiv/appendix_figures/vision_orientation_instruction.png",
                "caption": "Figure 9:Object appearance and orientation cues in the vision prompt.Objects with facing direction are shown from both the front and side views, while objects without inherent orientation are displayed only from the front view. This provides the agent with consistent visual references for recognizing shape and facing.",
                "position": 4205
            },
            {
                "img": "https://arxiv.org/html/2602.07055/x8.png",
                "caption": "Figure 10:Exploration prompts",
                "position": 4236
            },
            {
                "img": "https://arxiv.org/html/2602.07055/x9.png",
                "caption": "Figure 11:Evaluation prompt design. We show the prompt for each evaluation task.",
                "position": 4239
            },
            {
                "img": "https://arxiv.org/html/2602.07055/x10.png",
                "caption": "Figure 12:Belief probing prompt design. We use these prompts to ask the model to output a cognitive map or select unobserved points.",
                "position": 4242
            },
            {
                "img": "https://arxiv.org/html/2602.07055/x11.png",
                "caption": "Figure 13:The symbol map and the image mapprovide parallel representations of the same environment for text and vision settings in uncertainty probing prompts.",
                "position": 4245
            }
        ]
    },
    {
        "header": "Appendix BEvaluation Setups",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07055/x12.png",
                "caption": "Figure 14:Examples of task formats and answer styles used.Each block illustrates a spatial reasoning task type in our suite (Route-level and Survey-level), including the corresponding input context and an example open-ended answer that must follow a strict output format. In the vision setting, textual scene descriptions in the questions are replaced by rendered observation images.",
                "position": 5696
            },
            {
                "img": "https://arxiv.org/html/2602.07055/x13.png",
                "caption": "Figure 15:GPT-5.2’s turn-by-turn cognitive map in text world during exploration.",
                "position": 5716
            },
            {
                "img": "https://arxiv.org/html/2602.07055/x14.png",
                "caption": "Figure 16:GPT-5.2’s turn-by-turn cognitive map in vision world during exploration.",
                "position": 5720
            },
            {
                "img": "https://arxiv.org/html/2602.07055/Arxiv/appendix_figures/gpt5.2-systematic-sweeping.png",
                "caption": "Figure 17:Example trajectory illustratingGPT-5.2’s door-finding strategy and systematic sweeping pattern: Upon detecting a door, the agent navigates toward it and executes a strategic rotation to maximize environmental coverage. The process terminates once all target objects have been successfully identified.",
                "position": 5724
            },
            {
                "img": "https://arxiv.org/html/2602.07055/Arxiv/appendix_figures/gpt5.2-omission.png",
                "caption": "Figure 18:Example trajectory illustratingGPT-5.2’s omission pattern: Observing the door too early may lead the agent to skip the rest of the exploration, causing incomplete environmental discovery.",
                "position": 5728
            },
            {
                "img": "https://arxiv.org/html/2602.07055/Arxiv/appendix_figures/gemini-systematic-sweeping.png",
                "caption": "Figure 19:Example trajectory illustratingGEMINI-3 Pro’s door-finding strategy and systematic sweeping pattern in vision world: Upon detecting a door, the agent navigates toward it and executes a strategic rotation to maximize environmental coverage. The process terminates once all target objects have been successfully identified.",
                "position": 5732
            },
            {
                "img": "https://arxiv.org/html/2602.07055/Arxiv/appendix_figures/gemini-object-sweeping.png",
                "caption": "Figure 20:Example trajectory illustratingGEMINI-3 Pro’s object sweeping pattern mostly found in text world: Orbit the starting object using it as the pivot point. Randomly select an observed door to jump to a new object, then resume pivoting around the new target in a continuous loop.",
                "position": 5736
            },
            {
                "img": "https://arxiv.org/html/2602.07055/Arxiv/appendix_figures/claude_explore_pattern.jpg",
                "caption": "Figure 21:Example trajectory illustratingCLAUDE-4.5 Sonnet’s exploration pattern: There is no clear exploration pattern.",
                "position": 5740
            },
            {
                "img": "https://arxiv.org/html/2602.07055/Arxiv/appendix_figures/ui_chart.png",
                "caption": "Figure 22:Platform designed by us for analysis (chart)",
                "position": 5744
            },
            {
                "img": "https://arxiv.org/html/2602.07055/Arxiv/appendix_figures/ui_text.png",
                "caption": "Figure 23:Visualization Platform for analysis: Metrics for active exploration in text world",
                "position": 5748
            },
            {
                "img": "https://arxiv.org/html/2602.07055/Arxiv/appendix_figures/ui_vision.png",
                "caption": "Figure 24:Visualization Platform for analysis: Metrics for active exploration in vision world",
                "position": 5752
            },
            {
                "img": "https://arxiv.org/html/2602.07055/Arxiv/appendix_figures/ui_turn_log_text.png",
                "caption": "Figure 25:Visualization Platform for analysis: one turn of active exploration in text-world, including agent’s action and cognitive map.",
                "position": 5756
            },
            {
                "img": "https://arxiv.org/html/2602.07055/Arxiv/appendix_figures/ui_turn_log_vision.png",
                "caption": "Figure 26:Visualization Platform for analysis: one turn of active exploration in vision-world",
                "position": 5760
            }
        ]
    },
    {
        "header": "Appendix CAdditional Visualization Examples",
        "images": []
    }
]