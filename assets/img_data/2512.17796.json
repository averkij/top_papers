[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17796/x1.png",
                "caption": "",
                "position": 66
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17796/x2.png",
                "caption": "Figure 2:(a) Each training sample consists of a 3D character and a video depicting the character performing an action described by a short text. Through segmentation and inpainting, we obtain the corresponding scene video and character mask sequence. The VAE encoder is then applied to encode these inputs into tokens. (b) AniX predicts target video tokens conditioned on scene, mask, text, and multi-view character tokens within a Multi-Modal Diffusion Transformer, trained using Flow Matching[lipman2022flow]. Refer to Figure3for the training process of the auto-regressive mode, which enables iterative interaction with AniX, and Figure4for the inference.",
                "position": 144
            },
            {
                "img": "https://arxiv.org/html/2512.17796/x3.png",
                "caption": "Figure 3:Illustration of the auto-regressive mode. The only difference from the original architecture in Figure2is the addition of an extra conditioning input—the preceding video tokens. Note that a misalignment exists between training and inference: during training, the preceding video tokens are derived from ground-truth videos, whereas during inference, they are generated by the model itself. To mitigate this discrepancy, we add a small Gaussian noise to the preceding video tokens during training and refer to the resulting tokens as augmented preceding video tokens.",
                "position": 227
            },
            {
                "img": "https://arxiv.org/html/2512.17796/x4.png",
                "caption": "Figure 4:Inference of AniX. (a) Users first specify the inputs, including the character, 3DGS scene, virtual camera location, and character anchor. (b) The user-provided text instruction is parsed, and a corresponding camera path is generated. Applying this path to the 3DGS scene produces a rendered scene video. (c) AniX then takes multiple inputs as conditions to generate the final output. Steps (b) and (c) can be performed iteratively, enabling temporally consistent, long-horizon interactions.",
                "position": 242
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17796/figs/visualization_main.png",
                "caption": "Figure 5:Screenshot visualizations of videos generated by AniX, showcasing different characters performing various actions across two scenes. Additional examples are provided in the appendix.",
                "position": 786
            },
            {
                "img": "https://arxiv.org/html/2512.17796/figs/textImageSceneAblation_unified_v2.png",
                "caption": "Figure 6:Using both visual conditions—the 3DGS scene and multi-view character—significantly improves long-horizon interactive video generation across diverse video clips.",
                "position": 789
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6More Implementation Details",
        "images": []
    },
    {
        "header": "7More Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17796/figs/game_real_ablation.png",
                "caption": "Figure 7:Evaluation of the hybrid data training strategy. (a) Training solely on game data causes the model to inherit a game-engine rendering style in the synthesized characters. (b) Incorporating real-world data improves photorealism, enabling the model to capture high-frequency details—such as dynamic clothing wrinkles—that are absent from the GTA-V dataset.",
                "position": 971
            },
            {
                "img": "https://arxiv.org/html/2512.17796/figs/distill_comparison.png",
                "caption": "Figure 8:Qualitative comparison of three models: (a) the original model with a 30-step denoising schedule (no acceleration), (b) the accelerated model with a 4-step schedule, and (c) the original model restricted to 4 steps (no acceleration but fewer steps). The results show that our 4-step model matches the visual quality of the original model while achieving a7.5×7.5\\timesinference speedup.",
                "position": 977
            },
            {
                "img": "https://arxiv.org/html/2512.17796/figs/vis_ood_action.png",
                "caption": "Figure 9:Visualization of a character performing 84 randomly selected novel actions.",
                "position": 984
            },
            {
                "img": "https://arxiv.org/html/2512.17796/figs/action_generalization.png",
                "caption": "Figure 10:Visualization of a character performing 25 randomly selected novel actions with text annotations.",
                "position": 994
            },
            {
                "img": "https://arxiv.org/html/2512.17796/figs/scene_customization.png",
                "caption": "Figure 11:Visualization of a character exploring various 3DGS worlds.",
                "position": 997
            },
            {
                "img": "https://arxiv.org/html/2512.17796/figs/character_customization_0.png",
                "caption": "Figure 12:Visualization of diverse characters performing locomotion actions (Part 1).",
                "position": 1006
            },
            {
                "img": "https://arxiv.org/html/2512.17796/figs/character_customization_1.png",
                "caption": "Figure 13:Visualization of diverse characters performing locomotion actions (Part 2).",
                "position": 1009
            },
            {
                "img": "https://arxiv.org/html/2512.17796/figs/long_gen_0.png",
                "caption": "Figure 14:Visualization of long-horizon generation (Example 1).",
                "position": 1015
            },
            {
                "img": "https://arxiv.org/html/2512.17796/figs/long_gen_1.png",
                "caption": "Figure 15:Visualization of long-horizon generation (Example 2).",
                "position": 1018
            }
        ]
    },
    {
        "header": "8More Visualizations",
        "images": []
    }
]