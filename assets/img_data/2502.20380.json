[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20380/x1.png",
                "caption": "Figure 1:(a) We define the task of multi-turn code generation where for an initial problemx𝑥xitalic_x, the generatorπθsubscript𝜋𝜃\\pi_{\\theta}italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPTprovides a solutiony1subscript𝑦1y_{1}italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT.\nThis solution is evaluated with the public test to get execution feedbacko1subscript𝑜1o_{1}italic_o start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT.\nAt a turnt𝑡titalic_t, the generator is conditioned on the history to generate solutionyt∼πθ(.|x,y<t,o<t)y_{t}\\sim\\pi_{\\theta}(.|x,y_{<t},o_{<t})italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∼ italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( . | italic_x , italic_y start_POSTSUBSCRIPT < italic_t end_POSTSUBSCRIPT , italic_o start_POSTSUBSCRIPT < italic_t end_POSTSUBSCRIPT ).\nThe rollout ends when the turn limit is reached or the public tests pass upon which the solution is executed on private tests.\nSince, the agents can generate the optimal solution at any turn, this is a 1-step recoverable process.\n(b) Training loop of our methodμ𝜇\\muitalic_μCode– which comprises of a generator and a learned verifier.\nDuring each iteration, rollouts are collected usingπθsubscript𝜋𝜃\\pi_{\\theta}italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPTand we train a verifierRϕsubscript𝑅italic-ϕR_{\\phi}italic_R start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPTto rank candidate solutions for a prompt.\nThe verifierRϕsubscript𝑅italic-ϕR_{\\phi}italic_R start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPTis then used to construct a local expert and relabel the collected rollouts.\nLastly, the generator is fine-tuned with this expert dataset.",
                "position": 153
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3μ𝜇\\muitalic_μCode: Multi-turn Code Generation",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20380/extracted/6237315/assets/bar_plot_verifier_new.png",
                "caption": "Figure 2:Comparison of relabeling with learned verifier (LV) and oracle verifier (OV) with the 1B model. The variant OV+LV aggregates a dataset from both verifiers for fine-tuning the generator. Note that OV+LV performs better than OV. However, relabeling with LV outperforms on MBPP and performs comparably on HumanEval, thereby demonstrating the benefits of leveraging the learned verifier for training the generator.",
                "position": 655
            },
            {
                "img": "https://arxiv.org/html/2502.20380/extracted/6237315/assets/varying_generator_new.png",
                "caption": "Figure 3:Comparison ofμ𝜇\\muitalic_μCodeand baselines with 1B models on the ability of the learned generator to incorporate execution feedback at each turn.\nWe observe thatμ𝜇\\muitalic_μCodeconsistently improves the BoN accuracy across turns on both datasets, whereas the baselines show marginal improvements with turns.",
                "position": 660
            },
            {
                "img": "https://arxiv.org/html/2502.20380/extracted/6237315/assets/test_time_scaling_new.png",
                "caption": "Figure 4:Test-time scaling with different values of candidate solutionsN𝑁Nitalic_Nat each turn and different ways of learning verifiers.\nWe compare with verifiers learned on samples fromμ𝜇\\muitalic_μCodeand base policy.\nThe candidate solutions are obtained from the 1B generator ofμ𝜇\\muitalic_μCodeat each turn.\nWe observe that the BoN performance improves with larger values of N on both datasets.\nThe verifier learned with on-policy samples perform better.",
                "position": 830
            },
            {
                "img": "https://arxiv.org/html/2502.20380/extracted/6237315/assets/loss_ablation.png",
                "caption": "Figure 5:Comparison between BCE and BT loss function for training the verifier.\nWe train the verifiers on samples generated by the base model (Llama-3.2-1B-Instruct).\nThe learned verifier then ranks the candidate solutions from base model and the BoN performance of selected solution is reported.\nThe verifier trained with BT loss performs better increasing value of N.",
                "position": 864
            },
            {
                "img": "https://arxiv.org/html/2502.20380/extracted/6237315/assets/qualitative_example_old.png",
                "caption": "Figure 6:A qualitative example of multi-turn BoN search using dense rewards obtained via the learned verifier inμ𝜇\\muitalic_μCode.\nHere, we show the top 3 ranked solutions at each turnt𝑡titalic_twhereRϕ⁢(x,yti)≥Rϕ⁢(x,ytj)subscript𝑅italic-ϕ𝑥superscriptsubscript𝑦𝑡𝑖subscript𝑅italic-ϕ𝑥superscriptsubscript𝑦𝑡𝑗R_{\\phi}(x,y_{t}^{i})\\geq R_{\\phi}(x,y_{t}^{j})italic_R start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT ( italic_x , italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) ≥ italic_R start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT ( italic_x , italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT )fori<j𝑖𝑗i<jitalic_i < italic_j.\nWe observe that the learned verifier selects the better solution (in orange) at each turn. The selected solution is passed to public tests to retrieve execution feedback for the generator to improve the next code solution. The selected solution at each turn is better than the last (less errors highlighted in yellow), with the final solution passing all tests. Note that there are 2 correct solutions at the final turn.",
                "position": 871
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]