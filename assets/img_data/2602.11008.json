[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11008/x1.png",
                "caption": "Figure 1:Overview of the proposed method.Left:Budget allocation formulated as a shortest-path problem on a directed graph, where nodes represent compression options and edges encode cost (reconstruction error), solved via DP algorithm to find the optimal sequence of operations.Right:The selected optimal path determines per-layer compression parameters (rankKiK_{i}and sparsitySiS_{i}), which are then applied to each layer via Eigen value decomposition (EVD) followed by structured hard thresholding sparsification (ùíØ(.)\\mathcal{T}(.)) of coefficients.",
                "position": 179
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11008/x2.png",
                "caption": "Figure 3:Comparison of ROCKET with alternative budget allocation methods (Uniform, ARSGao et¬†al. (2024), Dobi-SVDWang et¬†al. (2025a), and ARAXv et¬†al. (2025)) on three model configurations: Qwen3-8B at 20% and 40% pruning, and LLaMA2-7B at 40% pruning. Subplots show normalized performance on eight benchmarks (C4 perplexity inverted so higher is better), scaled to each model‚Äôs dense baseline (value=1.0). ROCKET consistently retains the most performance under the same parameter constraints.",
                "position": 633
            }
        ]
    },
    {
        "header": "5Ablations",
        "images": []
    },
    {
        "header": "6Conclusion and Limitations",
        "images": []
    },
    {
        "header": "7Ethical Statement and Broader Impact",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProposed Algorithm",
        "images": []
    },
    {
        "header": "Appendix BNorm-Preserving Properties of Low-Rank Approximation and Sparsification",
        "images": []
    },
    {
        "header": "Appendix CMapping MCKP to Graph Theory.",
        "images": []
    },
    {
        "header": "Appendix DInference Optimization",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11008/x3.png",
                "caption": "Figure 5:Comparison between PyTorch‚Äôs built-in sparse-matrix vector multiplication andMackoacross MLP layers in Qwen3-8B.Mackoshows consistently better running time for large coefficients (gate and up projections), while being on par with PyTorch for smaller sparse matrices (down projection).",
                "position": 2114
            }
        ]
    },
    {
        "header": "Appendix EFurther results",
        "images": []
    },
    {
        "header": "Appendix FFurther Ablations",
        "images": []
    }
]