[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19440/extracted/6477588/Figures/Main.png",
                "caption": "Figure 1:Axes of our emergent-knowledge probe. We track how interpretable, categorical features surface in a language model over time (training checkpoints), space (depth across transformer blocks), and scale (parameter count), progressing from sparse or absent concepts (grey bulbs) to rich representations (yellow bulbs)",
                "position": 115
            }
        ]
    },
    {
        "header": "2Background and Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19440/extracted/6477588/Figures/sae_metrics_comparison.png",
                "caption": "Figure 2:Hyperparameter sweep for sparse‚Äìautoencoder interpretability.\nLeft: mean F1-score as the activation budgetkùëòkitalic_kvaries with width fixed ath=256‚Ñé256h=256italic_h = 256; right: mean F1-score as the latent widthh‚Ñéhitalic_hvaries withk=1ùëò1k=1italic_k = 1.\nThe optimal setting for our data isk=1,h=512formulae-sequenceùëò1‚Ñé512k=1,\\,h=512italic_k = 1 , italic_h = 512, which maximises mean F1.",
                "position": 251
            }
        ]
    },
    {
        "header": "3EyeSee: A Framework for Probing Categorical Concepts",
        "images": []
    },
    {
        "header": "4Temporal Emergence of Categorical Knowledge inPythia-12B",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19440/extracted/6477588/Figures/subject_activation_patterns_over_time.png",
                "caption": "Figure 3:Activation patterns of categorical concepts in a 12B-parameter language model across training checkpoints. The left panel illustrates the global activation trajectory, while panels on the right display domain-specific emergence patterns, highlighting distinct activation timings for various knowledge concepts.",
                "position": 555
            }
        ]
    },
    {
        "header": "5Analysis of Representation Space Across Model Layers",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19440/extracted/6477588/Figures/combined_layer_analysis.png",
                "caption": "Figure 4:Cosine similarity (left) reveals three macro blocks (embedding, processing core, output), while SAE probes (right) show that feature directions are highly local in depth‚Äîwith a striking echo between the first and last layers‚Äîindicating that the network temporarily hides early lexical axes during computation before restoring them for final prediction.",
                "position": 620
            }
        ]
    },
    {
        "header": "6Feature Emergence at Scale",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19440/extracted/6477588/Figures/subject_activation_patterns_across_scales.png",
                "caption": "Figure 5:Concept-activation saturation with model scale.Left:percentage of all concepts or features which is activated for each Pythia checkpoint from 14‚ÄâM to 12‚ÄâB parameters (log scale).\nA single inflection between the 160‚ÄâM and 410‚ÄâM models raises activation from<5%absentpercent5<\\!5\\%< 5 %to‚âà95%absentpercent95\\approx 95\\%‚âà 95 %, after which the curve plateaus.Right:per-domain activation profiles show similar critical points for most areas, while Business concepts rise more gradually.",
                "position": 677
            }
        ]
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19440/extracted/6477588/Figures/subject_activation_by_layer.png",
                "caption": "Figure 6:Percentage of high-fidelity neurons (F1‚â•0.9absent0.9\\geq 0.9‚â• 0.9) that fire for each subject in every transformer block of the 12-B Pythia model. Values are averaged over the combinedMMLU+MMLU-Proprompt set.",
                "position": 1885
            },
            {
                "img": "https://arxiv.org/html/2505.19440/extracted/6477588/Figures/alignment_metrics_plot.png",
                "caption": "Figure 7:Alignment quality across model scale.\nLeft: linear CKA between projected activationsXm‚Å¢Wm‚ãÜsubscriptùëãùëösuperscriptsubscriptùëäùëö‚ãÜX_{m}W_{m}^{\\star}italic_X start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT italic_W start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPTand the referenceX12‚Å¢Bsubscriptùëã12BX_{12\\text{B}}italic_X start_POSTSUBSCRIPT 12 B end_POSTSUBSCRIPT.\nRight: pairwise cosine matrix correlation for the same pairs.",
                "position": 1899
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]