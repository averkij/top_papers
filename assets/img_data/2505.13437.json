[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13437/x1.png",
                "caption": "Figure 1:Video generation results for fine-grained human action‚Äúsplit leap with 1 turn\". OurFinePhysdemonstrates superior performance in generating physically plausible fine-grained human actions, while SOTA methods exhibit significant issues, including severe temporal inconsistencies[23], noticeable limb distortions[46], and character anomalies[7].",
                "position": 118
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13437/x2.png",
                "caption": "Figure 2:Overview of Finephys.FinePhys addresses the challenging task of generating fine-grained human action videos by explicitly incorporating physical equations exploiting pose modality.\nThe pipeline begins with online extracting 2D poses, then transforms them into 3D using an in-context learning module, achieving the data-driven 3D skeleton sequenceSd‚Å¢d3‚Å¢DsubscriptsuperscriptùëÜ3ùê∑ùëëùëëS^{3D}_{dd}italic_S start_POSTSUPERSCRIPT 3 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_d italic_d end_POSTSUBSCRIPT.\nTo incorporate the physical laws of motion, we introduce a Phys-Net module to re-estimate the 3D positions of each human joint by accounting for second-order temporal variations (i.e., accelerations) in both forward and reverse directions, yielding physically predicted 3D posesSp‚Å¢p3‚Å¢DsubscriptsuperscriptùëÜ3ùê∑ùëùùëùS^{3D}_{pp}italic_S start_POSTSUPERSCRIPT 3 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p italic_p end_POSTSUBSCRIPT.\nSubsequently,Sd‚Å¢d3‚Å¢DsubscriptsuperscriptùëÜ3ùê∑ùëëùëëS^{3D}_{dd}italic_S start_POSTSUPERSCRIPT 3 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_d italic_d end_POSTSUBSCRIPTandSp‚Å¢p3‚Å¢DsubscriptsuperscriptùëÜ3ùê∑ùëùùëùS^{3D}_{pp}italic_S start_POSTSUPERSCRIPT 3 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p italic_p end_POSTSUBSCRIPTare fused, projected back into 2D space, encoded into multi-scale latent maps, and integrated into 3D-UNets to guide the denoising process.",
                "position": 209
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13437/x3.png",
                "caption": "Figure 3:The PhysNet Module.Given the inputSd‚Å¢d3‚Å¢DsuperscriptsubscriptùëÜùëëùëë3ùê∑S_{dd}^{3D}italic_S start_POSTSUBSCRIPT italic_d italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 3 italic_D end_POSTSUPERSCRIPT, PhysNet leverages both global and local temporal dynamics in a bi-directional manner to estimate the terms of the Euler-Lagrange equations. By integrating with an ODE solver, the module can predict future and past states, thereby enhancing the originalSd‚Å¢d3‚Å¢DsuperscriptsubscriptùëÜùëëùëë3ùê∑S_{dd}^{3D}italic_S start_POSTSUBSCRIPT italic_d italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 3 italic_D end_POSTSUPERSCRIPTacross both temporal directions and producing physically predicted 3D sequences, denoted asSp‚Å¢p3‚Å¢DsuperscriptsubscriptùëÜùëùùëù3ùê∑S_{pp}^{3D}italic_S start_POSTSUBSCRIPT italic_p italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 3 italic_D end_POSTSUPERSCRIPT.",
                "position": 501
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13437/x4.png",
                "caption": "Table 1:Comparison with state-of-the-art methods on two subsets of FineGym (FX-JUMP and FX-TURN).In this table, ‚ÄúT\" withinInputdenotes textual prompt, while ‚ÄúP\", ‚ÄúI\", ‚ÄúD\", and ‚ÄúC\" represent the pose, initial frame, depth map, and canny, respectively.\nIn all cases, our FinePhys outperforms various baselines based on diverse conditions by a large margin in terms of more reliable metrics, CLIP-SIM*, and User Study (the insufficiency of the CLIP-SIM has been shown).",
                "position": 598
            },
            {
                "img": "https://arxiv.org/html/2505.13437/x4.png",
                "caption": "Figure 4:Original CLIP-SIMmetricsfailto evaluate the generated results (e.g.,\nT2I-Zero produces entirely irrelevant outputs yet achieves the highest smooth score according to the original CLIP-SIM.\nIn contrast, ourenhanced CLIP-SIM*provides a more reliable evaluation that better aligns with human judgment.",
                "position": 889
            },
            {
                "img": "https://arxiv.org/html/2505.13437/x5.png",
                "caption": "Figure 5:Qualitative Results.Compared to other baselines, FinePhys demonstrates superior performance in understanding complex, fine-grained semantics, maintaining biomechanical consistency, and adhering to physical principles.",
                "position": 960
            },
            {
                "img": "https://arxiv.org/html/2505.13437/x6.png",
                "caption": "Figure 6:FinePhys effectively restores distorted and missing poses using the in-context learning and PhysNet modules, thereby providing enhanced skeletal guidance for the generation process.",
                "position": 1258
            },
            {
                "img": "https://arxiv.org/html/2505.13437/x7.png",
                "caption": "Figure 7:Generation results from FinePhys and Follow-Your-Pose[46]conditioned on noisy 2D pose input during inference.",
                "position": 1262
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining & Dataset Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13437/x8.png",
                "caption": "Figure 8:Example videos from FX-JUMP, FX-TURN and FX-SALTO.Each sample video has 16 frames, and the corresponding 2D skeleton sequence is also represented.",
                "position": 2491
            }
        ]
    },
    {
        "header": "Appendix BElaboration on Evaluation Metrics",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13437/x9.png",
                "caption": "Figure 9:Limitations of semantic consistency in original CLIP-SIM.We utilize CLIP models to obtain the embedded textual features and Probably Approximately Correct (PAC) for dimensionality reduction.\nThe distribution of embedded category labels from FX-JUMP, FX-TURN and FX-SALTO as well as UCF101 is shown. Label features from FineGym are entangled, while those from UCF101 are clearly seperated.",
                "position": 2919
            },
            {
                "img": "https://arxiv.org/html/2505.13437/x10.png",
                "caption": "Figure 10:Domain image of original CLIP-SIM and the improved CLIP-SIM* from FX-JUMP, FX-TURN and FX-SALTO.Reference images generated by Stable Diffusion may not accurately reflect the nuances of specific actions or their dynamics (Original CLIP-SIM), while CLIP-SIM* randomly selects one video from the given class and extracts three representative frames (start, middle, end) to form a more reasonable reference set.",
                "position": 2958
            },
            {
                "img": "https://arxiv.org/html/2505.13437/extracted/6455227/figs_supplementary/fig_C.2_jump_new.png",
                "caption": "Figure 11:Visualization of different pose sequenceson the class‚Äúswitch leap with 0.5 turn\"from theFX-Jumpsubset, demonstrating the complete transformation process within our framework.",
                "position": 3010
            },
            {
                "img": "https://arxiv.org/html/2505.13437/x11.png",
                "caption": "Figure 12:Visualization of different pose sequenceson the class‚Äú2 turn with free leg held upward in 180 split position throughout turn\"from theFX-Turnsubset, demonstrating the complete transformation process within our framework.",
                "position": 3158
            },
            {
                "img": "https://arxiv.org/html/2505.13437/x12.png",
                "caption": "Figure 13:Visualization of different pose sequenceson the class‚Äúsalto backward stretched with 2 twist\"from theFX-Saltosubset, demonstrating the complete transformation process within our framework.",
                "position": 3176
            },
            {
                "img": "https://arxiv.org/html/2505.13437/x13.png",
                "caption": "Figure 14:Limitations in intractable cases.For class 31:double salto backward stretched, FinePhys fails to generate a double salto, resulting in only a single flip being observed.",
                "position": 3213
            },
            {
                "img": "https://arxiv.org/html/2505.13437/x14.png",
                "caption": "Figure 15:Negative Impact of Initial Pose Detection.. Current online pose estimators may fail completely due to the complexity of fine-grained human actions, which affects subsequent processing stages in the FinePhys framework. Even when the physical structure of the human body is spatially restored, the intricate motion dynamics cannot be accurately reconstructed, resulting in unrealistic or static video outputs.",
                "position": 3217
            },
            {
                "img": "https://arxiv.org/html/2505.13437/x15.png",
                "caption": "Figure 16:Display of the interface of User Study.",
                "position": 3237
            },
            {
                "img": "https://arxiv.org/html/2505.13437/x16.png",
                "caption": "Figure 17:Qualitative Results on FX-JUMP.FX-JUMP focuses on the motion continuity of the gymnastics‚Äô body.\nCompared with other\nbaselines, our method demonstrates superior performance in\nunderstanding physical consistency.",
                "position": 3241
            },
            {
                "img": "https://arxiv.org/html/2505.13437/x17.png",
                "caption": "Figure 18:Qualitative Results on FX-TURN.FX-TURN focuses on the minor difference of the gymnastics‚Äô body.\nCompared with other\nbaselines, our method demonstrates superior performance in\nunderstanding complex and fine-grained semantics, keeping\nthe consistency of bio-physical characteristics, and adhering\nto the physical principles.",
                "position": 3249
            },
            {
                "img": "https://arxiv.org/html/2505.13437/x18.png",
                "caption": "Figure 19:Qualitative Results on FX-SALTO.FX-SALTO demands gymnastics‚Äôs body rotates 360¬∞ around a horizontal axis\nwith the feet passing over the head, which is the most difficult in all of three sub-datasets in FineGym. Compared with other\nbaselines, results in our methods maintain better temporal consistency, more adhering to the bio-physical rules.",
                "position": 3258
            }
        ]
    },
    {
        "header": "Appendix CAdditional Illustration & Analysis",
        "images": []
    }
]