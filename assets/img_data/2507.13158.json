[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Motivation: Reinforcement Learning in the Era of Large Language Models",
        "images": []
    },
    {
        "header": "2Revisiting the Foundations of Reinforcement Learning under an LLM Context",
        "images": []
    },
    {
        "header": "3Optimizing LLMs beyond Imitation: Why do we Need Neural Reward Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.13158/extracted/6631553/figs/teaser_0922.png",
                "caption": "Figure 1:A comparison of different LLM generation optimization approaches.The first row represents (1) direct generation, (2) prompt optimization, (3) Supervised Fine-Tuning (SFT) on a high-quality dataset. The second row represents methods that leverage reward models (i.e., the IRL approach): (4) reward models can be used to filter out low-quality generations, (5-6) reward models can be combined with prompt optimization or fine-tuning methods to improve the generation quality.Only reward models enable inference time optimization.",
                "position": 860
            }
        ]
    },
    {
        "header": "4From Real World Evidences to Alignment: Practical IRL via Reward Modeling",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.13158/extracted/6631553/figs/rm_scaling.png",
                "caption": "Figure 2:Reward model overoptimization (Figure 1 ofGao et al. (2023)). The x-axis represents the degree of optimization, measured by the KL divergence between the optimized policy and the original checkpoint. The y-axis indicates the reward score assigned by different reward models. The two panels correspond to different optimization methods: Best-of-N sampling and PPO-based training. Each curve color denotes a different reward model size. Across all settings, the gap between the solid line (score assigned by the optimized reward model) and the dashed line (score assigned by a held-out reference reward model) quantifies the degree of overoptimization—i.e., the extent to which the optimized policy exploits idiosyncrasies of the reward model rather than aligning with the intended objective.",
                "position": 1597
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]