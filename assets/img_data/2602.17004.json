[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.17004/x1.png",
                "caption": "Figure 1:The training loss graph for Trinity Large, with no sub-sampling or smoothing. For clarity, we indicate where the batch size was increased to 128M (134,217,728) tokens, as well as the points where we switch data mixtures.",
                "position": 101
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.17004/Trinity6.png",
                "caption": "Figure 2:The architecture of the Trinity model family.∗RoPE is only present in local layers.†The grouped-query attention has a sliding window for the local layers.",
                "position": 127
            }
        ]
    },
    {
        "header": "3Pre-training",
        "images": []
    },
    {
        "header": "4Post-Training",
        "images": []
    },
    {
        "header": "5Trinity Large Evaluation Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.17004/benchmark_comparison.png",
                "caption": "Figure 3:A comparison of Trinity Large Base to other similar open-weight base models.",
                "position": 1033
            },
            {
                "img": "https://arxiv.org/html/2602.17004/trinity-large-throughput-comp.png",
                "caption": "Figure 4:Throughput comparison of models. All tests were done with models quantized to FP8, using vLLM, on 8xH200.",
                "position": 1135
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "7Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Additional Contributors",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]