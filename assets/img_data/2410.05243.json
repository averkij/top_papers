[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05243/x1.png",
                "caption": "Figure 1:Examples of agent tasks across platforms and performance onGUI grounding(♣♣\\clubsuit♣: ScreenSpot),offline agent(♠♠\\spadesuit♠: Multimodal-Mind2Web, AndroidControl, and OmniAct), andonline agent benchmarks(♥: Mind2Web-Live and AndroidWorld) when using GPT-4 as the planner.",
                "position": 171
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05243/x2.png",
                "caption": "Figure 2:SeeAct-V, which uses screenshots as the only environmental observation (task instructions are input as text), without relying on HTML or a11y trees. It includes an MLLM that generates textual plans and a visual grounding model to map textual plans into coordinates on the screenshot.Note: “Click” is always automatically inserted before “Type”.",
                "position": 239
            },
            {
                "img": "https://arxiv.org/html/2410.05243/x3.png",
                "caption": "Figure 3:Examples ofvisual,positional, andfunctionalREs.",
                "position": 274
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05243/x4.png",
                "caption": "Figure 4:Error distribution from manual analysis.",
                "position": 977
            },
            {
                "img": "https://arxiv.org/html/2410.05243/x4.png",
                "caption": "Figure 4:Error distribution from manual analysis.",
                "position": 980
            }
        ]
    },
    {
        "header": "4Conclusions and Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelated Work",
        "images": []
    },
    {
        "header": "Appendix BExamples",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05243/x5.png",
                "caption": "Figure 1:Example of the Mind2Web Evaluation Pipeline.",
                "position": 2001
            },
            {
                "img": "https://arxiv.org/html/2410.05243/x6.png",
                "caption": "Figure 2:Example of the AndroidControl Evaluation Pipeline .",
                "position": 2008
            },
            {
                "img": "https://arxiv.org/html/2410.05243/x7.png",
                "caption": "Figure 3:OmniACT Evaluation Pipeline Example.",
                "position": 2015
            },
            {
                "img": "https://arxiv.org/html/2410.05243/x8.png",
                "caption": "Figure 4:Examples of Training Data from different sources.",
                "position": 2023
            }
        ]
    },
    {
        "header": "Appendix CData Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05243/x9.png",
                "caption": "Figure 5:Example of the image annotations of a bounding box and an arrow",
                "position": 2091
            }
        ]
    },
    {
        "header": "Appendix DModel and Training Details",
        "images": []
    },
    {
        "header": "Appendix EEvaluation Details",
        "images": []
    },
    {
        "header": "Appendix FPrompts",
        "images": []
    }
]