[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.09658/x1.png",
                "caption": "Figure 1:Examples from HumbleBench where correct options are marked green. This benchmark is derived from a panoptic scene graph dataset with rich annotations in object, relation, and attribute. Different from existing hallucination benchmarks, HumbleBench has a “None of the above” option in each question to test whether models can identify when no provided answer is valid.",
                "position": 100
            }
        ]
    },
    {
        "header": "2HumbleBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.09658/x2.png",
                "caption": "Figure 2:Distributions of answer choices (left) and question types (right).NOTAdenotes the “None of the above” option.",
                "position": 122
            },
            {
                "img": "https://arxiv.org/html/2509.09658/x3.png",
                "caption": "Figure 3:Construction pipeline of HumbleBench. For each image in the panoptic scene graph dataset, we first extract object and relation information from the scene graph annotations while using InstructBLIP to extract attribute information. Then, we prompt GPT-4-Turbo to combine all information and generate multiple-choice questions, each with five answer options (the last option is set to “None of the above”). Finally, we conduct a manual filtering process to refine the questions and options.",
                "position": 213
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.09658/x4.png",
                "caption": "Figure 4:Results on HumbleBench-E. The red dashed line indicates the random guess baseline (with five choices).",
                "position": 443
            },
            {
                "img": "https://arxiv.org/html/2509.09658/x5.png",
                "caption": "Figure 5:Accuracy on HumbleBench vs. HumbleBench-GN.This scatter plot correlates each model’s standard overall accuracy (x-axis) with its accuracy from our noise image test (y-axis), where red points indicate general purpose models and blue points indicate reasoning models. A truly robust model would appear in the top-right corner.",
                "position": 459
            },
            {
                "img": "https://arxiv.org/html/2509.09658/x6.png",
                "caption": "Figure 6:Error analysis of Qwen-VL 2.5 on the HumbleBench benchmark. Green options are correct answers while red options are predicted by the model. These examples highlight several key failure modes. (Question 1-4) demonstrate a frequent inability to select “None of the above” (NOTA), where the model instead hallucinates an object, attribute, or relation that is not present in the image. (Question 5) shows a standard object error, where the model fails a simple counting task. (Question6) illustrates a critical failure in visual faithfulness; when presented with a non-informative noise image, the model fabricates an answer instead of acknowledging the absence of visual evidence.",
                "position": 469
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix A",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.09658/x7.png",
                "caption": "Figure 7:The detailed prompt used to instruct GPT-4-Turbo for the generation of HumbleBench questions. It specifies the input format, output structure, and a series of strict constraints to ensure question quality.",
                "position": 639
            },
            {
                "img": "https://arxiv.org/html/2509.09658/figures/GUI.png",
                "caption": "Figure 8:Screenshot of the custom filtering GUI developed using PyQt5. The interface displays each image-question pair and provides annotators with dedicated controls to Keep, Modify, or Delete candidates, which was essential for ensuring the high quality of the final benchmark.",
                "position": 725
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]