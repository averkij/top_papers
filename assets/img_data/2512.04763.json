[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04763/x1.png",
                "caption": "Figure 1:Overview.\nWe employ specialized LoRA adapters to enable small (vision) language models to perform memory operations for on-device deployment. The base model\ndynamically switches between expert adapters, each trained for a distinct stage: (1)knowledge extraction, (2)memory update, (3)memory-augmented generation. In the last stage, the model can switch betweentext-onlyandmultimodaladapter, depending on the input. By specializing each adapter for its specific operation, MemLoRA(-V)\nachieves performance comparable to models 10-60x larger while enabling efficient local execution without cloud API dependencies.",
                "position": 102
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04763/x2.png",
                "caption": "Figure 2:Training Pipeline (ExtractionLoRA). We first generate outputs for the specific memory-related task via a larger model (teacher). Raw output is further cleaned and used as target for training LoRA parameters of a small model (student).",
                "position": 224
            },
            {
                "img": "https://arxiv.org/html/2512.04763/x3.png",
                "caption": "Figure 3:Our augmentation of LoCoMoincludes challenging VQA tasks about (a) counting object quantities, (b) identifying colors, and (c) asking about unusual objects.",
                "position": 273
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "ATeacher Data Preparation and Efficiency",
        "images": []
    },
    {
        "header": "BTraining Pipeline",
        "images": []
    },
    {
        "header": "CCreating the VQA benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04763/x4.png",
                "caption": "Figure C4:VQA Examples.LoCoMo images with corresponding three associated generated questions (Q), InternVL3-78B answers (A), and predictions with InternVL3-2B without (P(IVL2B)) and with (P(IVL2B+Exp)) expert adapters.",
                "position": 1652
            }
        ]
    },
    {
        "header": "DTechnical Details",
        "images": []
    },
    {
        "header": "EAdditional Experiments",
        "images": []
    }
]