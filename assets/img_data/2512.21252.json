[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21252/x1.png",
                "caption": "Figure 1:DreaMontage: Flexible Dreams, Seamless Montage.Our model generates one-shot, long-form videos guided by arbitrary keyframes or video clips anchored at precise temporal locations.",
                "position": 125
            },
            {
                "img": "https://arxiv.org/html/2512.21252/x2.png",
                "caption": "Figure 2:Overview of the DreaMontage.The left panel illustrates the multi-stage training pipeline, progressing from the Adaptive Tuning to the Visual Expression SFT and Tailored DPO. The right panel depicts the inference pipeline, where reference (condition) images/videos and rephrased prompts guide the generation process, supporting auto-regressive long-video generation.",
                "position": 128
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21252/x3.png",
                "caption": "Figure 3:The Interm-Cond Adaptation strategy.(a) Due to the Causalty VAE’s temporal downsampling, an intermediate latent aggregates information from multiple frames, making it an imprecise condition for a specific timestamp.\n(b) To resolve this, we align the training distribution with inference.\nEach single condition frame (or the initial frame of a condition video) is re-encoded while the subsequent frames of the condition video are re-sampled from the latent distribution.",
                "position": 203
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21252/x4.png",
                "caption": "Figure 4:The Shared-RoPE strategy for the super-resolution model.In addition to channel-wise concatenation, we introduce a sequence-wise conditioning mechanism to eliminate artifacts.\nCondition frames are appended to the tail of the sequence while share the same RoPE value as the target frames they guide (e.g.,CiC_{i}shares the RoPE oft1t_{1}).\nIn the case of video condition, this strategy is only applied to the first frame.",
                "position": 230
            },
            {
                "img": "https://arxiv.org/html/2512.21252/x5.png",
                "caption": "Figure 5:Illustration of the Tailored DPO.To eliminate specific generation artifacts, we construct preference pairs via two distinct pipelines:Pipeline Aaddresses abrupt cuts by leveraging a trained VLM discriminator to automatically select positive/negative samples, whereasPipeline Btargets subject motion rationality through human-annotated screening of challenging cases. These pairs subsequently drive the DPO training to optimize the policyπθ\\pi_{\\theta}against the reference modelπref\\pi_{\\text{ref}}, ensuring smoother transitions and physically plausible motions.",
                "position": 279
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21252/x6.png",
                "caption": "Figure 6:Qualitative visualization of arbitrary frame-guided generation.Red-borderedframes denote the user-provided conditions anchored at specific timestamps, while the other frames represent the generated content.",
                "position": 410
            },
            {
                "img": "https://arxiv.org/html/2512.21252/x7.png",
                "caption": "Figure 7:Quantitative comparison with state-of-the-art methods.\nThegreenbars represent the percentage of cases where our result is visually superior,redfavors the competitor, andgrayindicates comparable quality. Reported numbers are the GSB scores.",
                "position": 441
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]