[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.13264/extracted/6629836/images/header.jpeg",
                "caption": "",
                "position": 95
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Modeling",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.13264/extracted/6629836/images/voxtral-arch.002.jpeg",
                "caption": "Figure 1:Voxtral Architecture.The audio encoder processes the speech input, attending to 30-second chunks of audio independently. The audio embeddings are concatenated at the output, and downsampled by a factor of 4x in the audio-language adapter. The multimodal LLM decoder auto-regressively predicts text tokens, conditional on the audio and text inputs.",
                "position": 162
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.13264/extracted/6629836/images/PT.png",
                "caption": "Figure 2:Pretraining patterns. A single audio-text example(A,T)ùê¥ùëá(A,T)( italic_A , italic_T )is first segmented into a set of audio-text pairs{(An,Tn)}n=1Nsuperscriptsubscriptsubscriptùê¥ùëõsubscriptùëáùëõùëõ1ùëÅ\\left\\{(A_{n},T_{n})\\right\\}_{n=1}^{N}{ ( italic_A start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_T start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_n = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT, based on the timestamps and transcriptions returned by segmentation stage. For the audio-to-text repetition¬†pattern, a given audioAnsubscriptùê¥ùëõA_{n}italic_A start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPTis repeated in the text spaceTnsubscriptùëáùëõT_{n}italic_T start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT. For the cross-modal continuation¬†pattern, each audioAnsubscriptùê¥ùëõA_{n}italic_A start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPTis followed by its subsequent textTn+1subscriptùëáùëõ1T_{n+1}italic_T start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT. The task is signaled to the model by the<repeat>and<next>special tokens respectively.",
                "position": 252
            }
        ]
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.13264/extracted/6629836/results/multi_avg.png",
                "caption": "Figure 3:Speech Recognition Benchmarks.Macro-average WER results across tasks. Voxtral Small outperforms all open and closed-source models on English Short-Form and MCV. Voxtral Mini Transcribe beats GPT-4o mini Transcribe and Gemini 2.5 Flash in every task.",
                "position": 388
            },
            {
                "img": "https://arxiv.org/html/2507.13264/extracted/6629836/results/fleurs_translate.png",
                "caption": "Figure 4:FLEURS Translation.BLEU scores for source/target language pairs on the FLEURS Translation benchmark. Voxtral Small achieves state-of-the-art for every combination of languages.",
                "position": 398
            },
            {
                "img": "https://arxiv.org/html/2507.13264/extracted/6629836/results/audio_understanding.png",
                "caption": "Figure 5:Speech Understanding Benchmarks.We report the accuracy across three speech understanding benchmarks and three synthesized speech subsets of text benchmarks. Voxtral Small is competitive with closed-source models, surpassing GPT-4o mini Audio on three of the seven benchmarks.",
                "position": 408
            },
            {
                "img": "https://arxiv.org/html/2507.13264/extracted/6629836/results/text_with_voxtral_mini.png",
                "caption": "Figure 6:Text-Only Benchmarks.We report the accuracy across five standard text understanding benchmarks. Voxtral Small performs comparably to Mistral Small 3.1, highlighting its strong text capabilities.",
                "position": 418
            }
        ]
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.13264/x1.png",
                "caption": "Figure 7:Effect of Padding.Word error rate results on FLEURS English (left) and FLEURS French (middle), alongside 3-shot Accuracy on Llama QA (right) for models trained with and without 30-second padding.",
                "position": 438
            },
            {
                "img": "https://arxiv.org/html/2507.13264/x2.png",
                "caption": "Figure 8:Effect of Downsampling.Word error rate results on FLEURS English (left) and FLEURS French (middle), alongside 3-shot Accuracy on Llama QA (right) for various frame-rates, achieved by increasing the downsampling factor by powers of 2.",
                "position": 452
            },
            {
                "img": "https://arxiv.org/html/2507.13264/x3.png",
                "caption": "Figure 9:Pattern Proportions.Word error rate results on FLEURS English (left) and FLEURS French (middle), alongside 3-shot Accuracy on Llama QA (right) for varying proportions of pretrain patterns.",
                "position": 466
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]