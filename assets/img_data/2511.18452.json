[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background and Related Work",
        "images": []
    },
    {
        "header": "3Method: NAF",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.18452/x1.png",
                "caption": "Figure 2:NAF architectureallows to upsample low-resolution VFM features to any resolution, guided solely by the original high-resolution image.",
                "position": 384
            },
            {
                "img": "https://arxiv.org/html/2511.18452/x2.png",
                "caption": "Figure 3:Details of the dual-branch image encoder.NAF encoder considers both a pixel-wise branch and a local-contextual branch.",
                "position": 436
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Ablations",
        "images": []
    },
    {
        "header": "6Extension: Image Restoration",
        "images": []
    },
    {
        "header": "Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Table of contents",
        "images": []
    },
    {
        "header": "Appendix AMathematical Discussions",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.18452/x3.png",
                "caption": "Figure 4:Illustration of the mean and channel-specific cosine and sine ofΔ​Φc\\Delta\\Phi_{c}.We compute the mean across all channels and select a single random channel to illustrate its individual behavior. For the cosine, we observe an overall decreasing pattern as the distance from the center increases.",
                "position": 1771
            },
            {
                "img": "https://arxiv.org/html/2511.18452/x4.png",
                "caption": "Figure 5:Dot and cross products for a specific channelgiven a query point p on an image. We highlight the neighborhood around p using a dashed red square. On the feature side, after VFM-downsampling, we observe that implicitly NAF discriminates boundaries.",
                "position": 1800
            },
            {
                "img": "https://arxiv.org/html/2511.18452/x5.png",
                "caption": "Figure 6:Illustration of radial wavelets induced by NAFfor a9×99\\times 9neighborhood. We plotcos⁡(ωc⋅Δ​x)\\cos(\\omega_{c}\\cdot\\Delta x)for differentλ\\lambdawhereΔ​x\\Delta xis defined as theℓ1\\ell_{1}distance overxx-axis oryy-axis between two coordinates over a grid map. In this plot we set the periods as:λi=100i/c\\lambda_{i}=100^{i/c}.",
                "position": 1893
            },
            {
                "img": "https://arxiv.org/html/2511.18452/x6.png",
                "caption": "Figure 7:Attention maps:⟨Qp,Kq⟩\\langle Q_{p},K_{q}\\ranglebetween a query pointppand its9×99\\times 9patch neighborhood (qq). We take896×896896\\times 896input images to visualize finer details. In the first row, we see that NAF learns to discriminate between the sky and the tree, i.e., borders. On the second row, it learns to discriminate more complex shapes (dog). On the third row, in uniform region, it shows decreasing attention pattern, akin to the gaussian filter used in classical JBF.",
                "position": 1917
            }
        ]
    },
    {
        "header": "Appendix BFeature Upsampling Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.18452/x7.png",
                "caption": "Figure 8:PCA plots of different upsamplers for random images.The first colum represents RGB images, the second one, the low resolution features, the others the PCA after upsampling. We use the same basis decomposition for plotting. Only JAFAR[jafar], AnyUp[wimmer2025anyup]and NAF produce sharp PCAs while preserving input feature representation.",
                "position": 1993
            },
            {
                "img": "https://arxiv.org/html/2511.18452/x8.png",
                "caption": "Figure 9:Segmentation predictions using different upsamplers.The first two rows are on Cityscapes[cordts2016cityscapes], the last two rows on VOC[pascalvoc]. We see that while being VFM-agnostic NAF better preserves structure compared to JAFAR[jafar]and AnyUp[wimmer2025anyup]that tend to focus too much on colors leading to noisy semantics.",
                "position": 1996
            },
            {
                "img": "https://arxiv.org/html/2511.18452/x9.png",
                "caption": "Figure 10:Detph Estimation using different upsamplerson NYUv2[silberman2012nyuv2]. Compared to AnyUp[wimmer2025anyup], NAF outputs smoother predictions without the noisy effect we observe on some regions using AnyUp.",
                "position": 1999
            }
        ]
    },
    {
        "header": "Appendix CImage Restoration",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.18452/x10.png",
                "caption": "Figure 11:Image restoration using NAF. On the left two images we apply a gaussian noise. On the right we apply a channel-wise salt and pepper noise. NAF allows to restore very noisy images even on unseen noise range (rightmost image).",
                "position": 2481
            }
        ]
    },
    {
        "header": "Appendix DComputation footprint",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.18452/x11.png",
                "caption": "(a)Number of Parameters",
                "position": 2544
            },
            {
                "img": "https://arxiv.org/html/2511.18452/x11.png",
                "caption": "(a)Number of Parameters",
                "position": 2547
            },
            {
                "img": "https://arxiv.org/html/2511.18452/x12.png",
                "caption": "(b)Computational Cost (GFLOPs)",
                "position": 2552
            },
            {
                "img": "https://arxiv.org/html/2511.18452/x13.png",
                "caption": "(c)Avg. Forward Pass Time",
                "position": 2557
            },
            {
                "img": "https://arxiv.org/html/2511.18452/x14.png",
                "caption": "(d)Avg. Backward Pass Time",
                "position": 2563
            },
            {
                "img": "https://arxiv.org/html/2511.18452/x15.png",
                "caption": "(e)Peak Memory (Forward)",
                "position": 2568
            },
            {
                "img": "https://arxiv.org/html/2511.18452/x16.png",
                "caption": "(f)Peak Memory (Backward)",
                "position": 2573
            },
            {
                "img": "https://arxiv.org/html/2511.18452/x17.png",
                "caption": "(a)Number of Parameters",
                "position": 2580
            },
            {
                "img": "https://arxiv.org/html/2511.18452/x17.png",
                "caption": "(a)Number of Parameters",
                "position": 2583
            },
            {
                "img": "https://arxiv.org/html/2511.18452/x18.png",
                "caption": "(b)Computational Cost (GFLOPs)",
                "position": 2588
            },
            {
                "img": "https://arxiv.org/html/2511.18452/x19.png",
                "caption": "(c)Avg. Forward Pass Time",
                "position": 2593
            },
            {
                "img": "https://arxiv.org/html/2511.18452/x20.png",
                "caption": "(d)Avg. Backward Pass Time",
                "position": 2599
            },
            {
                "img": "https://arxiv.org/html/2511.18452/x21.png",
                "caption": "(e)Peak Memory (Forward)",
                "position": 2604
            },
            {
                "img": "https://arxiv.org/html/2511.18452/x22.png",
                "caption": "(f)Peak Memory (Backward)",
                "position": 2609
            }
        ]
    },
    {
        "header": "Appendix ELimitations and Perspectives",
        "images": []
    }
]