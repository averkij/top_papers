[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.06307/x1.png",
                "caption": "Figure 1:There are three challenges when modeling and translating non-compositional phrases.",
                "position": 115
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Improving Non-Compositional Translation",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.06307/x2.png",
                "caption": "Figure 2:Illustration of the distinction between all three GRPO-QE-* methods. (a)QE-Positivepulls semantically equivalent texts closer. (b)QE-Negativepulls semantically inequivalent texts apart. (c)QE-Constrainedbalances both. (d)QE-DAuses a ground-truth reference translation to inform MTQE.",
                "position": 213
            },
            {
                "img": "https://arxiv.org/html/2601.06307/x3.png",
                "caption": "",
                "position": 215
            },
            {
                "img": "https://arxiv.org/html/2601.06307/x4.png",
                "caption": "",
                "position": 216
            },
            {
                "img": "https://arxiv.org/html/2601.06307/x5.png",
                "caption": "",
                "position": 217
            },
            {
                "img": "https://arxiv.org/html/2601.06307/x6.png",
                "caption": "Figure 3:Evaluation of translation abilities ofChinese idioms. Here, we see that the LIA andTrainingFree(TF) baselines do well on Qwen-2.5-3B, but not on Llama-3.1-8B (hence is unreliable). The GRPO based methods are not onlyperformant, but alsoreliable.",
                "position": 291
            },
            {
                "img": "https://arxiv.org/html/2601.06307/x7.png",
                "caption": "Figure 4:Evaluation of translation abilities ofHindi idioms. Here, we see that the core translation models (NLLB and Command-R) translate Hindi idioms well, but nChinese idioms (Fig.3). The GRPO based methods are not onlyperformant, but alsoreliable.",
                "position": 294
            },
            {
                "img": "https://arxiv.org/html/2601.06307/x8.png",
                "caption": "Figure 5:Evaluation of translation abilities ofregular Chinese sentences. Here, it is shown that performance does not deteriorate when models are trained on idiomatic data.",
                "position": 297
            },
            {
                "img": "https://arxiv.org/html/2601.06307/x9.png",
                "caption": "Figure 6:Evaluation of translation abilities ofregular Hindi sentences. Here, it shows that performance does not deteriorate when models are trained on idiomatic data.",
                "position": 300
            },
            {
                "img": "https://arxiv.org/html/2601.06307/x10.png",
                "caption": "Figure 7:Evaluation ofHindi-trained models translating Chinese idioms. Per Fig.3, Qwen-2.5-3B achieved DA: 42.89, QE: 37.09, ROUGE: 8.04, ED: 50.76, LAJ: 1.79; Llama-3.1-8B achieved DA: 40.67, QE: 37.05, ROUGE: 7.16, ED: 45.94, LAJ: 1.66. GRPO models outperform base models.",
                "position": 303
            },
            {
                "img": "https://arxiv.org/html/2601.06307/x11.png",
                "caption": "Figure 8:Evaluation ofChinese-trained models translating Hindi idioms. Per Fig.4, Qwen-2.5-3B achieved DA: 46.08, QE: 48.92, ROUGE: 5.28, ED: 44.16, LAJ: 1.95; Llama-3.1-8B achieved DA: 43.87, QE: 47.31, ROUGE: 4.80, ED: 48.65, LAJ: 1.52. GRPO methods outperform base models.",
                "position": 306
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APrompts",
        "images": []
    },
    {
        "header": "Appendix BLicenses",
        "images": []
    },
    {
        "header": "Appendix CHuman Annotators",
        "images": []
    },
    {
        "header": "Appendix DUsage of AI Assistants",
        "images": []
    }
]