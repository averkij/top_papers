[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13128/extracted/6197501/Figure/motivation.png",
                "caption": "Figure 1:Traditional methods often rely on multi-stage processes, making pipelines inflexible and complex. SongGen simplifies this with a single-stage auto-regressive transformer that supports both mixed mode and dual-track mode song generation.",
                "position": 113
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13128/extracted/6197501/Figure/framework1.png",
                "caption": "Figure 2:Overview of SongGen: An auto-regressive transformer decoder generates audio tokens with diverse patterns, incorporating user-defined controls via cross-attention. The final song is synthesized from these tokens through the audio codec decoder.",
                "position": 174
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13128/extracted/6197501/Figure/pattern2.png",
                "caption": "Figure 3:Illustration of token patterns for different generation modes. The codebook-delay pattern (from MusicGen) is applied to every audio token. (a)Mixed Pro: Directly decoding mixed tokens, with an auxiliary vocal token prediction target to enhance vocal learning.Dual-track mode:(b) Parallel: Vocal and accompaniment tokens are concatenated along the codebook dimension, with three track order variants. (c) Interleaving: Tokens from both tracks are interleaved along the temporal dimension, with two track order variants.",
                "position": 204
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13128/extracted/6197501/Figure/vibrato.png",
                "caption": "Figure 4:Mel-spectrogram visualization of our generated song featuring various singing techniques.",
                "position": 617
            },
            {
                "img": "https://arxiv.org/html/2502.13128/extracted/6197501/Figure/attan.png",
                "caption": "Figure 5:Visualization of decoder attention.",
                "position": 637
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitations and Future Work",
        "images": []
    },
    {
        "header": "Appendix BImplementation and Training Details",
        "images": []
    },
    {
        "header": "Appendix CDetails in Evaluations",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13128/extracted/6197501/Figure/codec.png",
                "caption": "Table 5:Ablation results of different neural audio codecs.",
                "position": 1533
            }
        ]
    },
    {
        "header": "Appendix DThe Impact of Different Audio Codecs.",
        "images": []
    }
]