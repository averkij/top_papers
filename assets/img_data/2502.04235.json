[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04235/x1.png",
                "caption": "Figure 1:Overview ofMAGAframework. Our method expands the original corpus through a two-stage synthesis process. Each document is reformulated to 5 new documents, achieving 3.9Ã— token number expansion while maintaining diversity through massive (genre, audience) pairs.",
                "position": 172
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Massive Genre-Audience Reformulation",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04235/x2.png",
                "caption": "Figure 2:Implementation details. From a high-quality corpus, we sample a subset to serve as input for the LLM labeler and judger.\nThrough iterative filtering, we train and quantize SLM tool models for each stage to improve inference efficiency,\nwhich are used to generate the reformulated corpus.",
                "position": 243
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04235/x3.png",
                "caption": "Figure 3:The first and last two figures illustrate the training dynamics of EntireSet and SubSet data recipes, respectively.\nBenchmark details are provided inAppendixÂ D.",
                "position": 884
            }
        ]
    },
    {
        "header": "5Ablations and Discussions",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04235/x4.png",
                "caption": "Figure 4:Benchmark results and validation losses in the MAGA-Mix setting. The sensitivity to data repetition varies across capability domains, with knowledge dimension showing greater resilience.",
                "position": 1008
            },
            {
                "img": "https://arxiv.org/html/2502.04235/extracted/6183569/figures/ablation-pe-tsne.png",
                "caption": "Figure 5:t-SNE visualization results. Base (left) maintains a distribution that overlaps with but extends beyond the original data.\nStrict (middle) clusters also extend original data, but indicating limited diversity compared to left. Relaxed (right) shows significant distributional shift, explaining its poor performance.",
                "position": 1019
            },
            {
                "img": "https://arxiv.org/html/2502.04235/x5.png",
                "caption": "Figure 6:validation losses of experiments in Section4.2.",
                "position": 1056
            },
            {
                "img": "https://arxiv.org/html/2502.04235/extracted/6183569/figures/ablation2_losses.png",
                "caption": "Figure 7:Losses pattern analysis. Subfigures 1 and 3 shows comparison between models trained on different data settings, withlâ¢oâ¢sâ¢srealð‘™ð‘œð‘ subscriptð‘ realloss_{\\text{real}}italic_l italic_o italic_s italic_s start_POSTSUBSCRIPT real end_POSTSUBSCRIPTon y-axis andlâ¢oâ¢sâ¢ssyntð‘™ð‘œð‘ subscriptð‘ syntloss_{\\text{synt}}italic_l italic_o italic_s italic_s start_POSTSUBSCRIPT synt end_POSTSUBSCRIPTon x-axis.\nSubfigures 2 and 4 track the position wherelâ¢oâ¢sâ¢ssyntiâˆ’lâ¢oâ¢sâ¢srealið‘™ð‘œð‘ subscriptsuperscriptð‘ ð‘–syntð‘™ð‘œð‘ subscriptsuperscriptð‘ ð‘–realloss^{i}_{\\text{synt}}-loss^{i}_{\\text{real}}italic_l italic_o italic_s italic_s start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT synt end_POSTSUBSCRIPT - italic_l italic_o italic_s italic_s start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT real end_POSTSUBSCRIPT(lâ¢oâ¢sâ¢sdiffið‘™ð‘œð‘ subscriptsuperscriptð‘ ð‘–diffloss^{i}_{\\text{diff}}italic_l italic_o italic_s italic_s start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT diff end_POSTSUBSCRIPT) first becomes significantly higher than the sequenceâ€™s average difference (detailed definition in AppendixD.3).",
                "position": 1071
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitations and Opportunities",
        "images": []
    },
    {
        "header": "Appendix BTool Model Implementation",
        "images": []
    },
    {
        "header": "Appendix CPretraining Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04235/x6.png",
                "caption": "Figure 8:Detail evaluation results of EntireSet described inTableÂ 4. MAGACorpus group demonstrats advantages over other groups across most evaluation sets, consistently across models of sizes.",
                "position": 1632
            },
            {
                "img": "https://arxiv.org/html/2502.04235/x7.png",
                "caption": "Figure 9:Detail evaluation results of Subset described inTableÂ 4. As the model size increases, the performance gap between the upsampling group and MAGACorpus gradually widens in ARC, DROP, GSM8K, RACE, but with some variations observed in TriviaQA and WinoGrande.",
                "position": 1635
            }
        ]
    },
    {
        "header": "Appendix DFurther Analysis of Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04235/x8.png",
                "caption": "Figure 10:Corresponding benchmark results described in Section5.1.",
                "position": 2035
            },
            {
                "img": "https://arxiv.org/html/2502.04235/extracted/6183569/figures/token-diff-examples.png",
                "caption": "Figure 11:Random examples sampling from wheremeanâ¢(lâ¢oâ¢sâ¢sdiffi)>0.5meanð‘™ð‘œð‘ subscriptsuperscriptð‘ ð‘–diff0.5\\text{mean}(loss^{i}_{\\text{diff}})>0.5mean ( italic_l italic_o italic_s italic_s start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT diff end_POSTSUBSCRIPT ) > 0.5, the synthetic-trained model fail to predict the tokens in later sequence positions.",
                "position": 2086
            },
            {
                "img": "https://arxiv.org/html/2502.04235/x9.png",
                "caption": "Figure 12:Corresponding cases sampled from Fineweb-Edu, which align with the loss patterns shown in Figure11, with higher loss by synthetic-trained model highlighted inred.",
                "position": 2097
            },
            {
                "img": "https://arxiv.org/html/2502.04235/x10.png",
                "caption": "Figure 13:Chinese corpus samples with higher loss by synthetic-trained model inred.",
                "position": 2100
            },
            {
                "img": "https://arxiv.org/html/2502.04235/x11.png",
                "caption": "Table 9:Example outputs of SLM variants.",
                "position": 2113
            }
        ]
    },
    {
        "header": "Appendix EPrompts and Cases",
        "images": []
    }
]