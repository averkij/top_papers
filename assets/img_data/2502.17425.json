[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17425/x1.png",
                "caption": "(a)Response process of an MLLM equipped with Visual Perception Tokens.",
                "position": 89
            },
            {
                "img": "https://arxiv.org/html/2502.17425/x2.png",
                "caption": "(b)Average Performance.",
                "position": 92
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Visual Perception Token",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17425/x3.png",
                "caption": "Figure 2:The relationship between the Region Selection Tokens we used and a precise bbox. Region Selection Token uses the cells containing the top-left and bottom-right corners to describe the approximate location of the region. In this example, the image is evenly divided into4×4444\\times 44 × 4cells. In our main experiment, we divide images into8×8888\\times 88 × 8cells.",
                "position": 187
            },
            {
                "img": "https://arxiv.org/html/2502.17425/x4.png",
                "caption": "Figure 3:In a standard MLLM generation process, the model directly outputs an response based on the input image and query. However, an MLLM equipped with Visual Perception Tokens can first generate special tokens that trigger additional perception processes before responding. If the MLLM outputs a Region Selection Token, the original image is cropped and reprocessed through the visual encoder. The MLLM then bases its answer on two sets of visual embeddings: the first set contains the global embeddings from the original image, and the second set contains the local embeddings from the cropped image. If the MLLM outputs a DINO Feature Token, the DINO features of the image are used to supplement the original CLIP-based features. Additionally, besides the DINO features, the hidden state of the DINO Feature Token is also input to the projector as a condition to control which features are ultimately passed to the language model.",
                "position": 276
            }
        ]
    },
    {
        "header": "4MLLM with Visual Perception Token",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17425/x5.png",
                "caption": "Figure 4:Examples collected from the testing sets. The responses were generated by the 7B model and the 2B+VPT model. During the generation process, if Region Selection Tokens were utilized, the region selected by these tokens are highlighted with red boxes in the images. For additional examples, please refer to the supplementary material.",
                "position": 616
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "S1Implement Details",
        "images": []
    },
    {
        "header": "S2Supplementary Experiments",
        "images": []
    },
    {
        "header": "S3Further Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17425/x6.png",
                "caption": "Figure S1:This set of images demonstrates how the DINO Feature Token assists MLLMs in identifying specific objects within images. These objects are often difficult for MLLMs to recognize directly due to their small size or interference from surrounding objects.",
                "position": 1132
            },
            {
                "img": "https://arxiv.org/html/2502.17425/x7.png",
                "caption": "",
                "position": 1141
            },
            {
                "img": "https://arxiv.org/html/2502.17425/x8.png",
                "caption": "Figure S2:This set of images illustrates how the DINO Feature Token assists MLLMs in counting the number of objects in an image. Counting has long been a significant limitation for MLLMs. By leveraging the DINO Feature, the DINO Feature Token enables precise localization of individual objects within the image, thereby improving the counting capability of MLLMs.",
                "position": 1150
            },
            {
                "img": "https://arxiv.org/html/2502.17425/x9.png",
                "caption": "",
                "position": 1159
            },
            {
                "img": "https://arxiv.org/html/2502.17425/x10.png",
                "caption": "",
                "position": 1165
            },
            {
                "img": "https://arxiv.org/html/2502.17425/x11.png",
                "caption": "",
                "position": 1170
            },
            {
                "img": "https://arxiv.org/html/2502.17425/x12.png",
                "caption": "Figure S3:This group of examples shows how the Region Selection Token aids MLLMs in understanding textual information within images by correctly identifying the corresponding regions. The image inputs primarily consist of large but structured documents, such as tables, forms, or letters.",
                "position": 1179
            },
            {
                "img": "https://arxiv.org/html/2502.17425/x13.png",
                "caption": "",
                "position": 1188
            },
            {
                "img": "https://arxiv.org/html/2502.17425/x14.png",
                "caption": "",
                "position": 1194
            },
            {
                "img": "https://arxiv.org/html/2502.17425/x15.png",
                "caption": "",
                "position": 1199
            },
            {
                "img": "https://arxiv.org/html/2502.17425/x16.png",
                "caption": "Figure S4:This set of images illustrates how the Region Selection Token enables MLLMs to comprehend textual information within real-world scenes by accurately identifying the corresponding regions. The image inputs consist of real-world scenarios, such as signboards and trademarks, where the text occupies only a small portion of the overall scene and is highly susceptible to interference from the surrounding context.",
                "position": 1208
            },
            {
                "img": "https://arxiv.org/html/2502.17425/x17.png",
                "caption": "",
                "position": 1217
            },
            {
                "img": "https://arxiv.org/html/2502.17425/x18.png",
                "caption": "",
                "position": 1223
            },
            {
                "img": "https://arxiv.org/html/2502.17425/x19.png",
                "caption": "",
                "position": 1228
            }
        ]
    },
    {
        "header": "S4Additional Related Works",
        "images": []
    },
    {
        "header": "S5Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]