[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.12576/fig/intro.png",
                "caption": "Figure 1:Comparison between verifier-free RL with question-only training and CoVRL. Unlike prior methods that sample reasoning traces conditioned only on the question, CoVRL couples question-conditioned prior sampling with answer-conditioned posterior sampling via a hybrid variational framework, enabling efficient exploration while preserving strong trace–answer coherence.",
                "position": 163
            }
        ]
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.12576/fig/main.png",
                "caption": "Figure 2:CoVRL employs hybrid sampling between priorpϕ​(z|x)p_{\\phi}(z|x)and posteriorqψ​(z|x,y)q_{\\psi}(z|x,y)to generate reasoning traces. It optimizes the reconstruction term using GRPO and NLL loss, with KL regularization applied to ensure training-inference coherence.",
                "position": 224
            }
        ]
    },
    {
        "header": "3Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.12576/x1.png",
                "caption": "(a)Reward Score",
                "position": 803
            },
            {
                "img": "https://arxiv.org/html/2512.12576/x1.png",
                "caption": "(a)Reward Score",
                "position": 806
            },
            {
                "img": "https://arxiv.org/html/2512.12576/x2.png",
                "caption": "(b)Response Length",
                "position": 811
            },
            {
                "img": "https://arxiv.org/html/2512.12576/x3.png",
                "caption": "(c)NLL Loss",
                "position": 816
            },
            {
                "img": "https://arxiv.org/html/2512.12576/x4.png",
                "caption": "(d)KL Loss",
                "position": 821
            },
            {
                "img": "https://arxiv.org/html/2512.12576/x5.png",
                "caption": "Figure 5:Impact of prior sampling probabilityα\\alphain hybrid sampling strategy.",
                "position": 872
            }
        ]
    },
    {
        "header": "4Related Works",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix AKL Divergence Estimator",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.12576/x6.png",
                "caption": "Figure 6:Comparison of KL divergence estimators under different sampling strategies.",
                "position": 1286
            }
        ]
    },
    {
        "header": "Appendix BComparison between Existing Approaches",
        "images": []
    },
    {
        "header": "Appendix CEvaluation Implementation",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.12576/x7.png",
                "caption": "(a)Prior Reward Score",
                "position": 1385
            },
            {
                "img": "https://arxiv.org/html/2512.12576/x7.png",
                "caption": "(a)Prior Reward Score",
                "position": 1388
            },
            {
                "img": "https://arxiv.org/html/2512.12576/x8.png",
                "caption": "(b)Posterior Reward Score",
                "position": 1393
            },
            {
                "img": "https://arxiv.org/html/2512.12576/x9.png",
                "caption": "(c)Response Length",
                "position": 1398
            }
        ]
    },
    {
        "header": "Appendix DTraining Dynamics of Different Sampling Ratios",
        "images": []
    },
    {
        "header": "Appendix EInfluence of Loss Components",
        "images": []
    }
]