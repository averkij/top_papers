[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09577/x1.png",
                "caption": "(a)Simulation ability v.s. performance",
                "position": 138
            },
            {
                "img": "https://arxiv.org/html/2510.09577/x1.png",
                "caption": "(a)Simulation ability v.s. performance",
                "position": 141
            },
            {
                "img": "https://arxiv.org/html/2510.09577/x2.png",
                "caption": "(b)Dyna-Mind",
                "position": 146
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Dyna-Mind",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09577/x3.png",
                "caption": "Figure 2:ReSimintegrates simulation into reasoning (atReSima_{t}^{\\mathrm{ReSim}}) by using expanded search trees built throughrealenvironment interactions (left).ReSimthen trains an agent to directly generate such simulation-guided reasoning traceatReSima_{t}^{\\mathrm{ReSim}}without any algorithm support (right).",
                "position": 228
            },
            {
                "img": "https://arxiv.org/html/2510.09577/x4.png",
                "caption": "Figure 3:Dyna-GRPOiterates between policy improvement (left) and world model improvement (right), optimized by GRPO.\nDuring policy improvement, we perform grouped policy rollouts with GRPO.\nDuring simulation improvement, we perform both policy rollouts and simulation refinement rollouts (seeFigureËœ4), and trains the model to directlygenerate an improved policyas well as tobetter perform simulation refinementwhen provided with future-states information.",
                "position": 295
            },
            {
                "img": "https://arxiv.org/html/2510.09577/x5.png",
                "caption": "Figure 4:SimRolloutgenerates refined action per statests_{t}using real environment interactions",
                "position": 300
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALLM Usage",
        "images": []
    },
    {
        "header": "Appendix BEthics Statement",
        "images": []
    },
    {
        "header": "Appendix CAdditional Algorithmic Details",
        "images": []
    },
    {
        "header": "Appendix DAdditional Details on Text Games",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09577/x6.png",
                "caption": "Figure A1:Example task, input screenshot, and output generated by model trained using Dyna-Mind. For clarity, we directly rendered the proposed action inat+1a_{t+1}(click at 1023,387) in green onsts_{t}.",
                "position": 2093
            }
        ]
    },
    {
        "header": "Appendix EAdditional Details on AndroidWorld",
        "images": []
    }
]