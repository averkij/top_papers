[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.03895/x1.png",
                "caption": "Figure 1:LLaVA-Mini achieves comparable performance to LLaVA-v1.5 using only 1 vision token instead of 576, yielding efficient computation, lower latency, and reduced VRAM usage.",
                "position": 118
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3How Does LLaVA Understand Vision Tokens?",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.03895/x2.png",
                "caption": "(a)LLaVA-v1.5-Vicuna-7B",
                "position": 185
            },
            {
                "img": "https://arxiv.org/html/2501.03895/x2.png",
                "caption": "(a)LLaVA-v1.5-Vicuna-7B",
                "position": 188
            },
            {
                "img": "https://arxiv.org/html/2501.03895/x3.png",
                "caption": "(b)LLaVA-v1.5-Vicuna-13B",
                "position": 193
            },
            {
                "img": "https://arxiv.org/html/2501.03895/x4.png",
                "caption": "(c)LLaVA-v1.6-Mistral-7B",
                "position": 198
            },
            {
                "img": "https://arxiv.org/html/2501.03895/x5.png",
                "caption": "(d)LLaVA-NeXT-Vicuna-7B",
                "position": 203
            },
            {
                "img": "https://arxiv.org/html/2501.03895/x6.png",
                "caption": "(a)LLaVA-v1.5-Vicuna-7B",
                "position": 213
            },
            {
                "img": "https://arxiv.org/html/2501.03895/x6.png",
                "caption": "(a)LLaVA-v1.5-Vicuna-7B",
                "position": 216
            },
            {
                "img": "https://arxiv.org/html/2501.03895/x7.png",
                "caption": "(b)LLaVA-v1.5-Vicuna-13B",
                "position": 221
            },
            {
                "img": "https://arxiv.org/html/2501.03895/x8.png",
                "caption": "(c)LLaVA-v1.6-Mistral-7B",
                "position": 226
            },
            {
                "img": "https://arxiv.org/html/2501.03895/x9.png",
                "caption": "(d)LLaVA-NeXT-Vicuna-7B",
                "position": 231
            },
            {
                "img": "https://arxiv.org/html/2501.03895/x10.png",
                "caption": "Figure 4:Attention visualization at different layers in LLaVA-v1.5(color bar: logarithmic scale).",
                "position": 241
            },
            {
                "img": "https://arxiv.org/html/2501.03895/x11.png",
                "caption": "Figure 5:Performance of LLaVA-v1.5 when removing all vision tokens in various layers of LMM.",
                "position": 244
            }
        ]
    },
    {
        "header": "4LLaVA-Mini",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.03895/x12.png",
                "caption": "Figure 6:Architecture of LLaVA-Mini.Left: LLaVA-Mini represents each image with one vision token.Right: Detailed view of the proposed query-based compression and modality pre-fusion.",
                "position": 261
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.03895/x13.png",
                "caption": "Figure 7:FLOPs and latency of LLaVA-Mini.",
                "position": 4073
            },
            {
                "img": "https://arxiv.org/html/2501.03895/x13.png",
                "caption": "Figure 7:FLOPs and latency of LLaVA-Mini.",
                "position": 4076
            },
            {
                "img": "https://arxiv.org/html/2501.03895/x14.png",
                "caption": "Figure 8:FLOPs and latency of LLaVA-Mini-HD.",
                "position": 4081
            },
            {
                "img": "https://arxiv.org/html/2501.03895/x15.png",
                "caption": "Figure 9:VRAM usage (3-hour video) of LLaVA-Mini.",
                "position": 4086
            }
        ]
    },
    {
        "header": "6Analyses",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.03895/x16.png",
                "caption": "Figure 10:Case of image understanding.",
                "position": 4956
            },
            {
                "img": "https://arxiv.org/html/2501.03895/x16.png",
                "caption": "Figure 10:Case of image understanding.",
                "position": 4959
            },
            {
                "img": "https://arxiv.org/html/2501.03895/x17.png",
                "caption": "Figure 11:Case of video understanding.",
                "position": 4964
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetailed Setting of Preliminary Analyses",
        "images": []
    },
    {
        "header": "Appendix BTraining Details",
        "images": []
    },
    {
        "header": "Appendix CBenchmarks",
        "images": []
    },
    {
        "header": "Appendix DIntroduction to Baselines",
        "images": []
    },
    {
        "header": "Appendix EExtended Experimental Results",
        "images": []
    },
    {
        "header": "Appendix FVisualization of Compression",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.03895/x18.png",
                "caption": "Figure 12:Visualization of the cross-attention in the compression module introduced in LLaVA-Mini. The left side is the original image, and the right side is the cross-attention distribution heat map, where brighter areas are more heavily weighted during compression. The example images are all from the LLaVA-Bench-in-the-Wild benchmark.",
                "position": 6438
            }
        ]
    },
    {
        "header": "Appendix GMore Cases",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.03895/x19.png",
                "caption": "Figure 13:Example demonstrating LLaVA-Mini’s image understanding capability on more complex image reasoning. Output marked in red indicates factual errors.",
                "position": 6451
            },
            {
                "img": "https://arxiv.org/html/2501.03895/x20.png",
                "caption": "Figure 14:Example demonstrating LLaVA-Mini’s image understanding capability on more challenging OCR task. Output marked in red indicates recognition errors.",
                "position": 6454
            },
            {
                "img": "https://arxiv.org/html/2501.03895/x21.png",
                "caption": "Figure 15:Example demonstrating LLaVA-Mini’s video understanding capability on first-person view video. Output marked in red indicates factual errors.",
                "position": 6457
            }
        ]
    },
    {
        "header": "Appendix HDetailed Results on MVBench",
        "images": []
    }
]