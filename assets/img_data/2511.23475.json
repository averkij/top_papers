[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.23475/x1.png",
                "caption": "",
                "position": 82
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.23475/x2.png",
                "caption": "Figure 2:(a) The architecture of AnyTalker, which incorporates a novel multi-stream audio processing layer, Audio-Face Cross Attention, enables the handling of multiple facial and audio inputs.\n(b) The training of AnyTalker is divided into two stages: the first stage uses concatenated multi-person data derived from single-person data mixed with single-person data to learn accurate lip movements; the second stage employs authentic multi-person data to enhance the interactivity in generated videos.\n(c) The detailed implementation of Audio-Face Cross Attention, a recursively callable structure that applies masking to the output using face masks.",
                "position": 132
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.23475/x3.png",
                "caption": "Figure 3:(a) Mapping of video tokens to audio tokens, facilitated by a custom attention mask.\nEvery 4 audio tokens are bound to 1 video token, except for the first.\n(b) Mask token used for output masking in Audio-Face Cross Attention.",
                "position": 178
            },
            {
                "img": "https://arxiv.org/html/2511.23475/x4.png",
                "caption": "Figure 4:Two video clips from InteractiveEyes withM​o​t​i​o​n{Motion}score (px): left shows original video, right shows cropped face and eye landmarks. Head turn toward the speaker or eyebrow raise will increaseM​o​t​i​o​nMotionand Interactivity; sustained stillness keeps both low.",
                "position": 285
            }
        ]
    },
    {
        "header": "4Interactivity Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.23475/x5.png",
                "caption": "Figure 5:Listening and speaking periods of each speaker.",
                "position": 345
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.23475/x6.png",
                "caption": "Figure 6:Qualitative comparison of multiple multi-person driving methods.\nWith the same text prompt, reference images, and multiple audio streams as input, we compare the generation results of Bind-Your-Avatar, MultiTalk, and AnyTalker.\nThe left case uses the input image from the InteractiveEyes dataset, while the right case uses the image produced by a text-to-image generative model[kolors].",
                "position": 678
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Outline",
        "images": []
    },
    {
        "header": "A. Experimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.23475/x7.png",
                "caption": "Figure 7:SyncNet Score Matrix in two-person data.",
                "position": 936
            },
            {
                "img": "https://arxiv.org/html/2511.23475/x8.png",
                "caption": "Figure 8:Properties of training data.",
                "position": 953
            },
            {
                "img": "https://arxiv.org/html/2511.23475/x9.png",
                "caption": "Figure 9:Two cases from InteractiveEyes.\nBoth of the speakers have speaking periods.",
                "position": 956
            },
            {
                "img": "https://arxiv.org/html/2511.23475/x10.png",
                "caption": "Figure 10:Input cases from the benchmark dataset, from left to right: HDTF[zhang2021flow], VFHQ[xie2022vfhq], and EMTD[meng2025echomimicv2].",
                "position": 962
            },
            {
                "img": "https://arxiv.org/html/2511.23475/x11.png",
                "caption": "Figure 11:Four typical cases that will get a high Interactivity score.",
                "position": 1026
            },
            {
                "img": "https://arxiv.org/html/2511.23475/x12.png",
                "caption": "Figure 12:A bad case generated by Bind-Your-Avatar[huang2025bind].",
                "position": 1114
            },
            {
                "img": "https://arxiv.org/html/2511.23475/x13.png",
                "caption": "Figure 13:Qualitative results on HDTF[zhang2021flow](left) and VFHQ[xie2022vfhq](right) benchmark.\nThe positions of pronunciation have been highlighted inredandunderlined.",
                "position": 1119
            },
            {
                "img": "https://arxiv.org/html/2511.23475/x14.png",
                "caption": "Figure 14:More Results generated by AnyTalker.",
                "position": 1154
            },
            {
                "img": "https://arxiv.org/html/2511.23475/x15.png",
                "caption": "Figure 15:Improvement in interaction among each identity after fine-tuning on authentic multi-person data (right).",
                "position": 1159
            }
        ]
    },
    {
        "header": "B. Extended Experiments",
        "images": []
    }
]