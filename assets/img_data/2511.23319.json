[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.23319/x1.png",
                "caption": "Figure 1:Despite being pre-trained with an 8K context window and mid-trained up to 32K, HSA-UltraLong achieves near-perfect accuracy on S-NIAH even at a 16M-token context length. The red dashed line at 32K marks the boundary between in-domain (left) and out-of-domain (right).",
                "position": 76
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.23319/x2.png",
                "caption": "Figure 2:Hierarchical Sparse Attention (HSA) operates in a manner analogous to Mixture of Experts (MoE). First, the current tokenxtx_{t}computes dot products with the landmark representations of past chunks as retrieval scores, from which the top-kkchunks are selected—similar to how MoE uses a router to select top-kkexperts. Subsequently,xtx_{t}performs attention with each of thekkretrieved chunksseparately, mirroring the process in MoE wherextx_{t}independently conducts Feedforward withkkexperts. Finally, the attention outputs from each chunk are weighted by the softmax-normalized retrieval scores and summed, which is functionally equivalent to MoE’s fusion of outputs from the selected FFNs.",
                "position": 120
            }
        ]
    },
    {
        "header": "2Preliminary",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.23319/figs/model_arch.png",
                "caption": "Figure 3:HSA-UltraLong model architecture.",
                "position": 336
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.23319/x3.png",
                "caption": "(a)Before long-context mid-training.",
                "position": 1046
            },
            {
                "img": "https://arxiv.org/html/2511.23319/x3.png",
                "caption": "(a)Before long-context mid-training.",
                "position": 1049
            },
            {
                "img": "https://arxiv.org/html/2511.23319/x4.png",
                "caption": "(b)After long-context mid-training.",
                "position": 1054
            },
            {
                "img": "https://arxiv.org/html/2511.23319/x5.png",
                "caption": "(c)Multi-Query NIAH Task.",
                "position": 1060
            },
            {
                "img": "https://arxiv.org/html/2511.23319/x6.png",
                "caption": "(d)Variable Tracking Task.",
                "position": 1065
            },
            {
                "img": "https://arxiv.org/html/2511.23319/x7.png",
                "caption": "(a)Training efficiency.",
                "position": 1089
            },
            {
                "img": "https://arxiv.org/html/2511.23319/x7.png",
                "caption": "(a)Training efficiency.",
                "position": 1092
            },
            {
                "img": "https://arxiv.org/html/2511.23319/x8.png",
                "caption": "(b)Inference efficiency.",
                "position": 1097
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    }
]