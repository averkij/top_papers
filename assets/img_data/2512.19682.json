[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19682/x1.png",
                "caption": "Figure 1:GenEnv’s cross-benchmark gains and data efficiency.(a) We compareGenEnv(7B) against representative baselines (Qwen2.5-7B, ReSearch, SearchR1, ToRL) and larger open models (e.g., Qwen3-14B, GPT-OSS-20B).\nBlue callouts report theabsoluteimprovement ofGenEnvover Qwen2.5-7B on each benchmark.\n(b) Validation data-efficiency comparison on BFCL:GenEnvsurpasses RandomEnv and Static Augmentation, and outperforms Gemini-based offline augmentation even with 3.3×\\timesmore synthetic data.\nTogether, the figure shows thatdifficulty-aligned adaptive simulationcan outperform stronger static augmentation baselines under comparable training settings.",
                "position": 145
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19682/x2.png",
                "caption": "Figure 2:A comparison between the traditional training paradigm and our proposedGenEnvframework. The traditional approach (top) relies on high-cost interaction with the real world to create a static dataset, leading to inefficient training and poor generalization.GenEnv(bottom) creates a co-evolutionary loop where an Environment LLM generates adaptive tasks for the Agent LLM, enabling low-cost simulation, an adaptive curriculum, and improved efficiency.",
                "position": 163
            }
        ]
    },
    {
        "header": "2GenEnv: Difficulty-Aligned Co-Evolution",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19682/x3.png",
                "caption": "Figure 3:TheGenEnvCo-Evolutionary Loop.(1) The Environment Policy generates tasks. (2) The Agent Policy attempts them. (3) The environment reward (difficulty alignment) updates the simulator, while the agent reward (task success) updates the agent.",
                "position": 226
            }
        ]
    },
    {
        "header": "3Theoretical Analysis of Difficulty-AlignedGenEnv",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19682/x4.png",
                "caption": "Figure 4:Training dynamics ofGenEnv.From left to right: (a) training step-wise reward (critic/score/mean); (b) validation score across epochs; (c) batch-level ground-truth accuracy; and (d) per-epoch average reward. The curves show thatGenEnvtrains stably without reward collapse or divergence, with both reward and accuracy improving smoothly over time.",
                "position": 830
            },
            {
                "img": "https://arxiv.org/html/2512.19682/x5.png",
                "caption": "Figure 5:Emergent curriculum inGenEnv.Across training epochs, the environment simulator gradually increases task complexity (a), reflected by longer task descriptions;\nthe agent correspondingly produces longer reasoning chains (b) as it learns to solve harder tasks;\nand its success rate (c) remains within a controlled band despite rising difficulty.\nTogether these curves show thatGenEnvinduces an emergent curriculum in which task difficulty and agent capability co-evolve in a stable manner.",
                "position": 833
            },
            {
                "img": "https://arxiv.org/html/2512.19682/x6.png",
                "caption": "(a)Method comparison.GenEnvoutperforms both static Gemini-based augmentation and the GenEnv-Random variant, showing thathowdata is generated and aligned with the agent matters more than simply adding more offline synthetic data.",
                "position": 892
            },
            {
                "img": "https://arxiv.org/html/2512.19682/x6.png",
                "caption": "(a)Method comparison.GenEnvoutperforms both static Gemini-based augmentation and the GenEnv-Random variant, showing thathowdata is generated and aligned with the agent matters more than simply adding more offline synthetic data.",
                "position": 895
            },
            {
                "img": "https://arxiv.org/html/2512.19682/x7.png",
                "caption": "(b)Data efficiency.GenEnvachieves higher validation performance while using substantially fewer synthetic samples than Gemini-based offline augmentation, indicating that difficulty-aligned, on-policy simulation provides more learning signal per example than untargeted teacher-generated data.",
                "position": 900
            },
            {
                "img": "https://arxiv.org/html/2512.19682/x8.png",
                "caption": "Figure 7:Difficulty calibration via theα\\alpha-Curriculum Reward.As training progresses, the agent’s success rate on simulator-generated tasks converges to the target difficulty band\n(centered atα=0.5\\alpha=0.5), demonstrating that the environment policy reliably adapts task difficulty to match the agent’s current capability.\nThis empirically verifies the theoretical ranking consistency ofRenvR_{\\text{env}}and shows that the simulator self-calibrates to maintain tasks in the zone of proximal development.",
                "position": 923
            },
            {
                "img": "https://arxiv.org/html/2512.19682/x9.png",
                "caption": "Figure 8:Problem-solving behavior during training.(a)GenEnvconsistently increases the proportion of fully solved tasks per batch, surpassing the RandomEnv variant;\n(b) the rate of unsolved tasks decreases substantially faster underGenEnv.\nThese trends show that difficulty-aligned simulation not only improves average performance but also accelerates the elimination of failure modes compared to unguided task generation.",
                "position": 929
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]