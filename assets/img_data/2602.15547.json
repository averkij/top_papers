[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Model Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.15547/img/architecture.png",
                "caption": "Figure 1:Architecture ofjina-embeddings-v5-text.",
                "position": 228
            }
        ]
    },
    {
        "header": "4Training",
        "images": []
    },
    {
        "header": "5Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.15547/img/v5-small_language_heatmap.png",
                "caption": "Figure 2:Performance ofj-v5-text-smallon different languages on MMTEB compared to other models",
                "position": 994
            },
            {
                "img": "https://arxiv.org/html/2602.15547/x1.png",
                "caption": "Figure 3:Performance comparison of different training objectives. Average nDCG@10 on the MTEB (English, v2) benchmark forS2ORC(left) and the full training data mixture (right).",
                "position": 1490
            },
            {
                "img": "https://arxiv.org/html/2602.15547/img/s2orc_projection1.png",
                "caption": "Figure 4:Comparison of projection configurations onS2ORC. Performance is measured by average nDCG@10 on MTEB (English, v2).",
                "position": 1504
            },
            {
                "img": "https://arxiv.org/html/2602.15547/x2.png",
                "caption": "Figure 5:Average MMTEB score across reduced embedding dimensions.",
                "position": 1617
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.15547/img/lr_ablation_plot.png",
                "caption": "Figure 6:Learning rate sensitivity across different optimization objectives.We report the average nDCG@10 on the MTEB (English, v2) benchmark using theS2ORCdataset. The plots compare1×10−41\\times 10^{-4}(blue) and1×10−51\\times 10^{-5}(orange) learning rates for embedding-based distillation (ℒdistill\\mathcal{L}_{\\text{distill}}), InfoNCE (ℒNCEq→d\\mathcal{L}_{\\text{NCE}}^{q\\rightarrow d}), and score-based distillation (ℒscore\\mathcal{L}_{\\text{score}}), all utilizing a trainable student projection.",
                "position": 6842
            },
            {
                "img": "https://arxiv.org/html/2602.15547/img/combined_language_heatmap.png",
                "caption": "Figure 7:Performance of Models on different languages on MMTEB compared to average performance",
                "position": 6878
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]