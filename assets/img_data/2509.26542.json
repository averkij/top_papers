[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26542/x1.png",
                "caption": "Figure 1:Latency–accuracy frontier onVERA.Markers show model performance (black circles: text, blue triangles: voice, purple square: LiveAnswer cascade) with x-axis as first response time (log scale) and y-axis as accuracy. The green Pareto frontier reveals areal-time reasoning desert: models achieving≤1.5\\leq\\!1.5s response time (shaded band) plateau around 10% accuracy, while approaching the text upper bound (∼\\sim54%, dashed line) requires sacrificing real-time interaction.",
                "position": 125
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3The VERA Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26542/x2.png",
                "caption": "Figure 2:VERAat a glance.Five representative panels (Math, Web, Science, Long-Context, Factual) show how items are rewritten for voice while preserving reasoning difficulty.",
                "position": 315
            },
            {
                "img": "https://arxiv.org/html/2509.26542/x3.png",
                "caption": "Figure 3:Benchmark Construction Pipeline.From brainstorming to final audio generation through systematic filtering and quality control.",
                "position": 343
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26542/x4.png",
                "caption": "(a)GPT: GPT-5 text (high/low effort) vs. GPT-realtime voice.",
                "position": 736
            },
            {
                "img": "https://arxiv.org/html/2509.26542/x4.png",
                "caption": "(a)GPT: GPT-5 text (high/low effort) vs. GPT-realtime voice.",
                "position": 739
            },
            {
                "img": "https://arxiv.org/html/2509.26542/x5.png",
                "caption": "(b)Gemini: Text Pro/Flash vs. Flash native-audio voice.",
                "position": 744
            },
            {
                "img": "https://arxiv.org/html/2509.26542/x6.png",
                "caption": "(c)Qwen-family voice models across tracks.",
                "position": 749
            },
            {
                "img": "https://arxiv.org/html/2509.26542/x7.png",
                "caption": "Figure 5:LiveAnswer cascade latency.Stacked bars show STT (hatched) and LLM+TTS stages. Diamond marks user-perceived time to first audio. Mean latencies:TSTTT_{\\text{STT}}=9.68s for speech recognition,TTTFRpartialT_{\\text{TTFR}_{\\text{partial}}}=0.83s from STT completion to first audio output,TLLM+TTST_{\\text{LLM+TTS}}=63.40s for complete reasoning and synthesis. Total end-to-end:TSTTT_{\\text{STT}}+TTTFRpartialT_{\\text{TTFR}_{\\text{partial}}}+ remaining generation.",
                "position": 769
            },
            {
                "img": "https://arxiv.org/html/2509.26542/x8.png",
                "caption": "Figure 6:Failure-mode landscape.Heatmap shows deviationΔm​(c)=p​(c∣m)−p​(c)\\Delta_{m}(c)=p(c\\!\\mid\\!m)-p(c)from global baseline for each modelmmand error categorycc. Cool colors indicate over-production of errors relative to benchmark average; warm colors indicate under-production. Reveals not justhow oftenbuthowmodels fail.",
                "position": 798
            }
        ]
    },
    {
        "header": "6Future Directions",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APrevious Benchmarks",
        "images": []
    },
    {
        "header": "Appendix BASR Transcript Normalization",
        "images": []
    },
    {
        "header": "Appendix CHuman Evaluation and Judge Validation",
        "images": []
    },
    {
        "header": "Appendix DModel Implementation Details",
        "images": []
    },
    {
        "header": "Appendix EStatistical Validation",
        "images": []
    },
    {
        "header": "Appendix FPrompts",
        "images": []
    },
    {
        "header": "Appendix GDataset Selection Criteria",
        "images": []
    }
]