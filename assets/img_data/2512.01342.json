[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01342/imgs/figure1_polished.png",
                "caption": "Figure 1:Comparison with previous SOTA methods of size ViT-Large.With only public sources, our model excels at general video benchmarks within a probing setting where ViTs are frozen to directly show representationâ€™s quality. The benchmarks involve scene-related, motion-related, complex video-language related and implicit world knowledge (3D geometric prior, causal relations and fine-grained object motion) related tasks.",
                "position": 143
            },
            {
                "img": "https://arxiv.org/html/2512.01342/x1.png",
                "caption": "Figure 2:Our two-stage self-supervised video pretraining framework.For general video understanding and building real world-understanding video foundation models, we propose InternVideo-Next, which is simple, scalable, efficient and reproducible.",
                "position": 148
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01342/x2.png",
                "caption": "Figure 3:Video Depth estimation visualization.All models are trained with a frozen setting with only a probing head (VDA head from Video Depth Anything[videodepthanything]) module trainable. Graphs are from the first video of the ScanNet Dataset. Our model shows potential for building next-generation 3D-spatial intelligence models from video sources. Videos are resized with a spatial resolution of 224.",
                "position": 951
            }
        ]
    },
    {
        "header": "5Conclusion and Future Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01342/x3.png",
                "caption": "Figure 4:Explanations of different probing settings in our paper.",
                "position": 1571
            }
        ]
    },
    {
        "header": "Appendix ATraining GPU utilization",
        "images": []
    },
    {
        "header": "Appendix BMore implementation details",
        "images": []
    }
]