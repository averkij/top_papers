[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.18633/x1.png",
                "caption": "Figure 1:Video object removal results generated byROSE(zoom in for better view). Every two lines are an example where the above is input video with mask and the bottom is inference result. We sequentially show cases of various side effects studied in this paper.",
                "position": 109
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Dataset Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.18633/x2.png",
                "caption": "Figure 2:Paired video preparation pipeline using 3D data, which can be divided into: scene and object sampling, multi-view generation with masks, valid view filtering and video data rendering.",
                "position": 184
            },
            {
                "img": "https://arxiv.org/html/2508.18633/x3.png",
                "caption": "Figure 3:Illustration of the variousside-effect categoriesstudied in the dataset ofROSE.",
                "position": 219
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.18633/x4.png",
                "caption": "Figure 4:The framework ofROSE. We concatenate the noisy latents with the original input video and masks, consumed by a video inpainting model. An additional difference mask predictor is introduced to predict the correlated area in video, automatically computed from the input video pairs.",
                "position": 262
            },
            {
                "img": "https://arxiv.org/html/2508.18633/x5.png",
                "caption": "Figure 5:Visualization of various mask augmentation strategies adopted in training.",
                "position": 294
            },
            {
                "img": "https://arxiv.org/html/2508.18633/x5.png",
                "caption": "Figure 5:Visualization of various mask augmentation strategies adopted in training.",
                "position": 297
            },
            {
                "img": "https://arxiv.org/html/2508.18633/x6.png",
                "caption": "Figure 6:Comparison between the previous paradigm and our reference-based paradigm.",
                "position": 302
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.18633/x7.png",
                "caption": "Figure 7:Qualitative comparison between our method and existing approaches on real-world samples. Our model demonstrates superior ability and effectively handles complex object-environment interactions, including shadows, reflections, and illumination changes.",
                "position": 374
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]