[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03680/figures/overview.png",
                "caption": "Figure 1:Overview ofAgent Lightning, a flexible and extensible framework that enables reinforcement learning of LLMs for ANY AI agents.",
                "position": 187
            }
        ]
    },
    {
        "header": "2Modern AI Agents",
        "images": []
    },
    {
        "header": "3Agent Lightning",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03680/figures/data_interface.png",
                "caption": "Figure 2:Illustration of the unified data interface inAgent Lightning. The left panel depicts the agent execution flow, where each state transition is represented by the update of semantic variables (green rectangles denote variables with valid values; gray rectangles indicate variables not yet assigned in the current state). The right panel presents the corresponding trajectory collected throughout the agentâ€™s execution, demonstrating how the unified data interface systematically captures all relevant transitions for RL-based optimization.",
                "position": 386
            },
            {
                "img": "https://arxiv.org/html/2508.03680/figures/algorithm.png",
                "caption": "Figure 3:Illustration of the LightningRL algorithm.(a)Single-call GRPO, where the LLM generates a response to a task in one pass. Outputs for the same task are grouped together for advantage estimation.(b)Previous multi-turn GRPO. Each trajectory contains multiple LLM calls, with trajectories for the same task grouped for advantage estimation. Tokens not generated by the LLM are masked (gray dashed boxes) during optimization.(c)Our proposed LightningRL. Trajectories are decomposed into transitions, and transitions for the same task are grouped for advantage estimation. Each transition includes the current input/context, output, and reward. The input is part of the current agent state, with rewards computed by the credit assignment module.",
                "position": 560
            },
            {
                "img": "https://arxiv.org/html/2508.03680/figures/agent_lightning_architecture_v3.png",
                "caption": "Figure 4:Training-Agent Disaggregation architecture.",
                "position": 590
            }
        ]
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03680/figures/train_spider.png",
                "caption": "(a)Train reward",
                "position": 718
            },
            {
                "img": "https://arxiv.org/html/2508.03680/figures/train_spider.png",
                "caption": "(a)Train reward",
                "position": 721
            },
            {
                "img": "https://arxiv.org/html/2508.03680/figures/test_spider.png",
                "caption": "(b)Test reward",
                "position": 726
            },
            {
                "img": "https://arxiv.org/html/2508.03680/figures/train_rag_llama.png",
                "caption": "(a)Train reward",
                "position": 749
            },
            {
                "img": "https://arxiv.org/html/2508.03680/figures/train_rag_llama.png",
                "caption": "(a)Train reward",
                "position": 752
            },
            {
                "img": "https://arxiv.org/html/2508.03680/figures/test_rag_llama.png",
                "caption": "(b)Test reward",
                "position": 757
            },
            {
                "img": "https://arxiv.org/html/2508.03680/figures/train_calc.png",
                "caption": "(a)Train reward",
                "position": 780
            },
            {
                "img": "https://arxiv.org/html/2508.03680/figures/train_calc.png",
                "caption": "(a)Train reward",
                "position": 783
            },
            {
                "img": "https://arxiv.org/html/2508.03680/figures/test_calc.png",
                "caption": "(b)Test reward",
                "position": 788
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExample to Optimize an Existing Agent with Agent Lightning",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03680/figures/agent_lightning_process.png",
                "caption": "Figure 10:Process diagram",
                "position": 1726
            }
        ]
    },
    {
        "header": "Appendix BProcess Diagram of Agent Lightning",
        "images": []
    }
]