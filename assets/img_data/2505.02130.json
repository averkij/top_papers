[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02130/x1.png",
                "caption": "Figure 1:Attention distribution of different types of tokens before and after training. With Amazon-Ratings on the left, Roman-Empire in the middle, and Wikics on the right. The attention values have undergone log scaling and are plotted as a density distribution Figure.",
                "position": 168
            }
        ]
    },
    {
        "header": "3Empirical Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02130/x2.png",
                "caption": "Figure 2:The attention scores from neighboring nodes to the central node, both before and after training, were presented as mean values with standard deviations, using a 1:8 sampling ratio.",
                "position": 295
            },
            {
                "img": "https://arxiv.org/html/2505.02130/x3.png",
                "caption": "Figure 3:Illustration of all tokens to nodes attention. The x-coordinate refers to the relative position of the node token in the entire node list. We collected the attention scores of all tokens towards node tokens and plotted them in a line graph according to the relative position of nodes within the instructions.Upper:the mean values of attention scores from nodes(Querys) to nodes(Keys).Lower:the mean values of attention scores from texts(Querys) to nodes(Keys). The plot annotates nodes at different hierarchical levels. Our node sampling is 8*8.",
                "position": 322
            },
            {
                "img": "https://arxiv.org/html/2505.02130/x4.png",
                "caption": "Figure 4:Padding and Random Shuffling to ensure fixed sequence length and position. This minimizes interference with attention scores for other reasons.",
                "position": 327
            },
            {
                "img": "https://arxiv.org/html/2505.02130/x5.png",
                "caption": "Figure 5:Illustration of attention score interaction matrix(Nodes).Left:The left panel displays the attention interaction matrix between all nodes (1 central node + 8 first-order nodes + 8*8 second-order nodes), averaged across all heads and layers.Right:The right panel shows the attention interaction matrices between first-order and second-order nodes, with four selected groups highlighted for detailed analysis. We highlight the ‚Äúattention sink‚Äù with a gray box and the ‚ÄúSkewed Line Sink‚Äù with a black box.",
                "position": 347
            },
            {
                "img": "https://arxiv.org/html/2505.02130/x6.png",
                "caption": "Figure 6:Illustration of different global linkage horizon k. From left to right, the images represent k=1 to 4. To demonstrate the field of view of GNNs, the image for k=1 is depicted bidirectionally, while the rest are shown unidirectionally.",
                "position": 375
            }
        ]
    },
    {
        "header": "4Findings and Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "5Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BDatasets",
        "images": []
    },
    {
        "header": "Appendix CAdditional Related Work",
        "images": []
    },
    {
        "header": "Appendix DOther Base Models for Disruption Performance",
        "images": []
    },
    {
        "header": "Appendix EStatistical Analysis of Attention Score Distributions",
        "images": []
    },
    {
        "header": "Appendix FDisruption Attention Score",
        "images": []
    },
    {
        "header": "Appendix GAttention Score Matrix(Tokens)",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02130/x7.png",
                "caption": "Figure 7:Attention score interaction matrix(Nodes) in Wikics.",
                "position": 1751
            },
            {
                "img": "https://arxiv.org/html/2505.02130/x8.png",
                "caption": "Figure 8:Attention score interaction matrix(Nodes) in Roman-Empire.",
                "position": 1754
            },
            {
                "img": "https://arxiv.org/html/2505.02130/x9.png",
                "caption": "Figure 9:Attention score interaction matrix(Nodes) in Amazon-Ratings.",
                "position": 1757
            },
            {
                "img": "https://arxiv.org/html/2505.02130/x10.png",
                "caption": "Figure 10:Attention score interaction matrix(Text) in Wikics.",
                "position": 1765
            },
            {
                "img": "https://arxiv.org/html/2505.02130/x11.png",
                "caption": "Figure 11:Attention score interaction matrix(Text) in Roman-Empire.",
                "position": 1768
            },
            {
                "img": "https://arxiv.org/html/2505.02130/x12.png",
                "caption": "Figure 12:Attention score interaction matrix(Text) in Amazon-Ratings.",
                "position": 1771
            }
        ]
    },
    {
        "header": "Appendix HAttention Score among First Nodes(with child nodes)",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02130/x13.png",
                "caption": "Figure 13:Visualization of the average attention(Attention Score among First Nodes(with child nodes)) in Amazon-Ratings((1+8)*2).",
                "position": 1780
            },
            {
                "img": "https://arxiv.org/html/2505.02130/x14.png",
                "caption": "Figure 14:Visualization of the average attention(Attention Score among First Nodes(with child nodes)) in Roman-Empire((1+8)*2).",
                "position": 1784
            },
            {
                "img": "https://arxiv.org/html/2505.02130/x15.png",
                "caption": "Figure 15:Visualization of the average attention(Attention Score among First Nodes(with child nodes)) in Wikics((1+8)*2).",
                "position": 1788
            }
        ]
    },
    {
        "header": "Appendix IDifferent Layer Attention Score between First Nodes and child nodes",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02130/x16.png",
                "caption": "Figure 16:Visualization of the average attention(Center nodes and First-order nodes) in Amazon-Ratings.",
                "position": 1797
            },
            {
                "img": "https://arxiv.org/html/2505.02130/x17.png",
                "caption": "Figure 17:Visualization of the average attention(Center nodes and First-order nodes) in Roman-Empire.",
                "position": 1801
            },
            {
                "img": "https://arxiv.org/html/2505.02130/x18.png",
                "caption": "Figure 18:Visualization of the average attention(Center nodes and First-order nodes) in Wikics.",
                "position": 1805
            }
        ]
    },
    {
        "header": "Appendix JPrompt",
        "images": []
    },
    {
        "header": "Appendix KDifferent Templates",
        "images": []
    },
    {
        "header": "Appendix LTransfer Performance of Models across Differentkùëòkitalic_kValues.",
        "images": []
    }
]