[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "Foreword",
        "images": []
    },
    {
        "header": "Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12403/x1.png",
                "caption": "Figure 1:lerobotis the open-source library for end-to-end robotics developed by Hugging Face. The library is vertically integrated on the entire robotics stack, supporting low-level control of real-world robot devices, advanced data and inference optimizations, as well as SOTA robot learning methods with simple implementations in pure Pytorch.",
                "position": 188
            }
        ]
    },
    {
        "header": "Classical Robotics",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12403/x2.png",
                "caption": "Figure 2:Overview of methods to generate motion (clearly non-exhausitve, seeBekris et al.(2024)). The different methods can be grouped based on whether they explicitly (dynamics-based) or implicitly (learning-based) model robot-environment interactions.",
                "position": 736
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x3.png",
                "caption": "Figure 3:Different kinds of motions are achieved with potentially very different robotic platforms. From left to right, top to bottom: ViperX, SO-100, Boston Dynamics’ Spot, Open-Duck, 1X’s NEO, Boston Dynamics’ Atlas. This is an example list of robotic platforms and is (very) far from being exhaustive.",
                "position": 758
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x4.png",
                "caption": "Figure 4:Cheaper, more accessible robots are starting to rival traditional platforms like the Panda arm platforms in adoption in resource-constrained scenarios. The SO-100, in particular, has a cost in the 100s of Euros, and can be entirely 3D-printed in hours, while the industrially-manufactured Panda arm costs tens of thousands of Euros and is not openly available.",
                "position": 789
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x5.png",
                "caption": "Figure 5:The SO-100 arm is a 6-dof manipulator arm. Preventing some of its joints (shoulder pane, wrist flex and wrist roll) from actuating, it can be represented as a traditional 2-dof planar manipulator (the gripper joint in the end-effector is not considered towards the count of the degrees of freedom used to produce motion).",
                "position": 797
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x6.png",
                "caption": "(a)Free to move",
                "position": 814
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x6.png",
                "caption": "(a)Free to move",
                "position": 817
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x7.png",
                "caption": "(b)Constrained by the surface",
                "position": 822
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x8.png",
                "caption": "(c)Constrained by surface and (fixed) obstacle",
                "position": 827
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x9.png",
                "caption": "Figure 7:Planar manipulator robot in the presence of a moving obstacle.",
                "position": 909
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x10.png",
                "caption": "Figure 8:Dynamics-based approaches to robotics suffer from several limitations: (1) orchestrating multiple components poses integration challenges; (2) the need to develop custom processing pipelines for the sensing modalities and tasks considered hinders scalability; (3) simplified analytical models of physical phenomena (here friction at the gripper; credits toAntonova et al.(2017)) limit real-world performance. Lastly, (4) dynamics-based methods overlook trends in the availability and growth of robotics data.",
                "position": 939
            }
        ]
    },
    {
        "header": "Robot (Reinforcement) Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12403/x11.png",
                "caption": "Figure 9:Learning-based robotics streamlines perception-to-action by learning a (1) unified high-level controller capable to take (2) high-dimensional, unstructured sensorimotor information. Learning (3) does not require a dynamics model and instead focuses on interaction data, and (4) empirically correlates with\nthe scale of the data used.",
                "position": 990
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x12.png",
                "caption": "Figure 10:Overview of the robot learning methods implemented inlerobot. All algorithms are implemented in Pytorch. References:Zhao et al.(2023);Chi et al.(2024);Lee et al.(2024);Black et al.(2024);Shukor et al.(2025);Luo et al.(2024);Hansen et al.(2022)(top-to-bottom, left-to-right).",
                "position": 1008
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x13.png",
                "caption": "Figure 11:Examples of two different robotics tasks performed using RL. In the manipulation task (A) an agent learns to reach for a yellow plastic block in its environment, and to put it inside of a box. In the locomotion task (B) an agent learns to move its center of mass sideways without falling.",
                "position": 1017
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x14.png",
                "caption": "Figure 12:Agent-Environment interaction diagram (image credits toSutton and Barto(2018)).",
                "position": 1035
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x15.png",
                "caption": "Figure 13:Popular RL algorithms. SeeAchiam(2018)for a complete list of citations.",
                "position": 1182
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x16.png",
                "caption": "Figure 14:Simulated (left) vs. real-world (right) OpenDuck. Discrepancies in the simulation dynamics (reality gap) pose risks to policy transfer.",
                "position": 1206
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x17.png",
                "caption": "Figure 15:The same locomotion task can be carried out in different (simulated) domains (exemplified by the difference in terrains) at training time, resulting to increased robustness over diverse environment dynamics.",
                "position": 1218
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x18.png",
                "caption": "Figure 16:(A) HIL-SERL allows for real-world training of high performance RL agents by building on top advancements presented by of SAC, RLPD and SERL. (B) Example of human intervention during a HIL-SERL training process on a real-world SO-100.",
                "position": 1405
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x19.png",
                "caption": "Figure 17:HIL-SERL is a SOTA RL algorithm for training control policies directly in the real-world. Its implementation inlerobotrelies on a decoupled actor-learner architecture, communicating over processes (and possibly networks) with queues used to share (1) transitions(st,at,rt,st+1)(s_{t},a_{t},r_{t},s_{t+1})and (2) parametersθ\\theta.",
                "position": 1421
            }
        ]
    },
    {
        "header": "Robot (Imitation) Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12403/x20.png",
                "caption": "Figure 18:(A) Average (with standard deviation) evolution of the actuation levels over the first 5 recorded episodes inlerobot/svla_so101_pickplace. Proprioperceptive states provide invaluable to determine the robot’s state during an episode. (B) Camera frames are also recorded alongside measurements on the robot’s state, capturing information about the robot’s interaction with its environment.",
                "position": 2410
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x21.png",
                "caption": "Figure 19:Sample observations and action pairs over the course of a given trajectory recorded inlerobot/svla_so101_pickplace. Observations, comprising of both proprioperceptive and visual information, are recorded alongside the configuration of a second, leader robot controlled by a human expert, providing complete information for regressing actions given observations.",
                "position": 2430
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x22.png",
                "caption": "Figure 20:Point-wise policies suffer from limitations due to (A) covariate shifts and (B) poor approximation of multimodal demonstrations. (A) Small errors may drive the policy out of distribution, incuring in a vicious circle ultimately resulting in failure. (B) Both modes of reaching for a target object in the scene—either left or right-first—are equally as good and thus equally as likely to be present in a dataset of human demonstrations, ultimately resulting in multimodal demonstrations.",
                "position": 2463
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x23.png",
                "caption": "Figure 21:Intuitively, latent variable in a single latent model may contain information regarding the task being performed, which directly results in the likelihood of the same observation-action pair being different for two different tasks. When (A) picking a block the likelihood of a wide gripper’s opening should be higher than narrower one, while it should be the opposite when (B) pushing the block.",
                "position": 2485
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x24.png",
                "caption": "Figure 22:(A) The latent variable model in a robotics application regulates influence between observed (o,a)o,a)variables and an unobservable latent variable. (B) VAEs approximate exact latent variable models by means of variational inference.",
                "position": 2506
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x25.png",
                "caption": "Figure 23:HMLV models posit the data generation process is influenced by a stack of Markov-dependent latent variables, with samples from the posterior distribution being progressively higher up in the hierarchy.",
                "position": 2656
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x26.png",
                "caption": "Figure 24:DMs iteratively corrupt samples (left) from an unknown distribution into a quasi-standard Gaussian (center), learning the displacement field (right) that permits to reconstruct samples from the unknown target distribution by iteratively denoising samples of a tractable, easy-to-sample distribution.",
                "position": 2782
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x27.png",
                "caption": "Figure 25:A joint action-observation distribution, in the simplified case where the observation is the elbow-flex actuation in a SO-100, and the action is the recorded position for the same joint from the teleoperator arm. The motion recorded being teleoperated, the points distribute along a the diagonal.",
                "position": 2803
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x28.png",
                "caption": "Figure 26:Probability distributions can be modified differently by applying different vector fields, inducing different flows of mass across the same support (top versus bottom, using two different time-invariant 2D-fieldsu1​(x,y)=(x,0)u_{1}(x,y)=(x,0)andu2​(x,y)=(x/2,y/2)u_{2}(x,y)=(x/\\sqrt{2},y/\\sqrt{2})). Notice time flowscontinuouslyin[0,1][0,1]. FM models learn to approximate a target vector field, thereby producing arbitrary (goal) transformations of an easy-to-sample initial distribution.",
                "position": 2868
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x29.png",
                "caption": "Figure 27:Compared to diffusion, flow matching distorts distribution along a less randomic pattern, resulting in a clearer interpolation between source and target distribution. The visualization shows an example comparison between these two methods on joint distribution of robot observations and actions overT=50T=50steps.",
                "position": 2879
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x30.png",
                "caption": "Figure 28:The CVAE encoder used in ACT. Input action chunks are first embedded and aggregated with positional embeddings, before being processed alongside embedded proprioperceptive information, and a learned[CLS]token used to aggregate input level information, and predict the style variablezz. The encoder is exclusively used totrainthe decoder, and it is entirely disregarded at inference time.",
                "position": 2944
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x31.png",
                "caption": "Figure 29:The CVAE decoder used in ACT, comprising of a full encoder-decoder Transformer architecture. Camera observations from allnncamera views are first embedded using pre-trained visual encoders, and then aggregated with the corresponding positional embeddings. Then, the proprioperceptive information and style variablezzretrieved from the CVAE encoder, are fed to the encoder-decoder Transformer for inference. The encoder shares the matricesK,VK,Vwith the decoder, and is trained to decode fixed position embeddings into action chunks.",
                "position": 2953
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x32.png",
                "caption": "Figure 30:Action Chunking with Transformer (ACT), as inZhao et al.(2023). ACT introduces an action chunking paradigm to cope with high-dimensional multi-modal demonstration data, and a transformer-based CVAE architecture.",
                "position": 2958
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x33.png",
                "caption": "Figure 31:The Diffusion Policy archicture, as inChi et al.(2024). A stack ofHoH_{o}previous observations is used as external conditioning to denoise a group ofHaH_{a}actions. Conditioning is performed at every layer of a U-Net block. Diffusion Policy allows to obtain fully-formed action chunks with as little asT=10T=10denoising steps.",
                "position": 3325
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x34.png",
                "caption": "Figure 32:Asynchronous inference. Illustration of the asynchronous inference stack. Note that the policy can be run on a remote server, possibly with GPUs.",
                "position": 3712
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x35.png",
                "caption": "Figure 33:Action queue size evolution at runtime for various levels ofggwhen (A) not filtering out observation based on joint-space similarity and (B) filtering out near-duplicates observation, measuring their similarity in joint-space.",
                "position": 3819
            }
        ]
    },
    {
        "header": "Generalist Robot Policies",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12403/x36.png",
                "caption": "Figure 34:Fields within ML such as Computer Vision and NLP converged on the development of foundation models, trained on a variety of large scale models and capable to perform multiple downstream tasks (top). Conversely, robotics suffered from limited standardization in terms of the architectures used, and siloed, task specific datasets, incurring in a high degree of fragmentation which traditionally hindered the development of generalist models for robotics in favour of task-specific models (bottom).",
                "position": 4024
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x37.png",
                "caption": "Figure 35:Early efforts in the development of generalist models for robotics include BC-Zero(Jang et al.,2022), RT-1(Brohan et al.,2023b), and RT-2(Brohan et al.,2023a): large scale models trained on thousands of demonstrations. The open release of the Open-X(O’Neill et al.,2025)and DROID datasets(Khazatsky et al.,2025)fostered the development of open source models: OpenVLA(Kim et al.,2024),π0\\pi_{0}(Black et al.,2024)and SmolVLA(Shukor et al.,2025).",
                "position": 4040
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x38.png",
                "caption": "Figure 36:Robot learning is undergoing a paradigmatic shift: centralized data collections (A, left) are increasingly larger, often comprising millions of demonstrations, while (A, right) decentralized data collection efforts are becoming an alternative for large scale data collection. (B) Generalist models are also becoming increasingly smaller and easier to run on limited hardware.",
                "position": 4075
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x39.png",
                "caption": "Figure 37:Theπ0\\pi_{0}architecture, as inBlack et al.(2024). Vision and language tokens are routed to a VLM backbone which is prevented from attending robot proprioperceptive states and action tokens, which are instead routed to a smaller subset of weights within the architecture referred to as \"action expert\". The architecture is trained with Flow Matching on 10M+ trajectories from a mixture of closed and openly available datasets.",
                "position": 4126
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x40.png",
                "caption": "Figure 38:Unlike more traditional flow-matching algorithms,π0\\pi_{0}uses a modified distribution to sample the timestepτ\\taufrom during training and inference, favouring earlier timestamps corresponding to noisier chunks.",
                "position": 4189
            },
            {
                "img": "https://arxiv.org/html/2510.12403/x41.png",
                "caption": "Figure 39:The SmolVLA architecture, as inShukor et al.(2025). SmolVLA is a compact MoE model trained with flow matching to denoise action chunks. Vision and language tokens are fed to a VLM backbone, and share information with the proprioperceptive and action tokens via the attention mechanism. The attention expert interleaves SA and CA layers for further conditioning on the visual features from the VLM backbone. SmolVLA skips computations and reduces the visual tokens, resulting in 7x less memory usage thanπ0\\pi_{0}(450M parameters vs.π0\\pi_{0}’s 3.3B).",
                "position": 4367
            }
        ]
    },
    {
        "header": "Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]