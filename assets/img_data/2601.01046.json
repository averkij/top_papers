[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.01046/x1.png",
                "caption": "Figure 1:Overview of KV-Embedding. (a) Standard causal attention creates information asymmetry, as tokens only attend to preceding context. (b) KV-Embedding re-routes the last KV pair as a global prefix, enabling sequence-wide context access within one forward pass. (c) The pipeline identifies optimal re-routing anchors by locating layers with minimum intrinsic dimensionality, ensuring model-agnostic stability without manual tuning.",
                "position": 175
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Analysis and Ablation",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.01046/x2.png",
                "caption": "Figure 2:Average performance of Mistral-7B-Instruct with KV-Embedding across different attention bias values. A bias value of 1.0 corresponds to the highest average score.",
                "position": 912
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Appendix AAlgorithm Details",
        "images": []
    },
    {
        "header": "Appendix BProbing KV States for Sequence Information",
        "images": []
    },
    {
        "header": "Appendix CLayer-wise Representation Analysis",
        "images": []
    },
    {
        "header": "Appendix DBenchmark Details",
        "images": []
    },
    {
        "header": "Appendix EDetailed MTEB Results",
        "images": []
    },
    {
        "header": "Appendix FDetailed LoCoV1 Results",
        "images": []
    },
    {
        "header": "Appendix GAttention Bias Experiment",
        "images": []
    },
    {
        "header": "Appendix HGeometry of the Embedding Space",
        "images": []
    },
    {
        "header": "Appendix IIntrinsic Dimensionality Trajectories",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.01046/x3.png",
                "caption": "Figure 3:Intrinsic Dimensionality (ID) across layers for Mistral-7B-Instruct-v0.1 and Qwen3-4B. Different architectures exhibit distinct semantic compression patterns.",
                "position": 3470
            }
        ]
    },
    {
        "header": "Appendix JAnalysis of Layer Selection",
        "images": []
    },
    {
        "header": "Appendix KImpact of Attention Constraints",
        "images": []
    },
    {
        "header": "Appendix LPrompt Design",
        "images": []
    },
    {
        "header": "Appendix MPooling Strategy",
        "images": []
    }
]