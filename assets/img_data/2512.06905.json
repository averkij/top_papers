[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06905/x1.png",
                "caption": "Figure 1:Saber is a zero-shot reference-to-video method trained only on video-text pairs.\nIt preserves identity and appearance while coherently integrating single/multiple references into videos guided by text prompts.",
                "position": 173
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06905/x2.png",
                "caption": "Figure 2:Masked reference generation.Given a video, the mask generator produces diverse random masks, which are then applied to each randomly sampled video frame with mask augmentation.",
                "position": 337
            },
            {
                "img": "https://arxiv.org/html/2512.06905/x3.png",
                "caption": "Figure 3:Model design overview.Masked frames serve as reference images and are concatenated to the video tokens in latent space.\nSelf-attention enables interaction between video and reference tokens under the attention mask, while cross-attention incorporates text guidance for semantic alignment.\nThe VAE, text encoder, and timestep components are omitted for clarity.",
                "position": 399
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06905/x4.png",
                "caption": "Figure 4:Qualitative comparison with existing R2V methods.We compare Saber with Kling1.6(Team,2025a), Phantom(Liu et al.,2025), and VACE(Jiang et al.,2025)across four scenarios: single/multiple human and object references.\nSaber accurately preserves subject identity and appearance, integrates multiple references coherently, and generates smoother, more visually consistent videos.",
                "position": 649
            },
            {
                "img": "https://arxiv.org/html/2512.06905/x5.png",
                "caption": "Figure 5:Effect of mask augmentation.Without mask augmentation, the model shows copy-paste artifacts by directly copying reference content.\nApplying augmentation enables more natural and coherent video generation.",
                "position": 758
            },
            {
                "img": "https://arxiv.org/html/2512.06905/x6.png",
                "caption": "Figure 6:Effect of the attention mask.Removing the attention mask introduces gray artifacts around subjects, while applying it ensures clean separation from the gray background and smoother, more natural video results.",
                "position": 781
            },
            {
                "img": "https://arxiv.org/html/2512.06905/x7.png",
                "caption": "Figure 7:Qualitative results on multiple reference images of the same subject.Given the front, side, and back views of a robot as reference, Saber correctly recognizes them as the same subject and integrates multi-view appearance features into a coherent video, accurately preserving fine structural and surface details.",
                "position": 795
            },
            {
                "img": "https://arxiv.org/html/2512.06905/x8.png",
                "caption": "Figure 8:Qualitative results of cross-modal alignment between reference images and text prompts.By swapping subject descriptions in the prompts (e.g., clothing color or subject positions), Saber accurately reflects the corresponding visual changes, demonstrating robust alignment between reference images and textual descriptions through its attention mechanisms.",
                "position": 809
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]