[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01257/extracted/5894354/fig/helpfulness_diff_against_preference.png",
                "caption": "Figure 1:Distribution of preferences between responses in HelpSteer2Preference against the difference in helpfulness scores between them from HelpSteer2. For clarity, A refers to Response 1 while B refers to Response 2.>>>much-greater-thanabsent>>>> > >means much better,>>much-greater-than>>> >means better,>>>means slightly better and===means as good as.",
                "position": 263
            }
        ]
    },
    {
        "header": "3Reward Model",
        "images": []
    },
    {
        "header": "4Reward Model Results",
        "images": []
    },
    {
        "header": "5Aligned Models",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelated Works",
        "images": []
    },
    {
        "header": "Appendix BPreference Ranking Guidelines",
        "images": []
    },
    {
        "header": "Appendix CJustification Pre-processing",
        "images": []
    },
    {
        "header": "Appendix DPreference Justification Analysis",
        "images": []
    },
    {
        "header": "Appendix ETraining Hyper-parameters",
        "images": []
    },
    {
        "header": "Appendix FCompute requirements",
        "images": []
    },
    {
        "header": "Appendix GRewardbench Response Distribution",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01257/extracted/5894354/fig/scaled_bt_plus_expo_plot.png",
                "caption": "Figure 2:Distribution of Reward Scores for RewardBench responses by our best reward model (Scaled BT + ExPO, initialized on Helpfulness-Only SteerLM Regression model). Difference refers to the difference between the reward scores of chosen and rejected responses to the same prompt. When a response has reward of -15, it is equally likely to be chosen or rejected. This meaning the score of the response cannot be seen in isolation to determine if a response is good (or bad).",
                "position": 2565
            }
        ]
    },
    {
        "header": "Appendix HAligned Model Evaluation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01257/extracted/5894354/fig/reward_curves.png",
                "caption": "Figure 3:Reward curves of PPO and REINFORCE. Please note that REINFORCE samples four times as many responses as PPO per training step. For PPO, the reward curve is produced by concatenating the reward curves for Round 1 (ending at step 57) and Round 2. We select step 26 for PPO Round 1, step 40 for PPO Round 2 (whose reward is shown at step 97 above) while for REINFORCE, we select step 95.",
                "position": 2591
            }
        ]
    },
    {
        "header": "Appendix IReward Curves",
        "images": []
    }
]