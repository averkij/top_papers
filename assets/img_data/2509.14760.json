[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.14760/x1.png",
                "caption": "Figure 1:Illustration of our proposed specification alignment across diverse scenarios.",
                "position": 200
            },
            {
                "img": "https://arxiv.org/html/2509.14760/x2.png",
                "caption": "Figure 2:Representative results. x-axis: safety score, y-axis: behavioral score, both defined in Sec.5.1, measuring safety and helpfulness respectively.",
                "position": 224
            },
            {
                "img": "https://arxiv.org/html/2509.14760/x3.png",
                "caption": "Figure 3:Overview of our work. (a) introduces specification alignment by jointly optimizing safety and behavioral specifications (Sec.3). (b) details the construction ofSpecBench, covering scenario and specification design, data curation with LLMs and human verification, and an evaluation pipeline where eachspecis judged asYES,NO, orNA(Sec.4). (c) shows test-time deliberation methods that reason over specification boundaries, including our proposedAlign3(Sec.6).",
                "position": 288
            },
            {
                "img": "https://arxiv.org/html/2509.14760/1-figure/scenario_source_1.png",
                "caption": "Figure 4:Data sources for each scenario.",
                "position": 419
            },
            {
                "img": "https://arxiv.org/html/2509.14760/1-figure/qwen.png",
                "caption": "Table 2:TTD Results (%) of Qwen3-14B and Llama-3.1-8B variants.Redandbluesubscripts: changes relative to the vanilla instruct and reasoning models, respectively. Tokens: the averagecompletion tokensper sample. Qwen3-14B vanilla is equivalent to applying ZeroThink to Qwen3-14B-thinking.",
                "position": 964
            },
            {
                "img": "https://arxiv.org/html/2509.14760/1-figure/llama.png",
                "caption": "",
                "position": 981
            },
            {
                "img": "https://arxiv.org/html/2509.14760/1-figure/deepseek.png",
                "caption": "",
                "position": 1022
            },
            {
                "img": "https://arxiv.org/html/2509.14760/x4.png",
                "caption": "Figure 5:Metrics (%) across data splits, averaged over all models.",
                "position": 1084
            },
            {
                "img": "https://arxiv.org/html/2509.14760/x5.png",
                "caption": "Figure 6:SAR (%) across scenarios, averaged over representative models. Grey polar line: mean SAR over all models.",
                "position": 1094
            },
            {
                "img": "https://arxiv.org/html/2509.14760/1-figure/annotation.png",
                "caption": "Figure 7:The annotation interface of our human evaluation study. Human annotators were given the same evaluation information and rules as the LLM evaluators. The left panel contains the scenario, prompt, and response, while the right panel shows the corresponding safety and behavioral specifications for that scenario.",
                "position": 2878
            },
            {
                "img": "https://arxiv.org/html/2509.14760/x6.png",
                "caption": "Figure 8:Specification judgements of Llama-3.1-8B-Instruct across all scenarios, evaluated by GPT-4.1: top forbehavioral-spec, bottom forsafety-spec. Each bar corresponds to one specification within a scenario. For example, in the bottom figure, the second bar of the Biochem scenario represents asafety-spec, with the stacked segments indicating the proportions of 300 responses labeled asYES,NA, orNO.",
                "position": 2984
            },
            {
                "img": "https://arxiv.org/html/2509.14760/x7.png",
                "caption": "",
                "position": 2988
            },
            {
                "img": "https://arxiv.org/html/2509.14760/x8.png",
                "caption": "Figure 9:Specification judgements of DeepSeek-R1 across all scenarios evaluated by GPT-4.1: top forbehavioral-spec, bottom forsafety-spec. Each bar corresponds to one specification within a scenario. For example, in the bottom figure, the second bar of the Biochem scenario represents asafety-spec, with the stacked segments indicating the proportions of 300 responses labeled asYES,NA, orNO.",
                "position": 2994
            },
            {
                "img": "https://arxiv.org/html/2509.14760/x9.png",
                "caption": "",
                "position": 2998
            },
            {
                "img": "https://arxiv.org/html/2509.14760/x10.png",
                "caption": "Figure 10:SAR performance variation under different offsetsÎ±\\alphain Eq.4. Red and orange cells indicate safety and behavioral scores (%) described in Sec.4.3, and blue cells show the resulting SAR. Darker colors indicate higher values, and all numbers are rounded to the nearest integer.",
                "position": 3011
            },
            {
                "img": "https://arxiv.org/html/2509.14760/x11.png",
                "caption": "Figure 11:SAR performance of all LLMs across five scenarios, with bars showing scenario-level scores and gray dots indicating the average SAR. This highlights both overall performance and variation across scenarios.",
                "position": 3043
            },
            {
                "img": "https://arxiv.org/html/2509.14760/x12.png",
                "caption": "Figure 12:Overall evaluation results from GPT-4.1 () and Qwen3-32B-thinking (), reporting safety, behavioral, and SAR scores across 33 models.",
                "position": 3114
            },
            {
                "img": "https://arxiv.org/html/2509.14760/1-figure/openai.png",
                "caption": "",
                "position": 3116
            },
            {
                "img": "https://arxiv.org/html/2509.14760/x13.png",
                "caption": "Figure 13:Rank-rank scatter plot comparing GPT-4.1 (x-axis) and Qwen3-32B-thinking (y-axis) rankings on safety, behavioral, and SAR scores for 33 models. Each point corresponds to one model, with alignment to the diagonal indicating stronger agreement between evaluators.",
                "position": 3166
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]