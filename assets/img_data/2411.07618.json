[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.07618/x1.png",
                "caption": "Figure 1:Left.The DPO objective loss function and its two main improvement directions: SimPO and TDPO. SimPO focuses on simplifying the reference model, while TDPO concentrates on controlling the alignment process to enhance generation diversity.Right.The pipeline of FPO consists of sparse autoencoders and the feature-level MSE constraints.",
                "position": 99
            }
        ]
    },
    {
        "header": "2Preliminary",
        "images": []
    },
    {
        "header": "3Feature-Level Direct Preference Optimization",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.07618/x2.png",
                "caption": "Figure 2:Left.Top-50 SAE feature activation value distribution in Gemma-2-2b. We ranked the activated feature by its activation value. The vertical axis represents the activation values, while the horizontal axis shows the rank of the maximum activation values. This plot illustrates the sparsity of SAE—out of 16,000 features, fewer than 50 have significant activation values.Right.Comparison of existing alignment methods on (1) if they need to load a reference model when training the policy model. (2) Memory consumption. (3) Their ability to control the generation diversity.",
                "position": 271
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Results and Discussions",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.07618/x3.png",
                "caption": "Figure 3:Left.KL Divergence on the preferred responses (chosen).Center.KL Divergence on the dispreferred responses (rejected).Right.KL Divergence margini.e.,|βDSeqKL(x,yl;πref∥πθ)−βDSeqKL(x,yw;πref∥πθ)||\\beta D_{\\text{SeqKL}}\\left(x,y_{l};\\pi_{\\text{ref}}\\|\\pi_{\\theta}\\right)-%\n\\beta D_{\\text{SeqKL}}\\left(x,y_{w};\\pi_{\\text{ref}}\\|\\pi_{\\theta}\\right)|| italic_β italic_D start_POSTSUBSCRIPT SeqKL end_POSTSUBSCRIPT ( italic_x , italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ; italic_π start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT ∥ italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ) - italic_β italic_D start_POSTSUBSCRIPT SeqKL end_POSTSUBSCRIPT ( italic_x , italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT ; italic_π start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT ∥ italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ) |.",
                "position": 871
            },
            {
                "img": "https://arxiv.org/html/2411.07618/x4.png",
                "caption": "",
                "position": 874
            },
            {
                "img": "https://arxiv.org/html/2411.07618/x5.png",
                "caption": "",
                "position": 875
            },
            {
                "img": "https://arxiv.org/html/2411.07618/x6.png",
                "caption": "Figure 4:Left.GPU memory consumption on a single H100 with all methods. We average the average GPU memory in 1,000 steps at the beginning of the training.Center.Feature-level MSE Loss of all methods after the whole alignment process. Here margin is defined as|DFPOℓ(x,yl;πref∥πθ)−βDFPOℓ(x,yw;πref∥πθ)||D^{\\ell}_{\\text{FPO}}\\left(x,y_{l};\\pi_{\\text{ref}}\\|\\pi_{\\theta}\\right)-%\n\\beta D^{\\ell}_{\\text{FPO}}\\left(x,y_{w};\\pi_{\\text{ref}}\\|\\pi_{\\theta}\\right)|| italic_D start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT FPO end_POSTSUBSCRIPT ( italic_x , italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ; italic_π start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT ∥ italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ) - italic_β italic_D start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT FPO end_POSTSUBSCRIPT ( italic_x , italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT ; italic_π start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT ∥ italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ) |.Right.Win rates of FPO v.s. other methods above the improvements based on Gemma-2-2B evaluated by GPT-4 on different sampling temperatures.",
                "position": 881
            },
            {
                "img": "https://arxiv.org/html/2411.07618/x7.png",
                "caption": "",
                "position": 884
            },
            {
                "img": "https://arxiv.org/html/2411.07618/x8.png",
                "caption": "",
                "position": 885
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Reproducible Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining Settings",
        "images": []
    },
    {
        "header": "Appendix BBounding KL Divergence with MSE of Sparse Activation",
        "images": []
    }
]