[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18597/x1.png",
                "caption": "",
                "position": 99
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18597/x2.png",
                "caption": "Figure 2:MM-DiT Attention Analysis. We find the attention matrix in MM-DiT attention can be divided into four different regions. As for the prompt of‚Äú a cat watch a black mouse‚Äù, each text token shows a high-light response using the average of the text-to-video and video-to-text attention.",
                "position": 205
            },
            {
                "img": "https://arxiv.org/html/2412.18597/x3.png",
                "caption": "Figure 3:MM-DiT Text-to-Text and Video-to-Video Attention Visualization. We find that the current MM-DiT has a stronger potential to construct the individual attention in the previous UNet-like structure[10,11,41].",
                "position": 225
            },
            {
                "img": "https://arxiv.org/html/2412.18597/x4.png",
                "caption": "Figure 4:Pipeline of the proposed DiTCtrl. Our method tries to synthesize content-consistent and motion-consistent videos based on multi-prompts. The first video is synthesized with source text promptPi‚àí1subscriptùëÉùëñ1P_{i-1}italic_P start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT. During the denoising process for video synthesis, we convert the full-attention into masked-guided KV-sharing strategy to query video contents from source videoùí±i‚àí1subscriptùí±ùëñ1\\mathcal{V}_{i-1}caligraphic_V start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT, so that we can synthesize content-consistent video under the modified target promptPisubscriptùëÉùëñP_{i}italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. Note that initial latents are assumed to be 5 frames. The first three frames are used to generate the contents ofPi‚àí1subscriptùëÉùëñ1P_{i-1}italic_P start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT, and the last three frames are used to generate contents ofPisubscriptùëÉùëñP_{i}italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. Thepinklatent represents the overlapping frame, while theblueandgreenlatents are used to distinguish different prompt segments.",
                "position": 243
            },
            {
                "img": "https://arxiv.org/html/2412.18597/x5.png",
                "caption": "Figure 5:Latent blending strategyfor video transition between video clips.",
                "position": 314
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18597/x6.png",
                "caption": "Figure 6:Generation results on given prompts by our method and baseline models. Kling is the commercial model, and Freenoise+DiT is our implementation of Freenoise on CogVideoX.",
                "position": 326
            },
            {
                "img": "https://arxiv.org/html/2412.18597/x7.png",
                "caption": "Figure 7:T-SNE visualization of CLIP embeddings. Each point represents the CLIP embedding of a single video frame after dimensionality reduction. The visualization demonstrates that conventional multi-prompt videos form distinct clusters, while our method produces a more continuous distribution, indicating smoother semantic transitions.",
                "position": 353
            },
            {
                "img": "https://arxiv.org/html/2412.18597/x8.png",
                "caption": "Figure 8:Ablation Component in DiTCtrl. The first and second rows have 98 frames, while the remaining methods generate 105 frames.",
                "position": 504
            },
            {
                "img": "https://arxiv.org/html/2412.18597/x9.png",
                "caption": "Figure 9:single prompt longer video generation example.",
                "position": 529
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Overview",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18597/x10.png",
                "caption": "Figure 10:Mask-guided KV-sharing details.",
                "position": 1326
            }
        ]
    },
    {
        "header": "Appendix BMore Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18597/x11.png",
                "caption": "Figure 11:More multi-prompt results",
                "position": 1340
            },
            {
                "img": "https://arxiv.org/html/2412.18597/x12.png",
                "caption": "Figure 12:More multi-prompt results",
                "position": 1343
            },
            {
                "img": "https://arxiv.org/html/2412.18597/x13.png",
                "caption": "Figure 13:Motion and background transition.",
                "position": 1346
            },
            {
                "img": "https://arxiv.org/html/2412.18597/x14.png",
                "caption": "Figure 14:Background transition.",
                "position": 1349
            }
        ]
    },
    {
        "header": "Appendix CApplications",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18597/x15.png",
                "caption": "Figure 15:Visualization of single prompt longer video generation.",
                "position": 1362
            },
            {
                "img": "https://arxiv.org/html/2412.18597/x16.png",
                "caption": "Figure 16:Reweighting example of Video Editing.",
                "position": 1379
            },
            {
                "img": "https://arxiv.org/html/2412.18597/x17.png",
                "caption": "Figure 17:Word Swap example of Video Editing.",
                "position": 1385
            }
        ]
    },
    {
        "header": "Appendix DPrompt Generator",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18597/x18.png",
                "caption": "Figure 18:Ablation study of mask-guided KV-sharing results. First row shows our model without mask-guided KV-sharing, while the second row demonstrates our full model with mask-guided KV-sharing. The prompt for (a) transitions from ‚ÄúA powerful horse gallops across a field‚Ä¶‚Äù to ‚ÄúA striking zebra leads its herd across the field‚Ä¶‚Äù. The prompt for (b) evolves from ‚ÄúA white SUV drives a dirt road‚Ä¶‚Äù to ‚ÄúA white SUV powers through snow‚Ä¶‚Äù",
                "position": 1452
            },
            {
                "img": "https://arxiv.org/html/2412.18597/x19.png",
                "caption": "Figure 19:Our instruction to create multiple individual long prompts based on short prompts group of specified types",
                "position": 1465
            }
        ]
    },
    {
        "header": "Appendix EAblation Study",
        "images": []
    }
]