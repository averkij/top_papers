[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14135/extracted/6451190/figures/logo.png",
                "caption": "",
                "position": 138
            },
            {
                "img": "https://arxiv.org/html/2505.14135/extracted/6451190/figures/teaser_img.jpg",
                "caption": "Figure 1:Hunyuan-Game-Image. The image generation capabilities of Hunyuan-Game include text-to-image generation, text-to-game effects generation, reference-based game visual effects generation, transparent and seamless image generation, and game character/scene generation. These capabilities offer a powerful toolset that significantly reduces the time and resources required for content creation, thereby greatly enhancing the efficiency of game asset production.",
                "position": 163
            },
            {
                "img": "https://arxiv.org/html/2505.14135/x1.png",
                "caption": "Figure 2:Hunyuan-Game-Video. The five key video generation capabilities of Hunyuan-Game are demonstrated as follows: an Image-to-Video generation and 360-degree A/T Pose Avatar Video Synthesis (I2V); Dynamic Illustration Generation based on first and last frame generation (FLF2V); video super-resolution from original video content (V2V); and Interactive Game Video Generation based on text or image input.",
                "position": 166
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Hunyuan-Game-Image Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14135/extracted/6451190/figures/2.1.2_t2i_data_filtering.png",
                "caption": "Figure 3:The data filtering pipeline.",
                "position": 362
            },
            {
                "img": "https://arxiv.org/html/2505.14135/x2.png",
                "caption": "Figure 4:Examples of multi-dimensional aesthetic scores.",
                "position": 378
            },
            {
                "img": "https://arxiv.org/html/2505.14135/extracted/6451190/figures/caption-example_t2i.jpg",
                "caption": "Figure 5:Examples of Multi-Length Captions.",
                "position": 401
            },
            {
                "img": "https://arxiv.org/html/2505.14135/x3.png",
                "caption": "Figure 6:Prompt rewriting can significantly add content information to the picture, thus enhancing the quality of the image.Top row: w/o prompt rewriting.Bottom row: w/ prompt rewriting.",
                "position": 445
            },
            {
                "img": "https://arxiv.org/html/2505.14135/x4.png",
                "caption": "Figure 7:Visualizations of text-to-image generation results.",
                "position": 448
            },
            {
                "img": "https://arxiv.org/html/2505.14135/x5.png",
                "caption": "Figure 8:The pipeline to create effectualized materials.",
                "position": 547
            },
            {
                "img": "https://arxiv.org/html/2505.14135/extracted/6451190/figures/texiao_description.jpg",
                "caption": "Figure 9:Examples of brief, detailed, and comprehensive descriptions.",
                "position": 572
            },
            {
                "img": "https://arxiv.org/html/2505.14135/extracted/6451190/figures/t2v_e_recap.jpg",
                "caption": "Figure 10:Rewritten descriptions can significantly enhance the details and texture of generated images.Top row: w/o prompt rewriting.Bottom row: w/ prompt rewriting.",
                "position": 575
            },
            {
                "img": "https://arxiv.org/html/2505.14135/extracted/6451190/figures/t2effect_compare_v2.jpg",
                "caption": "Figure 11:Qualitative comparisons with State-of-The-Art methods,i.e, Midjourney 6.1 Pro and Jimeng 2.0.",
                "position": 621
            },
            {
                "img": "https://arxiv.org/html/2505.14135/x6.png",
                "caption": "Figure 12:Visualizations of text-to-game visual effects generation results.",
                "position": 624
            },
            {
                "img": "https://arxiv.org/html/2505.14135/extracted/6451190/figures/game_effects.jpg",
                "caption": "Figure 13:Game effects generation results based on (a) black sketch control, (b) color sketch control, (c) black-and-white draft, and (d) material transfer.",
                "position": 659
            },
            {
                "img": "https://arxiv.org/html/2505.14135/extracted/6451190/figures/control_pipeline.jpg",
                "caption": "Figure 14:The pipeline of black-and-white draft generation.",
                "position": 662
            },
            {
                "img": "https://arxiv.org/html/2505.14135/x7.png",
                "caption": "Figure 15:Visualizations of image-to-game visual effects generation results.",
                "position": 696
            },
            {
                "img": "https://arxiv.org/html/2505.14135/x8.png",
                "caption": "Figure 16:Hunyuan transparent image generation",
                "position": 757
            },
            {
                "img": "https://arxiv.org/html/2505.14135/x9.png",
                "caption": "Figure 17:Seamless tile image generation. The first row represents the generation of seamless tile images in the horizontal direction. The second row represents the generation of seamless tile images in both the horizontal and vertical directions. The last column verifies the effectiveness of the method by stitching together the generated seamless tile images.",
                "position": 762
            },
            {
                "img": "https://arxiv.org/html/2505.14135/extracted/6451190/figures/character_pipeline.jpg",
                "caption": "Figure 18:Method of Game Character Generation for lineart to grayscale image and grayscale image to character image.",
                "position": 839
            },
            {
                "img": "https://arxiv.org/html/2505.14135/x10.png",
                "caption": "Figure 19:Method of Game Character Generation for character consistency.",
                "position": 845
            },
            {
                "img": "https://arxiv.org/html/2505.14135/x11.png",
                "caption": "Figure 20:Visualizations of lineart→→\\to→Grayscale→→\\to→Character image generation.",
                "position": 855
            },
            {
                "img": "https://arxiv.org/html/2505.14135/x12.png",
                "caption": "Figure 21:Visualizations of consistent game character generation results.",
                "position": 916
            }
        ]
    },
    {
        "header": "3Hunyuan-Game-Video Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14135/extracted/6451190/figures/processing_flow2.png",
                "caption": "Figure 22:The data filtering pipeline.",
                "position": 975
            },
            {
                "img": "https://arxiv.org/html/2505.14135/x13.png",
                "caption": "Figure 23:An example of the structured caption for video clips. The VLM generated the caption sequentially from the most detailed caption to a summarised caption to several labels. It mocks a chain-of-thought process that can reduce the probability of model hallucination.",
                "position": 1003
            },
            {
                "img": "https://arxiv.org/html/2505.14135/x14.png",
                "caption": "Figure 24:Qualitative results of Hunyuan-Game. Our method exhibits robust ID preservation and prompt-following capability, achieving superior visual fidelity and motion naturalness.",
                "position": 1006
            },
            {
                "img": "https://arxiv.org/html/2505.14135/x15.png",
                "caption": "Figure 25:Examples of the prompt rewriting. Rewriting prompts can reduce the probability of subject distortion and enhance the model’s ability to follow instructions.",
                "position": 1052
            },
            {
                "img": "https://arxiv.org/html/2505.14135/x16.png",
                "caption": "Figure 26:Mesh data processing pipeline used to generate dataset used in training.\nWe filter character meshes by various methods, then render a group of images used in training process.",
                "position": 1149
            },
            {
                "img": "https://arxiv.org/html/2505.14135/x17.png",
                "caption": "Figure 27:Framework fo Hunyuan-Game 360° Character Video Generation",
                "position": 1169
            },
            {
                "img": "https://arxiv.org/html/2505.14135/x18.png",
                "caption": "Figure 28:Qualitative comparisons with State-of-The-Art methods,\nto align the results, for the Kling spinning effect, Wan2.1 360° LoRA, and our method, 4 frames are extracted from the generated videos for comparison. In contrast, it can be seen that our method has a better effect.",
                "position": 1172
            },
            {
                "img": "https://arxiv.org/html/2505.14135/x19.png",
                "caption": "Figure 29:Qualitative results of 360° Character Video Generation. Our method demonstrates excellent character consistency and rotation robustness, and it is capable of generating reasonable clothing and texture details from different viewpoints.",
                "position": 1176
            },
            {
                "img": "https://arxiv.org/html/2505.14135/x20.png",
                "caption": "Figure 30:The dynamic illustration training data we collected is divided into three levels:Level 1consists of high-quality videos with obvious looping motions;Level 2includes medium-quality videos (Case 2) or those with minimal, non-looping motions (Case 1); andLevel 3comprises static videos (Case 2) or those with transitions (Case 1).",
                "position": 1242
            },
            {
                "img": "https://arxiv.org/html/2505.14135/extracted/6451190/figures/lihui_compare.jpg",
                "caption": "Figure 31:Qualitative comparisons with State-of-The-Art methods,i.e., Wan2.1 and Kling. Our method successfully achieves a looping subtle motion effect. In contrast, the results of Wan2.1 and Kling show significant changes in the characters’ postures and movements.",
                "position": 1245
            },
            {
                "img": "https://arxiv.org/html/2505.14135/x21.png",
                "caption": "Figure 32:Framework of Hunyuan-Game-Generative Video Super-Resolution.",
                "position": 1319
            },
            {
                "img": "https://arxiv.org/html/2505.14135/x22.png",
                "caption": "Figure 33:Qualitative results of Hunyuan-Game-Video Super-Resolution. We compare our method with other SOTA methods, including APISR[66]and VEnhancer[19]. Ours present clearer and better restoration with more natural details, preserving original color saturation or contrast, without sharper edge lines.",
                "position": 1341
            },
            {
                "img": "https://arxiv.org/html/2505.14135/x23.png",
                "caption": "Figure 34:Overall framework of Hunyuan-GameCraft.",
                "position": 1427
            },
            {
                "img": "https://arxiv.org/html/2505.14135/x24.png",
                "caption": "Figure 35:Qualitative results of interactive game video generation. Given discrete keyboard action signal, Hunyuan-GameCraft supports action control across diverse open-domain scenes with spatial coherence. Besides, our introduced hybrid-conditioned auto-repressive video extension framework helps to extent short clips to long sequence with history information preservation. In our case, blue-lit keys indicate key presses. W, A, S, D represent transition movement and ↑, ←, ↓, → denote changes in view angles.",
                "position": 1430
            }
        ]
    },
    {
        "header": "4Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]