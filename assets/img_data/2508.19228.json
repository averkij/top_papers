[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.19228/assets/top_arch.png",
                "caption": "Figure 1:An overview of Token Order Prediction (TOP). Given an input token sequence, a vocabulary, a sequence length of 4 and window size of 4, a TOP target sequence is constructed via Algorithm1. The output hidden representation of the final layer goes to two separate unembedding heads for NTP and TOP. The final loss to optimize is a sum of the NTP and TOP loss.",
                "position": 54
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Motivation",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.19228/assets/mtp_loss.png",
                "caption": "Figure 2:Training loss of a small MTP transformer model with 16 MTP heads.",
                "position": 148
            }
        ]
    },
    {
        "header": "4Method",
        "images": []
    },
    {
        "header": "5Experiments and results",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Future Additions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]