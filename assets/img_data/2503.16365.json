[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16365/x1.png",
                "caption": "Figure 1:We presentJARVIS-VLA, a novel Vision-Language-Action (VLA) model trained withActVLPparadigm, post-trained on vision language tasks (non-decision-making tasks) before training on trajectory datasets to have better decision-making capabilities.",
                "position": 214
            },
            {
                "img": "https://arxiv.org/html/2503.16365/x2.png",
                "caption": "Figure 2:Previous VLA methods usually directly use imitation learning to finetune original vision-language models on large-scale multi-domain decision-making datasets to predict the actions[25,7]. OurActVLPtraining pipeline includes three stages: 1) post-training language models on text-only world knowledge with next-token prediction supervised fine-tuning, 2) post-training both vision encoder and language models on multimodal vision-language alignment and spatial grounding datasets with next-token prediction supervised fine-tuning, and 3) post-training only language models on multi-modal instruction following datasets with imitation learning.",
                "position": 231
            }
        ]
    },
    {
        "header": "2Learning to Act from Vision Language Post-Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16365/x3.png",
                "caption": "Figure 3:Illustration of various post-training datasets.Models can post-train on various vision-language datasets using a unified tokenizer and support diverse vision-language applications, such as question answering, image captioning, image/video question answering, visual grounding (including points and bounding box), and decision-making.\nMore examples can be found inAppendix D.",
                "position": 258
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16365/extracted/6297380/figures/avatar/oak_log.png",
                "caption": "",
                "position": 375
            },
            {
                "img": "https://arxiv.org/html/2503.16365/extracted/6297380/figures/avatar/grass2.png",
                "caption": "",
                "position": 375
            },
            {
                "img": "https://arxiv.org/html/2503.16365/extracted/6297380/figures/avatar/dirt.png",
                "caption": "",
                "position": 375
            },
            {
                "img": "https://arxiv.org/html/2503.16365/extracted/6297380/figures/avatar/obsidian.png",
                "caption": "",
                "position": 375
            },
            {
                "img": "https://arxiv.org/html/2503.16365/extracted/6297380/figures/avatar/iron_ore.png",
                "caption": "Table 1:Evaluation results of different policies on Minecraft tasks. Each group includes multiple tasks (at least 5), and the Avg. column reports the average success rate within each group. Qwen2-VL, Qwen2-VL (IL) andJARVIS-VLA-Qwen2-VL represent the training on the original qwen checkpoint, post-training on only large-scale imitation learning trajectories, and post-trained on VLP intermediate model. Qwen2-VL (ActVLP) achieves the highest success rates across all task groups. We testJARVIS-VLA-Qwen2-VL on 1k different instructions and the rollout videos can be found in the project page.",
                "position": 396
            },
            {
                "img": "https://arxiv.org/html/2503.16365/x4.png",
                "caption": "",
                "position": 415
            },
            {
                "img": "https://arxiv.org/html/2503.16365/x5.png",
                "caption": "",
                "position": 416
            },
            {
                "img": "https://arxiv.org/html/2503.16365/extracted/6297380/figures/avatar/crafting_table.png",
                "caption": "",
                "position": 419
            },
            {
                "img": "https://arxiv.org/html/2503.16365/extracted/6297380/figures/avatar/ladder.png",
                "caption": "",
                "position": 421
            },
            {
                "img": "https://arxiv.org/html/2503.16365/extracted/6297380/figures/avatar/cooked_beef.png",
                "caption": "",
                "position": 424
            },
            {
                "img": "https://arxiv.org/html/2503.16365/extracted/6297380/figures/avatar/iron_ingot1.jpg",
                "caption": "",
                "position": 425
            },
            {
                "img": "https://arxiv.org/html/2503.16365/x6.png",
                "caption": "Figure 4:Ablation results on different post-training datasets.We select knowledge datasets, visual question-answering datasets, and spatial grounding datasets to conduct ablation experiments. Our goal is to evaluate which capabilities and post-training datasets most significantly influence downstream decision-making tasks.",
                "position": 621
            },
            {
                "img": "https://arxiv.org/html/2503.16365/x7.png",
                "caption": "Figure 5:The relation between downstream task success rate, training loss, and training steps.The curve shows that scaling downstream finetuning trajectories can scale up the success rate when the loss is lower than 0.22.",
                "position": 626
            },
            {
                "img": "https://arxiv.org/html/2503.16365/x8.png",
                "caption": "Figure 6:The relationship between post-training loss and downstream task success rates.Our findings indicate that increasing the size of post-training non-trajectory datasets can significantly enhance downstream task success rates, even with a fixed number of fine-tuning trajectories.",
                "position": 652
            }
        ]
    },
    {
        "header": "4Related Works",
        "images": []
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AObservation and Action Space",
        "images": []
    },
    {
        "header": "Appendix BTraining Configurations",
        "images": []
    },
    {
        "header": "Appendix CDetails of Inference",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16365/x9.png",
                "caption": "",
                "position": 1532
            },
            {
                "img": "https://arxiv.org/html/2503.16365/extracted/6297380/figures/app/kill_zombie.png",
                "caption": "",
                "position": 1549
            },
            {
                "img": "https://arxiv.org/html/2503.16365/extracted/6297380/figures/app/craft_crafting_table.png",
                "caption": "",
                "position": 1552
            }
        ]
    },
    {
        "header": "Appendix DDatasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16365/x10.png",
                "caption": "",
                "position": 1646
            },
            {
                "img": "https://arxiv.org/html/2503.16365/extracted/6297380/figures/app/bbox1.jpg",
                "caption": "",
                "position": 1690
            },
            {
                "img": "https://arxiv.org/html/2503.16365/extracted/6297380/figures/app/bbox2.jpg",
                "caption": "",
                "position": 1695
            }
        ]
    },
    {
        "header": "Appendix EBenchmarks",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16365/x11.png",
                "caption": "Table 3:Summary of Vision Understanding Evaluation.",
                "position": 1865
            },
            {
                "img": "https://arxiv.org/html/2503.16365/x12.png",
                "caption": "",
                "position": 1935
            },
            {
                "img": "https://arxiv.org/html/2503.16365/x13.png",
                "caption": "",
                "position": 1962
            },
            {
                "img": "https://arxiv.org/html/2503.16365/x14.png",
                "caption": "",
                "position": 1989
            },
            {
                "img": "https://arxiv.org/html/2503.16365/x15.png",
                "caption": "",
                "position": 2016
            },
            {
                "img": "https://arxiv.org/html/2503.16365/x16.png",
                "caption": "",
                "position": 2039
            },
            {
                "img": "https://arxiv.org/html/2503.16365/x17.png",
                "caption": "Table 4:Summary of spatial grounding evaluation results for visual grounding tasks.",
                "position": 2055
            },
            {
                "img": "https://arxiv.org/html/2503.16365/x18.png",
                "caption": "",
                "position": 2125
            },
            {
                "img": "https://arxiv.org/html/2503.16365/extracted/6297380/figures/avatar/diamond_sword.png",
                "caption": "Table 6:Ablation experiments on base model and model structure. We adoptActVLPon Llava-Next-8B[27]and Qwen2-VL-7B[43]to validate the robustness across different base vision language models.",
                "position": 2290
            }
        ]
    },
    {
        "header": "Appendix FAblation with different Pre-trained VLMs",
        "images": []
    }
]