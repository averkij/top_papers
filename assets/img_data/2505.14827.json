[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14827/x1.png",
                "caption": "Figure 1:Comparison of the regular autoregressive generation pipeline (left) and our proposed Mixture of Inputs (MoI) strategy (right). In regular generation, only the discrete sampled token is fed back at each step, whereasMoIpreserves the full sampling distribution by computing a blended embeddingh=∑iwi⁢eiℎsubscript𝑖subscript𝑤𝑖subscript𝑒𝑖h=\\sum_{i}w_{i}e_{i}italic_h = ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT,\nwith weightswisubscript𝑤𝑖w_{i}italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTinterpolating embeddings{ei}i=1Vsuperscriptsubscriptsubscript𝑒𝑖𝑖1𝑉\\{e_{i}\\}_{i=1}^{V}{ italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT, letting the model consider several plausible tokens simultaneously within a single forward pass.",
                "position": 189
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methods: Mixture of Inputs",
        "images": []
    },
    {
        "header": "4Mixing Weight Estimation",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14827/x2.png",
                "caption": "",
                "position": 345
            }
        ]
    },
    {
        "header": "5Experimental Setup",
        "images": []
    },
    {
        "header": "6Main Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14827/x3.png",
                "caption": "Figure 2:Hyperparameter Importance Analysis.Comparison of three key hyperparameters (β𝛽\\betaitalic_βinMoI, top-p, and temperature) across four LLMs on two mathematical reasoning tasks.Left:Expected performance gain (%) when optimizing each hyperparameter individually through best-of-N-shots tuning. The graph showsβ𝛽\\betaitalic_βconsistently outperforms other parameters as N increases.Right:Relative feature importance derived from random forest regression analysis, confirmingβ𝛽\\betaitalic_β’s strong influence (0.41) on model performance compared to top-p (0.32) and temperature (0.27). These results demonstrate thatβ𝛽\\betaitalic_βis highly influential for effectively controlling input mixing during chain-of-thought reasoning.",
                "position": 730
            }
        ]
    },
    {
        "header": "7Analysis",
        "images": []
    },
    {
        "header": "8Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14827/x4.png",
                "caption": "",
                "position": 932
            }
        ]
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AComparing over Hyperparameter Grid Search",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14827/x5.png",
                "caption": "Figure 4:We show a comparison of distributions of evaluation results across the best top-p𝑝pitalic_pand temperature hyperparameter for baseline and withMoI. The results indicate strong performance gain brought by incorporating the sampling distribution in the generation process.",
                "position": 1568
            },
            {
                "img": "https://arxiv.org/html/2505.14827/x5.png",
                "caption": "",
                "position": 1571
            },
            {
                "img": "https://arxiv.org/html/2505.14827/x6.png",
                "caption": "",
                "position": 1576
            },
            {
                "img": "https://arxiv.org/html/2505.14827/x7.png",
                "caption": "Figure 5:We show a comparison of distributions of evaluation results across all top-p𝑝pitalic_pand temperature hyperparameters. The results indicate almost universal performance gain across average hyperparameter settings.",
                "position": 1582
            },
            {
                "img": "https://arxiv.org/html/2505.14827/x7.png",
                "caption": "",
                "position": 1585
            },
            {
                "img": "https://arxiv.org/html/2505.14827/x8.png",
                "caption": "",
                "position": 1590
            }
        ]
    },
    {
        "header": "Appendix BAdditional Setups",
        "images": []
    },
    {
        "header": "Appendix CHyperparameters",
        "images": []
    },
    {
        "header": "Appendix DImplementation Details",
        "images": []
    }
]