[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14827/x1.png",
                "caption": "Figure 1:Comparison of the regular autoregressive generation pipeline (left) and our proposed Mixture of Inputs (MoI) strategy (right). In regular generation, only the discrete sampled token is fed back at each step, whereasMoIpreserves the full sampling distribution by computing a blended embeddingh=âˆ‘iwiâ¢eiâ„subscriptğ‘–subscriptğ‘¤ğ‘–subscriptğ‘’ğ‘–h=\\sum_{i}w_{i}e_{i}italic_h = âˆ‘ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT,\nwith weightswisubscriptğ‘¤ğ‘–w_{i}italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTinterpolating embeddings{ei}i=1Vsuperscriptsubscriptsubscriptğ‘’ğ‘–ğ‘–1ğ‘‰\\{e_{i}\\}_{i=1}^{V}{ italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT, letting the model consider several plausible tokens simultaneously within a single forward pass.",
                "position": 189
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methods: Mixture of Inputs",
        "images": []
    },
    {
        "header": "4Mixing Weight Estimation",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14827/x2.png",
                "caption": "",
                "position": 345
            }
        ]
    },
    {
        "header": "5Experimental Setup",
        "images": []
    },
    {
        "header": "6Main Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14827/x3.png",
                "caption": "Figure 2:Hyperparameter Importance Analysis.Comparison of three key hyperparameters (Î²ğ›½\\betaitalic_Î²inMoI, top-p, and temperature) across four LLMs on two mathematical reasoning tasks.Left:Expected performance gain (%) when optimizing each hyperparameter individually through best-of-N-shots tuning. The graph showsÎ²ğ›½\\betaitalic_Î²consistently outperforms other parameters as N increases.Right:Relative feature importance derived from random forest regression analysis, confirmingÎ²ğ›½\\betaitalic_Î²â€™s strong influence (0.41) on model performance compared to top-p (0.32) and temperature (0.27). These results demonstrate thatÎ²ğ›½\\betaitalic_Î²is highly influential for effectively controlling input mixing during chain-of-thought reasoning.",
                "position": 730
            }
        ]
    },
    {
        "header": "7Analysis",
        "images": []
    },
    {
        "header": "8Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14827/x4.png",
                "caption": "",
                "position": 932
            }
        ]
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AComparing over Hyperparameter Grid Search",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14827/x5.png",
                "caption": "Figure 4:We show a comparison of distributions of evaluation results across the best top-pğ‘pitalic_pand temperature hyperparameter for baseline and withMoI. The results indicate strong performance gain brought by incorporating the sampling distribution in the generation process.",
                "position": 1568
            },
            {
                "img": "https://arxiv.org/html/2505.14827/x5.png",
                "caption": "",
                "position": 1571
            },
            {
                "img": "https://arxiv.org/html/2505.14827/x6.png",
                "caption": "",
                "position": 1576
            },
            {
                "img": "https://arxiv.org/html/2505.14827/x7.png",
                "caption": "Figure 5:We show a comparison of distributions of evaluation results across all top-pğ‘pitalic_pand temperature hyperparameters. The results indicate almost universal performance gain across average hyperparameter settings.",
                "position": 1582
            },
            {
                "img": "https://arxiv.org/html/2505.14827/x7.png",
                "caption": "",
                "position": 1585
            },
            {
                "img": "https://arxiv.org/html/2505.14827/x8.png",
                "caption": "",
                "position": 1590
            }
        ]
    },
    {
        "header": "Appendix BAdditional Setups",
        "images": []
    },
    {
        "header": "Appendix CHyperparameters",
        "images": []
    },
    {
        "header": "Appendix DImplementation Details",
        "images": []
    }
]