[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11512/x1.png",
                "caption": "Figure 1:Overview ofLikePhys.Our intuition is that a video diffusion model with a well learned underlying physics distribution should assign higher likelihoods to valid samples that obey physics laws and lower likelihoods to those invalid samples that violate them. We use Blender to create valid and invalid sample pairs through controlled physics parameters over multiple physics domains and scenarios (left). We then compare the diffusion model likelihood estimates over the constructed dataset to extract a quantitative intuitive physics understanding measure, the Plausibility Preference Error (PPE) (middle). Hence, we can compute a ranking of average PPE across pre-trained video diffusion models, that correlates with human preference. Lower values indicate stronger intuitive physics understanding (right).",
                "position": 118
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11512/image/VDMPhysEvalMain.png",
                "caption": "Figure 2:Method Overview.We prepare groups of videos via physics simulations with valid samples obeying physical laws and invalid samples containing deliberate violations. We then inject Gaussian noise into these videos and use a diffusion model to predict the noise and compute the denoising loss. For each validâ€“invalid pair, we compute a likelihood preference ratio that quantifies how the model favors physically plausible sequences, serving as a proxy for physics understanding.",
                "position": 185
            },
            {
                "img": "https://arxiv.org/html/2510.11512/image/vdmphydataset.png",
                "caption": "Figure 3:Evaluation benchmark.We organise 12 scenarios derived from four physical domains (Rigid Body Mechanics, Fluid Mechanics, Continuum Mechanics, Optical Effects) with their relative proportions (left).\nRows from top to bottom (middle and right) show examples from four optical phenomena, rigid-body mechanics, continuum mechanics, and fluid mechanics. We display valid simulation (middle) and corresponding invalid variants (right) for physics violation in each domain.",
                "position": 258
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11512/x2.png",
                "caption": "Figure 4:Analysis on influencing factors.Effect of model size, showing a steady overall decline in PPE(top left).\nEffect of training data size, where larger corpora generally yield lower error, though the correlation is weaker than that of model size(bottom left).\nEffect of context window size, showing consistent improvement in physics understanding as the window increases(top right).\nEffect of classifier-free guidance (CFG) strength, indicating that physics understanding remains largely stable across different strengths(bottom right).",
                "position": 797
            },
            {
                "img": "https://arxiv.org/html/2510.11512/x3.png",
                "caption": "Figure 5:Analysis across physics domains and laws.On the left, we report detailed PPE across the four physics domains. Fluid Mechanics cases exhibit both the highest average error, while Rigid Body and Continuum Mechanics scenarios show moderate errors; Optical Effects cases lie in between.\nOn the right, we map our domains to seven physics laws for a fine-grained analysis. Temporal continuity and conservation of energy show wide variation and higher median errors, whereas geometric invariance and optical consistency are handled more reliably.",
                "position": 822
            }
        ]
    },
    {
        "header": "5Discussion and Limitation",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "LLM Usage",
        "images": []
    },
    {
        "header": "Appendix ADetails on Evaluation Protocol",
        "images": []
    },
    {
        "header": "Appendix BDetails on Benchmark Dataset Generation",
        "images": []
    },
    {
        "header": "Appendix CAdditional Details of Inference Settings",
        "images": []
    },
    {
        "header": "Appendix DAdditional Experimental Setting Details",
        "images": []
    },
    {
        "header": "Appendix EAblation on Evaluation Protocol Design",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11512/image/normalized_loss_diff_2x2_avg.png",
                "caption": "Figure 6:Ablation on Evaluation Protocol Design Factors.Left: Denoising loss differences between valid and invalid sample pairs across 1000 uniformly sampled timesteps for four representative physics scenarios.Right: PPE across multiple text-prompt variants, demonstrating robustness to prompt choice.",
                "position": 2343
            },
            {
                "img": "https://arxiv.org/html/2510.11512/image/normalized_loss_diff_2x2_avg.png",
                "caption": "",
                "position": 2346
            },
            {
                "img": "https://arxiv.org/html/2510.11512/image/misrank_all_categories_by_model_color.png",
                "caption": "",
                "position": 2350
            }
        ]
    },
    {
        "header": "Appendix FApplicability to Other Intuitive Physics Benchmarks",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11512/x4.png",
                "caption": "Figure 7:Visual samples from Ball Drop, Ball Collision, Pendulum Oscillation, and Block Slide scenarios.",
                "position": 2456
            },
            {
                "img": "https://arxiv.org/html/2510.11512/x5.png",
                "caption": "Figure 8:Visual sample from Droplet Fall, Faucet Flow, Cloth Drape, Cloth waving scenarios.",
                "position": 2459
            },
            {
                "img": "https://arxiv.org/html/2510.11512/x6.png",
                "caption": "Figure 9:Visual sample from River Flow, Moving Shadow, Pyramid Impact, Orbit Shadow scenario.",
                "position": 2463
            }
        ]
    },
    {
        "header": "Appendix GVisual Samples",
        "images": []
    }
]