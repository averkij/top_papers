[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07242/x1.png",
                "caption": "Figure 1:Comparison of reward signals from different supervision sources.Reward Models (a) provide smooth but sometimes misaligned scores, occasionally assigning high values to incorrect responses and low values to correct ones. Rule-based rewards (b) enforce a strict binary (0â€“1) boundary: they rarely give false positives, but due to their stringent criteria, many predictions that are actually correct receive a reward of 0 simply because they fail to pass the rule. HERO (c) uses the rule as a gate, which significantly reduces false positives. At the same time, by integrating the reward model signal, HERO assigns higher reward scores to those cases that would have been false negatives under (b), resulting in more accurate and informative supervision.",
                "position": 182
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07242/x2.png",
                "caption": "Figure 2:(a) Impact of using positive and negative dense ranges.Dense negative rewards contribute more to stable learning than positive samples.(b) Effect of varying reward ranges under different training regimes.Smaller ranges perform best on verifiable tasks, while larger ranges benefit mixed settings by providing denser feedback.",
                "position": 813
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07242/x3.png",
                "caption": "Figure 5:Reward model qualification ability on mixed groups: (a) distribution of AUROC scores, (b) AUROC box plot, (c) cumulative distribution of AUROC, and (d) AUROC performance categories.",
                "position": 2038
            },
            {
                "img": "https://arxiv.org/html/2510.07242/fig/reward_analysis.png",
                "caption": "Figure 6:Reward model qualification ability on mixed groups: (a) distribution of AUROC scores, (b) AUROC box plot, (c) cumulative distribution of AUROC, and (d) AUROC performance categories.",
                "position": 2056
            }
        ]
    },
    {
        "header": "Appendix BQualitative analysis",
        "images": []
    }
]