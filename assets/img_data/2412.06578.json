[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06578/x1.png",
                "caption": "",
                "position": 77
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3MoViE",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06578/x2.png",
                "caption": "Figure 2:Multimodal Guidance Distillation Overview: Standard classifier-free guidance inference pipeline (left) with two input conditionings (image and text) requires three inference runs per diffusion step. Our distilled pipeline (right) incorporates guidance scalessIsubscriptùë†ùêºs_{I}italic_s start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPTandsTsubscriptùë†ùëás_{T}italic_s start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPTinto UNet and only performs one inference run.",
                "position": 282
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06578/x3.png",
                "caption": "Figure 3:Adversarial Distillation: We distill a multi-step teacher into a single step student using adversarial losses. Unlike existing adversarial distillation approaches[51,67]that forego guidance flexibility for faster sampling, we preserve guidance strength property during adversarial training by providing the synthetic latentxtsubscriptùë•ùë°x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTfrom teacher‚Äôs denoising process and conditioning the student on the corresponding guidance scales.",
                "position": 382
            },
            {
                "img": "https://arxiv.org/html/2412.06578/x4.png",
                "caption": "Figure 4:MoViE at text guidance[4.0,8.0,12.0]4.08.012.0[4.0,8.0,12.0][ 4.0 , 8.0 , 12.0 ]and image guidance[1.25,1.75]1.251.75[1.25,1.75][ 1.25 , 1.75 ]. Our adversarial training maintains guidance scales, allowing us to control edit strength during inference. (Prompt: In Van Gogh Style)",
                "position": 386
            },
            {
                "img": "https://arxiv.org/html/2412.06578/x5.png",
                "caption": "Figure 5:CLIP metrics for InstructPix2Pix, Mobile-Pix2pix, Multi-modal Guidance (MMG) Mobile-Pix2pix and MoViE. As shown in the graphs, proposed optimizations improve the efficiency greatly with minimum quality drop.",
                "position": 399
            },
            {
                "img": "https://arxiv.org/html/2412.06578/x6.png",
                "caption": "Figure 6:Qualitative results of MoViE on DAVIS. Our method can handle complex global edits as well as perform more nuanced attribute editing while requiring very few computational resources. Please refer to the Appendix for video results.",
                "position": 533
            },
            {
                "img": "https://arxiv.org/html/2412.06578/x7.png",
                "caption": "Figure 7:CLIP metrics for different autoencoder configurations. A substational FLOPs reduction can be achieved by incorporating TAESD, with minimal drop in editing performance.",
                "position": 536
            },
            {
                "img": "https://arxiv.org/html/2412.06578/x8.png",
                "caption": "Figure 8:Qualitative comparison of our method to the base model. The efficiency is greatly improved whereas quality is not compromised both for style transfer and attribute edits. Please refer to the Appendix for video results.",
                "position": 540
            },
            {
                "img": "https://arxiv.org/html/2412.06578/x9.png",
                "caption": "Figure 9:Qualitative comparison of our method MoViE to other SOTA video editing algorithms. We evaluate on two challenging editing scenarios. MoViE produces good quality edits yet far outperforms other competing methods in terms of efficiency. Please refer to the Appendix for video results.",
                "position": 543
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional results",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06578/x10.png",
                "caption": "Figure 10:Human Evaluation results comparing MoViE to TokenFlow[11], InsV2V[7], and Rerender-A-Video[63].",
                "position": 1517
            },
            {
                "img": "https://arxiv.org/html/2412.06578/x11.png",
                "caption": "Figure 11:Adversarial Head Architecture.",
                "position": 1530
            }
        ]
    },
    {
        "header": "Appendix BTraining Details",
        "images": []
    }
]