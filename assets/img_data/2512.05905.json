[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05905/x1.png",
                "caption": "",
                "position": 87
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05905/x2.png",
                "caption": "Figure 2:Overview of the proposed 3D-consistent pose. For scaling implementation, we take the clavicle or the pelvis as the central reference, applying scaling from proximal to distal along each limb in bones set‚Ñ¨\\mathcal{B}.Aug‚Äã(‚ãÖ)\\textit{Aug}(\\cdot)denotes augmentation in training,Ret‚Äã(‚ãÖ)\\textit{Ret}(\\cdot)denotes retargeting in inference, andùí´ref={Pjref‚à£1‚â§j‚â§N}\\mathcal{P}^{\\text{ref}}=\\{\\text{P}^{\\text{ref}}_{j}\\mid 1\\leq j\\leq N\\}denotesNNestimated 2D keypoints in the reference image.\nWe further incorporate hand and face controls by overlaying 2D hand and face keypoints onto the rendered sequences, and align them with the projection of 3D joints during augmentation or retargeting. For better clarity, we omit the drawing process of 2D hand and face in the figure.",
                "position": 128
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05905/x3.png",
                "caption": "Figure 3:Overview ofSCAIL‚Äôs model architecture.SCAILbuilds upon I2V model and incorporate pose control as an explicit context for the model to learn spatial-temporal motion. To accommodate to the training setting where reference image and video input are sampled from different parts of the video, we modify the I2V model‚Äôs input structure by concatenating the reference image at the beginning of the sequence and initiating generation fromT=1T=1, using the original I2V pattern to inject the reference CLIP feature. To help the model better distinguish the conditional tokens and the noisy video sequence, we leverage the original mask mechanisim of Wan-I2V model architecture, applying an all-one mask for the reference image and the driving sequence, and an all-zero mask for the noisy video sequence.",
                "position": 187
            },
            {
                "img": "https://arxiv.org/html/2512.05905/x4.png",
                "caption": "Figure 4:Exploration of different strategies for pose injection.",
                "position": 239
            },
            {
                "img": "https://arxiv.org/html/2512.05905/x5.png",
                "caption": "Figure 5:The data curation pipeline. We perform character filtering and motion-speed filtering to construct high-quality training data.",
                "position": 242
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05905/x6.png",
                "caption": "Figure 6:User study for comparing our model with popular community and commercial projects.",
                "position": 397
            },
            {
                "img": "https://arxiv.org/html/2512.05905/x7.png",
                "caption": "Figure 7:Qualitative comparison for single-character animation. Rendered pose in Cross-Driven Animation are omitted for clarity. Zoom in for better visualization.",
                "position": 416
            },
            {
                "img": "https://arxiv.org/html/2512.05905/x8.png",
                "caption": "Figure 8:Qualitative comparison for dual-character animation. Zoom in for better visualization.",
                "position": 419
            },
            {
                "img": "https://arxiv.org/html/2512.05905/x9.png",
                "caption": "Figure 9:Visualization of ablation study.",
                "position": 422
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "A1Details on Pose Conditioning",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05905/x10.png",
                "caption": "Figure A1:Ablation studies on pose representation. Anomalies in the human body or deviations from correct posture are boxed.",
                "position": 597
            }
        ]
    },
    {
        "header": "A2More Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05905/x11.png",
                "caption": "Figure A2:Ablation studies on pose retargeting. 2D Retarget are visualized for comparison. Regions where the body proportions deviate from the reference image are boxed.",
                "position": 627
            },
            {
                "img": "https://arxiv.org/html/2512.05905/x12.png",
                "caption": "Figure A3:User study results of ablation on 3D Augmentation.",
                "position": 680
            },
            {
                "img": "https://arxiv.org/html/2512.05905/x13.png",
                "caption": "Figure A4:Qualitative results of ablation on 3D Augmentation.",
                "position": 683
            }
        ]
    },
    {
        "header": "A3Details on Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05905/x14.png",
                "caption": "Figure A5:Visualization of the distribution of data annotations inStudio-Bench. We categorize and annotate videos based on motion types. The annotation of a single video can contain multiple tags, such as ‚Äùturning‚Äù and ‚Äùballet‚Äù.",
                "position": 700
            },
            {
                "img": "https://arxiv.org/html/2512.05905/x15.png",
                "caption": "Figure A6:Comparison of model‚Äôs ability to preserve body structure for non-standard character figures.",
                "position": 732
            },
            {
                "img": "https://arxiv.org/html/2512.05905/x16.png",
                "caption": "Figure A7:Visualization of our model‚Äôs performance under both high-dynamic motion and non-standard character figures.",
                "position": 735
            },
            {
                "img": "https://arxiv.org/html/2512.05905/x17.png",
                "caption": "Figure A8:Visualization of our model‚Äôs performance underreverse drivingsettings.",
                "position": 738
            }
        ]
    },
    {
        "header": "A4Discussion",
        "images": []
    }
]