[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminary: Peak Memory Cost during Backpropagation",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03077/x1.png",
                "caption": "Figure 1:Memory profile during backpropagation of Qwen 3-4B with gradient checkpointing, visualized using PyTorch‚Äôs memory profiler. Sequence length is set to 8192. Optimizer states are excluded as we focus on the BP process.",
                "position": 129
            }
        ]
    },
    {
        "header": "3Memory-Efficient Exact Stream Backpropagation",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03077/x2.png",
                "caption": "Figure 2:StreamBP for transformer layer (best view in color). The stored activations of StreamBP is highlighted inorange.\nUnlike the gradient checkpointing approach that reforwardHinsubscriptùêªinH_{\\text{in}}italic_H start_POSTSUBSCRIPT in end_POSTSUBSCRIPTto compute all the activatioins required for the backward ofHoutsubscriptùêªoutH_{\\text{out}}italic_H start_POSTSUBSCRIPT out end_POSTSUBSCRIPT, StreamBP computes the activations only for one partition ofHout(i)superscriptsubscriptùêªoutùëñH_{\\text{out}}^{(i)}italic_H start_POSTSUBSCRIPT out end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPTat a time, which reduces the memory cost by a large margin.",
                "position": 449
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03077/x3.png",
                "caption": "(a)Qwen 3-8B",
                "position": 545
            },
            {
                "img": "https://arxiv.org/html/2506.03077/x3.png",
                "caption": "(a)Qwen 3-8B",
                "position": 548
            },
            {
                "img": "https://arxiv.org/html/2506.03077/x4.png",
                "caption": "(b)Qwen 3-14B",
                "position": 553
            },
            {
                "img": "https://arxiv.org/html/2506.03077/x5.png",
                "caption": "(c)Qwen 3-32B (LoRA Grad.)",
                "position": 558
            },
            {
                "img": "https://arxiv.org/html/2506.03077/x6.png",
                "caption": "",
                "position": 696
            },
            {
                "img": "https://arxiv.org/html/2506.03077/x7.png",
                "caption": "(a)Time v.s. sequence length.",
                "position": 914
            },
            {
                "img": "https://arxiv.org/html/2506.03077/x7.png",
                "caption": "(a)Time v.s. sequence length.",
                "position": 917
            },
            {
                "img": "https://arxiv.org/html/2506.03077/x8.png",
                "caption": "(b)Memory v.s. sequence length.",
                "position": 922
            }
        ]
    },
    {
        "header": "5Conclusion and Discussions on Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Details of Stream Backpropagation",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03077/x9.png",
                "caption": "Figure 6:Design of distributed StreamBP. When backward through a transformer layer, its parameter is gathered beforehand. During the StreamBP of the layer, no gradient or parameter communication is fired. When BP of the layer is finished, the gradient will be reduced across layers and the paramater will be sharded across GPUs.",
                "position": 1680
            }
        ]
    },
    {
        "header": "Appendix BAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03077/x10.png",
                "caption": "Figure 7:Peak BP memory cost measurement of Qwen 3-8B with LoRA (rank=32) on a single RTX3090-24GB GPU",
                "position": 1769
            }
        ]
    },
    {
        "header": "Appendix CAnalysis of Memory Profile",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03077/x11.png",
                "caption": "Figure 8:Memory profile with StreamBP under the same settings asFigure1. The partition size of logits and transformer layer are set to 100 and 500, respectively.",
                "position": 1795
            }
        ]
    },
    {
        "header": "Appendix DExperiment Setup",
        "images": []
    }
]