[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11690/x1.png",
                "caption": "Figure 1:Representation Autoencoder(RAE) uses frozen pretrained representations as the encoder with a lightweight decoder to reconstruct input images without compression. RAE enables faster convergence and higher-quality samples in latent diffusion training compared to VAE-based models.",
                "position": 146
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11690/x2.png",
                "caption": "Figure 2:Comparison of SD-VAE and RAE (DINOv2-B).The VAE relies on convolutional backbones with aggressive down- and up-sampling, while the RAE uses a ViT architecturewithoutcompression. SD-VAE is also more computationally expensive, requiring about 6× and 3× more GFLOPs than RAE for the encoder and decoder, respectively. GFlops are evaluated on one256×256256\\times 256image.",
                "position": 174
            }
        ]
    },
    {
        "header": "2Related works",
        "images": []
    },
    {
        "header": "3High Fidelity Reconstruction from Frozen Encoders",
        "images": []
    },
    {
        "header": "4Taming Diffusion Transformers for RAE",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11690/x3.png",
                "caption": "Figure 3:Overfitting to a single sample.Left: increasing model width lead to lower loss and better sample quality; Right: changing model depth has marginal effect on overfitting results.",
                "position": 434
            },
            {
                "img": "https://arxiv.org/html/2510.11690/x4.png",
                "caption": "",
                "position": 443
            },
            {
                "img": "https://arxiv.org/html/2510.11690/x5.png",
                "caption": "Figure 4:DiT w/ RAE reaches much faster convergence and better FID than SiT or REPA.",
                "position": 667
            }
        ]
    },
    {
        "header": "5Improving the Model Scalability with Wide Diffusion Head",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11690/x6.png",
                "caption": "Figure 5:The Wide DDT Head.",
                "position": 682
            },
            {
                "img": "https://arxiv.org/html/2510.11690/x7.png",
                "caption": "(a)DiTDH\\text{DiT}^{\\text{DH}}scales much better than DiT with RAE latents.",
                "position": 705
            },
            {
                "img": "https://arxiv.org/html/2510.11690/x7.png",
                "caption": "(a)DiTDH\\text{DiT}^{\\text{DH}}scales much better than DiT with RAE latents.",
                "position": 708
            },
            {
                "img": "https://arxiv.org/html/2510.11690/x8.png",
                "caption": "(b)DiTDH\\text{DiT}^{\\text{DH}}with RAE converges faster than VAE-based methods.",
                "position": 719
            },
            {
                "img": "https://arxiv.org/html/2510.11690/x9.png",
                "caption": "(c)DiTDH\\text{DiT}^{\\text{DH}}with RAE reaches better FID than VAE-based methods at all model scales. Bubble area indicates the flops of the model.",
                "position": 731
            },
            {
                "img": "https://arxiv.org/html/2510.11690/assets/superimage-class-89.png",
                "caption": "Figure 7:Qualitative samplesfrom our model trained at512×512512\\times 512resolution with AutoGuidance. The RAE-based DiT demonstrates strong diversity, fine-grained detail, and high visual quality.",
                "position": 1230
            },
            {
                "img": "https://arxiv.org/html/2510.11690/assets/superimage-class-33.png",
                "caption": "",
                "position": 1239
            },
            {
                "img": "https://arxiv.org/html/2510.11690/assets/superimage-class-437.png",
                "caption": "",
                "position": 1244
            },
            {
                "img": "https://arxiv.org/html/2510.11690/assets/superimage-class-933.png",
                "caption": "",
                "position": 1250
            }
        ]
    },
    {
        "header": "6Discussions",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Acknowlegments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExtended Related Work",
        "images": []
    },
    {
        "header": "Appendix BProofs",
        "images": []
    },
    {
        "header": "Appendix CRAE Implementation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11690/x10.png",
                "caption": "Figure 8:Reconstruction examples.From left to right: input image, RAE (DINOv2-B), RAE (SigLIP2-B), RAE (MAE-B), SD-VAE. Zoom in for details.",
                "position": 3164
            }
        ]
    },
    {
        "header": "Appendix DDiffusion Model Implementation",
        "images": []
    },
    {
        "header": "Appendix ESampling details for FID Evaluation",
        "images": []
    },
    {
        "header": "Appendix FTheory Experiment Setup",
        "images": []
    },
    {
        "header": "Appendix GAdditional Ablation Studies",
        "images": []
    },
    {
        "header": "Appendix HAdditional Scaling Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11690/x11.png",
                "caption": "Figure 9:Training loss ofDiTDH\\text{DiT}^{\\text{DH}}on DINOv2-B.We use an EMA weight of 0.9 to smooth the loss.",
                "position": 3657
            }
        ]
    },
    {
        "header": "Appendix IGuidance",
        "images": []
    },
    {
        "header": "Appendix JDescriptions for Flow-based Models",
        "images": []
    },
    {
        "header": "Appendix KEvaluation Details",
        "images": []
    },
    {
        "header": "Appendix LUnconditional Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11690/x12.png",
                "caption": "Figure 10:Uncurated512×512512\\times 512DiTDH\\text{DiT}^{\\text{DH}}-XL samples.AutoGudance Scale = 1.5Class label = ”golden retriever” (207)",
                "position": 3811
            },
            {
                "img": "https://arxiv.org/html/2510.11690/x12.png",
                "caption": "Figure 10:Uncurated512×512512\\times 512DiTDH\\text{DiT}^{\\text{DH}}-XL samples.AutoGudance Scale = 1.5Class label = ”golden retriever” (207)",
                "position": 3814
            },
            {
                "img": "https://arxiv.org/html/2510.11690/x13.png",
                "caption": "Figure 11:Uncurated512×512512\\times 512DiTDH\\text{DiT}^{\\text{DH}}-XL samples.AutoGudance Scale = 1.5Class label = ”husky” (250)",
                "position": 3822
            },
            {
                "img": "https://arxiv.org/html/2510.11690/x14.png",
                "caption": "Figure 12:Uncurated512×512512\\times 512DiTDH\\text{DiT}^{\\text{DH}}-XL samples.AutoGudance Scale = 1.5Class label = ”cliff” (972)",
                "position": 3831
            },
            {
                "img": "https://arxiv.org/html/2510.11690/x14.png",
                "caption": "Figure 12:Uncurated512×512512\\times 512DiTDH\\text{DiT}^{\\text{DH}}-XL samples.AutoGudance Scale = 1.5Class label = ”cliff” (972)",
                "position": 3834
            },
            {
                "img": "https://arxiv.org/html/2510.11690/x15.png",
                "caption": "Figure 13:Uncurated512×512512\\times 512DiTDH\\text{DiT}^{\\text{DH}}-XL samples.AutoGudance Scale = 1.5Class label = ”macaw” (88)",
                "position": 3842
            },
            {
                "img": "https://arxiv.org/html/2510.11690/x16.png",
                "caption": "Figure 14:Uncurated512×512512\\times 512DiTDH\\text{DiT}^{\\text{DH}}-XL samples.AutoGudance Scale = 1.5Class label = ”arctic fox” (279)",
                "position": 3851
            },
            {
                "img": "https://arxiv.org/html/2510.11690/x16.png",
                "caption": "Figure 14:Uncurated512×512512\\times 512DiTDH\\text{DiT}^{\\text{DH}}-XL samples.AutoGudance Scale = 1.5Class label = ”arctic fox” (279)",
                "position": 3854
            },
            {
                "img": "https://arxiv.org/html/2510.11690/x17.png",
                "caption": "Figure 15:Uncurated512×512512\\times 512DiTDH\\text{DiT}^{\\text{DH}}-XL samples.AutoGudance Scale = 1.5Class label = ”balloon” (417)",
                "position": 3862
            }
        ]
    },
    {
        "header": "Appendix MVisual Results",
        "images": []
    }
]