[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15185/x1.png",
                "caption": "Figure 1:Illustration of three properties of LlamaGen-B model.(a) Attention map from the last layer, highlighting the current token in red and tokens with larger attention weights in yellow. (b) Linear probing results on features from the6​-​t​h6\\text{-}thlayer at 8 uniformly selected steps. (c) Visual token indices from two slightly different views of the same image.",
                "position": 99
            },
            {
                "img": "https://arxiv.org/html/2509.15185/x2.png",
                "caption": "",
                "position": 109
            },
            {
                "img": "https://arxiv.org/html/2509.15185/x3.png",
                "caption": "",
                "position": 115
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15185/x4.png",
                "caption": "Figure 2:Attention maps of LlamaGen-B across layers and steps.These attention maps consistently show that conditional and spatially adjacent tokens receive the highest attention weights, while other tokens have significantly lower weights.",
                "position": 245
            },
            {
                "img": "https://arxiv.org/html/2509.15185/x5.png",
                "caption": "Figure 3:Overview of Self-Guided Training Pipeline.We incorporate masked image modeling (ℒMIM\\mathcal{L}_{\\text{MIM}}) to expand the effective field of visual autoregressive models. Additionally, we introduce inter-step contrastive learning (ℒs​t​e​p\\mathcal{L}_{step}) to ensure global consistency, as well as inter-view contrastive learning (ℒv​i​e​w\\mathcal{L}_{view}) for consistency in visual representations.",
                "position": 289
            },
            {
                "img": "https://arxiv.org/html/2509.15185/x6.png",
                "caption": "Figure 4:Linear probing results of LlamaGen-B and our ST-AR.Our method demonstrates consistent improvements in image understanding.",
                "position": 414
            },
            {
                "img": "https://arxiv.org/html/2509.15185/x6.png",
                "caption": "Figure 4:Linear probing results of LlamaGen-B and our ST-AR.Our method demonstrates consistent improvements in image understanding.",
                "position": 416
            },
            {
                "img": "https://arxiv.org/html/2509.15185/x7.png",
                "caption": "Figure 5:Attention maps of LlamaGen-B model trained with our ST-AR method.We utilize features from the final transformer layer, selecting random steps to draw attention maps. These maps exhibit an expanded effective receptive field, moving beyond mere focus on spatially adjacent and conditional tokens, and reveal distinct semantic patterns.",
                "position": 929
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]