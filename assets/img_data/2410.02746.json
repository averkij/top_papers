[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02746/extracted/5897759/figures/cloc_overview.png",
                "caption": "Figure 1:Overview of ourCLOCpre-training framework.(1)A visually-enriched and spatially-localized captioning pipeline generates pseudo-labeled bounding boxes with detailed descriptions for key image regions.(2)A lightweightPrompterattached on top of the CLIP image encoder can be prompted to transform the image embedding into the region-focused feature. All parameters are trained end-to-end from scratch with our contrastive localized language-image loss on the annotated region-text datasets. After pre-training,(3a)region features can be generated via thePrompterfor region-text tasks like object classification in a training-free fashion.(3b)The image encoder, along with the optionalPrompter, can also strengthen MLLMs fine-tuning by enhancing their fine-grained image understanding capabilities.",
                "position": 132
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3CLOC: Contrastive Localized Language-Image Pre-Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02746/extracted/5897759/figures/cloc_arch.png",
                "caption": "Figure 2:CLOCpromptable embedding architecture.CLOCbuilds upon the image embedding from CLIP (before pooling and projection) and transforms it into a region-aware vision embedding given an encoded prompt;e.g., positional encodings of box coordinates or regional caption encoded by the CLIP text encoder.",
                "position": 264
            }
        ]
    },
    {
        "header": "4Visually-Enriched and Spatially-Localized Captioning Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02746/extracted/5897759/figures/cloc_vesl.png",
                "caption": "Figure 3:Overview of the Visually-Enriched and Spatially-Localized (VESL) captioning pipeline.We leverage an existing open-vocabulary detector (e.g., OWLv2) that typically predicts bounding boxes on the images, and assigns the labels from the given text phrase candidates. Previous methods do not tailor how the text phrases are prepared and often use the alt-text attached to the images, which is prone to insufficient region descriptions. We found it crucial forCLOCto train on data from ourVESLthat re-captions images with the visually-enriched captioner VeCap(Lai et¬†al.,2024)for better visual concept exploitation of the detector.",
                "position": 362
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AExperiment Details",
        "images": []
    },
    {
        "header": "Appendix BVESLData Engine",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02746/extracted/5897759/figures/cloc_vis.png",
                "caption": "Figure A:Examples comparing ourVESLand the labeling approach inMinderer et¬†al. (2024)that directly uses thenùëõnitalic_n-grams of the crawled alt-text. ForVESL, each image is annotated with the visual-enriched caption to replace the alt-text, which is used to generate region text candidates that capture the image content better.",
                "position": 2361
            }
        ]
    },
    {
        "header": "Appendix CMore Discussions",
        "images": []
    }
]