[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.10647/x1.png",
                "caption": "",
                "position": 218
            },
            {
                "img": "https://arxiv.org/html/2511.10647/x2.png",
                "caption": "Figure 1:Given any number of images and optional camera poses,Depth Anything 3reconstructs the visual space, producing consistent depth and ray maps that can be fused into accurate point clouds, resulting in high-fidelity 3D Gaussians and geometry.\nIt significantlyoutperforms VGGT in multi-view geometry and pose accuracy; with monocular inputs, it also surpasses Depth Anything 2 while matching its detail and robustness.",
                "position": 286
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.10647/figs/pdfs/pipeline.png",
                "caption": "Figure 2:Pipeline of Depth Anything 3.Depth Anything 3 employs a single transformer (vanilla DINOv2 model) without any architectural modifications. To enable cross-view reasoning, an input-adaptive cross-view self-attention mechanism is introduced. A dual-DPT head is used to predict depth and ray maps from visual tokens.\nCamera parameters, if available, are encoded as camera tokens and concatenated with patch tokens, participating in all attention operations.",
                "position": 355
            }
        ]
    },
    {
        "header": "3Depth Anything 3",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.10647/x3.png",
                "caption": "Figure 3:Dual-DPT Head.Two branchs share reassembly modules for better outputs alignment.",
                "position": 460
            }
        ]
    },
    {
        "header": "4Teacher-Student Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.10647/x4.png",
                "caption": "Figure 4:Poor quality real-world datasets.We show some examples of the poor quality real-world datasets.",
                "position": 729
            }
        ]
    },
    {
        "header": "5Application: Feed-Forward 3D Gaussian Splattings",
        "images": []
    },
    {
        "header": "6Visual Geometry Benchmark",
        "images": []
    },
    {
        "header": "7Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.10647/figs/pngs/vis_traj.png",
                "caption": "Figure 5:Comparisons of pose estimation quality.Camera trajectories for two videos are shown. Ground-truth trajectories are derived using COLMAP on images with dynamic objects masked.",
                "position": 1126
            },
            {
                "img": "https://arxiv.org/html/2511.10647/x5.png",
                "caption": "Figure 6:Comparisons of point cloud quality.Our model produces point clouds that are more geometrically regular and substantially less noisy than those generated by other methods.",
                "position": 1303
            },
            {
                "img": "https://arxiv.org/html/2511.10647/figs/pngs/vis_depth.png",
                "caption": "Figure 7:Comparisons of depth quality.Compared with other methods, our depth maps exhibit finer structural detail and higher semantic correctness across diverse scenes.",
                "position": 1372
            },
            {
                "img": "https://arxiv.org/html/2511.10647/x6.png",
                "caption": "Figure 8:Comparison of teacher-label supervision.Supervision with teacher-generated labels yields depth maps with substantially richer detail and finer structures.",
                "position": 1864
            },
            {
                "img": "https://arxiv.org/html/2511.10647/x7.png",
                "caption": "Figure 9:Visualizations of camera pose and depth estimation on in-the-wild scenes.",
                "position": 1939
            },
            {
                "img": "https://arxiv.org/html/2511.10647/x8.png",
                "caption": "Figure 10:Effectiveness of Teacher model for supervising metric depth estimation.Incorporating Teacher model for supervision significantly improves the metric depth sharpness.",
                "position": 2155
            },
            {
                "img": "https://arxiv.org/html/2511.10647/x9.png",
                "caption": "Figure 11:Qualitative comparisons with state-of-the-art methods for visual rendering.The first column shows the selected input views, while the remaining columns display novel views rendered by comparison models and ground truth. For each scene, two rendered novel viewpoints are presented in consecutive rows. The first three scenes are from DL3DV, the following two are from Tanks and Temples, and the last three are from MegaDepth. Compared to other methods, our model consistently achieves superior rendering quality across diverse and challenging scenes.",
                "position": 2158
            }
        ]
    },
    {
        "header": "8Conclusion and Discussion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.10647/x10.png",
                "caption": "Figure 12:Invalid background ofamusementscene in TartanAir dataset.",
                "position": 2216
            },
            {
                "img": "https://arxiv.org/html/2511.10647/x11.png",
                "caption": "Figure 13:Image-depth spatial misalignment in IRS dataset.",
                "position": 2225
            },
            {
                "img": "https://arxiv.org/html/2511.10647/x12.png",
                "caption": "Figure 14:Erroneous samples in UnrealStereo4K dataset.The windows in scene00003are transparent; scene00004suffers from clipping issue; the sea in scene00008does not have depth values.",
                "position": 2234
            }
        ]
    },
    {
        "header": "Appendix AData Processing",
        "images": []
    }
]