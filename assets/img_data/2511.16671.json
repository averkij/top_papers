[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16671/figs/logo5.png",
                "caption": "",
                "position": 72
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16671/x1.png",
                "caption": "Figure 1:Interleaving Textual Reasoning throughout Visual Generation.Inspired by the image-interleaved reasoning in textual responses[su2025thinking,zheng2025deepeyes,openai_o3,chen2025mint], we reverse the modality flow and weave textual thoughts into the unfolding canvas, delivering on-the-fly guidance and reflection throughout synthesis.",
                "position": 95
            },
            {
                "img": "https://arxiv.org/html/2511.16671/x2.png",
                "caption": "Figure 2:Comparison ofWherethe Textual Reasoning is Applied in Visual Generation:(a)Think-before-Generation[jiang2025t2i,fang2025got,liao2025imagegen]injects a pre-planning thought prior to the synthesis, limiting fine-grained control and later correction; (b)Think-after-Generation[guo2025can,qin2025uni,li2025reflect]verifies and revise the image once it is complete, lacking nuanced, timely adjustment with extra inference cost; (c) OurThinking-while-Generatinginterleaves thoughts and reflections throughout the synthesis, providing on-the-fly. co-evolving guidance.",
                "position": 98
            }
        ]
    },
    {
        "header": "2Thinking-while-Generating",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16671/x3.png",
                "caption": "Figure 3:Overall Pipeline ofThinking-while-Generating.The framework comprises three components:When to Thinkfor globally determining the interleaved generation schedule;What to Sayfor producing the step-by-step textual thought as fine-grained guidance; andHow to Refinefor a region-level reflection on the current canvas with optional corrective updates.ULMu\\mathrm{ULM}_{u}andULMg\\mathrm{ULM}_{g}denote to apply a single ULM for understanding and generation, respectively.",
                "position": 177
            },
            {
                "img": "https://arxiv.org/html/2511.16671/x4.png",
                "caption": "Figure 4:Illustration of Interleaved Token Sequence:(a) InWhat to Say, the textual pre-context extends from{Ï„j}j<k\\{\\tau_{j}\\}_{j<k}to{Ï„j}jâ‰¤k\\{\\tau_{j}\\}_{j\\leq k}(K=2K=2), guiding the generation of the nextð’±k\\mathcal{V}_{k}while leaving the earlier{ð’±j}j<k\\{\\mathcal{V}_{j}\\}_{j<k}untouched; (b) InHow to Refine, the thoughtÏ„k{\\tau}_{k}is revised toÏ„^k\\hat{\\tau}_{k}, and only the local regionð’±^k\\hat{\\mathcal{V}}_{k}is re-generated to replaceð’±k\\mathcal{V}_{k}. Neither operation requires the ULM to possess image-to-image capabilities, and both preserve a single text-to-image generation trajectory without launching a fresh pass or full re-generation.",
                "position": 234
            }
        ]
    },
    {
        "header": "3Implementation Exploration",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16671/x5.png",
                "caption": "Figure 5:Qualitative Comparison ofTwiGVariants:the baseline (Janus-Pro-7B[chen2025janus]),TwiG-ZS,-SFT, and-RL. Our method demonstrates progressive improvements in compositional fidelity, object counting, and visual realism.",
                "position": 1018
            },
            {
                "img": "https://arxiv.org/html/2511.16671/x6.png",
                "caption": "Figure 6:The Reflection Capacity ofTwiG-RL. The reflection within ourThinking-while-Generatingrefines both semantic and visual consistency, e.g., improving spatial alignment, shadow coherence, and overall realism across diverse prompts.",
                "position": 1021
            },
            {
                "img": "https://arxiv.org/html/2511.16671/x7.png",
                "caption": "Figure 7:Thinking-while-GeneratingProcess ofTwiG-RL.Each example showcases how the model iteratively interleaves its textual reasoning and visual outputs, progressively improving compositional accuracy, spatial alignment, and scene coherence.",
                "position": 1024
            }
        ]
    },
    {
        "header": "4Conclusion",
        "images": []
    }
]