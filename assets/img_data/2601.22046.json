[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22046/x1.png",
                "caption": "",
                "position": 145
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22046/x2.png",
                "caption": "Figure 2:PLANING consistently outperforms existing streaming and per-scene reconstruction methods across geometry accuracy, rendering quality, computational efficiency, and memory usage, while maintaining clear and well-structured planar geometry.",
                "position": 173
            },
            {
                "img": "https://arxiv.org/html/2601.22046/x3.png",
                "caption": "Figure 3:Pipeline of PLANING.PLANING adopts a hybrid representation in which triangles explicitly model scene geometry, while neural Gaussians decoded from these triangles render appearance. Built upon this representation, we develop a streaming reconstruction framework that takes unposed monocular image sequences as input and comprises a frontend for camera tracking, a backend for global pose optimization, and a mapper for scene reconstruction. Specifically, the mapper incorporates an efficient primitive initialization strategy to reduce redundancy. The recontructed triangle soup further enables efficient planar abstraction, facilitating a range of downstream tasks.",
                "position": 209
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22046/x4.png",
                "caption": "Figure 4:Definition of the local frameandresults of forward rendering. Our triangle rasterizer enables correct and reliable forward rendering of triangles.",
                "position": 241
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22046/x5.png",
                "caption": "Figure 5:Qualitative comparison of geometric reconstruction.We visualize planar reconstruction and geometric modeling across different primitives, with 2DGS shown as dense mesh for comparison. Overall, our method preserves planar structures while capturing fine geometric details.",
                "position": 1095
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22046/x6.png",
                "caption": "Figure 6:Qualitative comparison of appearance rendering.We evaluate our method against state-of-the-art approaches. White wireframes highlight regions where our method excels, faithfully reconstructing fine structures and complete surface.",
                "position": 1150
            },
            {
                "img": "https://arxiv.org/html/2601.22046/x7.png",
                "caption": "Figure 7:Locomotion.To demonstrate the utility of our geometric output as a robust simulation environment, we trained two motion policies using Proximal Policy Optimization (PPO) within the Isaac Lab framework: (a) indoor walking with a Unitree H1 humanoid, and (b) stair climbing with a Unitree A1 quadruped. These experiments validate that our reconstructed geometry provides a high-fidelity foundation for reinforcement learning.",
                "position": 1153
            },
            {
                "img": "https://arxiv.org/html/2601.22046/x8.png",
                "caption": "Figure 8:Large-scale indoor reconstruction.We captured over 2000 monocular images of an indoor corridor using a mobile phone. Leveraging our dynamic loading strategy, our method achieves high-quality dense mesh reconstruction and rendering.",
                "position": 1156
            },
            {
                "img": "https://arxiv.org/html/2601.22046/x9.png",
                "caption": "Figure 9:Effect of plane-guided camera pose optimization.Feeding back planar map constraints into pose estimation effectively reduces drift.",
                "position": 1181
            },
            {
                "img": "https://arxiv.org/html/2601.22046/x10.png",
                "caption": "Figure 10:Ablation on triangle representation.Compared to surfels, our representation produces clearer, opaque surfaces and enables finer rendering details.",
                "position": 1184
            },
            {
                "img": "https://arxiv.org/html/2601.22046/x11.png",
                "caption": "Figure 11:Ablation on hybrid representation.Our design effectively reduces representation redundancy and mitigates the geometric inconsistencies commonly observed in depth predicted by feed-forward methods. The point clouds visualize the centers of Gaussians.",
                "position": 1207
            },
            {
                "img": "https://arxiv.org/html/2601.22046/x12.png",
                "caption": "Figure 12:Ablation on global map update.Our framework effectively improves the global consistency.",
                "position": 1210
            }
        ]
    },
    {
        "header": "5Limitations",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATechnical Details",
        "images": []
    },
    {
        "header": "Appendix BApplication Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22046/x13.png",
                "caption": "Figure 14:More geometric comparison results.We visualize planar reconstruction and geometric modeling across different primitives, with 2DGS shown as dense mesh for comparison. Overall, our method preserves planar structures while capturing fine geometric details.",
                "position": 2223
            },
            {
                "img": "https://arxiv.org/html/2601.22046/x14.png",
                "caption": "Figure 15:More rendering comparison results.White boxes highlight artifacts and fine-grained details from baseline methods. Our approach yields significantly sharper results on intricate structures, such as text, while achieving superior overall rendering quality.",
                "position": 2292
            }
        ]
    },
    {
        "header": "Appendix CSupplementary Experiments",
        "images": []
    }
]