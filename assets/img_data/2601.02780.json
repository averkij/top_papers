[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02780/x1.png",
                "caption": "Figure 1:Benchmark performance of MiMo-V2-Flash.",
                "position": 155
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2MiMo-V2-Flash Model Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02780/x2.png",
                "caption": "Figure 2:An illustration of MiMo-V2-Flash model architecture. The model comprisesM=8M=8Hybrid Blocks, where each Hybrid Block interleavesN=5N=5Sliding Window Attention (SWA) blocks with one Global Attention (GA) block. Both are equipped with a sparse MoE FFN. The only exception is the first block, which uses GA with a dense FFN. The MTP blocks employ SWA and a dense FFN.",
                "position": 286
            }
        ]
    },
    {
        "header": "3Pre-Training",
        "images": []
    },
    {
        "header": "4Post-Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02780/x3.png",
                "caption": "Figure 3:Overview of MiMo-V2-Flash post-training stages.",
                "position": 1130
            },
            {
                "img": "https://arxiv.org/html/2601.02780/x4.png",
                "caption": "Figure 4:Code-agentic RL scaling curves. The X-axis represents total interactive environments consumed during on-policy RL rollouts; the Y-axis shows resolved rates on SWE-Bench-Verified and SWE-Bench-Multilingual.",
                "position": 1454
            },
            {
                "img": "https://arxiv.org/html/2601.02780/x5.png",
                "caption": "Figure 5:Generalization of code-agentic RL training to other task domains.",
                "position": 1457
            },
            {
                "img": "https://arxiv.org/html/2601.02780/x6.png",
                "caption": "Figure 6:Comparison of different post-training methods on math and code tasks. Three lines represent training RL with ORM, MOPD without outcome rewards (MOPD w/o ORM), and MOPD.",
                "position": 1521
            },
            {
                "img": "https://arxiv.org/html/2601.02780/x7.png",
                "caption": "",
                "position": 1524
            }
        ]
    },
    {
        "header": "5MTP Speedup",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02780/x8.png",
                "caption": "Figure 7:The correlation between next token cross-entropy and Average Accept Length across different datasets. The orange dashed line represents the best-fit curve (R2=0.995R^{2}=0.995).",
                "position": 1832
            }
        ]
    },
    {
        "header": "6Conclusion, Limitation, and Future Work",
        "images": []
    },
    {
        "header": "Appendix AContributions and Acknowledgments",
        "images": []
    },
    {
        "header": "Appendix BReward Hacking of SWE-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02780/x9.png",
                "caption": "Figure 8:The tendency of our experiment on Qwen3-32B to exhibit reward hacking during RL training within unprocessed images. We quantify the model’s git hacking attempts by counting a set of keywords, such as \"git log ---all\", within the model’s rollout trajectories.",
                "position": 2156
            }
        ]
    },
    {
        "header": "Appendix CContext Management",
        "images": []
    }
]