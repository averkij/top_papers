[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.18634/extracted/6650287/figures/title.png",
                "caption": "",
                "position": 104
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.18634/x1.png",
                "caption": "Figure 1:Captain Cinema: “I can film this all day¡‘Captain Cinema bridges top-down interleaved keyframe planning with bottom-up interleaved-conditioning video generation, taking a step toward the first multi-scene, whole-movie generation, preserving high visual consistency in scenes and identities. All the movie frames here aregenerated.",
                "position": 166
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.18634/x2.png",
                "caption": "Figure 2:Learn from whole movies.Here is an interleaved data sample processed from full-length movies. Our data pipeline extract structured narrative and visual information across scenes. Each frame is annotated with detailed visual description with major <Character Names> of the movie.",
                "position": 207
            },
            {
                "img": "https://arxiv.org/html/2507.18634/x3.png",
                "caption": "Figure 3:Method Overview.Captain Cinema bridges top-down and bottom-up interleaved for one-stage multi-scene movie generation. It introduces a hybrid attention masking strategy with GoldenMem context compression to learn and generate long movies efficiently and effectively. The number of GoldenMem tokens (referring to the short side of encoded image latents) is an example to show the inverse Fibonacci downsampling.",
                "position": 305
            },
            {
                "img": "https://arxiv.org/html/2507.18634/x4.png",
                "caption": "Figure 4:GoldenMem compresses the the context length.The x-axis shows the number of image-text pairs, and the y-axis is the total number of tokens. Initial resolution is 400×\\times×800 (H×\\times×W).",
                "position": 308
            },
            {
                "img": "https://arxiv.org/html/2507.18634/x4.png",
                "caption": "Figure 4:GoldenMem compresses the the context length.The x-axis shows the number of image-text pairs, and the y-axis is the total number of tokens. Initial resolution is 400×\\times×800 (H×\\times×W).",
                "position": 311
            },
            {
                "img": "https://arxiv.org/html/2507.18634/extracted/6650287/figures/avg_cover_rate.png",
                "caption": "Figure 5:Semantic-oriented context retrieval.CLIP text-to-image beats T5 text-to-text for history context retrieval.",
                "position": 316
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.18634/x5.png",
                "caption": "Figure 6:Qualitative results.From the narrative prompt “An interstellar voyage with Bruce Wayne, the Joker, and Alfred Pennyworth,\" Gemini 2.5 Pro composes shot-level descriptions that guide our top-down key-frame generator, yielding the storyboard panels shown above. Each text–key-frame pair then conditions our bottom-up video model, which synthesises the full multi-scene film. The figure highlights twenty-four representative shots demonstrating sustained narrative coherence, character fidelity, and visual style across the entire production.",
                "position": 363
            },
            {
                "img": "https://arxiv.org/html/2507.18634/extracted/6650287/figures/appendix/stress_test/consistency.png",
                "caption": "aConsistency",
                "position": 609
            },
            {
                "img": "https://arxiv.org/html/2507.18634/extracted/6650287/figures/appendix/stress_test/consistency.png",
                "caption": "aConsistency",
                "position": 612
            },
            {
                "img": "https://arxiv.org/html/2507.18634/extracted/6650287/figures/appendix/stress_test/visual_quality.png",
                "caption": "bVisual Quality",
                "position": 617
            },
            {
                "img": "https://arxiv.org/html/2507.18634/extracted/6650287/figures/appendix/stress_test/diversity.png",
                "caption": "cDiversity",
                "position": 623
            },
            {
                "img": "https://arxiv.org/html/2507.18634/extracted/6650287/figures/appendix/stress_test/narrative.png",
                "caption": "dNarrative Coherence",
                "position": 628
            },
            {
                "img": "https://arxiv.org/html/2507.18634/x6.png",
                "caption": "Figure 8:Progressive long-context finetuning needs modest warmup.As the base model (FLUX 1.Dev) is a distillation model with guidance, it requires progressive finetuning with growing context length to warmup the model. However, over warmup can also lead the model to forget the distilled knowledge for high-quality keyframe generation, generating artifacts or messy textures. Picking a proper warm-up length is therefore important for long context tuning.",
                "position": 650
            },
            {
                "img": "https://arxiv.org/html/2507.18634/x7.png",
                "caption": "Figure 9:Disentangled task modeling makes robust movie generation.With the high-quality keyframes generated by our top-down keyframe generation model, the bottom-up video generation model can now focus on motion dynamics. The generation performs higher temporal consistency, world-context awareness and smoother camera movement.",
                "position": 653
            },
            {
                "img": "https://arxiv.org/html/2507.18634/x8.png",
                "caption": "Figure 10:Visual context conditioning needs moderate noise injection.Each conditioning frame has individually been injected with random noise at varying levels:1st: 751∼similar-to\\sim∼1000;2nd: 501∼similar-to\\sim∼750;3rd: 251∼similar-to\\sim∼500;4th: 1∼similar-to\\sim∼250. Context conditioning with moderate levels of noise lead to better character and scene consistency.",
                "position": 735
            },
            {
                "img": "https://arxiv.org/html/2507.18634/x9.png",
                "caption": "aCreative scene generation.Batman is locked in prison while Joker takes control of Gotham.",
                "position": 788
            },
            {
                "img": "https://arxiv.org/html/2507.18634/x9.png",
                "caption": "aCreative scene generation.Batman is locked in prison while Joker takes control of Gotham.",
                "position": 791
            },
            {
                "img": "https://arxiv.org/html/2507.18634/x10.png",
                "caption": "bCross-movie character swapping.Bruce Wayne and Alfred Pennyworth appear in “Interstellar.”",
                "position": 796
            }
        ]
    },
    {
        "header": "5Limitations & Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BData Annotation Prompt",
        "images": []
    },
    {
        "header": "Appendix CAdditional Qualitative Results for Keyframe Generation.",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.18634/extracted/6650287/figures/appendix/32frame_case1.png",
                "caption": "Figure 12:Qualitative Result.Our method shows superior generation quality with character and scene consistency. 32 frames are generated by our method through diffusion forcing with bidirectional masking strategy. The prompt here is from the validation split with ChatGPT-4o re-imagined.",
                "position": 1772
            },
            {
                "img": "https://arxiv.org/html/2507.18634/extracted/6650287/figures/appendix/32frame_case2.png",
                "caption": "Figure 13:Qualitative Result.Our method shows superior generation quality with character and scene consistency. 32 frames are generated by our method through diffusion forcing with bidirectional masking strategy. The prompt here is from the validation split with ChatGPT-4o re-imagined.",
                "position": 1775
            },
            {
                "img": "https://arxiv.org/html/2507.18634/extracted/6650287/figures/appendix/32frame_case3.png",
                "caption": "Figure 14:Qualitative Result.Our method shows superior generation quality with character and scene consistency. 32 frames are generated by our method through diffusion forcing with bidirectional masking strategy. The prompt here is from the validation split with ChatGPT-4o re-imagined.",
                "position": 1778
            },
            {
                "img": "https://arxiv.org/html/2507.18634/extracted/6650287/figures/appendix/32frame_case4.png",
                "caption": "Figure 15:Qualitative Result.Our method shows superior generation quality with character and scene consistency. 32 frames are generated by our method through diffusion forcing with bidirectional masking strategy. The prompt here is from the validation split with ChatGPT-4o re-imagined.",
                "position": 1782
            }
        ]
    },
    {
        "header": "Appendix DLong-Context Stress Test Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.18634/extracted/6650287/figures/appendix/identity_test/yann_lecun_case1.png",
                "caption": "Figure 16:Out-of-domain Identity-Preservation Test : Different Dressing.Each test case begins with 12 randomly sampled context images that are injected with noise; the model then generates another four frames conditioned on textual prompts as well as the pretext interleaved conditions. Our interleaved key-frame generator accurately maintains the character’s identity, demonstrating strong out-of-domain generalizability.",
                "position": 1853
            },
            {
                "img": "https://arxiv.org/html/2507.18634/extracted/6650287/figures/appendix/identity_test/yann_lecun_case1.png",
                "caption": "",
                "position": 1856
            },
            {
                "img": "https://arxiv.org/html/2507.18634/extracted/6650287/figures/appendix/identity_test/yann_lecun_case2.png",
                "caption": "",
                "position": 1861
            },
            {
                "img": "https://arxiv.org/html/2507.18634/extracted/6650287/figures/appendix/identity_test/yann_same_appearance_case1.png",
                "caption": "Figure 17:Out-of-domain Identity-Preservation Test : Same Dressing.Each test case begins with 12 randomly sampled context images that are injected with noise; the model then generates another four frames conditioned on textual prompts as well as the pretext interleaved conditions. Our interleaved key-frame generator accurately maintains the character’s identity, demonstrating strong out-of-domain generalizability.",
                "position": 1867
            },
            {
                "img": "https://arxiv.org/html/2507.18634/extracted/6650287/figures/appendix/identity_test/yann_same_appearance_case1.png",
                "caption": "",
                "position": 1870
            },
            {
                "img": "https://arxiv.org/html/2507.18634/extracted/6650287/figures/appendix/identity_test/yann_same_appearance_case2.png",
                "caption": "",
                "position": 1875
            },
            {
                "img": "https://arxiv.org/html/2507.18634/extracted/6650287/figures/appendix/identity_test/elon_musk_case1.png",
                "caption": "Figure 18:Out-of-domain Identity-Preservation Test : Different Dressing.Each test case begins with 12 randomly sampled context images that are injected with noise; the model then generates another four frames conditioned on textual prompts as well as the pretext interleaved conditions. Our interleaved key-frame generator accurately maintains the character’s identity while follows the instruction prompts, demonstrating strong identity preservation ability to out-of-domain characters.",
                "position": 1881
            },
            {
                "img": "https://arxiv.org/html/2507.18634/extracted/6650287/figures/appendix/identity_test/elon_musk_case1.png",
                "caption": "",
                "position": 1884
            },
            {
                "img": "https://arxiv.org/html/2507.18634/extracted/6650287/figures/appendix/identity_test/elon_musk_case2.png",
                "caption": "",
                "position": 1889
            },
            {
                "img": "https://arxiv.org/html/2507.18634/extracted/6650287/figures/appendix/identity_test/elon_same_appearance_case1.png",
                "caption": "Figure 19:Out-of-domain Identity-Preservation Test : Same Dressing.Each test case begins with 12 randomly sampled context images that are injected with noise; the model then generates another four frames conditioned on textual prompts as well as the pretext interleaved conditions. Our interleaved key-frame generator accurately maintains the character’s identity, demonstrating strong out-of-domain generalizability.",
                "position": 1895
            },
            {
                "img": "https://arxiv.org/html/2507.18634/extracted/6650287/figures/appendix/identity_test/elon_same_appearance_case1.png",
                "caption": "",
                "position": 1898
            },
            {
                "img": "https://arxiv.org/html/2507.18634/extracted/6650287/figures/appendix/identity_test/elon_same_appearance_case2.png",
                "caption": "",
                "position": 1903
            }
        ]
    },
    {
        "header": "Appendix EOut-of-domain Identity Preservation Generalizability",
        "images": []
    }
]