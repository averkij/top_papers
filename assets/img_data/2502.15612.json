[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3LaTIM",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15612/x1.png",
                "caption": "Figure 1:Heatmaps generated by different interpretability methods for Mamba-2. The interaction between source and copied tokens (along the diagonal line) is more clearly highlighted withLaTIM.",
                "position": 770
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x1.png",
                "caption": "",
                "position": 773
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x2.png",
                "caption": "",
                "position": 777
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x3.png",
                "caption": "",
                "position": 781
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x4.png",
                "caption": "",
                "position": 785
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15612/x5.png",
                "caption": "Table 2:Faithfulness evaluation on the copying task in terms of Area Under the Curve (AUC), Average Precision (AP), and Recall at K (R@K).",
                "position": 827
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x5.png",
                "caption": "Figure 2:Interpretability heatmaps for Mamba-1 (370M) fine-tuned onde‚Üí‚Üí\\rightarrow‚Üíendata from the IWSLT17 dataset.LaTIM(‚Ñì2subscript‚Ñì2\\ell_{2}roman_‚Ñì start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT) produces alignments that more closely match the ground truth.",
                "position": 925
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x6.png",
                "caption": "",
                "position": 928
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x7.png",
                "caption": "",
                "position": 929
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x8.png",
                "caption": "",
                "position": 930
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x9.png",
                "caption": "Table 3:Alignment Error Rate (AER) per interpretability method. M1 and M2 stand for Mamba-1 and Mamba-2, with subscript S and M denoting the small (130M) and large (370M) versions, respectively.",
                "position": 939
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x9.png",
                "caption": "Figure 3:Left:\nAttention map fromLaTIM(‚Ñì2subscript‚Ñì2\\ell_{2}roman_‚Ñì start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT) for a Passkey Retrieval sample where the key is ‚Äúitchy-obligation‚Äù Instead of predicting 5661907, the model incorrectly produces 4612365.\nRight: Average contribution scores for token ranges preceding each extracted frequent word.\nNotably, the focus over the token ranges ‚Äúfdcv‚Äù and ‚Äúvgpn‚Äù aligns well with the two most frequent tokens in the sample (‚Äúfdcvcu‚Äù, ‚Äúvgpnki‚Äù). However, when generating ‚Äúuqbcr‚Äù, it fails to focus on the 3rd most frequent token, suggesting that it relies more on morphological patterns than frequency.",
                "position": 1096
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x10.png",
                "caption": "",
                "position": 1099
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Potential Risks",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AHidden Attention Derivation in Mamba",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15612/x11.png",
                "caption": "Figure 4:Error amounting to the average difference between the regular Mamba-1 (left) and Mamba-2 (right) layer output and the interpretable version with different approximationsfùëìfitalic_fin Equation19.",
                "position": 2101
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x12.png",
                "caption": "",
                "position": 2104
            }
        ]
    },
    {
        "header": "Appendix BExperimental Details",
        "images": []
    },
    {
        "header": "Appendix CExtended Approximation Error",
        "images": []
    },
    {
        "header": "Appendix DAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15612/x13.png",
                "caption": "Figure 5:Heatmaps produced by different interpretable approaches for Mamba-1.\nThe interaction between source and copied tokens (along the diagonal line) becomes clearer withLaTIM.",
                "position": 2169
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x14.png",
                "caption": "",
                "position": 2172
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x15.png",
                "caption": "",
                "position": 2173
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x16.png",
                "caption": "",
                "position": 2174
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x17.png",
                "caption": "",
                "position": 2175
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x18.png",
                "caption": "",
                "position": 2201
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x19.png",
                "caption": "",
                "position": 2202
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x20.png",
                "caption": "",
                "position": 2203
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x21.png",
                "caption": "",
                "position": 2204
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x22.png",
                "caption": "",
                "position": 2215
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x23.png",
                "caption": "",
                "position": 2218
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x24.png",
                "caption": "",
                "position": 2219
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x25.png",
                "caption": "",
                "position": 2220
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x26.png",
                "caption": "",
                "position": 2222
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x27.png",
                "caption": "",
                "position": 2226
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x28.png",
                "caption": "",
                "position": 2226
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x29.png",
                "caption": "",
                "position": 2226
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x30.png",
                "caption": "",
                "position": 2226
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x31.png",
                "caption": "",
                "position": 2226
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x32.png",
                "caption": "Figure 8:Attention plots obtained byLaTIM(‚Ñì2subscript‚Ñì2\\ell_{2}roman_‚Ñì start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT) (left) and MambaAttention (right) on a Passkey Retrieval sample, showing that MambaAttention focuses on several misleading tokens, such as ‚Äúdetermined‚Äù, ‚Äúnumber for‚Äù, and ‚Äúmentioned‚Äù. In contrast,LaTIM(‚Ñì2subscript‚Ñì2\\ell_{2}roman_‚Ñì start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT) focuses only on meaningful strings, like ‚Äú4612365‚Äù (the predicted key) and ‚Äú5661907‚Äù (the correct key).",
                "position": 2348
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x33.png",
                "caption": "",
                "position": 2351
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x34.png",
                "caption": "Figure 9:Left: Attention map obtained byLaTIM(‚Ñì2subscript‚Ñì2\\ell_{2}roman_‚Ñì start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT) on the Frequent Word Extraction task, showing that the model is focusing on the incorrectly generated ‚Äúand uqbcr‚Äù token range (Mamba-2 370M layer 23).\nRight: Average attention score per word instance, showing that the model‚Äôs focus reduces heavily after the first few word occurrences.",
                "position": 2361
            },
            {
                "img": "https://arxiv.org/html/2502.15612/x35.png",
                "caption": "",
                "position": 2364
            }
        ]
    },
    {
        "header": "Appendix EAI Assistants",
        "images": []
    }
]