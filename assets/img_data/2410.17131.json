[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17131/x1.png",
                "caption": "(a)Online Training on Llama3.1-8B. (Iteration 3)",
                "position": 160
            },
            {
                "img": "https://arxiv.org/html/2410.17131/x1.png",
                "caption": "(a)Online Training on Llama3.1-8B. (Iteration 3)",
                "position": 163
            },
            {
                "img": "https://arxiv.org/html/2410.17131/x2.png",
                "caption": "(b)Offline Training on Llama3.1-8B.",
                "position": 168
            },
            {
                "img": "https://arxiv.org/html/2410.17131/x3.png",
                "caption": "(c)RM Training on Llama3.1-8B-Instruct.",
                "position": 173
            },
            {
                "img": "https://arxiv.org/html/2410.17131/x4.png",
                "caption": "Figure 2:The philosophical motivation of our methods. Greater overlap on the x-axis (performance) between the generated distributions (red and blue) and the original distribution (orange) indicates stronger on-policy behavior.\nPrevious automated methods extract chosen and rejected distributions through different methods, which may be less learnable for the policy model and hard to distinguish after iterative training.\nOur approach (S⁢S⁢O𝑆𝑆𝑂SSOitalic_S italic_S italic_O) optimizes models to generate near-on-policy signals where there remains a gap between chosen and rejected distributions, which benefits the automated alignment process.",
                "position": 187
            }
        ]
    },
    {
        "header": "2Self-Steering Optimization",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17131/x5.png",
                "caption": "Figure 3:Our approach consists of two iterative steps:\n1) Constructing contrastive prompts and sampling responses.\nGiven a query, the policy model first identifies the most relevant features and principles to the query. We then construct a pair of contrastive prompts based on these principles and sample corresponding responses. These responses are then used to form three preference pairs for alignment.\n2) Training the model with a weighted objective incorporating three distinct losses.",
                "position": 224
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": []
    },
    {
        "header": "4Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17131/x6.png",
                "caption": "(a)”S⁢S⁢O𝑆𝑆𝑂SSOitalic_S italic_S italic_O” represents the number of right pairs divided by the total number, and ”S⁢S⁢O𝑆𝑆𝑂SSOitalic_S italic_S italic_O(WithUnsure)” represents the number of right and unsure pairs divided by the total number.",
                "position": 940
            },
            {
                "img": "https://arxiv.org/html/2410.17131/x6.png",
                "caption": "(a)”S⁢S⁢O𝑆𝑆𝑂SSOitalic_S italic_S italic_O” represents the number of right pairs divided by the total number, and ”S⁢S⁢O𝑆𝑆𝑂SSOitalic_S italic_S italic_O(WithUnsure)” represents the number of right and unsure pairs divided by the total number.",
                "position": 943
            },
            {
                "img": "https://arxiv.org/html/2410.17131/x7.png",
                "caption": "(b)Compared to IPO,S⁢S⁢O𝑆𝑆𝑂SSOitalic_S italic_S italic_Osignificantly raises theπ⁢(y+|x)𝜋conditionalsuperscript𝑦𝑥\\pi(y^{+}|x)italic_π ( italic_y start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT | italic_x )andπ⁢(y−|x)𝜋conditionalsuperscript𝑦𝑥\\pi(y^{-}|x)italic_π ( italic_y start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT | italic_x ).",
                "position": 948
            },
            {
                "img": "https://arxiv.org/html/2410.17131/x8.png",
                "caption": "(a)Results on AlpacaEval 2.0.",
                "position": 1090
            },
            {
                "img": "https://arxiv.org/html/2410.17131/x8.png",
                "caption": "(a)Results on AlpacaEval 2.0.",
                "position": 1093
            },
            {
                "img": "https://arxiv.org/html/2410.17131/x9.png",
                "caption": "(b)Results on MT Bench.",
                "position": 1098
            }
        ]
    },
    {
        "header": "5Related Works",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "8Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]