[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20218/x1.png",
                "caption": "Figure 1:(a) Existing approaches only predict a single, sparse reward at the end of the denoising trajectory, which is naively applied to optimize all intermediate steps.\n(b) DenseGRPO estimates step-wise rewards of individual steps, densifying the feedback signal for the denoising process.",
                "position": 118
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20218/x2.png",
                "caption": "Figure 2:Overview of DenseGRPO. Given theii-th trajectory within a GRPO group, we first predict the rewards{Rti}\\{R^{i}_{t}\\}of latents{ùíôti}\\{\\bm{x}^{i}_{t}\\}via ODE denoising. By capturing the reward gain{Œî‚ÄãRti}\\{\\Delta R^{i}_{t}\\}at each step, we obtain the dense reward that reliably evaluates the step-wise contribution.",
                "position": 288
            }
        ]
    },
    {
        "header": "4DenseGRPO",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20218/x3.png",
                "caption": "Figure 3:Visualization of dense rewards, where each polyline denotes an SDE-sampled trajectory: (a)(b)(c) existing GRPO-based methods utilize a uniform setting of noise levelaa, such asa=0.7a=0.7,a=0.5a=0.5, anda=0.8a=0.8, leading to an inappropriate exploration space; (d) DenseGRPO calibrates a timestep-specific noise intensityœà‚Äã(t)\\psi(t), enabling a suitable exploration space for all timesteps.",
                "position": 324
            },
            {
                "img": "https://arxiv.org/html/2601.20218/x4.png",
                "caption": "Figure 4:Comparison of learning curves. Figures (a) to (c) correspond to the tasks of compositional image generation, visual text rendering, and human preference alignment, respectively.",
                "position": 619
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20218/x5.png",
                "caption": "Figure 5:Qualitative comparison on three benchmarks: Compositional Image Generation, Visual Text Rendering, and Human Preference Alignment.\nOur DenseGRPO generates high-quality outcomes across all tasks, excelling in color accuracy, text fidelity, and content alignment.",
                "position": 635
            },
            {
                "img": "https://arxiv.org/html/2601.20218/x6.png",
                "caption": "Figure 6:Ablation studies on our critical designs.\n(a) Step-wise dense reward aligns with contribution, surpassing trajectory-wise sparse reward.\n(b) Our time-specific noise level enables a suitable exploration space.\n(c) Increased ODE denoising steps (nn) improve dense reward accuracy, yielding superior results.\nThe vertical axis denotes the PickScore results. The horizontal axis of (a) and (b) is training steps, while the horizontal axis of (c) denotes training time for training cost comparison.",
                "position": 687
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Detail",
        "images": []
    },
    {
        "header": "Appendix BMore Result",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20218/x7.png",
                "caption": "Figure 7:Training curves of KL loss. Figures (a) to (c) correspond to the tasks of compositional image generation, visual text rendering, and human preference alignment, respectively.",
                "position": 1108
            },
            {
                "img": "https://arxiv.org/html/2601.20218/x8.png",
                "caption": "Figure 8:Visualization of ODE-based latent rewards,i.e.,RtiR^{i}_{t}predicted by Eq.9, where each polyline denotes a sampled trajectory.timestep=0\\rm{timestep=0}represents the terminal reward of the SDE sampling trajectory.",
                "position": 1116
            },
            {
                "img": "https://arxiv.org/html/2601.20218/x9.png",
                "caption": "Figure 9:Performance of DenseGRPO compared with Flow-GRPO on additional models: (a) FLUX.1-dex, (b) SD 3.5-M on1024√ó10241024\\times 1024resolution, and (c) diffusion model.",
                "position": 1135
            },
            {
                "img": "https://arxiv.org/html/2601.20218/x10.png",
                "caption": "Figure 10:Visualization of reward hacking.",
                "position": 1156
            }
        ]
    },
    {
        "header": "Appendix CLLM Usage",
        "images": []
    }
]