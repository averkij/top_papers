[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03566/x1.png",
                "caption": "Figure 1:The inference and training stages of EAGLE-2, HASS, and ourPosSmethod. The dashed lines represent autoregressive decoding or training, and the solid lines represent parallel training. The input concatenates context word embeddingsxğ‘¥xitalic_xand features from previous stepfğ‘“fitalic_f.\nDuringinference, EAGLE-2 and HASS use a single draft modelÎ¸Dsubscriptğœƒğ·\\theta_{D}italic_Î¸ start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPTto generate featuresf(Di)superscriptğ‘“subscriptğ·ğ‘–f^{(D_{i})}italic_f start_POSTSUPERSCRIPT ( italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_POSTSUPERSCRIPTfor each positioniğ‘–iitalic_irecursively.\nFordraft model training, EAGLE-2 uses the target model featuref(T)superscriptğ‘“ğ‘‡f^{(T)}italic_f start_POSTSUPERSCRIPT ( italic_T ) end_POSTSUPERSCRIPTas input for training (teacher forcing), and HASS uses draft model predicted featuresf(Di)superscriptğ‘“subscriptğ·ğ‘–f^{(D_{i})}italic_f start_POSTSUPERSCRIPT ( italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_POSTSUPERSCRIPTto replace.\nDifferent from them,PosSintroduces different position specialistsÎ¸Sjsubscriptğœƒsuperscriptğ‘†ğ‘—\\theta_{S^{j}}italic_Î¸ start_POSTSUBSCRIPT italic_S start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT end_POSTSUBSCRIPT. During inference, the position-specialized draft models autoregressively generate featuresf(Sij)superscriptğ‘“superscriptsubscriptğ‘†ğ‘–ğ‘—f^{(S_{i}^{j})}italic_f start_POSTSUPERSCRIPT ( italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT ) end_POSTSUPERSCRIPT, where positioniğ‘–iitalic_icorresponds to the specialistÎ¸Sjsubscriptğœƒsuperscriptğ‘†ğ‘—\\theta_{S^{j}}italic_Î¸ start_POSTSUBSCRIPT italic_S start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT end_POSTSUBSCRIPT. At training stage,PosSapplies position-specialized training: A specialistÎ¸Sjsubscriptğœƒsuperscriptğ‘†ğ‘—\\theta_{S^{j}}italic_Î¸ start_POSTSUBSCRIPT italic_S start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT end_POSTSUBSCRIPTis trained on theithsuperscriptğ‘–thi^{\\text{th}}italic_i start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPTposition using the previous step specialist feature.",
                "position": 184
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03566/x2.png",
                "caption": "Figure 2:Position-wise acceptance rate (pos-acc) of theitâ¢hsuperscriptğ‘–ğ‘¡â„i^{th}italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPTtoken on MT_Bench dataset by various speculative decoding methods. Thepos-accof EAGLE-2 and HASS decays fast as the draft sequence gets longer. Our proposedPosSmethod keeps a stable and higherpos-acceven at the deepest position (draft model prediction depthL=6ğ¿6L=6italic_L = 6).",
                "position": 263
            },
            {
                "img": "https://arxiv.org/html/2506.03566/x3.png",
                "caption": "Figure 3:This figure shows a comparison of hidden state (feature) training betweenPosSand previous work. The draft model generated feature inevitably deviates from the target model, and the deviation level increases with error propagation in the draft model(s). In previous work, one draft model learns to mitigate\nvarying levels of feature gaps. InPosSmethod, position specialists can focus on mitigating expected levels of feature deviation.",
                "position": 284
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": []
    },
    {
        "header": "5Results",
        "images": []
    },
    {
        "header": "6Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03566/x4.png",
                "caption": "Figure 4:The position-wise acceptance rate of EAGLE-2, HASS, and variants ofPosS. Experiments are conducted on MT-Bench dataset, with base model Llama-3-8B-Instruct and draft depth=8. Three variations of our method maintain a relatively higher pos-acc even at the8tâ¢hsuperscript8ğ‘¡â„8^{th}8 start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPTposition.",
                "position": 969
            },
            {
                "img": "https://arxiv.org/html/2506.03566/x5.png",
                "caption": "Figure 5:Computation time of different phases on MT-Bench dataset on different models across varying draft depths. The bar plots present the decomposition of time spent on each phase of speculative decoding, where subfigure (a) measures the time spent on 5k rounds and subfigure (b) measures the time to complete an entire test set. The line plot presents the number of rounds needed to complete a dataset. The lower the metrics are, the better the method is.",
                "position": 999
            }
        ]
    },
    {
        "header": "7Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03566/x6.png",
                "caption": "Figure 6:The throughput and average acceptance length of 4 models on different draft depths. The experiments are conducted on MT-Bench dataset. The acceptance length consistently increases as depth rises, while the throughput peaks at depth=5.",
                "position": 1030
            }
        ]
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitations",
        "images": []
    },
    {
        "header": "Appendix BTraining Configurations",
        "images": []
    },
    {
        "header": "Appendix CDifferent Drafting Hyperparameters",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03566/x7.png",
                "caption": "Figure 7:The Inference-time GPU memory usage of different speculative decoding methods. The memory usage is measured on the MT-bench test dataset.PosSmethods require slightly more GPU memory than EAGLE-2, the baseline method.",
                "position": 2077
            }
        ]
    },
    {
        "header": "Appendix DExtra Memory Usage During Inference",
        "images": []
    }
]