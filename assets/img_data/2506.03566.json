[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03566/x1.png",
                "caption": "Figure 1:The inference and training stages of EAGLE-2, HASS, and ourPosSmethod. The dashed lines represent autoregressive decoding or training, and the solid lines represent parallel training. The input concatenates context word embeddingsx𝑥xitalic_xand features from previous stepf𝑓fitalic_f.\nDuringinference, EAGLE-2 and HASS use a single draft modelθDsubscript𝜃𝐷\\theta_{D}italic_θ start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPTto generate featuresf(Di)superscript𝑓subscript𝐷𝑖f^{(D_{i})}italic_f start_POSTSUPERSCRIPT ( italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_POSTSUPERSCRIPTfor each positioni𝑖iitalic_irecursively.\nFordraft model training, EAGLE-2 uses the target model featuref(T)superscript𝑓𝑇f^{(T)}italic_f start_POSTSUPERSCRIPT ( italic_T ) end_POSTSUPERSCRIPTas input for training (teacher forcing), and HASS uses draft model predicted featuresf(Di)superscript𝑓subscript𝐷𝑖f^{(D_{i})}italic_f start_POSTSUPERSCRIPT ( italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_POSTSUPERSCRIPTto replace.\nDifferent from them,PosSintroduces different position specialistsθSjsubscript𝜃superscript𝑆𝑗\\theta_{S^{j}}italic_θ start_POSTSUBSCRIPT italic_S start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT end_POSTSUBSCRIPT. During inference, the position-specialized draft models autoregressively generate featuresf(Sij)superscript𝑓superscriptsubscript𝑆𝑖𝑗f^{(S_{i}^{j})}italic_f start_POSTSUPERSCRIPT ( italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT ) end_POSTSUPERSCRIPT, where positioni𝑖iitalic_icorresponds to the specialistθSjsubscript𝜃superscript𝑆𝑗\\theta_{S^{j}}italic_θ start_POSTSUBSCRIPT italic_S start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT end_POSTSUBSCRIPT. At training stage,PosSapplies position-specialized training: A specialistθSjsubscript𝜃superscript𝑆𝑗\\theta_{S^{j}}italic_θ start_POSTSUBSCRIPT italic_S start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT end_POSTSUBSCRIPTis trained on theithsuperscript𝑖thi^{\\text{th}}italic_i start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPTposition using the previous step specialist feature.",
                "position": 184
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03566/x2.png",
                "caption": "Figure 2:Position-wise acceptance rate (pos-acc) of theit⁢hsuperscript𝑖𝑡ℎi^{th}italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPTtoken on MT_Bench dataset by various speculative decoding methods. Thepos-accof EAGLE-2 and HASS decays fast as the draft sequence gets longer. Our proposedPosSmethod keeps a stable and higherpos-acceven at the deepest position (draft model prediction depthL=6𝐿6L=6italic_L = 6).",
                "position": 263
            },
            {
                "img": "https://arxiv.org/html/2506.03566/x3.png",
                "caption": "Figure 3:This figure shows a comparison of hidden state (feature) training betweenPosSand previous work. The draft model generated feature inevitably deviates from the target model, and the deviation level increases with error propagation in the draft model(s). In previous work, one draft model learns to mitigate\nvarying levels of feature gaps. InPosSmethod, position specialists can focus on mitigating expected levels of feature deviation.",
                "position": 284
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": []
    },
    {
        "header": "5Results",
        "images": []
    },
    {
        "header": "6Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03566/x4.png",
                "caption": "Figure 4:The position-wise acceptance rate of EAGLE-2, HASS, and variants ofPosS. Experiments are conducted on MT-Bench dataset, with base model Llama-3-8B-Instruct and draft depth=8. Three variations of our method maintain a relatively higher pos-acc even at the8t⁢hsuperscript8𝑡ℎ8^{th}8 start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPTposition.",
                "position": 969
            },
            {
                "img": "https://arxiv.org/html/2506.03566/x5.png",
                "caption": "Figure 5:Computation time of different phases on MT-Bench dataset on different models across varying draft depths. The bar plots present the decomposition of time spent on each phase of speculative decoding, where subfigure (a) measures the time spent on 5k rounds and subfigure (b) measures the time to complete an entire test set. The line plot presents the number of rounds needed to complete a dataset. The lower the metrics are, the better the method is.",
                "position": 999
            }
        ]
    },
    {
        "header": "7Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03566/x6.png",
                "caption": "Figure 6:The throughput and average acceptance length of 4 models on different draft depths. The experiments are conducted on MT-Bench dataset. The acceptance length consistently increases as depth rises, while the throughput peaks at depth=5.",
                "position": 1030
            }
        ]
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitations",
        "images": []
    },
    {
        "header": "Appendix BTraining Configurations",
        "images": []
    },
    {
        "header": "Appendix CDifferent Drafting Hyperparameters",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03566/x7.png",
                "caption": "Figure 7:The Inference-time GPU memory usage of different speculative decoding methods. The memory usage is measured on the MT-bench test dataset.PosSmethods require slightly more GPU memory than EAGLE-2, the baseline method.",
                "position": 2077
            }
        ]
    },
    {
        "header": "Appendix DExtra Memory Usage During Inference",
        "images": []
    }
]