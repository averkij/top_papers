[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.07403/pictures_source/LongEmotion_logo.png",
                "caption": "",
                "position": 127
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.07403/x1.png",
                "caption": "(a)Token distributions across tasks.",
                "position": 172
            },
            {
                "img": "https://arxiv.org/html/2509.07403/x1.png",
                "caption": "(a)Token distributions across tasks.",
                "position": 174
            },
            {
                "img": "https://arxiv.org/html/2509.07403/x2.png",
                "caption": "(b)Distribution of sample counts.",
                "position": 177
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.07403/x3.png",
                "caption": "Figure 2:An illustrative overview of the LongEmotion dataset. To comprehensively evaluate the Emotional Intelligence of LLMs in long-context interaction, we design six tasks: Emotion Classification, Emotion Detection, Emotion QA, Emotion Conversation, Emotion Summary, and Emotion Expression.",
                "position": 216
            }
        ]
    },
    {
        "header": "3LongEmotion: Construction and Task",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.07403/pictures_source/data_annotation.png",
                "caption": "Figure 3:Annotation process of Emotion QA.",
                "position": 406
            },
            {
                "img": "https://arxiv.org/html/2509.07403/pictures_source/pipeline.png",
                "caption": "Figure 4:The pipeline of Collaborative Emotional Modeling (CoEM). CoEM consists of five stages: Chunking, Initial Ranking, Multi-Agent Enrichment, Re-Ranking, and Emotional Ensemble Generation.",
                "position": 412
            },
            {
                "img": "https://arxiv.org/html/2509.07403/pictures_source/conv_evaluator_ablation.png",
                "caption": "(a)Impact of different CoEM-Sage models on MC-4.",
                "position": 635
            },
            {
                "img": "https://arxiv.org/html/2509.07403/pictures_source/conv_evaluator_ablation.png",
                "caption": "(a)Impact of different CoEM-Sage models on MC-4.",
                "position": 637
            },
            {
                "img": "https://arxiv.org/html/2509.07403/pictures_source/summary_enhancer_ablation.png",
                "caption": "(b)Impact of different CoEM-Sage models on ES.",
                "position": 640
            }
        ]
    },
    {
        "header": "4Collaborative Emotional Modeling",
        "images": []
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.07403/pictures_source/model_accuracy_by_length.png",
                "caption": "Figure 6:Model accuracy by context length range on Emotion Classification.",
                "position": 828
            },
            {
                "img": "https://arxiv.org/html/2509.07403/x4.png",
                "caption": "Figure 7:Impact of chunk size and retrieved count on GPT-4o-mini’s RAG performance on Emotion QA.",
                "position": 834
            },
            {
                "img": "https://arxiv.org/html/2509.07403/pictures_source/case_gpt_QA.png",
                "caption": "Figure 8:Comparison of the performance of different versions of GPT models on Emotion QA.",
                "position": 850
            },
            {
                "img": "https://arxiv.org/html/2509.07403/pictures_source/case_gpt_conv.png",
                "caption": "Figure 9:Comparison of the performance of different versions of GPT models on Emotion Conversation.",
                "position": 856
            },
            {
                "img": "https://arxiv.org/html/2509.07403/pictures_source/case_gpt_expression.png",
                "caption": "Figure 10:Comparison of the performance of different versions of GPT models on Emotion Expression.",
                "position": 862
            },
            {
                "img": "https://arxiv.org/html/2509.07403/pictures_source/case_gpt_summary.png",
                "caption": "Figure 11:Comparison of the performance of different versions of GPT models on Emotion Summary.",
                "position": 868
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AQualifications of Annotators",
        "images": []
    },
    {
        "header": "Appendix BInter-annotator Agreement",
        "images": []
    },
    {
        "header": "Appendix CCase Study",
        "images": []
    },
    {
        "header": "Appendix DSynthetic Data Ablation",
        "images": []
    },
    {
        "header": "Appendix EDetails of RAG and CoEM",
        "images": []
    },
    {
        "header": "Appendix FLLM as Judge Metrics Design",
        "images": []
    },
    {
        "header": "Appendix GUnified Format of Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.07403/pictures_source/case_study_all.png",
                "caption": "Figure 12:An overview of retrieved chunks across all tasks. In models’ final generation prompts, the Base setting includes none of the information; the RAG setting includes only the Content information; and the CoEM setting includes both the Content and Summary information.",
                "position": 1695
            },
            {
                "img": "https://arxiv.org/html/2509.07403/prompts/conv_synthetic.png",
                "caption": "Figure 13:Dataset generation prompt for Emotion Conversation.",
                "position": 1777
            },
            {
                "img": "https://arxiv.org/html/2509.07403/example-data/emo_class.png",
                "caption": "Figure 14:Emotion Classification dataset example.",
                "position": 2426
            },
            {
                "img": "https://arxiv.org/html/2509.07403/example-data/emo_detect.png",
                "caption": "Figure 15:Emotion Detection dataset example.",
                "position": 2429
            },
            {
                "img": "https://arxiv.org/html/2509.07403/example-data/QA.png",
                "caption": "Figure 16:Emotion QA dataset example.",
                "position": 2432
            },
            {
                "img": "https://arxiv.org/html/2509.07403/example-data/Conversation.png",
                "caption": "Figure 17:Emotion Conversation dataset example.",
                "position": 2435
            },
            {
                "img": "https://arxiv.org/html/2509.07403/example-data/summary.png",
                "caption": "Figure 18:Emotion Summary dataset example.",
                "position": 2438
            },
            {
                "img": "https://arxiv.org/html/2509.07403/example-data/expression.png",
                "caption": "Figure 19:Emotion Expression dataset example.",
                "position": 2441
            },
            {
                "img": "https://arxiv.org/html/2509.07403/prompts/conv_eval_1.png",
                "caption": "Figure 20:Evaluation prompt for the first stage of Emotion Conversation.",
                "position": 2444
            },
            {
                "img": "https://arxiv.org/html/2509.07403/prompts/conv_eval_2.png",
                "caption": "Figure 21:Evaluation prompt for the second stage of Emotion Conversation.",
                "position": 2447
            },
            {
                "img": "https://arxiv.org/html/2509.07403/prompts/conv_eval_3.png",
                "caption": "Figure 22:Evaluation prompt for the third stage of Emotion Conversation.",
                "position": 2450
            },
            {
                "img": "https://arxiv.org/html/2509.07403/prompts/conv_eval_4.png",
                "caption": "Figure 23:Evaluation prompt for the fourth stage of Emotion Conversation.",
                "position": 2453
            },
            {
                "img": "https://arxiv.org/html/2509.07403/prompts/summary_eval.png",
                "caption": "Figure 24:Evaluation prompt for Emotion Summary.",
                "position": 2456
            },
            {
                "img": "https://arxiv.org/html/2509.07403/prompts/expression_eval.png",
                "caption": "Figure 25:Evaluation prompt for Emotion Expression.",
                "position": 2459
            },
            {
                "img": "https://arxiv.org/html/2509.07403/prompts/Emo_class_aug.png",
                "caption": "Figure 26:Multi-agent enrichment prompt for Emotion Classification.",
                "position": 2462
            },
            {
                "img": "https://arxiv.org/html/2509.07403/prompts/Emo_detect_aug.png",
                "caption": "Figure 27:Multi-agent enrichment prompt for Emotion Detection.",
                "position": 2465
            },
            {
                "img": "https://arxiv.org/html/2509.07403/prompts/conv_aug.png",
                "caption": "Figure 28:Multi-agent enrichment prompt for Emotion Conversation.",
                "position": 2468
            },
            {
                "img": "https://arxiv.org/html/2509.07403/prompts/qa_aug.png",
                "caption": "Figure 29:Multi-agent enrichment prompt for Emotion QA.",
                "position": 2471
            },
            {
                "img": "https://arxiv.org/html/2509.07403/prompts/summary_aug.png",
                "caption": "Figure 30:Multi-agent enrichment prompt for Emotion Summary.",
                "position": 2474
            },
            {
                "img": "https://arxiv.org/html/2509.07403/prompts/expression_aug.png",
                "caption": "Figure 31:Multi-agent enrichment prompt for Emotion Expression.",
                "position": 2477
            },
            {
                "img": "https://arxiv.org/html/2509.07403/prompts/Emo_class_gen.png",
                "caption": "Figure 32:Emotional ensemble generation prompt for Emotion Classification.",
                "position": 2480
            },
            {
                "img": "https://arxiv.org/html/2509.07403/prompts/Emo_detect_gen.png",
                "caption": "Figure 33:Emotional ensemble generation prompt for Emotion Detection.",
                "position": 2483
            },
            {
                "img": "https://arxiv.org/html/2509.07403/prompts/conv_gen.png",
                "caption": "Figure 34:Emotional ensemble generation prompt for Emotion Conversation.",
                "position": 2486
            },
            {
                "img": "https://arxiv.org/html/2509.07403/prompts/qa_gen.png",
                "caption": "Figure 35:Emotional ensemble generation prompt for Emotion QA.",
                "position": 2489
            },
            {
                "img": "https://arxiv.org/html/2509.07403/prompts/summary_gen.png",
                "caption": "Figure 36:Emotional ensemble generation prompt for Emotion Summary.",
                "position": 2492
            },
            {
                "img": "https://arxiv.org/html/2509.07403/prompts/expression_gen.png",
                "caption": "Figure 37:Emotional ensemble generation prompt for Emotion Expression. The prompt for the Emotion Expression task was originally structured in multiple stages; for better clarity and intuitive understanding, it has been consolidated into a single prompt.",
                "position": 2495
            }
        ]
    },
    {
        "header": "Appendix HComprehensive Prompt Collections",
        "images": []
    }
]