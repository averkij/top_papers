[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16767/x1.png",
                "caption": "",
                "position": 120
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16767/x2.png",
                "caption": "Figure 2:Pipeline of our character posing framework.Given a source shape and source/target skeletons, we encode them into latent representations with dense correspondence.\nA latent posing transformer then predicts the target shape tokens, which are finally decoded into the posed mesh.\nThis framework is trained in two stages.\nFirst, a latent loss is established to preserve geometric details.\nSecond, an adaptive completion module is exclusively finetuned with an SDF loss to synthesize plausible geometry for newly exposed structures.",
                "position": 186
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16767/x3.png",
                "caption": "Figure 3:Illustration of our key designs.(a) The skeleton encoder (Sec.3.2) produces dense pose representations with latent-level one-to-one correspondence. (b) Latent-space supervision (Sec.3.4) ensures a semantically meaningful token transformation path to preserve geometric details. (c) Adaptive tokens (Sec.3.5) are introduced in the finetuning stage to handle newly exposed structures after deformation.",
                "position": 287
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16767/x4.png",
                "caption": "Figure 4:Qualitative comparison on diverse characters and poses.We showcase results for re-posing each character into a widely-adopted T-pose and an additional random pose.\nOur method produces high-fidelity results across various cases. It robustly handles challenging inputs where MIA[MIA]and Puppeteer[song2025puppeteer]produce significant artifacts, and gives better pose conformance and detail preservation compared to HY3D-Omni[hunyuan3domni].\nNote that HY3D-Omni takes the rendered image as input, not the 3D shape itself.",
                "position": 416
            },
            {
                "img": "https://arxiv.org/html/2512.16767/x5.png",
                "caption": "Figure 5:Our method enables various applications,e.g., (a) animation, (b) part segmentation, (c) replacement, and (d) refinement.",
                "position": 493
            },
            {
                "img": "https://arxiv.org/html/2512.16767/x6.png",
                "caption": "Figure 6:Qualitative ablation results.Our designs are crucial for handling issues including rotational ambiguity, source-target correspondence, detail preservation, and newly exposed structures.",
                "position": 522
            }
        ]
    },
    {
        "header": "5Conclusion and Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experiments and Visualization",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16767/x7.png",
                "caption": "Figure S1:Visualization of newly exposed structures.Top row:The yellow areas highlight newly exposed surfaces (i.e., armpits) in the target pose that are not sampled from the source shape due to self-contact. Our adaptive tokens are trained to reconstruct these missing regions.Bottom row:A comparison of inference results with and without adaptive tokens. The model with adaptive tokens successfully reconstructs plausible shapes, whereas the model without them fails to produce the complete geometry.",
                "position": 1513
            },
            {
                "img": "https://arxiv.org/html/2512.16767/x8.png",
                "caption": "Figure S2:Textured results.We apply textures to our generated posed shapes using Hunyuan3D-Paint[hunyuan3d21]to demonstrate one feasible solution for realistic rendering.",
                "position": 1533
            }
        ]
    },
    {
        "header": "Appendix CSkinning Issue of Geometry-Space Rigging",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16767/x9.png",
                "caption": "Figure S3:Skinning artifacts in geometry-space rigging methods.The right hand of the tiger is positioned very close to its head (without making contact though).\nWhen re-posed, MIA[MIA]exhibits skinning leakage, causing some hand vertices to deform unnaturally with the head.\nPuppeteer[song2025puppeteer]also suffers from noticeable spiking artifacts due to the leaked skinning weights from hand to face.\nIn contrast, our method, which is not constrained by skinning, produces a clean posing result.",
                "position": 1548
            }
        ]
    },
    {
        "header": "Appendix DMore about Limitations and Future Work",
        "images": []
    }
]