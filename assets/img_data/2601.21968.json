[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Why Verbal Distillation?",
        "images": []
    },
    {
        "header": "3On-policy Verbal Distillation",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21968/x1.png",
                "caption": "Figure 3:The policy model performs trajectory sampling (left), while the teacher model applies rejection sampling and returns search results for Web Q&A (right).",
                "position": 373
            },
            {
                "img": "https://arxiv.org/html/2601.21968/pic/reward_convergence_qwen.png",
                "caption": "Table 3:Main results using different LLMs as the backbone. Results are obtained with 7B models unless otherwise specified (14B). The best performance is set in bold.â€ denotes sampling trajectories based on token probabilities over the score vocabulary during testing.",
                "position": 583
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21968/pic/reward_convergence_qwen.png",
                "caption": "Figure 5:Reward convergence on Qwen-2.5-7B across training steps under different rejection thresholds",
                "position": 2814
            },
            {
                "img": "https://arxiv.org/html/2601.21968/pic/reward_convergence_qwen.png",
                "caption": "Figure 5:Reward convergence on Qwen-2.5-7B across training steps under different rejection thresholds",
                "position": 2817
            },
            {
                "img": "https://arxiv.org/html/2601.21968/pic/reward_convergence_llama.png",
                "caption": "Figure 6:Reward convergence on LLaMA-3.2-3B across training steps under different rejection thresholds",
                "position": 2822
            }
        ]
    },
    {
        "header": "Appendix BRelated Work",
        "images": []
    }
]