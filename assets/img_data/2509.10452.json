[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.10452/x1.png",
                "caption": "Fig. 1:Our WhisTLE adaptation process. Speech–text pairs are\ntreated as usual (see left), while text data with no paired audio is passed through our frozen text-to-latent encoder (TLE), a trained variational auto-encoder (VAE) instead of the Whisper encoder, before being fed to the decoder (see right).",
                "position": 76
            }
        ]
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.10452/x2.png",
                "caption": "Fig. 2:Model architecture of our text-to-latent encoding VAE (purple), which also follows an encoder–decoder paradigm.",
                "position": 138
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": []
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusions and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]