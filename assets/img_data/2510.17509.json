[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17509/x1.png",
                "caption": "",
                "position": 133
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17509/x2.png",
                "caption": "Figure 1:The model’s confidence in answering a question is represented by the confidence of its most confident answer, computed via self-consistency as the proportion of generations agreeing with the greedy-search answer (Top). The model’s capability is reflected by the proportion of correct responses, measured as the fraction of generations matching the ground-truth answer (Bottom). These two signals show high correlation across questions.",
                "position": 172
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary",
        "images": []
    },
    {
        "header": "4EliCal:Elicitation-Then-Calibration",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17509/x3.png",
                "caption": "Figure 2:Self-consistency confidence vs. correctness on TQ (Qwen2.5-7B-Instruct).",
                "position": 319
            },
            {
                "img": "https://arxiv.org/html/2510.17509/x4.png",
                "caption": "Figure 3:EliCal reframes honesty alignment as a two-stage learning problem: 1) Confidence Elicitation, which constructs training data from a large set of questions with labels derived through self-consistency; 2) Confidence Calibration, which constructs correctness annotation using a small set of QA pairs to bridge the gap between the model’s expressed confidence and its actual accuracy.",
                "position": 367
            }
        ]
    },
    {
        "header": "5HonestyBench",
        "images": []
    },
    {
        "header": "6Experimental Setup",
        "images": []
    },
    {
        "header": "7Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17509/x5.png",
                "caption": "Figure 4:Average performance of training-free methods across all models in the in-domain setting.",
                "position": 576
            },
            {
                "img": "https://arxiv.org/html/2510.17509/x6.png",
                "caption": "Figure 5:AUROC of EliCal and Cal-Only as the scale of annotated data varies.",
                "position": 786
            },
            {
                "img": "https://arxiv.org/html/2510.17509/x7.png",
                "caption": "Figure 6:Alignment of EliCal and Cal-Only as the scale of annotated data varies.",
                "position": 815
            },
            {
                "img": "https://arxiv.org/html/2510.17509/x8.png",
                "caption": "Figure 7:The impact of training size on elicitation performance of Qwen2.5-7B-Instruct in the in-domain setting.",
                "position": 824
            }
        ]
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Ethics statement",
        "images": []
    },
    {
        "header": "Reproducibility statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelated Work",
        "images": []
    },
    {
        "header": "Appendix BFurther Analysis Using ECE",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17509/x9.png",
                "caption": "Figure 8:AUROC of EliCal and Cal-Only with different amounts of annotated data.",
                "position": 1586
            },
            {
                "img": "https://arxiv.org/html/2510.17509/x10.png",
                "caption": "Figure 9:Alignment of EliCal and Cal-Only with different amounts of annotated data.",
                "position": 1589
            },
            {
                "img": "https://arxiv.org/html/2510.17509/x11.png",
                "caption": "Figure 10:Alignment of EliCal and Cal-Only with different amounts of annotated data. Both methods just train a linear head.",
                "position": 1592
            },
            {
                "img": "https://arxiv.org/html/2510.17509/x12.png",
                "caption": "Figure 11:ECE of EliCal and Cal-Only with different amounts of annotated data.",
                "position": 1595
            }
        ]
    },
    {
        "header": "Appendix CImplementation Details",
        "images": []
    },
    {
        "header": "Appendix DDetails of LoRA",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17509/x13.png",
                "caption": "Figure 12:An example prompt for judging whether two responses are semantically consistent.",
                "position": 2239
            },
            {
                "img": "https://arxiv.org/html/2510.17509/x14.png",
                "caption": "Figure 13:An example prompt for asking the model to generate confidence in words.",
                "position": 2242
            },
            {
                "img": "https://arxiv.org/html/2510.17509/x15.png",
                "caption": "Figure 14:An example prompt for asking the model to generate confidence with 10 examples.",
                "position": 2245
            },
            {
                "img": "https://arxiv.org/html/2510.17509/x16.png",
                "caption": "Figure 15:An example QA prompt. For this question, the correct answer is Wilhelm Conrad Röntgen.",
                "position": 2248
            },
            {
                "img": "https://arxiv.org/html/2510.17509/x17.png",
                "caption": "Figure 16:An example prompt for judging whether a generated answer is correct.",
                "position": 2251
            }
        ]
    },
    {
        "header": "Appendix EDetails of Baselines",
        "images": []
    },
    {
        "header": "Appendix FThe Use of Large Language Models",
        "images": []
    },
    {
        "header": "Appendix GPrompts",
        "images": []
    }
]