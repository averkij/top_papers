[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.09789/img/Untitled.jpg",
                "caption": "Figure 1.A qualitative breakdown of a gameplay clip from a Naruto mobile fighting game (video ID 9183), demonstrating the potential of MLLMs. A MLLM (here, Qwen-VL) (i) extracts on-screen text via OCR (green), (ii) grounds entities and actions in the franchise’s world knowledge (red), and (iii) composes fine-grained, time-aligned contextual descriptions of the battle dynamics (blue). These detailed outputs could resonate with users, thereby improving the recommendation system’s performance.",
                "position": 85
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": []
    },
    {
        "header": "2.Related Work",
        "images": []
    },
    {
        "header": "3.Research Hypotheses",
        "images": []
    },
    {
        "header": "4.The Framework",
        "images": []
    },
    {
        "header": "5.Experimental Results",
        "images": []
    },
    {
        "header": "6.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]