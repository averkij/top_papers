[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Background: Mamba Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09590/extracted/6271945/figures/model.png",
                "caption": "Figure 1:Our proposed BIMBA model  uses a Mamba-based Spatiotemporal Token Selector to select a reduced number of salient tokens from a long sequence of features extracted via a pretrained image encoder. The token selection is optionally conditioned using the textual query to identify the features that are most informative for answering a given question. Finally, the selected and transformed tokens are passed to a large language model with a tokenized version of the input question to generate the answer.",
                "position": 157
            }
        ]
    },
    {
        "header": "4Technical Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09590/extracted/6271945/figures/scan.png",
                "caption": "Figure 2:(a): Architecture of our Spatiotemporal Token Selector. (b): Traditional selective scan with queries appended at the sequence’s start or end introduces positional biases that often lead to suboptimal performance. (c) We propose to interleave the queries uniformly to capture interactions between spatiotemporal tokens across the video more evenly. (d) Furthermore, we introduce a bidirectional selective scan (forward and backward) operation to improve the long-range modeling further.",
                "position": 227
            }
        ]
    },
    {
        "header": "5Experimental Setup",
        "images": []
    },
    {
        "header": "6Results and Analysis",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "S1Additional Implementation Details",
        "images": []
    },
    {
        "header": "S2Additional Quantitative Results",
        "images": []
    },
    {
        "header": "S3Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09590/extracted/6271945/figures/conversation_movie.png",
                "caption": "(a)Example 1 of open-ended video question answering.",
                "position": 1082
            },
            {
                "img": "https://arxiv.org/html/2503.09590/extracted/6271945/figures/conversation_movie.png",
                "caption": "(a)Example 1 of open-ended video question answering.",
                "position": 1085
            },
            {
                "img": "https://arxiv.org/html/2503.09590/extracted/6271945/figures/conversation_egoschema.png",
                "caption": "(b)Example 2 of open-ended video question answering.",
                "position": 1091
            },
            {
                "img": "https://arxiv.org/html/2503.09590/extracted/6271945/figures/example_nextqa_no_face.png",
                "caption": "Figure S4:Qualitative Results on NextQA. Our model generates the correct answer while both PLLaVA and LLaMA-3.2 (video) baselines fail.",
                "position": 1098
            },
            {
                "img": "https://arxiv.org/html/2503.09590/extracted/6271945/figures/example_egoschema.png",
                "caption": "Figure S5:Qualitative Results on EgoSchema. Our model generates the correct answer while both PLLaVA and LLaMA-3.2 (video) baselines fail.",
                "position": 1101
            },
            {
                "img": "https://arxiv.org/html/2503.09590/extracted/6271945/figures/question_condition_no_face.png",
                "caption": "Figure S6:Qualitative Results on Question Conditioned Token Selection on (a) NextQA and (b) EgoSchema datasets. Incorporating question tokens into our spatiotemporal token selector leads to the correct answer in both examples. Using the information from the questions allows our spatiotemporal selection module to focus on the most relevant video parts for answering the question.",
                "position": 1104
            },
            {
                "img": "https://arxiv.org/html/2503.09590/extracted/6271945/figures/mamba_bidir_inter_ego.png",
                "caption": "Figure S7:Visualization of Bidirectional Mamba and Interleave Queries. Utilizing bidirectional Mamba and interleaved queries leads to the correct answer, while the unidirectional Mamba and standard queries fail.",
                "position": 1107
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]