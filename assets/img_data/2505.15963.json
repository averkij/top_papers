[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15963/x1.png",
                "caption": "Figure 1:Offline training (a) relies on static, predefined datasets and fails to adapt to the model‚Äôs evolving failure patterns, limiting its ability to address hallucinations effectively. Moreover, neglecting the role of visual input will result in overfitting to language priors. In contrast, OViP (b) combines online preference learning with image-aware training in a unified framework, enabling real-time data construction grounded in model behavior.",
                "position": 166
            }
        ]
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15963/x2.png",
                "caption": "Figure 2:Overview of OViP. Given an image and a query, we employ the current modelœÄtsubscriptùúãùë°\\pi_{t}italic_œÄ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTto generate multiple responses, which are then evaluated by an external LLM with reference to the ground truth. We filter and select response pairs and use a diffusion model to generate corresponding negative images. The collected data are used to updateœÄtsubscriptùúãùë°\\pi_{t}italic_œÄ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. The filtering strategy is detailed in Section2.2.",
                "position": 175
            }
        ]
    },
    {
        "header": "3Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15963/x3.png",
                "caption": "Figure 3:Effect of auxiliary loss on text-side and image-side losses. HRI measures hallucination-reduction, while General Score reflects general performance change. Marker color indicates the primary loss function used, and marker shape denotes the usage of auxiliary loss.",
                "position": 976
            },
            {
                "img": "https://arxiv.org/html/2505.15963/x3.png",
                "caption": "Figure 3:Effect of auxiliary loss on text-side and image-side losses. HRI measures hallucination-reduction, while General Score reflects general performance change. Marker color indicates the primary loss function used, and marker shape denotes the usage of auxiliary loss.",
                "position": 979
            },
            {
                "img": "https://arxiv.org/html/2505.15963/x4.png",
                "caption": "Figure 4:Results of offline and online training strategy with DPO and OViP. Cover measures the informativeness of model response from AMBER generative benchmark.",
                "position": 984
            }
        ]
    },
    {
        "header": "4Further Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15963/x5.png",
                "caption": "Figure 5:Results using different training strategy. Offline represents training with existing dataset. Off Policy means training with sampled data and Iterative means the dataset of the epoch 2 is from model sampling.",
                "position": 1087
            },
            {
                "img": "https://arxiv.org/html/2505.15963/x5.png",
                "caption": "Figure 5:Results using different training strategy. Offline represents training with existing dataset. Off Policy means training with sampled data and Iterative means the dataset of the epoch 2 is from model sampling.",
                "position": 1090
            },
            {
                "img": "https://arxiv.org/html/2505.15963/x6.png",
                "caption": "Figure 6:Comparison of response quality distributions after training with different methods.\nThe x-axis represents the semantic quality of model responses, with the leftmost region indicating severe hallucinations. The y-axis denotes the probability density over sampled responses. A rightward shift in the distribution corresponds to higher overall response quality and reduced hallucination.",
                "position": 1096
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AEvaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15963/x7.png",
                "caption": "Figure 7:Text-only LLM can not correctly judge the response.",
                "position": 1625
            },
            {
                "img": "https://arxiv.org/html/2505.15963/x8.png",
                "caption": "Figure 8:Rule-based extraction will lead to misjudgments to some extent.",
                "position": 1628
            },
            {
                "img": "https://arxiv.org/html/2505.15963/x9.png",
                "caption": "Figure 9:OPA-DPO fails to mention the man, a deficiency that is captured by the ‚ÄúCover‚Äù metric but often overlooked in previous evaluations. ‚Äúvehicles‚Äù is incorrectly identified as a hallucination word.",
                "position": 1631
            }
        ]
    },
    {
        "header": "Appendix BExperiments",
        "images": []
    },
    {
        "header": "Appendix CAlgorithm",
        "images": []
    },
    {
        "header": "Appendix DEfficiency and Time Consuming",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15963/x10.png",
                "caption": "Figure 10:Time consumption for each stage during training.",
                "position": 2091
            }
        ]
    },
    {
        "header": "Appendix ELimitations",
        "images": []
    },
    {
        "header": "Appendix FBroader Impacts",
        "images": []
    },
    {
        "header": "Appendix GPrompts for Judgment and Negative Image Generation",
        "images": []
    }
]