[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06855/x1.png",
                "caption": "Figure 1:Example of anAIRS-Benchtask. Each task is specified by a {problem, dataset, metric} triplet. Theproblemdefines the core computational challenge to be solved (e.g. textual similarity); thedatasetspecifies which data to solve the challenge over (e.g. SICK); finally, themetricis used to quantify performance (e.g. Spearman correlation). The agent receives the full task specification and is expected to develop a solution that in most cases generates predictions on the test labels file, which are then evaluated and compared with the state-of-the-art result.",
                "position": 341
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Agents, Scaffolds, Harnesses",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06855/x2.png",
                "caption": "Figure 2:We define anagentas a pair consisting of a large language model (LLM) and a scaffold. Ascaffoldcomprises a set of mechanisms, such as operators and search algorithms, that enable the LLM to explore the solution space effectively. Scaffolds are instantiated by aharness, which serves as a system that encapsulates the agent and manages its research process. Theenvironmentprovides the agent with the problem specifications, as well as any constraints and resources available for its exploration.",
                "position": 1099
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06855/x3.png",
                "caption": "Figure 3:Distribution ofAIRS-Benchtasks by category. We consider 7 distinct task categories in total:Code,Math,Molecules & Proteins ML,Question Answering,Text Classification,Text Extraction & Matching, andTime Series.",
                "position": 1120
            },
            {
                "img": "https://arxiv.org/html/2602.06855/x4.png",
                "caption": "Table 2:Core fields of theMathQuestionAnsweringSVAMPAccuracytask stored in itsmetadata.yamlfile.",
                "position": 1142
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06855/x4.png",
                "caption": "Figure 4:Overall performance of the 14 evaluated agents on the three metrics introduced in Section 5.2, namely valid submission rate, average normalized score and Elo rating. Results are ordered by increasing average normalized score.",
                "position": 1324
            },
            {
                "img": "https://arxiv.org/html/2602.06855/x5.png",
                "caption": "Figure 5:Submission rate distribution for the 14 agents tested. Each bar shows the distribution of submission rates across tasks for a given agent. The categories are defined as follows:invalidindicates that the agent did not provide any valid submission for that task (0% valid submissions);low (1–33%)indicates a valid submission for between 1% and 33% of seeds;medium (34–66%)indicates a valid submission for between 34% and 66% of seeds; andhigh (67–100%)indicates a valid submission for more than 66% of seeds. Agents are sorted by the combined percentage of seeds in themediumandhighcategories, highlighting those most reliable across the benchmark.",
                "position": 1333
            },
            {
                "img": "https://arxiv.org/html/2602.06855/x6.png",
                "caption": "Figure 6:Performance distribution for the 14 agents evaluated. Each bar represents the percentage of tasks across all seeds for which a given agent falls into one of five performance categories:invalid(no valid submission for the task),worst(the lowest normalized score among all agents for the task),below average(normalized score below the mean but not the worst),above average(normalized score above the mean but not the best), andbest(the highest normalized score for the task). Normalized scores are computed per task according to equations2and3. Agents are sorted by the number of tasks for which they achieved thebestandabove averageperformances, highlighting those with the most consistent top performance across the benchmark.",
                "position": 1343
            },
            {
                "img": "https://arxiv.org/html/2602.06855/x7.png",
                "caption": "Figure 7:Mean valid submission rate (VSR) for the 14 agents evaluated, with error bars indicating the95%95\\%confidence intervals. VSR is computed according to Eq.1. The overall VSR across all runs and agents averages at59.3%59.3\\%indicating that even submitting a valid solution is non-trivial for the agents’ capabilities.",
                "position": 1351
            },
            {
                "img": "https://arxiv.org/html/2602.06855/x8.png",
                "caption": "Figure 8:Average normalized scores for the 14 agents evaluated, with error bars indicating the95%95\\%confidence intervals. Scores are computed according to Equations2and3. The overall average normalized score across all runs and agents averages at24.1%24.1\\%, highlighting the challenging nature ofAIRS-Bench.",
                "position": 1354
            },
            {
                "img": "https://arxiv.org/html/2602.06855/x9.png",
                "caption": "Figure 9:Average normalized scores with each row corresponding to an AIRS-Bench task and each point to an agent’s normalized score for that task averaged across multiple seeds. For each task, the outcome of the worst-performing run is used as the baseline score. SOTA always corresponds to a normalized score of 1. Tasks are ranked in decreasing order according to the average score across all agents. See TableLABEL:tab:task-keyfor the correspondence between tasks numbers on the y axis and names.",
                "position": 1363
            },
            {
                "img": "https://arxiv.org/html/2602.06855/x10.png",
                "caption": "Figure 10:Normalized score per task difficulty level computed according to Equations2-3. We divide the task ranking of Figure9into four categories with decreasing normalized scores:easy,medium,hardandexpert.",
                "position": 1373
            },
            {
                "img": "https://arxiv.org/html/2602.06855/x11.png",
                "caption": "Figure 11:Elo ratings of all agents, estimated by fitting a Bradley–Terry model on the pairwise comparisons of agents’ scores for each task. The human SOTA score is also included as an additional opponent. The Greedy scaffold outperforms other scaffolds in most cases. Bar height represents the median of a bootstrap distribution using 100 resamples, with the error bars representing the 95% confidence intervals.",
                "position": 1376
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ATask Selection",
        "images": []
    },
    {
        "header": "Appendix BAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06855/x12.png",
                "caption": "Figure 12:Normalized score per task averaged over seeds, computed according to Equations2-8. For each task, the outcome of the worst-performing run is used as the baseline scorestmins^{\\textrm{min}}_{t}. SOTA always corresponds to a normalized score of 1. Tasks are ranked in decreasing order according to the average score across all agents. See TableLABEL:tab:task-keyfor the correspondence between tasks ranking and name.",
                "position": 3024
            },
            {
                "img": "https://arxiv.org/html/2602.06855/x13.png",
                "caption": "Figure 13:Normalized score per task difficulty level computed according to Equations2-8. We divide the task ranking of Figure12into four categories with decreasing normalized scores:easy,medium,hardandexpert.",
                "position": 3029
            }
        ]
    },
    {
        "header": "Appendix CHarness Setup",
        "images": []
    },
    {
        "header": "Appendix DCompute Requirements of Benchmarks",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06855/x14.png",
                "caption": "Table 8:Summary of compute, runtime, and cost information for recent LLM-agent benchmarks.",
                "position": 4947
            }
        ]
    },
    {
        "header": "Appendix ECached Models",
        "images": []
    },
    {
        "header": "Appendix FDistribution of tasks SOTA venue and year",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06855/x14.png",
                "caption": "(a)Breakdown of tasks by SOTA publication venue.",
                "position": 6133
            },
            {
                "img": "https://arxiv.org/html/2602.06855/x14.png",
                "caption": "(a)Breakdown of tasks by SOTA publication venue.",
                "position": 6136
            },
            {
                "img": "https://arxiv.org/html/2602.06855/x15.png",
                "caption": "(b)Breakdown of tasks by SOTA publication year.",
                "position": 6141
            }
        ]
    },
    {
        "header": "Appendix GAIRS-Bench Task Description",
        "images": []
    }
]