[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15207/x1.png",
                "caption": "Figure 1:Top: Comparison between distribution-matching and reward-maximizing approaches. FlowRL (left) learns to match the full reward distribution, maintaining diversity across multiple modes with low KL divergence. In contrast, reward-maximizing methods like GRPO (right) concentrate on a single high-reward peak, leading to mode collapse and higher KL divergence.Bottom: Performance comparison. FlowRL consistently outperforms GRPO across math and code domains.",
                "position": 217
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15207/x2.png",
                "caption": "Figure 2:GFlowNets[Bengio et al.,2023a], a flow-balance perspective on reinforcement learning.\nThe initial flowZϕ​(s0)Z_{\\phi}(s_{0})injects probability mass into the environment, which is transported through intermediate states by the policyπθ\\pi_{\\theta}and accumulated at terminal states in proportion to the scalar rewards.",
                "position": 298
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Experiment Settings",
        "images": []
    },
    {
        "header": "6Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15207/x3.png",
                "caption": "Figure 3:Ablation study on theβ\\betain FlowRL.β=15\\beta=15(highlighted in blue) achieves the best performance.",
                "position": 759
            }
        ]
    },
    {
        "header": "7Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15207/x4.png",
                "caption": "Figure 4:GPT-judged diversity scores on rollouts of AIME 24/25 problems. FlowRL generates more diverse solutions than R++, GRPO, and PPO.",
                "position": 783
            }
        ]
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProof of Proposition1",
        "images": []
    },
    {
        "header": "Appendix BTheoretical Analysis",
        "images": []
    },
    {
        "header": "Appendix CGFlowNets",
        "images": []
    }
]