[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.01830/x1.png",
                "caption": "Figure 1:Comparison between previous red-teaming approaches andAuto-RT. Previous works focused on identifying safety flaws of the target model under given attack strategies, whereasAuto-RTdirectly explores systematic safety flaws in target models starting from searching strategies itself, enabling a fully automated process.",
                "position": 136
            },
            {
                "img": "https://arxiv.org/html/2501.01830/x2.png",
                "caption": "Figure 2:The framework ofAuto-RT, comprising two key components: 1) Early-terminated Exploration, which assesses the diversity of the generated strategies and the consistency of the rephrased prompt with the initial toxic behavior to determine the necessity of safety evaluation. If either constraint is unmet, the process immediately terminate, and a penalty is applied; 2) Progressive Reward\nTracking, which enhances the density of safety rewards by utilizing a degrade model derived from the target model, thereby improving the efficiency and effectiveness of the exploration process.",
                "position": 145
            }
        ]
    },
    {
        "header": "1Preliminary: Red-Teaming Aligned LLMs",
        "images": []
    },
    {
        "header": "2Auto Red-Teaming",
        "images": []
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.01830/x3.png",
                "caption": "Figure 4:Comparison of attack efficiency betweenAuto-RTand RL.The violin plots represent the distribution of attack success rates for every 1k sampled strategies, with lighter colors indicatingAuto-RTand darker colors representing RL.Auto-RTachieves higher attack success rates than RL under the same number of samples, and with larger variance, indicating that it achieves more comprehensive exploration.",
                "position": 1697
            }
        ]
    },
    {
        "header": "4Main Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.01830/x4.png",
                "caption": "Figure 5:The relationship between the red-teaming outcomes (Attack ASR) following reward shaping with a series of intermediate models (M1 to M6), the safety levels of these models (Weaken ASR), and their first inverse rate for additional toxic behavior (Weaken FIR). These intermediate models are derived by fine-tuning on six target models using varying amounts of toxic data.The optimal red-teaming results are achieved by selecting the last intermediate model before a sudden spike in FIR (represented by the dark-colored bar in the figure) as the degrade model for reward shaping.",
                "position": 2435
            }
        ]
    },
    {
        "header": "5Related Works",
        "images": []
    },
    {
        "header": "6Conclusions and Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ATarget Model Used",
        "images": []
    },
    {
        "header": "Appendix BBaseline implementation Details",
        "images": []
    },
    {
        "header": "Appendix CEvaluation Metrics",
        "images": []
    },
    {
        "header": "Appendix DImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.01830/extracted/6109106/resources/sampling.png",
                "caption": "Figure 9:We evaluate the attack success rates ofFew-Shotattack against different target models under varying sampling sizes. The entire attack process is segmented into multiple stages based on the sampling size, and the distribution of attack outcomes within each stage is then analyzed.",
                "position": 4083
            },
            {
                "img": "https://arxiv.org/html/2501.01830/x5.png",
                "caption": "Figure 10:We evaluate the attack success rates ofRLattack against different target models under varying sampling sizes. The entire attack process is segmented into multiple stages based on the sampling size, and the distribution of attack outcomes within each stage is then analyzed.",
                "position": 4086
            },
            {
                "img": "https://arxiv.org/html/2501.01830/x6.png",
                "caption": "Figure 11:We evaluate the attack success rates ofImitate Learningattack against different target models under varying sampling sizes. The entire attack process is segmented into multiple stages based on the sampling size, and the distribution of attack outcomes within each stage is then analyzed.",
                "position": 4089
            },
            {
                "img": "https://arxiv.org/html/2501.01830/x7.png",
                "caption": "Figure 12:We evaluate the attack success rates ofAuto-RTagainst different target models under varying sampling sizes. The entire attack process is segmented into multiple stages based on the sampling size, and the distribution of attack outcomes within each stage is then analyzed.",
                "position": 4092
            }
        ]
    },
    {
        "header": "Appendix EMore Experimental Results",
        "images": []
    }
]