[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01853/x1.png",
                "caption": "Figure 1:ShapeLLM-Omni inherits Qwen2.5-vlâ€™s strong multimodal capabilities and additionally supports text-to-3D, image-to-3D, 3D captioning, and 3D editing using text instruction.",
                "position": 125
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01853/x2.png",
                "caption": "Figure 2:The pipeline of 3D VQVAE, which can compress voxels into discrete tokens.",
                "position": 198
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01853/x3.png",
                "caption": "Figure 3:Our proposed 3D-Alpaca dataset comprises 3D generation, 3D understanding, and 3D editing components, providing a comprehensive foundation for training and evaluating 3D large language models.",
                "position": 340
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01853/x4.png",
                "caption": "Figure 4:Comparisons with other baselines on the image-to-3D task.Our results demonstrate more complete geometry and high-fidelity textures compared to baselines, enabling photorealistic image-to-3D generation.",
                "position": 596
            },
            {
                "img": "https://arxiv.org/html/2506.01853/x5.png",
                "caption": "Figure 5:Comparisons with other baselines on text-to-3d task.Compared to other methods, our results show better text alignment, with generated 3D shapes accurately reflecting the input descriptions.",
                "position": 685
            },
            {
                "img": "https://arxiv.org/html/2506.01853/x6.png",
                "caption": "Figure 6:Some cases of 3D editing result from our method.Our method enables the editing of 3D assets based on textual instructions while preserving their original identity and visual consistency.",
                "position": 694
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix AMore details about 3D-Alpaca",
        "images": []
    },
    {
        "header": "Appendix BMore Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01853/extracted/6505128/fig2/pipe2.jpg",
                "caption": "Figure 7:About how to generate 3d mesh from voxel.The upper part illustrates the process of reconstructing a textured mesh from voxel inputs using a texture transformer[91]and mesh decoder. In contrast, the lower part demonstrates the pipeline for reconstructing a non-textured mesh directly from voxels. Both reconstruction pathways are optional and can be flexibly applied based on the needs of specific applications.",
                "position": 736
            },
            {
                "img": "https://arxiv.org/html/2506.01853/extracted/6505128/fig2/loss1.jpg",
                "caption": "Figure 8:Training Loss Curve and Testing Loss Curve",
                "position": 749
            },
            {
                "img": "https://arxiv.org/html/2506.01853/x7.png",
                "caption": "Figure 9:Qualitative results on Objaverse.",
                "position": 757
            },
            {
                "img": "https://arxiv.org/html/2506.01853/x8.png",
                "caption": "Figure 10:Some cases of 3D editing result from our method.",
                "position": 763
            },
            {
                "img": "https://arxiv.org/html/2506.01853/x9.png",
                "caption": "Figure 11:Some cases of our 3D-Editing Data",
                "position": 1905
            },
            {
                "img": "https://arxiv.org/html/2506.01853/x10.png",
                "caption": "Figure 12:More cases of Image-to-3D result from our method.",
                "position": 1908
            },
            {
                "img": "https://arxiv.org/html/2506.01853/x11.png",
                "caption": "Figure 13:More cases of Image-to-3D result from our method.",
                "position": 1911
            },
            {
                "img": "https://arxiv.org/html/2506.01853/",
                "caption": "Figure 14:More cases of Image-to-3D result from our method.",
                "position": 1914
            },
            {
                "img": "https://arxiv.org/html/2506.01853/x13.png",
                "caption": "Figure 15:More cases of Text-to-3D result from our method.",
                "position": 1917
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]