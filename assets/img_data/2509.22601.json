[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22601/figures/spear-logo.png",
                "caption": "",
                "position": 44
            },
            {
                "img": "https://arxiv.org/html/2509.22601/figures/spear-logo_blue.png",
                "caption": "",
                "position": 155
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22601/figures/framework-concept_benchmark.png",
                "caption": "Figure 1:The core concept of our proposedSPEARfor training long-horizon LLM agents via group-based RL.\nCompared with the vanilla GRPO-like algorithms, we introduce the curriculum-based self-imitation learning with intrinsic reward shaping.\nGiven the same data input, a group of trajectories are generated with multi-turn tool interactions and then engaged for episode-level reward computation and advantage estimation.\nThen, we propose filtering valuable good trajectories to update the replay buffer, where the stored past experiences guide the agent to explore effectively on sparsely rewarded tasks via self-imitation.\nThe total training batch contains both on-policy and off-policy data from the replay buffer.\nWith self-guided progressive exploration,SPEARboosts performance of various LLMs and baselines on ALFWorld, WebShop, and AIME24/25 in a plug-and-play manner.",
                "position": 182
            },
            {
                "img": "https://arxiv.org/html/2509.22601/figures/overview.png",
                "caption": "Figure 2:Overview ofSPEARin terms of data flow. During each episode, the agent interacts with the environment to generate a set of trajectories. These trajectories are processed along two complementary paths. First, they are used for intrinsic reward shaping, advantage estimation, and on-policy updates, following a mechanism similar to the vanilla GRPO. Second, they are selectively filtered and stored in a replay buffer, enabling off-policy updates through the proposed self-imitation scheme with advantage recalibration and regularization. This dual integration allows the agent to maximize the utility of rewarding past experiences, thereby expanding the exploration space effectively, while simultaneously mitigating persistent over-uncertainty in decision-making under shifting distributions of external feedback. As a result,SPEARachieves a stable balance between exploration and exploitation through self-guided policy adaptation.",
                "position": 225
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Training Agentic LLMs withSPEAR",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22601/figures/entropy-anno.png",
                "caption": "(a)Entropy (seq-mean-token-sum-norm).",
                "position": 659
            },
            {
                "img": "https://arxiv.org/html/2509.22601/figures/entropy-anno.png",
                "caption": "(a)Entropy (seq-mean-token-sum-norm).",
                "position": 662
            },
            {
                "img": "https://arxiv.org/html/2509.22601/figures/aime25_sil_anno.png",
                "caption": "(b)Accuracy on AIME 2025.",
                "position": 667
            },
            {
                "img": "https://arxiv.org/html/2509.22601/x1.png",
                "caption": "Figure 4:Visualization ofŒ≥\\gammawithTwarm-up=200T_{\\text{warm-up}}=200.\nThe weight of SIL loss gradually increases from 0 to 1 in the firstTwarm-upT_{\\text{warm-up}}steps.",
                "position": 766
            },
            {
                "img": "https://arxiv.org/html/2509.22601/figures/num_turns_toolcall-anno.png",
                "caption": "(a)Number of tool-call turns.",
                "position": 831
            },
            {
                "img": "https://arxiv.org/html/2509.22601/figures/num_turns_toolcall-anno.png",
                "caption": "(a)Number of tool-call turns.",
                "position": 834
            },
            {
                "img": "https://arxiv.org/html/2509.22601/figures/aime25_toolcall-anno.png",
                "caption": "(b)Accuracy on AIME 2025.",
                "position": 839
            },
            {
                "img": "https://arxiv.org/html/2509.22601/x2.png",
                "caption": "Figure 6:Visualization of the composite intrinsic reward of outcome reward, tool-call reward, and format reward (Tdecay=200T_{\\text{decay}}=200).\nThe tool-call reward gradually decays from 1 to 0 in the first 200 training steps.",
                "position": 904
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22601/figures/buffer_size.jpg",
                "caption": "(a)Replay Buffer SizeNùíüN_{\\mathcal{D}}.",
                "position": 2203
            },
            {
                "img": "https://arxiv.org/html/2509.22601/figures/buffer_size.jpg",
                "caption": "(a)Replay Buffer SizeNùíüN_{\\mathcal{D}}.",
                "position": 2206
            },
            {
                "img": "https://arxiv.org/html/2509.22601/figures/baseline_buffer_size.jpg",
                "caption": "(b)Baseline Buffer SizeNùíüRN_{\\mathcal{D}_{R}}.",
                "position": 2211
            },
            {
                "img": "https://arxiv.org/html/2509.22601/figures/clipping_ratio.jpg",
                "caption": "(c)Covariance-based Clipping RatioŒª\\lambda.",
                "position": 2217
            },
            {
                "img": "https://arxiv.org/html/2509.22601/figures/warm-up.jpg",
                "caption": "(d)Warm-up StepsTwarm-upT_{\\text{warm-up}}.",
                "position": 2222
            },
            {
                "img": "https://arxiv.org/html/2509.22601/figures/decay.jpg",
                "caption": "(e)Decay StepsTdecayT_{\\text{decay}}.",
                "position": 2228
            },
            {
                "img": "https://arxiv.org/html/2509.22601/figures/before-purpose.png",
                "caption": "(a)Before RL training.",
                "position": 2308
            },
            {
                "img": "https://arxiv.org/html/2509.22601/figures/before-purpose.png",
                "caption": "(a)Before RL training.",
                "position": 2311
            },
            {
                "img": "https://arxiv.org/html/2509.22601/figures/after-purpose.png",
                "caption": "(b)After RL training.",
                "position": 2316
            },
            {
                "img": "https://arxiv.org/html/2509.22601/figures/coding-case.png",
                "caption": "(c)The evolution of efficient coding from the purpose of computation to verification.",
                "position": 2322
            },
            {
                "img": "https://arxiv.org/html/2509.22601/figures/webshop-action-case.png",
                "caption": "Figure 9:The advancement of strategy from the search query perfectionism to goal-oriented active progression.",
                "position": 2343
            },
            {
                "img": "https://arxiv.org/html/2509.22601/figures/time-alfworld.png",
                "caption": "(a)ALFWorld.",
                "position": 2496
            },
            {
                "img": "https://arxiv.org/html/2509.22601/figures/time-alfworld.png",
                "caption": "(a)ALFWorld.",
                "position": 2499
            },
            {
                "img": "https://arxiv.org/html/2509.22601/figures/time-webshop.png",
                "caption": "(b)WebShop.",
                "position": 2504
            },
            {
                "img": "https://arxiv.org/html/2509.22601/figures/time-retool.png",
                "caption": "(c)DAPO-MATH-17K.",
                "position": 2509
            },
            {
                "img": "https://arxiv.org/html/2509.22601/figures/Dr.BoT-before.png",
                "caption": "Table 6:Success rate (%) of the visual agent for playing Sokoban.",
                "position": 2545
            },
            {
                "img": "https://arxiv.org/html/2509.22601/figures/Dr.BoT-before.png",
                "caption": "(a)Before (step 15).",
                "position": 2619
            },
            {
                "img": "https://arxiv.org/html/2509.22601/figures/Dr.BoT-after.png",
                "caption": "(b)After (step 125).",
                "position": 2622
            }
        ]
    },
    {
        "header": "6Conclusions and Limitations",
        "images": []
    },
    {
        "header": "Contributions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]