[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05396/x1.png",
                "caption": "Figure 1:Analysis of attention patterns in Mistral-7B performing In-context Ranking (ICR) on MSMarco. (left) Attention averaged over middle layers 16-21 reveals structural sparsity — a strong diagonal (intra-document attention needed for local context processing) and significant attention to the first row (focus on the query-based instruction). (middle) Attention in Layer 18 from individual query tokens to document segments.\nCertain tokens (the last token, ‘:’) attend primarily to the relevant document only (i.e., Doc24, highlighted in green).\n(right) Attention from final query tokens across layers shows retrieval signals strengthening in middle layers. These patterns motivate ourBlockRankapproach.",
                "position": 174
            },
            {
                "img": "https://arxiv.org/html/2510.05396/x2.png",
                "caption": "",
                "position": 183
            },
            {
                "img": "https://arxiv.org/html/2510.05396/x3.png",
                "caption": "",
                "position": 188
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05396/x4.png",
                "caption": "Figure 2:BlockRankstarts with chunking the full prompt into segments and then processes it using structured attention, where the documents only attend to themselves and the instruction segment, while the query segment attends to the full prompt. It also incorporates an auxiliary attention loss (ℒaux\\mathcal{L}_{\\text{aux}}) from a middle layer (l∗l^{*}) that increases sharpness of attention on the relevant documents and enables an alternate inference mechanism using attention scores derived froml∗l^{*}.",
                "position": 246
            }
        ]
    },
    {
        "header": "2Problem Setup and Related Work",
        "images": []
    },
    {
        "header": "3Emergence of Structured Attention in In-context Ranking",
        "images": []
    },
    {
        "header": "4BlockRank: Blockwise In-context Ranking",
        "images": []
    },
    {
        "header": "5Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05396/x5.png",
                "caption": "Figure 4:P@1 and Latency (annotated) ofBlockRankvs Full-FT Mistral, scalingNNon MSMarco.",
                "position": 903
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASocietal Impact",
        "images": []
    },
    {
        "header": "Appendix BDataset and Hyperparameter Details",
        "images": []
    },
    {
        "header": "Appendix CAttention Complexity Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05396/x6.png",
                "caption": "Figure 5:Performance of Full-FT model’s attention-based inference vs the query token for which attention scores are extracted from.",
                "position": 1680
            },
            {
                "img": "https://arxiv.org/html/2510.05396/x7.png",
                "caption": "Figure 6:Performance of Full-FT model’s attention-based inference as a function of the Transformer Layer Index from which attention scores are extracted (MSMarco).",
                "position": 1686
            },
            {
                "img": "https://arxiv.org/html/2510.05396/x8.png",
                "caption": "Figure 7:Layerwise Attention Precision@1 on a held-out subset of MSMarco training data vs training steps for Full-FT model",
                "position": 1697
            }
        ]
    },
    {
        "header": "Appendix DAdditional Results",
        "images": []
    }
]