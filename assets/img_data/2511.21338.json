[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Experimental Setup",
        "images": []
    },
    {
        "header": "4Are Diffusion Language Models Location-Sensitive?",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21338/x1.png",
                "caption": "Figure 1:MDLMs display a recency bias.The performance of both MDLMs (LLaDA and Dream) and ARLMs is sensitive to the placement of relevant information within the context. For MDLMs, the performance degrades significantly when the relevant information is placed far away from the test question, suggesting a recency bias.",
                "position": 286
            },
            {
                "img": "https://arxiv.org/html/2511.21338/x2.png",
                "caption": "Figure 2:MDLMs prioritise information placed closest to the mask.All studied MDLMs perform best when relevant information is near the masked token, regardless of question position.",
                "position": 296
            },
            {
                "img": "https://arxiv.org/html/2511.21338/x3.png",
                "caption": "Figure 3:Gradient attribution analysis further illuminates the locality bias of the models.Although all models display the characteristic U-shaped behaviour, MDLMs demonstrate more uniform gradients across different positions, indicating reduced locality bias compared to their ARLM counterparts.",
                "position": 313
            }
        ]
    },
    {
        "header": "5The Distracting Effect of Extra Masks",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21338/x4.png",
                "caption": "Figure 4:Performance of LLaDA decreases significantly with added mask tokens,while Dream is more robust to the extra masks.",
                "position": 344
            },
            {
                "img": "https://arxiv.org/html/2511.21338/x5.png",
                "caption": "Figure 5:For LLaDA, performance degradation becomes more significant as the context length increases.We do not observe a similar effect for Dream, which is robust to the effect of extra masks.",
                "position": 366
            },
            {
                "img": "https://arxiv.org/html/2511.21338/x6.png",
                "caption": "Figure 6:Extra dots do not degrade performance as strongly as extra masks.",
                "position": 383
            },
            {
                "img": "https://arxiv.org/html/2511.21338/x7.png",
                "caption": "Figure 7:Unmasking recovers accuracy lost to mask-induced distraction. Both unmasking strategies improve performance compared to no (i.e. 1-step) unmasking (None, blue).",
                "position": 401
            },
            {
                "img": "https://arxiv.org/html/2511.21338/x8.png",
                "caption": "Figure 8:Extra masks diminish the locality bias.We measure performance sensitivity to the location of relevant information as extra masks are added. With more masks, accuracy becomes less dependent on the location of the relevant examples, mainly because it declines across all positions.",
                "position": 420
            }
        ]
    },
    {
        "header": "6Reducing the Distracting Effect Through Mask-Agnostic SFT",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21338/x9.png",
                "caption": "Figure 9:MA loss rectifies the effect of extra masks.",
                "position": 482
            },
            {
                "img": "https://arxiv.org/html/2511.21338/x10.png",
                "caption": "Figure 10:MA loss (CE+TV) reduces the locality bias of LLaDA-Base.",
                "position": 494
            }
        ]
    },
    {
        "header": "7Discussion, Conclusions and Future Work",
        "images": []
    },
    {
        "header": "Appendix Contents",
        "images": []
    },
    {
        "header": "Appendix AAdditional Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21338/x11.png",
                "caption": "Figure 11:Recency bias in LLaDA-MoE (re: Fig 1).LLaDA-Moe-Instruct displays a strong recency bias, as seen also in other MDLMs, while the performance of LLaDA-MoE-Base is more agnostic to the location of relevant examples within the context.",
                "position": 553
            },
            {
                "img": "https://arxiv.org/html/2511.21338/x12.png",
                "caption": "Figure 12:Locality bias in LLaDA-MoE (re: Fig 2).LLaDA-MoE-Instruct displays a strong locality bias, as seen also in other MDLMs (the performance is best when the relevant examples are located close to the masked question). The performance of LLaDA-MoE-Base is more uniform across the locations of relevant examples, although at a significantly lower level overall.",
                "position": 556
            },
            {
                "img": "https://arxiv.org/html/2511.21338/x13.png",
                "caption": "Figure 13:Gradient attribution analysis reveals recency bias in LLaDA-MoE (re: Fig 3).Both LLaDA-MoE-Base and LLaDA-MoE-Instruct display a strong recency and primacy bias, based on the gradient attribution analysis.",
                "position": 559
            },
            {
                "img": "https://arxiv.org/html/2511.21338/x14.png",
                "caption": "Figure 14:Performance of LLaDA-MoE decreases significantly with added masks (re: Fig. 4).",
                "position": 562
            },
            {
                "img": "https://arxiv.org/html/2511.21338/x15.png",
                "caption": "Figure 15:For LLaDA-MoE, the performance degradation becomes more significant as the context length increases (re: Fig 5).This effect is particularly visible in LLaDA-MoE-Instruct.",
                "position": 565
            },
            {
                "img": "https://arxiv.org/html/2511.21338/x16.png",
                "caption": "Figure 16:For LLaDA-MoE, extra dots do not degrade performance as strongly as extra masks (re: Fig 6).",
                "position": 568
            },
            {
                "img": "https://arxiv.org/html/2511.21338/x17.png",
                "caption": "Figure 17:Extra masks alter the locality bias in LLaDA-MoE (re: Fig. 8). For both the Base and the Instruct model, the performance becomes significantly worse as we add extra masks, across all locations. For LLaDA-MoE-Instruct in particular, the performance is more uniform across most locations with the extra masks.",
                "position": 571
            },
            {
                "img": "https://arxiv.org/html/2511.21338/x18.png",
                "caption": "Figure 18:Gradient attribution analysis confirms locality bias in MDLMs.Normalised gradient attribution results for when the target question is placed in themiddleof the input context.",
                "position": 593
            },
            {
                "img": "https://arxiv.org/html/2511.21338/x19.png",
                "caption": "Figure 19:Gradient attribution analysis confirms locality bias in MDLMs.Normalised gradient attribution results for when the target question is placed on theleftend of the input context.",
                "position": 596
            },
            {
                "img": "https://arxiv.org/html/2511.21338/x20.png",
                "caption": "Figure 20:For LLaDA models, tasks that benefit more from additional ICL shots exhibit stronger performance degradation under extra masks.Dream shows no such trend, remaining more robust to extra masks.",
                "position": 683
            },
            {
                "img": "https://arxiv.org/html/2511.21338/x21.png",
                "caption": "Figure 21:Performance across varying numbers of mask tokens for different decoding steps (2, 4, and 6) using random unmasking strategy. The base model (None) shows significant performance degradation as the number of mask tokens increases, while our CE+TV fine-tuned model maintains more stable performance across all configurations.",
                "position": 709
            },
            {
                "img": "https://arxiv.org/html/2511.21338/x22.png",
                "caption": "Figure 22:Relative performance degradation (measured asmax accuracy−min accuracymax accuracy×100%\\frac{\\text{max accuracy}-\\text{min accuracy}}{\\text{max accuracy}}\\times 100\\%) across different numbers of decoding steps. Lower values indicate better robustness to increasing mask tokens. Our CE+TV fine-tuning reduces degradation by 38-49% compared to the base model, demonstrating significantly improved robustness with minimal accuracy trade-offs.",
                "position": 712
            },
            {
                "img": "https://arxiv.org/html/2511.21338/x23.png",
                "caption": "Figure 23:MA loss (CE+TV) reduces the degrading effect of extra masks in LLaDA-Instruct, and removes the locality of model, however, at the cost of an overall performance decrease.",
                "position": 762
            },
            {
                "img": "https://arxiv.org/html/2511.21338/x24.png",
                "caption": "Figure 24:MA loss (CE + TV) decreases the model’s entropy and increases the confidence in the generated token, while making both a smoother function of the number of extra masks, thus increasing the robustness of the model.",
                "position": 778
            },
            {
                "img": "https://arxiv.org/html/2511.21338/x25.png",
                "caption": "Figure 25:On the HotPotQA dataset, the MA loss also improves the robustness of the models to the varying number of masks.We observe improved performance particularly for the LLaDA-Base model.",
                "position": 818
            },
            {
                "img": "https://arxiv.org/html/2511.21338/x26.png",
                "caption": "Figure 26:In the multidimensional classification dataset, across all models, performance degrades when the relevant information is distant from the test question.We report the accuracy difference when placing relevant information on the left versus randomly (blue line), and on the right versus randomly (orange line). For DLMs, we additionally vary the position of the masked question–placing it at either the left or right end of the in-context examples (solid vs. dashed lines). Across all models performance consistently drops when the relevant information is far from the masked question (blue solid and orange dashed lines), with the effect being most pronounced in ARLMs. Notably, Dream exhibits a stronger recency bias when the masked question is positioned on the right than on the left, suggesting an underlying AR bias.Shaded regions indicate 95% confidence intervals computed across the 4 dataset types, and 5 seeds.",
                "position": 899
            },
            {
                "img": "https://arxiv.org/html/2511.21338/x27.png",
                "caption": "Figure 27:In the multidimensional-classification dataset, the MA loss prevents performance degradation with the extra masks (particularly for the Base model).We observe how the performance of the models changes under different ordering schemes of the in-context examples: random ordering, ordering by decreasing distance (most relevant information is located close to the test question) and ordering by increasing distance (most relevant information is located far from the test question).Shaded regions indicate 95% confidence intervals computed across the 4 dataset types, and 5 seeds.",
                "position": 905
            }
        ]
    },
    {
        "header": "Appendix BDetails of the Supervised Fine-Tuning Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21338/x28.png",
                "caption": "Figure 28:Quantisation has no significant effect on the performance under varying numbers of mask tokens.",
                "position": 1279
            },
            {
                "img": "https://arxiv.org/html/2511.21338/x29.png",
                "caption": "Figure 29:Quantisation has no significant effect on the the locality of the models.",
                "position": 1282
            }
        ]
    },
    {
        "header": "Appendix CExperimental Details",
        "images": []
    }
]