[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21989/x1.png",
                "caption": "Figure 1:Mind-the-Glitchis the first pipeline that enables computingvisual correspondencesbased on the backbone features of pre-trained diffusion models.\nThe pipeline separates backbone features into semantic and visual components, allowing forvisually matchingkeypoints across images, analogous to the well-establishedsemantic correspondencetask. This provides the first empirical framework for evaluating and localizing visual inconsistencies in subject-driven image generation.",
                "position": 72
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21989/x2.png",
                "caption": "Figure 2:Automated Dataset generation pipeline for producing controlled visual inconsistencies. Access to such pairs of images enables separating visual features from pre-trained backbones in a contrastive manner.",
                "position": 193
            },
            {
                "img": "https://arxiv.org/html/2509.21989/x3.png",
                "caption": "Figure 3:Examples illustrating how the skewness of the matching scores distribution correlates with matching ambiguity. High skewness implies a distinct match, while low skewness indicates diffuse or ambiguous correspondences.",
                "position": 250
            },
            {
                "img": "https://arxiv.org/html/2509.21989/x4.png",
                "caption": "Figure 4:Overview of the proposed architecture for disentangling semantic and visual features from a frozen diffusion backboneÎ¦\\Phi.\nInconsistent regions between the input image pairs are indicated by binary masks (left).\nThe semantic branch (blue) encourages the features of corresponding semantic points in both images,P1P_{1}andP2P_{2}, to align.\nIn the visual branch (yellow), we bring visually consistent pointsoutsidethe inpainted regions,P1outP_{1}^{\\text{out}}andP2outP_{2}^{\\text{out}}, closer together, while pushing apart features at inconsistent pointsinsidethe inpainted regions,P1inP_{1}^{\\text{in}}andP2inP_{2}^{\\text{in}}.",
                "position": 267
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21989/x5.png",
                "caption": "Figure 5:Qualitative examples of semantic and visual features, along with their correspondences, produced by our architecture.\nRegions that fall within the inpainting mask exhibit visually dissimilar features, enabling the detection of visual inconsistencies based on feature similarity.Dark Redis most consistent andYellowis least consistent.",
                "position": 445
            },
            {
                "img": "https://arxiv.org/html/2509.21989/x6.png",
                "caption": "Figure 6:Qualitative examples of evaluating subject-driven image generation approaches using our proposed VSM metric and other existing approaches. Our VSM metric can accurately quantify and localize inconsistency and is more consistent with the oracle.Dark Redis most consistent andYellowis least consistent.",
                "position": 540
            },
            {
                "img": "https://arxiv.org/html/2509.21989/x7.png",
                "caption": "Figure 7:Score Distribution of different metrics compared to the Oracle.",
                "position": 543
            },
            {
                "img": "https://arxiv.org/html/2509.21989/x7.png",
                "caption": "Figure 7:Score Distribution of different metrics compared to the Oracle.",
                "position": 545
            },
            {
                "img": "https://arxiv.org/html/2509.21989/x8.png",
                "caption": "Figure 8:Aggregation weights.",
                "position": 549
            }
        ]
    },
    {
        "header": "5Limitations and Future Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]