[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.18659/x1.png",
                "caption": "Figure 1:(a) During training, we firstpretrain the compressorto encourage it to retain only essential information. Next, weperform offline compressionof the documents. After that, we encode the query using thequery reasoner, retrieve the compressed document representations for generation, anduse only the final next-token prediction lossto jointly update both the query reasoner and the generator.\n(b) An example from the inference stage: thetokensrepresent key clue words related to the question. When we decode the continuous query embedding, we find that it contains information not present in the original query, indicating that it has learned some of the intermediate reasoning keywords.",
                "position": 237
            },
            {
                "img": "https://arxiv.org/html/2511.18659/x2.png",
                "caption": "Figure 2:Overview of theSCP(Salient Compressor Pretraining) framework. It includes (a) synthetic data construction for pretraining,\n(2) compressor training using the pretraining data.",
                "position": 268
            }
        ]
    },
    {
        "header": "2SCP: Salient Compressor Pretraining",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.18659/x3.png",
                "caption": "Figure 3:CLaRa end-to-end training: update query reasoner (θq​r\\theta_{qr}) and generator (θg\\theta_{g}) via language modeling loss using candidate document–question–answer triples.",
                "position": 404
            }
        ]
    },
    {
        "header": "3CLaRa: Retrieval and generation joint training",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.18659/images/main_recall1.png",
                "caption": "Figure 4:Retrieval performance (Recall@1/3/5) on theMistral-7Bmodel across different reranking methods under compression ratios = 4 and various initialization settings on NQ and HotpotQA datasets.Sup-denotes models trained with labeled data using contrastive learning for the reranker.-Pretraindenotes experiments conducted using the model checkpoint obtained after pretraining, while-Instructdenotes experiments conducted using the model checkpoint obtained after instruction tuning.",
                "position": 1393
            }
        ]
    },
    {
        "header": "5Ablation Study",
        "images": []
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AGradients for Non-shared vs. Shared Representations in RAG",
        "images": []
    },
    {
        "header": "Appendix BDetailed experimental setup",
        "images": []
    },
    {
        "header": "Appendix CPretraining Data Quality",
        "images": []
    },
    {
        "header": "Appendix DPretraining Data Scaling",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.18659/images/compression_stage1_curve.pdf.png",
                "caption": "Figure 5:Validation loss curves during the compression pretraining stage under different compression ratios (CR) on thePhi-4-mini(left) andMistral-7B(right) models.",
                "position": 5744
            },
            {
                "img": "https://arxiv.org/html/2511.18659/images/end_to_end_stage2_curve.png",
                "caption": "Figure 6:Validation trends of recall and evaluation loss during the end-to-end training stage under different compression ratios (CR) on theNQ(top) andMusique(bottom) datasets.",
                "position": 5747
            }
        ]
    },
    {
        "header": "Appendix ETraining Curves",
        "images": []
    },
    {
        "header": "Appendix FMore Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.18659/images/different_top_k.png",
                "caption": "Figure 7:Performance of varying the number of retrieved documents (kk) during testing on different QA datasets.",
                "position": 6191
            }
        ]
    },
    {
        "header": "Appendix GFidelity and Grounding Analysis",
        "images": []
    },
    {
        "header": "Appendix HParaphrase Case Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.18659/images/mistral_32_no_mse.png",
                "caption": "(a)Without MSE",
                "position": 7371
            },
            {
                "img": "https://arxiv.org/html/2511.18659/images/mistral_32_no_mse.png",
                "caption": "(a)Without MSE",
                "position": 7374
            },
            {
                "img": "https://arxiv.org/html/2511.18659/images/mistral_32_with_mse.png",
                "caption": "(b)With MSE",
                "position": 7379
            }
        ]
    },
    {
        "header": "Appendix IPrompts",
        "images": []
    }
]