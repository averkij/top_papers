[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23693/x1.png",
                "caption": "",
                "position": 158
            },
            {
                "img": "https://arxiv.org/html/2505.23693/x2.png",
                "caption": "",
                "position": 163
            },
            {
                "img": "https://arxiv.org/html/2505.23693/x3.png",
                "caption": "Figure 1:Overview of our research: (a) Collection of AIGC videos: We compile a diverse set of video generation prompts to instruct both proprietary and open-source T2V models for generating AIGC videos. (b) Illustration of errors occurring within the same AIGC video. (c) Analytics of the dataset:VF-Evalcovering a diverse range of reasoning tasks. And it contains AIGC videos with durations between 4 to 12 seconds, reflecting the typical output length of current T2V models.",
                "position": 187
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related works",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23693/x4.png",
                "caption": "Figure 2:Illustration of four proposed tasks and the corresponding question types in theVF-Evalbenchmark. Detailed examples for each reasoning task are provided in AppendixC.1.",
                "position": 266
            }
        ]
    },
    {
        "header": "3VF-EvalBenchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23693/x5.png",
                "caption": "Table 3:Model performance (i.e.,accuracy %) onVF-Eval.CPdenotes “Commonsense and physics”.",
                "position": 670
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23693/x5.png",
                "caption": "Figure 3:Performance Comparison of InternVL3-38B.",
                "position": 909
            }
        ]
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23693/x6.png",
                "caption": "Figure 4:Performance comparison within four models on six reasoning sub-tasks.",
                "position": 961
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AVF-EvalAnnotation",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23693/x7.png",
                "caption": "Figure 5:UI of annotation.",
                "position": 1947
            }
        ]
    },
    {
        "header": "Appendix BExperiment Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23693/x8.png",
                "caption": "Table 6:Details of the multimodal models inVF-Eval. The “Source” column lists URLs for proprietary models and Hugging Face names for open-source models. The “# Input Frames” column shows the default input frames, chosen from 2, 4, 8, 16, based on the context window. “HF” refers to Hugging Face.",
                "position": 1962
            },
            {
                "img": "https://arxiv.org/html/2505.23693/x8.png",
                "caption": "Figure 13:Detailed examples of reasoning tasks.",
                "position": 2301
            },
            {
                "img": "https://arxiv.org/html/2505.23693/x9.png",
                "caption": "Figure 14:Misconception of Video Creation",
                "position": 2317
            },
            {
                "img": "https://arxiv.org/html/2505.23693/x10.png",
                "caption": "Figure 15:Excessive Dependence on Textual Cues",
                "position": 2320
            },
            {
                "img": "https://arxiv.org/html/2505.23693/x11.png",
                "caption": "Figure 16:Neglect of Critical Details",
                "position": 2323
            },
            {
                "img": "https://arxiv.org/html/2505.23693/x12.png",
                "caption": "Figure 17:Over-reliance on Commonsense Knowledge",
                "position": 2326
            }
        ]
    },
    {
        "header": "Appendix CCase Study and Error Analysis",
        "images": []
    }
]