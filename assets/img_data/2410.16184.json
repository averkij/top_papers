[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3RM-BenchConstruction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.16184/x1.png",
                "caption": "Figure 2:Style-Substance Eval Matrix ofsfairXC/FsfairX-LLaMA3-RM-v0.1in Chat Domain",
                "position": 640
            }
        ]
    },
    {
        "header": "4Evaluation Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.16184/x2.png",
                "caption": "Table 3:Top-20 reward models onRM-Bench.\nChat, Math, Code, Safety show the model’s Average Accuracy on each domain.\nEasy, Normal, Hard show the model’s Accuracy on each difficulty level across all domains.\nAvg shows the model’s overall Average Accuracy inRM-Bench.\nIcons refer to model types: Sequence Classifier (), Direct Preference Optimization (), Custom Classifier ().\nAs a baseline, the accuracy of random guessing is50%percent5050\\%50 %.",
                "position": 704
            },
            {
                "img": "https://arxiv.org/html/2410.16184/x3.png",
                "caption": "",
                "position": 710
            },
            {
                "img": "https://arxiv.org/html/2410.16184/x4.png",
                "caption": "",
                "position": 710
            },
            {
                "img": "https://arxiv.org/html/2410.16184/x5.png",
                "caption": "Figure 3:Scatter plot of correctness and verbosity scores of responses inRM-Bench.",
                "position": 1058
            }
        ]
    },
    {
        "header": "5Correlation with Policy Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.16184/x6.png",
                "caption": "Figure 4:Line-chart of the policy model style-bias score and the reward model hard accuracy onRM-Benchchat.",
                "position": 1094
            },
            {
                "img": "https://arxiv.org/html/2410.16184/x7.png",
                "caption": "Figure 5:Correlation between reward model perf. onRM-Benchand policy model perf. on downstream tasks.",
                "position": 1115
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ALimitations ofRM-Bench",
        "images": []
    },
    {
        "header": "Appendix BBorder Impacts",
        "images": []
    },
    {
        "header": "Appendix CPotential bias Introduced by GPT-4o",
        "images": []
    },
    {
        "header": "Appendix DThe Scalability of Our Data Construction Method",
        "images": []
    },
    {
        "header": "Appendix ECorrelation with Length Controlled Alpaca Eval",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.16184/x8.png",
                "caption": "Figure 6:Correlation between the reward model performance onRM-Benchand the policy model performance on Alpaca Eval.",
                "position": 2010
            },
            {
                "img": "https://arxiv.org/html/2410.16184/x8.png",
                "caption": "Figure 6:Correlation between the reward model performance onRM-Benchand the policy model performance on Alpaca Eval.",
                "position": 2013
            },
            {
                "img": "https://arxiv.org/html/2410.16184/x9.png",
                "caption": "Figure 7:Correlation between the reward model performance on Reward Bench and the policy model performance on downstream tasks.",
                "position": 2018
            }
        ]
    },
    {
        "header": "Appendix FCorrelation of Reward Bench",
        "images": []
    },
    {
        "header": "Appendix GSystem Prompt for Wary LLM",
        "images": []
    },
    {
        "header": "Appendix HBest-of-N Correlation",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.16184/x10.png",
                "caption": "Figure 8:Correlation between reward model performance onRM-Benchand policy model performance with Best-of-N strategy, including code (left) and math (right).",
                "position": 2067
            },
            {
                "img": "https://arxiv.org/html/2410.16184/x11.png",
                "caption": "",
                "position": 2076
            }
        ]
    },
    {
        "header": "Appendix IMany Shot Jailbreak Prompt",
        "images": []
    },
    {
        "header": "Appendix JExample Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.16184/x2.png",
                "caption": "Table 12:The full results of tested reward models onRM-Bench.\nChat, Math, Code, Safety show the model’s Average Accuracy on each domain.\nEasy, Normal, Hard show the model’s Accuracy on each difficulty level across all domains.\nAvg shows the model’s overall Average Accuracy inRM-Bench.\nIcons refer to model types: Sequence Classifier (), Direct Preference Optimization (), Custom Classifier ().\nAs a baseline, the accuracy of random guessing is50%percent5050\\%50 %.",
                "position": 2926
            },
            {
                "img": "https://arxiv.org/html/2410.16184/x2.png",
                "caption": "Table 13:Detailed Chat Domain Results inRM-Bench.\nIcons refer to model types: Sequence Classifier (), Direct Preference Optimization (), Custom Classifier ().",
                "position": 3389
            },
            {
                "img": "https://arxiv.org/html/2410.16184/x2.png",
                "caption": "Table 14:Math Domain Results inRM-Bench.\nIcons refer to model types: Sequence Classifier (), Direct Preference Optimization (), Custom Classifier ().",
                "position": 3678
            },
            {
                "img": "https://arxiv.org/html/2410.16184/x2.png",
                "caption": "Table 15:Detailed Code Domain Results inRM-Bench.\nIcons refer to model types: Sequence Classifier (), Direct Preference Optimization (), Custom Classifier ().",
                "position": 3967
            },
            {
                "img": "https://arxiv.org/html/2410.16184/x2.png",
                "caption": "Table 16:Satety-Should-Respond Domain Results inRM-Bench.\nIcons refer to model types: Sequence Classifier (), Direct Preference Optimization (), Custom Classifier ().",
                "position": 4256
            },
            {
                "img": "https://arxiv.org/html/2410.16184/x2.png",
                "caption": "Table 17:Safety-Should-Refuse Domain Results inRM-Bench.\nIcons refer to model types: Sequence Classifier (), Direct Preference Optimization (), Custom Classifier ().",
                "position": 4545
            }
        ]
    },
    {
        "header": "Appendix KDetailed Eval Results",
        "images": []
    }
]