[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03178/x1.png",
                "caption": "Figure 1:Main results of models on the instruction following benchmarks.",
                "position": 79
            },
            {
                "img": "https://arxiv.org/html/2508.03178/x2.png",
                "caption": "Figure 2:An example of different thinking patterns between Qwen3-32B and Light-IF-32B. Reasoning refers to the content between <think> and </think>. The original text is in Chinese and translated here for readability.",
                "position": 82
            },
            {
                "img": "https://arxiv.org/html/2508.03178/x3.png",
                "caption": "Figure 3:The overall framework of the proposed method.",
                "position": 126
            }
        ]
    },
    {
        "header": "Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03178/x4.png",
                "caption": "Figure 4:Pipeline of hardness-aware prompt synthesis.",
                "position": 145
            },
            {
                "img": "https://arxiv.org/html/2508.03178/x5.png",
                "caption": "Figure 5:Response length andRcR_{c}during Zero-RL training.",
                "position": 167
            },
            {
                "img": "https://arxiv.org/html/2508.03178/x6.png",
                "caption": "Figure 6:Left: Top-100 tokens with high selection rank at the beginning of the cold-start stage. Right: Top-100 tokens with low selection rank at the end of the cold-start stage. Larger size indicates larger frequency and brighter color indicates higher rank.",
                "position": 187
            }
        ]
    },
    {
        "header": "Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03178/x7.png",
                "caption": "Figure 7:Entropy dynamics. Each point represents a model, and branching paths indicates the adoption of different training strategies in each stage.",
                "position": 994
            },
            {
                "img": "https://arxiv.org/html/2508.03178/x8.png",
                "caption": "Figure 8:High-entropy token dynamics throughout training. Tokens with Top-200 average entropy and frequency more than 100 are viewed high-entropy.",
                "position": 1000
            }
        ]
    },
    {
        "header": "Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AA. Light Training Cost",
        "images": []
    },
    {
        "header": "Appendix BB. Implementation Details",
        "images": []
    },
    {
        "header": "Appendix CC. Cold-Start Data Filter",
        "images": []
    },
    {
        "header": "Appendix DD. Entropy Control during RL",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03178/x9.png",
                "caption": "Figure 9:Entropy control during RL.",
                "position": 1837
            },
            {
                "img": "https://arxiv.org/html/2508.03178/x10.png",
                "caption": "Figure 10:Appearances of the dense and sparse reward.",
                "position": 1850
            }
        ]
    },
    {
        "header": "Appendix EE. Dense Reward versus Sparse Reward",
        "images": []
    }
]