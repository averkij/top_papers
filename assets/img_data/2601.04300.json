[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04300/x1.png",
                "caption": "Figure 1:Existing methods rely on coarse-grained, scalar or binary image-level reward signals. In contrast, our method leverages human expert knowledge for fine-grained attribute decoupling, guiding the model directly from the noise space to approach positive and avoid negative directions.",
                "position": 134
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Domain-specific Fine-grained Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04300/x2.png",
                "caption": "Figure 2:The pipeline of our framework.\nThe Domain-Expert Agent decomposes image along 7 dimensions, which are represented as:Brushstroke and Texture,Light and Shadow,Shape and Posture,Composition,Perspective and Space,Color relationship,\nandEdge relationship.\nNotice that the visualization of the attribute hierarchy in the agent is simplified.\nThe full hierarchy is of 5 levels with 246 attribute pairs in the leaf nodes. Post-annotation, we first conduct SFT to obtain the modelθ1\\theta_{1}. This model is then used to dynamically acquire noise signals that aggregate decoupled attribute information. Subsequently, the aligned model is trained to learn the positive direction while suppressing the negative direction.",
                "position": 239
            },
            {
                "img": "https://arxiv.org/html/2601.04300/sec/figures/icon_brushstroke.png",
                "caption": "",
                "position": 243
            },
            {
                "img": "https://arxiv.org/html/2601.04300/sec/figures/icon_light.png",
                "caption": "",
                "position": 244
            },
            {
                "img": "https://arxiv.org/html/2601.04300/sec/figures/icon_shape.png",
                "caption": "",
                "position": 245
            },
            {
                "img": "https://arxiv.org/html/2601.04300/sec/figures/icon_composition.png",
                "caption": "",
                "position": 246
            },
            {
                "img": "https://arxiv.org/html/2601.04300/sec/figures/icon_perspective.png",
                "caption": "",
                "position": 247
            },
            {
                "img": "https://arxiv.org/html/2601.04300/sec/figures/icon_color.png",
                "caption": "",
                "position": 248
            },
            {
                "img": "https://arxiv.org/html/2601.04300/sec/figures/icon_edge.png",
                "caption": "",
                "position": 249
            }
        ]
    },
    {
        "header": "4Preliminary",
        "images": []
    },
    {
        "header": "5Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04300/x3.png",
                "caption": "Figure 3:Illustration of the CPO sampling trajectory. At each timesteptt, CPO employs the expert modelθ1\\theta_{1}to provide deterministic positive and negative noise guidance, directing the trajectory toward virtual winning and losing samples, respectively. Owing to the determinism of the noise trajectory, the final samplex0x_{0}can be precisely reconstructed back toxtx_{t}. Compared with original DPO, this design enables process-level guidance for model training rather than relying solely on the final endpoints, thereby making the training process more efficient.",
                "position": 442
            },
            {
                "img": "https://arxiv.org/html/2601.04300/x4.png",
                "caption": "Figure 4:Visual comparison of different baselines and our CPO. #A_neg (↓\\downarrow) and PickScore (↑\\uparrow) are annotated in the lower-left and lower-right corners of each image, respectively. CPO outperforms all baselines in both negative-attribute avoidance and preference scoring.",
                "position": 662
            }
        ]
    },
    {
        "header": "6Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04300/x5.png",
                "caption": "Figure 5:Curves of the win and lose parts of the loss function over training steps. The configuration with stabilization demonstrates significantly greater stability compared to the one without.",
                "position": 827
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Supplementary Material",
        "images": []
    },
    {
        "header": "S1Description of the Fine-grained Hierarchical Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04300/x6.png",
                "caption": "Figure S6:Illustration of the domain-specific fine-grained evaluation framework. Best viewed magnified on screen.",
                "position": 1544
            }
        ]
    },
    {
        "header": "S2Description of Complex Preference Learning Tasks",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04300/x7.png",
                "caption": "Figure S7:Description of tasks targeted by CPO. Image (a) and (b) are generated from the same prompt, yet each exhibits its own strengths and weaknesses; thus, it is inappropriate to generalize that either image is universally superior. Image (c), generated from a different prompt, should be evaluated using criteria distinct from those applied to (a) and (b).",
                "position": 1557
            }
        ]
    },
    {
        "header": "S3User Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04300/sec/figures/eg.1-sdxl-cponpo.jpg",
                "caption": "Please use the following 7 dimensions as criteria to conduct pairwise comparisons for the image pairs in Group G1 and Group G2, respectively. For each dimension, select the image that performs better: Brushwork and Texture Generation, Edge Relationship Generation, Composition Generation, Light and Shadow Generation, Color Relationship Generation, Perspective and Space Generation, and Shape and Form Generation.",
                "position": 1567
            },
            {
                "img": "https://arxiv.org/html/2601.04300/sec/figures/eg.1-sdxl-dponpo.jpg",
                "caption": "",
                "position": 1575
            },
            {
                "img": "https://arxiv.org/html/2601.04300/sec/figures/eg.1-flux-dpo.jpg",
                "caption": "",
                "position": 1576
            },
            {
                "img": "https://arxiv.org/html/2601.04300/sec/figures/eg.1-flux-cpo.jpg",
                "caption": "",
                "position": 1577
            },
            {
                "img": "https://arxiv.org/html/2601.04300/sec/figures/eg.2-sdxl-npocpo.jpg",
                "caption": "",
                "position": 1583
            },
            {
                "img": "https://arxiv.org/html/2601.04300/sec/figures/eg.2-sdxl-dponpo.jpg",
                "caption": "",
                "position": 1585
            },
            {
                "img": "https://arxiv.org/html/2601.04300/sec/figures/eg.2-flux-dpo.jpg",
                "caption": "",
                "position": 1586
            },
            {
                "img": "https://arxiv.org/html/2601.04300/sec/figures/eg.2-flux-cpo.jpg",
                "caption": "",
                "position": 1587
            }
        ]
    },
    {
        "header": "S4More Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04300/x8.png",
                "caption": "Figure S9:Visual comparison of different baselines and our CPO. #A_neg (↓\\downarrow) and PickScore (↑\\uparrow) are annotated in the lower-left and lower-right corners of each image, respectively. CPO outperforms all baselines in both negative-attribute avoidance and preference scoring.",
                "position": 1667
            },
            {
                "img": "https://arxiv.org/html/2601.04300/x9.png",
                "caption": "Figure S10:Visual comparison of different baselines and our CPO. #A_neg (↓\\downarrow) and PickScore (↑\\uparrow) are annotated in the lower-left and lower-right corners of each image, respectively. CPO outperforms all baselines in both negative-attribute avoidance and preference scoring.",
                "position": 1670
            }
        ]
    },
    {
        "header": "S5Additional Explanation on Stabilization Strategy",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04300/x10.png",
                "caption": "(a)Visualization of the winning and losing parts of the loss function.",
                "position": 1680
            },
            {
                "img": "https://arxiv.org/html/2601.04300/x10.png",
                "caption": "(a)Visualization of the winning and losing parts of the loss function.",
                "position": 1683
            },
            {
                "img": "https://arxiv.org/html/2601.04300/x11.png",
                "caption": "(b)Visualization of the overall trend of the loss function.",
                "position": 1689
            },
            {
                "img": "https://arxiv.org/html/2601.04300/x12.png",
                "caption": "Figure S12:Illustration of the function transformation in the stabilization strategy. It transforms the originally concave losing term into an equivalent convex formulation. The transformed term preserves the direction of the original losing term, but its optimization magnitude is matched to that of the winning term, ensuring stability during training.",
                "position": 1761
            }
        ]
    },
    {
        "header": "S6Ablation Study of the Dynamic Process Reward Parameterω\\omega",
        "images": []
    },
    {
        "header": "S7Additional Details on CPO",
        "images": []
    },
    {
        "header": "S8Hallucination in Agent Behaviors",
        "images": []
    },
    {
        "header": "S9Reliability of the SFT Model",
        "images": []
    },
    {
        "header": "S10Negative Noise Construction",
        "images": []
    },
    {
        "header": "S11Differences from and Advantages over Inversion-Based DPO",
        "images": []
    },
    {
        "header": "S12Discussion",
        "images": []
    },
    {
        "header": "S13Failure Cases and Limitation",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04300/x13.png",
                "caption": "Figure S13:Additional results of failure examples.",
                "position": 2150
            }
        ]
    },
    {
        "header": "S14Social Impact",
        "images": []
    }
]