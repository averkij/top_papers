[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05282/x1.png",
                "caption": "Figure 1:Pose Estimation via Rectified Point Flow. Our formulation supportsshape assemblyandpairwise registrationtasks in a single framework. Given a set of unposed part point clouds{𝑿¯i}i∈Ωsubscriptsubscript¯𝑿𝑖𝑖Ω\\{\\bar{\\bm{X}}_{i}\\}_{i\\in\\Omega}{ over¯ start_ARG bold_italic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i ∈ roman_Ω end_POSTSUBSCRIPT, Rectified Point Flow predicts each part’s point cloud at the target assembled state{𝑿^i⁢(0)}i∈Ωsubscriptsubscript^𝑿𝑖0𝑖Ω\\{\\hat{\\bm{X}}_{i}{(0)}\\}_{i\\in\\Omega}{ over^ start_ARG bold_italic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( 0 ) } start_POSTSUBSCRIPT italic_i ∈ roman_Ω end_POSTSUBSCRIPT. Subsequently, we solve Procrustes problem via SVD between the condition point cloud𝑿¯isubscript¯𝑿𝑖\\bar{\\bm{X}}_{i}over¯ start_ARG bold_italic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTand the estimated point cloud𝑿^i⁢(0)subscript^𝑿𝑖0\\hat{\\bm{X}}_{i}(0)over^ start_ARG bold_italic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( 0 )to recover the rigid transformationT^isubscript^𝑇𝑖\\hat{T}_{i}over^ start_ARG italic_T end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTfor each non-anchored part.",
                "position": 211
            }
        ]
    },
    {
        "header": "3Pose Estimation via Rectified Point Flow",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05282/x2.png",
                "caption": "Figure 2:Encoder pre-training via overlap points prediction.Given unposed multi-part point clouds, our encoder with a point-wise overlap prediction head performs a binary classification to identify overlapping points. Predicted overlap points are shown in blue. For comparison, the ground-truth overlap points are visualized on the assembled object for clarity (target overlap).",
                "position": 243
            },
            {
                "img": "https://arxiv.org/html/2506.05282/x3.png",
                "caption": "Figure 3:Learning Rectified Point Flow.The input to Rectified Point Flow are the condition point clouds{𝑿~i}i∈Ωsubscriptsubscript~𝑿𝑖𝑖Ω\\{\\tilde{\\bm{X}}_{i}\\}_{i\\in\\Omega}{ over~ start_ARG bold_italic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i ∈ roman_Ω end_POSTSUBSCRIPTand noised point clouds{𝑿i⁢(t)}i∈Ωsubscriptsubscript𝑿𝑖𝑡𝑖Ω\\{\\bm{X}_{i}(t)\\}_{i\\in\\Omega}{ bold_italic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t ) } start_POSTSUBSCRIPT italic_i ∈ roman_Ω end_POSTSUBSCRIPTat timestept𝑡titalic_t. They are first encoded by the pre-trained encoder and the positional encoding, respectively. The encoded features are concatenated and passed through the flow model, which predicts per-point velocity vectors{d⁢𝑿i⁢(t)/d⁢t}i∈Ωsubscriptdsubscript𝑿𝑖𝑡d𝑡𝑖Ω\\{\\mathrm{d}\\bm{X}_{i}(t)/\\mathrm{d}t\\}_{i\\in\\Omega}{ roman_d bold_italic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t ) / roman_d italic_t } start_POSTSUBSCRIPT italic_i ∈ roman_Ω end_POSTSUBSCRIPTand defines the flow used to predict the part point cloud in its assembled state.",
                "position": 289
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05282/x4.png",
                "caption": "Figure 4:Qualitative Results on PartNet-Assembly.Columns show objects with increasing number of parts (left to right). Rows display (1) colored input point clouds of each part, (2) GARF outputs (dashed boxes indicate samples limited to 20 by GARF’s design, selecting the top 20 parts by volume), (3) Rectified Point Flow outputs, and (4) ground-truth assemblies. Compared to GARF, our method produces more accurate pose estimation on most parts, especially as the number of parts increases.",
                "position": 468
            },
            {
                "img": "https://arxiv.org/html/2506.05282/x5.png",
                "caption": "Figure 5:Qualitative Results Across Registration and Assembly Tasks.From left to right: pairwise registration on ModelNet 40 and TUD-L, shape assembly on BreakingBad-Everyday. From top to bottom: Colored input point clouds, GARF results, ours, and ground truth (GT). Our single model performs the best across registration and assembly tasks.",
                "position": 647
            },
            {
                "img": "https://arxiv.org/html/2506.05282/x6.png",
                "caption": "Figure 6:Learning from a symmetric assembly.Left to right: (1) a single training sample from IKEA-Manual[17], and (2–4) three independent samples generated, conditioned on the same inputs. Parts are color-coded consistently across plots. (Best viewed in color.)",
                "position": 771
            },
            {
                "img": "https://arxiv.org/html/2506.05282/x6.png",
                "caption": "Figure 6:Learning from a symmetric assembly.Left to right: (1) a single training sample from IKEA-Manual[17], and (2–4) three independent samples generated, conditioned on the same inputs. Parts are color-coded consistently across plots. (Best viewed in color.)",
                "position": 774
            },
            {
                "img": "https://arxiv.org/html/2506.05282/x7.png",
                "caption": "Figure 7:Two common failure types.First column: Assemblies that are geometrically plausible but mechanically nonfunctional. Second column: Objects with high geometric complexity.",
                "position": 780
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Appendix AModel Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05282/x8.png",
                "caption": "Figure 8:Details of the DiT Block.Our flow model consists of an Encoder and a position embedding (Pos. Emb.), and sequential DiT blocks (N=6𝑁6N=6italic_N = 6). Each block comprises Part-wise Attention, Global Attention, MLP, and AdaLayerNorm layers.",
                "position": 928
            }
        ]
    },
    {
        "header": "Appendix BAdditional Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05282/x9.png",
                "caption": "Figure 9:Sampling in Noise Space.For each fixed condition input point clouds, we sample four independent Gaussian noise vectors to generate distinct assembly outputs (shown in columns 2–5). While all samples preserve the object’s structure, they show meaningful variation in part placement, orientation, and overall geometry, particularly for symmetric parts (e.g., armrests and chair bases). For comparison, the first column shows the ground-truth assemblies.",
                "position": 1381
            }
        ]
    },
    {
        "header": "Appendix CRandomness in Assembly Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05282/x10.png",
                "caption": "Figure 10:Linear Interpolation in Noise Space.For different objects in each row, we fix the same conditional input and decode two independently sampled Gaussian noise vectors,𝒁0subscript𝒁0\\bm{Z}_{0}bold_italic_Z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT(leftmost) and𝒁1subscript𝒁1\\bm{Z}_{1}bold_italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT(rightmost), into plausible part configurations. The three center columns show outputs from the linearly interpolated noises between𝒁0subscript𝒁0\\bm{Z}_{0}bold_italic_Z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTand𝒁1subscript𝒁1\\bm{Z}_{1}bold_italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. We observe a continuous, semantically meaningful mapping from Gaussian noise to valid assemblies.",
                "position": 1397
            }
        ]
    },
    {
        "header": "Appendix DGeneralization Ability",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05282/x11.png",
                "caption": "Figure 11:Generalization to Unseen Assemblies Within the Same Category: We select parts from two objects of the same category in the PartNet-Assembly test set. Parts from Object 1 are shown in blue, and parts from Object 2 in red; unselected parts are shown in gray. The results demonstrate that the model comprehends the underlying geometric structure of the category and can re-target parts to construct the final shape of the same category.",
                "position": 1425
            },
            {
                "img": "https://arxiv.org/html/2506.05282/x12.png",
                "caption": "Figure 12:Generalization to Unseen Assemblies Across Categories: We select parts from two objects of different categories in the PartNet-Assembly test set. Parts from Object 1 are shown in blue, and parts from Object 2 in red; unselected parts are shown in gray. The results demonstrate that the model can reason about part compositionality and re-target parts to construct a plausible final shape even if some of them originate in completely different objects.",
                "position": 1444
            }
        ]
    },
    {
        "header": "Appendix EProof of Theorem 1",
        "images": []
    },
    {
        "header": "Appendix FGeneralization Bounds",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]