[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.11412/extracted/6280840/figs/teaser.jpg",
                "caption": "Figure 1.MTV-Inpaint is a unified video inpainting framework that supports multiple tasks, such as text/image-guided object insertion, scene completion, and derived applications like object editing and removal. It is also capable of handling long videos with hundreds of frames.",
                "position": 166
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": []
    },
    {
        "header": "2.Related Work",
        "images": []
    },
    {
        "header": "3.Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.11412/x1.png",
                "caption": "Figure 2.Our VideoPaint framework. During training, we train object insertion and scene completion tasks with dual-branch U-Net, using object-aware masks and random masks respectively. Concurrently, we employ three frame masking modes: text-to-video(T2V), image-to-video (I2V), and keyframe-to-video (K2V). During the inference, our method can perform basic T2V inpainting, or I2V inpainting, given that the first frame is obtained from 3rd party image inpainting tool. To handle longer video, we first use T2V/I2V mode to inpaint keyframes, then use K2V mode to inpaint remaining in-between frames.",
                "position": 268
            }
        ]
    },
    {
        "header": "4.Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.11412/extracted/6280840/figs/obj_ins.jpg",
                "caption": "Figure 3.Quantitative comparison for object insertion evaluation. We recommend watching our supplementary video for dynamic results. Methods marked with an asterisk are not existing works but have been implemented by us.",
                "position": 516
            },
            {
                "img": "https://arxiv.org/html/2503.11412/extracted/6280840/figs/scn_comp.jpg",
                "caption": "Figure 4.Quantitative comparison for scene completion evaluation. We recommend watching our supplementary video for dynamic results.",
                "position": 533
            },
            {
                "img": "https://arxiv.org/html/2503.11412/extracted/6280840/figs/user_study.jpg",
                "caption": "Figure 5.User study results of different methods on(a)object insertion task, and(b)scene completion task.",
                "position": 659
            },
            {
                "img": "https://arxiv.org/html/2503.11412/extracted/6280840/figs/long_example.jpg",
                "caption": "Figure 6.Visual examples from different long video generation strategies.",
                "position": 662
            },
            {
                "img": "https://arxiv.org/html/2503.11412/extracted/6280840/figs/dual.jpg",
                "caption": "Figure 7.Comparison of dual-branch and single-branch architectures for scene completion tasks. The dual-branch approach effectively separates scene completion from object insertion task. The single-branch model sometimes fails to distinguish between the two tasks, leading to undesired object insertion during scene completion (circled in red).",
                "position": 734
            },
            {
                "img": "https://arxiv.org/html/2503.11412/x2.png",
                "caption": "Figure 8.Illustration of different long video generation strategies.",
                "position": 839
            },
            {
                "img": "https://arxiv.org/html/2503.11412/x3.png",
                "caption": "Figure 9.Effect of different noising timestepœÑùúè\\tauitalic_œÑon frame consistency, dashed lines represent mean values.",
                "position": 886
            }
        ]
    },
    {
        "header": "5.Other Applications",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.11412/extracted/6280840/figs/mm.jpg",
                "caption": "Figure 10.By integrating our method with existing image inpainting model, our framework allows multi-modal guided video inpainting",
                "position": 905
            },
            {
                "img": "https://arxiv.org/html/2503.11412/extracted/6280840/figs/object_edit.jpg",
                "caption": "Figure 11.Object editing results. We recommend watching our supplementary video for dynamic results.",
                "position": 916
            },
            {
                "img": "https://arxiv.org/html/2503.11412/extracted/6280840/figs/object_removal.jpg",
                "caption": "Figure 12.Object removal results. We recommend watching our supplementary video for dynamic results.",
                "position": 927
            },
            {
                "img": "https://arxiv.org/html/2503.11412/extracted/6280840/figs/object_brush.jpg",
                "caption": "Figure 13.Image object brush: users can draw box trajectories to iteratively add objects into static image to make a dynamic video. We recommend watching our supplementary video for dynamic results.",
                "position": 938
            }
        ]
    },
    {
        "header": "6.Limitation and Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.11412/extracted/6280840/figs/limit.jpg",
                "caption": "Figure 14.Examples of failure cases.(a)Text-guided object insertion with a moving mask for a static object (vase) leads to unrealistic motion following the mask trajectory.(b)Image-guided object insertion using third-party image inpainting models may occasionally generate a first frame with incorrect object orientation, leading to unnatural motion, such as the lateral sliding of the car shown in the 2nd row.(c)Our object removal results will leave unrealistic residual shadows (highlighted in the red circles) when the mask fails to capture object‚Äôs shadow regions.(d)Limited synthesis capabilities for complex motions (skateboarding tricks) due to base model constraints.",
                "position": 946
            }
        ]
    },
    {
        "header": "7.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]