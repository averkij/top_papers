[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.01496/extracted/6248080/figs/liger_emoji.png",
                "caption": "",
                "position": 53
            },
            {
                "img": "https://arxiv.org/html/2503.01496/x1.png",
                "caption": "Figure 1:Liger Performance and Efficiency.Our proposed Liger recovers nearly 93% performance of Llama-3.2-1B and outperforms pretrained gated recurrent models at only 0.02% of the pre-training tokens cost.",
                "position": 80
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.01496/x2.png",
                "caption": "Figure 2:Overall Framework of Liger.We linearize the Transformer-based large language model (LLM) architecture into a gated linear recurrent model by 1) ReplacingSoftmax Attentionwith aGated Recurrent Memorymodule, and 2) EmployingLoRAto fine-tune the Liger architecture while frozen most original weight parameters. The Liger architecture enables efficient chunk-wise parallel training also enjoying cheap linear recurrent inference.",
                "position": 217
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.01496/x3.png",
                "caption": "Figure 3:Hybrid Architecture of Liger.Liger adopts intra-hybrid Liger Attention and inter-hybrid model architecture by stacking a layer of standard attention Transformer blocks every a few (e.g. 7) layers of Liger Transformer blocks.",
                "position": 447
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.01496/x4.png",
                "caption": "Figure 4:Decoding Latency Time and GPU Memory Usage of Each 8B Models.We variate the decoding length from 1K to 32K with fixed batch size of 16 on single A800 80GB GPU to evaluate the modelsâ€™ efficiency. Liger enjoys linear-time inference with constant GPU memory usage.",
                "position": 1043
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADatasets and Benchmarks",
        "images": []
    },
    {
        "header": "Appendix BExperiment Details",
        "images": []
    },
    {
        "header": "Appendix CFull Results",
        "images": []
    }
]