[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01576/all-twemojis.pdf",
                "caption": "",
                "position": 161
            },
            {
                "img": "https://arxiv.org/html/2602.01576/figs/Trillion.png",
                "caption": "",
                "position": 173
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01576/x1.png",
                "caption": "Figure 1:Average Instruction Accuracy (IAcc.) across all six benchmarks.gWorld 8Band32Bachieve a new pareto frontier in terms of model size (log10scaled). The existing pareto frontier was defined byQwen3 VL 8B,32B, andGLM 4.6V 106B. Notably, extremely large models (e.g.,Llama 4 402B) do not reach this pareto frontier, while text-image-to-image models (e.g.,Emu3.5 34B) struggle with mobile GUI dynamics.",
                "position": 178
            },
            {
                "img": "https://arxiv.org/html/2602.01576/x2.png",
                "caption": "Figure 2:Mobile GUI world modeling via renderable code.Given an image stateStS_{t}and actionAtA_{t}, the model predicts the next stateSt+1S_{t+1}. Our model,gWorld, generates renderable web code to ensure pixel-perfect text and structurally accurate layouts. In contrast, image-gen baselines (e.g.,Qwen-Image-Edit 20B) struggle with the discrete nature of GUIs, frequently producing illegible text and distorted layouts. See Appendix Fig.13,14,15for additional qualitative examples.",
                "position": 184
            },
            {
                "img": "https://arxiv.org/html/2602.01576/x3.png",
                "caption": "Figure 3:Schematic diagram of our data generation pipeline.We construct VLM world modeling data via three steps:(1)Repurposing offline policy trajectories into transition triplets;(2)Cross-modal relabeling of the ground-truth next state from pixels (St+1imageS^{\\text{image}}_{t+1}) to renderable web code (St+1c​o​d​eS_{t+1}^{code}); and(3)Synthesizing reasoning traces (RtR_{t}) using look-ahead access to the target state.\nThe final training objective is to predict both the reasoning trace and the code-based next state:(St,At)→(Rt,St+1code)(S_{t},A_{t})\\rightarrow({\\color[rgb]{0.9140625,0.33984375,0.046875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.9140625,0.33984375,0.046875}R_{t},S_{t+1}^{\\text{code}}}). For visual succintness we denoteStimageS_{t}^{\\text{image}}asStS_{t}without the superscript in the diagram.",
                "position": 194
            }
        ]
    },
    {
        "header": "2gWorld: Generative Visual Code World Modeling",
        "images": []
    },
    {
        "header": "3MWMBench: Comprehensive Mobile GUI World Modeling Benchmark",
        "images": []
    },
    {
        "header": "4Empirical Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01576/x4.png",
                "caption": "Figure 4:Correlation between input-output similarity and model performance.Top:Pearson correlationρ\\rhobetween Sim(StS_{t},St+1S_{t+1}) and Sim(S^t+1\\hat{S}_{t+1},St+1S_{t+1}). Image generation models show strong positive correlations (ρ>0.7\\rho>0.7), suggesting output quality largely depends on how similarStS_{t}andSt+1S_{t+1}already are.Bottom:Sim(S^t+1\\hat{S}_{t+1},St+1S_{t+1})−-Sim(StS_{t},St+1S_{t+1}) vs. Sim(StS_{t},St+1S_{t+1}), with the gray line indicating the score ceiling.Emu3.5 34Bclusters near zero, implying Sim(S^t+1\\hat{S}_{t+1},St+1S_{t+1})≈\\approxSim(StS_{t},St+1S_{t+1}); i.e., outputs nearly identical to inputs,St≈S^t+1S_{t}\\approx\\hat{S}_{t+1}. In contrast,gWorld 32Bshows a wide vertical spread, indicating active state transformation with many samples achieving large positive gains toward the ceiling. Same analysis withQwen-Image-Edit 20Bis available in Appendix Fig.21with equivalent results.",
                "position": 833
            },
            {
                "img": "https://arxiv.org/html/2602.01576/x5.png",
                "caption": "Figure 5:Data scaling laws for mobile world modeling at8B.We fit power-law curves (y=a​xby=ax^{b}) to the test performance across five distinct benchmarks as a function of training dataset size.\nThe high coefficients of determination (R2≥0.94R^{2}\\geq 0.94for most splits) indicate apredictableandnon-saturatingrelationship between data scale and performance.\nThis suggests that our data generation pipeline has not yet reached its upper bound and will continue to improve with larger-scale repurposed trajectories.",
                "position": 863
            },
            {
                "img": "https://arxiv.org/html/2602.01576/x6.png",
                "caption": "Figure 6:Ablation onRtR_{t}train data quality.Our method consistently outperforms the naïve alternative in terms of IAcc. Both models are trained on 37K samples on top the base modelQwen3 VL 8B.",
                "position": 921
            }
        ]
    },
    {
        "header": "5Additional Discussion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AFurther Details on our Method",
        "images": []
    },
    {
        "header": "Appendix BExtended Details on MWMBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01576/x7.png",
                "caption": "(a)MWMBench-AndroidWorld: 686 transitions across 18 apps",
                "position": 2101
            },
            {
                "img": "https://arxiv.org/html/2602.01576/x7.png",
                "caption": "(a)MWMBench-AndroidWorld: 686 transitions across 18 apps",
                "position": 2104
            },
            {
                "img": "https://arxiv.org/html/2602.01576/x8.png",
                "caption": "(b)MWMBench-KApps: 495 transitions across 14 apps",
                "position": 2110
            }
        ]
    },
    {
        "header": "Appendix CFurther Details on Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01576/figs/kapps_collection.png",
                "caption": "Figure 8:Data collection interface software built for KApps.",
                "position": 2218
            }
        ]
    },
    {
        "header": "Appendix DExtended World Modeling Experiment Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01576/x9.png",
                "caption": "Figure 9:Data scaling analysis.We report average Instruction Accuracy (IAcc.) across the four in-distribution test splits as we scale the repurposed training data from 37K to 240K examples.\nThe results demonstratestrong positive scaling.",
                "position": 3665
            },
            {
                "img": "https://arxiv.org/html/2602.01576/x10.png",
                "caption": "Figure 10:Data scaling analysis.We report average Instruction Accuracy (IAcc.) inMWMBench-AndroidWorld. as we scale the repurposed training data from 37K to 240K examples.\nThe results demonstratestrong positive scaling.",
                "position": 3670
            },
            {
                "img": "https://arxiv.org/html/2602.01576/x11.png",
                "caption": "Figure 11:Inter-judge agreement on world model instruction-following.Each scatter plot compares Instruction-following Accuracy (IAcc., %) scored by two different VLM-as-a-Judge models (Gemini 3 Flash, GPT-5 Mini, Claude Haiku 4.5) acrossMWMBenchdatasets (AitW, GUIOdyssey, AndroidControl, AMEX, AndroidWorld, KApps). Each point corresponds to a (WM model, dataset) result, colors denote datasets, and the dashed line shows the linear regression fit. Spearman’sρ\\rhoand Kendall’sτ\\taushow strong rank and score consistency across judges.",
                "position": 3674
            },
            {
                "img": "https://arxiv.org/html/2602.01576/x12.png",
                "caption": "Figure 12:Consistency of model rankings across VLM-as-a-Judge choices.Bump chart shows the relative rank (1 = best) of each world model under different judges (Gemini 3 Flash,GPT-5 Mini,Claude Haiku 4.5), where ranks are determined by average IAcc. overMWMBenchdatasets. Rankings remain largely stable across judges, withgWorldconsistently achieving top performance compared to other code-generation and image-generation baselines.",
                "position": 3677
            },
            {
                "img": "https://arxiv.org/html/2602.01576/x13.png",
                "caption": "Figure 13:Additional qualitative example 1.",
                "position": 3680
            },
            {
                "img": "https://arxiv.org/html/2602.01576/x14.png",
                "caption": "Figure 14:Additional qualitative example 2.",
                "position": 3683
            },
            {
                "img": "https://arxiv.org/html/2602.01576/x15.png",
                "caption": "Figure 15:Additional qualitative example 3.",
                "position": 3686
            },
            {
                "img": "https://arxiv.org/html/2602.01576/x16.png",
                "caption": "Figure 16:Additional qualitative example 4.The red marker on the input is for visualization only and was not provided to the model. Action:clickat normalized coordinates (802, 394).",
                "position": 3689
            },
            {
                "img": "https://arxiv.org/html/2602.01576/x17.png",
                "caption": "Figure 17:Additional qualitative example 5.The red marker on the input is for visualization only and was not provided to the model. Action:clickat normalized coordinates (913, 143).",
                "position": 3692
            },
            {
                "img": "https://arxiv.org/html/2602.01576/x18.png",
                "caption": "Figure 18:Additional qualitative example 6 (Android World).The red marker on the input is for visualization only and was not provided to the model. Action:clickat normalized coordinates (756, 685).",
                "position": 3695
            },
            {
                "img": "https://arxiv.org/html/2602.01576/x19.png",
                "caption": "Figure 19:Additional qualitative example 7 (Android World).The red marker on the input is for visualization only and was not provided to the model. Action:clickat normalized coordinates (819, 549).",
                "position": 3698
            },
            {
                "img": "https://arxiv.org/html/2602.01576/x20.png",
                "caption": "Figure 20:Additional qualitative example 8.The red marker on the input is for visualization only and was not provided to the model. Action:TAPat normalized coordinates (857, 421).",
                "position": 3701
            },
            {
                "img": "https://arxiv.org/html/2602.01576/x21.png",
                "caption": "Figure 21:Same analysis as Figure4(bottom) withQwen-Image-Edit 20B(Qwen-I-E).Qwen-Image-Edit 20Bexhibits Sim(S^t+1\\hat{S}_{t+1},St+1S_{t+1})≈\\approxSim(StS_{t},St+1S_{t+1}), indicating that its outputs are nearly identical to the inputs (St≈S^t+1S_{t}\\approx\\hat{S}_{t+1}) regardless of the required action.",
                "position": 3704
            }
        ]
    },
    {
        "header": "Appendix ELimitations and Future Work",
        "images": []
    }
]