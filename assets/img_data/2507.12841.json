[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12841/x1.png",
                "caption": "Figure 1:The overall framework of ACM.Given image, video, and audio inputs, base captioning models first generate initial outputs. ACM refines these outputs by aligning modality features with specific user instructions, enabling controllable captioning without retraining the base models.",
                "position": 150
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3AnyCap Project",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12841/x2.png",
                "caption": "Figure 2:The workflow of AnyCapModel.(a) Integrating ACM into frozen base models enhances caption controllability across multiple modalities. (b) ACM aligns modality features, initial captions, and user instructions to produce refined, instruction-compliant captions.",
                "position": 224
            },
            {
                "img": "https://arxiv.org/html/2507.12841/x3.png",
                "caption": "Figure 3:Evaluation methodology of AnyCapEval.(a) Examples demonstrating content evaluation via Keypoint Density (KPD) and style scoring rules. (b) KPD correlation analysis showing superior reliability. (c) Radar chart depicting performance gains from ACM integration.",
                "position": 540
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12841/extracted/6630326/figures/ratio_effect_plot.png",
                "caption": "Table 4:Results on audio captioning using AnyCapEval. The table shows that ACM consistently boosts content and style metrics, demonstrating its strong effectiveness and adaptability even in low-resource audio modality environments.",
                "position": 2201
            },
            {
                "img": "https://arxiv.org/html/2507.12841/extracted/6630326/figures/ratio_effect_plot.png",
                "caption": "Table 5:Comparison across Training Methods.",
                "position": 2593
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ADatasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12841/x4.png",
                "caption": "Figure 5:ACM enables controllable captioning across modalities by refining base model outputs to better align with user instructions. Given a user instruction, it takes initial captions from a foundation model and corrects instruction violations (highlighted in red), producing compliant, instruction-following outputs (green) — all without requiring fine-tuning of the base model.",
                "position": 4718
            }
        ]
    },
    {
        "header": "Appendix BAnyCapEval Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12841/x5.png",
                "caption": "Figure 6:Qualitative examples of controllable caption generation from images.",
                "position": 9004
            },
            {
                "img": "https://arxiv.org/html/2507.12841/x6.png",
                "caption": "Figure 7:Qualitative examples of controllable caption generation from videos.",
                "position": 9007
            },
            {
                "img": "https://arxiv.org/html/2507.12841/x7.png",
                "caption": "Figure 8:Qualitative examples of controllable caption generation from audios.",
                "position": 9010
            },
            {
                "img": "https://arxiv.org/html/2507.12841/x8.png",
                "caption": "Figure 9:Images in the first column are original real images. The second column shows images generated from GPT-4o captions using DALL·E 3, while the third column shows results from our refined captions. Our method leads to more faithful visual content and better alignment with the original image semantics.",
                "position": 9017
            },
            {
                "img": "https://arxiv.org/html/2507.12841/x9.png",
                "caption": "Figure 10:Enhanced text-to-video generation through refined caption quality. Videos in the top row are generated from original dataset captions. The bottom row showcases videos generated using our model’s refined captions, demonstrating improved visual fidelity and more expressive camera motion.",
                "position": 9020
            },
            {
                "img": "https://arxiv.org/html/2507.12841/extracted/6630326/figures/human_preference.png",
                "caption": "Figure 11:Human evaluation of ACM-8B versus GPT-4o with respect to content and style. Bars indicate the proportion of responses rating ACM-8B as worse, similar, or better. Overall, ACM-8B demonstrates superior performance in most cases.",
                "position": 9030
            },
            {
                "img": "https://arxiv.org/html/2507.12841/x10.png",
                "caption": "Figure 12:Screenshot of human evaluation.",
                "position": 9036
            }
        ]
    },
    {
        "header": "Appendix CAdditional Experimental Details",
        "images": []
    }
]