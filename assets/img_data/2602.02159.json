[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Motivation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02159/x1.png",
                "caption": "Figure 1:Confidence dynamics analysis for LLaDA-8B-InstructNieet al.(2025)on GSM8​K8KCobbeet al.(2021)(L=76L=76,N=128N=128, andT=128T=128). (Left) Confidence score correlation between adjacent steps. (Right) Step-wise recall rates of predicting the unmasked tokens atttusing the remasked tokens with top-44highest confidence scores att−1t-1.",
                "position": 224
            }
        ]
    },
    {
        "header": "5Focus-dLLM",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02159/x2.png",
                "caption": "Figure 2:Attention patterns across decoding steps and layers in LLaDA-8B-InstructNieet al.(2025)(L=49L=49,N=128N=128,T=128T=128). More visual results can be found in the Appendix.",
                "position": 254
            },
            {
                "img": "https://arxiv.org/html/2602.02159/x3.png",
                "caption": "Figure 3:Overview of Focus-dLLM. We predict unmasked positions at the current step using previous confidence scores. These positions act as queries to retrieve relevant prompt blocks, where attention is computed over the union of these blocks and dynamically identified attention sinks.",
                "position": 257
            }
        ]
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02159/x4.png",
                "caption": "Figure 4:NiahKamradt (2023)results on UltraLLaDAHeet al.(2025)under long-context settings with a maximum context length of32​K32Kacross different layer depths.",
                "position": 673
            },
            {
                "img": "https://arxiv.org/html/2602.02159/x5.png",
                "caption": "Figure 5:Efficiency evaluation. Comparison of decoding throughput (tokens/s) on UltraLLaDAHeet al.(2025)(Left) and Dream-7B-InstructYeet al.(2025)(Right) across varying context lengths. Red numbers indicate the speedup ratio of Focus-dLLM relative to the Vanilla baseline.",
                "position": 676
            },
            {
                "img": "https://arxiv.org/html/2602.02159/x6.png",
                "caption": "Figure 6:Accuracyvs.throughput for UltraLLaDAHeet al.(2025)on LongBenchBaiet al.(2024)with16​K16K.",
                "position": 702
            }
        ]
    },
    {
        "header": "7Ablation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02159/x7.png",
                "caption": "Figure 7:Ablations on hyperparameters of Focus-dLLM on LongBenchBaiet al.(2024).",
                "position": 831
            }
        ]
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BDetails of Accuracyvs.Efficiency Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02159/x8.png",
                "caption": "Figure 8:Attention patterns in LLaDA-8B-InstructNieet al.(2025)across various layers and denoising steps. The heatmaps demonstrate the emergence of attention sinks (vertical bands) and their strong positional consistency across different layers within the same step.",
                "position": 2116
            }
        ]
    },
    {
        "header": "Appendix CAttention Patterns of dLLM",
        "images": []
    }
]