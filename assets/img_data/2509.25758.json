[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25758/x1.png",
                "caption": "Figure 1:Reasoning circuits trace the internal computations of LRMs at each checkpoint. After post-training, newly activated attention heads influence the performance at those checkpoints.",
                "position": 146
            },
            {
                "img": "https://arxiv.org/html/2509.25758/x2.png",
                "caption": "Figure 2:Analysis of Emergent Attention Head in Qwen2.5-Math-1.5B during GRPO.\n(A) denotes a cohort analysis of attention head activation across trained checkpoints.\nThe blue line tracks the absolute number of newly activated heads compared to the base model, while the red dashed line indicates the number of original heads that are maintained.\nThe stacked areas represent cohorts of heads, color-coded by the checkpoint at which they first emerged, showing their persistence and evolution over time.\nThe fluctuation in newly activated heads shows a similar trend to the (B), accuracy reward curve.\n(C) shows a heatmap detailing the changes in activation frequency.\nRed cells denote heads from the original base model, with fading intensity indicating their gradual deactivation.\nBlue cells represent newly emerged heads, with darker shades signifying higher activation frequency across checkpoints.\nHeads active in the final checkpoint are outlined with a black border.",
                "position": 163
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Identifying Emergent Attention Heads with Circuits",
        "images": []
    },
    {
        "header": "4In-Depth Analysis on SFT & Distillation",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25758/x3.png",
                "caption": "Figure 3:Analysis of Emergent Attention Head in Qwen2.5-Math-1.5B during SFT.\n(A) denotes a cohort analysis of attention head activation over training checkpoints.\nThe blue line tracks the absolute number of newly activated heads compared to the base model, while the red dashed line indicates the number of original heads that are maintained.\nThe stacked areas represent cohorts of heads, color-coded by the checkpoint at which they first emerged, showing their persistence and evolution over time.\n(B) shows a heatmap detailing the changes in activation frequency.\nRed cells denote heads from the original base model, with fading intensity indicating their gradual deactivation.\nBlue cells represent newly emerged heads, with darker shades signifying higher activation frequency across checkpoints.\nHeads active in the final checkpoint are outlined with a black border.",
                "position": 553
            }
        ]
    },
    {
        "header": "5In-Depth Analysis on GRPO",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25758/x4.png",
                "caption": "Figure 4:Performance change among various benchmarks for each checkpoints of GRPO training with two different training dataset: GSM8K(Cobbe et al.,2021)and OpenR1-Math-220k(Hugging Face,2025).\nThe green and red arrow indicate impressive performance gain and lose among various checkpoints, and the captions are the summaries of qualitative analysis.\nThe performance trade-off of each checkpoints is similarly reproduced when we apply attention head scaling with emergent reasoning heads for the baseline model.\nActual examples are presented in the AppendixA.4toA.5.",
                "position": 609
            }
        ]
    },
    {
        "header": "6In-Depth Analysis on Think On/Off",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25758/x5.png",
                "caption": "Figure 5:Performance difference against increasing coverage.\nThe left figure shows pass@k difference when sampling coverage increased, while the right figure shows efficient correctness with success@k.",
                "position": 717
            }
        ]
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Conclusion and Limitation",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25758/x6.png",
                "caption": "Figure 6:Analysis of Emergent Attention Head in Qwen2.5-Math-1.5B during GRPO with GSM8k(Cobbe et al.,2021)dataset.\n(A) denotes a cohort analysis of attention head activation over training checkpoints.\nThe blue line tracks the absolute number of newly activated heads compared to the base model, while the red dashed line indicates the number of original heads that are maintained.\nThe stacked areas represent cohorts of heads, color-coded by the checkpoint at which they first emerged, showing their persistence and evolution over time.\n(B) shows a heatmap detailing the changes in activation frequency.\nRed cells denote heads from the original base model, with fading intensity indicating their gradual deactivation.\nBlue cells represent newly emerged heads, with darker shades signifying higher activation frequency across checkpoints.\nHeads active in the final checkpoint are outlined with a black border.",
                "position": 2173
            },
            {
                "img": "https://arxiv.org/html/2509.25758/x7.png",
                "caption": "Figure 7:Analysis of Emergent Attention Head in Qwen2.5-Math-1.5B during GRPO with OpenR1-Math-220k(Hugging Face,2025)dataset with learning rate 2e-05.\n(A) denotes a cohort analysis of attention head activation over training checkpoints.\nThe blue line tracks the absolute number of newly activated heads compared to the base model, while the red dashed line indicates the number of original heads that are maintained.\nThe stacked areas represent cohorts of heads, color-coded by the checkpoint at which they first emerged, showing their persistence and evolution over time.\nThe fluctuation in newly activated heads shows a similar trend to the (B), accuracy reward curve.\n(C) shows a heatmap detailing the changes in activation frequency.\nRed cells denote heads from the original base model, with fading intensity indicating their gradual deactivation.\nBlue cells represent newly emerged heads, with darker shades signifying higher activation frequency across checkpoints.\nHeads active in the final checkpoint are outlined with a black border.",
                "position": 2184
            },
            {
                "img": "https://arxiv.org/html/2509.25758/x8.png",
                "caption": "Figure 8:Analysis of Emergent Attention Head in Qwen2.5-Math-1.5B during GRPO with OpenR1-Math-220k(Hugging Face,2025)dataset, and circuit construction with AMC(AI-MO,2024)benchmark.\n(A) denotes a cohort analysis of attention head activation over training checkpoints.\nThe blue line tracks the absolute number of newly activated heads compared to the base model, while the red dashed line indicates the number of original heads that are maintained.\nThe stacked areas represent cohorts of heads, color-coded by the checkpoint at which they first emerged, showing their persistence and evolution over time.\n(B) shows a heatmap detailing the changes in activation frequency.\nRed cells denote heads from the original base model, with fading intensity indicating their gradual deactivation.\nBlue cells represent newly emerged heads, with darker shades signifying higher activation frequency across checkpoints.\nHeads active in the final checkpoint are outlined with a black border.",
                "position": 2196
            },
            {
                "img": "https://arxiv.org/html/2509.25758/x9.png",
                "caption": "Figure 9:Analysis of Emergent Attention Head in Qwen2.5-Math-1.5B during SFT with OpenR1-Math-220k(Hugging Face,2025)dataset, and circuit construction with AMC(AI-MO,2024)benchmark.\n(A) denotes a cohort analysis of attention head activation over training checkpoints.\nThe blue line tracks the absolute number of newly activated heads compared to the base model, while the red dashed line indicates the number of original heads that are maintained.\nThe stacked areas represent cohorts of heads, color-coded by the checkpoint at which they first emerged, showing their persistence and evolution over time.\nThe fluctuation in newly activated heads shows a similar trend to the (B), accuracy reward curve.\n(C) shows a heatmap detailing the changes in activation frequency.\nRed cells denote heads from the original base model, with fading intensity indicating their gradual deactivation.\nBlue cells represent newly emerged heads, with darker shades signifying higher activation frequency across checkpoints.\nHeads active in the final checkpoint are outlined with a black border.",
                "position": 2207
            },
            {
                "img": "https://arxiv.org/html/2509.25758/x10.png",
                "caption": "Figure 10:Map of Reasoning: Visualization of emergent reasoning heads in circuits based on Qwen2.5-Math-1.5B with various post-training, and DeepSeek-R1-Distill-Qwen-1.5B.\n(Top) A map of emergent attention heads for each post-training method, compared to the baseline model (white).\n(Bottom) A cumulative map of the reasoning heads, with columns sorted by the number of newly activated heads.\nEach GRPO and SFT category encompass both AIME and AMC benchmark based circuits, with checkpoints of both training using OpenR1-Math-220k and GSM8k dataset.\nDeepSeek Distillation activates enormous heads (blue), as SFT activates similarly large amount of heads, though SFT heads are mostly concentrated in mid-to-late layer (green).\nSome of attention heads from GRPO training are also common in the SFT and Distillation reasoning heads (yellow and purple), however, the number of GRPO heads are much smaller and distributed across layers (red).",
                "position": 2219
            },
            {
                "img": "https://arxiv.org/html/2509.25758/x11.png",
                "caption": "Figure 11:Actual Example of Circuits.\nColor of nodes are randomly mapped to differentiate each others.\n(A) denotes AIME circuit with baseline model, Qwen-2.5-Math-7B.\n(B) shows AIME circuit with DeepSeek-R1-Distill-Qwen-7B.\n(C) is the comparative example with same AIME dataset, which is constructed with DeekSeek-R1-Distill-Qwen-7B and its own sampled answer, without explicit<think>.\n(C) is more complex than other two circuits, which could be mixed with confusable attention heads.\nThe trend of this enormous attention heads in (C) is also similar with the thinking off mode in Figure13(B), where the model compensate its performance gap through large emergent attention heads.",
                "position": 2228
            },
            {
                "img": "https://arxiv.org/html/2509.25758/x12.png",
                "caption": "Figure 12:Actual Example of Circuits After Post-Training.\nColor of nodes are randomly mapped to differentiate each others.\n(A) denotes AIME circuit after SFT with baseline model, Qwen-2.5-Math-1.5B.\n(B) shows AIME circuit after GRPO with the same baseline model.\n(A) activates more attention heads while (B) has more complexly connected specific nodes which refer its internalized high-level mathematical reasoning.",
                "position": 2238
            },
            {
                "img": "https://arxiv.org/html/2509.25758/x13.png",
                "caption": "Figure 13:Actual Example of Circuits of Think On/Off.\nColor of nodes are randomly mapped to differentiate each others.\n(A) denotes AIME circuit of Thinking on mode in Qwen3-8B.\n(B) shows AIME circuit of Thinking off on the same baseline model.\n(B) activates more attention heads, in contrast, (A) has more complexly connected specific nodes which refer its internalized high-level mathematical reasoning, similar as GRPO circuit in Figure12(B).",
                "position": 2247
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]