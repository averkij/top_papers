[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.00710/figures/dis.jpg",
                "caption": "Figure 1:(A) Step-length distribution in the AlphaMaze[8]training set, where the number of movess∈{1,2,3,4,5}s\\in\\{1,2,3,4,5\\}is sampled according to an inverted Gaussian-like distribution centered ats=3s=3, ensuring higher frequencies for both simple and complex cases.\n(B) Distribution of directional turns in the training set under the same controlled sampling scheme.\n(C) Step-length distribution in the AlphaMaze[8]testing set, constructed via uniform sampling.\n(D) Distribution of directional turns in the testing set under the same controlled sampling scheme.",
                "position": 387
            },
            {
                "img": "https://arxiv.org/html/2511.00710/figures/benchmark_example.jpg",
                "caption": "Figure 2:Illustrative examples from our two controlled benchmarks for path-finding evaluation.MapBench(left) features human-readable, outdoor navigation tasks derived from challenging real-world scenarios (e.g., mall navigation), designed to assess naturalistic instruction-following and local decision-making.ReasonMap(right) uses high-resolution transit maps from global metropolitan systems (e.g., Los Angeles, Toronto, Beijing), with a two-tier evaluation (short vs. long questions) to probe both fine-grained visual comprehension and global route planning.",
                "position": 453
            },
            {
                "img": "https://arxiv.org/html/2511.00710/figures/curves.jpg",
                "caption": "Figure 3:Training reward dynamics and evaluation of path-following ability.(A)Reward curve during GRPO training, showing steady improvement in rewards and stable learning progress.(B, D)For Qwen2.5-VL-7B-Instruct, the success rate rapidly collapses to zero (at cases with 3 movement steps or 3 turns), while token length increases, suggesting that the VLM generates longer but unsuccessful trajectories.(C, E)In contrast, our Ariadne framework demonstrates a remarkable improvement in success rates at the base VLM boundary, raising performance from 0% to 50% on 3-step cases and from 0% to over 10% on 3-turn cases.\nToken length grows moderately, and the collapse point shifts from 3 to 5, reflecting that after RLVR training, the VLM succeeds on tasks where the base model consistently failed, indicating an extended reasoning boundary.",
                "position": 458
            },
            {
                "img": "https://arxiv.org/html/2511.00710/figures/cases.jpg",
                "caption": "Figure 4:Representative success (top row) and failure (bottom row) cases from the AlphaMaze test set under controlled step sizes (4, 6, 8).\nSuccess cases generally correspond to smoother layouts with limited detour requirements, enabling coherent long-range navigation.\nFailure cases, by contrast, arise in locally complex structures characterized by dense turns, narrow passages, and elongated detours, which challenge the model’s ability to maintain global path consistency.",
                "position": 489
            },
            {
                "img": "https://arxiv.org/html/2511.00710/figures/MapBench.jpg",
                "caption": "Figure 5:Navigation results on MapBench[10].Left:Trailtask with an unstructured outdoor layout.Right:Museumtask with a structured indoor layout.\nThe baseline Qwen2.5-VL-7B-Instruct model[18](red) produces incomplete or less feasible trajectories due to issues such as target misidentification, inefficient detours, or violations of environmental constraints.\nIn comparison, Ariadne (green) demonstrates more consistent and goal-directed planning while adhering to structural boundaries.",
                "position": 503
            },
            {
                "img": "https://arxiv.org/html/2511.00710/figures/reasonmap.jpg",
                "caption": "Figure 6:Performance comparison on ReasonMap across varying question and map difficulty levels.\n(a) Accuracy comparison showing that Ariadne consistently outperforms Qwen2.5-VL-7B-Instruct after GRPO training.\n(b–d) Average map scores across different combinations of question and map difficulty, illustrating that Ariadne achieves higher scores in most settings, particularly in more complex reasoning categories.",
                "position": 510
            },
            {
                "img": "https://arxiv.org/html/2511.00710/figures/id-ood_step_count_distribution.png",
                "caption": "Figure 7:Distribution of successful trajectory step counts generated by Ariadne and Qwen2.5-VL-7B-Instruct on the MapBench benchmark.\nAriadne is trained exclusively on trajectories with 1 to 5 steps. This range is marked as ’In-Distribution (Step Counts)’ (light green).\nThe ’Out-of-Distribution (Step Counts)’ region (dark green) evaluates the models’ ability to generalize to longer, unseen path lengths (6 steps and beyond).",
                "position": 515
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]