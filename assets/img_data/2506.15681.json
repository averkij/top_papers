[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.15681/x1.png",
                "caption": "Figure 1:(Left) Visualizing the token indices of a given image and text prompt and representing the possibility of distillation among various VLM pair combinations, comparing traditional distillation with our proposed distillation framework, GenRecal. Note that, the parentheses mean each VLMâ€™s LLM tokenizer, â€˜â€¦â€™ indicates the placement of image features, and the number of these features varies depending on the image embedding strategy. (Right) Comparing the performance of a challenging evaluation benchmark, MM-Vet[1], with [A] baseline, [B] SFT on the baseline, [C] traditional distillation and [D] GenRecal from same token types of large VLMs, and GenRecal with more powerful [E] teacher and [F] student VLMs.",
                "position": 121
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.15681/x2.png",
                "caption": "Figure 2:(Left) Comparison of the challenging benchmark performances, MMB[20], MM-Vet[1], MMMU[21], and MMMU-Pro[22]by changing teacher vision-language models (VLMs) to distill the knowledge into small VLMs. Notably, the more powerful the teacher VLMs we select, the greater the performance improvement we can achieve. (Right) Comparing the performance of the challenging benchmark: MMMU[21], with GenRecal and various vision-language models across model sizes.",
                "position": 141
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.15681/x3.png",
                "caption": "Figure 3:Overview of GenRecal architecture and its training stages. We letqssubscriptğ‘ğ‘ q_{s}italic_q start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPTandassubscriptğ‘ğ‘ a_{s}italic_a start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPTdenote small VLMâ€™s embedded tokens (i.e.,image and text tokens are included together) for question and answer in visual instruction tuning dataset. In addition,qlsubscriptğ‘ğ‘™q_{l}italic_q start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPTandalsubscriptğ‘ğ‘™a_{l}italic_a start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPTare denoted by the large VLMâ€™s embedded tokens. Note that, vision-related modules such as vision encoder and projector for image tokens are omitted in this figure.",
                "position": 179
            }
        ]
    },
    {
        "header": "3GenRecal: Generation after Recalibration",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.15681/x4.png",
                "caption": "Figure 4:Overview of regularization simultaneously done with first training stage-alignment.",
                "position": 202
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.15681/x5.png",
                "caption": "Figure 5:Distillation performance on MMMU[21]for various pairings os teacher and student VLM. Each cell indicates the resulting score when using the corresponding teacher (rows) and student (columns) model sizes.",
                "position": 950
            },
            {
                "img": "https://arxiv.org/html/2506.15681/x6.png",
                "caption": "Figure 6:An overview of our training pipeline, illustrating both the question prompt and the measurement/legend annotations (top), followed by t-SNE visualizations (bottom) of teacher and student VLM pairings at the initial and final training stages. The question prompt (upper-left) shows the format of the question, while the measurement and legend box (upper-right) shows key model components to measure. Each scatter plot in the lower panels corresponds to a different combination of teacher and student VLM sizes, capturing how the learned representations evolve from early to later training iterations.",
                "position": 963
            },
            {
                "img": "https://arxiv.org/html/2506.15681/x7.png",
                "caption": "Table 3:Comparing the final distillation performances from the teacher model: InternVL2.5-78B[13]with and without the regularization term (denoted as â€œRegâ€). Note that, the student models are each InternVL2.5-8B, 4B, 2B, and 1B.",
                "position": 969
            },
            {
                "img": "https://arxiv.org/html/2506.15681/x7.png",
                "caption": "Table 3:Comparing the final distillation performances from the teacher model: InternVL2.5-78B[13]with and without the regularization term (denoted as â€œRegâ€). Note that, the student models are each InternVL2.5-8B, 4B, 2B, and 1B.",
                "position": 970
            },
            {
                "img": "https://arxiv.org/html/2506.15681/x7.png",
                "caption": "Figure 7:Depicting the process of (a) computing cosine similarity, (b) the result without regularization or (c) with regularization. Because the number of the embedded tokens for small and large VLMs are naturally different due to vocabulary size, token split, and token ordering, therefore we do average the output tokens and compute cosine similarity.",
                "position": 1203
            }
        ]
    },
    {
        "header": "5Discussion and Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AToken Type Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.15681/x8.png",
                "caption": "",
                "position": 2068
            }
        ]
    },
    {
        "header": "Appendix BPossible Distillation Pair for VLM Combinations: Traditional Distillation versus GenRecal",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.15681/x9.png",
                "caption": "Figure 8:We explore the range of distillation combinations between teacher and student VLMs using two approaches: (a) traditional distillation[17]and (b) our proposed model, GenRecal. Unlike traditional distillationâ€”which support only a limited set of pairingsâ€”GenRecal offers the flexibility to select any model for distillation, thereby enabling a more versatile and comprehensive distillation framework.",
                "position": 2074
            }
        ]
    },
    {
        "header": "Appendix CExtended Related Works",
        "images": []
    },
    {
        "header": "Appendix DLoss Graphs for Recalibrator",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.15681/x10.png",
                "caption": "Figure 9:Illustrating the loss graphs of training the Recalibrator, where we deal with various combinations of VLMs: NVLM[12], Qwen2-VL[10], InternVL2[45], and InternVL2.5[13]. Note that, the parenthesis in the figure means the name of LLM used in VLMs. â€˜Recalib(qs,al)subscriptğ‘ğ‘ subscriptğ‘ğ‘™(q_{s},a_{l})( italic_q start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT )â€™ and â€˜Recalib(ql,al)subscriptğ‘ğ‘™subscriptğ‘ğ‘™(q_{l},a_{l})( italic_q start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT )â€™ represent the cross entropy loss with Recalibrator logits andVLM-headof large VLM (see Section 3.2). â€˜SmallVLM(qs,as)subscriptğ‘ğ‘ subscriptğ‘ğ‘ (q_{s},a_{s})( italic_q start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT )â€™ and â€˜LargeVLM(ql,al)subscriptğ‘ğ‘™subscriptğ‘ğ‘™(q_{l},a_{l})( italic_q start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT )â€™ means original cross entropy loss for SFT without Recalibrator. They are not used in training Recalibrator. They just represent the averaged cross entropy (perplexity) during the whole training to compare them with â€˜Recalib(qs,al)subscriptğ‘ğ‘ subscriptğ‘ğ‘™(q_{s},a_{l})( italic_q start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT )â€™ and â€˜Recalib(ql,al)subscriptğ‘ğ‘™subscriptğ‘ğ‘™(q_{l},a_{l})( italic_q start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT )â€™.",
                "position": 2097
            }
        ]
    },
    {
        "header": "Appendix EImplementation Details of GenRecal for Algorithms",
        "images": []
    },
    {
        "header": "Appendix FDataset Composition Analysis for Building GenRecal",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.15681/x11.png",
                "caption": "Figure 10:Accuracy trends across different datasets: (Left) MMMU[21], and (Right) MathVista[38]over three training stages. The performance is shown for different category of visual instruction tuning dataset: â€œAllâ€ (without any exclusion), â€œw.o. Knowledgeâ€, â€œw.o. Science & Mathâ€, and â€œw.o. Chart & Documentâ€. The results indicate the impact of these exclusions on accuracy progression, highlighting how the absence of specific knowledge domains affects. Note that, Note that each data point within the same stage represents 20% of the training progress, with five data points measured per stage.",
                "position": 2203
            }
        ]
    },
    {
        "header": "Appendix GAdditional Distillation Comparison",
        "images": []
    },
    {
        "header": "Appendix HBroader Impacts",
        "images": []
    }
]