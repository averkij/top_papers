[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07854/x1.png",
                "caption": "",
                "position": 165
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07854/x2.png",
                "caption": "Figure 2:Method overview.(a) ViewRopecomputes per-patch viewing rays from intrinsics, constructs local rotations, and rotates\nquery/key feature subvectors in attention. The resulting dot product encodes relative angular relationships\nbetween viewing rays.(b) Geometry-Aware Frame Sparse Attentionestimates block (frame) relevance and selects top-kkgeometrically relevant historical frames, replacing quadratic dense attention with geometry-driven sparsity.",
                "position": 375
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07854/x3.png",
                "caption": "Figure 3:Visualization of attention specialization.Left: A standard temporal head focuses on recent or temporally periodic frames. Middle: A geometry-aware head captures long-range spatial overlap (evident in the antidiagonal activation during loop closure). Right: The aggregated attention map illustrates how geometric cues guide sparse block selection.",
                "position": 851
            },
            {
                "img": "https://arxiv.org/html/2602.07854/x4.png",
                "caption": "Figure 4:Case study.Upper and lower sequences show ViewRope with Sliding Window and Sparse attention, respectively.",
                "position": 865
            }
        ]
    },
    {
        "header": "5Conclusion and Future Work",
        "images": []
    },
    {
        "header": "6Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetailed Training And Inference Pipeline",
        "images": []
    },
    {
        "header": "Appendix BLoop Closure Formulation",
        "images": []
    },
    {
        "header": "Appendix CViewBench Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07854/x5.png",
                "caption": "Figure 5:Case 1: Yaw + Pitch loop closure in an urban street.M-G 2.0 suffers from brightness collapse. HY-WorldPlay exhibits geometric drift. ViewRope maintains structural and lighting consistency.",
                "position": 2094
            },
            {
                "img": "https://arxiv.org/html/2602.07854/x6.png",
                "caption": "Figure 6:Case 2: Pure yaw loop closure in an Asian street.M-G 2.0 hallucinates entirely different content on return. HY-WorldPlay introduces nonexistent elements. ViewRope recovers the original scene faithfully.",
                "position": 2097
            },
            {
                "img": "https://arxiv.org/html/2602.07854/x7.png",
                "caption": "Figure 7:Case 3: Pure pitch loop closure in Roman architecture.M-G 2.0 generates a completely different scene upon return. HY-WorldPlay produces blurry, inconsistent structures. ViewRope accurately restores the original arched architecture.",
                "position": 2100
            }
        ]
    },
    {
        "header": "Appendix DAdditional Results",
        "images": []
    }
]