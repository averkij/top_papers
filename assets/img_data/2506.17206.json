[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.17206/x1.png",
                "caption": "",
                "position": 76
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.17206/x2.png",
                "caption": "Figure 2:Motivation.Previous works[50,55]on RGB-D panorama generation is based on equirectangular representations and only supports Euclidean depth instead of the more popular Z-depth. However, the distribution of Euclidean depth is quite different from that of RGB images (e.g., circles on flat surfaces as highlighted byorangedashed boxes), which hinders the use of pre-trained 2D diffusion priors. Multi-plane methods[21,51]support Z-depth, but their adopted FoV overlapping techniques lead to depth ambiguity, as highlighted byreddashed boxes. Different from existing works, our approach supports multi-plane Z-depth generation without using FoV overlapping techniques.",
                "position": 100
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Multi-plane Synchronization",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.17206/x3.png",
                "caption": "(a)Generated multi-plane images.",
                "position": 319
            },
            {
                "img": "https://arxiv.org/html/2506.17206/x3.png",
                "caption": "(a)Generated multi-plane images.",
                "position": 322
            },
            {
                "img": "https://arxiv.org/html/2506.17206/x4.png",
                "caption": "(b)Converted equirectangular images.",
                "position": 327
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.17206/x5.png",
                "caption": "Figure 4:Training and inference framework ofDreamCubefor RGB-D cube map generation. At training time, RGB-D cube faces are encoded by synced VAE and injected masked Gaussian noises, obtaining image and depth latents. These latents are concatenated with positional encoding and mask as diffusion U-Net‚Äôs input. The entire U-Net is fine-tuned withvùë£vitalic_v-objective[42]to learn to jointly denoise RGB and depth latents. At inference time, DreamCube receives single-view RGB-D images and multi-view texts as input and generates completed RGB-D cube map representations via iterative diffusion denoising and synced VAE decoding.",
                "position": 363
            },
            {
                "img": "https://arxiv.org/html/2506.17206/x6.png",
                "caption": "Figure 5:Comparison between UV encoding and XYZ encoding for omnidirectional image representations.",
                "position": 417
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.17206/x7.png",
                "caption": "Figure 6:Qualitative results of the proposed DreamCubeon RGB-D panorama generation compared with RGB-D panorama generation methods: LDM3D-Pano[50]and PanoDiffusion[55]. Thegreendashed boxes highlight the input image condition.",
                "position": 632
            },
            {
                "img": "https://arxiv.org/html/2506.17206/x8.png",
                "caption": "Figure 7:Qualitative results of the proposed Multi-plane Synchronizationon panoramic depth estimation compared with recent panoramic depth estimation methods: Depth Any Camera (DAC)[12]and Depth Anywhere[54].",
                "position": 635
            },
            {
                "img": "https://arxiv.org/html/2506.17206/x9.png",
                "caption": "Figure 8:Panorama-to-3D scene reconstruction.Based on the RGB-D cubemap generated by DreamCube, we can reconstruct the corresponding 3D scenes in seconds and obtain both 3D mesh and 3D Gaussian[23]representations.",
                "position": 638
            },
            {
                "img": "https://arxiv.org/html/2506.17206/x10.png",
                "caption": "Figure 9:Qualitative comparison of 3D point cloudsreconstructed from equirectangular-based and cubemap-based RGB-D panoramas. Equirectangular panoramas produce an uneven, ring-shaped 3D point distribution dense near the poles, while cubemap panoramas yield a more uniform and regular distribution.",
                "position": 641
            },
            {
                "img": "https://arxiv.org/html/2506.17206/x11.png",
                "caption": "Figure 10:Ablation analysis of Multi-plane Synchronization.We adopt Stable-Diffusion v2 as baseline model for multi-plane generation, and the text prompt used is ‚ÄúThe vast cosmos in the style of Van Gogh, with swirling patterns and vibrant colors.‚Äù",
                "position": 739
            },
            {
                "img": "https://arxiv.org/html/2506.17206/x12.png",
                "caption": "Figure 11:Ablation analysis of XYZ Positional Encoding.We present the qualitative results of the back view of cubemap, where the UV positional encoding introduces discontinuous numerical steps. This leads to line artifacts (Case 1) and incoherent visual contents (Case 2), as indicated by thereddashed box. In contrast, our proposed XYZ positional encoding alleviates these issues in both cases, as shown within thegreendashed box.",
                "position": 748
            },
            {
                "img": "https://arxiv.org/html/2506.17206/x13.png",
                "caption": "Figure 12:Out-domain RGB-D panorama generation.The RGB-D inputs are obtained by Flux.1-dev[27]and Depth Anything v2[59]. DreamCube demonstrates generalization ability on diverse inputs, maintaining high-quality RGB appearance and geometric consistency.",
                "position": 751
            },
            {
                "img": "https://arxiv.org/html/2506.17206/x14.png",
                "caption": "(a)Generated results from real-world inputs captured by sensors[3]. Even though the input depth is low-resolution (as indicated by theblackdashed circles), our method is still able to generate high-definition depth maps (as indicated by thegreendashed circles).",
                "position": 760
            },
            {
                "img": "https://arxiv.org/html/2506.17206/x14.png",
                "caption": "(a)Generated results from real-world inputs captured by sensors[3]. Even though the input depth is low-resolution (as indicated by theblackdashed circles), our method is still able to generate high-definition depth maps (as indicated by thegreendashed circles).",
                "position": 763
            },
            {
                "img": "https://arxiv.org/html/2506.17206/x15.png",
                "caption": "(b)Generated results from inputs with extreme viewing angles, where thegreendashed boxes highlight the input views.",
                "position": 769
            }
        ]
    },
    {
        "header": "6Limitation",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]