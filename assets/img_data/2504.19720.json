[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.19720/x1.png",
                "caption": "",
                "position": 143
            },
            {
                "img": "https://arxiv.org/html/2504.19720/x2.png",
                "caption": "",
                "position": 172
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.19720/x3.png",
                "caption": "Figure 1:Overview of the paper, detailing Instance, Cluster, Emerging Scenarios, and Miscellaneous Areas.ğ‘ğ‘\\mathbf{R}bold_Rrepresents a request. In Inter-request scheduling, two requests,ğ‘ğŸğ‘ğŸ\\mathbf{R1}bold_R1(10 toks) andğ‘ğŸğ‘ğŸ\\mathbf{R2}bold_R2(2 toks), arrive simultaneously. Ignoring the prefill process, ifğ‘ğŸğ‘ğŸ\\mathbf{R1}bold_R1is processed first, its generation rate is 1 tok/s, andğ‘ğŸğ‘ğŸ\\mathbf{R2}bold_R2â€™s rate is 0.2 tok/s. Reversing the order givesğ‘ğŸğ‘ğŸ\\mathbf{R2}bold_R2a rate of 1 tok/s andğ‘ğŸğ‘ğŸ\\mathbf{R1}bold_R10.9 tok/s. The default decoding speed is 1 token/s.",
                "position": 212
            }
        ]
    },
    {
        "header": "2Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.19720/x4.png",
                "caption": "Figure 2:Illustration of the LLM Inference process.",
                "position": 344
            }
        ]
    },
    {
        "header": "3LLM Inference Serving in Instance",
        "images": []
    },
    {
        "header": "4LLM Inference Serving in Cluster",
        "images": []
    },
    {
        "header": "5Emerging Scenarios",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.19720/x5.png",
                "caption": "Figure 6:This figure illustrates a MoE architecture, highlighting expert placement, All-to-All communication (left), and load balancing (right). On the right, high-traffic Expertğ¦ğ¦\\mathbf{m}bold_mand low-traffic Expertğ§ğ§\\mathbf{n}bold_nare shown. For example, two strategies are presented: replicating m to a new GPU or offloadingğ§ğ§\\mathbf{n}bold_nto free space forğ¦ğ¦\\mathbf{m}bold_m.",
                "position": 1083
            }
        ]
    },
    {
        "header": "6Miscellaneous Areas",
        "images": []
    },
    {
        "header": "7Future Works",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]