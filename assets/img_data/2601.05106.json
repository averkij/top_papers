[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05106/figures/5361767044637_.pic.jpg",
                "caption": "Figure 1:Sequence-level collaboration is coarse and inefficient, while prior token-level methods are unstable.FusionRouteachieves fine-grained, efficient, and robust token-level collaboration through complementary routing.",
                "position": 203
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3FusionRoute",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05106/figures/5391767045229_.pic.jpg",
                "caption": "Figure 2:Top:Overall architectural design ofFusionRoute.FusionRouteenables multiple specialized LLMs to work collaboratively through a carefully designed router.\nThe router outputsboththe weights for token-level routing and the logits to complement the experts’ output.Bottom:The training process is decomposed into two phases.\nIn the SFT phase (§3.2.1), the router learns token-wise mixture weights over the expert models and is jointly fine-tuned to become a good response generator.\nIn the CDPO phase (§3.2.2),FusionRouterefines the final policy by using preference optimization to improve upon the experts’ outputs.",
                "position": 352
            }
        ]
    },
    {
        "header": "4Theoretical Analysis",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05106/figures/llama_performance.png",
                "caption": "(a)Llama3-8B Family",
                "position": 1104
            },
            {
                "img": "https://arxiv.org/html/2601.05106/figures/llama_performance.png",
                "caption": "(a)Llama3-8B Family",
                "position": 1107
            },
            {
                "img": "https://arxiv.org/html/2601.05106/figures/gemma_performance.png",
                "caption": "(b)Gemma2-2B Family",
                "position": 1112
            }
        ]
    },
    {
        "header": "6Ablation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05106/figures/ablation_sft.png",
                "caption": "Figure 5:GPT-4o winrate comparison:FusionRoutewith vs. without CDPO training on both Llama-3 and Gemma-2 Family.",
                "position": 1166
            }
        ]
    },
    {
        "header": "7Related Works",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "9Proof of Theorem4.3",
        "images": []
    },
    {
        "header": "10Theoretical Discussion of Prior Token-Level Approaches",
        "images": []
    },
    {
        "header": "11Experiment Details",
        "images": []
    }
]