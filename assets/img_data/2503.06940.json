[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06940/extracted/6265938/figures/cine_icon.png",
                "caption": "",
                "position": 73
            },
            {
                "img": "https://arxiv.org/html/2503.06940/x1.png",
                "caption": "",
                "position": 90
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06940/x2.png",
                "caption": "Figure 2:Visualization of fMRI and EEG Responses in CineBrain.fMRI and EEG responses of subjects 1–4 to identical stimuli, illustrating individual differences in brain activation.",
                "position": 258
            }
        ]
    },
    {
        "header": "3Experimental Designs and Curated Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06940/x3.png",
                "caption": "Figure 3:ROIs from the fMRI signals used in our experiments.",
                "position": 311
            },
            {
                "img": "https://arxiv.org/html/2503.06940/x4.png",
                "caption": "Figure 4:Overview of the CineSync Framework.CineSync first employs a Multimodal Fusion Encoder to extract features from fMRI and EEG data, with a modality alignment module to align these features with semantic information. Subsequently, it utilizes a LoRA-tuned neural latent decoder to reconstruct videos based on the fused brain features.Note: The gray box is used only during training.",
                "position": 317
            }
        ]
    },
    {
        "header": "4Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06940/x5.png",
                "caption": "Figure 5:Qualitative comparison of our method with baselines.We compare the results of CineSync, CineSync-fMRI, and CineSync-EEG with the ground truth (GT). CineSync demonstrates higher accuracy, greater temporal consistency, and improved video quality.",
                "position": 466
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06940/x6.png",
                "caption": "Figure A.1:Architecture of CineSync for audio reconstruction.We concatenate the fused brain features with the noised latent representation of the mel spectrogram as the training input. We then apply LoRA[21]to the attention and feed-forward layers in the DiT[34]blocks of our neural latent decoder.",
                "position": 1290
            }
        ]
    },
    {
        "header": "Appendix BEEG Experimental Device",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06940/x7.png",
                "caption": "Figure B.1:Electrode montage of a 64-channel EEG cap using the GSN-HydroCel-64_1.0 layout. Sensor positions are annotated with their corresponding channel labels.",
                "position": 1304
            },
            {
                "img": "https://arxiv.org/html/2503.06940/x8.png",
                "caption": "Figure B.2:More Results of CineSync:We present 12 frames with timestamps compared with the ground truth (GT).",
                "position": 1311
            },
            {
                "img": "https://arxiv.org/html/2503.06940/x9.png",
                "caption": "Figure B.3:More Results of CineSync:We present 12 frames with timestamps compared with the ground truth (GT).",
                "position": 1317
            },
            {
                "img": "https://arxiv.org/html/2503.06940/x10.png",
                "caption": "Figure B.4:More Results of CineSync:We present 12 frames with timestamps compared with the ground truth (GT).",
                "position": 1324
            },
            {
                "img": "https://arxiv.org/html/2503.06940/x11.png",
                "caption": "Figure B.5:Qualitative results of CineSync and the two strong baselines are presented. “GT” indicates the ground-truth mel spectrogram.",
                "position": 1413
            }
        ]
    },
    {
        "header": "Appendix CPrompts for Generating Text Descriptions",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06940/x12.png",
                "caption": "Figure C.1:Prompt utilized by Qwen2.5-VL for generating video descriptions.",
                "position": 1427
            },
            {
                "img": "https://arxiv.org/html/2503.06940/x13.png",
                "caption": "Figure C.2:Prompt utilized by Llava-Video for generating video descriptions.",
                "position": 1433
            }
        ]
    },
    {
        "header": "Appendix DLimitations and Future Work",
        "images": []
    }
]