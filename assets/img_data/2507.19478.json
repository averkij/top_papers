[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.19478/x1.png",
                "caption": "Figure 1:MMBench-GUI: a hierarchical benchmark spanning four levels of increasing difficulty, covering over 8,000 tasks across six commonly used platforms. From L1 to L4, task complexity increases progressively, placing growing demands on the agent’s generalization and reasoning abilities. Based on this benchmark, we visualize the performance of various models in the right figure, clearly illustrating their respective strengths as well as areas with substantial room for improvement.",
                "position": 124
            }
        ]
    },
    {
        "header": "2Related works",
        "images": []
    },
    {
        "header": "3MMBench-GUI",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.19478/x2.png",
                "caption": "Figure 2:Examples for L1&L2.Both of them are offline tasks. We provide examples from different platforms for each level. For clarity, some less critical fields are not shown here and full examples are available for download in our public repository.",
                "position": 186
            },
            {
                "img": "https://arxiv.org/html/2507.19478/x3.png",
                "caption": "Figure 3:Examples for L3&L4.Tasks of these levels are evaluated in the virtual environment with an online manner. In L4, we provide two images belonging to different applications as examples to demonstrate that collaboration is the core aspect for this level.",
                "position": 636
            }
        ]
    },
    {
        "header": "4Benchmarking GUI Agent Baselines",
        "images": []
    },
    {
        "header": "5Analysis and discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.19478/x4.png",
                "caption": "Figure 4:Left:Demonstrates the relative contribution of visual grounding versus planning in driving performance gains under current conditions. We consider two experimental conditions—fixing the planner while varying the grounder, and vice versa—and examine how different combinations affect task success rate. Similar color hues denote groups with the same fixed planner or grounder.Right:Task success grows roughly linearly with visual-grounding accuracy. General-purpose language models are virtually “blind” at the L2 grounding stage, which drives their L3 automation success rate (SR) sharply down. Plugging in a dedicated visual grounder restores precise perception and, in turn, lifts SR dramatically—highlighting fine-grained grounding as the principal bottleneck.",
                "position": 5363
            },
            {
                "img": "https://arxiv.org/html/2507.19478/x5.png",
                "caption": "Figure 5:EQA visualization across different models under L3 for different allowed steps.As discussed in Section3.4, EQA reflects a combination of task completion and efficiency (i.e., the number of steps used upon completion). In practice, we compute it by interpolating both the step budget and the success rate (SR) 100 times. The area under the curve formed by these interpolated SR values yields the final EQA score.",
                "position": 5406
            },
            {
                "img": "https://arxiv.org/html/2507.19478/x6.png",
                "caption": "Figure 6:Difficulty-Gradient Heatmap.Models’ scores across difficulty levels are encoded with a single-hue palette whose saturation fades from high (dark) to low (light). Colored rectangles outline comparable model groups. Within and across these groups, the color consistently fades from L1, L2 to L3 and L4, indicating that higher task complexity amplifies each model’s weaknesses and causes a steep performance drop-off.",
                "position": 5840
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]