[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.18931/x1.png",
                "caption": "Figure 1:Overview ofCoMP. Our method accepts an image at native resolution and its corresponding text. Then, in addition to training through text decoding in next-token prediction paradigm, we also explicitly project the visual features into the language space of LLM using Alignment Loss.",
                "position": 69
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.18931/x2.png",
                "caption": "Figure 2:Architecture of ourCoMP.(a) Overview of our pre-training framework;\n(b) Detail ofC-RoPE. For ease of visualization, the projection layersğ’«â¢râ¢oâ¢jq,k,v,oğ’«ğ‘Ÿğ‘œsubscriptğ‘—ğ‘ğ‘˜ğ‘£ğ‘œ\\mathcal{P}roj_{q,k,v,o}caligraphic_P italic_r italic_o italic_j start_POSTSUBSCRIPT italic_q , italic_k , italic_v , italic_o end_POSTSUBSCRIPTand scale operators are omitted.",
                "position": 148
            },
            {
                "img": "https://arxiv.org/html/2503.18931/x3.png",
                "caption": "Figure 3:Alignment Loss. We illustrate it in the case of one single pair of global vision and text featuresğ…vsubscriptğ…ğ‘£\\mathbf{F}_{v}bold_F start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPTandğ…tsubscriptğ…ğ‘¡\\mathbf{F}_{t}bold_F start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTfor simplicity.ğ…vsubscriptğ…ğ‘£\\mathbf{F}_{v}bold_F start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPTandğ…tsubscriptğ…ğ‘¡\\mathbf{F}_{t}bold_F start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTare mapped by frozen learned prototypeğ–ğ–\\mathbf{W}bold_W,i.e., the word embedding of LLMs. Then, they are converted into normalized probabilities using the Softmax function and iterative Sinkhorn-Knopp algorithm[12], respectively. Then, cross-entropy is applied as the loss. To prevent information leakage, the text features are extracted without image prefixes.",
                "position": 455
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.18931/x4.png",
                "caption": "Figure 4:Varying the image resolution during inference.We investigate the impact of image resolution on DocVQA[47]and ChartQA[46]by ourCoMP-MM-1B.",
                "position": 1129
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AHyperparameters",
        "images": []
    }
]