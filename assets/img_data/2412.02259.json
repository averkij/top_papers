[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.02259/x1.png",
                "caption": "",
                "position": 98
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries and Problem Formulation",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.02259/x2.png",
                "caption": "Figure 2:The FlowChart ofVideoGen-of-Thought.Left:Shot descriptions are generated based on user prompts, describing various attributes including character details, background, relations, camera pose, and lighting HDR. Pre-shot descriptions provide a broader context for the upcoming scenes.Middle Top:Keyframes are generated using a text-to-image diffusion model conditioned with identity-preserving (IP) embeddings, which ensures consistent representation of characters throughout the shots. IP portraits help maintain visual identity consistency.Right:The shot-level video clips are generated from keyframes, followed by shot-by-shot smooth inference to ensure temporal consistency across different shots. This collaborative framework ultimately produces a cohesive narrative-driven video.",
                "position": 165
            }
        ]
    },
    {
        "header": "4Method: VideoGen-of-Thought",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.02259/x3.png",
                "caption": "Figure 3:Visual comparison ofVGoTand baselines",
                "position": 327
            },
            {
                "img": "https://arxiv.org/html/2412.02259/x4.png",
                "caption": "Figure 4:Visual showcases ofVGoTgenerated multi-shot videos.",
                "position": 330
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.02259/x5.png",
                "caption": "Figure 5:Visual Demonstration of the ablation studies ofVGoT",
                "position": 746
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details of Script Module",
        "images": []
    },
    {
        "header": "Appendix BAdditional Results",
        "images": []
    },
    {
        "header": "Appendix CUser Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.02259/x6.png",
                "caption": "Figure 6:VGoTVisual complement of the multi-camera video generated.",
                "position": 1905
            },
            {
                "img": "https://arxiv.org/html/2412.02259/x7.png",
                "caption": "Figure 7:Visual comparison ofVGoTwith baselines Supplement.",
                "position": 1908
            },
            {
                "img": "https://arxiv.org/html/2412.02259/x8.png",
                "caption": "Figure 8:Designed user study interface. Each participant was required to rate 50 videos by answering three sub-questions for each video. Due to page limitations, only two videos are shown here.",
                "position": 1911
            }
        ]
    },
    {
        "header": "Appendix DLimitations and Future Work",
        "images": []
    }
]