[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.19730/extracted/5961192/fig111.png",
                "caption": "Figure 1:Experimental results on average counting accuracy based on different tokenization choices, using GPT-4o mini. Our approach treats the model as ablack-box, manipulating BPE tokenizers to function differently through carefully engineered string formats.",
                "position": 90
            }
        ]
    },
    {
        "header": "2Neural Networks and Counting: A Revisit",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.19730/extracted/5961192/Inductive.png",
                "caption": "Figure 2:Illustration of inductive counting as performed by humans, RNNs, and LLMs with CoT, respectively.",
                "position": 118
            }
        ]
    },
    {
        "header": "3CoT + Ideal Assumption = Complete Counting Ability",
        "images": []
    },
    {
        "header": "4Tokenization as a Black Box Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.19730/extracted/5961192/tokenization_gpt4o.png",
                "caption": "Figure 3:Four types of string formatting used for counting tasks to manipulate tokenization in LLMs. Examples in the figure are tokenized using the GPT-4o tokenizer. Each string-token type is labeled as (a), (b), (c), and (d) in the diagram. Note that changing the format does not alter the fundamental nature or difficulty of the counting task.",
                "position": 164
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.19730/extracted/5961192/shift-30-40-a.png",
                "caption": "Figure 4:Distribution of shift from correct count to GPT-calculated count, for each type of string-token fomrat (a), (b), (c) and (d) in order. The statsiticas show the results for letteraat length range [30, 40], as this range the error rate is high. We only calculate the shift when error is made, as correct counting instance does not have any shift.",
                "position": 486
            },
            {
                "img": "https://arxiv.org/html/2410.19730/extracted/5961192/violin_plot.png",
                "caption": "Figure 5:Pairwise comparison of counting accuracy for different letters in strings. The left plot shows the distribution of accuracy foraandbinabstrings, with each dot representing the average accuracy forain a given CoT case (e.g., spaced-string in the [10,20] range), connected to the corresponding accuracy forbin the same setting. The right plot illustrates a similar case foreandzinezstrings. Note: The y-axis limit exceeds [0,1] as the distribution is calculated based on variance and mean, with larger variance pushing the upper bound of the confidence interval beyond the maximum value.",
                "position": 529
            },
            {
                "img": "https://arxiv.org/html/2410.19730/extracted/5961192/letter_freq.png",
                "caption": "Figure 6:Counting accuracy (Orange) with respect to target letter frequency (Blue) in Human Natural Language.",
                "position": 595
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ASupervised Chain of Thought",
        "images": []
    },
    {
        "header": "Appendix BReplication Experiments Note",
        "images": []
    },
    {
        "header": "Appendix CTokenization in Different LLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.19730/extracted/5961192/tokenization-diff.png",
                "caption": "Figure 7:Difference in tokenization on strings when counting instances are presented in different formats, across different LLMs.",
                "position": 1482
            }
        ]
    },
    {
        "header": "Appendix DCase Study",
        "images": []
    }
]