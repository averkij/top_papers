[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.14249/x1.png",
                "caption": "Figure 1:Compared against the saturation of some existing benchmarks,Humanity’s Last Examaccuracy remains low across several frontier models, demonstrating its effectiveness for measuring advanced, closed-ended, academic capabilities. The sources for our evaluation metrics are detailed inSectionC.5. We further evaluate more frontier models onHLEinTable1.",
                "position": 162
            },
            {
                "img": "https://arxiv.org/html/2501.14249/x2.png",
                "caption": "Figure 2:Samples of the diverse and challenging questions submitted toHumanity’s Last Exam.",
                "position": 168
            },
            {
                "img": "https://arxiv.org/html/2501.14249/x3.png",
                "caption": "Figure 3:HLEconsists of3,00030003{,}0003 , 000exam questions in over a hundred subjects, grouped into high level categories here. We provide a more detailed list of subjects inSectionB.3.",
                "position": 171
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.14249/x4.png",
                "caption": "Figure 4:Dataset creation pipeline. We accept questions that make frontier LLMs fail, then iteratively refine them with the help of expert peer reviewers. Each question is then manually approved by organizers or expert reviewers trained by organizers. A private held-out set is kept in addition to the public set to assess model overfitting and gaming on the public benchmark.",
                "position": 231
            }
        ]
    },
    {
        "header": "4Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.14249/x5.png",
                "caption": "Figure 5:Average completion token counts of reasoning models tested, including both reasoning and output tokens. We also plot average token counts for non-reasoning models inSectionC.3.",
                "position": 328
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAuthors",
        "images": []
    },
    {
        "header": "Appendix BDataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.14249/x6.png",
                "caption": "Figure 6:Average output token counts of non-reasoning models.",
                "position": 3731
            }
        ]
    },
    {
        "header": "Appendix CEvaluation",
        "images": []
    }
]