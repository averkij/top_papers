[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07464/x1.png",
                "caption": "",
                "position": 100
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Architecture",
        "images": []
    },
    {
        "header": "3Pre-training",
        "images": []
    },
    {
        "header": "4Training System Optimization",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07464/x2.png",
                "caption": "Figure 1:Illustration of the All-to-All gather and scatter process in Parallel Muon. Each rank exchanges its sharded gradients with all other ranks during the gather phase and redistributes the computed results in the scatter phase.",
                "position": 820
            },
            {
                "img": "https://arxiv.org/html/2511.07464/x3.png",
                "caption": "Figure 2:Illustration of pipelined execution of all-to-all gather, computation, and scatter phases. A total of eight gradients are distributed across two ranks and partitioned into chunks of size two. Computation and communication are scheduled in a pipelined manner, allowing overlap between them and improving overall efficiency.",
                "position": 830
            }
        ]
    },
    {
        "header": "5Post-training",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Appendix",
        "images": []
    }
]