[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23581/x1.png",
                "caption": "Figure 1:Lookahead Anchoring enables robust long-form audio-driven animation.While autoregressive generation with HunyuanAvatar(Chen et al.,2025a)(left) and OmniAvatar(Gan et al.,2025)(right) progressively loses character identity and lip sync quality, our approach maintains both throughout extended generation. We provide video results inthe project page.",
                "position": 143
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23581/x2.png",
                "caption": "Figure 2:Motivation.(a) We depart from the convention of using conditional keyframes as generation window endpoints. Instead, we reposition keyframes as temporally distant anchors beyond the window, decoupling them from the actual generated sequence. This eliminates constraints such as audio synchronization requirements while enabling flexible conditioning. (b) Models naturally learn that longer temporal distances allow for greater scene variation. We exploit this prior strategically: distant keyframes provide high-level guidance without imposing strict physical constraints, enabling diverse yet coherent generation.",
                "position": 225
            },
            {
                "img": "https://arxiv.org/html/2510.23581/x3.png",
                "caption": "Figure 3:Exploration of distant frame relationships in a pretrained video DiT(Gan et al.,2025).Given a conditional frame, we generate separate two-frame videos with artificially increased temporal gaps. Testing beyond the training distribution naturally degrades visual quality but reveals adaptive motion behavior. We propose fine-tuning to harness this observed temporal structure.",
                "position": 308
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23581/x4.png",
                "caption": "Figure 4:Qualitative results.We compare our method with three audio-conditioned DiT baselines under the temporal sement-wise autoregressive framework on AVSpeech(Ephrat et al.,2018)and HDTF(Zhang et al.,2021), presenting mid-sequence frames to demonstrate generation quality. Video results are available inthe project page.",
                "position": 589
            },
            {
                "img": "https://arxiv.org/html/2510.23581/x5.png",
                "caption": "Figure 5:Performance over time.We report FID computed with 1-second sliding windows, normalized relative to the first window.",
                "position": 595
            },
            {
                "img": "https://arxiv.org/html/2510.23581/x5.png",
                "caption": "Figure 5:Performance over time.We report FID computed with 1-second sliding windows, normalized relative to the first window.",
                "position": 597
            },
            {
                "img": "https://arxiv.org/html/2510.23581/x6.png",
                "caption": "Figure 6:Effect of temporal distance between conditional keyframes and generation windows.As we increase the temporal distance, our approach (blue line) yields increased dynamicity over the baseline (green line), at the expense of facial consistency. On the other hand, lip synchronization is noticeably improved when the temporal distance is smaller (≤12\\leq 12), but degrades sharply when it is increased further.",
                "position": 727
            },
            {
                "img": "https://arxiv.org/html/2510.23581/x7.png",
                "caption": "Figure 7:Narrative-driven generation.We edit a reference image using text prompts to create keyframes with different states, then position them as lookahead anchors during autoregressive generation to guide narrative transitions while maintaining audio synchronization. More results can be found in Fig.11.",
                "position": 799
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AMore qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23581/x8.png",
                "caption": "Figure 8:Additional qualitative results across diverse characters and scenarios.Our method consistently maintains character identity and visual quality across different subjects, backgrounds, and lighting conditions. Each row compares baseline methods with our Lookahead Anchoring approach, demonstrating superior identity preservation and natural motion generation.",
                "position": 1344
            },
            {
                "img": "https://arxiv.org/html/2510.23581/x9.png",
                "caption": "Figure 9:Additional qualitative comparisons on AVSpeech(Ephrat et al.,2018). We compare baseline models (top rows) with our Lookahead Anchoring approach (bottom rows). Our method maintains superior character consistency and facial detail preservation throughout extended generation sequences, while baselines exhibit progressive identity drift.",
                "position": 1347
            },
            {
                "img": "https://arxiv.org/html/2510.23581/x10.png",
                "caption": "Figure 10:Additional qualitative comparisons on AVSpeech(Ephrat et al.,2018). We compare baseline models (top rows) with our Lookahead Anchoring approach (bottom rows). Our method maintains superior character consistency and facial detail preservation throughout extended generation sequences, while baselines exhibit progressive identity drift.",
                "position": 1350
            },
            {
                "img": "https://arxiv.org/html/2510.23581/x11.png",
                "caption": "Figure 11:Narrative-driven long video generation application.Our approach seamlessly integrates with external text-to-image editing models to create dynamic storylines. By positioning edited reference images as distant lookahead anchors, we achieve smooth narrative transitions while maintaining audio synchronization and character identity throughout the sequence.",
                "position": 1353
            }
        ]
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    },
    {
        "header": "Appendix CUser Study Protocol",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23581/figures/screenshot_userstudy.png",
                "caption": "Figure 12:An example of the screen shown to participants in user study.",
                "position": 1491
            }
        ]
    },
    {
        "header": "Appendix DLimitations and Future Work",
        "images": []
    }
]