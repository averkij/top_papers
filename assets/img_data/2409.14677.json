[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.14677/x1.png",
                "caption": "",
                "position": 120
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.14677/x2.png",
                "caption": "Figure 2:Images generated from Stable Diffusion 2.1[41]. Text-to-image models, when prompted to generate reflections, struggle to generate consistent and controlled mirror reflections.",
                "position": 183
            }
        ]
    },
    {
        "header": "3SynMirror: A synthetic dataset of mirror reflections",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.14677/x3.png",
                "caption": "Figure 3:SynMirror: a)Dataset creation pipeline- We sample diverse 3D objects, mirrors as 2D planes and diverse floor textures to compose a scene in a blender environment. To enhance realism, we sample high-quality HDRI environment maps as backgrounds. We sample cameras from varied viewpoints, capturing the mirror and the object, and use Blender to render RGB images and dense 2D annotations. b)Samples from SynMirror- The generated scenes have complex geometry, textures, and high diversity. The renderings have accurate dense annotations for semantic, depth and normal maps at the original image resolution.",
                "position": 259
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.14677/x4.png",
                "caption": "Figure 4:Overview of the architecture.We encode the input imagex𝑥xitalic_xusing a pre-trained image encoder from Stable Diffusion to getzmsubscript𝑧𝑚z_{m}italic_z start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT. Subsequently, we resize the mirror maskm𝑚mitalic_mand depth mapd𝑑ditalic_dto obtain resized maskxmsubscript𝑥𝑚x_{m}italic_x start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPTand depthxdsubscript𝑥𝑑x_{d}italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT. Then, we concatenate noisy latentsztsubscript𝑧𝑡z_{t}italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT,zmsubscript𝑧𝑚z_{m}italic_z start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT,xmsubscript𝑥𝑚x_{m}italic_x start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPTandxdsubscript𝑥𝑑x_{d}italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPTwhich are fed into the Conditioning U-Netϵθ′subscriptsuperscriptitalic-ϵ′𝜃\\epsilon^{{}^{\\prime}}_{\\theta}italic_ϵ start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT ′ end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT. Each layer of the Generation U-Netϵθsubscriptitalic-ϵ𝜃\\epsilon_{\\theta}italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPTis conditioned via zero convolutions with corresponding layers ofϵθ′subscriptsuperscriptitalic-ϵ′𝜃\\epsilon^{{}^{\\prime}}_{\\theta}italic_ϵ start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT ′ end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT. Additionally,ϵθsubscriptitalic-ϵ𝜃\\epsilon_{\\theta}italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPTis conditioned by text embeddings. The pre-trained decoder then decodes the denoised latent to produce an image with mirror reflections. Detailed information can be found in Sec.4.2",
                "position": 310
            },
            {
                "img": "https://arxiv.org/html/2409.14677/x5.png",
                "caption": "Figure 5:Impact of depth conditioningon the reflection generation quality. Notice the irregular shape of the “baseball” and “chair” marked inred. In comparison, our method preserves the structure of the object (marked ingreen).",
                "position": 379
            }
        ]
    },
    {
        "header": "5Experiments & Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.14677/x6.png",
                "caption": "Figure 6:Additional Results.Our method effectively preserves the shape of objects, as demonstrated in (a) the lawn chair and (b) the swivel chair. Check in the zoomed-in regions. Additionally, our method accurately positions the objects within the mirror (c) and (d), corroborating the effectiveness of depth-conditioning in our method. Text-prompts used are described in AppendixE.2.",
                "position": 395
            },
            {
                "img": "https://arxiv.org/html/2409.14677/x7.png",
                "caption": "Figure 7:Reflection generation comparison with general inpainting methods.We compare our results with zero-shot baselines Stable Diffusion 1.5 Inpainting-ZS, PowerPaint-ZS and BrushNet-ZS. Further, we finetune BrushNet on SynMirror and refer to it as BrushNet-FT. The top four rows compare results on“unknown”categories, and the bottom two rows show results on“known”categories from MirrorBench. Zero-shot methods either fail to generate a reflection on the mirror or generate a reflection at an incorrect position. In comparison, BrushNet-FT generates plausible reflections, but with geometric inaccuracies. Our method improves on shape preservation of the object, floor texture and correct placement of the object in the mirror reflection.",
                "position": 402
            },
            {
                "img": "https://arxiv.org/html/2409.14677/x8.png",
                "caption": "Figure 8:Change of Viewpoints for mirror and object.Our method preserves the shape of the object from different viewpoints. This illustrates our method’s ability to utilize 3D cues and generate accurate reflection of the object.",
                "position": 450
            },
            {
                "img": "https://arxiv.org/html/2409.14677/x9.png",
                "caption": "Figure 9:Generalization on GSO[7].Our method generates accurate reflections for unseen real-world scanned objects. This substantiates the generalization capabilities of our method.",
                "position": 454
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix ADataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.14677/x10.png",
                "caption": "Figure 10:Samples from SynMirror.",
                "position": 701
            },
            {
                "img": "https://arxiv.org/html/2409.14677/x11.png",
                "caption": "Figure 11:Samples from SynMirror.",
                "position": 704
            },
            {
                "img": "https://arxiv.org/html/2409.14677/x12.png",
                "caption": "Figure 12:Samples from MirrorBench.The first two rows contain samples from“Unknown”categories and the bottom two rows contain samples from“Known”categories. Notice the challenging nature of MirrorBench. We provide more details in AppendixA.2",
                "position": 707
            }
        ]
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    },
    {
        "header": "Appendix CAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.14677/x13.png",
                "caption": "Figure 13:Performance on Real-world scenesWe show results on images from MSD[55]dataset (a) & (b) and examples from images captured using a smartphone device (c) & (d). AppendixC.2describes the experimental details and text prompts used for the inference. We observe that “BrushNet-FT” does not generate accurate reflections, whereas our method is able to generate plausible reflections on the mirror.",
                "position": 747
            },
            {
                "img": "https://arxiv.org/html/2409.14677/x14.png",
                "caption": "Figure 14:Qualitative Comparison with Commercial ProductsWe compare our results with Adobe Firefly. Our method is significantly better than the existing commercial product. This highlights the challenging nature of the task and the effectiveness of our proposed method in addressing it.",
                "position": 808
            },
            {
                "img": "https://arxiv.org/html/2409.14677/x15.png",
                "caption": "Figure 15:Qualitative Comparison on unseen 3D assets from GSO.We show results from (a) & (b) “3D Dollhouse Sofa”, (c) & (d) “3D Dollhouse Swing”, (e) & (f) “3D Dollhouse TablePurple”, (g) & (h) “Big Dot Aqua Pencil Case”, (i) & (j) “Digital Camo Double Decker Lunch Bag”, (k) & (l) “INTERNATIONAL PAPER Willamette 4 Brown Bag” , (m) & (n) “Room Essentials Mug White Yellow” and (o) & (p) “Toys R Us Treat Dispenser Smart Puzzle Foobler”. AppendixC.1describes how images are generated and text-prompts used for the inference. We observe that “BrushNet-FT” does not generate accurate reflections in (c),(d),(f),(g),(h) whereas our method is able to generate correct reflections on the mirror.",
                "position": 850
            },
            {
                "img": "https://arxiv.org/html/2409.14677/x16.png",
                "caption": "Figure 16:Choice of pre-trained monocular depth estimation method during inference.We observe negligible differences in the reflection generation across both choices, Marigold[15]and DepthAnythingV2[54], supporting the stability of our method regardless of the chosen pre-trained monocular depth estimation technique. We use “Marigold” in all our experiments.",
                "position": 898
            }
        ]
    },
    {
        "header": "Appendix DLimitations and Social Impact",
        "images": []
    },
    {
        "header": "Appendix EAdditional Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.14677/x17.png",
                "caption": "Figure 17:Qualitative Comparison.We observe that the state-of-the-art inpainting method “BrushNet-ZS” is not able to generate plausible reflections(2n⁢dsuperscript2𝑛𝑑2^{nd}2 start_POSTSUPERSCRIPT italic_n italic_d end_POSTSUPERSCRIPTcolumn). “BrushNet-FT” which is fine-tuned on SynMirror is able to generate plausible reflections,3r⁢dsuperscript3𝑟𝑑3^{rd}3 start_POSTSUPERSCRIPT italic_r italic_d end_POSTSUPERSCRIPTcolumn, but fails to accurately get the shape of the object. For example, the top surface of “dvd-player” in1s⁢tsuperscript1𝑠𝑡1^{st}1 start_POSTSUPERSCRIPT italic_s italic_t end_POSTSUPERSCRIPTrow is completely missing. The ”flashlight” reflection’s structure and appearance do not correspond with the object (2n⁢dsuperscript2𝑛𝑑2^{nd}2 start_POSTSUPERSCRIPT italic_n italic_d end_POSTSUPERSCRIPTrow). Compared to these baselines MirrorFusion generates plausible reflections. Still there is issue in the shape of the “flashlight” in2n⁢dsuperscript2𝑛𝑑2^{nd}2 start_POSTSUPERSCRIPT italic_n italic_d end_POSTSUPERSCRIPTrow. These issues are mitigated by  MirrorFusion**{}^{\\text{*}}start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT, which generates realistic, plausible and geometrically accurate reflections on the mirror.",
                "position": 1156
            },
            {
                "img": "https://arxiv.org/html/2409.14677/x18.png",
                "caption": "Figure 18:Qualitative Comparison.Similar to the observation in Fig.17, we observe that the state-of-the-art inpainting method “BrushNet-ZS” is not able to generate plausible reflections(2n⁢dsuperscript2𝑛𝑑2^{nd}2 start_POSTSUPERSCRIPT italic_n italic_d end_POSTSUPERSCRIPTcolumn). “BrushNet-FT” which is fine-tuned on SynMirror is able to generate plausible reflections,3r⁢dsuperscript3𝑟𝑑3^{rd}3 start_POSTSUPERSCRIPT italic_r italic_d end_POSTSUPERSCRIPTcolumnbut fails to get shape of the object in the reflection. For example, observe the “chair” in3r⁢dsuperscript3𝑟𝑑3^{rd}3 start_POSTSUPERSCRIPT italic_r italic_d end_POSTSUPERSCRIPTrow, the head of the chair is missing. The pose of the toy in2n⁢dsuperscript2𝑛𝑑2^{nd}2 start_POSTSUPERSCRIPT italic_n italic_d end_POSTSUPERSCRIPTrow does not correspond to that of the real object. Compared to this MirrorFusion and  MirrorFusion**{}^{\\text{*}}start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPTgenerates plausible reflections on the mirror.",
                "position": 1159
            },
            {
                "img": "https://arxiv.org/html/2409.14677/x19.png",
                "caption": "Figure 19:Additional results of images generated from Stable Diffusion 3[8].Text-to-image models struggle to produce consistent and controlled mirror reflections when prompted to generate them. We use the prefix“A perfect plane mirror reflection of”and suffix“in front of the mirror positioned at an angle with respect to the mirror.”along with the object description.",
                "position": 1162
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]