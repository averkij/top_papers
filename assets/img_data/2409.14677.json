[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.14677/x1.png",
                "caption": "",
                "position": 120
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.14677/x2.png",
                "caption": "Figure 2:Images generated from Stable Diffusion 2.1[41]. Text-to-image models, when prompted to generate reflections, struggle to generate consistent and controlled mirror reflections.",
                "position": 183
            }
        ]
    },
    {
        "header": "3SynMirror: A synthetic dataset of mirror reflections",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.14677/x3.png",
                "caption": "Figure 3:SynMirror: a)Dataset creation pipeline- We sample diverse 3D objects, mirrors as 2D planes and diverse floor textures to compose a scene in a blender environment. To enhance realism, we sample high-quality HDRI environment maps as backgrounds. We sample cameras from varied viewpoints, capturing the mirror and the object, and use Blender to render RGB images and dense 2D annotations. b)Samples from SynMirror- The generated scenes have complex geometry, textures, and high diversity. The renderings have accurate dense annotations for semantic, depth and normal maps at the original image resolution.",
                "position": 259
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.14677/x4.png",
                "caption": "Figure 4:Overview of the architecture.We encode the input imagexğ‘¥xitalic_xusing a pre-trained image encoder from Stable Diffusion to getzmsubscriptğ‘§ğ‘šz_{m}italic_z start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT. Subsequently, we resize the mirror maskmğ‘šmitalic_mand depth mapdğ‘‘ditalic_dto obtain resized maskxmsubscriptğ‘¥ğ‘šx_{m}italic_x start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPTand depthxdsubscriptğ‘¥ğ‘‘x_{d}italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT. Then, we concatenate noisy latentsztsubscriptğ‘§ğ‘¡z_{t}italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT,zmsubscriptğ‘§ğ‘šz_{m}italic_z start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT,xmsubscriptğ‘¥ğ‘šx_{m}italic_x start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPTandxdsubscriptğ‘¥ğ‘‘x_{d}italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPTwhich are fed into the Conditioning U-NetÏµÎ¸â€²subscriptsuperscriptitalic-Ïµâ€²ğœƒ\\epsilon^{{}^{\\prime}}_{\\theta}italic_Ïµ start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT â€² end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT. Each layer of the Generation U-NetÏµÎ¸subscriptitalic-Ïµğœƒ\\epsilon_{\\theta}italic_Ïµ start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPTis conditioned via zero convolutions with corresponding layers ofÏµÎ¸â€²subscriptsuperscriptitalic-Ïµâ€²ğœƒ\\epsilon^{{}^{\\prime}}_{\\theta}italic_Ïµ start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT â€² end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT. Additionally,ÏµÎ¸subscriptitalic-Ïµğœƒ\\epsilon_{\\theta}italic_Ïµ start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPTis conditioned by text embeddings. The pre-trained decoder then decodes the denoised latent to produce an image with mirror reflections. Detailed information can be found in Sec.4.2",
                "position": 310
            },
            {
                "img": "https://arxiv.org/html/2409.14677/x5.png",
                "caption": "Figure 5:Impact of depth conditioningon the reflection generation quality. Notice the irregular shape of the â€œbaseballâ€ and â€œchairâ€ marked inred. In comparison, our method preserves the structure of the object (marked ingreen).",
                "position": 379
            }
        ]
    },
    {
        "header": "5Experiments & Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.14677/x6.png",
                "caption": "Figure 6:Additional Results.Our method effectively preserves the shape of objects, as demonstrated in (a) the lawn chair and (b) the swivel chair. Check in the zoomed-in regions. Additionally, our method accurately positions the objects within the mirror (c) and (d), corroborating the effectiveness of depth-conditioning in our method. Text-prompts used are described in AppendixE.2.",
                "position": 395
            },
            {
                "img": "https://arxiv.org/html/2409.14677/x7.png",
                "caption": "Figure 7:Reflection generation comparison with general inpainting methods.We compare our results with zero-shot baselines Stable Diffusion 1.5 Inpainting-ZS, PowerPaint-ZS and BrushNet-ZS. Further, we finetune BrushNet onÂ SynMirror and refer to it as BrushNet-FT. The top four rows compare results onâ€œunknownâ€categories, and the bottom two rows show results onâ€œknownâ€categories fromÂ MirrorBench. Zero-shot methods either fail to generate a reflection on the mirror or generate a reflection at an incorrect position. In comparison, BrushNet-FT generates plausible reflections, but with geometric inaccuracies. Our method improves on shape preservation of the object, floor texture and correct placement of the object in the mirror reflection.",
                "position": 402
            },
            {
                "img": "https://arxiv.org/html/2409.14677/x8.png",
                "caption": "Figure 8:Change of Viewpoints for mirror and object.Our method preserves the shape of the object from different viewpoints. This illustrates our methodâ€™s ability to utilize 3D cues and generate accurate reflection of the object.",
                "position": 450
            },
            {
                "img": "https://arxiv.org/html/2409.14677/x9.png",
                "caption": "Figure 9:Generalization on GSO[7].Our method generates accurate reflections for unseen real-world scanned objects. This substantiates the generalization capabilities of our method.",
                "position": 454
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix ADataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.14677/x10.png",
                "caption": "Figure 10:Samples fromÂ SynMirror.",
                "position": 701
            },
            {
                "img": "https://arxiv.org/html/2409.14677/x11.png",
                "caption": "Figure 11:Samples fromÂ SynMirror.",
                "position": 704
            },
            {
                "img": "https://arxiv.org/html/2409.14677/x12.png",
                "caption": "Figure 12:Samples fromÂ MirrorBench.The first two rows contain samples fromâ€œUnknownâ€categories and the bottom two rows contain samples fromâ€œKnownâ€categories. Notice the challenging nature ofÂ MirrorBench. We provide more details in AppendixA.2",
                "position": 707
            }
        ]
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    },
    {
        "header": "Appendix CAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.14677/x13.png",
                "caption": "Figure 13:Performance on Real-world scenesWe show results on images from MSD[55]dataset (a)Â & (b) and examples from images captured using a smartphone device (c)Â & (d). AppendixC.2describes the experimental details and text prompts used for the inference. We observe that â€œBrushNet-FTâ€ does not generate accurate reflections, whereas our method is able to generate plausible reflections on the mirror.",
                "position": 747
            },
            {
                "img": "https://arxiv.org/html/2409.14677/x14.png",
                "caption": "Figure 14:Qualitative Comparison with Commercial ProductsWe compare our results with Adobe Firefly. Our method is significantly better than the existing commercial product. This highlights the challenging nature of the task and the effectiveness of our proposed method in addressing it.",
                "position": 808
            },
            {
                "img": "https://arxiv.org/html/2409.14677/x15.png",
                "caption": "Figure 15:Qualitative Comparison on unseen 3D assets from GSO.We show results from (a)Â & (b) â€œ3D Dollhouse Sofaâ€, (c)Â & (d) â€œ3D Dollhouse Swingâ€, (e)Â & (f) â€œ3D Dollhouse TablePurpleâ€, (g)Â & (h) â€œBig Dot Aqua Pencil Caseâ€, (i)Â & (j) â€œDigital Camo Double Decker Lunch Bagâ€, (k)Â & (l) â€œINTERNATIONAL PAPER Willamette 4 Brown Bagâ€ , (m)Â & (n) â€œRoom Essentials Mug White Yellowâ€ and (o)Â & (p) â€œToys R Us Treat Dispenser Smart Puzzle Fooblerâ€. AppendixC.1describes how images are generated and text-prompts used for the inference. We observe that â€œBrushNet-FTâ€ does not generate accurate reflections in (c),(d),(f),(g),(h) whereas our method is able to generate correct reflections on the mirror.",
                "position": 850
            },
            {
                "img": "https://arxiv.org/html/2409.14677/x16.png",
                "caption": "Figure 16:Choice of pre-trained monocular depth estimation method during inference.We observe negligible differences in the reflection generation across both choices, Marigold[15]and DepthAnythingV2[54], supporting the stability of our method regardless of the chosen pre-trained monocular depth estimation technique. We use â€œMarigoldâ€ in all our experiments.",
                "position": 898
            }
        ]
    },
    {
        "header": "Appendix DLimitations and Social Impact",
        "images": []
    },
    {
        "header": "Appendix EAdditional Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.14677/x17.png",
                "caption": "Figure 17:Qualitative Comparison.We observe that the state-of-the-art inpainting method â€œBrushNet-ZSâ€ is not able to generate plausible reflections(2nâ¢dsuperscript2ğ‘›ğ‘‘2^{nd}2 start_POSTSUPERSCRIPT italic_n italic_d end_POSTSUPERSCRIPTcolumn). â€œBrushNet-FTâ€ which is fine-tuned onÂ SynMirror is able to generate plausible reflections,3râ¢dsuperscript3ğ‘Ÿğ‘‘3^{rd}3 start_POSTSUPERSCRIPT italic_r italic_d end_POSTSUPERSCRIPTcolumn, but fails to accurately get the shape of the object. For example, the top surface of â€œdvd-playerâ€ in1sâ¢tsuperscript1ğ‘ ğ‘¡1^{st}1 start_POSTSUPERSCRIPT italic_s italic_t end_POSTSUPERSCRIPTrow is completely missing. The â€flashlightâ€ reflectionâ€™s structure and appearance do not correspond with the object (2nâ¢dsuperscript2ğ‘›ğ‘‘2^{nd}2 start_POSTSUPERSCRIPT italic_n italic_d end_POSTSUPERSCRIPTrow). Compared to these baselinesÂ MirrorFusion generates plausible reflections. Still there is issue in the shape of the â€œflashlightâ€ in2nâ¢dsuperscript2ğ‘›ğ‘‘2^{nd}2 start_POSTSUPERSCRIPT italic_n italic_d end_POSTSUPERSCRIPTrow. These issues are mitigated by Â MirrorFusion**{}^{\\text{*}}start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT, which generates realistic, plausible and geometrically accurate reflections on the mirror.",
                "position": 1156
            },
            {
                "img": "https://arxiv.org/html/2409.14677/x18.png",
                "caption": "Figure 18:Qualitative Comparison.Similar to the observation in Fig.17, we observe that the state-of-the-art inpainting method â€œBrushNet-ZSâ€ is not able to generate plausible reflections(2nâ¢dsuperscript2ğ‘›ğ‘‘2^{nd}2 start_POSTSUPERSCRIPT italic_n italic_d end_POSTSUPERSCRIPTcolumn). â€œBrushNet-FTâ€ which is fine-tuned onÂ SynMirror is able to generate plausible reflections,3râ¢dsuperscript3ğ‘Ÿğ‘‘3^{rd}3 start_POSTSUPERSCRIPT italic_r italic_d end_POSTSUPERSCRIPTcolumnbut fails to get shape of the object in the reflection. For example, observe the â€œchairâ€ in3râ¢dsuperscript3ğ‘Ÿğ‘‘3^{rd}3 start_POSTSUPERSCRIPT italic_r italic_d end_POSTSUPERSCRIPTrow, the head of the chair is missing. The pose of the toy in2nâ¢dsuperscript2ğ‘›ğ‘‘2^{nd}2 start_POSTSUPERSCRIPT italic_n italic_d end_POSTSUPERSCRIPTrow does not correspond to that of the real object. Compared to thisÂ MirrorFusion and Â MirrorFusion**{}^{\\text{*}}start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPTgenerates plausible reflections on the mirror.",
                "position": 1159
            },
            {
                "img": "https://arxiv.org/html/2409.14677/x19.png",
                "caption": "Figure 19:Additional results of images generated from Stable Diffusion 3[8].Text-to-image models struggle to produce consistent and controlled mirror reflections when prompted to generate them. We use the prefixâ€œA perfect plane mirror reflection ofâ€and suffixâ€œin front of the mirror positioned at an angle with respect to the mirror.â€along with the object description.",
                "position": 1162
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]