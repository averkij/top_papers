[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07064/x1.png",
                "caption": "(a)Scaling of Training Computation (1.7B)",
                "position": 170
            },
            {
                "img": "https://arxiv.org/html/2410.07064/x1.png",
                "caption": "(a)Scaling of Training Computation (1.7B)",
                "position": 173
            },
            {
                "img": "https://arxiv.org/html/2410.07064/x2.png",
                "caption": "(b)Scaling of Model Size",
                "position": 181
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07064/x3.png",
                "caption": "Figure 2:An illustration of Theorem2.1.Left:𝝀t+1∗∈ℝNsubscriptsuperscript𝝀𝑡1superscriptℝ𝑁{\\bm{\\lambda}}^{*}_{t+1}\\in\\mathbb{R}^{N}bold_italic_λ start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPTdefines a “target vector” aligning with the optimization direction towards optimal data selection, as in Eq. (5).Right: data quality scores are positively correlated with how close the gradient direction of each instance is to the target direction, calculated as the dot-product between𝝀t+1∗subscriptsuperscript𝝀𝑡1{\\bm{\\lambda}}^{*}_{t+1}bold_italic_λ start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPTand∇li,t=∇l⁢(xi,𝜽t∗)∇subscript𝑙𝑖𝑡∇𝑙subscript𝑥𝑖subscriptsuperscript𝜽𝑡\\nabla l_{i,t}=\\nabla l(x_{i},{\\bm{\\theta}}^{*}_{t})∇ italic_l start_POSTSUBSCRIPT italic_i , italic_t end_POSTSUBSCRIPT = ∇ italic_l ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_italic_θ start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )fori=n,m,k𝑖𝑛𝑚𝑘i=n,m,kitalic_i = italic_n , italic_m , italic_k, as in Eq. (6).",
                "position": 351
            },
            {
                "img": "https://arxiv.org/html/2410.07064/x4.png",
                "caption": "Figure 3:ThePDSframework. We compute data quality scores𝜸∗superscript𝜸{\\bm{\\gamma}}^{*}bold_italic_γ start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPTon a proxy dataset𝒟prxsuperscript𝒟prx\\mathcal{D}^{\\mathrm{prx}}caligraphic_D start_POSTSUPERSCRIPT roman_prx end_POSTSUPERSCRIPTusing Algorithm1, which is derived from the Pontryagin’s Maximum Principle[63](Section2.3.1). After that, the data scorer learns to predict quality scores from instances, which then infers scores for a large corpus𝒟𝒟\\mathcal{D}caligraphic_D(Section2.3.2). Finally, a high-quality corpus𝒟′superscript𝒟′\\mathcal{D}^{\\prime}caligraphic_D start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPTis selected based on the inferred scores to pre-train an LM (Section2.3.3).",
                "position": 393
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07064/x5.png",
                "caption": "Figure 4:Test losses on the DCLM corpus[48]for 160M, 470M, 1B and 1.7B LMs.",
                "position": 874
            },
            {
                "img": "https://arxiv.org/html/2410.07064/x6.png",
                "caption": "Figure 5:Test losses on DCLM corpus[48]in the data-constrained setting. We select data withPDSfor different selection ratiosr𝑟ritalic_rand train the model for multiple epochs to reach the same token number budgets.",
                "position": 926
            },
            {
                "img": "https://arxiv.org/html/2410.07064/x6.png",
                "caption": "Figure 5:Test losses on DCLM corpus[48]in the data-constrained setting. We select data withPDSfor different selection ratiosr𝑟ritalic_rand train the model for multiple epochs to reach the same token number budgets.",
                "position": 929
            },
            {
                "img": "https://arxiv.org/html/2410.07064/x7.png",
                "caption": "Figure 6:Comparison between exact and efficient implementation to solve the data quality scores in a simulated setting. The effectiveness is measured byJ⁢(𝜽t)𝐽subscript𝜽𝑡J({\\bm{\\theta}}_{t})italic_J ( bold_italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ). The efficient implementation saves computation and preserves most of the performance of the exact solution.",
                "position": 934
            },
            {
                "img": "https://arxiv.org/html/2410.07064/x8.png",
                "caption": "Table 5:Data selection based on different kinds of learning information. We report the LM’s zero-shot accuracy on the OLMo evaluation tasks (Acc.) and the Spearman Correlation of the data scorer (Corr.).",
                "position": 1007
            },
            {
                "img": "https://arxiv.org/html/2410.07064/x8.png",
                "caption": "Figure 7:LM performance on the OLMo evaluation tasks (Average Accuracy) and data scorer performance (Spearman Correlation) when different proxy model and proxy data sizes are adopted.",
                "position": 1056
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AConnection Between AUC and Scaling Law Constants",
        "images": []
    },
    {
        "header": "Appendix BProof of Theorem2.1",
        "images": []
    },
    {
        "header": "Appendix CDerivation for Adam",
        "images": []
    },
    {
        "header": "Appendix DAlgorithm1as Proximal Gradient Decent",
        "images": []
    },
    {
        "header": "Appendix EImplementation Details",
        "images": []
    },
    {
        "header": "Appendix FComplexity Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07064/x9.png",
                "caption": "(a)Model Size = 160M",
                "position": 3428
            },
            {
                "img": "https://arxiv.org/html/2410.07064/x9.png",
                "caption": "(a)Model Size = 160M",
                "position": 3431
            },
            {
                "img": "https://arxiv.org/html/2410.07064/x10.png",
                "caption": "(b)Model Size = 470M",
                "position": 3439
            },
            {
                "img": "https://arxiv.org/html/2410.07064/x11.png",
                "caption": "(c)Mode Size = 1B",
                "position": 3447
            },
            {
                "img": "https://arxiv.org/html/2410.07064/x12.png",
                "caption": "Figure 9:Effect of the data selection ratioα𝛼\\alphaitalic_α. We report the average accuracy on the OLMo evaluation datasets forα∈[0.3,0.4,0.5,0.6,1.0]𝛼0.30.40.50.61.0\\alpha\\in\\left[0.3,0.4,0.5,0.6,1.0\\right]italic_α ∈ [ 0.3 , 0.4 , 0.5 , 0.6 , 1.0 ].",
                "position": 3584
            },
            {
                "img": "https://arxiv.org/html/2410.07064/x12.png",
                "caption": "Figure 9:Effect of the data selection ratioα𝛼\\alphaitalic_α. We report the average accuracy on the OLMo evaluation datasets forα∈[0.3,0.4,0.5,0.6,1.0]𝛼0.30.40.50.61.0\\alpha\\in\\left[0.3,0.4,0.5,0.6,1.0\\right]italic_α ∈ [ 0.3 , 0.4 , 0.5 , 0.6 , 1.0 ].",
                "position": 3587
            },
            {
                "img": "https://arxiv.org/html/2410.07064/x13.png",
                "caption": "Figure 10:Effect of the strengthτ𝜏\\tauitalic_τin Gumble-Top-K𝐾Kitalic_K. We report the average accuracy on the OLMo evaluation datasets forτ∈[0.0,0.1,0.3,0.5]𝜏0.00.10.30.5\\tau\\in\\left[0.0,0.1,0.3,0.5\\right]italic_τ ∈ [ 0.0 , 0.1 , 0.3 , 0.5 ].",
                "position": 3592
            }
        ]
    },
    {
        "header": "Appendix GMore Results",
        "images": []
    }
]