[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01017/x1.png",
                "caption": "Figure 1:We compare the scaling behavior of visual SSL and CLIP on 16 VQA tasks from the Cambrian-1 suite under different data and model size regimes. Prior visual SSL methods achieved strong performance on classic vision tasks, but have underperformed as encoders for multimodal instruction-tuned VQA tasks. Our results show that with appropriate scaling of models and data, visual SSL can match the performance of language-supervised models across all evaluated domains—even OCR & Chart.",
                "position": 157
            },
            {
                "img": "https://arxiv.org/html/2504.01017/x2.png",
                "caption": "Figure 2:Visual SSL 2.0 changes.In this work, we adopt three improvements to the visual SSL pipeline: 1) Training on billion-scale web data, curated through the MetaCLIP pipeline, to move beyond “conventional” datasets; 2) Scaling model architecture from sub-billion parameter models to models exceeding 1 billion parameters; and 3) Incorporating VQA as a complementary evaluation protocol to comprehensively assess visual features. These changes enable us to study visual SSL at a larger scale and observe scaling trends previously unobserved in smaller-scale experiments.",
                "position": 161
            }
        ]
    },
    {
        "header": "2From Visual SSL 1.0 to 2.0",
        "images": []
    },
    {
        "header": "3Scaling Visual SSL",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01017/x3.png",
                "caption": "Figure 3:Scaling behavior of Web-DINO and CLIP ViTs trained on MC-2B.The x-axis shows model sizes from 1B to 7B parameters on a log scale. We observe novel “scaling behavior” with Web-DINO models across all categories, with particularly pronounced improvements in the OCR & Chart and Vision-Centric domains as model size increases. In contrast, CLIP models demonstrate limited scaling benefits, with performance saturating at moderate model sizes. The two model families exhibit complementary strengths: CLIP models excel at OCR & Chart VQA, and Web-DINO models are superior at Vision-Centric VQA, while remaining competitive in all other categories.",
                "position": 300
            },
            {
                "img": "https://arxiv.org/html/2504.01017/x4.png",
                "caption": "Figure 4:Scaling up examples seen when training Web-DINO-7B.Performance across different VQA categories as training data increases from 1B to 8B images. While General and Vision-Centric tasks show diminishing returns after 2B images, OCR & Chart tasks demonstrate continued improvement, contributing to steady gains in average performance. Further, Web-DINO consistently outperforms same-size (ViT-7B) CLIP models with different training samples seen. The x-axis plots training data size on a log-scale.",
                "position": 333
            }
        ]
    },
    {
        "header": "4Scaling Analysis and Findings",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01017/x5.png",
                "caption": "Figure 5:Web-MAE trained on MC-2B.Web-MAE also exhibits consistent scaling behavior as model size increases. Notably, Web-MAE demonstrates better performance in OCR & Chart tasks, achieving higher accuracy than Web-DINO across all model sizes.",
                "position": 357
            },
            {
                "img": "https://arxiv.org/html/2504.01017/x6.png",
                "caption": "Figure 6:Comparison of ImageNet-1k and MC-2B Pretraining.Increasing the diversity and scale of pretraining data improves model performance on VQA accuracy and ImageNet linear probing. Unlike MC-2B pretraining, training on ImageNet does not exhibit a clear scaling trend.",
                "position": 379
            },
            {
                "img": "https://arxiv.org/html/2504.01017/x7.png",
                "caption": "Figure 7:Performance of Web-DINO models on classic vision tasks.All models achieve strong performance across ImageNet-1k classification, ADE20K segmentation, and NYU Depth estimation, and all tasks experience moderate improvements from increasing model size from 1B to 7B parameters. Web-DINO outperforms MetaCLIP (HF) and is competitive with DINOv2 (HF). (HF) denotes the largest official Hugging Face released version.",
                "position": 392
            },
            {
                "img": "https://arxiv.org/html/2504.01017/x8.png",
                "caption": "Figure 8:Examples of filtered MC-2B images.The Light filter (Middle) identifies images containing text, retaining 50.3% of the images. The Heavy filter (Right) identifies images explicitly containing charts and documents, retaining only 1.3% of MC-2B.",
                "position": 402
            },
            {
                "img": "https://arxiv.org/html/2504.01017/x9.png",
                "caption": "Figure 9:Alignment score between Web-DINO and LLMs.Moving from DINOv2 to Web-DINO improves the alignment between the image and the corresponding text representations obtained by LLMs. Increasing model size from 1B to 7B parameters shows gradual improvement, while training on larger data quantities (4B/8B samples) yields the most significant alignment gains.",
                "position": 583
            }
        ]
    },
    {
        "header": "5The Web-SSL Model Family",
        "images": []
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "8Discussion",
        "images": []
    },
    {
        "header": "9Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BFull Results",
        "images": []
    },
    {
        "header": "Appendix CHigh Resolution Adaption of Web-SSL",
        "images": []
    },
    {
        "header": "Appendix DEvaluation",
        "images": []
    },
    {
        "header": "Appendix EPretraining Dataset Cards",
        "images": []
    }
]