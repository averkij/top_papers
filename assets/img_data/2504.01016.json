[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01016/x1.png",
                "caption": "",
                "position": 76
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01016/x2.png",
                "caption": "Figure 2:Diffusion-based depth estimation methods,e.g., DepthCrafter[31]and DAV[80], suffer from significant metric errors in distant regions due to the compression of unbounded depth values into the bounded input range of VAEs.",
                "position": 114
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01016/x3.png",
                "caption": "Figure 3:Architecture of our point map VAE.The point map VAE encodes and decodes point maps with unbounded values, alleviating the inaccurate prediction in distant regions.\nWe adopt a dual-encoder design: the native encoder‚Ñ∞SVDsubscript‚Ñ∞SVD\\mathcal{E}_{\\text{SVD}}caligraphic_E start_POSTSUBSCRIPT SVD end_POSTSUBSCRIPTinherited from SVD captures normalized disparity maps, while a residual encoder‚Ñ∞œµsubscript‚Ñ∞italic-œµ\\mathcal{E}_{\\epsilon}caligraphic_E start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPTembeds remaining information as an offset.\nIt preserves the original latent space by regulating the latents via the original decoderùíüSVDsubscriptùíüSVD\\mathcal{D}_{\\text{SVD}}caligraphic_D start_POSTSUBSCRIPT SVD end_POSTSUBSCRIPT, enabling the utilization of pretrained diffusion priors.\nA point map decoderùíüpmapsubscriptùíüpmap\\mathcal{D}_{\\text{pmap}}caligraphic_D start_POSTSUBSCRIPT pmap end_POSTSUBSCRIPTrecovers the final point maps from the latent codes.",
                "position": 149
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01016/x4.png",
                "caption": "Figure 4:Diffusion UNet.We jointly condition the diffusion model on video latents and per-frame geometry priors from an image MGE model‚Ñ≥imgsubscript‚Ñ≥img\\mathcal{M}_{\\text{img}}caligraphic_M start_POSTSUBSCRIPT img end_POSTSUBSCRIPT.\nThe geometry is encoded into latent space via our point map VAE, while the video latents are obtained from the native VAE.",
                "position": 296
            },
            {
                "img": "https://arxiv.org/html/2504.01016/x5.png",
                "caption": "Figure 5:Qualitative comparison of point map estimation.Disparity maps are derived from estimated point maps viaEq.1.\nThe green boxes highlight temporal profiles of the disparity maps, sliced along the time axis at the green lines.\nZoom in for better visualization.",
                "position": 694
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01016/x6.png",
                "caption": "Figure 6:Qualitative comparison of depth map estimation.We transform point maps and disparity maps into metric depth maps for better visualization of distant regions. Zoom in for better visualization.",
                "position": 896
            },
            {
                "img": "https://arxiv.org/html/2504.01016/x7.png",
                "caption": "Figure 7:Comparison on disparity (top) and depth (bottom) quality between our full model and the w/o point map VAE variant.",
                "position": 910
            },
            {
                "img": "https://arxiv.org/html/2504.01016/x8.png",
                "caption": "Figure 8:Effectiveness of latent alignment.",
                "position": 1226
            },
            {
                "img": "https://arxiv.org/html/2504.01016/x9.png",
                "caption": "Figure 9:Application of depth-conditioned video generation.The prompt is ‚Äúa car is drifting on roads, snowy day, artstation‚Äù.",
                "position": 1252
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "1Datasets",
        "images": []
    },
    {
        "header": "2Loss Functions of VAE and UNet",
        "images": []
    },
    {
        "header": "3More Implementation Details",
        "images": []
    },
    {
        "header": "4Camera Pose Estimation",
        "images": []
    },
    {
        "header": "5Limitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01016/x10.png",
                "caption": "Figure 1:Visual results on Sora-generated videos.The rows from left to right are the input videos, the disparity maps and the point cloud of the first frame.",
                "position": 2913
            },
            {
                "img": "https://arxiv.org/html/2504.01016/x11.png",
                "caption": "Figure 2:Visual comparison with monocular geometry estimation methods.All point maps are converted to disparity maps for better visualization the sharpness of depth prediction.",
                "position": 2916
            },
            {
                "img": "https://arxiv.org/html/2504.01016/x12.png",
                "caption": "Figure 3:Visual results on DL3DV[45]with camera poses estimated from the output point maps.We concatenate 8 aligned point maps from the original point map sequence for visualization.",
                "position": 2919
            },
            {
                "img": "https://arxiv.org/html/2504.01016/x13.png",
                "caption": "Figure 4:Visual results on DAVIS[54]with camera poses estimated from the output point maps.We concatenate 8 aligned point maps from the original point map sequence for visualization.",
                "position": 2922
            }
        ]
    },
    {
        "header": "6More results",
        "images": []
    }
]