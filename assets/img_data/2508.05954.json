[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05954/x1.png",
                "caption": "Figure 1:Comparison of different approaches in using LLM for image generation.\n(a) Single architecture handling both text and image tokens into the LLM/MLLM.\n(b) Bridging LLM and Diffusion model with a 1D sequence (image tokens or text tokens).\n(c)Bifrost-1(ours), which bridges MLLM with diffusion models with 2D image tokens aligned with MLLM embeddings.",
                "position": 137
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Bifrost-1",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05954/x2.png",
                "caption": "Figure 2:Overview ofBifrost-1.Bifrost-1equips the backbone MLLM with a visual generation branch, which is a trainable copy of a pretrained MLLM parameters (i.e., QKV, MLP, normalization layers) and a newly added vision head (i.e., a linear layer).\nThe visual generation branch outputs patch-level CLIP latents, which are then downsampled and reshaped into 2D (HxW), provided to latent ControlNet, and finally guiding image generation of a pretrained diffusion model.\nDuring training, a portion of the image patches is randomly replaced with learnable mask tokens<M>.\nDuring inference, we start with fully masked image tokens and autoregressively predict them.",
                "position": 280
            },
            {
                "img": "https://arxiv.org/html/2508.05954/x3.png",
                "caption": "Figure 3:Attention mask. X-axis is input and Y-axis is output.",
                "position": 321
            }
        ]
    },
    {
        "header": "4Experiment Setup",
        "images": []
    },
    {
        "header": "5Results and Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05954/x4.png",
                "caption": "Figure 4:Image reconstruction scores with different numbers of 2D CLIP latent tokens used withinBifrost-1on ImageNet for around one training epoch (âˆ¼\\sim26K steps).\nResults indicate that using more tokens achieves better data efficiency.",
                "position": 585
            },
            {
                "img": "https://arxiv.org/html/2508.05954/x5.png",
                "caption": "Figure 5:Visual samples for image reconstruction with different numbers of patch-level CLIP tokens generated from MLLM. The Latent ControlNet models with varying numbers of tokens are trained for only 1 epoch on the ImageNet training split.",
                "position": 590
            },
            {
                "img": "https://arxiv.org/html/2508.05954/x6.png",
                "caption": "Figure 6:Image reconstruction results. Latent ControlNet inBifrost-1is only trained on ImageNet training split for 3 epochs.",
                "position": 616
            },
            {
                "img": "https://arxiv.org/html/2508.05954/x7.png",
                "caption": "Figure 7:Qualitative comparision with state-of-the-art methods for image generation on MJHQ30kli2024playgroundbenchmark.",
                "position": 1065
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AFormal Description of theBifrost-1MLLM Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05954/x8.png",
                "caption": "Figure 8:Visualization examples from MJHQ30k dataset.",
                "position": 1252
            }
        ]
    },
    {
        "header": "Appendix BBroader Impacts",
        "images": []
    },
    {
        "header": "Appendix CSafeguards",
        "images": []
    },
    {
        "header": "Appendix DLimitations",
        "images": []
    },
    {
        "header": "Appendix ELicense",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]