[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02760/figure/anydepth_logo.png",
                "caption": "",
                "position": 92
            },
            {
                "img": "https://arxiv.org/html/2601.02760/x1.png",
                "caption": "Figure 1:We presentAnyDepth, a simple and efficient training framework for zero-shot monocular depth estimation, which achieves impressive performance across a variety of indoor and outdoor scenes.",
                "position": 119
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02760/x2.png",
                "caption": "(a)Model Comparison",
                "position": 125
            },
            {
                "img": "https://arxiv.org/html/2601.02760/x2.png",
                "caption": "(a)Model Comparison",
                "position": 128
            },
            {
                "img": "https://arxiv.org/html/2601.02760/x3.png",
                "caption": "(b)FLOPs Comparison",
                "position": 133
            },
            {
                "img": "https://arxiv.org/html/2601.02760/x4.png",
                "caption": "Figure 3:Comparison of inference time between AnyDepth and DPT at different input resolutions. Our method consistently achieves lower latency, especially at higher resolutions.",
                "position": 143
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3The Proposed Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02760/x5.png",
                "caption": "Figure 4:AnyDepth architecture overview.The input image is encoded into tokens by a frozen DINOv3 backbone network, then decoded by our lightweight SDT decoder. Tokens undergo only a single projection and weighted fusion. The Spatial Detail Enhancer (SDE) module ensures finer-grained predictions. The feature map is upsampled by an efficient and learnable upsampler dysample, and the depth is finally output by the head.",
                "position": 203
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02760/x6.png",
                "caption": "(a)Total Score",
                "position": 336
            },
            {
                "img": "https://arxiv.org/html/2601.02760/x6.png",
                "caption": "(a)Total Score",
                "position": 339
            },
            {
                "img": "https://arxiv.org/html/2601.02760/x7.png",
                "caption": "(b)Depth Distribution Score",
                "position": 344
            },
            {
                "img": "https://arxiv.org/html/2601.02760/x8.png",
                "caption": "(c)Gradient Continuity Score",
                "position": 349
            },
            {
                "img": "https://arxiv.org/html/2601.02760/x9.png",
                "caption": "Figure 6:Qualitative results of zero-shot monocular depth estimation usingAnyDepthof ViT-B and comparison with DPT-B.",
                "position": 815
            },
            {
                "img": "https://arxiv.org/html/2601.02760/x10.png",
                "caption": "Figure 7:Hardware and Evaluation Pipeline for Real-World Experiments",
                "position": 983
            }
        ]
    },
    {
        "header": "5Limitations and Future Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02760/x11.png",
                "caption": "Figure 8:RGB images and GT of each dataset, showing that the depth value distribution of some samples is not uniform.",
                "position": 1259
            },
            {
                "img": "https://arxiv.org/html/2601.02760/x12.png",
                "caption": "Figure 9:Examples of RGB, gradient, and GT depth from five datasets.\nThe dotted box highlights the noisy area.",
                "position": 1262
            },
            {
                "img": "https://arxiv.org/html/2601.02760/x13.png",
                "caption": "Figure 10:Qualitative results of zero-shot monocular depth estimation with different decoders (DPT, Dual-DPT, and SDT) using the same encoder.",
                "position": 1266
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]