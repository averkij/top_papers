[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03164/x1.png",
                "caption": "Figure 1:Comparison of the original caption and ourChartCapcaption. The original caption includes extraneous information (inred), such as additional contextual details (e.g., missing error bars) and references to parameters (EpE_{p},EsE_{s},hminh_{\\text{min}}), which cannot be inferred from the chart image. In contrast,ChartCapcaption follows the line chart schema, relying on the information visible in the image. It includes a structural description (ingreen) and key insights (inblue). The chart is sourced from[6], collected by[25], and included inChartCap.",
                "position": 110
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03164/x2.png",
                "caption": "Figure 2:An example of the four-stage pipeline for ourChartCap: (a) filtering non-chart images, (b) classifying the chart type and extracting titles, (c) retrieving structural components and key insights, and (d) transforming the accumulated information into a coherent, sentence-level caption.",
                "position": 368
            }
        ]
    },
    {
        "header": "3TheChartCapDataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03164/x3.png",
                "caption": "Figure 3:An illustration of the cycle consistency-based human verification forChartCap. The original chart image (left) is compared with a reconstructed one (right) using a Python code from the caption (bottom). This process enables efficient human verification by assessing the accuracy and informativeness of the generated captions through visual consistency.",
                "position": 440
            },
            {
                "img": "https://arxiv.org/html/2508.03164/x4.png",
                "caption": "Figure 4:Results of the head-to-head human evaluation comparingChartCapwith ChartSumm[50].",
                "position": 563
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03164/x5.png",
                "caption": "Figure 5:Results of human evaluation results comparing Phi3.5-Vision-4BChartCap{}_{\\text{{ChartCap}}}against Claude 3.5 Sonnet (top) and Phi3.5-Vision-4B (bottom) on theChartCaptest set.",
                "position": 744
            },
            {
                "img": "https://arxiv.org/html/2508.03164/x6.png",
                "caption": "Figure 6:Human evaluation results comparing Phi3.5-Vision-4BChartCap{}_{\\text{{ChartCap}}}against ground-truth captions (top) and Claude 3.5 Sonnet (bottom) on the VisText test set.",
                "position": 834
            },
            {
                "img": "https://arxiv.org/html/2508.03164/x7.png",
                "caption": "Figure 7:Qualitative examples from VisText, comparing (a) the ground-truth chart image with captions and their reconstructed charts from the captions of (b) human-authored ground-truth, (c) Phi3.5-Vision-4BChartCap{}_{\\text{{ChartCap}}}, and (d) Claude 3.5 Sonnet.",
                "position": 837
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AType-specific Caption Schema",
        "images": []
    },
    {
        "header": "Appendix BPrompt Demonstrations",
        "images": []
    },
    {
        "header": "Appendix CTask Allocation Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03164/x8.png",
                "caption": "Figure 8:Accuracy of GPT-4o and Claude 3.5 Sonnet on coarse-grained tasks and fine-grained tasks.",
                "position": 2789
            },
            {
                "img": "https://arxiv.org/html/2508.03164/x9.png",
                "caption": "Figure 9:Examples of the four main scenarios that arise during the cycle consistency-based human verification process. In Scenario A, the caption incorrectly describes aline chart with a shaded areaas anarea chart. Consequently, the generated code reflects this incorrect information, leading to the reconstruction of a chart that does not match the original. In Scenario B, the caption oversimplifies the data trend by merely describing it asdecreasing. As a result, the reconstructed chart follows a simple downward trend, failing to capture the original complexity. In Scenario C, an error occurs during the code generation process, leading to the creation of an incorrect chart. Such coding errors were primarily observed when using NumPyâ€™s nonlinear functions. Scenario D shows that both the caption and the generated code must be accurate for the reconstructed chart to correctly match the original chart, demonstrating the necessity of precise and informative captions.",
                "position": 2802
            }
        ]
    },
    {
        "header": "Appendix DValidation of Cycle Consistency-based Human Verification Process",
        "images": []
    },
    {
        "header": "Appendix EValidation of VCS with Human Evaluation",
        "images": []
    },
    {
        "header": "Appendix FLLM Fidelity in Caption-to-Code Translation",
        "images": []
    },
    {
        "header": "Appendix GSensitivity of VCS to Structural Errors",
        "images": []
    },
    {
        "header": "Appendix HAdditional Baselines",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03164/x10.png",
                "caption": "Figure 10:Human evaluation results comparing Phi3.5-Vision-4BChartCap{}_{\\text{{ChartCap}}}and Phi3.5-Vision-4BChartSumm{}_{\\text{ChartSumm}}on the VisText test set.",
                "position": 3029
            }
        ]
    },
    {
        "header": "Appendix ITraining Hyperparameters",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03164/x11.png",
                "caption": "Figure 11:User interface for human evaluation comparing captions from different models on informativeness, accuracy, and fewer hallucinations.",
                "position": 3201
            },
            {
                "img": "https://arxiv.org/html/2508.03164/x12.png",
                "caption": "Figure 12:User interface for human evaluation comparing datasets (ChartCapvs. ChartSumm) on informativeness, accuracy, fewer hallucinations, and overall preference.",
                "position": 3204
            }
        ]
    },
    {
        "header": "Appendix JDetails of Human Evaluation",
        "images": []
    }
]