[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02792/x1.png",
                "caption": "Figure 1:Unified World Models integrates action and video diffusion in a unified transformer architecture controlled by modality-specific diffusion timesteps. The model can flexibly be trained on large robotics datasets and then flexibly perform a variety of different inferences at test time. Doing so naturally enables improved robustness and generalization for imitation learning.",
                "position": 144
            }
        ]
    },
    {
        "header": "IIntroduction",
        "images": []
    },
    {
        "header": "IIPreliminaries",
        "images": []
    },
    {
        "header": "IIIMethod",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02792/x2.png",
                "caption": "Figure 2:Unified World Model training and policy inference pipeline. The left panel shows UWM pretraining on robot trajectories with actions and co-training on action-free videos by masking out actions using diffusion timesteps. The right panel illustrates marginal and conditional inference modes, corresponding to the policy and the inverse dynamics.",
                "position": 298
            },
            {
                "img": "https://arxiv.org/html/2504.02792/x3.png",
                "caption": "Figure 3:A single Unified World Model (UWM) block consists of a transformer block with observations and diffusion timesteps conditioning via adaptive layer norm. In addition, we add randomly initialized register tokens which allows for better multi-modal feature sharing.",
                "position": 494
            }
        ]
    },
    {
        "header": "IVExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02792/x4.png",
                "caption": "Figure 4:Visualization of datasets used for pretraining and finetuning. The pretraining/cotraining dataset consists of diverse tasks performed by Franka robots in various environments to ensure broad generalization capabilities. The finetuning datasets include five tasks, each designed to evaluate task-specific performance under controlled conditions.",
                "position": 527
            },
            {
                "img": "https://arxiv.org/html/2504.02792/x5.png",
                "caption": "Figure 5:Real-world setup for real robot tasks: Stack-Bowls, Block-Cabinet, Paper-Towel, Hang-Towel, and Rice-Cooker. The first row illustrates the initial configurations for each task, while the second row demonstrates successful task completions. The third row highlights the out-of-distribution (OOD) configurations designed to evaluate the robustness of each method.",
                "position": 571
            },
            {
                "img": "https://arxiv.org/html/2504.02792/x6.png",
                "caption": "Figure 6:Average success rates across all real robot tasks and in-distribution and out-of-distribution settings. UWM exhibits strong performance and can further improve by co-training from action-free videos.",
                "position": 617
            },
            {
                "img": "https://arxiv.org/html/2504.02792/x7.png",
                "caption": "Figure 7:Visualization of the LIBERO datasets. The pretraining dataset (LIBERO-90) consists of 90 tasks sampled across the kitchen, living room, and study scenes. The finetuning datasets (LIBERO-10) consist of 10 tasks used for evaluation. Tasks from LIBERO-10 are fine-tuned and evaluated under distribution shifts, with unseen initializations and modified configurations.",
                "position": 797
            },
            {
                "img": "https://arxiv.org/html/2504.02792/x8.png",
                "caption": "Figure 8:Visualization of the forward dynamics model predictions. The model accurately predicts the robot and object poses conditioned on the initial observation and actions.",
                "position": 823
            },
            {
                "img": "https://arxiv.org/html/2504.02792/x9.png",
                "caption": "Figure 9:Visualization of categorized out-of-distribution (OOD) settings. We construct scenes with varied lighting conditions, backgrounds, and clutter to analyze the models’ generalization patterns.",
                "position": 875
            },
            {
                "img": "https://arxiv.org/html/2504.02792/x10.png",
                "caption": "Figure 10:Training models from scratch vs finetuning pretrained models. UWM scales more effectively from pretraining than DP.",
                "position": 975
            }
        ]
    },
    {
        "header": "VRelated Work",
        "images": []
    },
    {
        "header": "VIDiscussion",
        "images": []
    },
    {
        "header": "VIILimitations",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02792/x11.png",
                "caption": "Figure 11:Setup of the robot experiments. We adopt the DROID[25]setup which consists of two scene cameras and one wrist camera. We use an additional evaluation camera to track the initialization of evaluation seeds.",
                "position": 1993
            },
            {
                "img": "https://arxiv.org/html/2504.02792/x12.png",
                "caption": "Figure 12:Screenshots of the evaluation tracker. We use the same interface to track the randomization for all real robot tasks.",
                "position": 2089
            },
            {
                "img": "https://arxiv.org/html/2504.02792/x13.png",
                "caption": "Figure 13:Visualization of the robot’s perspective in in-distribution, standard out-of-distribution (Table.I), and categorized out-of-distribution (Table.IV) scenarios.",
                "position": 2102
            },
            {
                "img": "https://arxiv.org/html/2504.02792/x14.png",
                "caption": "Figure 14:Visualization of Internet video dataset. We curate the dataset by combining human activity videos from Kinetics-400[8]and Something-Something-v2[18].",
                "position": 2209
            }
        ]
    },
    {
        "header": "Appendix AAdditional Details on Simulated Experiments",
        "images": []
    }
]