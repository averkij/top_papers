[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11140/extracted/6445601/figures/hf-logo.png",
                "caption": "",
                "position": 119
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11140/x1.png",
                "caption": "Figure 1:Snapshot of Experiments.We show a snapshot of the experiments executed in this study. There are four settings on how a question can be answered; (1) direct answer from an instruction-tuned model, (2) step-by-step reasoning via an instruction-tuned model, (3) thinking trajectory, and (4) knowledge-graph enhanced thinking. We show an example of KG paths from Wikidata. We take the paths with the natural language to enhance the reasoning traces.222wdt:https://www.wikidata.org/prop/direct/, wd:https://www.wikidata.org/entity/",
                "position": 141
            },
            {
                "img": "https://arxiv.org/html/2505.11140/x2.png",
                "caption": "Figure 2:Distribution of Reasoning Traces.In the figure, we show the distribution of the reasoning trace length among the queried models. On the left column, we show the reasoning traces from rt and on the right column, we show fs1. We show that particularly for fs1 andDeepseek-R1, the reasoning length is shorter.",
                "position": 256
            }
        ]
    },
    {
        "header": "2Reasoning Data",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11140/x3.png",
                "caption": "Figure 3:Data Overlap Analysis.We show data overlap between the training set/benchmarks. On the left, one can observe the count of similar questions when the cosine similarity>>>0.90 (measured withparaphrase-MiniLM-L6-v2;Reimers and Gurevych,2019). In the middle, we measure exact match counts. On the right, we show the average pairwise cosine similarity across the full test sets.",
                "position": 407
            }
        ]
    },
    {
        "header": "4Results and Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11140/x4.png",
                "caption": "Figure 4:Main Results for Benchmark.We show the accuracy for asingle runfor six datasets with four baselines and 24 different Qwen2.5 results. For the Qwen2.5 models, from left-to-right, we show the performance of the original instruction-tuned model, chain-of-thought, fine-tuned on rt, and fine-tuned on fs1. We show that for the smaller models, rt and fs1 outperform their original counterparts, but become less pronounced the larger the model gets. We show exact numbers inAppendixE.",
                "position": 446
            },
            {
                "img": "https://arxiv.org/html/2505.11140/x5.png",
                "caption": "Figure 5:Test-Time Scaling for Factual Reasoning.We show withQwen2.5-32B-fs1that parallel scaling via majority voting (measured in any@k or consensus@k) is beneficial for complex multi-hop question answering.",
                "position": 464
            },
            {
                "img": "https://arxiv.org/html/2505.11140/x6.png",
                "caption": "Figure 6:Budget Forcing.We show the performance of budget forcing at token budgets of {256256256256,512512512512,1,02410241{,}0241 , 024,2,04820482{,}0482 , 048,4,09640964{,}0964 , 096,8,19281928{,}1928 , 192}. The performance increases the more “thinking time” we give the model and show that it stagnates after a2,04820482{,}0482 , 048token budget.",
                "position": 480
            },
            {
                "img": "https://arxiv.org/html/2505.11140/x6.png",
                "caption": "Figure 6:Budget Forcing.We show the performance of budget forcing at token budgets of {256256256256,512512512512,1,02410241{,}0241 , 024,2,04820482{,}0482 , 048,4,09640964{,}0964 , 096,8,19281928{,}1928 , 192}. The performance increases the more “thinking time” we give the model and show that it stagnates after a2,04820482{,}0482 , 048token budget.",
                "position": 482
            },
            {
                "img": "https://arxiv.org/html/2505.11140/x7.png",
                "caption": "Figure 7:Trade-off Results.We take theQwen2.5-32B-rtandfs1and compare both budget forcing (until 2K tokens) and parallel sampling (taking any@k). We evaluate via pass@k. Parallel sampling overtakes budget forcing in both cases for factual reasoning.",
                "position": 486
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion and Outlook",
        "images": []
    },
    {
        "header": "Acknowledgments and Disclosure of Funding",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining and Inference",
        "images": []
    },
    {
        "header": "Appendix BExample Reasoning Traces",
        "images": []
    },
    {
        "header": "Appendix CTest Benchmark",
        "images": []
    },
    {
        "header": "Appendix DLLM-as-a-judge",
        "images": []
    },
    {
        "header": "Appendix EExact Numbers",
        "images": []
    },
    {
        "header": "Appendix FSPARQL queries",
        "images": []
    },
    {
        "header": "Appendix GBudget Forcing Output Examples",
        "images": []
    }
]