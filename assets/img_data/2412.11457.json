[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.11457/x1.png",
                "caption": "",
                "position": 132
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.11457/x2.png",
                "caption": "Figure 2:Overview ofMOVIS.Our model performsNVSfrom the input image and relative camera change. We introduce structure-aware features as additional inputs and employ mask prediction as an auxiliary task¬†(Sec.3.2). The model is trained with a structure-guided timestep sampling scheduler (Fig.3) to balance the learning of global object placement and local detail recovery.",
                "position": 212
            },
            {
                "img": "https://arxiv.org/html/2412.11457/x3.png",
                "caption": "Figure 3:Visualization of inference.The early stage of the denoising process focuses on restoring global object placements, while the prediction of object masks requires a relatively noiseless image to recover fine-grained geometry. This motivates us to seek a balanced timestep sampling scheduler during training. The model trainedw/ shiftyields better mask prediction and thus recovers an image with more details and sharp object boundary. Thew/o shifthere refers to not shifting theŒºùúá\\muitalic_Œºvalue.",
                "position": 287
            },
            {
                "img": "https://arxiv.org/html/2412.11457/x4.png",
                "caption": "Figure 4:Qualitative results ofNVSand cross-view matching.Our method generates plausible novel-view images across various datasets, surpassing baselines regarding object placement, shape, and appearance. In cross-view matching, points of the same color indicate correspondences between the input and target views. We achieve a higher number of matched points with more precise locations.",
                "position": 323
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.11457/x5.png",
                "caption": "Figure 5:Qualitative comparison for ablation study. Excluding mask predictions or the scheduler reduces the model‚Äôs ability to learn object placement, as shown by the brown cabinet example.",
                "position": 526
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AModel details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.11457/x6.png",
                "caption": "Figure S.6:Illustration of different timestep sampling strategies.",
                "position": 1554
            },
            {
                "img": "https://arxiv.org/html/2412.11457/x7.png",
                "caption": "Figure S.7:Comparison of different strategies. The predicted images and mask images under novel views using different strategies are visualized. We can observe that images predicted by the KMS strategy possess weird and blurry color while LDC strategy seems to be slightly better than LIND.",
                "position": 1605
            }
        ]
    },
    {
        "header": "Appendix BExperiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.11457/x8.png",
                "caption": "Figure S.8:Visualized comparison on Room-Texture[35], SUNRGB-D[49], and 3D-FRONT[14].",
                "position": 1851
            },
            {
                "img": "https://arxiv.org/html/2412.11457/x9.png",
                "caption": "Figure S.9:Continuous rotation examples on SUNRGB-D and 3D-FRONT. We rotate the camera around the multi-object composites, successfully synthesizing plausible novel-view images across a wide range of camera pose variations. This first five examples are from SUNRGB-D, and the last three examples are from 3D-FRONT.",
                "position": 1854
            },
            {
                "img": "https://arxiv.org/html/2412.11457/x10.png",
                "caption": "Figure S.10:Visualized cross-view matching results. Since we do not have ground truth image for 3D-FRONT and SUNRGB-D, we only visualize cross-view matching results using our predicted images. But we can still observe a strong cross-view consistency from the accurate matching results.",
                "position": 1857
            },
            {
                "img": "https://arxiv.org/html/2412.11457/x11.png",
                "caption": "Figure S.11:Failure Cases. It is hard for our model to learn extremely fine-grained consistency on objects with delicate structure and texture.",
                "position": 2056
            },
            {
                "img": "https://arxiv.org/html/2412.11457/x12.png",
                "caption": "Figure S.12:Occlusion Synthesis Capability. Our proposed method can synthesize new occlusion relationship under novel views as shown in the highlighted area of sofa or cabinet in (a). Our method can also hallucinate occluded parts as shown in the highlighted area of chairs in (b).",
                "position": 2059
            },
            {
                "img": "https://arxiv.org/html/2412.11457/x13.png",
                "caption": "Figure S.13:Object Removal Example. We can remove an object under novel views by setting a threshold to the predicted mask image and delete corresponding pixels.",
                "position": 2142
            },
            {
                "img": "https://arxiv.org/html/2412.11457/x14.png",
                "caption": "Figure S.14:Reconstruction results using DUSt3R.We rotate our camera around the multi-object composite and use the predicted images along with the input-view image for reconstruction.",
                "position": 2145
            },
            {
                "img": "https://arxiv.org/html/2412.11457/x15.png",
                "caption": "",
                "position": 2154
            }
        ]
    },
    {
        "header": "Appendix CFailure Cases and Limitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.11457/x16.png",
                "caption": "Figure S.15:More visualized results on C3DFS dataset.",
                "position": 2185
            },
            {
                "img": "https://arxiv.org/html/2412.11457/x17.png",
                "caption": "Figure S.16:More visualized results on Objaverse dataset.",
                "position": 2188
            },
            {
                "img": "https://arxiv.org/html/2412.11457/x18.png",
                "caption": "Figure S.17:More visualized results on Room-Texture dataset.",
                "position": 2191
            }
        ]
    },
    {
        "header": "Appendix DPotential Negative Impact",
        "images": []
    }
]