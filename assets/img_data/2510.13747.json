[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13747/figures/radar_chart.png",
                "caption": "Figure 1:Evaluation across image, video, and audio modalities on open-source benchmarks. InteractiveOmni outperforms the current leading multi-modal models such as Qwen2.5-VL-7Bbai2025qwen25,\nKimi-Audiokimiteam2025kimiaudiotechnicalreport, MiniCPM-o-2.6minicpm-vand Qwen2.5-Omni-7Bxu2025qwen2.",
                "position": 174
            },
            {
                "img": "https://arxiv.org/html/2510.13747/figures/demo_interaction.png",
                "caption": "Figure 2:The schematic diagram of multi-turn audio-visual interaction. InteractiveOmni can perceive external audio and video inputs like a human, actively interact with users, and has the capabilities of multi-turn memory and empathy.",
                "position": 187
            }
        ]
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13747/figures/model_architecture.png",
                "caption": "Figure 3:The overview framework of InteractiveOmni. InteractiveOmni is composed of vision encoder, audio encoder, LLM decoder and streaming speech decoder. The extracted visual and audio tokens are processed by the LLM to generate text tokens and speech tokens sequentially.",
                "position": 239
            },
            {
                "img": "https://arxiv.org/html/2510.13747/figures/data_pipeline.png",
                "caption": "Figure 4:Data construction pipeline for multi-turn dialogue. In each turn, the visual element is sampled from a dedicated image and video repository. The corresponding question is then generated by a vision-language model using a specific prompt tailored to the desired question type. To ensure the dialogue effectively tests long-term memory, we specifically design turns that require recalling historical images and previous dialogue text. Finally, the generated text-format question and answer can be transformed into speech-based question-answer pairs using a TTS system, facilitating end-to-end training.",
                "position": 288
            }
        ]
    },
    {
        "header": "3Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13747/figures/recall_images_distance.png",
                "caption": "Figure 5:The sketch of performance degradation with the increase of recall burden considering the turn distance and number of memorized images.\nInteractiveOmni is comparable to proprietary models like GPT-4o-mini and Gemini-2.5-Flash, consistently outperforming open-source models such as InternVL3-8B, Qwen2.5-VL-7B, and Qwen2.5-Omni-7B.",
                "position": 852
            },
            {
                "img": "https://arxiv.org/html/2510.13747/figures/mmmb_case.png",
                "caption": "Figure 6:An example of multi-turn conversations requiring historical image context. InteractiveOmni demonstrates enhanced long-term memory performance for historical images compared to Qwen2.5-Omni-7B.",
                "position": 860
            },
            {
                "img": "https://arxiv.org/html/2510.13747/figures/average_quality_radar.png",
                "caption": "Figure 7:Human evaluation of the speech-to-speech interactions on MSIB.",
                "position": 1097
            }
        ]
    },
    {
        "header": "4Related works",
        "images": []
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AEvaluation Details of Multi-turn Speech Interaction Benchmark",
        "images": []
    }
]