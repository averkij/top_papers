[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3LALM Evaluation Challenges",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.08031/x1.png",
                "caption": "Figure 1:Architecture overview of AU-Harness evaluation framework.Our system comprises three core components: (1)Configmodule for hierarchical task configuration and standardized prompting, (2)Request Controllermanaging token-based concurrency limits across all engines with adaptive retry mechanisms, and (3)Concurrent Enginesexecuting parallel model evaluation with dataset sharding. The Request Controller maintains a global token pool accessible to all engines, enabling efficient resource utilization and scalable throughput. Multiple concurrent connections between the controller and inference models illustrate parallel request dispatch, with each engine supporting multi-model evaluation on targeted datasets.",
                "position": 627
            },
            {
                "img": "https://arxiv.org/html/2509.08031/x2.png",
                "caption": "Figure 2:Task distribution and coverage in AU-Harness.Our framework encompasses six major task categories with balanced representation: Speech Recognition (ASR variants), Paralinguistics (emotion, speaker, accent recognition), Spoken Language Understanding (QA, translation, summarization), Audio Understanding (scene, music), Spoken Language Reasoning (function calling, coding, instruction following), and Safety & Security (robustness, spoofing detection). The distribution reflects comprehensive coverage from basic perception to complex reasoning, with novel emphasis on prompted temporal understanding and audio-conditioned cognitive tasks.",
                "position": 637
            }
        ]
    },
    {
        "header": "4AU-Harness",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.08031/x3.png",
                "caption": "Figure 3:LLM-Adaptive Diarization methodology comparison.222Figure is adapted fromNeMo documentationTraditional diarization (top, bottom-right) outputs time-stamped audio segments with speaker annotations, ideal for specialized neural architectures. LLM-Adaptive approach (bottom-left) integrates speaker information directly into transcripts, enabling evaluation through prompting-based generation evaluated via word-level metrics (WDER, cpWER). This approach leverages LALMs’ inherent language modeling capabilities while addressing temporal precision challenges through specialized evaluation protocols.",
                "position": 693
            }
        ]
    },
    {
        "header": "5Results & Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.08031/x4.png",
                "caption": "(a)",
                "position": 997
            },
            {
                "img": "https://arxiv.org/html/2509.08031/x4.png",
                "caption": "(a)",
                "position": 1000
            },
            {
                "img": "https://arxiv.org/html/2509.08031/x5.png",
                "caption": "(b)",
                "position": 1006
            },
            {
                "img": "https://arxiv.org/html/2509.08031/x6.png",
                "caption": "Figure 5:Parallel runtime efficiency analysis across evaluation frameworks.Scatter plot comparing frameworks under optimal parallel execution conditions, plotting Real-time Factor (x-axis,↓\\downarrowbetter) against Processed Samples per Second (y-axis,↑\\uparrowbetter). Our framework (rightmost cluster) achieves superior performance in both dimensions, demonstrating the effectiveness of token-based request scheduling, dataset sharding, and vLLM integration for large-scale LALM evaluation.",
                "position": 1013
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]