[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08525/x1.png",
                "caption": "",
                "position": 141
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Thought Collapse",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08525/x2.png",
                "caption": "Figure 2:Thought collapse persists for larger model scales and training budgets.We train LLaVA 7b and 13b models for 30k RL steps but still observe the degraded performance.",
                "position": 208
            }
        ]
    },
    {
        "header": "4Guided Thought Reinforcement",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08525/x3.png",
                "caption": "Figure 3:Overview of the GTR framework.We modify the RL finetuning (orange region) of VLM agents by introducing automated thought correction (green region) as guidance, leveraging an off-the-shelf VLM model as a corrector (purple region). GTR performs SFT updating for the agent‚Äôs thought tokens and PPO updating for its action tokens, thereby training thoughts and actions simultaneously.",
                "position": 218
            },
            {
                "img": "https://arxiv.org/html/2503.08525/x4.png",
                "caption": "Figure 4:Comparison among different process guidance methods in the 24 points card game.Trivial numerical rewards provided by the VLM judge and rule-based evaluation cannot incentivize the agent‚Äôs reasoning thoughts and higher success rate.",
                "position": 283
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08525/x5.png",
                "caption": "Figure 5:Training curves on the 24 points game environment.Compared to the baseline methods, our GTR framework integrating process guidance with RL achieves better performance while maintaining a rational reasoning process. Curves are smoothed for better readability. Since GTR and SFT-only employ truncation strategies, we plotŒ≥=0.9ùõæ0.9\\gamma=0.9italic_Œ≥ = 0.9discounted returns in the figure for a fair comparison.",
                "position": 490
            },
            {
                "img": "https://arxiv.org/html/2503.08525/x6.png",
                "caption": "Figure 7:Comparison of training curves between GTR and RL4VLM in the ALFWorld environment.RL4VLM fails to effectively facilitate model learning, leading to thought collapse. GTR, however, enables the agent‚Äôs performance to improve steadily, ultimately achieving superior results.",
                "position": 671
            },
            {
                "img": "https://arxiv.org/html/2503.08525/x7.png",
                "caption": "Figure 9:Ablation study of the GTR framework.Its superior performance highlights the importance of full-process thought guidance, task-specific knowledge, and DAgger.",
                "position": 838
            },
            {
                "img": "https://arxiv.org/html/2503.08525/x8.png",
                "caption": "Figure 10:Comparing thought cloning with supervise finetuning on both thoughts and actions.This full response cloning approach does not yield satisfactory results due to its vulnerability to the corrector‚Äôs hallucinations.",
                "position": 847
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Details on Environments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08525/x9.png",
                "caption": "Figure 11:The Points24 task.",
                "position": 1719
            },
            {
                "img": "https://arxiv.org/html/2503.08525/x10.png",
                "caption": "Figure 12:The ALFWorld task.",
                "position": 1753
            },
            {
                "img": "https://arxiv.org/html/2503.08525/x11.png",
                "caption": "Figure 13:Other tasks in thegym_cardsenvironment.",
                "position": 1781
            }
        ]
    },
    {
        "header": "Appendix BAdditional Details on Training",
        "images": []
    },
    {
        "header": "Appendix CPrompts of the Corrector Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08525/x12.png",
                "caption": "Figure 14:Examples of thought collapse trajectories in Points24 and ALFWorld.",
                "position": 2151
            }
        ]
    },
    {
        "header": "Appendix DExample Trajectories of Thought Collapse",
        "images": []
    }
]