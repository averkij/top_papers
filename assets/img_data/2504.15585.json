[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15585/extracted/6377930/Fig/insight.png",
                "caption": "",
                "position": 967
            },
            {
                "img": "https://arxiv.org/html/2504.15585/x1.png",
                "caption": "Figure 1:The overview of the safety of LLM-based agent systems.",
                "position": 989
            }
        ]
    },
    {
        "header": "2Data Safety",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15585/x2.png",
                "caption": "Figure 2:LLMs encounter a wide range of data safety risks throughout their lifecycle, from the initial stages of data collection and pre-processing to model training, deployment, and ongoing updates.",
                "position": 1009
            }
        ]
    },
    {
        "header": "3Pre-training Safety",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15585/x3.png",
                "caption": "Figure 3:Pipeline of the Strategies for Pre-training Safety. We divide the existing methods into filtering- and augmentation-based pre-training safety.",
                "position": 1338
            },
            {
                "img": "https://arxiv.org/html/2504.15585/x4.png",
                "caption": "Figure 4:The taxonomy illustration of LLM post-training safety.",
                "position": 1361
            }
        ]
    },
    {
        "header": "4Post-training Safety",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15585/x5.png",
                "caption": "Figure 5:The taxonomy illustration of LLM alignment safety.",
                "position": 1529
            }
        ]
    },
    {
        "header": "5Safety in Model Editing & Unlearning",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15585/x6.png",
                "caption": "Figure 6:The taxonomy illustration of LLM Unlearning for safety.",
                "position": 2105
            },
            {
                "img": "https://arxiv.org/html/2504.15585/x7.png",
                "caption": "Figure 7:We define the goal of unlearning as maximizing both model utility and forget quality, meaning that algorithms positioned closer to the top-right corner are considered more reliable.",
                "position": 2252
            }
        ]
    },
    {
        "header": "6LLM(-Agent) Deployment Safety",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15585/x8.png",
                "caption": "Figure 8:The overview of attacks in single LLM’s deployment phase.",
                "position": 2281
            },
            {
                "img": "https://arxiv.org/html/2504.15585/x9.png",
                "caption": "Figure 9:The overview of attacks in single LLM’s deployment phase.",
                "position": 4413
            },
            {
                "img": "https://arxiv.org/html/2504.15585/x10.png",
                "caption": "Figure 10:The overview of evaluation and benchmarks in single LLM’s deployment phase.",
                "position": 4516
            },
            {
                "img": "https://arxiv.org/html/2504.15585/extracted/6377930/Fig/Agent.png",
                "caption": "Figure 11:The overview of LLM-based single-agent and multi-agent systems.",
                "position": 4978
            },
            {
                "img": "https://arxiv.org/html/2504.15585/x11.png",
                "caption": "Figure 12:The overview of the safety of LLM-based agent systems.",
                "position": 5013
            },
            {
                "img": "https://arxiv.org/html/2504.15585/extracted/6377930/Fig/env.png",
                "caption": "Figure 13:The overview of agent and environment interactions.",
                "position": 5065
            }
        ]
    },
    {
        "header": "7Safety in LLM-Based Application",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15585/x12.png",
                "caption": "Figure 14:We illustrate the diverse applications of AI in enterprise productivity, content generation, programming, healthcare, finance, customer support, education, and cyber-security. We also highlight critical issues related to truthfulness and privacy, including data leakage, security threats, property rights, fairness, and regulatory compliance, underscoring the need for robust safeguards in AI deployment",
                "position": 5469
            }
        ]
    },
    {
        "header": "8Potential Research Directions",
        "images": []
    },
    {
        "header": "9Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]