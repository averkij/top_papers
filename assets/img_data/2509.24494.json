[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary: GRPO",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24494/x1.png",
                "caption": "Figure 1:The operational flow of advantage estimation in GRPO and GRPO-MA. In the baseline GRPO framework (top), the advantage is computed from a single thought–answer pair, inherently coupling the estimation of thought and answer advantages to a single reward signal. In contrast, GRPO-MA (bottom) extends this setting by sampling multiple answers for each thought. This design decouples the estimation of thought and answer advantages and leverages aggregated information from multiple reward signals, thereby yielding richer supervision and enabling more robust and stable estimation of thought-level advantages.",
                "position": 282
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24494/x2.png",
                "caption": "Figure 2:A case study comparing the baseline GRPO with our proposed GRPO-MA on a referring expression grounding task. The prompt is to locate the “purple bottled beverage”. The baseline model, GRPO (T4A1), recognizes the target’s existence but its reasoning is distracted by other salient objects (the snacks), leading to a failure in grounding. In contrast, our GRPO-MA (T4A4) correctly reasons about the scene’s context, focuses on the target object held by the robotic arm, and successfully provides the precise bounding box. This demonstrates the superior robustness of GRPO-MA in complex scene understanding and reasoning.",
                "position": 454
            },
            {
                "img": "https://arxiv.org/html/2509.24494/x3.png",
                "caption": "Figure 3:Ablation Study on Trajectory PredictionWhile maintaining the number of thoughtsK=4K=4, we gradually increase the number of responsesMMper thought from 1 to 8 (i.e., the number of responses is 4, 8, 12…32).",
                "position": 1075
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24494/x4.png",
                "caption": "Figure 4:Case Study on Object DetectionGreen text indicates key reasoning content.",
                "position": 2787
            },
            {
                "img": "https://arxiv.org/html/2509.24494/x5.png",
                "caption": "Figure 5:Case Study on Trajectory PredictionGreen text indicates key reasoning content.",
                "position": 2790
            },
            {
                "img": "https://arxiv.org/html/2509.24494/x6.png",
                "caption": "Figure 6:Case Study on Trajectory PredictionGreen text indicates key reasoning content.",
                "position": 2793
            },
            {
                "img": "https://arxiv.org/html/2509.24494/x7.png",
                "caption": "Figure 7:Visualization on Simulator-based Visual ManipulationRed dots indicate failures, while green dots represent successes. We can observe that most GRPO-MA-T4A4 points are located on the object. In contrast, GRPO-T4A1 frequently misses the object, resulting in a lower success rate.",
                "position": 2796
            },
            {
                "img": "https://arxiv.org/html/2509.24494/x8.png",
                "caption": "Figure 8:Accuracy Reward Curve on Object Detection",
                "position": 2799
            },
            {
                "img": "https://arxiv.org/html/2509.24494/x9.png",
                "caption": "Figure 9:Accuracy Reward Curve on Affordance Prediction",
                "position": 2802
            },
            {
                "img": "https://arxiv.org/html/2509.24494/x10.png",
                "caption": "Figure 10:Accuracy Reward Curve on Demand Prediction",
                "position": 2805
            },
            {
                "img": "https://arxiv.org/html/2509.24494/x11.png",
                "caption": "Figure 11:Accuracy Reward Curve on OCR-based VQA",
                "position": 2808
            },
            {
                "img": "https://arxiv.org/html/2509.24494/x12.png",
                "caption": "Figure 12:Accuracy Reward Curve on Trajectory Prediction",
                "position": 2811
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]