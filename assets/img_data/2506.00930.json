[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00930/x1.png",
                "caption": "Figure 1:Individuals with different Role-Sets exhibit personalized situated cognition when encountering the same ‚Äúbroken swing‚Äù visual scene, resulting in personalized expectations for the VLM assistants‚Äô responses.",
                "position": 238
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Problem Definition",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00930/x2.png",
                "caption": "Figure 2:Overview of the collection process of PCogAlignBench.\nTheLS1subset considers roles in ‚Äú{Home, Community, Museum, Airport, Store}‚Äù.\nTheLS2subset considers roles in ‚Äú{Home, Community, School, Hospital, Restaurant}‚Äù.\nHere we take the ‚ÄúIndividual I1 in LS1‚Äù as an example to illustrate the process of image collection.",
                "position": 356
            }
        ]
    },
    {
        "header": "4PCogAlignBench",
        "images": []
    },
    {
        "header": "5PCogAlign",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00930/x3.png",
                "caption": "Figure 3:Overview of the proposed PCogAlign. We first estimate the situated cognition and optimal action in (a). Then, the ‚ÄúKeyPoints Generator (KeyG)‚Äù and ‚ÄúResponse Generator (ResG)‚Äù agents cooperate to sampleNùëÅNitalic_Ncandidate personalized responses in (b). Finally, we utilize a specific reward model to select the optimal response as feedback for alignment training in (c).",
                "position": 471
            },
            {
                "img": "https://arxiv.org/html/2506.00930/x4.png",
                "caption": "Figure 4:Overview of the reward model construction.",
                "position": 493
            }
        ]
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00930/x5.png",
                "caption": "Figure 5:Heatmaps showing the comparison between automatic and human evaluations on two subsets, LS1 and LS2. The automatic evaluation agrees with human assessment in 88% of cases (the sum of the diagonal elements), highlighting the reliability of the proposed automatic evaluation method.",
                "position": 563
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASupplementary Experimental Results",
        "images": []
    },
    {
        "header": "Appendix BPCogAlignBench Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00930/x6.png",
                "caption": "Figure 6:Word cloud visualization of user queries from the test split of LS1 and LS2 subsets, illustrating the variety of queries in the dataset. Note that LS2 includes an individual who is a ‚Äúchild‚Äù, and individuals may frequently encounter scenarios in various locations where child-related issues need to be addressed. This results in a significant number of queries involving ‚Äúchild‚Äù.",
                "position": 2405
            }
        ]
    },
    {
        "header": "Appendix CExperimental Details",
        "images": []
    }
]