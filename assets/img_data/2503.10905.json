[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10905/x1.png",
                "caption": "Figure 1:Top:AdaLLaVAempowers a base LLaVA model with the ability to adapt to varying compute budgets at inference time while maintaining minimal performance loss.Bottom: Given an image, a text query and a latency budget, AdaLLaVAÂ learns to reconfigure operations within a base MLLM, generating appropriate responses while sticking to the budget.",
                "position": 73
            },
            {
                "img": "https://arxiv.org/html/2503.10905/x2.png",
                "caption": "",
                "position": 77
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Adaptive Inference of MLLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10905/x3.png",
                "caption": "Figure 2:Overview ofAdaLLaVA.(a) Model architecture: Our latency encoder embeds an input latency budget into a latency token, which is further processed by the early part of the LLM. The resulting embedding is then fed into the scheduler, leading to the output of an execution plan that control individual operations in the remaining part of the LLM. Our latency encoder and scheduler are jointly trained with the MLLM.(b)AdaLLaVA-L: the scheduler controls the execution of entire Transformer blocks.(c)AdaLLaVA-H: the scheduler controls the execution of attention heads and MLP neurons, by masking out their activation values and the corresponding weights.",
                "position": 175
            }
        ]
    },
    {
        "header": "4Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10905/x4.png",
                "caption": "Figure 3:Accuracy-latency tradoffs of AdaLLaVAwith LLaVA-1.5-7B and additional token selection techniques (PruMerge+ / FastV). Results reported on VQAv2.",
                "position": 873
            },
            {
                "img": "https://arxiv.org/html/2503.10905/x5.png",
                "caption": "Figure 4:Visualization of attention between the input latency token and visual tokenswith a 100% latency budget.",
                "position": 902
            },
            {
                "img": "https://arxiv.org/html/2503.10905/x6.png",
                "caption": "Figure 5:Visualization of execution plans for different input. The plan is color-coded withenableordisablefor the 16th to 32th Transformer blocks (left to right). The latency budget is 75%.",
                "position": 912
            }
        ]
    },
    {
        "header": "5Conclusion and Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AFurther Implementation Details",
        "images": []
    },
    {
        "header": "Appendix BDetailed Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10905/x7.png",
                "caption": "Figure 6:Relative performance of applying AdaLLaVA-L to Mipha-3B under various latency budget. The center of the radar corresponds to 60% performance of the base Mipha-3B.",
                "position": 2115
            }
        ]
    },
    {
        "header": "Appendix CAdditional Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10905/x8.png",
                "caption": "Figure 7:Ablation studies on switch design choices.",
                "position": 2755
            },
            {
                "img": "https://arxiv.org/html/2503.10905/x9.png",
                "caption": "",
                "position": 2765
            },
            {
                "img": "https://arxiv.org/html/2503.10905/extracted/6288110/figure/quali_latency_aware/man_red.jpeg",
                "caption": "Table 6:AdaLLaVA-L on LLaVA 1.5-7b model, generating appropriate responses while adapting to varying computational budgets.",
                "position": 2791
            },
            {
                "img": "https://arxiv.org/html/2503.10905/extracted/6288110/figure/quali_latency_aware/man_red.jpeg",
                "caption": "Table 6:AdaLLaVA-L on LLaVA 1.5-7b model, generating appropriate responses while adapting to varying computational budgets.",
                "position": 2792
            },
            {
                "img": "https://arxiv.org/html/2503.10905/extracted/6288110/figure/quali_latency_aware/press_stop.jpg",
                "caption": "",
                "position": 2812
            },
            {
                "img": "https://arxiv.org/html/2503.10905/extracted/6288110/figure/quali_latency_aware/woman_nature.jpg",
                "caption": "",
                "position": 2817
            },
            {
                "img": "https://arxiv.org/html/2503.10905/x10.png",
                "caption": "Figure 8:The key-query attention scores between latency token and visual tokens. The latency input is 1.0 in these examples.",
                "position": 2941
            },
            {
                "img": "https://arxiv.org/html/2503.10905/x11.png",
                "caption": "",
                "position": 2945
            },
            {
                "img": "https://arxiv.org/html/2503.10905/x12.png",
                "caption": "",
                "position": 2947
            },
            {
                "img": "https://arxiv.org/html/2503.10905/x13.png",
                "caption": "Figure 9:Evolution of latency token across layers in AdaLLaVA-L on 7b model.",
                "position": 2956
            }
        ]
    },
    {
        "header": "Appendix DAdditional Results on Latency and Content Adaptivity",
        "images": []
    }
]