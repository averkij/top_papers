[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02375/x1.png",
                "caption": "Figure 1:Left:Schematic of pretraining-with-memories: some parameters are always used (anchor parameters), others are fetched per input document (memory parameters).Middle:Accuracy improvement over baseline when≃10%\\simeq 10\\%of parameters are allocated as memories for a knowledge-intensive task (predicting the atomic numbers of elements), using models with 160M, 410M, and 1.4B parameters, corresponding to rows A2, B2, and C2 inTable˜1.Right:Elements sorted by their frequency of appearance in the DCLM-Baseline dataset (5 buckets, each with≃24\\simeq 24elements). With the proposedpretraining-with-memories, we observe significant improvements, especially on long-tail data. While the baseline 1.4B model has only 17% accuracy on the least frequent element bucket, augmenting it with only 10% memory parameters increases the accuracy to 83%.",
                "position": 90
            },
            {
                "img": "https://arxiv.org/html/2510.02375/x2.png",
                "caption": "",
                "position": 93
            },
            {
                "img": "https://arxiv.org/html/2510.02375/figures/elements_accuracies.png",
                "caption": "",
                "position": 94
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Pretraining with memories",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02375/x3.png",
                "caption": "Figure 2:Proposed architecture:For a given contextxx(such as a question text), the memory retriever module selects relevant parameters from a large set of memory bank parameters. These memory parameters are organized hierarchically based on the hierarchical clustering of the pretraining data. The anchor model, together with the retrieved memories, then responds to the question.",
                "position": 129
            }
        ]
    },
    {
        "header": "3Design choices for pretraining with memories",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02375/figures/memory_type_knowledge.png",
                "caption": "(a)",
                "position": 215
            },
            {
                "img": "https://arxiv.org/html/2510.02375/figures/memory_type_knowledge.png",
                "caption": "(a)",
                "position": 218
            },
            {
                "img": "https://arxiv.org/html/2510.02375/figures/memory_type_wiki.png",
                "caption": "(b)",
                "position": 223
            },
            {
                "img": "https://arxiv.org/html/2510.02375/figures/memory_size_knowledge.png",
                "caption": "(c)",
                "position": 229
            },
            {
                "img": "https://arxiv.org/html/2510.02375/figures/memory_bank_size_knowledge.png",
                "caption": "(d)",
                "position": 234
            },
            {
                "img": "https://arxiv.org/html/2510.02375/figures/runtime_vs_bank_size_knowledge.png",
                "caption": "(a)",
                "position": 254
            },
            {
                "img": "https://arxiv.org/html/2510.02375/figures/runtime_vs_bank_size_knowledge.png",
                "caption": "(a)",
                "position": 257
            },
            {
                "img": "https://arxiv.org/html/2510.02375/figures/model_size_ablation.png",
                "caption": "(b)",
                "position": 262
            },
            {
                "img": "https://arxiv.org/html/2510.02375/figures/hierarchy_advantage.png",
                "caption": "Figure 5:Deployment advantages of hierarchical memories.Left:Memory loading latency is reduced by using the hardware hierarchy.Right:Latency is reduced by exploiting compositionality over time: larger memories for low-level clusters, once loaded, are less likely to need reloading.",
                "position": 279
            }
        ]
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02375/figures/fetched_vs_generic_knowledge.png",
                "caption": "(a)",
                "position": 466
            },
            {
                "img": "https://arxiv.org/html/2510.02375/figures/fetched_vs_generic_knowledge.png",
                "caption": "(a)",
                "position": 469
            },
            {
                "img": "https://arxiv.org/html/2510.02375/figures/memory_block_results.png",
                "caption": "(b)",
                "position": 474
            }
        ]
    },
    {
        "header": "5Related works",
        "images": []
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix Contents",
        "images": []
    },
    {
        "header": "Appendix ATraining details",
        "images": []
    },
    {
        "header": "Appendix BEvaluation",
        "images": []
    },
    {
        "header": "Appendix CData clustering details",
        "images": []
    },
    {
        "header": "Appendix DTokens per parameter in memory learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02375/figures/tpp_ck.png",
                "caption": "(a)",
                "position": 2503
            },
            {
                "img": "https://arxiv.org/html/2510.02375/figures/tpp_ck.png",
                "caption": "(a)",
                "position": 2506
            },
            {
                "img": "https://arxiv.org/html/2510.02375/figures/tpp_sk.png",
                "caption": "(b)",
                "position": 2511
            },
            {
                "img": "https://arxiv.org/html/2510.02375/figures/tpp_wiki_en.png",
                "caption": "(c)",
                "position": 2516
            }
        ]
    },
    {
        "header": "Appendix EFetched memory vs generic memory (additional results)",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02375/figures/fetched_vs_generic_knowledge.png",
                "caption": "(a)",
                "position": 2544
            },
            {
                "img": "https://arxiv.org/html/2510.02375/figures/fetched_vs_generic_knowledge.png",
                "caption": "(a)",
                "position": 2547
            },
            {
                "img": "https://arxiv.org/html/2510.02375/figures/fetched_vs_generic_knowledge_diff.png",
                "caption": "(b)",
                "position": 2552
            },
            {
                "img": "https://arxiv.org/html/2510.02375/figures/fetched_vs_generic_wikien.png",
                "caption": "(c)",
                "position": 2558
            },
            {
                "img": "https://arxiv.org/html/2510.02375/figures/fetched_vs_generic_wikien_diff.png",
                "caption": "(d)",
                "position": 2563
            }
        ]
    },
    {
        "header": "Appendix FMemory location",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02375/x4.png",
                "caption": "Figure 9:Different distribution of memory along the layers of the anchor model.",
                "position": 2583
            },
            {
                "img": "https://arxiv.org/html/2510.02375/x4.png",
                "caption": "Figure 9:Different distribution of memory along the layers of the anchor model.",
                "position": 2586
            }
        ]
    },
    {
        "header": "Appendix GWhat tasks benefit more from memories",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02375/figures/knowledge_specificity_vs_gain.png",
                "caption": "Figure 11:Fetched memories improve performance on knowledge-intensive benchmarks.Accuracy gain (fetched memory vs. generic memory) for the 1.4B model (row C2 inTable˜1) as a function of the knowledge specificity score of each benchmark. Knowledge specificity is determined by GPT-4 ratings of 100 sampled entries per dataset, and error bars reflect the standard error of the mean. The positive correlation highlights the value of fetched memories for knowledge-intensive tasks. Note that this plot shows the improvement of fetched memories compared to generic memories; the improvement is even greater when comparing fetched memories with the baseline model without memory (row C1 inTable˜1).",
                "position": 2797
            },
            {
                "img": "https://arxiv.org/html/2510.02375/figures/knowledge_specificity_vs_gain_410m.png",
                "caption": "Figure 12:Fetched memories improve performance on knowledge-intensive benchmarks.Accuracy gain (fetched memory vs. generic memory) for the 410M model (row B2 inTable˜1) as a function of the knowledge specificity score of each benchmark. Knowledge specificity is determined by GPT-4 ratings of 100 sampled entries per dataset, and error bars reflect the standard error of the mean. The positive correlation highlights the value of fetched memories for knowledge-intensive tasks. Note that this plot shows the improvement of fetched memories compared to generic memories; the improvement is even greater when comparing fetched memories with the baseline model without memory (row B1 inTable˜1).",
                "position": 2800
            },
            {
                "img": "https://arxiv.org/html/2510.02375/figures/knowledge_specificity_vs_gain_160m.png",
                "caption": "Figure 13:Fetched memories improve performance on knowledge-intensive benchmarks.Accuracy gain (fetched memory vs. generic memory) for the 160M model (row A2 inTable˜1) as a function of the knowledge specificity score of each benchmark. Knowledge specificity is determined by GPT-4 ratings of 100 sampled entries per dataset, and error bars reflect the standard error of the mean. The positive correlation highlights the value of fetched memories for knowledge-intensive tasks. Note that this plot shows the improvement of fetched memories compared to generic memories; the improvement is even greater when comparing fetched memories with the baseline model without memory (row A1 inTable˜1).",
                "position": 2803
            }
        ]
    },
    {
        "header": "Appendix HRetrieval augmented generation detail",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02375/x5.png",
                "caption": "Figure 14:Base architecture of a transformer block with SwiGLU FFN layer.",
                "position": 2901
            },
            {
                "img": "https://arxiv.org/html/2510.02375/x6.png",
                "caption": "Figure 15:A transformer block with LoRa memories on queries and keys projection layers.",
                "position": 2904
            },
            {
                "img": "https://arxiv.org/html/2510.02375/x7.png",
                "caption": "Figure 16:A transformer block with LoRa memories on values and output projection layers.",
                "position": 2907
            },
            {
                "img": "https://arxiv.org/html/2510.02375/x8.png",
                "caption": "Figure 17:A transformer block with LoRa memories on SwiGLU-FFN linear layers.",
                "position": 2910
            },
            {
                "img": "https://arxiv.org/html/2510.02375/x9.png",
                "caption": "Figure 18:A transformer block with learned KV memories.",
                "position": 2913
            },
            {
                "img": "https://arxiv.org/html/2510.02375/x10.png",
                "caption": "Figure 19:A transformer block with FFN memories.",
                "position": 2916
            }
        ]
    },
    {
        "header": "Appendix IMemory augmented transformer architecture",
        "images": []
    }
]