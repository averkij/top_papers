[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.05178/extracted/6182739/figures/teaser.png",
                "caption": "Figure 1:State-of-the-art visual tokenizers excel at either understanding (high zero-shot accuracy,e.g.SigLIP[96]) or reconstruction (low reconstruction FID,e.g.MAGVIT2[93]), but not both.\nQLIP can perform well on both understanding and reconstruction with a marginal performance drop, opening up an opportunity for unified multi-modal understanding and generation.",
                "position": 100
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.05178/x1.png",
                "caption": "Figure 2:Overview.(a-b)Two-stage training pipeline of QLIP.(a)In Stage 1, we train QLIP with a combination of alignment loss and MSE loss.(b)In Stage 2, we drop the text encoder, freeze the visual encoder, and no longer optimize the contrastive loss.\nOnly the bottleneck quantizer and the decoder are fine-tuned.(c)With the text-aligned visual tokenizer, we transform the image into visual tokens, concatenate them with text tokens, and use an auto-regressive multi-modal model (Sec4.1) to model jointly.",
                "position": 163
            }
        ]
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Quantized Language-Image Pre-training",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.05178/x2.png",
                "caption": "Figure 3:Memory usage of QLIP.",
                "position": 276
            },
            {
                "img": "https://arxiv.org/html/2502.05178/x3.png",
                "caption": "Figure 4:Comparison of reconstruction results to the input image after the first and second stage.The second-stage model produces more high-frequency details.\nThe figure is best viewed on a PDF viewer with zoom-in.",
                "position": 287
            },
            {
                "img": "https://arxiv.org/html/2502.05178/x4.png",
                "caption": "Figure 5:Comparison of gradient magnitude.Here,ùíòùíò{\\bm{w}}bold_italic_wrefers to the linear layer in the visual encoder‚Äôs last MLP.",
                "position": 338
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.05178/x5.png",
                "caption": "Figure 6:Comparison of generated images with conditioning captions in the bottom.For each pair, the left is from LlamaGen+VQGAN and the right is from LlamaGen+QLIP-B/16 (ours).\nThe caption is also provided at the bottom.",
                "position": 1682
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BMore Results on QLIP",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.05178/x6.png",
                "caption": "Figure 7:Comparison of generated images with conditioning captions in the bottom.For each pair, the left is from LlamaGen+VQGAN and the right is from LlamaGen+QLIP-B/16 (ours).\nThe caption is also provided at the bottom.",
                "position": 3747
            }
        ]
    },
    {
        "header": "Appendix CMore Results on Generation Benchmarks",
        "images": []
    },
    {
        "header": "Appendix DMore Generation Results",
        "images": []
    }
]