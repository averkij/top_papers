[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26391/x1.png",
                "caption": "Figure 1:Illustration of cross-domain motion transfer.Our approach retrieves videos of people riding horses and transfers their motion priors to generate an astronaut riding a horse on the moon, while preserving the appearance characteristics of the input image.",
                "position": 94
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26391/x2.png",
                "caption": "Figure 2:Our MotionRAG framework.Text prompts retrieve relevant videos from a database. Motion information from these references are adapted to the input image via our Motion Context Transformer, then injected into an image-to-video generator to produce the final output.",
                "position": 131
            },
            {
                "img": "https://arxiv.org/html/2509.26391/x3.png",
                "caption": "Figure 3:Context-Aware Motion Adaptation (CAMA) architecture.Appearance and motion features from retrieved videos and the target image are processed through a causal transformer, which learns to predict appropriate motion features for the target image through in-context learning.",
                "position": 198
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26391/x4.png",
                "caption": "Figure 4:Qualitative comparison between baseline models and our retrieval-augmented approach across diverse scenarios.Our method generates more physically plausible and coherent motion, such as realistic object physics, natural animal/human movements, and corrects static or artifacts found in baseline models. Video results are available at ourwebsite",
                "position": 683
            }
        ]
    },
    {
        "header": "5Conclusion and Limitations",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BExtended Visualization Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26391/x5.png",
                "caption": "Figure 5:Retrieval and generation examples.Each panel shows a different scenario: (top-left) metal balls suspended in air with pendulum-like motion, (top-right) a person pouring water into a teacup, (bottom-left) a man running on a dirt road, and (bottom-right) a person riding on a horse led by another person. For each example, the top row displays frames from our generated video, while the rows below show frames from retrieved reference videos. Note how our system extracts relevant motion patterns from visually different but semantically similar videos.",
                "position": 1785
            },
            {
                "img": "https://arxiv.org/html/2509.26391/x6.png",
                "caption": "Figure 6:Additional video generation results.Each row displays five frames from a generated video sequence. The first four rows show results from CogVideoX+RAG, while the remaining rows present Dynamicrafter+RAG outputs. Our approach successfully captures motion characteristics across these diverse scenarios.",
                "position": 1813
            }
        ]
    },
    {
        "header": "NeurIPS Paper Checklist",
        "images": []
    }
]