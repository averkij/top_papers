[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16825/x1.png",
                "caption": "Figure 1:We orderly show (A) the procedure of preference data construction; (B) the conventional preference data construction strategy; (C) our exploration and proposed method.",
                "position": 201
            }
        ]
    },
    {
        "header": "2Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16825/x2.png",
                "caption": "(a)",
                "position": 243
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x2.png",
                "caption": "(a)",
                "position": 246
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x3.png",
                "caption": "",
                "position": 248
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x4.png",
                "caption": "(b)",
                "position": 255
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x5.png",
                "caption": "",
                "position": 257
            }
        ]
    },
    {
        "header": "3Case Study on Conventional Preference Data Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16825/x6.png",
                "caption": "(a)",
                "position": 355
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x6.png",
                "caption": "(a)",
                "position": 358
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x7.png",
                "caption": "",
                "position": 360
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x8.png",
                "caption": "(b)",
                "position": 367
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x9.png",
                "caption": "",
                "position": 369
            }
        ]
    },
    {
        "header": "4Preference Data Construction via Reward Distribution",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16825/x10.png",
                "caption": "Figure 4:Alpaca evaluation results. We report the length-controlled win rate for each preference dataset here. y-axis is the reward point where the rejected response is selected, while x-axis is the reward point where the chosen response is selected.",
                "position": 400
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x11.png",
                "caption": "",
                "position": 402
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x12.png",
                "caption": "",
                "position": 402
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x13.png",
                "caption": "",
                "position": 402
            },
            {
                "img": "https://arxiv.org/html/2502.16825/",
                "caption": "Figure 5:We record the training loss for six datasets (m⁢a⁢x𝑚𝑎𝑥maxitalic_m italic_a italic_x,m⁢i⁢n𝑚𝑖𝑛minitalic_m italic_i italic_n), (m⁢a⁢x𝑚𝑎𝑥maxitalic_m italic_a italic_x,μ−2⁢σ𝜇2𝜎\\mu-2\\sigmaitalic_μ - 2 italic_σ), (m⁢a⁢x𝑚𝑎𝑥maxitalic_m italic_a italic_x,μ−σ𝜇𝜎\\mu-\\sigmaitalic_μ - italic_σ), (m⁢a⁢x𝑚𝑎𝑥maxitalic_m italic_a italic_x,μ𝜇\\muitalic_μ), (m⁢a⁢x𝑚𝑎𝑥maxitalic_m italic_a italic_x,μ+σ𝜇𝜎\\mu+\\sigmaitalic_μ + italic_σ) and (m⁢a⁢x𝑚𝑎𝑥maxitalic_m italic_a italic_x,μ+2⁢σ𝜇2𝜎\\mu+2\\sigmaitalic_μ + 2 italic_σ) for Llama-3-8B-Instruct and Mistral-7B-Instruct-v0.2 every five steps. x-axis is the step and y-axis is the loss.",
                "position": 449
            }
        ]
    },
    {
        "header": "5Scaling Samples to Improve Alignment",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16825/x15.png",
                "caption": "(a)",
                "position": 528
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x15.png",
                "caption": "(a)",
                "position": 531
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x16.png",
                "caption": "",
                "position": 533
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x17.png",
                "caption": "(b)",
                "position": 540
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x18.png",
                "caption": "",
                "position": 542
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x19.png",
                "caption": "Figure 7:Alpaca evaluation results. We demonstrate the effectiveness of preference data construction strategy on Skywork reward model. x-axis is the number of sample (n𝑛nitalic_n), y-axis is the performance.",
                "position": 550
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16825/x20.png",
                "caption": "Figure 8:Loss of Meta-Llama-3-8B-Instruct when training with the data by selecting the response of maximal reward as the chosen and selecting the response of minimal reward as the rejected amongn𝑛nitalic_nresponses.",
                "position": 1721
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x21.png",
                "caption": "Figure 9:The average reward of 3 top ranking responses.",
                "position": 1727
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x22.png",
                "caption": "",
                "position": 1731
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x23.png",
                "caption": "Figure 10:Win rate results of Alpaca evaluation.",
                "position": 1735
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x24.png",
                "caption": "",
                "position": 1737
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x25.png",
                "caption": "",
                "position": 1737
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x26.png",
                "caption": "",
                "position": 1737
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]