[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16825/x1.png",
                "caption": "Figure 1:We orderly show (A) the procedure of preference data construction; (B) the conventional preference data construction strategy; (C) our exploration and proposed method.",
                "position": 201
            }
        ]
    },
    {
        "header": "2Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16825/x2.png",
                "caption": "(a)",
                "position": 243
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x2.png",
                "caption": "(a)",
                "position": 246
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x3.png",
                "caption": "",
                "position": 248
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x4.png",
                "caption": "(b)",
                "position": 255
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x5.png",
                "caption": "",
                "position": 257
            }
        ]
    },
    {
        "header": "3Case Study on Conventional Preference Data Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16825/x6.png",
                "caption": "(a)",
                "position": 355
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x6.png",
                "caption": "(a)",
                "position": 358
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x7.png",
                "caption": "",
                "position": 360
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x8.png",
                "caption": "(b)",
                "position": 367
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x9.png",
                "caption": "",
                "position": 369
            }
        ]
    },
    {
        "header": "4Preference Data Construction via Reward Distribution",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16825/x10.png",
                "caption": "Figure 4:Alpaca evaluation results. We report the length-controlled win rate for each preference dataset here. y-axis is the reward point where the rejected response is selected, while x-axis is the reward point where the chosen response is selected.",
                "position": 400
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x11.png",
                "caption": "",
                "position": 402
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x12.png",
                "caption": "",
                "position": 402
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x13.png",
                "caption": "",
                "position": 402
            },
            {
                "img": "https://arxiv.org/html/2502.16825/",
                "caption": "Figure 5:We record the training loss for six datasets (mâ¢aâ¢xğ‘šğ‘ğ‘¥maxitalic_m italic_a italic_x,mâ¢iâ¢nğ‘šğ‘–ğ‘›minitalic_m italic_i italic_n), (mâ¢aâ¢xğ‘šğ‘ğ‘¥maxitalic_m italic_a italic_x,Î¼âˆ’2â¢Ïƒğœ‡2ğœ\\mu-2\\sigmaitalic_Î¼ - 2 italic_Ïƒ), (mâ¢aâ¢xğ‘šğ‘ğ‘¥maxitalic_m italic_a italic_x,Î¼âˆ’Ïƒğœ‡ğœ\\mu-\\sigmaitalic_Î¼ - italic_Ïƒ), (mâ¢aâ¢xğ‘šğ‘ğ‘¥maxitalic_m italic_a italic_x,Î¼ğœ‡\\muitalic_Î¼), (mâ¢aâ¢xğ‘šğ‘ğ‘¥maxitalic_m italic_a italic_x,Î¼+Ïƒğœ‡ğœ\\mu+\\sigmaitalic_Î¼ + italic_Ïƒ) and (mâ¢aâ¢xğ‘šğ‘ğ‘¥maxitalic_m italic_a italic_x,Î¼+2â¢Ïƒğœ‡2ğœ\\mu+2\\sigmaitalic_Î¼ + 2 italic_Ïƒ) for Llama-3-8B-Instruct and Mistral-7B-Instruct-v0.2 every five steps. x-axis is the step and y-axis is the loss.",
                "position": 449
            }
        ]
    },
    {
        "header": "5Scaling Samples to Improve Alignment",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16825/x15.png",
                "caption": "(a)",
                "position": 528
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x15.png",
                "caption": "(a)",
                "position": 531
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x16.png",
                "caption": "",
                "position": 533
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x17.png",
                "caption": "(b)",
                "position": 540
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x18.png",
                "caption": "",
                "position": 542
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x19.png",
                "caption": "Figure 7:Alpaca evaluation results. We demonstrate the effectiveness of preference data construction strategy on Skywork reward model. x-axis is the number of sample (nğ‘›nitalic_n), y-axis is the performance.",
                "position": 550
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16825/x20.png",
                "caption": "Figure 8:Loss of Meta-Llama-3-8B-Instruct when training with the data by selecting the response of maximal reward as the chosen and selecting the response of minimal reward as the rejected amongnğ‘›nitalic_nresponses.",
                "position": 1721
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x21.png",
                "caption": "Figure 9:The average reward of 3 top ranking responses.",
                "position": 1727
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x22.png",
                "caption": "",
                "position": 1731
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x23.png",
                "caption": "Figure 10:Win rate results of Alpaca evaluation.",
                "position": 1735
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x24.png",
                "caption": "",
                "position": 1737
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x25.png",
                "caption": "",
                "position": 1737
            },
            {
                "img": "https://arxiv.org/html/2502.16825/x26.png",
                "caption": "",
                "position": 1737
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]