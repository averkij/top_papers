[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.02878/x1.png",
                "caption": "Figure 1:Self-taught lookahead self-improves the value model by learning from state-transition dynamics. During the data generation phase (top left), tree search is used to discover diverse states. For every observed states𝑠sitalic_sencountered during the search, successor states are expanded using base policyπθsubscript𝜋𝜃\\pi_{\\theta}italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPTand the current value modelVϕksubscript𝑉subscriptitalic-ϕ𝑘V_{\\phi_{k}}italic_V start_POSTSUBSCRIPT italic_ϕ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT, and a formatted textual training example is formed using verbal representations of the next best action and successor state, as well asVϕksubscript𝑉subscriptitalic-ϕ𝑘V_{\\phi_{k}}italic_V start_POSTSUBSCRIPT italic_ϕ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT’s outputted value reasoning (r𝑟ritalic_r) and numerical value (v𝑣vitalic_v) (top middle). These examples are used to fine-tuneVϕk+1subscript𝑉subscriptitalic-ϕ𝑘1V_{\\phi_{k+1}}italic_V start_POSTSUBSCRIPT italic_ϕ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT, which will be used in the next iteration of the algorithm (top right). Value models learned during self-taught lookahead can be used to evaluate unseen states encountered during search on unseen tasks by simulating a step of lookahead including the next best action and the best successor states~′superscript~𝑠′\\tilde{s}^{\\prime}over~ start_ARG italic_s end_ARG start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT(bottom).",
                "position": 154
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Better State-Value Estimation with Self-Taught Lookahead",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.02878/x2.png",
                "caption": "Figure 2:Breadth-first search performance onGame-of-24task on sets of tasks both seen and unseen during the self-improvement.",
                "position": 791
            },
            {
                "img": "https://arxiv.org/html/2503.02878/x3.png",
                "caption": "Figure 3:Compute and environmental efficiency during evaluation onWebShopwith agpt-3.5-turbopolicy. Compute efficiency is measured in total (prompt and completion) tokens broken down by model type (closed and open source). Environmental efficiency is measured by the number of states expanded (webpages visited).",
                "position": 814
            }
        ]
    },
    {
        "header": "5Efficiency Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.02878/x4.png",
                "caption": "Figure 4:Tradeoff between performance and efficiency onWebShopwith agpt-3.5-turbopolicy. Pareto frontiers of existing methods and baselines are shown, illustrating the optimality of STL when considering the tradeoff between cost and average reward(top), environmental usage and average reward(middle), and cost and success rate(bottom).",
                "position": 832
            },
            {
                "img": "https://arxiv.org/html/2503.02878/x5.png",
                "caption": "Figure 5:STL scaling trends onWebShopforllama-3andqwen-2.5model families when using agpt-3.5-turbopolicy with performance measured by average reward(top)and success rate(bottom).",
                "position": 854
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASelf-Taught Lookahead Algorithm",
        "images": []
    },
    {
        "header": "Appendix BSelf-Taught Lookahead onWebShop",
        "images": []
    },
    {
        "header": "Appendix CMath Tasks",
        "images": []
    },
    {
        "header": "Appendix DModel Fine-tuning and Serving",
        "images": []
    },
    {
        "header": "Appendix ESignificance Testing",
        "images": []
    }
]