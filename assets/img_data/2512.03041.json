[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03041/x1.png",
                "caption": "",
                "position": 88
            },
            {
                "img": "https://arxiv.org/html/2512.03041/x2.png",
                "caption": "Figure 2:Overview of MultiShotMaster.We extend a pretrained single-shot T2V model by two key RoPE variants: Multi-Shot Narrative RoPE for flexible shot arrangement with temporal narrative order, and Spatiotemporal Position-Aware RoPE for grounded reference injection. To manage in-context information flows, we design a Multi-Shot & Multi-Reference Attention Mask. We finetune temporal attention, cross attention and FFN, leveraging the intrinsic architectural properties to achieve flexible and controllable multi-shot video generation.",
                "position": 112
            },
            {
                "img": "https://arxiv.org/html/2512.03041/x3.png",
                "caption": "Figure 3:Data Curation Pipeline:(1) We employ a shot transition detection model[43]to cut the collected long videos into short clips, use a scene segmentation model[54]to cluster clips within the same scene, and then sample multi-shot videos. (2) We introduce a hierarchical caption structure and use Gemini-2.5[10]in a two-stage process to produce global caption and per-shot captions. (3) We integrate YOLOv11[23], ByteTrack[66]and SAM[25]to detect, track and segment subject images. Then we use Gemini-2.5 to merge the per-shot tracking results by subject appearance. We obtain clean backgrounds by using OmniEraser[52].",
                "position": 173
            },
            {
                "img": "https://arxiv.org/html/2512.03041/x4.png",
                "caption": "Figure 4:Qualitative Comparisons.We compare with two multi-shot video generation methods[57,46]in the upper part, and two single-shot reference-to-video methods[21,29]under multi-shot setting in the lower part. [ ] denotes the placeholder of character descriptions for baselines. The character introductions of the bottom part are omitted for brevity.",
                "position": 242
            },
            {
                "img": "https://arxiv.org/html/2512.03041/x5.png",
                "caption": "Figure 5:Limitation visualization. We only explicitly control the subject motion, while the camera position is controlled by text prompts, which might cause the motion coupling issue.",
                "position": 402
            },
            {
                "img": "https://arxiv.org/html/2512.03041/x6.png",
                "caption": "Figure 6:Prompts of labeling global caption and per-shot captions. We first label the global caption by sampling frames from the input multi-shot video. Then we label the per-shot caption one by one.",
                "position": 1744
            },
            {
                "img": "https://arxiv.org/html/2512.03041/x7.png",
                "caption": "Figure 7:Multi-shot video data example. By employing Gemini-2.5[10]with the carefully-designed prompts as shown in Fig.6, the labeled subjects could be consistent in global and per-shot captions.",
                "position": 1747
            },
            {
                "img": "https://arxiv.org/html/2512.03041/x8.png",
                "caption": "Figure 8:By employing Gemini-2.5[10]to group the subject images, we obtain complete multi-shot tracking results.",
                "position": 1750
            },
            {
                "img": "https://arxiv.org/html/2512.03041/x9.png",
                "caption": "Figure 9:We require Gemini-2.5[10]to strictly adhere to cinematic narrative logic and scrutinize cross-shot content across four core dimensions: Scene Consistency, Subject Consistency, Action Coherence, and Spatial Consistency.",
                "position": 1753
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]