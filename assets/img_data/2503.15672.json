[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15672/x1.png",
                "caption": "Figure 1:GASPlearns a structured, generalizable representation of the environment and its evolution and can be further trained to perform well on downstream AD tasks.\nWe outperform SotA¬†pre-trainingUnO[2]across the board, especially on primarily semantic tasks like map segmentation.No pre-trainingis displayed for reference. Downstream tasks requiring additional labels are post-trained using 1000 samples (‚àºsimilar-to\\sim‚àº1% of pre-training scale).",
                "position": 114
            }
        ]
    },
    {
        "header": "2Related work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15672/x2.png",
                "caption": "Figure 2:Overview of GASP. Past lidar¬†scans are encoded into a BEV feature map. These features are used by implicit decoders to predict DINOv2 featuresùíü^^ùíü\\hat{\\mathcal{D}}over^ start_ARG caligraphic_D end_ARG, occupancyùí™^^ùí™\\hat{\\mathcal{O}}over^ start_ARG caligraphic_O end_ARG, and ego-path‚Ñ∞^^‚Ñ∞\\hat{\\mathcal{E}}over^ start_ARG caligraphic_E end_ARGat the query pointsùí¨ùí¨\\mathcal{Q}caligraphic_Qgenerated from future sensor data during pre-training.\nWe also show that the learned representation is useful when transferred to an array of downstream AD tasks.",
                "position": 167
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15672/x3.png",
                "caption": "Figure 3:Predicted occupancy (colored by depth and height respectively) and DINOv2 features (mapped to RGB using the three most important features) projected into camera views, as well as a holistic view from slightly above and behind the ego vehicle. Different type of objects such as road, vehicles, buildings, and trees have different features, indicating the model has semantic understanding of the objects in the scene.\nThe injected white box represents the ego vehicle for clarity.",
                "position": 329
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15672/x4.png",
                "caption": "",
                "position": 382
            },
            {
                "img": "https://arxiv.org/html/2503.15672/x5.png",
                "caption": "",
                "position": 386
            },
            {
                "img": "https://arxiv.org/html/2503.15672/x6.png",
                "caption": "Figure 4:Predicted future VLM features from a Bird‚Äôs Eye View. The model correctly predicts the car taking a right turn as well as those going straight through the crossing.",
                "position": 391
            },
            {
                "img": "https://arxiv.org/html/2503.15672/x7.png",
                "caption": "(a)Frozen () encoder.",
                "position": 419
            },
            {
                "img": "https://arxiv.org/html/2503.15672/x7.png",
                "caption": "(a)Frozen () encoder.",
                "position": 422
            },
            {
                "img": "https://arxiv.org/html/2503.15672/x10.png",
                "caption": "(b)Unfrozen () encoder.",
                "position": 430
            },
            {
                "img": "https://arxiv.org/html/2503.15672/x12.png",
                "caption": "",
                "position": 432
            },
            {
                "img": "https://arxiv.org/html/2503.15672/x13.png",
                "caption": "(a)Frozen () encoder.",
                "position": 458
            },
            {
                "img": "https://arxiv.org/html/2503.15672/x13.png",
                "caption": "(a)Frozen () encoder.",
                "position": 461
            },
            {
                "img": "https://arxiv.org/html/2503.15672/x15.png",
                "caption": "(b)Unfrozen () encoder.",
                "position": 469
            },
            {
                "img": "https://arxiv.org/html/2503.15672/x17.png",
                "caption": "Figure 7:Ego path in a crossing, colored by distance to ego-vehicle.\nLidar¬†point cloud (grey) and true ego path (dashed line) are displayed for reference.\nAt timet0subscriptùë°0t_{0}italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTGASP¬†predicts multiple possible modes (A, B), and once it is no longer probable to continue towards A (att+subscriptùë°t_{+}italic_t start_POSTSUBSCRIPT + end_POSTSUBSCRIPT), the predictions collapse to only one mode.",
                "position": 498
            },
            {
                "img": "https://arxiv.org/html/2503.15672/x9.png",
                "caption": "Table 1:Ego-trajectory prediction the full Argoverse 2 sensor dataset, using frozen () and unfrozen () encoders.",
                "position": 525
            },
            {
                "img": "https://arxiv.org/html/2503.15672/x18.png",
                "caption": "Table 2:Ablation over the pre-training components ofGASP.\nWe show performance directly obtained from pre-training (4D occ. P@R70) and its generalization to downstream tasks (Semantic BEV Forecasting and Map Segmentation) when finetuned on different amounts of labeled samples.\nWe ablate each component added toUnOwith the sensor encoder frozen () and unfrozen () as well as performance withno pre-training.",
                "position": 613
            },
            {
                "img": "https://arxiv.org/html/2503.15672/x19.png",
                "caption": "",
                "position": 634
            },
            {
                "img": "https://arxiv.org/html/2503.15672/x22.png",
                "caption": "Figure 8:Effect of pre-training with/without missing rays. Without missing rays we observe artifacts in regions where the model is never supervised. Using missing rays as unoccupied supervision, these artifacts are greatly reduced. Geometries are colored by height; blue down and red up.",
                "position": 708
            },
            {
                "img": "https://arxiv.org/html/2503.15672/x23.png",
                "caption": "Figure 9:Scaling properties. We vary the number of pre-training samples and evaluate performance (red‚Üë‚Üë\\uparrow‚Üë) and generalization (blue‚Üë‚Üë\\uparrow‚Üë), demonstrating GASP‚Äôs remarkably predictable, logarithmic, scaling behavior.",
                "position": 718
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "GASP: Unifying Geometric and Semantic Self-SupervisedPre-trained for Autonomous Driving",
        "images": []
    },
    {
        "header": "Appendix ABaseline reimplementation",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15672/x24.png",
                "caption": "Figure 10:Semantic forecasting AP across number of training samples for our reimplemented baseline (solid) and the results reported in[2](dashed).",
                "position": 1646
            }
        ]
    },
    {
        "header": "Appendix BAdditional results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15672/x9.png",
                "caption": "Table 6:Semantic BEV forecasting performance (Soft-IoU) for GASP and UnO across different number of fine-tuning samples with frozen () and unfrozen () sensor encoder.",
                "position": 1714
            },
            {
                "img": "https://arxiv.org/html/2503.15672/x9.png",
                "caption": "Table 7:BEV semantic forecasting performance (AP) showed across number of fine-tuning samples with frozen () and unfrozen () sensor encoder.",
                "position": 1785
            },
            {
                "img": "https://arxiv.org/html/2503.15672/x9.png",
                "caption": "Table 8:Map segmentation mIoU (mean and std. dev) across a number of labeled samples with frozen () and unfrozen () sensor encoder.\nGASP¬† outperforms the baseline across all amounts of training samples indicating that the BEV features contain a richer BEV representation.",
                "position": 1860
            },
            {
                "img": "https://arxiv.org/html/2503.15672/x25.png",
                "caption": "Table 9:Average Precision (AP) for Semantic 4D Occupancy Forecasting. We evaluate performance across different numbers of fine-tuning samples and report results for vehicle segmentation.",
                "position": 1944
            }
        ]
    },
    {
        "header": "Appendix CAdditional ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15672/x26.png",
                "caption": "Table 10:Performance across different downstream task when varying the number of DINOv2 dimensions used in the regression objective.",
                "position": 2004
            },
            {
                "img": "https://arxiv.org/html/2503.15672/x27.png",
                "caption": "",
                "position": 2071
            },
            {
                "img": "https://arxiv.org/html/2503.15672/x28.png",
                "caption": "Figure 11:Occupancy and Dino features projected into camera view. Note that the white-box representing ego vehicle has been injected for illustrative purposes.",
                "position": 2309
            },
            {
                "img": "https://arxiv.org/html/2503.15672/x29.png",
                "caption": "Figure 12:Occupancy and Dino features projected into a camera view, a holistic view, and a bird‚Äôs-eye view.",
                "position": 2312
            },
            {
                "img": "https://arxiv.org/html/2503.15672/x30.png",
                "caption": "Figure 13:Dino features and ego path in a three-way intersection.",
                "position": 2315
            },
            {
                "img": "https://arxiv.org/html/2503.15672/x31.png",
                "caption": "Figure 14:BEV segmentation forecasting results showing A) a typical scenario, B) a scenario with uncommon road users (in this case an excavator), and C) a more complex scenario. An unfrozen GASP¬†representation with 100 samples available in the post-training task is used.",
                "position": 2318
            },
            {
                "img": "https://arxiv.org/html/2503.15672/x32.png",
                "caption": "Figure 15:Prediction results from the map segmentation post-training task of GASP¬†with frozen encoder. Note that predictions are only made based on lidar input. Camera images are only provided as visual clarity for the reader regarding what scene is being predicted.",
                "position": 2321
            },
            {
                "img": "https://arxiv.org/html/2503.15672/x33.png",
                "caption": "Figure 16:Ego-trajectory prediction results using a frozen GASP¬†representation. Expert trajectories, the groundtruth, are shown in green while predictions are shown in blue. Note that camera inputs are only provided as visual support for the reader and are not part of the prediction.",
                "position": 2324
            },
            {
                "img": "https://arxiv.org/html/2503.15672/extracted/6268688/assets/qual_examples/super_combined_9a448a80-0e9a-3bf0-90f3-21750dfef55a_315975813859980000_00.png",
                "caption": "Figure 17:Ego path along with the first and second three most important features, highlighting the complementing aiding properties of the ego path task and the DINO feature prediction task in encoding information about drivable area in the representation.",
                "position": 2328
            },
            {
                "img": "https://arxiv.org/html/2503.15672/extracted/6268688/assets/qual_examples/man_with_bag_v2.png",
                "caption": "Figure 18:A qualitative example of the feature-level information predicted by the representation produced by GASP, showcasing its capability of contrasting otherwise diffuse scene elements such as the lane-dividers (marked A.) or the person carrying a bag (marked B.) from the background.",
                "position": 2331
            }
        ]
    },
    {
        "header": "Appendix DAdditional visualizations",
        "images": []
    }
]