[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.05541/extracted/6347039/figs/teaser.png",
                "caption": "Figure 1:Comparison of video captioning approaches: Vanilla (top-left), Dense (top-right), Dense Object (bottom-left), and our CAT-V framework (bottom-right) with integrated modules for user-controlled object-centric captioning via integrated modules (Segmenter, Temporal Analyzer, Captioner with CoT reasoning).",
                "position": 74
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.05541/extracted/6347039/figs/framework.png",
                "caption": "Figure 2:CAT-V consists of three modules: Segmenter, Temporal Analyzer, and Captioner. The Segmenter precisely segments objects in video frames using user-defined prompts (points, bounding boxes, or regions). The Temporal Analyzer captures video dynamics hierarchically. The Captioner creates object-centric captions using upstream information and CoT reasoning.",
                "position": 125
            }
        ]
    },
    {
        "header": "2CAT-V: Caption Anything in Video",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.05541/extracted/6347039/figs/diff_object.png",
                "caption": "Figure 3:CAT-V can focus on different objects within the same video. The top sequence shows object-centric captioning for a horse, while the bottom sequence demonstrates captioning for the cowboy, each with precise temporal segmentation of their respective actions and states.",
                "position": 203
            },
            {
                "img": "https://arxiv.org/html/2504.05541/extracted/6347039/figs/diff_vprompt.png",
                "caption": "Figure 4:Examples of CAT-V’s support for various visual prompting formats. The system effectively handles points, bounding boxes, and irregular regions to identify and track diverse objects including pandas, birds, bottles, and people, demonstrating its flexibility and accuracy in accommodating different user input preferences.",
                "position": 206
            },
            {
                "img": "https://arxiv.org/html/2504.05541/extracted/6347039/figs/diff_vid_prompt.png",
                "caption": "Figure 5:Comparison of different visual prompt styles (Bounding Box, Blur, Circle, Color Block, Halo, Mask, and Polygon) for highlighting a blue plastic cup, demonstrating their effects on object-centric captioning accuracy.",
                "position": 209
            },
            {
                "img": "https://arxiv.org/html/2504.05541/extracted/6347039/figs/language_prompt.png",
                "caption": "Figure 6:Comparison of object-centric video captioning using CAT-V with CoT prompting, without CoT prompting, and using only the Temporal Analyzer.",
                "position": 212
            },
            {
                "img": "https://arxiv.org/html/2504.05541/extracted/6347039/figs/chatting.png",
                "caption": "Figure 7:Example of object-centric multi-round chatting with CAT-V, demonstrating the system’s ability to maintain reference to the highlighted object while answering specific questions about its attributes and actions.",
                "position": 215
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": []
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]