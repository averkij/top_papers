[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07310/x1.png",
                "caption": "",
                "position": 199
            },
            {
                "img": "https://arxiv.org/html/2510.07310/x2.png",
                "caption": "Figure 1:Teaser:We reveal how video diffusion transformers (DiTs) represent multi-instance or subject-object interactions during video generation. Building on this, our MATRIX framework further enhances the interaction-awareness of video DiTs via the proposed Semantic Grounding Alignment (SGA,‚ÑíSGA\\mathcal{L}_{\\mathrm{SGA}}) and Semantic Propagation Alignment (SPA,‚ÑíSPA\\mathcal{L}_{\\mathrm{SPA}}) losses.",
                "position": 219
            },
            {
                "img": "https://arxiv.org/html/2510.07310/x3.png",
                "caption": "Figure 2:Failure cases of existing video DiTs:(a) semantic grounding failures, where subjects, objects, or their verb relations are mismatched, and (b) semantic propagation failures, where bindings break over time, leading to hallucinations or duplications. Overlays indicate the intended instances.",
                "position": 222
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07310/x4.png",
                "caption": "Figure 3:Attention maps per token type.Noun tokens (subject, object) align with their respective regions (e.g., layer 11); verb tokens aligns with the union of subject‚Äìobject regions (e.g., layer 7).",
                "position": 245
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3MATRIX-11K Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07310/x5.png",
                "caption": "Figure 4:Our dataset curation pipeline.An LLM identifies interaction triplets, filters them using Dynamism and Contactness, and extracts per-ID appearance descriptions (Sec.3.1). A VLM then verifies candidate to select an anchor frame, from which SAM2 propagates masks to produce instance mask tracksMkM_{k}. We drop instances and related interactions that fail verification or propagation (Sec.3.2)",
                "position": 312
            }
        ]
    },
    {
        "header": "4Interaction-Awareness Analysis in Video DiTs",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07310/x6.png",
                "caption": "Figure 5:Illustration of full 3D attention in video DiTs.",
                "position": 349
            },
            {
                "img": "https://arxiv.org/html/2510.07310/x7.png",
                "caption": "Figure 6:Layer Analysis.(a)Influential layers: layers with high AAS that rank in the Top-10 for videos. Circles denote magnitude (mean AAS) and squares denote frequency; green circles / blue squares mark top-10 layers by magnitude / frequency, and yellow circles / red squares are the remainder.\n(b)Dominant layers: the influential layers whose mean AAS clearly separates successes from failures. (Best viewed in zoom.)",
                "position": 416
            }
        ]
    },
    {
        "header": "5MATRIX Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07310/x8.png",
                "caption": "Figure 7:Main Architecture of MATRIX.",
                "position": 500
            },
            {
                "img": "https://arxiv.org/html/2510.07310/x9.png",
                "caption": "Figure 8:Qualitative Comparison.",
                "position": 533
            }
        ]
    },
    {
        "header": "6Experiments",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ADataset Curation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07310/x10.png",
                "caption": "Figure 9:Effects of Our VLM Verification.The first row (GroundingDINO with class) and the second row (GroundingDINO with appearance) often pick a wrong same-class instance. The third row (Ours) verifies candidates with a VLM and keeps exactly one box per noun, resolving (a) to (d). Best viewed in zoom.",
                "position": 2003
            },
            {
                "img": "https://arxiv.org/html/2510.07310/x11.png",
                "caption": "Figure 10:Dataset Statistics.(a) Source composition from two primary dataset. (b) Joint distributions ofContactandDynamism, covering the full spectrum with a tilt toward contact-heavy, high motion events. (c) Per-clip counts of interaction triplets and tracked instance IDs; most clips are modest in complexity, motivating a fixed track budget of|ùí¶|=5|\\mathcal{K}|=5. (d) Frequency of interaction verbs and (e) frequency of instance nouns, indicating broad lexical coverage.",
                "position": 2019
            }
        ]
    },
    {
        "header": "Appendix BAnalysis Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07310/x12.png",
                "caption": "Figure 11:Analysis Dataset Pairs Example.",
                "position": 2037
            },
            {
                "img": "https://arxiv.org/html/2510.07310/x13.png",
                "caption": "Figure 12:Attention Map Details for Grounding and Propagation.",
                "position": 2077
            },
            {
                "img": "https://arxiv.org/html/2510.07310/x14.png",
                "caption": "Figure 13:Visualization of Propagation Attention Maps.(a) In human-rated success cases, per-query propagation attention map remains stable across frames. (b) In human-rated failure cases, the maps are unstable and drift over time. This is in line with human judgments, supporting propagation attention map as a meaningful signal for interaction fidelity.",
                "position": 2100
            },
            {
                "img": "https://arxiv.org/html/2510.07310/x15.png",
                "caption": "Figure 14:Layer-wise Comparison of Semantic Grounding Maps.",
                "position": 2109
            }
        ]
    },
    {
        "header": "Appendix CMATRIX Framework Details",
        "images": []
    },
    {
        "header": "Appendix DTraining-free Cross-Modal Guidance Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07310/x16.png",
                "caption": "Figure 15:Guidance Mechanism Details.(a) Cross-Attention Guidance (CAG) zeros out cross-frame video-to-video (v2v) attention. (b) Cross-Modal Guidance (CMG) zeros out cross-modal attentions, including text-to-video (t2v) and video-to-text (v2t) attention. The mechanisms differ in the location of zeroing.",
                "position": 2154
            },
            {
                "img": "https://arxiv.org/html/2510.07310/x17.png",
                "caption": "Figure 16:Effects of Guidance.",
                "position": 2200
            }
        ]
    },
    {
        "header": "Appendix EEvaluation Protocol Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07310/x18.png",
                "caption": "Figure 17:Limitations of Existing Semantic Alignment Metrics using BLEU and CLIP.(a) Cross-model comparison: despite clear human preference for one model, BLEU and CLIP favor the other, assigning high scores to implausible or semantically misaligned results. (b) Within-model frames: frames preferred by humans receive lower scores than other frames from the same clip, showing insensitivity to instance-level grounding and temporal consistency. This gap motivates InterGenEval, an interaction-aware evaluation.",
                "position": 2294
            },
            {
                "img": "https://arxiv.org/html/2510.07310/x19.png",
                "caption": "Figure 18:InterGenEval Pipeline.",
                "position": 2310
            }
        ]
    },
    {
        "header": "Appendix FEvaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07310/x20.png",
                "caption": "Figure 19:Human Evaluation Results.",
                "position": 2419
            }
        ]
    },
    {
        "header": "Appendix GAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07310/x21.png",
                "caption": "Figure 20:Limitations of VBench metrics.",
                "position": 2563
            }
        ]
    },
    {
        "header": "Appendix HLimitations and Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07310/x22.png",
                "caption": "Figure 21:Prompt Design for Interaction Identification and Instance Assignment.",
                "position": 2591
            },
            {
                "img": "https://arxiv.org/html/2510.07310/x23.png",
                "caption": "Figure 22:Prompt Design for Interaction Scoring and Filtering.",
                "position": 2594
            },
            {
                "img": "https://arxiv.org/html/2510.07310/x24.png",
                "caption": "Figure 23:Prompt Design for Instance Description Extraction.",
                "position": 2597
            },
            {
                "img": "https://arxiv.org/html/2510.07310/x25.png",
                "caption": "Figure 24:Prompt Design for Vision-Language Verification.",
                "position": 2600
            },
            {
                "img": "https://arxiv.org/html/2510.07310/x26.png",
                "caption": "Figure 25:Dataset Examples.",
                "position": 2603
            },
            {
                "img": "https://arxiv.org/html/2510.07310/x27.png",
                "caption": "Figure 26:Additional Dataset Example.",
                "position": 2607
            },
            {
                "img": "https://arxiv.org/html/2510.07310/x28.png",
                "caption": "Figure 27:Prompt design for KISA and SGI in Evaluation Protocol.",
                "position": 2611
            },
            {
                "img": "https://arxiv.org/html/2510.07310/x29.png",
                "caption": "Figure 28:Prompt design for SPI in Evaluation Protocol.",
                "position": 2614
            },
            {
                "img": "https://arxiv.org/html/2510.07310/x30.png",
                "caption": "Figure 29:Generated Evaluation Dataset Pairs Example.",
                "position": 2617
            },
            {
                "img": "https://arxiv.org/html/2510.07310/x31.png",
                "caption": "Figure 30:Sampled Evaluation Dataset Pairs Example.",
                "position": 2620
            },
            {
                "img": "https://arxiv.org/html/2510.07310/x32.png",
                "caption": "Figure 31:An example of human evaluation.",
                "position": 2623
            },
            {
                "img": "https://arxiv.org/html/2510.07310/x33.png",
                "caption": "Figure 32:Additional Qualitative Results.",
                "position": 2627
            },
            {
                "img": "https://arxiv.org/html/2510.07310/x34.png",
                "caption": "Figure 33:Additional Qualitative Results.",
                "position": 2631
            }
        ]
    },
    {
        "header": "Appendix IThe Use of Large Language Models (LLMs)",
        "images": []
    }
]