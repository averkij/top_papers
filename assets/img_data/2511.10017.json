[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.10017/x1.png",
                "caption": "Figure 1:We propose fine-grained 3D embodied reasoning: given a 3D scene and a language task instruction, the agent must identify relevant affordance elements and predict a structured triplet for each: its 3D mask, motion type, and motion axis direction.",
                "position": 65
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.10017/x2.png",
                "caption": "Figure 2:AffordBot Overview.\nOur method first constructs a holistic multimodal representation designed to bridge 3D scenes with 2D-native MLLMs. This process involves view synthesis, extraction of geometric-semantic descriptors, and their association. Then, our designed Chain-of-Thought (CoT) paradigm guides the MLLM to ultimately predict a structured triplet for the task.",
                "position": 118
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.10017/x3.png",
                "caption": "(a)Lack of Context.",
                "position": 142
            },
            {
                "img": "https://arxiv.org/html/2511.10017/x3.png",
                "caption": "(a)Lack of Context.",
                "position": 145
            },
            {
                "img": "https://arxiv.org/html/2511.10017/x4.png",
                "caption": "(b)Incomplete target coverage.",
                "position": 151
            },
            {
                "img": "https://arxiv.org/html/2511.10017/x5.png",
                "caption": "Figure 4:AffordBot’s Chain-of-Thought Pipeline for Embodied Reasoning.This structured observe-then-infer process leverages multimodal inputs to perform: (1) Active View Selection to identify the most informative view, which may involve zooming in to better see the details of the images, followed by (2) Affordance Grounding to localize target elements, and finally (3) Motion Estimation to infer the required action details.",
                "position": 246
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.10017/x6.png",
                "caption": "Figure 5:Qualitative Results.The figure showcases visual examples of AffordBot performing fine-grained grounding. The illustrated examples include:\n(1)“Turn on the TV using the remote control on the table.”(2)“Open the middle drawer of the TV stand.”(3)“Close the bedroom door.”(4)“Open the window above the radiator”.Please zoom in digitally to view more details.",
                "position": 794
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AOverview",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    },
    {
        "header": "Appendix CAdditional Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.10017/x7.png",
                "caption": "Figure 6:Additional qualitative results on our fine-grained embodied reasoning task.",
                "position": 1907
            }
        ]
    },
    {
        "header": "Appendix DLimitations and Future Directions",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.10017/x8.png",
                "caption": "(a)Segmentation Failure: Affordance segmentation misses the bottom drawer, preventing it from subsequent reasoning about the corresponding instruction.",
                "position": 1923
            },
            {
                "img": "https://arxiv.org/html/2511.10017/x8.png",
                "caption": "(a)Segmentation Failure: Affordance segmentation misses the bottom drawer, preventing it from subsequent reasoning about the corresponding instruction.",
                "position": 1926
            },
            {
                "img": "https://arxiv.org/html/2511.10017/x9.png",
                "caption": "(b)Viewpoint Limitation: Improper observation position induces occlusion, resulting in partial or ambiguous scene observation.",
                "position": 1932
            },
            {
                "img": "https://arxiv.org/html/2511.10017/x10.png",
                "caption": "Figure 8:Chain-of-thought prompt for active view selection.",
                "position": 1939
            },
            {
                "img": "https://arxiv.org/html/2511.10017/x10.png",
                "caption": "Figure 8:Chain-of-thought prompt for active view selection.",
                "position": 1942
            },
            {
                "img": "https://arxiv.org/html/2511.10017/x11.png",
                "caption": "Figure 9:Chain-of-thought prompt for affordance grounding.",
                "position": 1948
            },
            {
                "img": "https://arxiv.org/html/2511.10017/x12.png",
                "caption": "Figure 10:Chain-of-thought prompt for motion estimation.",
                "position": 1955
            }
        ]
    },
    {
        "header": "Appendix EBroader Impact",
        "images": []
    },
    {
        "header": "Appendix FLicenses for Used Assets",
        "images": []
    }
]