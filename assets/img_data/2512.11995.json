[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.11995/x1.png",
                "caption": "Figure 1:Overview of Chain-of-Questions (CoQ). The left represents the manually formulated ground truth QA chain. The middle represents thePlanningtask, evaluating the model’s capability in selecting sub-questions that are helpful to answer the original question. The right represents theFollowingtask, evaluating the model’s capability in answering each sub-question.",
                "position": 133
            },
            {
                "img": "https://arxiv.org/html/2512.11995/x2.png",
                "caption": "Figure 2:An example fromV-REXwith correspondingplanningandfollowingtasks.In theplanningtask, the model is given the original question and asked to select a sub-question in each step that is necessary and helpful for solving the original question.\nIn thefollowingtask, the model is asked to answer the ground truth sub-questions step-by-step.",
                "position": 136
            },
            {
                "img": "https://arxiv.org/html/2512.11995/x3.png",
                "caption": "Figure 3:Scenarios inV-REX.\nWith various samples,V-REXspans1515real-world scenarios across44reasoning categories (Deduction,Guessing,Navigation, &Retrieval), covering diverse settings such as diagrams, time estimation, GUI interpretation, and others.",
                "position": 148
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Exploratory Reasoning in Visual Space",
        "images": []
    },
    {
        "header": "4V-REXBenchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.11995/x4.png",
                "caption": "Figure 4:Statistics ofV-REX, including question distributions (left) and reasoning-step distributions (right).",
                "position": 300
            },
            {
                "img": "https://arxiv.org/html/2512.11995/x5.png",
                "caption": "",
                "position": 310
            }
        ]
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.11995/x6.png",
                "caption": "Figure 5:The ratio of change on final accuracy brought by CoQ evaluation across all VLMs.The X-axis denotes task categories, and the Y-axis represents the performance changing ratio.",
                "position": 963
            },
            {
                "img": "https://arxiv.org/html/2512.11995/x7.png",
                "caption": "Figure 6:FollowingandPlanningability on models of different sizes (logarithmic scale for the x-axis).Overall, the model’s ability on both tasks positively correlates with model size. Notably, the variance offollowingability among same-sized models is smaller than that ofPlanningability.",
                "position": 1007
            },
            {
                "img": "https://arxiv.org/html/2512.11995/x8.png",
                "caption": "Figure 7:Correlation betweenFollowingand Overall performance (left), and betweenPlanningand Overall performance (right).BothFollowingandPlanningabilities are positively correlated with the overall performance of models.",
                "position": 1020
            },
            {
                "img": "https://arxiv.org/html/2512.11995/x9.png",
                "caption": "Figure 8:The ratio betweenFollowingandPlanningability at different model sizes.The abilities ofPlanningandFollowingare more balanced when the ratios are closer to1.01.0.Smaller models are better atFollowingthanPlanningwhile larger models have more balanced ability.",
                "position": 1046
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Table of Contents for Appendix",
        "images": []
    },
    {
        "header": "7Image Sources",
        "images": []
    },
    {
        "header": "8Detailed Taxonomy",
        "images": []
    },
    {
        "header": "9Data Curation forPlanning",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.11995/x10.png",
                "caption": "Figure 9:Generation Pipeline forPlanning. For each QA chain, GPT-5 generates candidate distractors using two complementary strategies: (1)step-level, which introduces diverse distracting questions at each reasoning step, and (2)chain-level, which constructs full distracting reasoning chains.\nFor each sample, we select11most challenging distracting chain and22diverse distracting questions per step to form the final distractor set.",
                "position": 1245
            }
        ]
    },
    {
        "header": "10Experiment Details",
        "images": []
    },
    {
        "header": "11Final Accuracy for VLMs onV-REX",
        "images": []
    },
    {
        "header": "12Stepwise Recovery from Failure Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.11995/x11.png",
                "caption": "Figure 10:Stepwise result of recovery from failed planning",
                "position": 2319
            },
            {
                "img": "https://arxiv.org/html/2512.11995/x12.png",
                "caption": "Figure 11:Stepwise result of recovery from failed following",
                "position": 2322
            },
            {
                "img": "https://arxiv.org/html/2512.11995/x13.png",
                "caption": "Figure 12:Success case forPlanningtask.Without CoQ, the model fails to reach the correct final answer due to perceptual and counting errors. Under thePlanningsetting, however, once the model selects the correct intermediate question, it receives the corresponding answer, which compensates for its perceptual limitations and enables it to arrive at the correct final prediction.",
                "position": 2336
            },
            {
                "img": "https://arxiv.org/html/2512.11995/x14.png",
                "caption": "Figure 13:Success case forFollowingtask.By decomposing the final question into informative sub-questions, the model correctly identifies the algorithm in the flowchart and uses this intermediate insight to arrive at the correct final answer.",
                "position": 2339
            },
            {
                "img": "https://arxiv.org/html/2512.11995/x15.png",
                "caption": "Figure 14:Failure case in the Retrieval category where CoQ provides limited benefit.The model can directly identify the correct word from the puzzle without CoQ.\nHowever, when required to follow or plan using the CoQ chain, the model becomes misled by intermediate steps and incorrectly answers “Whistle.”\nThe example shows how structured CoQ exploration can interfere with tasks that primarily require precise visual matching rather than sequential reasoning.",
                "position": 2362
            },
            {
                "img": "https://arxiv.org/html/2512.11995/x16.png",
                "caption": "Figure 15:Failure case in the Retrieval category where CoQ provides limited benefit.Without CoQ, the model naturally checks each car one by one and correctly identifies three fully visible cars.\nUnderPlanning, the imposed CoQ chain forces the model into a reasoning path that diverges from its native strategy, leading to incorrect intermediate questions and an incorrect final count.\nEven underFollowing, the model cannot recover, showing that the human-designed chain is incompatible with the model’s retrieval-oriented reasoning.",
                "position": 2368
            },
            {
                "img": "https://arxiv.org/html/2512.11995/x17.png",
                "caption": "Figure 16:Failure case in thePlanningtask.\nAlthough the model correctly identifies the repeating cycle and the position of the fourteenth animal, its third step selects an irrelevant question about birds.\nThis detour misleads the final prediction, causing the model to answer with a bird rather than the correct frog.",
                "position": 2384
            },
            {
                "img": "https://arxiv.org/html/2512.11995/x18.png",
                "caption": "Figure 17:Failure case in thePlanningtask.\nThe model selects a chain of CoQ steps centered on color descriptions, which are uninformative for determining the intended concept of the drawing.\nThis distractive chain introduces additional noise and prevents the model from connecting the pig and canned meat sketches to the correct answer “Spam.”",
                "position": 2389
            },
            {
                "img": "https://arxiv.org/html/2512.11995/x19.png",
                "caption": "Figure 18:Failure case in theFollowingtask. Although the model correctly answers all the intermediate questions about the book cover, it fails to leverage the information extracted from previous reasoning steps to answer the final question correctly.",
                "position": 2401
            },
            {
                "img": "https://arxiv.org/html/2512.11995/x20.png",
                "caption": "Figure 19:Failure case in theFollowingtask. The model incorrectly answers the intermediate question about the least numbers of protusions and indentations of the missing puzzle piece, leading to an incorrect final answer.",
                "position": 2404
            }
        ]
    },
    {
        "header": "13Cases Study",
        "images": []
    }
]