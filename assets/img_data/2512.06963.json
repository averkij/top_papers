[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06963/x1.png",
                "caption": "Figure 1:Illustration of VideoVLA. Given a language instruction and the current visual observation,\nVideoVLA jointly predicts the appropriate sequence of next actions and generates video content that illustrates how these actions will influence physical interactions in the environment.\nIn addition to delivering strong performance on in-domain tasks, VideoVLA demonstrates robust generalization to novel objects and unseen skills. This capability stems from its use of pre-trainedvideo generation models—distinct from prior vision-language-action approachesteam2024octo;o2023open_x_embodiment;kim2024openvla;li2024cogact;wang2024HPT;cheang2024gr2;liu2024rdtthat primarily rely on pre-trained vision-languageunderstanding models—as well as its dual-objective strategy.",
                "position": 81
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06963/x2.png",
                "caption": "Figure 2:Overview of VideoVLA. (a) The text encoder converts the language instruction into a fixed-length token sequence, while the video encoder transforms a video clip into a sequence of frame latents, where the first latent corresponds to the first frame (i.e., the current visual observation). (b) VideoVLA adopts a Diffusion Transformerpeebles2023DITarchitecture that conditions on the encoded language tokens and the first frame latent to jointly predict the next action chunk required to accomplish the task, along with the future frame latents that represent the anticipated visual outcomes of executing that action chunk. The video decoder, highlighted in pink, is optional and only used when visualizing the imagined future frames.",
                "position": 159
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06963/figures/SIM_google.png",
                "caption": "(a)Google robot.",
                "position": 1166
            },
            {
                "img": "https://arxiv.org/html/2512.06963/figures/SIM_google.png",
                "caption": "(a)Google robot.",
                "position": 1169
            },
            {
                "img": "https://arxiv.org/html/2512.06963/figures/SIM_WINX.png",
                "caption": "(b)WidowX robot.",
                "position": 1174
            },
            {
                "img": "https://arxiv.org/html/2512.06963/x3.png",
                "caption": "Figure 4:Visualization of VideoVLA’s predicted visual imaginations and corresponding real-world executions during task completion, demonstrating a strong correlation between imagined and actual outcomes. Additional visualizations are provided in the appendix.",
                "position": 1181
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AEvaluation Details",
        "images": []
    },
    {
        "header": "Appendix BMore Analysis",
        "images": []
    },
    {
        "header": "Appendix CMore Visualizations",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06963/x4.png",
                "caption": "Figure 5:Visualizations of VideoVLA’s predicted visual imaginations and the corresponding executions during task completion in real-world experiments.",
                "position": 1421
            },
            {
                "img": "https://arxiv.org/html/2512.06963/x5.png",
                "caption": "Figure 6:Visualizations of VideoVLA’s predicted visual imaginations and the corresponding executions during task completion in simulation experiments.",
                "position": 1424
            }
        ]
    },
    {
        "header": "Appendix DLimitations and Broader Impacts",
        "images": []
    }
]