[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.24138/x1.png",
                "caption": "Figure 1:Setting OCR as the proxy reward, vanilla RL methods like Flow-GRPO exploit the OCR reward at the cost of losing real image quality, leading to reward hacking. It generates unrealistic, noisy images with blurry backgrounds and visual artifacts. In contrast, our method maintains better image quality and diversity. Prompt:“A storefront with ‘GARDO’ written on it”.",
                "position": 175
            },
            {
                "img": "https://arxiv.org/html/2512.24138/x2.png",
                "caption": "Figure 2:Overview of GARDO. GARDO introduces an uncertainty-driven, gated KL mechanism to control the proportion of regularization, avoiding unnecessary penalties. Our proposed diversity-aware advantage shaping effectively encourages exploration of novel states.",
                "position": 178
            },
            {
                "img": "https://arxiv.org/html/2512.24138/x3.png",
                "caption": "Figure 3:We train a diffusion model with33-layer MLP on Gaussian mixtures (pre-trained distribution), with the goal to capture multimodal high-reward clusters as shown in the reward landscape. The vanilla RL method (DDPO[DDPO]) with a large KL coefficientβ\\betais overly constrained and fails to increase rewards. Conversely, a smallβ\\betaincurs severe mode collapse. Our proposed diversity-aware optimization, when applied alone, successfully captures the multimodal modes, including the central cluster with the lowest probability density in the reference policyπref\\pi_{\\rm ref}. Our full GARDO framework simultaneously achieves maximum reward and discovers all high-reward clusters.",
                "position": 262
            },
            {
                "img": "https://arxiv.org/html/2512.24138/x4.png",
                "caption": "Figure 4:Learning curves and o.o.d. generalization results across different methods. GARDO not only matches the sample efficiency of the KL-free baseline, but also mitigates reward hacking effectively, as evidenced by the superior performance on unseen metrics.",
                "position": 507
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.24138/x5.png",
                "caption": "Figure 5:Our diversity-aware advantage shaping effectively improves the generation diversity.",
                "position": 531
            },
            {
                "img": "https://arxiv.org/html/2512.24138/x6.png",
                "caption": "Figure 6:Qualitative images generated by GARDO and vanilla GRPO across different prompts. Only GARDO generates images correctly aligned with the prompt, while maintaining a satisfactory perceptual quality and diversity.",
                "position": 534
            },
            {
                "img": "https://arxiv.org/html/2512.24138/x7.png",
                "caption": "Figure 7:Samples that are identified with high uncertainty over the training process. While these samples reach high proxy rewards, they get very low unseen rewards.",
                "position": 543
            },
            {
                "img": "https://arxiv.org/html/2512.24138/x8.png",
                "caption": "(a)",
                "position": 549
            },
            {
                "img": "https://arxiv.org/html/2512.24138/x8.png",
                "caption": "(a)",
                "position": 552
            },
            {
                "img": "https://arxiv.org/html/2512.24138/x9.png",
                "caption": "(b)",
                "position": 558
            },
            {
                "img": "https://arxiv.org/html/2512.24138/x10.png",
                "caption": "Figure 9:Visualization of GARDO counting 11 objects.",
                "position": 624
            }
        ]
    },
    {
        "header": "6Conclusion and Limitations",
        "images": []
    },
    {
        "header": "AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.24138/x11.png",
                "caption": "(a)",
                "position": 953
            },
            {
                "img": "https://arxiv.org/html/2512.24138/x11.png",
                "caption": "(a)",
                "position": 956
            },
            {
                "img": "https://arxiv.org/html/2512.24138/x12.png",
                "caption": "(b)",
                "position": 962
            },
            {
                "img": "https://arxiv.org/html/2512.24138/x13.png",
                "caption": "Figure 11:Qualitative results on Flux.1-dev across GARDO and baselines.While both GRPO and GARDO significantly improve the visual quality, GRPO tends to hack the HPSv2 reward, generating unnecessary or even undesired details.",
                "position": 969
            },
            {
                "img": "https://arxiv.org/html/2512.24138/x14.png",
                "caption": "Figure 12:Qualitative results on Flux.1-dev across GARDO and baselines.While both GRPO and GARDO significantly improve the visual quality, GRPO tends to hack the HPSv2 reward, generating unnecessary or even undesired details.",
                "position": 972
            },
            {
                "img": "https://arxiv.org/html/2512.24138/x15.png",
                "caption": "Figure 13:Generated images along the training process.",
                "position": 982
            },
            {
                "img": "https://arxiv.org/html/2512.24138/x16.png",
                "caption": "Figure 14:Generated images along the training process.",
                "position": 985
            }
        ]
    },
    {
        "header": "BAdditional Results",
        "images": []
    }
]