[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.19103/fig/cancer-screening.png",
                "caption": "",
                "position": 65
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.19103/x1.png",
                "caption": "Figure 1:(a) The pan-cancer dataset used in this study, encompassing 5,117 CT scans across 9 different types of lesions from 16 internal and 7 external datasets. (b) Significant foreground-background imbalance: in our dataset, lesions occupy only 0.085% area proportions in CT volumes. (c) Comparisons in pan-cancer segmentation. (d) Comparisons in pan-cancer detection. (e) Inference efficiency (GFLOP per scan) and segmentation DSC on the FLARE23 validation dataset. Compared with the second-best model, GF-Screen is 5.7×\\timesfaster with higher segmentation DSC. (f) Comparisons on the FLARE25 challenge validation leaderboard. GF-Screen outperforms the second-ranked algorithm (champion solution of FLARE24) by a large margin (+25.6% DSC and +28.2% NSD).",
                "position": 102
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.19103/x2.png",
                "caption": "Figure 2:Comparison between previous cancer screening methods and GF-Screen.",
                "position": 127
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.19103/x3.png",
                "caption": "Figure 3:The overall framework of GF-Screen, including a Glance model to localize diseased regions and a Focus model to precisely segment lesions. (a) In the training stage, we conduct segmentation on all sub-volumes and leverage the segmentation results to reward the Glance model via a novel group-relative learning paradigm. (b) In the inference stage, a dynamic number of sub-volumes classified as “with lesions” by the Glance model will be input to the Focus model for segmentation, where the redundant regions will be discarded.",
                "position": 148
            },
            {
                "img": "https://arxiv.org/html/2601.19103/x4.png",
                "caption": "Figure 4:Illustration of sub-volume variation. Theblueregions represent the challenging view with partial lesions and poor angles. Whileredregions indicate the optimal diagnostic view containing complete lesion information, generally with more precise segmentation results. Thus, we propose to leverage segmentation results as reward signals for RL.",
                "position": 196
            },
            {
                "img": "https://arxiv.org/html/2601.19103/x5.png",
                "caption": "Figure 5:The Group Relative Learning (GRL) paradigm in GF-Screen. The Glance modelGGis trainable while the reference modelGr​e​fG_{ref}is frozen. We first generate selection outputsoofrom the input sub-volumesvv, then use the reward function Eq.2to calculate the rewardsrr. Finally, we compute the relative advantagesAAvia the GAE function Eq.3.",
                "position": 230
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.19103/x6.png",
                "caption": "Table 8:We report the classification sensitivity and specificity of the Glance model.",
                "position": 1071
            },
            {
                "img": "https://arxiv.org/html/2601.19103/x6.png",
                "caption": "Figure 6:RL loss curve in GRL training.",
                "position": 1107
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.19103/x7.png",
                "caption": "Figure A1:We further summarize the comparisons with the second-best model(Wuet al.,2025c). GF-Screen is 5.7 times faster with higher performance simultaneously.",
                "position": 3208
            },
            {
                "img": "https://arxiv.org/html/2601.19103/x8.png",
                "caption": "Figure A2:We present the visual activation maps of the Glance model compared with the Ground truth lesion masks. In the last row, we show some failure cases. For example, in the first case of the last row, the model misses the tiny lesion in the left upper part.",
                "position": 3211
            },
            {
                "img": "https://arxiv.org/html/2601.19103/x9.png",
                "caption": "Figure A3:We present the segmentation results on healthy regions (no lesions), compared with the best-competing method(Wuet al.,2025c).",
                "position": 3230
            },
            {
                "img": "https://arxiv.org/html/2601.19103/x10.png",
                "caption": "Figure A4:An illustrative case to demonstrate the effectiveness of selecting optimal views. (a) Without GF-Screen, segmentation predictions from challenging views can degrade the average performance of the final results. (b) With GF-Screen, only the segmentation results from optimal diagnostic views are retained, while predictions from suboptimal views are filtered out.",
                "position": 3241
            },
            {
                "img": "https://arxiv.org/html/2601.19103/x11.png",
                "caption": "Figure A5:We present the segmentation results of GF-Screen across different types of lesions.",
                "position": 3249
            },
            {
                "img": "https://arxiv.org/html/2601.19103/x12.png",
                "caption": "Figure A6:We randomly split our training dataset into five subsets and perform training to analyze the loss curves of RL.",
                "position": 3253
            }
        ]
    },
    {
        "header": "6Appendix",
        "images": []
    }
]