[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09507/imgs/hammer_and_wrench.png",
                "caption": "",
                "position": 59
            },
            {
                "img": "https://arxiv.org/html/2510.09507/x1.png",
                "caption": "Figure 1:For an Embodied Agent, using physical tools is crucial in many tasks. The understanding of physical tools significantly impacts the task’s success rate and execution efficiency (Top). PhysToolBench (Bottom) systematically evaluates the understanding of physical tools of multimodal LLMs. The benchmark is designed with three progressive levels of difficulty and employs a Visual Question Answering (VQA) format. Notice that in the actual benchmark, tools in the images are numerically labeled, and images here are for illustrative purposes only.",
                "position": 87
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3The PhysToolBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09507/x2.png",
                "caption": "Figure 2:Statistics of PhysToolBench. (a) is the distribution of the category. (b) is the distribution of the difficulty level. (c) is the word cloud of the task description given to MLLMs.",
                "position": 165
            }
        ]
    },
    {
        "header": "4Experiments on PhysToolBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09507/x3.png",
                "caption": "Figure 3:MLLM Leaderboard on our PhysToolBench, ranked by overall performance.",
                "position": 198
            },
            {
                "img": "https://arxiv.org/html/2510.09507/x4.png",
                "caption": "Figure 4:Overall performance v.s. model sizefor open-source MLLMs. A significant correlation is observed between performance and model size.",
                "position": 720
            },
            {
                "img": "https://arxiv.org/html/2510.09507/x4.png",
                "caption": "Figure 4:Overall performance v.s. model sizefor open-source MLLMs. A significant correlation is observed between performance and model size.",
                "position": 723
            },
            {
                "img": "https://arxiv.org/html/2510.09507/x5.png",
                "caption": "Figure 5:Performance comparison between the embodied models and their base model.",
                "position": 728
            },
            {
                "img": "https://arxiv.org/html/2510.09507/x6.png",
                "caption": "Figure 6:Some results of PhysToolBench. We showcase illustrative examples for each difficulty level, along with the answers from several top-tier models and human participants. Note that the markers are intentionally enlarged for visualization purposes.",
                "position": 762
            },
            {
                "img": "https://arxiv.org/html/2510.09507/x7.png",
                "caption": "Figure 7:Comparison Between (a) Text-Level Reasoning and (b) Our proposed Vision-Centric Reasoning.",
                "position": 770
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix AMore Details About Benchmark Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09507/x8.png",
                "caption": "Figure 8:Task-Scene Pair Brainstorming",
                "position": 910
            },
            {
                "img": "https://arxiv.org/html/2510.09507/x9.png",
                "caption": "Figure 9:Example failure cases and the final revised images",
                "position": 918
            },
            {
                "img": "https://arxiv.org/html/2510.09507/x10.png",
                "caption": "Figure 10:UI demonstration of our annotation app",
                "position": 925
            },
            {
                "img": "https://arxiv.org/html/2510.09507/x11.png",
                "caption": "Figure 11:We utilize GPT-4o to evaluate the realism of the final images. Above is the system prompt we provided to GPT-4o.",
                "position": 938
            }
        ]
    },
    {
        "header": "Appendix BComplete Demonstration of Image–Question–Answer Triplets",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09507/x12.png",
                "caption": "Figure 12:Examples of data in PhysToolBenchwith GPT-4o predictions.",
                "position": 949
            },
            {
                "img": "https://arxiv.org/html/2510.09507/x13.png",
                "caption": "Figure 13:Examples of data in PhysToolBenchwith GPT-4o predictions.",
                "position": 952
            },
            {
                "img": "https://arxiv.org/html/2510.09507/x14.png",
                "caption": "Figure 14:Examples of data in PhysToolBenchwith GPT-4o predictions.",
                "position": 955
            },
            {
                "img": "https://arxiv.org/html/2510.09507/x15.png",
                "caption": "Figure 15:Examples of data in PhysToolBenchwith GPT-4o predictions.",
                "position": 958
            },
            {
                "img": "https://arxiv.org/html/2510.09507/x16.png",
                "caption": "Figure 16:Examples of data in PhysToolBenchwith GPT-4o predictions.",
                "position": 961
            },
            {
                "img": "https://arxiv.org/html/2510.09507/x17.png",
                "caption": "Figure 17:Examples of data in PhysToolBenchwith GPT-4o predictions.",
                "position": 964
            },
            {
                "img": "https://arxiv.org/html/2510.09507/x18.png",
                "caption": "Figure 18:Examples of data in PhysToolBenchwith GPT-4o predictions.",
                "position": 967
            },
            {
                "img": "https://arxiv.org/html/2510.09507/x19.png",
                "caption": "Figure 19:Examples of data in PhysToolBenchwith GPT-4o predictions.",
                "position": 970
            },
            {
                "img": "https://arxiv.org/html/2510.09507/x20.png",
                "caption": "Figure 20:Examples of data in PhysToolBenchwith GPT-4o predictions.",
                "position": 973
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]