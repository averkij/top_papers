[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01782/x1.png",
                "caption": "Figure 1:Inference pipeline in our framework,Open-RAG. It learns to generate retrieval/no_retrieval tokens, contrasts between relevant and irrelevant contexts, and categorizes answers as partially, fully, or not supported. Then at inference,\ngiven a (multi-hop) user query,\nwe first enforce the model to generate an answer with conditional to no_retrieval as input, and based on the model confidence we dynamically determine if retrieval is needed.",
                "position": 143
            },
            {
                "img": "https://arxiv.org/html/2410.01782/x2.png",
                "caption": "Figure 2:Open-RAGtraining data preparation involves generating four variations of new training instances from each original pair (qùëûqitalic_q,yùë¶yitalic_y), each incorporating differentreflectiontokens using ground truth/LLM critic and retrieved passages. Our approach enables an LLM not only to reflect on generation quality but also to contrast distractors.",
                "position": 162
            }
        ]
    },
    {
        "header": "2Open-RAG: Enhanced Retrieval-Augmented Reasoning",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01782/x3.png",
                "caption": "Figure 3:Architechture transformation (dense to PEFT MoE) inOpen-RAG. Router‚Ñõ‚Ñõ\\mathcal{R}caligraphic_Ris trained from scratch.\nThe FFN layer is kept frozen and adapted by parallel-adapter-based expertsùêÑùêÑ\\mathbf{E}bold_E. Other layers are being copied.",
                "position": 218
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01782/x4.png",
                "caption": "Figure 4:(Top) Performance vs Retrieval by different adaptive retrieval strategies. \n(Bottom) Performance vs scores from adaptive retrieval.fr‚Å¢e‚Å¢tsubscriptùëìùëüùëíùë°f_{ret}italic_f start_POSTSUBSCRIPT italic_r italic_e italic_t end_POSTSUBSCRIPTdenotes probability score from external model distilled/predictedreflectiontoken.",
                "position": 798
            },
            {
                "img": "https://arxiv.org/html/2410.01782/x4.png",
                "caption": "",
                "position": 801
            },
            {
                "img": "https://arxiv.org/html/2410.01782/x5.png",
                "caption": "",
                "position": 806
            }
        ]
    },
    {
        "header": "4Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01782/x6.png",
                "caption": "Figure 5:Model performances utilizing CRAG contexts",
                "position": 852
            },
            {
                "img": "https://arxiv.org/html/2410.01782/x7.png",
                "caption": "Figure 6:Layer-wise expert activation on single-hop (PopQA, PubHealth) vs multi-hop tasks (HotpotQA, MuSiQue).",
                "position": 855
            },
            {
                "img": "https://arxiv.org/html/2410.01782/x8.png",
                "caption": "Figure 7:Performances (MAUVE for ALCE-ASQA; EM for HotpotQA and MuSiQue-Ans; and accuracy for PopQA and PubHealth ) with different architecture.",
                "position": 863
            }
        ]
    },
    {
        "header": "5Related work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining Details",
        "images": []
    },
    {
        "header": "Appendix BInference Details",
        "images": []
    }
]