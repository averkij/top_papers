[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21066/x1.png",
                "caption": "(a) Image Fill",
                "position": 82
            },
            {
                "img": "https://arxiv.org/html/2508.21066/x1.png",
                "caption": "(a) Image Fill",
                "position": 85
            },
            {
                "img": "https://arxiv.org/html/2508.21066/x2.png",
                "caption": "(b) Image Extend with Prompt",
                "position": 90
            },
            {
                "img": "https://arxiv.org/html/2508.21066/x3.png",
                "caption": "(c) Image Extend without Prompt",
                "position": 96
            },
            {
                "img": "https://arxiv.org/html/2508.21066/x4.png",
                "caption": "(d) Object Removal",
                "position": 101
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21066/x5.png",
                "caption": "Figure 2:Visual showcase of Seedream 3.0 Fill results across four scenario: image fill, image extend, object removal and text rendering. Each column presents a representative example with corresponding prompts and outputs, demonstrating the model’s unified capability across diverse generation objectives.",
                "position": 152
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4METHODOLOGY",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21066/x6.png",
                "caption": "Figure 3:Overall pipeline of our unified RL procedure. We first random sample image and conditions from different task with a certain probability. Start with same condition and different init noise, the reference image is fully denoised using the reference model, denoted asπr​e​f​(⋅)\\pi_{ref}(\\cdot). While the evaluation image is partially denoised with randomly selected step and directly predictx0′x_{0}^{\\prime}based on the policy model, denoted asπθ​(⋅)\\pi_{\\theta}(\\cdot). The reward model guides learning by encouraging the policy model to achieve superior performance to the reference model across all evaluation dimensions and tasks.",
                "position": 224
            },
            {
                "img": "https://arxiv.org/html/2508.21066/x7.png",
                "caption": "Figure 4:Illustration of the pairwise annotation process. Given multiple\ncandidate outputs for the same prompt and binary mask, annotators identify the\nbest and worst samples under each evaluation dimension to form a winner/loser\npair. If the differences between candidates are negligible, the dimension is\ndiscarded (denoted by∅\\emptyset), ensuring that only\ninformative comparisons are retained. To clarify, this showcase uses an all-one mask, meaning the entire image region is generated.",
                "position": 314
            },
            {
                "img": "https://arxiv.org/html/2508.21066/x8.png",
                "caption": "Figure 5:The detail of our one reward model. We utilize VLM to judge whether the first image is better than the second one. In the process of reward feedback learning, the probability ofy+y^{+}token is treated as the reward to the diffusion models. We simplely add the edit task and the evaluation dimensions to the user query, achieving the goal of training for different task and dimensions. The content of angle brackets is optional, only add when the evaluation dimension is Text Alignment.",
                "position": 334
            },
            {
                "img": "https://arxiv.org/html/2508.21066/x9.png",
                "caption": "Figure 6:We visualize the reward curves of Consistency, Structure, Text Alignment, Aesthetics for image fill(blue) and image extend(green), Removal Quality for object removal(orange).",
                "position": 444
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21066/x10.png",
                "caption": "Figure 7:Comparison of performance between Seedream 3.0 [OneReward] and Seedream 3.0 [Base] using Good–Same–Bad (GSB) evaluation. Each group corresponds to a specific image editing task. Bars represent the relative proportions of outputs judged as Good (orange), Same (green), or Bad (blue) across different model pairs.\nThis visualization highlights the distribution of relative preferences, showing where OneReward-enhanced models outperform the base model.",
                "position": 806
            },
            {
                "img": "https://arxiv.org/html/2508.21066/x11.png",
                "caption": "Figure 8:Visual comparison of editing results for Seedream 3.0 Fill and its competitors across different tasks. Rows correspond to different methods, columns show task-specific prompts and outputs. The source images are shown in the first row. The blank row at the bottom indicates that the case is prompt-free.",
                "position": 820
            },
            {
                "img": "https://arxiv.org/html/2508.21066/x12.png",
                "caption": "Figure 9:Visual comparison of editing results for Flux Fill and our RL model across different tasks. Rows correspond to different methods, columns show task-specific prompts and outputs. The source images are shown in the first row. The last two rows stands for our RL-enhanced model, trained via Alg.1and Alg.2.",
                "position": 853
            }
        ]
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21066/x13.png",
                "caption": "Figure 10:Schematic illustration of reward computation in the baseline and our dynamic framework. In the baseline1, rewards are measured as the vertical gap between the policy (blue) and a fixed reference (red). In the dynamic method2, rewards are computed against an EMA-updated reference (orange) that evolves smoothly with the policy (green), forming a dynamic baseline. This figure is for conceptual understanding only and does not reflect actual parameter values or training dynamics.",
                "position": 1368
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]