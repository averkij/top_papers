[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06674/extracted/6056458/figs/teaser.png",
                "caption": "Figure 1:Top:Performancevs.Parameterswith concurrent methods. Our EMOv2 achieves significant accuracy with fewer parameters. Superscript‚àó‚àó\\ast‚àó: The comparison methods employ more robust training strategies described in their papers, while ours uses the strategy mentioned inTab.XVII(e).Bottom:The range of token interactions varies with different window attention mechanisms. Our EMOv2, with parameter-shared spanning attention inSec.3.3.1, has a larger and correspondingly stronger Effective Receptive Field (ERF).",
                "position": 118
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06674/extracted/6056458/figs/emo2.png",
                "caption": "Figure 2:Left: Abstracted unifiedMeta-Mobile BlockfromMulti-Head Self-Attention,Feed-Forward Network[35], andInverted Residual Block[9](c.f. Sec3.2.1). The inductive block can be deduced into specific modules using differentexpansion ratioŒªùúÜ\\lambdaitalic_Œªandefficient operator‚Ñ±‚Ñ±\\mathcal{F}caligraphic_F.Middle: We construct a family of vision models based on our i2RMB module: 4-stageEMOv2, composed solely of the deducedi2RMB(c.f. Sec3.2.2), for various perception tasks (image classification, detection, and segmentation inSec.4.2). Additionally, we introduce the temporally extendedV-EMOfor video classification, the U-EMO based on an encoder-decoder architecture, and D-EMO to replace the Transformer block in DiT[67]. These downstream models are typically built based on the i2RMB.Right: Performance comparison with different SoTAs on various tasks.",
                "position": 204
            },
            {
                "img": "https://arxiv.org/html/2412.06674/x1.png",
                "caption": "Figure 3:Meta-paradigm comparison between our MMBlock and MetaFormer[52]. We integrateùìïùìï{\\color[rgb]{0.69140625,0.140625,0.09375}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{0.69140625,0.140625,0.09375}\\bm{\\mathcal{F}}}bold_caligraphic_Finto expended FFN to construct a more streamlined and shallower single-module block.",
                "position": 605
            },
            {
                "img": "https://arxiv.org/html/2412.06674/extracted/6056458/figs/i2rmb.png",
                "caption": "Figure 4:Detailed implementation comparison of the Inverted Residual Mobile Block (iRMBinSec.3.2.2) and the improved version (i2RMBinSec.3.3.1). i2RMB designs a parameter-sharing spanning window attention mechanism that simultaneously models the interaction of distant and close window information.",
                "position": 797
            },
            {
                "img": "https://arxiv.org/html/2412.06674/extracted/6056458/figs/v1v2.png",
                "caption": "Figure 5:Downstream gains of EMOv2-5M over EMOv1-5M.",
                "position": 1146
            }
        ]
    },
    {
        "header": "4Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06674/extracted/6056458/figs/qualitative.png",
                "caption": "Figure 6:Qualitative comparisons between EMOv1/v2 on downstream SSDLite[10]and DeepLabv3[102]. EMOv2 demonstrates higher accuracy in class and boundary detection. Zoom in for more details.",
                "position": 3401
            },
            {
                "img": "https://arxiv.org/html/2412.06674/extracted/6056458/figs/cam.png",
                "caption": "Figure 7:Visualizations by Grad-CAM. EMOv2 generates sharper and higher confidence attention maps than EMOv1.",
                "position": 3408
            },
            {
                "img": "https://arxiv.org/html/2412.06674/extracted/6056458/figs/ablation_trajectory.png",
                "caption": "Figure 8:Overall incremental trajectory from baseline to modern EMOv2 at the 5M magnitude.Each line is based on a modification of the immediately preceding line. Detailed ablations inSec.4.3. Parameters and FLOPs are marked ingreenandyellow.",
                "position": 3416
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Overview",
        "images": []
    }
]