[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10613/x1.png",
                "caption": "Figure 1:CoSTA‚àówith different cost-quality trade-off coefficientsŒ±ùõº\\alphaitalic_Œ±vs. four recent image-editing models/agents. CoSTA‚àóachieves Pareto optimality and dominates baselines on both metrics.",
                "position": 182
            },
            {
                "img": "https://arxiv.org/html/2503.10613/x2.png",
                "caption": "Figure 2:Comparison of CoSTA‚àówith State-of-the-Art image editing models/agents, which include GenArtist(Wang et¬†al.,2024b), MagicBrush(Zhang et¬†al.,2024a), InstructPix2Pix(Brooks et¬†al.,2023), and CLOVA(Gao et¬†al.,2024). The input images and prompts are shown on the left of the figure. The outputs generated by each method illustrate differences in accuracy, visual coherence, and the ability to multimodal tasks. Figure9shows examples of step-by-step editing using CoSTA‚àówith intermediate subtask outputs presented.",
                "position": 188
            },
            {
                "img": "https://arxiv.org/html/2503.10613/x3.png",
                "caption": "Figure 3:Comparison of CoSTA‚àówith other planning agents. LLM-only planning is efficient but prone to failure and heuristics. Search algorithms like A‚àóguarantee optimal paths but are computationally expensive. CoSTA‚àóbalances cost and quality by first pruning the subtask tree using an LLM, which reduces the graph of tools we conduct fine-grained A‚àósearch on.",
                "position": 197
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10613/x4.png",
                "caption": "Figure 4:Tool Dependency Graph (TDG). A directed graph where nodes represent tools and edges indicate dependencies. An edge(v1,v2)subscriptùë£1subscriptùë£2(v_{1},v_{2})( italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT )meansv1subscriptùë£1v_{1}italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT‚Äôs output is a legal input ofv2subscriptùë£2v_{2}italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. It enables toolpath search for multi-turn image-editing tasks with composite instructions.",
                "position": 251
            }
        ]
    },
    {
        "header": "3Foundations of CoSTA‚àó",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10613/x5.png",
                "caption": "Figure 5:Three stages in CoSTA‚àó: (1) an LLM generates a subtask tree based on the input and task dependencies; (2) the subtask tree spans a tool subgraph that maintains tool dependencies; and (3) A‚àósearch finds the best toolpath balancing efficiency and quality.",
                "position": 345
            }
        ]
    },
    {
        "header": "4CoSTA‚àó: Cost-Sensitive Toolpath Agent",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10613/x6.png",
                "caption": "Figure 6:Distribution of image-only (left) and text+image tasks (middle) inour proposed benchmark, and quality comparisonof different methods on the benchmark (right). CoSTA‚àóexcels in complex multimodal tasks and outperforms all the baselines.",
                "position": 390
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10613/x7.png",
                "caption": "Figure 7:Comparison of a task withh‚Å¢(x)‚Ñéùë•h(x)italic_h ( italic_x )andh‚Å¢(x)+g‚Å¢(x)‚Ñéùë•ùëîùë•h(x)+g(x)italic_h ( italic_x ) + italic_g ( italic_x ), showing how real-time feedback improves path selection and execution.",
                "position": 850
            },
            {
                "img": "https://arxiv.org/html/2503.10613/x8.png",
                "caption": "Figure 8:Qualitative comparison of image editing tools vs. CoSTA‚àófor text-based tasks, highlighting the advantages of our multimodal support in preserving visual and textual fidelity.",
                "position": 908
            }
        ]
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10613/x9.png",
                "caption": "Figure 9:Step-by-step execution of editing tasks using CoSTA‚àó. Each row illustrates an input image, the corresponding subtask breakdown, and intermediate outputs at different stages of the editing process. This visualization highlights how CoSTA‚àósystematically refines outputs by leveraging specialized models for each subtask, ensuring greater accuracy and consistency in multimodal tasks.",
                "position": 1443
            }
        ]
    },
    {
        "header": "Appendix AStep-by-Step Execution of Tasks in Figure2",
        "images": []
    },
    {
        "header": "Appendix BHuman Evaluation for Accuracy Calculation",
        "images": []
    },
    {
        "header": "Appendix CAutomatic Construction of the Tool Dependency Graph",
        "images": []
    },
    {
        "header": "Appendix DDataset Generation and Evaluation Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10613/x10.png",
                "caption": "Figure 10:Distribution of the number of instances for each subtask in the dataset.",
                "position": 1757
            },
            {
                "img": "https://arxiv.org/html/2503.10613/x11.png",
                "caption": "Figure 11:An overview of the dataset used for evaluation, showcasing representative input images and prompts across different task categories. The top section presents examples from image-only tasks, while the bottom section includes text+image tasks. These examples illustrate the diversity of tasks in our dataset, highlighting the range of modifications required for both visual and multimodal editing scenarios.",
                "position": 1777
            }
        ]
    },
    {
        "header": "Appendix EConsistency in CoSTA* Outputs",
        "images": []
    },
    {
        "header": "Appendix FModel Description Table (MDT)",
        "images": []
    },
    {
        "header": "Appendix GBenchmark Table (BT)",
        "images": []
    },
    {
        "header": "Appendix HAlgorithms",
        "images": []
    },
    {
        "header": "Appendix IA* Execution Strategy",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10613/x12.png",
                "caption": "Figure 12:Visualization of the Retry Mechanism",
                "position": 2842
            }
        ]
    },
    {
        "header": "Appendix JCorrelation Analysis of CLIP Scores and Human Accuracy",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10613/x13.png",
                "caption": "Figure 13:Scatter plot of CLIP scores vs. human accuracy across 40 tasks. The weak correlation (Spearman‚ÄôsœÅ=0.59ùúå0.59\\rho=0.59italic_œÅ = 0.59, Kendall‚ÄôsœÑ=0.47ùúè0.47\\tau=0.47italic_œÑ = 0.47) highlights CLIP‚Äôs limitations in capturing nuanced inaccuracies, particularly in complex, multi-step tasks.",
                "position": 2852
            }
        ]
    },
    {
        "header": "Appendix KLLM Prompt for Generating Subtask Tree",
        "images": []
    },
    {
        "header": "Appendix LLLM Prompt for Getting Bounding Box and Text for Replacement",
        "images": []
    }
]