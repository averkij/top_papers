[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.15552/images/MERA_Multi_Main.png",
                "caption": "Figure 1:Overview of MERA Multi.\nThe benchmark unites multimodal evaluation, taxonomy-based skill assessment, and data leakage protection across 18 tasks covering (default) text, image, audio, and video modalities. It employs standardized block-prompting, compound scoring, and integrates methods for multimodal content protection, forming a transparent and robust methodology for culturally grounded multimodal evaluation in Russian.",
                "position": 455
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Overview of Multimodal Benchmark",
        "images": []
    },
    {
        "header": "4Baselines",
        "images": []
    },
    {
        "header": "5Results",
        "images": []
    },
    {
        "header": "Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethical Statement",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AMultimodal Benchmarks Comparison",
        "images": []
    },
    {
        "header": "Appendix BDataset Description",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.15552/md_to_latex/images/LabTabVQA.png",
                "caption": "",
                "position": 2156
            },
            {
                "img": "https://arxiv.org/html/2511.15552/md_to_latex/images/RealVQA.jpg",
                "caption": "",
                "position": 2322
            },
            {
                "img": "https://arxiv.org/html/2511.15552/md_to_latex/images/ruCLEVER.png",
                "caption": "",
                "position": 2368
            },
            {
                "img": "https://arxiv.org/html/2511.15552/md_to_latex/images/ruCommonVQA.jpg",
                "caption": "",
                "position": 2404
            },
            {
                "img": "https://arxiv.org/html/2511.15552/md_to_latex/images/ruHHH-Image.jpeg",
                "caption": "",
                "position": 2673
            },
            {
                "img": "https://arxiv.org/html/2511.15552/md_to_latex/images/ruMathVQA.jpg",
                "caption": "",
                "position": 2894
            },
            {
                "img": "https://arxiv.org/html/2511.15552/md_to_latex/images/ruNaturalScienceQA.jpeg",
                "caption": "",
                "position": 2935
            },
            {
                "img": "https://arxiv.org/html/2511.15552/md_to_latex/images/ruTiE-Vision.png",
                "caption": "",
                "position": 3124
            },
            {
                "img": "https://arxiv.org/html/2511.15552/md_to_latex/images/SchoolScienceVQA.jpg",
                "caption": "",
                "position": 3199
            },
            {
                "img": "https://arxiv.org/html/2511.15552/md_to_latex/images/ruMuTau.jpg",
                "caption": "",
                "position": 3263
            },
            {
                "img": "https://arxiv.org/html/2511.15552/md_to_latex/images/weird.jpg",
                "caption": "",
                "position": 3301
            }
        ]
    },
    {
        "header": "Appendix CSkill taxonomy",
        "images": []
    },
    {
        "header": "Appendix DData leakage details",
        "images": []
    },
    {
        "header": "Appendix ELLM-as-a-judge details",
        "images": []
    },
    {
        "header": "Appendix FBlock prompts analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.15552/images/prompts_analysis.png",
                "caption": "Figure 2:The relative (with regard to baseline prompt (0)) effects of different formulations of prompts for each dataset. There are ten different formulations of prompts for one dataset, hence nine corresponding bars (one formulation is baseline category). Red bars reflect statistically significant (at 95% confidence level) effects.",
                "position": 5737
            }
        ]
    },
    {
        "header": "Appendix GBaselines Details",
        "images": []
    }
]