[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.16153/x1.png",
                "caption": "(a)AgentFly vs. Baselines on GAIA validation and test sets.",
                "position": 159
            },
            {
                "img": "https://arxiv.org/html/2508.16153/x1.png",
                "caption": "(a)AgentFly vs. Baselines on GAIA validation and test sets.",
                "position": 162
            },
            {
                "img": "https://arxiv.org/html/2508.16153/x2.png",
                "caption": "(b)Ablation study of AgentFly across benchmarks.",
                "position": 167
            },
            {
                "img": "https://arxiv.org/html/2508.16153/x3.png",
                "caption": "(c)Continual learning curves across memory designs.",
                "position": 173
            },
            {
                "img": "https://arxiv.org/html/2508.16153/x4.png",
                "caption": "(d)AgentFly’s accuracy improvement on OOD datasets.",
                "position": 178
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.16153/x5.png",
                "caption": "Figure 2:A graphical model of memory-based markov decision process.",
                "position": 237
            }
        ]
    },
    {
        "header": "3Methodology: Memory-Based MDP with Case-based Reasoning Policy",
        "images": []
    },
    {
        "header": "4Implementation: Deep Research Agent",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.16153/x6.png",
                "caption": "Figure 3:The architecture of AgentFly with parametric memory. AgentFly is instantiated as a planner–executor framework alternating between Case-Based Planning (Stage 1) and Tool-Based Execution (Stage 2). The planner is an LLM-based CBR agent enhanced by a Case Memory module that supports both Write, which records new cases and online refines the Q-function, and Read, which retrieves cases via the learned retrieval policy for adaptive case selection. The executor is an LLM-based MCP client that invokes external tools hosted on the MCP servers through the MCP protocol.",
                "position": 451
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.16153/x7.png",
                "caption": "Figure 4:Performance on SimpleQA and HLE. The SimpleQA results are from WebSailor(Li et al.,2025b), and the HLE results are from the official website.",
                "position": 948
            },
            {
                "img": "https://arxiv.org/html/2508.16153/x8.png",
                "caption": "Figure 5:The average number of each task type per level, highlighting the dominance of code, search, and crawl tasks as difficulty level increases.",
                "position": 1490
            }
        ]
    },
    {
        "header": "6Discussion and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.16153/x9.png",
                "caption": "Figure 6:Token costs on the GAIA.",
                "position": 1512
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADerivation of the Optimal Policy in Soft-Q Learning",
        "images": []
    },
    {
        "header": "Appendix BAnalysis of Memory Mechanisms",
        "images": []
    }
]