[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14246/x1.png",
                "caption": "Figure 1:The benefits of ourVisualAgenticReinforcementFine-Tuning (Visual-ARFT) to perform complex multi-modal reasoning tasks, such as (top) write and execute python code to accurately read text within a specified image region and (bottom) use internet search to answer a multi-hop question.",
                "position": 114
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Visual Agentic Reinforcement Fine-Tuning (Visual ARFT)",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14246/x2.png",
                "caption": "Figure 2:Overview of Visual-ARFT.We successfully empower LVLMs with multimodal agentic capabilities, including (a) agentic search and (b) agentic coding, enabling them to solve complex multimodal tasks through reasoning, decomposition, and tool interaction.",
                "position": 207
            }
        ]
    },
    {
        "header": "4Multimodal Agentic Tool (MAT) Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14246/x3.png",
                "caption": "Figure 3:Data Annotation Pipelineof our proposed Multimodal Agentic Tool Bench (MAT): (a) MAT-Search, a manually annotated and verified dataset for agentic search, and (b) MAT-Coding, an automatically generated dataset for agentic coding with a structured pipeline.",
                "position": 303
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14246/x4.png",
                "caption": "Figure 4:Visualization Inference Cases of Visual-ARFT.Demonstrating Visual-ARFTâ€™s multi-modal agentic capabilities: processing an image and answering a question via code generation and execution (left), and solving multi-hop VQA through query decomposition and search tool invocation (right).",
                "position": 1076
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix of Visual Agentic Reinforcement Fine-Tuning",
        "images": []
    },
    {
        "header": "Appendix APrompt Used",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14246/x5.png",
                "caption": "Figure 5:Prompt for Agentic Searching Tasks",
                "position": 1837
            },
            {
                "img": "https://arxiv.org/html/2505.14246/x6.png",
                "caption": "Figure 6:Prompt for Agentic Coding Tasks",
                "position": 1840
            },
            {
                "img": "https://arxiv.org/html/2505.14246/x7.png",
                "caption": "Figure 7:MAT-Coding Data Examples",
                "position": 1843
            },
            {
                "img": "https://arxiv.org/html/2505.14246/x8.png",
                "caption": "Figure 8:MAT-Search Data Examples",
                "position": 1846
            },
            {
                "img": "https://arxiv.org/html/2505.14246/x9.png",
                "caption": "Figure 9:Examples of Visual-ARFT Inference on MAT.",
                "position": 1849
            }
        ]
    },
    {
        "header": "Appendix BTraining Data and Benchmark",
        "images": []
    },
    {
        "header": "Appendix CLimitation",
        "images": []
    },
    {
        "header": "Appendix DPotential Societal Impact",
        "images": []
    }
]