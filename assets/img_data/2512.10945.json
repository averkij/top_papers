[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.10945/x1.png",
                "caption": "Figure 1:Examples fromMotionexpressionsVideoSegmentation (MeViS) showing the dataset’s nature and complexity. The expressions in MeViS primarily focus on motion attributes, making it impossible to identify the target object from a single frame. For example, the first example has three parrots with similar appearances, and the target object is identified as“The bird flying away”. This object can only be recognized by capturing its motion throughout the video. The updated MeViSv2 further provides motion-reasoning and no-target expressions (seeFig.3), adds audio▲\\blacktriangle)))expressions alongside text, and provides mask and bounding box trajectory annotations.",
                "position": 90
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3MeViS Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.10945/x2.png",
                "caption": "Figure 2:Flowchart of the language expression annotation and validation process of MeViS. Samples that are found ambiguous or too simple by validators will be rejected and discarded.",
                "position": 236
            },
            {
                "img": "https://arxiv.org/html/2512.10945/x3.png",
                "caption": "Figure 3:Examples of the newly added motion reasoning and no-target expressions in MeViSv2. Motion reasoning expressions refer to the targets the masked in orange, while no-target expressions, though deceptive, do not refer to any objects.",
                "position": 542
            },
            {
                "img": "https://arxiv.org/html/2512.10945/x4.png",
                "caption": "Figure 4:The duration of videos and objects of MeViS and Refer-YouTube-VOS[seo2020urvos], in seconds. The vertical lines and values in the legends represent the mean duration across the two datasets. The duration of both videos and objects in MeViS is significantly longer than Refer-YouTube-VOS.",
                "position": 553
            },
            {
                "img": "https://arxiv.org/html/2512.10945/x5.png",
                "caption": "",
                "position": 556
            },
            {
                "img": "https://arxiv.org/html/2512.10945/x6.png",
                "caption": "Figure 5:Comparison of MeViS and Refer-YouTube-VOS[seo2020urvos]: (a) Example from MeViS. (b) Example from Refer-YouTube-VOS[seo2020urvos]. Compared to Refer-YouTube-VOS:∙\\bulletVideos in MeViS containmore objectsin complex environments, making it impossible to identify the target object via saliency or category information alone.∙\\bulletThe number of target objects indicated by language expression in MeViS isarbitrary, from 0 to many.",
                "position": 569
            },
            {
                "img": "https://arxiv.org/html/2512.10945/x7.png",
                "caption": "Figure 6:Word cloud of the top 100 words in the MeViS dataset. MeViS has a large number of words that describe motions, like “walking”, “moving”, “playing”, and many position words that are related to motions, such as “left”, “right”.",
                "position": 592
            },
            {
                "img": "https://arxiv.org/html/2512.10945/x8.png",
                "caption": "Figure 7:The overview architecture of the proposed Language-guided Motion Perception and Matching (LMPM++). We first detect all possible target objects in each video frame via Language-Guided Extractor and represent them using object embeddings. Then, a large language model is used to capture and reason the global temporal context from object embeddings, outputting the number of target objectsNoN_{o}and the correspondingNoN_{o}<SEG>tokens for subsequent mask generation. Temporal-level contrastive loss is designed to enhance the understanding of temporal structure.\nFinally, object mask trajectories are generated using<SEG>token with Mask Decoder.",
                "position": 609
            }
        ]
    },
    {
        "header": "4LMPM++: A Baseline Approach",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.10945/x9.png",
                "caption": "Figure 8:Examples (a)-(d) show success cases, while (e)-(h) display failure cases of LMPM++. Examples (c)-(d) and (f)-(h) correspond to no-target samples.",
                "position": 1118
            },
            {
                "img": "https://arxiv.org/html/2512.10945/x10.png",
                "caption": "Figure 9:REMG failure cases of VideoLLaMA 2[cheng2024videollama2advancingspatialtemporal]on MeViS.",
                "position": 1832
            }
        ]
    },
    {
        "header": "6Conclusion and Discussion",
        "images": []
    }
]