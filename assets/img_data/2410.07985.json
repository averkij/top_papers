[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07985/extracted/5918443/img/github-sign.png",
                "caption": "",
                "position": 144
            },
            {
                "img": "https://arxiv.org/html/2410.07985/extracted/5918443/img/FullLogo_Transparent.png",
                "caption": "",
                "position": 149
            },
            {
                "img": "https://arxiv.org/html/2410.07985/x1.png",
                "caption": "",
                "position": 154
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07985/x2.png",
                "caption": "Figure 1:Comparisons among different models on GSM8K, MATH, andOmni-MATH, where the models are ranked based on their performance on MATH, and those marked with ”⋆⋆\\star⋆” are closed-source models. As observed, the iterative advancements of these models show that existing benchmarks are nearing saturation. Our proposed Omni-MATH introduces a challenging benchmark to further advance mathematical intelligence in large language models.",
                "position": 168
            },
            {
                "img": "https://arxiv.org/html/2410.07985/x3.png",
                "caption": "Figure 2:An overall illustration of Omni-MATH. The left-top part presents the diverse and well-structured data sources of Omni-MATH. The left-bottom part represents the hierarchical mathematical domains of Omni-Math. The right part presents a concrete data example of Omni-Math.",
                "position": 202
            }
        ]
    },
    {
        "header": "2Omni-MATH Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07985/x4.png",
                "caption": "Figure 3:The overall data collection and annotation process of Omni-MATH.",
                "position": 310
            },
            {
                "img": "https://arxiv.org/html/2410.07985/x5.png",
                "caption": "Figure 4:Difficulty distribution across contests.",
                "position": 386
            },
            {
                "img": "https://arxiv.org/html/2410.07985/x6.png",
                "caption": "Figure 5:Examples of the problem, reference answer, and model-generated answer from different datasets, where the model-generated answers are all correct. On the right side, model-generated answers have issues in reformatting or requiring additional reasoning to correctly evaluate.",
                "position": 410
            }
        ]
    },
    {
        "header": "3Olympiad-Level Math Evaluation on Existing LLMs",
        "images": []
    },
    {
        "header": "4Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07985/x7.png",
                "caption": "Figure 6:Results of data selection experiments. (a) The impact of different demonstration domains on model performance. (b) The effect of the distance within the domain tree on model performance. (c) The influence of the difficulty level of in-context examples on model performance.",
                "position": 900
            },
            {
                "img": "https://arxiv.org/html/2410.07985/x8.png",
                "caption": "Figure 7:Performance and Consistency Comparison of Different Models. Consistency is measured by Equation1. The difficulty consistency represents shows that our difficulty level is generally aligned with the performance of models.",
                "position": 948
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAnalysis of process-level Assessment",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07985/extracted/5918443/img/Pi-Process-eval.png",
                "caption": "(a)",
                "position": 1468
            },
            {
                "img": "https://arxiv.org/html/2410.07985/extracted/5918443/img/Pi-Process-eval.png",
                "caption": "(a)",
                "position": 1471
            },
            {
                "img": "https://arxiv.org/html/2410.07985/extracted/5918443/img/radar_process_eval_scaled_y_axis_with_finer_grid.png",
                "caption": "(b)",
                "position": 1476
            }
        ]
    },
    {
        "header": "Appendix BDetailed Data Leakage Information",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07985/extracted/5918443/img/contamination.png",
                "caption": "(a) Number of instances of different models affected by contamination and the correct instances under contaminated conditions.",
                "position": 1498
            },
            {
                "img": "https://arxiv.org/html/2410.07985/extracted/5918443/img/contamination.png",
                "caption": "(a) Number of instances of different models affected by contamination and the correct instances under contaminated conditions.",
                "position": 1501
            },
            {
                "img": "https://arxiv.org/html/2410.07985/extracted/5918443/img/contamination_difficulty.png",
                "caption": "(b) Average difficulty of contaminated and correct instances.",
                "position": 1506
            }
        ]
    },
    {
        "header": "Appendix CAnnotation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07985/x9.png",
                "caption": "Figure 10:The illustration of the UI page of the annotation task.",
                "position": 1520
            }
        ]
    },
    {
        "header": "Appendix DData crawling details",
        "images": []
    },
    {
        "header": "Appendix EDetailed Data Information of Omni-MATH",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07985/x10.png",
                "caption": "Figure 11:The difficulty distribution of Omni-MATH.",
                "position": 1747
            },
            {
                "img": "https://arxiv.org/html/2410.07985/x11.png",
                "caption": "Figure 12:The total domain tree of Omni-MATH.",
                "position": 1754
            },
            {
                "img": "https://arxiv.org/html/2410.07985/x12.png",
                "caption": "Figure 13:The detailed difficulty classification prompt.",
                "position": 1764
            },
            {
                "img": "https://arxiv.org/html/2410.07985/x13.png",
                "caption": "Figure 14:The detailed domain classification prompt.",
                "position": 1774
            }
        ]
    },
    {
        "header": "Appendix FDetailed Experimental Setup",
        "images": []
    },
    {
        "header": "Appendix GMetrics with Distinct Difficulty Level",
        "images": []
    },
    {
        "header": "Appendix HDetailed Evaluation Information",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07985/x14.png",
                "caption": "Figure 15:The evaluation prompt of GPT-4o.",
                "position": 2067
            },
            {
                "img": "https://arxiv.org/html/2410.07985/x15.png",
                "caption": "Figure 16:The case study of GPT-4o-based evalution.",
                "position": 2078
            },
            {
                "img": "https://arxiv.org/html/2410.07985/x16.png",
                "caption": "Figure 17:Results of predicted accuracy on different models, conducted by Omni-Judge and GPT-4o, respectively.",
                "position": 2210
            }
        ]
    },
    {
        "header": "Appendix IAnalysis of Omni-Judge",
        "images": []
    }
]