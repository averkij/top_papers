[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Proposed Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25716/arch_big.png",
                "caption": "Figure 1:Overview of the proposed multi-stage retrieval pipeline.",
                "position": 273
            }
        ]
    },
    {
        "header": "4Dataset and Index Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25716/io_refined.png",
                "caption": "Figure 2:Task anatomy:highlighting howcode_before, andcode_afterare used to recover the ground-truth Script Include required forcode_middle.",
                "position": 343
            }
        ]
    },
    {
        "header": "5Experimental Setup",
        "images": []
    },
    {
        "header": "6Experiments and Results",
        "images": []
    },
    {
        "header": "7Post Training and Optimization",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25716/images/performace_3_combined.png",
                "caption": "Figure 3:Qwen 0.6B: Top-5 accuracy after finetuning, Mean Reciprocal Rank (MRR) and Inference latency",
                "position": 638
            }
        ]
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Declaration on Generative AI",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAblation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25716/images/code_trim.png",
                "caption": "Figure 4:A4 code trimming and context length",
                "position": 1018
            },
            {
                "img": "https://arxiv.org/html/2509.25716/images/code_before_after_ablation.png",
                "caption": "Figure 5:Performance of Code Context Ablation",
                "position": 1028
            },
            {
                "img": "https://arxiv.org/html/2509.25716/images/code_line_proximity.png",
                "caption": "Figure 6:Effect of Code Proximity on Performance",
                "position": 1045
            }
        ]
    },
    {
        "header": "Appendix BKnowledge Graph Analysis",
        "images": []
    },
    {
        "header": "Appendix CAblation Study on Model Selection",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25716/ablation_plot.png",
                "caption": "Figure 7:Ablation study comparing the retrieval performance of various models.",
                "position": 1063
            }
        ]
    },
    {
        "header": "Appendix DCode Summarization Strategy",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25716/images/jsdoc_descr_compare.png",
                "caption": "Figure 8:Performance comparison between JSDoc-based indexing and raw code descriptions across different retrieval methods. JSDoc indexing shows consistent improvements in retrieval accuracy.",
                "position": 1073
            }
        ]
    },
    {
        "header": "Appendix ETraining Loss Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25716/images/SFT_train_loss.png",
                "caption": "Figure 9:Training loss progression during supervised fine-tuning (SFT).",
                "position": 1089
            },
            {
                "img": "https://arxiv.org/html/2509.25716/images/SFT_epoch_loss.png",
                "caption": "Figure 10:Epoch-wise loss during supervised fine-tuning (SFT).",
                "position": 1095
            },
            {
                "img": "https://arxiv.org/html/2509.25716/rl/train_reward.png",
                "caption": "Figure 11:Reward progression during reinforcement learning training.",
                "position": 1105
            },
            {
                "img": "https://arxiv.org/html/2509.25716/rl/train_loss.png",
                "caption": "Figure 12:Training loss during reinforcement learning.",
                "position": 1115
            }
        ]
    },
    {
        "header": "Appendix FOut-of-Distribution Generalization",
        "images": []
    },
    {
        "header": "Appendix GReward Function for Reinforcement Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25716/rl/train_rewards_reward_yesno_from_completion_mean.png",
                "caption": "Figure 13:Log-probability of answering yes on positive labels.",
                "position": 1203
            }
        ]
    },
    {
        "header": "Appendix HAnalysis of Query Enhancement Techniques",
        "images": []
    },
    {
        "header": "Appendix IQualitative Examples of Retrieval Techniques",
        "images": []
    },
    {
        "header": "Appendix JPrompts for AI Models",
        "images": []
    }
]