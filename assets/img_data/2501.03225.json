[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.03225/x1.png",
                "caption": "Figure 1:Overview.(Left)We analyze existing open-ended VQA evaluation metrics, underscoring their limitations in providing accurate and reproducible assessments.(Middle)We introduceAutoConverter, a multi-agent system that automatically converts open-ended questions into multiple-choice format, enabling objective assessment while reducing the costly question creation process.(Right)UsingAutoConverter, we convert and refine 20 existing VQA datasets into a unified multiple-choice benchmark to support future VLM research.",
                "position": 95
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.03225/x2.png",
                "caption": "Figure 2:Challenges in evaluating open-ended questions.(Left)Rule-based metrics significantly underestimate model performance and penalize models that do not strictly follow the expected format.(Right)Model-based evaluations using two different versions of GPT yield substantially different scores, making comparisons inconsistent and raising reproducibility issues.",
                "position": 126
            }
        ]
    },
    {
        "header": "3Open-Ended Question Evaluation Challenge",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.03225/x3.png",
                "caption": "(a)AutoConverterframework.",
                "position": 143
            },
            {
                "img": "https://arxiv.org/html/2501.03225/x3.png",
                "caption": "(a)AutoConverterframework.",
                "position": 146
            }
        ]
    },
    {
        "header": "4AutoConverter: An Agentic Pipeline Generating Challenging Multi-Choice Questions",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/figures/superhuman_v2.png",
                "caption": "Figure 4:AutoConvertergenerates challenging multiple-choice questions.UsingAutoConverter, we generated distractors for questions and answers from three existing multiple-choice datasets: MMMU, MathVista, and AI2D, and compared them with original human-created distractors. We evaluated various VLMs on both theAutoConverter-generated and the original questions, finding that VLMs consistently achieved similar or even lower accuracy on theAutoConverter-generated questions compared to the original ones.",
                "position": 298
            },
            {
                "img": "https://arxiv.org/html/2501.03225/x4.png",
                "caption": "Figure 5:Qualitative comparison of the original questions, naive baseline-generated questions, andAutoConverter-generated questions.AutoConvertersimulates errors from different perspectives and produces correct and challenging multiple-choice questions.",
                "position": 331
            },
            {
                "img": "https://arxiv.org/html/2501.03225/x5.png",
                "caption": "Figure 6:VMCBenchoverview.(Left)VMCBenchis constructed by converting 12 open-ended (OE) and refining 8 multiple-choice (MC) VQA datasets into a unified multiple-choice format, with human validation ensuring correctness. The number of questions per dataset is listed.(Right)Example questions fromVMCBench, showcasing diverse question types across multiple domains.",
                "position": 354
            }
        ]
    },
    {
        "header": "5VMCBench: A Unified Multiple-Choice Visual Question Answering Benchmark",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Broader Impacts and Ethics Statement",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Summary of Appendix",
        "images": []
    },
    {
        "header": "Appendix ASupplementary Section 3",
        "images": []
    },
    {
        "header": "Appendix BSupplementary Section 4",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/rule1.png",
                "caption": "Table 2:Examples of rule-based evaluation failures of open-ended questions.Rule-based methods fail to account for semantic similarity and penalize formatting errors, resulting in highly inaccurate evaluation results.",
                "position": 1718
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/rule2.png",
                "caption": "",
                "position": 1790
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/rule3.png",
                "caption": "",
                "position": 1822
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/rule4.png",
                "caption": "",
                "position": 1854
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/rule5.png",
                "caption": "",
                "position": 1886
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/rule6.png",
                "caption": "",
                "position": 1918
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/model1.jpg",
                "caption": "Table 3:Examples of model-based evaluation failures of open-ended questions.GPT-4o-0806 often assigns a perfect score of for similar predictions and answers, whereas GPT-4o-0513 tends to assign a score of 0.9. This behavior variation introduces significant differences in evaluation results and raises concerns about reproducibility in future research.",
                "position": 1951
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/model2.jpg",
                "caption": "",
                "position": 2023
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/model3.jpg",
                "caption": "",
                "position": 2055
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/model4.jpg",
                "caption": "",
                "position": 2087
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/model5.jpg",
                "caption": "",
                "position": 2119
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/model6.jpg",
                "caption": "",
                "position": 2151
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/figures/superhuman1.png",
                "caption": "Figure 7:AutoConvertergenerates challenging multiple-choice questions.UsingAutoConverter, we generated distractors for questions and answers from five multiple-choice datasets: A-OKVQA, RealWorldQA, ScienceQA, SEEDBench, and MMStar, and compared them with original human-created distractors. We evaluated various VLMs on both theAutoConverter-generated and the original questions, finding that VLMs consistently achieved similar or even lower accuracy on theAutoConverter-generated questions compared to the original ones.",
                "position": 3143
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/figures/superhuman2.png",
                "caption": "",
                "position": 3147
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/figures/different_generator.png",
                "caption": "Figure 8:AutoConverterresults for different models.To examine whether GPT-4o used inAutoConverterintroduces model bias, we used three state-of-the-art proprietary VLMs—GPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-Pro—to generate questions. We evaluated 18 VLMs on these questions and computed the correlations of their performance rankings. We observe high correlations across all question sets, indicating no model bias exists, and these questions reflect the true discriminative power of the models.",
                "position": 3153
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/figures/corr1.png",
                "caption": "Figure 9:Converting to multiple-choice questions improves evaluation accuracy and retains discriminative power.We treat model-based evaluation of open-ended questions as a proxy for ground-truth evaluation. We compare the correlation between model-based evaluation of open-ended questions and rule-based evaluation of multiple-choice questions against the correlation between model-based and rule-based evaluation of open-ended questions. We find that the correlation for multiple-choice questions is significantly higher compared to rule-based open-ended evaluations, demonstrating that converting open-ended questions into multiple-choice format preserves their discriminative power and simplifies the evaluation.",
                "position": 3156
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/figures/corr2.png",
                "caption": "",
                "position": 3160
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/figures/corr3.png",
                "caption": "",
                "position": 3162
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/figures/scaling_overall.png",
                "caption": "Figure 10:Scaling trends onVMCBench.We observe a clearlog-linearscaling trend across most VLM families, indicating thatVMCBenchoffers a smooth evaluation gradient for varying capabilities.",
                "position": 3166
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/figures/scaling1.png",
                "caption": "",
                "position": 3169
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/figures/scaling2.png",
                "caption": "",
                "position": 3171
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/figures/scaling3.png",
                "caption": "",
                "position": 3172
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/figures/scaling4.png",
                "caption": "",
                "position": 3174
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/8255.jpg",
                "caption": "Table 5:Examples ofVMCBench(1/7).Each example has a dataset source, image, question, and four choices (correct choice highlighted inorange).",
                "position": 4376
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/7998.jpg",
                "caption": "",
                "position": 4436
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/8430.jpg",
                "caption": "",
                "position": 4461
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/1667.jpg",
                "caption": "",
                "position": 4486
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/1293.jpg",
                "caption": "",
                "position": 4511
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/1275.jpg",
                "caption": "",
                "position": 4536
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/7120.jpg",
                "caption": "",
                "position": 4561
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/7076.jpg",
                "caption": "",
                "position": 4586
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/7073.jpg",
                "caption": "",
                "position": 4611
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/5929.jpg",
                "caption": "Table 6:Examples ofVMCBench(2/7).Each example has a dataset source, image, question, and four choices (correct choice highlighted inorange).",
                "position": 4632
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/5663.jpg",
                "caption": "",
                "position": 4692
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/5758.jpg",
                "caption": "",
                "position": 4717
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/7896.jpg",
                "caption": "",
                "position": 4742
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/7537.jpg",
                "caption": "",
                "position": 4767
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/7776.jpg",
                "caption": "",
                "position": 4792
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/5343.jpg",
                "caption": "",
                "position": 4817
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/4993.jpg",
                "caption": "",
                "position": 4842
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/5165.jpg",
                "caption": "",
                "position": 4867
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/343.jpg",
                "caption": "Table 7:Examples ofVMCBench(3/7).Each example has a dataset source, image, question, and four choices (correct choice highlighted inorange).",
                "position": 4888
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/284.jpg",
                "caption": "",
                "position": 4948
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/333.jpg",
                "caption": "",
                "position": 4973
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/1194.jpg",
                "caption": "",
                "position": 4998
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/1202.jpg",
                "caption": "",
                "position": 5023
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/916.jpg",
                "caption": "",
                "position": 5048
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/2784.jpg",
                "caption": "",
                "position": 5073
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/2773.jpg",
                "caption": "",
                "position": 5098
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/2902.jpg",
                "caption": "",
                "position": 5123
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/3460.jpg",
                "caption": "Table 8:Examples ofVMCBench(4/7).Each example has a dataset source, image, question, and four choices (correct choice highlighted inorange).",
                "position": 5144
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/3221.jpg",
                "caption": "",
                "position": 5204
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/3270.jpg",
                "caption": "",
                "position": 5229
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/755.jpg",
                "caption": "",
                "position": 5254
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/766.jpg",
                "caption": "",
                "position": 5279
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/583.jpg",
                "caption": "",
                "position": 5304
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/6164.jpg",
                "caption": "",
                "position": 5329
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/6102.jpg",
                "caption": "",
                "position": 5354
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/6145.jpg",
                "caption": "",
                "position": 5379
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/8935.jpg",
                "caption": "Table 9:Examples ofVMCBench(5/7).Each example has a dataset source, image, question, and four choices (correct choice highlighted inorange).",
                "position": 5400
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/8827.jpg",
                "caption": "",
                "position": 5460
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/8885.jpg",
                "caption": "",
                "position": 5485
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/4406.jpg",
                "caption": "",
                "position": 5510
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/4389.jpg",
                "caption": "",
                "position": 5535
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/4311.jpg",
                "caption": "",
                "position": 5560
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/2720.jpg",
                "caption": "",
                "position": 5585
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/2551.jpg",
                "caption": "",
                "position": 5610
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/2614.jpg",
                "caption": "",
                "position": 5635
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/2139.jpg",
                "caption": "Table 10:Examples ofVMCBench(6/7).Each example has a dataset source, image, question, and four choices (correct choice highlighted inorange).",
                "position": 5656
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/2001.jpg",
                "caption": "",
                "position": 5716
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/1815.jpg",
                "caption": "",
                "position": 5741
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/3799.jpg",
                "caption": "",
                "position": 5766
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/3764.jpg",
                "caption": "",
                "position": 5791
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/3504.jpg",
                "caption": "",
                "position": 5816
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/4635.jpg",
                "caption": "",
                "position": 5841
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/4515.jpg",
                "caption": "",
                "position": 5866
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/4865.jpg",
                "caption": "",
                "position": 5891
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/9311.jpg",
                "caption": "Table 11:Examples ofVMCBench(7/7).Each example has a dataset source, image, question, and four choices (correct choice highlighted inorange).",
                "position": 5912
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/9364.jpg",
                "caption": "",
                "position": 5972
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/8959.jpg",
                "caption": "",
                "position": 5997
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/6886.jpg",
                "caption": "",
                "position": 6022
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/6830.jpg",
                "caption": "",
                "position": 6047
            },
            {
                "img": "https://arxiv.org/html/2501.03225/extracted/6113068/images/6891.jpg",
                "caption": "",
                "position": 6072
            }
        ]
    },
    {
        "header": "Appendix CSupplementary Section 5",
        "images": []
    }
]