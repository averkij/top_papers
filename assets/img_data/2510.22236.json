[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22236/x1.png",
                "caption": "Figure 1:Diffusion model for lane detection. (a) The diffusion model whereqqdenotes the diffusion process andpθp_{\\theta}represents the reverse process. (b)Noise-to-imageprocess from noisy pixels to target image. (c) Ournoise-to-laneparadigm from noisy lane anchors to target lanes.",
                "position": 113
            }
        ]
    },
    {
        "header": "Related Work",
        "images": []
    },
    {
        "header": "Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22236/x2.png",
                "caption": "Figure 2:Illustration of lane representation. We take 7 sampling points as the example.",
                "position": 183
            },
            {
                "img": "https://arxiv.org/html/2510.22236/x3.png",
                "caption": "Figure 3:Training pipeline of DiffusionLane. The image encoder and SAFPN(Xiao et al.2023)extract the multi-scale image featuresMi,i∈[0,2]M_{i},i\\in[0,2]and feed them into the hybrid diffusion decoder. The hybrid diffusion decoder, containing a stack of hybrid diffusion blocks, refines the noisy lanes to target lanes iteratively.\nAuxiliary head, only used in the training process, improves the feature representation of the encoder via learnable lane anchors.",
                "position": 186
            },
            {
                "img": "https://arxiv.org/html/2510.22236/x4.png",
                "caption": "Figure 4:Visualization of the attention map of the encoder.",
                "position": 284
            },
            {
                "img": "https://arxiv.org/html/2510.22236/x5.png",
                "caption": "Figure 5:Architecture of a hybrid diffusion block.",
                "position": 313
            }
        ]
    },
    {
        "header": "Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22236/x6.png",
                "caption": "Figure 6:Visualization results of CLRNet, Lane2Seq, and DiffusionLane on four benchmarks.",
                "position": 1078
            }
        ]
    },
    {
        "header": "Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMethod",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experiments",
        "images": []
    },
    {
        "header": "Appendix CLimitation and Broader Impact",
        "images": []
    }
]