[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15098/x1.png",
                "caption": "",
                "position": 103
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15098/x2.png",
                "caption": "Figure 2:Overview of the Diffusion Transformer (DiT) architecture and integration methods for image conditioning.",
                "position": 142
            }
        ]
    },
    {
        "header": "2Related works",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15098/x3.png",
                "caption": "Figure 3:Comparison of results using two methods for integrating image conditions. The multi-modal approach demonstrates better condition following compared to direct addition.",
                "position": 252
            },
            {
                "img": "https://arxiv.org/html/2411.15098/x4.png",
                "caption": "(a)Training losses for different image condition integration methods.",
                "position": 275
            },
            {
                "img": "https://arxiv.org/html/2411.15098/x4.png",
                "caption": "(a)Training losses for different image condition integration methods.",
                "position": 278
            },
            {
                "img": "https://arxiv.org/html/2411.15098/x5.png",
                "caption": "(b)Training loss for shared vs. shifting position.",
                "position": 283
            },
            {
                "img": "https://arxiv.org/html/2411.15098/x6.png",
                "caption": "Figure 5:(a) Attention maps for the Canny-to-image task, showing interactions between noisy image tokensXùëãXitalic_Xand image condition tokensCIsubscriptùê∂ùêºC_{I}italic_C start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT. Strong diagonal patterns indicate effective spatial alignment. (b) Subject-driven generation task, with input condition and output image. Attention maps forX‚ÜíCi‚Üíùëãsubscriptùê∂ùëñX\\to C_{i}italic_X ‚Üí italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTandCi‚ÜíX‚Üísubscriptùê∂ùëñùëãC_{i}\\to Xitalic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ‚Üí italic_Xillustrate accurate subject-focused attention.",
                "position": 303
            },
            {
                "img": "https://arxiv.org/html/2411.15098/extracted/6018223/fig/data.png",
                "caption": "Figure 6:Examples from our Subjects200Kdataset. Each pair of images shows the same object in varying positions, angles, and lighting conditions.\nThe dataset includes diverse objects such as clothing, furniture, vehicles, and animals, totaling over 200,000 images.\nThis dataset, along with the generation pipeline, will be publicly released.",
                "position": 385
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15098/extracted/6018223/fig/compair.jpg",
                "caption": "Figure 7:Qualitative results comparing different methods.\nLeft: Spatially aligned tasks across Canny, depth, out-painting, deblurring, colorization.\nRight: Subject-driven generation with beverage can, shoes and robot toy.\nOur method demonstrates superior controllability and visual quality across all tasks.",
                "position": 615
            },
            {
                "img": "https://arxiv.org/html/2411.15098/x7.png",
                "caption": "Figure 8:Radar charts visualization comparing our method (blue) with baselines across five evaluation metrics.",
                "position": 634
            },
            {
                "img": "https://arxiv.org/html/2411.15098/x8.png",
                "caption": "Figure 9:Comparison of models trained with different data.\nThe model trained by data augmentation tends to copy inputs directly, while model trained by our Subjects200Kgenerates novel views while preserving identity.",
                "position": 738
            },
            {
                "img": "https://arxiv.org/html/2411.15098/x9.png",
                "caption": "Figure 10:Comparison of models trained with different data.\nThe model trained by data augmentation tends to copy inputs directly, while model trained by our Subjects200Kgenerates novel views while preserving identity.",
                "position": 763
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails of Subjects200K¬†datasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15098/x10.png",
                "caption": "Figure S1:Examples of successful and failed generation results from Subjects200K¬†dataset. Green checks indicate successful cases where subject identity and characteristics are well preserved, while red crosses show failure cases.",
                "position": 1485
            },
            {
                "img": "https://arxiv.org/html/2411.15098/x11.png",
                "caption": "Figure S4:User study results comparing different methods across three metrics: identity consistency, text-image alignment, and visual coherence.",
                "position": 1727
            },
            {
                "img": "https://arxiv.org/html/2411.15098/extracted/6018223/fig/dreambooth.jpg",
                "caption": "Figure S5:More results on Dreambooth dataset.",
                "position": 1746
            },
            {
                "img": "https://arxiv.org/html/2411.15098/extracted/6018223/fig/more_results.jpg",
                "caption": "Figure S6:More results on other subject-driven generation tasks.",
                "position": 1753
            }
        ]
    },
    {
        "header": "Appendix BAdditional experimental results",
        "images": []
    }
]