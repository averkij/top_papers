[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.14981/x1.png",
                "caption": "",
                "position": 162
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3SpatialGenDataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.14981/figures/dataset.png",
                "caption": "Figure 2:Illustration of our dataset. For each scene, we provide comprehensive panoramic renderings and 3D layout annotation.",
                "position": 334
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.14981/x2.png",
                "caption": "Figure 3:Overall pipeline.SpatialGentakes as input a 3D semantic layout and one or more posed images, to create a 3D scene. First, we generate per-view RGB images, scene coordinate maps, and semantic segmentation maps from a Layout-Guided Multi-view Multi-modal diffusion model. Then, we adopt an iterative dense view generation strategy to generate images at more sampled viewpoints. Finally, these images are fed into a 3D reconstruction method to produce the final result.",
                "position": 353
            },
            {
                "img": "https://arxiv.org/html/2509.14981/x3.png",
                "caption": "Figure 4:Multi-view and multi-modal alternating attention. It alternates between enforcing multi-view consistency and multi-modal fidelity within a unified attention mechanism.",
                "position": 432
            },
            {
                "img": "https://arxiv.org/html/2509.14981/x4.png",
                "caption": "Figure 5:Comparison of reconstruction results for scene coordinate map. The image VAE (a) generates noisy results, and the SCM-VAE without gradient loss (b) produces distorted results. Our SCM-VAE (c) accurately reconstructs the scene geometry.",
                "position": 444
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.14981/x5.png",
                "caption": "Figure 6:Qualitative comparison to score distillation methods on Hypersim[34](top row) and our dataset (bottom row), In each case, we show the generated color images and depth maps.",
                "position": 554
            },
            {
                "img": "https://arxiv.org/html/2509.14981/x6.png",
                "caption": "Figure 7:Qualitative comparison between our method and the panorama-as-proxy baseline[8]on Structured3D[62](top row) and our dataset (bottom row).",
                "position": 725
            },
            {
                "img": "https://arxiv.org/html/2509.14981/x7.png",
                "caption": "Figure 8:Qualitative comparison of image-to-3D scene generation on our dataset. Given a single input image, our method with layout guidance consistently generates better color images, scene coordinate maps, and semantic maps.",
                "position": 860
            }
        ]
    },
    {
        "header": "6Conclusion & Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.14981/x8.png",
                "caption": "Figure 9:Examples ofSpatialGendataset.",
                "position": 1761
            }
        ]
    },
    {
        "header": "Appendix ASpatialGenDataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.14981/figures/dataset/intersection.jpg",
                "caption": "Figure 10:Example of low-quality renderings.",
                "position": 1790
            },
            {
                "img": "https://arxiv.org/html/2509.14981/figures/dataset/over-exposure.jpg",
                "caption": "",
                "position": 1795
            },
            {
                "img": "https://arxiv.org/html/2509.14981/figures/dataset/insufficient_illumination.jpg",
                "caption": "",
                "position": 1796
            },
            {
                "img": "https://arxiv.org/html/2509.14981/x9.png",
                "caption": "Figure 11:Camera configuration.",
                "position": 1807
            },
            {
                "img": "https://arxiv.org/html/2509.14981/x10.png",
                "caption": "Figure 12:Room type distribution.",
                "position": 1827
            },
            {
                "img": "https://arxiv.org/html/2509.14981/x11.png",
                "caption": "Figure 13:Object category distribution.",
                "position": 1835
            }
        ]
    },
    {
        "header": "Appendix BAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.14981/x12.png",
                "caption": "Figure 14:Qualitative comparison of text-to-3D scene on Hypersim[34]dataset. In each case, we show the generated color images and depth map.",
                "position": 1860
            },
            {
                "img": "https://arxiv.org/html/2509.14981/x13.png",
                "caption": "Figure 15:Qualitative comparison of text-to-3D scene onSpatialGendataset. In each case, we show the generated color images.",
                "position": 1863
            },
            {
                "img": "https://arxiv.org/html/2509.14981/x14.png",
                "caption": "Figure 16:Qualitative comparison with Ctrl-Room on Structured3D for panorama generation. We split the panorama into eight perspective images for a direct comparison. Our method achieves competitive RGB synthesis compared with Ctrl-Room, resulting in photo-realistic scenes that are well-aligned with the provided layout.",
                "position": 1866
            },
            {
                "img": "https://arxiv.org/html/2509.14981/x15.png",
                "caption": "Figure 17:Qualitative comparison with Ctrl-Room onSpatialGendataset. Ctrl-Room exhibits severe stretching artifacts and scale misalignments at novel viewpoints. In contrast, our method consistently produces photorealistic and fully 3D-consistent renderings from all views.",
                "position": 1869
            },
            {
                "img": "https://arxiv.org/html/2509.14981/x16.png",
                "caption": "Figure 18:Qualitative results onSpatialGendataset under various camera trajectories. From left to right: input view and target views.First Row (forward): sampled views follow a progressive forward-moving path.Second row (inward orbit): views are directed toward the center of the room, ensuring substantial overlap between them.Third row (outward orbit): views are positioned at the center of the room, looking outward, with an angle of less than45âˆ˜45^{\\circ}between two adjacent views.Bottom (random walk): views are selected from a continuous random-walk camera trajectory, producing aggressive viewpoint changes.",
                "position": 1886
            },
            {
                "img": "https://arxiv.org/html/2509.14981/x17.png",
                "caption": "Figure 19:Comparing geometric prediction quality between our(W/ layout)and(W/O layout). The first two columns show the predicted scene coordinate maps, where our method (W/ layout) achieves better alignment with ground-truth geometry (brown color point cloud) compared to the counterpart without layout guidance (W/O layout). Correspondingly, the warped images projected by the predicted scene coordinates demonstrate improved spatial consistency and reduced artifacts.",
                "position": 1906
            },
            {
                "img": "https://arxiv.org/html/2509.14981/x18.png",
                "caption": "Figure 20:Ablation on the effectiveness of layout as guidance.",
                "position": 1909
            },
            {
                "img": "https://arxiv.org/html/2509.14981/x19.png",
                "caption": "Figure 21:Video-to-New-3D Scene Generation on the SpatialLM Test set[27]. By leveraging the state-of-the-art scene layout estimation method, SpatialLM[27], we get the reconstructed 3D layout from the video. Then, we perform text-to-3D scene generation conditioned on this layout and additional user-provided text prompts. For clearer visualization of 3D consistency and multi-modal prediction capabilities, we put depth maps here instead of displaying the coordinate maps directly.",
                "position": 1974
            }
        ]
    },
    {
        "header": "Appendix CAdditional Experiments",
        "images": []
    }
]