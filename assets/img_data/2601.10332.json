[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10332/x1.png",
                "caption": "Figure 1:An overview of ourthink-then-generatemethod. Beyond using LLM as a frozen text encoder, we train it to think and refine the raw user prompts guided by the reward of output images.",
                "position": 103
            },
            {
                "img": "https://arxiv.org/html/2601.10332/x2.png",
                "caption": "Figure 2:Comparison of T2I models on conceptual visual generation.Our Qwen-Image underthink-then-generatepipeline produces semantically aligned and visually coherent results, correctly interpreting user intent given prompt “Holiday celebrating the birth of Jesus Christ.” (e.g., generating a warm Christmas celebration rather than literally depicting Jesus), whereas vanilla Qwen-Image behaves like a simple text–pixel mapper and often fails to capture conceptual meanings.",
                "position": 108
            }
        ]
    },
    {
        "header": "2Preliminary: Group Relative PolicyOptimization",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10332/x3.png",
                "caption": "Figure 3:t-SNE visualization of the embedding distributions before and after SFT.The distributions remain highly consistent, indicating that our SFT procedure preserves the embedding space structure and thus maintains compatibility with the DiT, enabling it to render stable and coherent visual outputs.",
                "position": 208
            },
            {
                "img": "https://arxiv.org/html/2601.10332/x4.png",
                "caption": "Figure 4:Dual-GRPO training trajectories. (a) Tree-structured rollout for a given user promptqq: the LLM encoder samplesJJreasoning traces, each rewritten prompt conditions the DiT to generateKKimages. Image-grounded rewards are aggregated to compute group-relative advantages for updating both the LLM and the DiT. (b) Evolution of alignment and style scores during training, demonstrating how DiT training improves both semantic alignment and visual quality over time.",
                "position": 224
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10332/x5.png",
                "caption": "Figure 5:Comparison of T2I models on conceptual image editing.Vanilla Qwen-Image fails to interpret instructions (e.g., showing an ice cream under sunlight instead of melting), behaving as a text–pixel mapper. Our model correctly infers intended semantics, producing coherent, aesthetically pleasing edits with high consistency to the original image, outperforming unified models like Emu2 and Bagel.",
                "position": 570
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "1Ablation on Different Reward Schedulers",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10332/x6.png",
                "caption": "Figure 6:Evolution of alignment and style scores during training with the new scheduler, whereβ1​(τ)=β2​(τ)=0.5\\beta_{1}(\\tau)=\\beta_{2}(\\tau)=0.5, and samples demonstrating how DiT training improves both semantic alignment and visual quality over time.",
                "position": 1553
            },
            {
                "img": "https://arxiv.org/html/2601.10332/x7.png",
                "caption": "Figure 7:Comparison of scores on user study. Our model performs closely to GPT-4o and clearly surpasses Qwen-Image, highlighting its strong ability in challenging real-world T2I scenarios.",
                "position": 1556
            },
            {
                "img": "https://arxiv.org/html/2601.10332/x8.png",
                "caption": "Figure 8:User interface for our human evaluation study. Images 1–4 correspond to Bagel-think, our model, vanilla Qwen-Image, and GPT-4o, respectively. Notably, our model provides the most comprehensive deduction for this math problem, uniquely arriving at the correct final answer, thereby demonstrating its superior reasoning ability and practical effectiveness in real-world tasks.",
                "position": 1559
            }
        ]
    },
    {
        "header": "2Detailed Results on Image Editing Task",
        "images": []
    },
    {
        "header": "3User Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10332/x9.png",
                "caption": "Figure 9:More demos for image editing task.",
                "position": 1705
            },
            {
                "img": "https://arxiv.org/html/2601.10332/x10.png",
                "caption": "Figure 10:More demos for the T2I task.",
                "position": 1708
            }
        ]
    },
    {
        "header": "4More Demos",
        "images": []
    }
]