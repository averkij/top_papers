[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.06426/x1.png",
                "caption": "Figure 1:Demonstrations of shortcut learning in generalist robot policies.Left:Three generalist robot policies trained on the OXE dataset exhibit shortcut behavior in the SIMPLER environment[4]. Despite being tasked with “put the spoon on the towel”, a task present in the Bridge sub-dataset[5], all models consistently perform the task “pick up the coke” which is exclusive to the RT-1 sub-dataset[6].Right:π0\\pi_{0}[7]policy after finetuning on real-world data exhibits shortcut behavior. The policy was finetuned on two distinct data subsets: (Viewpoint A, Instruction C) and (Viewpoint B, Instruction D). When tasked with instruction D from the novel configuration of Viewpoint A, the policy incorrectly executes Instruction C. This indicates that the policy has learned to associate the viewpoint with the action, ignoring the provided instruction.",
                "position": 117
            }
        ]
    },
    {
        "header": "2Analysis of Dataset Diversity and Fragmentation of Robot Datasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.06426/x2.png",
                "caption": "Figure 2:Comparison of visual (left) and text (right) diversity (log scale) between OXE Sub-Datasets and vision/multimodal Datasets.OXE sub-datasets exhibit significantly lower diversity compared to their vision and multimodal counterparts. We simply choset=20t=20as it does not influence the general trend.",
                "position": 184
            },
            {
                "img": "https://arxiv.org/html/2508.06426/x3.png",
                "caption": "Figure 3:Comparison of t-SNE visualizations for vision/multimodal datasets (left) versus OXE Magic Soup++ (right).The figure shows the clear data fragmentation in the OXE dataset, in contrast to the more intertwined data structure observed in the visual and multimodal datasets.",
                "position": 187
            },
            {
                "img": "https://arxiv.org/html/2508.06426/x4.png",
                "caption": "Figure 4:Comparison of the visual disparity metricSdisparityS_{\\mathrm{disparity}}(top) and the combined metricSdisparitySdiversity\\frac{S_{\\mathrm{disparity}}}{S_{\\mathrm{diversity}}}(bottom) between OXE and vision/multimodal datasets at different temperatures.",
                "position": 193
            }
        ]
    },
    {
        "header": "3The Role of Dataset Diversity and Fragmentation in Shortcut Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.06426/x5.png",
                "caption": "Figure 5:An example of our LIBERO experiment setting, with only one task (or equivalently, one object position/language) within each sub-dataset.",
                "position": 334
            },
            {
                "img": "https://arxiv.org/html/2508.06426/x6.png",
                "caption": "Figure 6:Impact of sub-dataset diversity and disparity on the degree of shortcut leaning and out-of-distribution (OOD) performance of robot policies, analyzing task-relevant factors (object position, language) and task-irrelevant factors (viewpoint).Note: Performance metrics are not directly comparable across models due to intentionally varied experimental settings (see AppendixD.4).",
                "position": 346
            },
            {
                "img": "https://arxiv.org/html/2508.06426/x7.png",
                "caption": "Figure 7:Diversity does not always help.Increasing viewpoint diversity (2 to 10) by assigning each task a distinct viewpoint induces factor correlations in sub-datasets, aggregating fragmentation. This fosters shortcut learning and harms OOD performance.",
                "position": 355
            },
            {
                "img": "https://arxiv.org/html/2508.06426/x8.png",
                "caption": "Figure 8:Building a “bridge” to connect sub-datasets for theπ0\\pi_{0}fine-tuning experiment. Data from a third object is added under both viewpoints.",
                "position": 372
            }
        ]
    },
    {
        "header": "4Alleviating Shortcut Learning in Offline Datasets via Data Augmentation",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.06426/x9.png",
                "caption": "Figure 9:Example of viewpoint augmentation. We generate novel views to create shared visual contexts between sub-datasets, breaking the spurious correlation between viewpoint and task.",
                "position": 421
            },
            {
                "img": "https://arxiv.org/html/2508.06426/x10.png",
                "caption": "Figure 10:Using object augmentation to break spurious correlations in real-world data. The original images (top) show objects that are tied to specific scenes. The augmentation process (bottom) swaps these objects between the scenes, forcing the model to learn object identity independent of visual context like viewpoint or background.",
                "position": 431
            }
        ]
    },
    {
        "header": "5Discussion and Conclusion",
        "images": []
    },
    {
        "header": "6Limitations and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelated Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.06426/x11.png",
                "caption": "Figure 11:Experimental setup for object position layouts across 10 tasks.Objects from the same sub-dataset share the same color. In the left plot, object positions are spatially intertwined between sub-datasets, whereas in the right plot, they are spatially separated. Unless otherwise specified, experiments employ the high-disparity configuration (right).",
                "position": 1858
            }
        ]
    },
    {
        "header": "Appendix BThe Influence of Temperature",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.06426/x12.png",
                "caption": "Figure 12:The similarity metric in[28]when varying the temperaturett.",
                "position": 1865
            }
        ]
    },
    {
        "header": "Appendix CAdditional Dataset Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.06426/x13.png",
                "caption": "Figure 13:Three fragmented sub-datasets from OXE, each demonstrating distinct fragmentation patterns: (1) CMU Stretch, decomposable into disjoint scenes and tasks; (2) Berkeley Autolab UR5, featuring several factor with time-correlated variations (e.g., background and tasks); (3) Language Table, with only one sparsely changing factor (e.g., lighting).",
                "position": 1880
            },
            {
                "img": "https://arxiv.org/html/2508.06426/x14.png",
                "caption": "Figure 14:Comparison of the textual disparity metricSdisparityS_{\\mathrm{disparity}}(left) and the combined metricSdisparitySdiversity\\frac{S_{\\mathrm{disparity}}}{S_{\\mathrm{diversity}}}(right) between OXE and vision/multimodal datasets at different temperatures.",
                "position": 1883
            },
            {
                "img": "https://arxiv.org/html/2508.06426/x15.png",
                "caption": "Figure 15:The number of distinct tasks (languages) within each sub-dataset of OXE. Most sub-datasets only have less than 10 tasks, which leads to extremely low task diversity.",
                "position": 1886
            }
        ]
    },
    {
        "header": "Appendix DLIBERO Experiment Details",
        "images": []
    },
    {
        "header": "Appendix EReal-world Experiment Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.06426/x16.png",
                "caption": "Figure 16:Real-world object augmentation experiment setup.Two cameras are positioned in front of an AgileX PIPER robotic arm. For training, two distinct sub-datasets were created, each featuring a single, specific combination of object type (banana or watermelon), camera viewpoint (left or right), and background (with or without a yellow tablecloth). During evaluation, tests were conducted on both two original combinations of viewpoints and backgrounds. In these evaluations, both objects (banana and watermelon) were simultaneously present on the table, and the robot was guided by language instructions referring to object-scene configurationsnotexplicitly encountered in the training combinations.",
                "position": 2067
            }
        ]
    },
    {
        "header": "Appendix FObject Augmentation Details",
        "images": []
    },
    {
        "header": "Appendix GMethodology for Human-Assisted Shortcut Scoring",
        "images": []
    },
    {
        "header": "Appendix HProofs of Propositions in Section3",
        "images": []
    },
    {
        "header": "Appendix ILinear Model Analysis for the Impact of Disparity Between Sub-datasets on Shortcut Learning",
        "images": []
    },
    {
        "header": "Appendix JVision and Multimodal Datasets",
        "images": []
    }
]