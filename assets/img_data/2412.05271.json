[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05271/x1.png",
                "caption": "Figure 1:Performance of various MLLMs on the OpenCompass leaderboard.InternVL 2.5 showcases strong multimodal capabilities, rivaling closed-source models like GPT-4o[192]and Claude-3.5-Sonnet[8].\nHowever, since the OpenCompass score is derived from 8 academic VQA benchmarks and covers only a subset of overall capabilities, we still need further effort to match the performance with closed-source models.",
                "position": 251
            }
        ]
    },
    {
        "header": "2Model Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05271/x2.png",
                "caption": "Figure 2:Overall architecture.InternVL 2.5 retains the same model architecture as InternVL 1.5[35]and InternVL 2.0,i.e.the widely-used ‚ÄúViT-MLP-LLM‚Äù paradigm, which combines a pre-trained InternViT-300M or InternViT-6B with LLMs[19,229]of various sizes via an MLP projector. Consistent with previous versions, we apply a pixel unshuffle operation to reduce the 1024 visual tokens produced by each 448√ó\\times√ó448 image tile to 256 tokens.\nMoreover, compared to InternVL 1.5, InternVL 2.0 and 2.5 introduced additional data types, incorporating multi-image and video data alongside the existing single-image and text-only data.",
                "position": 283
            },
            {
                "img": "https://arxiv.org/html/2412.05271/x3.png",
                "caption": "Figure 3:Illustration of the data formats for various data types.(a) For single-image datasets, the maximum number of tilesnmaxsubscriptùëõmaxn_{\\text{max}}italic_n start_POSTSUBSCRIPT max end_POSTSUBSCRIPTis allocated to a single image, ensuring maximum resolution for the input.\n(b) For multi-image datasets, the total number of tilesnmaxsubscriptùëõmaxn_{\\text{max}}italic_n start_POSTSUBSCRIPT max end_POSTSUBSCRIPTis distributed proportionally across all images within the sample.\n(c) For video datasets, the method simplifies the approach by settingnmax=1subscriptùëõmax1n_{\\text{max}}=1italic_n start_POSTSUBSCRIPT max end_POSTSUBSCRIPT = 1, resizing individual frames to a fixed resolution of 448√ó\\times√ó448.",
                "position": 575
            }
        ]
    },
    {
        "header": "3Training Strategy",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05271/x4.png",
                "caption": "Figure 4:Illustration of the training pipeline and progressive scaling strategy.(a) Single model training pipeline: The training process is divided into three stages‚ÄîStage 1 (MLP warmup), optional Stage 1.5 (ViT incremental learning), and Stage 2 (full model instruction tuning).\nThe multi-stage design progressively enhances vision-language alignment, stabilizes training, and prepares modules for integration with larger LLMs.\n(b) Progressive scaling strategy: The ViT module trained with a smaller LLM in earlier stages can be easily integrated with larger LLMs, enabling scalable model alignment with affordable resource overhead.",
                "position": 688
            },
            {
                "img": "https://arxiv.org/html/2412.05271/x5.png",
                "caption": "Figure 5:Dataset configuration.In InternVL 2.0 and 2.5, data augmentation is applied selectively, enabled for image datasets and disabled for videos and text. The maximum tile number (nmaxsubscriptùëõn_{\\max}italic_n start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT) controls the resolution of inputs, with higher values for multi-image datasets and lower values for videos. The repeat factor (rùëüritalic_r) balances dataset sampling by adjusting the frequency of each dataset, ensuring robust and balanced training.",
                "position": 1175
            }
        ]
    },
    {
        "header": "4Data Organization",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05271/x6.png",
                "caption": "Figure 6:Visualization of abnormal samples in open-source datasets.Abnormal samples are prevalent across various data types, including single-image, multi-image, video, and pure text datasets, with ‚Äúrepetitive outputs‚Äù being a prominent issue. We identify this as one of the most detrimental problems for test-time scaling, often leading models into loops in long-form outputs and CoT reasoning tasks.",
                "position": 1233
            },
            {
                "img": "https://arxiv.org/html/2412.05271/x7.png",
                "caption": "Figure 7:Statistics of the fine-tuning data mixture.The dataset shows consistent growth from InternVL 1.5 to 2.5 in terms of (a) the number of samples and (b) the number of tokens across multiple dataset types, including single-image, multi-image, video, and text. These statistics reflect iterative improvements in data scale and diversity, which enhance the model‚Äôs multimodal understanding capabilities.",
                "position": 1259
            },
            {
                "img": "https://arxiv.org/html/2412.05271/x8.png",
                "caption": "Figure 8:Dataset filtering pipeline.For text data, we use three methods: (a) LLM-based quality scoring to assign domain-specific quality scores and filter low-quality samples; (b) Repetition detection to identify and remove data with repetitive patterns; and (c) heuristic rule-based filtering to detect anomalies using predefined rules.\nFor multimodal data, only (b) repetition detection and (c) heuristic rule-based filtering are applied to mitigate repetitive patterns and ensure dataset integrity.",
                "position": 1265
            }
        ]
    },
    {
        "header": "5Evaluation on Multimodal Capability",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05271/x9.png",
                "caption": "Figure 9:CoT prompts used in our model testing.By leveraging these prompts for CoT reasoning, we can scale up testing time, significantly enhancing the performance of InternVL 2.5 models on MMMU[289].",
                "position": 2385
            },
            {
                "img": "https://arxiv.org/html/2412.05271/x10.png",
                "caption": "Figure 10:Performance on LongVideoBench with varying input video frames.",
                "position": 5146
            }
        ]
    },
    {
        "header": "6Evaluation on Language Capability",
        "images": []
    },
    {
        "header": "7Evaluation on Vision Capability",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]