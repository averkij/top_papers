[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09558/x1.png",
                "caption": "",
                "position": 173
            },
            {
                "img": "https://arxiv.org/html/2510.09558/x2.png",
                "caption": "",
                "position": 174
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09558/x3.png",
                "caption": "Figure 1:Overview of our study: Automatic Promotion (AutoPR) task, its benchmark PRBench, and the associated method PRAgent. The details of citation trend analysis are shown in AppendixA.",
                "position": 182
            },
            {
                "img": "https://arxiv.org/html/2510.09558/x4.png",
                "caption": "Figure 2:The definition and overview of Automatic Promotion (AutoPR) Task.",
                "position": 225
            }
        ]
    },
    {
        "header": "2Task: AutoPR",
        "images": []
    },
    {
        "header": "3Benchmark: PRBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09558/x5.png",
                "caption": "Figure 3:overview of PRAgent, including: (1) Content extraction for preparing multimodal research material; (2) Multi-agent synthesis to transform structured data from Stage 1 into refined drafts; (3) Platform-specific adaptation to finalize the draft for publication.",
                "position": 333
            }
        ]
    },
    {
        "header": "4Methodology: PRAgent",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09558/x6.png",
                "caption": "Figure 4:AI-generated academic promotion analysis with three primary limitations. The analysis is based on 512 posts generated by the Qwen-2.5-VL-32B-Ins.",
                "position": 939
            },
            {
                "img": "https://arxiv.org/html/2510.09558/x7.png",
                "caption": "Figure 5:PRAgent significantly outperforms a direct-prompt baseline in a 10-day real-world study on the social media platform RedNote, with both methods using GPT-5 as the backbone model.",
                "position": 1388
            }
        ]
    },
    {
        "header": "6Related work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ACitation Trend Analysis Details",
        "images": []
    },
    {
        "header": "Appendix BHuman Annotation Protocol",
        "images": []
    },
    {
        "header": "Appendix CEvaluation Prompts for PRBench",
        "images": []
    },
    {
        "header": "Appendix DPRAgent Prompts",
        "images": []
    },
    {
        "header": "Appendix EAcademic Promotion Quality Assessment",
        "images": []
    },
    {
        "header": "Appendix FDirect Prompting Baseline Implementation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09558/x8.png",
                "caption": "Figure 23:Various strategies for improving Large Language Model performance on the AutoPR task. Enabling Long CoT reasoning does not consistently improve performance across different model sizes(left).In contrast, Overall performance generally increases with model parameter size, aligning with established scaling laws(middle).However, simply increasing inference-time computation not only fails to improve results but also exhibits a slight negative correlation with the final score(right).",
                "position": 3841
            }
        ]
    },
    {
        "header": "Appendix GHow do general strategies effect performance on PRBench?",
        "images": []
    },
    {
        "header": "Appendix HHuman Preference Analysis",
        "images": []
    },
    {
        "header": "Appendix IEach Stage Matters for PRAgent.",
        "images": []
    },
    {
        "header": "Appendix JReal-World Study Setting Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09558/x9.png",
                "caption": "Figure 24:A RedNote post (translated from Chinese to English) generated by Direct Prompt using GPT-5 as the backbone, based on the original paper fromLin et al. [36].",
                "position": 4710
            },
            {
                "img": "https://arxiv.org/html/2510.09558/x10.png",
                "caption": "Figure 25:A RedNote post (translated from Chinese to English) generated by PRAgent using GPT-5 as the backbone, based on the original paper fromLin et al. [36].",
                "position": 4713
            },
            {
                "img": "https://arxiv.org/html/2510.09558/x11.png",
                "caption": "Figure 26:A RedNote post generated by Direct Prompt using GPT-5 as the backbone, based on the original paper fromWang et al. [52].",
                "position": 4716
            },
            {
                "img": "https://arxiv.org/html/2510.09558/x12.png",
                "caption": "Figure 27:A RedNote post (translated from Chinese to English) generated by PRAgent using GPT-5 as the backbone, based on the original paper fromWang et al. [52].",
                "position": 4719
            },
            {
                "img": "https://arxiv.org/html/2510.09558/x13.png",
                "caption": "Figure 28:A Twitter post generated by PRAgent using GPT-5 as the backbone, based on the original paper fromGao et al. [21].",
                "position": 4722
            },
            {
                "img": "https://arxiv.org/html/2510.09558/x14.png",
                "caption": "Figure 29:A Twitter post generated by Direct Prompt using GPT-5 as the backbone, based on the original paper fromGao et al. [21].",
                "position": 4725
            },
            {
                "img": "https://arxiv.org/html/2510.09558/x14.png",
                "caption": "Figure 29:A Twitter post generated by Direct Prompt using GPT-5 as the backbone, based on the original paper fromGao et al. [21].",
                "position": 4728
            },
            {
                "img": "https://arxiv.org/html/2510.09558/x15.png",
                "caption": "Figure 30:A Twitter post generated by Direct Prompt using GPT-5 as the backbone, based on the original paper fromZuo et al. [64].",
                "position": 4733
            },
            {
                "img": "https://arxiv.org/html/2510.09558/x16.png",
                "caption": "Figure 31:A Twitter post generated by PRAgent using GPT-5 as the backbone, based on the original paper fromZuo et al. [64].",
                "position": 4739
            }
        ]
    },
    {
        "header": "Appendix KShowcase of Generated Examples",
        "images": []
    }
]