[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22805/x1.png",
                "caption": "Figure 1:We present CogIP-Bench, a comprehensive cognition benchmark that evaluates the alignment of cognition score prediction between MLLM and humans.Left:example datapoints for each dimension: aesthetics, funniness, emotion and memorability.Middle:post-training results of three popular MLLMs across different dimensions.Right:results of swapping the MLLM backbone, comparing the effect of cognition-related image generation with the Qwen-Image pipeline.",
                "position": 89
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22805/x2.png",
                "caption": "Figure 2:Examples of the CogIP-Bench, for each cognition dimension, we show two images along with their cognition scores and the interpretation of that cognitive dimension.",
                "position": 137
            }
        ]
    },
    {
        "header": "3CogIP-Bench",
        "images": []
    },
    {
        "header": "4Benchmarking Results",
        "images": []
    },
    {
        "header": "5Image Generation Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22805/x3.png",
                "caption": "Figure 3:Qualitative comparison of images generated by the Qwen-Image pipeline using different LLM backbones (with the same prompt). The figure shows the effect of pretraining versus supervised fine-tuning (SFT) on image cognition properties. For each image pair, Left: Base model; right: SFT model. Generation prompts are shown under each image pair. We can see that images generated with our SFT MLLM backbone better demonstrate the cognitive cues embedded in the prompts.",
                "position": 715
            },
            {
                "img": "https://arxiv.org/html/2511.22805/x4.png",
                "caption": "Figure 4:Preference percentages of images generated by Qwen-Image using the baseline MLLM backbone and our fine-tuned version.",
                "position": 829
            }
        ]
    },
    {
        "header": "6Discussions",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendices",
        "images": []
    },
    {
        "header": "Appendix ADetailes of Results on Other Benchmarks",
        "images": []
    },
    {
        "header": "Appendix BTraining Details",
        "images": []
    },
    {
        "header": "Appendix CExample fo User Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22805/figures/supp/usr_aes.png",
                "caption": "(a)Aesthetics",
                "position": 2254
            },
            {
                "img": "https://arxiv.org/html/2511.22805/figures/supp/usr_aes.png",
                "caption": "(a)Aesthetics",
                "position": 2257
            },
            {
                "img": "https://arxiv.org/html/2511.22805/figures/supp/usr_fun.png",
                "caption": "(b)Funniness",
                "position": 2263
            },
            {
                "img": "https://arxiv.org/html/2511.22805/figures/supp/usr_emo.png",
                "caption": "(c)Emotional Valence",
                "position": 2269
            },
            {
                "img": "https://arxiv.org/html/2511.22805/figures/supp/usr_mem.png",
                "caption": "(d)Memorability",
                "position": 2275
            }
        ]
    },
    {
        "header": "Appendix DExamples of SFT Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22805/x5.png",
                "caption": "Figure S2:Examples of CogIP-Bench samples for Aesthetics and Funniness",
                "position": 2289
            },
            {
                "img": "https://arxiv.org/html/2511.22805/x6.png",
                "caption": "Figure S3:Examples of CogIP-bench samples of Emotional Valence and Memorability.",
                "position": 2292
            },
            {
                "img": "https://arxiv.org/html/2511.22805/x7.png",
                "caption": "Figure S4:Prompts for rewriting “from human” instruction in the dataset.",
                "position": 2325
            },
            {
                "img": "https://arxiv.org/html/2511.22805/x8.png",
                "caption": "Figure S5:Prompts for rewriting “from gpt” GT in the dataset.",
                "position": 2328
            },
            {
                "img": "https://arxiv.org/html/2511.22805/x9.png",
                "caption": "Figure S6:Prompt to generate T2I generation prompts for Qwen-Image",
                "position": 2331
            }
        ]
    },
    {
        "header": "Appendix EPrompts",
        "images": []
    }
]