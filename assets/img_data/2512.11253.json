[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.11253/x1.png",
                "caption": "",
                "position": 68
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.11253/x2.png",
                "caption": "Figure 2:Overview of the three-stage pipeline of PersonaLive.\n(a) Image-level hybrid motion training: Learns expressive motion control using implicit facial representations and 3D implicit keypoints. (b) Fewer-step appearance distillation: Eliminates appearance redundancy in the denoising process, improving inference efficiency without compromising visual quality. (c) Micro-chunk streaming video generation: An autoregressive micro-chunk paradigm, equipped with sliding training and historical keyframes, enables low-latency and temporally coherent real-time video generation.",
                "position": 135
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.11253/x3.png",
                "caption": "Figure 3:The denoising trajectory without CFG[cfg].",
                "position": 197
            },
            {
                "img": "https://arxiv.org/html/2512.11253/x4.png",
                "caption": "Figure 4:Motion interpolation for the first denoising window initialization.",
                "position": 259
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.11253/x5.png",
                "caption": "Figure 5:Qualitative comparisons. PersonaLive achieves high-quality portrait animation using significantly fewer denoising steps, while preserving identity, expression fidelity, and facial detail.",
                "position": 287
            },
            {
                "img": "https://arxiv.org/html/2512.11253/x6.png",
                "caption": "Figure 6:Ablation on appearance distillation strategy. All results are generated using 4 denoising steps without the CFG technique.",
                "position": 471
            },
            {
                "img": "https://arxiv.org/html/2512.11253/x7.png",
                "caption": "Figure 7:Ablation study on the core components of the micro-chunk streaming generation paradigm.",
                "position": 544
            },
            {
                "img": "https://arxiv.org/html/2512.11253/x8.png",
                "caption": "Figure 8:Failure cases. Some details of our method may fail when the given reference images are out of the training domain.",
                "position": 563
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix APreliminary: Latent Diffusion Model",
        "images": []
    },
    {
        "header": "Appendix BExperimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.11253/x9.png",
                "caption": "Figure 9:Examples from LV100.",
                "position": 818
            },
            {
                "img": "https://arxiv.org/html/2512.11253/x10.png",
                "caption": "Figure 10:The implicit 3D keypoints used in our hybrid motion control.",
                "position": 825
            }
        ]
    },
    {
        "header": "Appendix CMore Ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.11253/x11.png",
                "caption": "Figure 11:Effect of implicit 3D keypoints and facial motion embedding.",
                "position": 890
            }
        ]
    },
    {
        "header": "Appendix DMore results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.11253/x12.png",
                "caption": "Figure 12:More visualizations of self-reenactment comparison (1/2). The images with red borders are the reference images.",
                "position": 1030
            },
            {
                "img": "https://arxiv.org/html/2512.11253/x13.png",
                "caption": "Figure 13:Long avatar video results (1/4).",
                "position": 1034
            },
            {
                "img": "https://arxiv.org/html/2512.11253/x14.png",
                "caption": "Figure 14:More visualizations of self-reenactment comparison (2/2). The images with red borders are the reference images.",
                "position": 1038
            },
            {
                "img": "https://arxiv.org/html/2512.11253/x15.png",
                "caption": "Figure 15:Long avatar video results (2/4).",
                "position": 1042
            },
            {
                "img": "https://arxiv.org/html/2512.11253/x16.png",
                "caption": "Figure 16:More visualizations of cross-reenactment comparison (1/2).",
                "position": 1046
            },
            {
                "img": "https://arxiv.org/html/2512.11253/x17.png",
                "caption": "Figure 17:Long avatar video results (3/4).",
                "position": 1050
            },
            {
                "img": "https://arxiv.org/html/2512.11253/x18.png",
                "caption": "Figure 18:More visualizations of cross-reenactment comparison (2/2).",
                "position": 1054
            },
            {
                "img": "https://arxiv.org/html/2512.11253/x19.png",
                "caption": "Figure 19:Long avatar video results (4/4).",
                "position": 1058
            }
        ]
    },
    {
        "header": "Appendix EEthics Statement.",
        "images": []
    }
]