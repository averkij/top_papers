[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Isolating Mamba-2 Accuracy Gains",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.17363/figures_png/1_base_models/1_test_loss.png",
                "caption": "Figure 1:Accuracy of linear attention, Mamba, and softmax attention, keeping everything but the attention mechanism constant across experiments.",
                "position": 393
            },
            {
                "img": "https://arxiv.org/html/2602.17363/figures_png/2_lin_attn_norm_type/2_test_loss.png",
                "caption": "Figure 2:Accuracy of various norm types. Softmax normalization requires a positive inner-product space image, as such we use ReLU.",
                "position": 461
            },
            {
                "img": "https://arxiv.org/html/2602.17363/figures_png/4_isolated/4_test_loss.png",
                "caption": "",
                "position": 524
            },
            {
                "img": "https://arxiv.org/html/2602.17363/figures_png/5_buildup_main/5_test_loss.png",
                "caption": "(a)Main buildup",
                "position": 585
            },
            {
                "img": "https://arxiv.org/html/2602.17363/figures_png/5_buildup_main/5_test_loss.png",
                "caption": "(a)Main buildup",
                "position": 588
            },
            {
                "img": "https://arxiv.org/html/2602.17363/figures_png/6_buildup_red/6_test_loss.png",
                "caption": "(b)Redundant components after buildup",
                "position": 593
            }
        ]
    },
    {
        "header": "4Building Up to the Mamba-2S Base Model",
        "images": []
    },
    {
        "header": "5Mamba-2 with a Squared Hidden State",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.17363/figures_png/7_sm_comp/7_test_loss.png",
                "caption": "(a)Small model performance comparison.",
                "position": 765
            },
            {
                "img": "https://arxiv.org/html/2602.17363/figures_png/7_sm_comp/7_test_loss.png",
                "caption": "(a)Small model performance comparison.",
                "position": 768
            },
            {
                "img": "https://arxiv.org/html/2602.17363/figures_png/9_unstable/9_test_loss.png",
                "caption": "(b)Instability in the discretization variant.",
                "position": 773
            },
            {
                "img": "https://arxiv.org/html/2602.17363/figures_png/8.1_full_comp/8.1_test_loss.png",
                "caption": "Figure 6:First row: Small model (∼\\sim300M params) test loss for Mamba, our proposed algorithm, and softmax attention trained on 2048, 4096, and 8192 sequence lengths.Second row: Medium model (∼\\sim700M params) test loss for Mamba, our proposed algorithm, and softmax attention trained on 2048, 4096, and 8192 sequence lengths",
                "position": 870
            },
            {
                "img": "https://arxiv.org/html/2602.17363/figures_png/8.1_full_comp/8.1_test_loss.png",
                "caption": "",
                "position": 873
            },
            {
                "img": "https://arxiv.org/html/2602.17363/figures_png/8.2_full_comp/8.2_test_loss.png",
                "caption": "",
                "position": 877
            },
            {
                "img": "https://arxiv.org/html/2602.17363/figures_png/8.3_full_comp/8.3_test_loss.png",
                "caption": "",
                "position": 881
            },
            {
                "img": "https://arxiv.org/html/2602.17363/figures_png/8.4_full_comp/8.4_test_loss.png",
                "caption": "",
                "position": 886
            },
            {
                "img": "https://arxiv.org/html/2602.17363/figures_png/8.5_full_comp/8.5_test_loss.png",
                "caption": "",
                "position": 890
            },
            {
                "img": "https://arxiv.org/html/2602.17363/figures_png/8.6_full_comp/8.6_test_loss.png",
                "caption": "",
                "position": 894
            },
            {
                "img": "https://arxiv.org/html/2602.17363/figures_png/hidden_state_memory.png",
                "caption": "Figure 7:Memory usage of a single head of softmax attention and 2Mamba, utilizing softmax-like normalization and a convolution with kernel size 2.",
                "position": 924
            },
            {
                "img": "https://arxiv.org/html/2602.17363/figures_png/11_niah/niah.png",
                "caption": "Figure 8:One-shot NIAH benchmark results using maximum likelihood sampling over 1000 sequences. Green indicates the predicted needle was correct more often while red indicates the predicted needle was predicted less often. Scores are indicative of the proportion of sequences in which the model predicted the needle correctly.",
                "position": 927
            },
            {
                "img": "https://arxiv.org/html/2602.17363/figures_png/11_niah/11_test_loss.png",
                "caption": "Figure 9:Long training run of various models with a batch size of 64 on8,1928,192max sequence length.",
                "position": 930
            }
        ]
    },
    {
        "header": "62Mamba With an Exponentiated Hidden State",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.17363/figures_png/10.1_exp/10.1_test_loss.png",
                "caption": "Figure 10:Medium model (∼\\sim700M params) test loss for Mamba, the exponentiated variation of our proposed algorithm, and softmax attention trained on 2048, 4096, and 8192 sequence lengths.",
                "position": 948
            },
            {
                "img": "https://arxiv.org/html/2602.17363/figures_png/10.1_exp/10.1_test_loss.png",
                "caption": "",
                "position": 951
            },
            {
                "img": "https://arxiv.org/html/2602.17363/figures_png/10.2_exp/10.2_test_loss.png",
                "caption": "",
                "position": 955
            },
            {
                "img": "https://arxiv.org/html/2602.17363/figures_png/10.3_exp/10.3_test_loss.png",
                "caption": "",
                "position": 959
            }
        ]
    },
    {
        "header": "7Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AModel Ablation Details",
        "images": []
    },
    {
        "header": "Appendix BGradients",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.17363/figures_png/12_pile/12_test_loss.png",
                "caption": "(a)Test loss on The Pile",
                "position": 1949
            },
            {
                "img": "https://arxiv.org/html/2602.17363/figures_png/12_pile/12_test_loss.png",
                "caption": "(a)Test loss on The Pile",
                "position": 1952
            },
            {
                "img": "https://arxiv.org/html/2602.17363/figures_png/13_slimpj/13_test_loss.png",
                "caption": "(b)Test loss on SlimPajama",
                "position": 1957
            }
        ]
    },
    {
        "header": "Appendix CPile and SlimPJ Loss Curves",
        "images": []
    }
]