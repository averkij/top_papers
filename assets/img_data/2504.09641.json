[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.09641/x1.png",
                "caption": "Figure 1:A case of TinyLLaVA-Video-R1 on video understanding data, sourced from MVBench. The model demonstrates the ability to perceive video scenes and analyze options, while also exhibiting reflective and backtracking behavior (highlighted in blue).",
                "position": 99
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.09641/x2.png",
                "caption": "Figure 2:A case of TinyLLaVA-Video-R1 on video reasoning data, sourced from MMVU. The model demonstrates comprehensive video content understanding and the capability to derive correct answers through analytical reasoning.",
                "position": 120
            }
        ]
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.09641/x3.png",
                "caption": "Figure 3:Cases of ”aha moment”, where the model demonstrates reflection and backtracking during its reasoning process (highlighted in blue). The cases are from MVBench and MMVU respectively.",
                "position": 137
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.09641/extracted/6357976/sec/fig/completion_length_smoothed.png",
                "caption": "(a)Evolution in completion length.",
                "position": 370
            },
            {
                "img": "https://arxiv.org/html/2504.09641/extracted/6357976/sec/fig/completion_length_smoothed.png",
                "caption": "(a)Evolution in completion length.",
                "position": 373
            },
            {
                "img": "https://arxiv.org/html/2504.09641/extracted/6357976/sec/fig/accuracy_reward_smoothed.png",
                "caption": "(b)Evolution in accuracy reward.",
                "position": 378
            },
            {
                "img": "https://arxiv.org/html/2504.09641/extracted/6357976/sec/fig/format_reward_smoothed.png",
                "caption": "(c)Evolution in format reward.",
                "position": 383
            },
            {
                "img": "https://arxiv.org/html/2504.09641/extracted/6357976/sec/fig/comparison_length.png",
                "caption": "Figure 5:The variation in response length during training under different settings.",
                "position": 419
            },
            {
                "img": "https://arxiv.org/html/2504.09641/extracted/6357976/sec/fig/ablation.png",
                "caption": "Figure 6:Ablation study on TinyLLaVA-R1 variants across multiple benchmarks. We compare the original TinyLLaVA-Video-R1 with two ablated versions: removing the KL divergence term (Del KL) and replacing the original GRPO with Dr. GRPO. Results are reported on MVBench, Video-MME (without subtitle input), MLVU, and MMVU (multiple-choice subset).Boldvalues indicate the best performance for each benchmark.",
                "position": 425
            }
        ]
    },
    {
        "header": "5Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]