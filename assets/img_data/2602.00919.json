[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.00919/images/hero/green_hero_background.png",
                "caption": "",
                "position": 109
            }
        ]
    },
    {
        "header": "1  Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.00919/x1.png",
                "caption": "Figure 1:Green-VLA architecture. A multimodal vision–language model encodes instructions, camera views, and proprioception into tokens that feed a flow-matching action expert. A high-level task planner decomposes user goals into subtasks, queries the VLA loop, and uses auxiliary signals (episode end, OOD, and JPM-based guidance for precise target points) to ensure safe, instruction-faithful execution across embodiments.",
                "position": 134
            }
        ]
    },
    {
        "header": "2  Why a Staged VLA Pipeline Matters",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.00919/x2.png",
                "caption": "Figure 2:Green-VLA’s robot-specific training stages use visual question answering (VQA) and robotics data and enable robot adaptation and specialization for new embodiments, spatial reasoning, task generalization, dexterous manipulation, and failure recovery.",
                "position": 266
            }
        ]
    },
    {
        "header": "3  Green-VLA Data Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.00919/x3.png",
                "caption": "Figure 3:Datasets mixture used in L1 training phase.Left: distribution of sample counts across sub-datasets.Right: sampling weight allocation across categories.\nThe data corpus integrates diverse web sources covering spatial reasoning, pointing, robotics-related VQA, and multi-view QA.",
                "position": 355
            },
            {
                "img": "https://arxiv.org/html/2602.00919/x4.png",
                "caption": "Figure 4:Left: Dataset sampling rates used during the R0 phase of GreenVLA training.Right: Number of data samples (frames) per dataset, illustrating relative temporal coverage. The corpus combines large-scale open datasets (e.g., AgibotWorld, DROID, Galaxea) with internally collected humanoid and dexterous-hand data.",
                "position": 456
            },
            {
                "img": "https://arxiv.org/html/2602.00919/x5.png",
                "caption": "Figure 5:Overview of the data pipeline for robot learning. Data collection and processing loop integrating robot-side teleoperation, cloud-based data verification, open-source dataset mining, and model training. The pipeline supports iterative model updates via RL fine-tuning and feedback from real-robot deployments.",
                "position": 466
            }
        ]
    },
    {
        "header": "4  Green-VLA Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.00919/x6.png",
                "caption": "Figure 6:Mean optical flow magnitude per dataset used for temporal alignment (higher values correspond to faster apparent motion).",
                "position": 834
            },
            {
                "img": "https://arxiv.org/html/2602.00919/x7.png",
                "caption": "Figure 7:Robot state density is modeled with a Gaussian Mixture Model (GMM). Actions that would cause the robot to enter an out-of-distribution state with low GMM density are corrected based on the GMM density gradient.",
                "position": 857
            },
            {
                "img": "https://arxiv.org/html/2602.00919/x8.png",
                "caption": "Figure 8:Phase R2: RL alignment. Optimization of the source noise distribution: an actorπθn​o​i​s​e​(ϵ|s)\\pi_{\\theta_{noise}}(\\epsilon|s)learns to sample noise that improves the flow-matching policy’s actions. PARL-style trajectory optimization: experience and teleoperation data are iteratively refined with Q-function gradients and added to train set.",
                "position": 978
            }
        ]
    },
    {
        "header": "5  Experiment Metrics Across Phases",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.00919/images/aloha/f11.png",
                "caption": "(a)Setup for \"pick tape\" task.",
                "position": 1014
            },
            {
                "img": "https://arxiv.org/html/2602.00919/images/aloha/f11.png",
                "caption": "(a)Setup for \"pick tape\" task.",
                "position": 1017
            },
            {
                "img": "https://arxiv.org/html/2602.00919/images/aloha/f22.png",
                "caption": "(b)Setup for \"pick pliers\" task.",
                "position": 1022
            },
            {
                "img": "https://arxiv.org/html/2602.00919/images/aloha/c11.png",
                "caption": "(c)Cleaning table setup",
                "position": 1027
            },
            {
                "img": "https://arxiv.org/html/2602.00919/images/guidance/jpm_seccim.png",
                "caption": "Figure 10:First, we ground the affordance in 2D, and then, by lifting it to 3D, we initialize the target for Green-VLA guidance.",
                "position": 1348
            },
            {
                "img": "https://arxiv.org/html/2602.00919/x9.png",
                "caption": "Figure 11:E-commerce shelf picking: top-1 success rate (%) for Green-VLA with and without JPM guidance. Columns report In-Domain—Coarse (brand/category), In-Domain—SKU (exact variant), and Out-of-Domain (unseen SKUs/packaging). Higher is better.",
                "position": 1354
            },
            {
                "img": "https://arxiv.org/html/2602.00919/x10.png",
                "caption": "Figure 12:Quantitative evaluation of the humanoid policy.\nThe figure summarizes performance across instruction-conditioned manipulation tasks, including pick, place, handover, fruit sorting, and full table cleaning.\nThe final group shows average success across all tasks, for both in-domain and out-of-domain settings.",
                "position": 1366
            },
            {
                "img": "https://arxiv.org/html/2602.00919/x11.png",
                "caption": "Figure 13:Humanoid executing a task-planned pick-and-place sequence.\nThe high-level task planner decomposes the user’s request (“sort apples and oranges into the basket”) into subtasks such as “pick the small green apple with your left hand,” “pick the large orange with your right hand,” and “place the big apple in the target basket.”\nGreen-VLA then executes each subtask, coordinating arm choice, precise placement, full upper-body control, and task-following to complete the full sorting task.",
                "position": 1374
            },
            {
                "img": "https://arxiv.org/html/2602.00919/x12.png",
                "caption": "(a)Success rates for e-commerce items after R1 and R2 training stages.",
                "position": 1406
            },
            {
                "img": "https://arxiv.org/html/2602.00919/x12.png",
                "caption": "(a)Success rates for e-commerce items after R1 and R2 training stages.",
                "position": 1409
            },
            {
                "img": "https://arxiv.org/html/2602.00919/x13.png",
                "caption": "(b)CALVIN benchmark: ACL forπ0\\pi_{0}, Flower and Green-VLA across R1/R2.",
                "position": 1415
            }
        ]
    },
    {
        "header": "6  Conclusion",
        "images": []
    },
    {
        "header": "7  Contributors and Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]