[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.04447/x1.png",
                "caption": "",
                "position": 106
            },
            {
                "img": "https://arxiv.org/html/2507.04447/x2.png",
                "caption": "",
                "position": 106
            },
            {
                "img": "https://arxiv.org/html/2507.04447/x3.png",
                "caption": "",
                "position": 106
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.04447/x4.png",
                "caption": "Figure 1:(a) Vanilla VLA directly maps visual observations and language instructions to actions. (b) Models leveraging separate image/video generation or copilot models to generate future frames or trajectories, subsequently guiding an action head. (c) VLA variants explicitly predict a subgoal image as an intermediate visual reasoning step prior to action generation. (d) Our proposedDreamVLA, which explicitly predicts dynamic regions, depth map, semantics (DINOv2 and SAM) knowledge, significantly enhances the model‚Äôs action reasoning and generalization.",
                "position": 139
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.04447/x5.png",
                "caption": "Figure 2:Framework Overview.\nGiven the current robot statestsubscriptùë†ùë°s_{t}italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, observationotsubscriptùëúùë°o_{t}italic_o start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, and language instruction, DreamVLA encodes multimodal inputs via frozen text, visual encoders and a tunable state encoder.\nThese tokens, together with a learnable set of <dream> queries, are processed by a large language model to produceworld embedding.\nThree lightweight decoders then project each corresponding element of this embedding into the dynamics regionf^t+nsubscript^ùëìùë°ùëõ\\hat{f}_{t+n}over^ start_ARG italic_f end_ARG start_POSTSUBSCRIPT italic_t + italic_n end_POSTSUBSCRIPT, monocular depthd^t+nsubscript^ùëëùë°ùëõ\\hat{d}_{t+n}over^ start_ARG italic_d end_ARG start_POSTSUBSCRIPT italic_t + italic_n end_POSTSUBSCRIPTand high-level semanticsc^t+nsubscript^ùëêùë°ùëõ\\hat{c}_{t+n}over^ start_ARG italic_c end_ARG start_POSTSUBSCRIPT italic_t + italic_n end_POSTSUBSCRIPT.\nA separate <action> query draws a latent action embedding, which conditions a diffusion transformer that refines Gaussian noise into annùëõnitalic_n-step action sequencea^t:t+n‚àí1subscript^ùëé:ùë°ùë°ùëõ1\\hat{a}_{t:t+n-1}over^ start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_t : italic_t + italic_n - 1 end_POSTSUBSCRIPT.\nThe dashed box highlights prediction heads that are used only during training, inference skips these heads and operates directly on the world embedding.",
                "position": 216
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.04447/x6.png",
                "caption": "Figure 3:Visualization of dynamic regions over time.We show the static camera (left) and wrist-mounted camera (right) observations alongside the corresponding dynamic masks generated by our method at multiple time steps.\nThe masks highlight dynamic regions by leveraging optical flow trajectories extracted via CoTracker[64,63].\nCompared to the original observations, our method effectively suppresses irrelevant background and focuses on interaction-relevant areas (e.g., moving objects and end-effector), enabling more structured and efficient action reasoning.",
                "position": 293
            },
            {
                "img": "https://arxiv.org/html/2507.04447/x7.png",
                "caption": "Figure 4:Block-wise structured attention.",
                "position": 395
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.04447/x8.png",
                "caption": "Figure 5:Real-world experiment setup.",
                "position": 1010
            },
            {
                "img": "https://arxiv.org/html/2507.04447/x9.png",
                "caption": "Figure 6:CALVIN ABC-D performance with respect to different combinations of knowledge prediction. All=all of five models, and All-X=taking X out of All.",
                "position": 1327
            }
        ]
    },
    {
        "header": "5Limitation & Future Works",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.04447/x10.png",
                "caption": "Figure 7:Qualitative resultsof the CALVIN long horizon task.",
                "position": 4376
            },
            {
                "img": "https://arxiv.org/html/2507.04447/x11.png",
                "caption": "Figure 8:Visualization resultsof the dynamic region predictions.",
                "position": 4381
            },
            {
                "img": "https://arxiv.org/html/2507.04447/x12.png",
                "caption": "Figure 9:Visualization resultsof the depth maps.",
                "position": 4386
            },
            {
                "img": "https://arxiv.org/html/2507.04447/x13.png",
                "caption": "Figure 10:Qualitative resultsof real world language-grounded manipulation.",
                "position": 4419
            }
        ]
    },
    {
        "header": "Appendix CAdditional Related Works",
        "images": []
    },
    {
        "header": "Appendix DAdditional Discussions and Future Work",
        "images": []
    },
    {
        "header": "Appendix EBroader Impacts",
        "images": []
    }
]