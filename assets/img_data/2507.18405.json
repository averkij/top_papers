[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.18405/x1.png",
                "caption": "(a)",
                "position": 196
            },
            {
                "img": "https://arxiv.org/html/2507.18405/x1.png",
                "caption": "(a)",
                "position": 199
            },
            {
                "img": "https://arxiv.org/html/2507.18405/x2.png",
                "caption": "(b)",
                "position": 205
            }
        ]
    },
    {
        "header": "IIRelated Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.18405/x3.png",
                "caption": "Figure 2:Illustration of Iwin attention. In the left image, the green triangles and red stars representing tokens are connected through convolutions in the original image. In the right image, all green triangles representing tokens are assigned to the same window through the RTR (Reshape-Transpose-Reshape) operation and window segmentation, executing window attention to establish connections among them. All red stars representing tokens do the same thing. The result is that global convolution and window attention on the interleaved sequence work together to effectively approximate standard global attention, which means that connections are established between any tokens in the original image.",
                "position": 265
            }
        ]
    },
    {
        "header": "IIIMethodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.18405/x4.png",
                "caption": "(a)",
                "position": 845
            },
            {
                "img": "https://arxiv.org/html/2507.18405/x4.png",
                "caption": "(a)",
                "position": 848
            },
            {
                "img": "https://arxiv.org/html/2507.18405/x5.png",
                "caption": "(b)",
                "position": 854
            },
            {
                "img": "https://arxiv.org/html/2507.18405/x6.png",
                "caption": "(c)",
                "position": 860
            }
        ]
    },
    {
        "header": "IVExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.18405/x7.png",
                "caption": "Figure 4:The visualization of heatmap. The left column shows input images, while subsequent columns show results from native VIT, PVTv2, Swin, and Iwin (our method). Results demonstrate that Iwin effectively concentrates activation on target objects.",
                "position": 1452
            },
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/heatmap/n01531178_5040.jpg",
                "caption": "",
                "position": 1456
            },
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/heatmap/n01644900_9216.jpg",
                "caption": "",
                "position": 1456
            },
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/heatmap/n01978455_11509.jpg",
                "caption": "",
                "position": 1456
            },
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/heatmap/n01693334_10154.jpg",
                "caption": "",
                "position": 1456
            },
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/heatmap/n01796340_9287.jpg",
                "caption": "",
                "position": 1456
            },
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/heatmap/n02006656_1364.jpg",
                "caption": "",
                "position": 1456
            },
            {
                "img": "https://arxiv.org/html/2507.18405/x8.png",
                "caption": "",
                "position": 1461
            },
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/heatmap/n02012849_3811.jpg",
                "caption": "",
                "position": 1461
            },
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/heatmap/n02018207_23365.jpg",
                "caption": "",
                "position": 1461
            },
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/heatmap/n01770393_26120.jpg",
                "caption": "",
                "position": 1461
            },
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/heatmap/n01847000_13220.jpg",
                "caption": "",
                "position": 1461
            },
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/heatmap/n01910747_2662.jpg",
                "caption": "",
                "position": 1461
            },
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/heatmap/n02012849_10008.jpg",
                "caption": "",
                "position": 1461
            },
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/fig/det/000000371506_.jpg",
                "caption": "Figure 5:The visualization of object detection on the COCO2017. The leftmost column shows the input images. From left to right, the results generated by PVTv2-based, Swin-based, and Iwin-based Mask R-CNN are shown.",
                "position": 1730
            },
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/fig/det/000000106941.jpg",
                "caption": "",
                "position": 1734
            },
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/fig/det/000000525716.jpg",
                "caption": "",
                "position": 1736
            },
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/fig/det/000000118478.jpg",
                "caption": "",
                "position": 1738
            },
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/fig/det/000000009448.jpg",
                "caption": "",
                "position": 1740
            },
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/fig/det/000000497600.jpg",
                "caption": "",
                "position": 1742
            },
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/fig/det/000000580137.jpg",
                "caption": "",
                "position": 1744
            },
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/fig/det/000000065072.jpg",
                "caption": "",
                "position": 1746
            },
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/fig/det/000000028607.jpg",
                "caption": "",
                "position": 1748
            },
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/fig/jumps.png",
                "caption": "Figure 6:BBox mAP and Learning Rate Progression for Iwin-T vs. Swin-T on the COCO (Cascade Mask-RCNN 3×\\times×schedule).",
                "position": 1752
            },
            {
                "img": "https://arxiv.org/html/2507.18405/x9.png",
                "caption": "Figure 7:(a) The overall architecture of the Iwin Transformer (Iwin-T). (b) Single Iwin Transformer Block. IW-MSA involves applying interleaved window multi-head self-attention and depthwise separable convolution in parallel.",
                "position": 1953
            },
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/fig/seg/ADE_train_00003353_.jpg",
                "caption": "Figure 8:Results for semantic segmentation on ADE20K. The first column shows the input images. From left to right: Ground Truth, Swin-based, and Iwin-based UperNet.",
                "position": 2140
            },
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/fig/seg/ADE_train_00003907.jpg",
                "caption": "",
                "position": 2144
            },
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/fig/seg/ADE_train_00004941.jpg",
                "caption": "",
                "position": 2146
            },
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/fig/seg/ADE_train_00000177.jpg",
                "caption": "",
                "position": 2148
            },
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/fig/seg/ADE_train_00007969.jpg",
                "caption": "",
                "position": 2150
            },
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/fig/seg/ADE_train_00006864.jpg",
                "caption": "",
                "position": 2152
            },
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/fig/seg/ADE_train_00006446.jpg",
                "caption": "",
                "position": 2154
            },
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/fig/seg/ADE_train_00014087.jpg",
                "caption": "",
                "position": 2156
            },
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/fig/seg/ADE_train_00017144.jpg",
                "caption": "",
                "position": 2158
            },
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/fig/seg/ADE_train_00011063.jpg",
                "caption": "",
                "position": 2160
            },
            {
                "img": "https://arxiv.org/html/2507.18405/x10.png",
                "caption": "Figure 9:Illustration of Iwin 3D Attention. In the left image, depth-wise separable convolution is performed on each frame, as in 2D. On the right, Interleaved Window Attention is performed in 3D. This effectively approximates 3D full attention, allowing connections between any tokens within a video. The only disadvantage is that position coding is required for the time dimension. To get rid of it completely, we propose two options: perform RTR operations in the time dimension and cooperate with one-dimensional convolution; the second is to increase convolution kernel size in the time dimension to span frames. Due to paper limitations, this part will be reserved for future work.",
                "position": 2181
            }
        ]
    },
    {
        "header": "VDiscussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.18405/x11.png",
                "caption": "Figure 10:The illustration of Iwin 1D Attention, shows how to the apply Iwin Transformer concepts into large language models (LLMs). The red boxes represent causal depthwise separable convolution, while the black boxes denote causal attention within a window. Neither process leaks future information. The final output, derived from the combination of these two causal operations, adheres to the principle of causality. This approach might be also helpful in solving the problem of high complexity with long sequences in LLMs.",
                "position": 2892
            }
        ]
    },
    {
        "header": "VIConclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/hsm.jpg",
                "caption": "",
                "position": 3446
            },
            {
                "img": "https://arxiv.org/html/2507.18405/extracted/6650193/NL.jpg",
                "caption": "",
                "position": 3460
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]