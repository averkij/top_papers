[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09560/extracted/6197604/pics/embodied-logo.png",
                "caption": "",
                "position": 125
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09560/x1.png",
                "caption": "Figure 1:Overview ofEmbodiedBench. Two key features of our benchmark: various action levels and capability-oriented evaluation.",
                "position": 184
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Problem Formulation",
        "images": []
    },
    {
        "header": "4EmbodiedBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09560/x2.png",
                "caption": "Figure 2:The vision-driven agent pipeline used inEmbodiedBench. This pipeline serves as a robust framework for processing multimodal inputs, reflection and reasoning, and generating executable plans. For detailed descriptions, refer to Section4.3.",
                "position": 401
            },
            {
                "img": "https://arxiv.org/html/2502.09560/x3.png",
                "caption": "Figure 3:Planning examples in EB-ALFRED and EB-Manipulation based on GPT-4o.",
                "position": 430
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09560/x4.png",
                "caption": "Figure 4:Language-centric ablations on EB-ALFRED.",
                "position": 2855
            },
            {
                "img": "https://arxiv.org/html/2502.09560/x5.png",
                "caption": "Figure 5:Visual-centric ablations on EB-Manipulation.",
                "position": 2865
            },
            {
                "img": "https://arxiv.org/html/2502.09560/x6.png",
                "caption": "Figure 6:Error Analysis.",
                "position": 2892
            }
        ]
    },
    {
        "header": "6Conlcusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Related Works",
        "images": []
    },
    {
        "header": "Appendix BDetails aboutEmbodiedBenchEnvironments and Datasets",
        "images": []
    },
    {
        "header": "Appendix CModel Versions",
        "images": []
    },
    {
        "header": "Appendix DDefinitions and Examples of Capability-oriented Subsets",
        "images": []
    },
    {
        "header": "Appendix EAdditional Experiment Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09560/x7.png",
                "caption": "Figure 7:Impact of different camera resolutions onEmbodiedBench.",
                "position": 6553
            },
            {
                "img": "https://arxiv.org/html/2502.09560/x8.png",
                "caption": "Figure 8:Impact of detection boxes onEmbodiedBench.",
                "position": 6563
            },
            {
                "img": "https://arxiv.org/html/2502.09560/x9.png",
                "caption": "Figure 9:Multi-step observation example in EB-Navigation",
                "position": 6573
            },
            {
                "img": "https://arxiv.org/html/2502.09560/x10.png",
                "caption": "Figure 10:Multi-step observation example in EB-Manipulation",
                "position": 6576
            },
            {
                "img": "https://arxiv.org/html/2502.09560/x11.png",
                "caption": "Figure 11:Impact of multi-step images onEmbodiedBench.",
                "position": 6579
            },
            {
                "img": "https://arxiv.org/html/2502.09560/x12.png",
                "caption": "Figure 12:Multi-view observation example in EB-Navigation (left) and EB-Manipulation (right).",
                "position": 6589
            },
            {
                "img": "https://arxiv.org/html/2502.09560/x13.png",
                "caption": "Figure 13:Impact of multi-view images onEmbodiedBench.",
                "position": 6592
            },
            {
                "img": "https://arxiv.org/html/2502.09560/x14.png",
                "caption": "Figure 14:Visual in-context learning examples for EB-Navigation & EB-Manipulation",
                "position": 6603
            },
            {
                "img": "https://arxiv.org/html/2502.09560/x15.png",
                "caption": "Figure 15:Impact of visual in-context learning onEmbodiedBench.",
                "position": 6606
            }
        ]
    },
    {
        "header": "Appendix FError Definitions and Additional Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09560/x16.png",
                "caption": "Figure 16:Error Analysis on EB-Navigation.",
                "position": 6833
            }
        ]
    },
    {
        "header": "Appendix GInput of the Vision-driven Embodied Agent",
        "images": []
    },
    {
        "header": "Appendix HSupplementary Case Studies of Successful Planning",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09560/x17.png",
                "caption": "Figure 17:Planning example of Claude-3.5-Sonnet in EB-AFRED.",
                "position": 7975
            },
            {
                "img": "https://arxiv.org/html/2502.09560/x18.png",
                "caption": "Figure 18:Planning example in EB-Habitat for InternVL2.5-78B.",
                "position": 7978
            },
            {
                "img": "https://arxiv.org/html/2502.09560/x19.png",
                "caption": "Figure 19:Planning example of GPT-4o in EB-Navigation.",
                "position": 7981
            },
            {
                "img": "https://arxiv.org/html/2502.09560/x20.png",
                "caption": "Figure 20:Planning example of Gemini-1.5-pro in EB-Manipulation.",
                "position": 7984
            },
            {
                "img": "https://arxiv.org/html/2502.09560/x21.png",
                "caption": "Figure 21:Planning Error Example in EB-ALFRED: The agent was supposed to locate “Book_2” by the 7th action but instead continued interacting with the first book.",
                "position": 7995
            },
            {
                "img": "https://arxiv.org/html/2502.09560/x22.png",
                "caption": "Figure 22:Perception Error Example in EB-Manipulation: the agent erroneously observed the color of the object.",
                "position": 7998
            },
            {
                "img": "https://arxiv.org/html/2502.09560/x23.png",
                "caption": "Figure 23:Reasoning Error Example in EB-Navigation: the agent recognized it was blocked by the countertop but failed to attempt navigating around it.",
                "position": 8001
            }
        ]
    },
    {
        "header": "Appendix ISupplementary Case Studies of Unsuccessful Planning",
        "images": []
    }
]