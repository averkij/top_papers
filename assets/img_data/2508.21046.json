[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21046/main_figs/et.png",
                "caption": "",
                "position": 92
            },
            {
                "img": "https://arxiv.org/html/2508.21046/main_figs/introductionv7.jpg",
                "caption": "Figure 1:Overview of our proposed CogVLA.Traditional VLA models process initial observations (Fig.(a)) without vision compression, leading to high computational cost. As shown in Fig.(b)and Fig.(d), existing compression methods retain irrelevant inputsand fail to focus on instruction-relevant targets. CogVLA employs EFA-Routing and LFP-Routing to sparsify visual inputs based on instruction relevance. Comparing Fig.(c)and Fig.(e), CAtten further enhances logical consistency and action coherence for final targeted objects. Fig.(f), Fig.(g), and Fig.(h)illustrate the architectural innovations of CogVLA and its superiority in efficiency and performance.",
                "position": 112
            },
            {
                "img": "https://arxiv.org/html/2508.21046/main_figs/gray.png",
                "caption": "",
                "position": 113
            },
            {
                "img": "https://arxiv.org/html/2508.21046/main_figs/purple.png",
                "caption": "",
                "position": 113
            },
            {
                "img": "https://arxiv.org/html/2508.21046/main_figs/red.png",
                "caption": "",
                "position": 113
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21046/main_figs/frameworkv7.png",
                "caption": "Figure 2:Overview of CogVLA Framework.CogVLA employs a cognition-aligned, instruction-driven routing & sparsification strategy for efficient action chunk prediction. Inspired by human multimodal coordination, it integrates task-guided visual aggregation, semantic pruning, and coherent decoding, ensuring efficient cross-modal representation alignment from perception to control.",
                "position": 221
            },
            {
                "img": "https://arxiv.org/html/2508.21046/main_figs/3stagev5.png",
                "caption": "Figure 3:Illustration of 3-Stage Progressive Design.CogVLA emulates human multimodal coordination via instruction-driven routing and sparsification. EFA-Routing (Stage 1), LFP-Routing (Stage 2), and CAtten (Stage 3) correspond to the VAS, SMA, and PMC, respectively. Fig.(c) highlights the advantages of CAtten over prior attention mechanisms in combining uni-&bi-directional attention, injecting action intent, enabling parallel decoding, and leveraging sparse visual tokens.",
                "position": 281
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21046/main_figs/visualizationv4.jpg",
                "caption": "Figure 4:Visualization comparison between CogVLA and OpenVLA-OFT.CogVLA outperforms OpenVLA-OFT in success rates on both simulation and real-world tasks, achieving state-of-the-art performance with a 31% reduction in inference time. It also demonstrates superior training efficiency, requiring 3.1×\\timesfewer FLOPs and 2.7×\\timesshorter training time.",
                "position": 909
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BExperimental Details",
        "images": []
    },
    {
        "header": "Appendix CSupplementary Quantitative Analysis",
        "images": []
    },
    {
        "header": "Appendix DSupplementary Qualitative Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21046/app_figs/appx_d1.jpg",
                "caption": "Figure 5:Real-world Manipulation Workflows and Visualizations for Tasks 1–5.Each task panel illustrates the initial setup and CogVLA’s execution process based on the natural language instruction. For Task 1, multi-view observations from theFront Camera,Left Wrist Camera, andRight Wrist Cameraare provided to capture dual-arm coordination. For Tasks 2–5, representative frames from theFront Camerahighlight key manipulation stages. These visualizations support interpretation of task complexity and grounding behavior.",
                "position": 1627
            },
            {
                "img": "https://arxiv.org/html/2508.21046/app_figs/appx_d2.jpg",
                "caption": "Figure 6:Third-person visualization of CogVLA performing a manipulation task.The corresponding video is provided in the supplementary materials. Gripper details are highlighted with red circles.",
                "position": 1630
            },
            {
                "img": "https://arxiv.org/html/2508.21046/app_figs/appx_d3.jpg",
                "caption": "Figure 7:Attention maps between aggregation tokens and patch tokens in DINOv2 and SigLIP.We visualize the attention maps from 17 out of 64 aggregation tokens to the patch tokens of the observation, covering four sets of visualizations across two visual encoders and two camera views. The input language instruction is: “Pick up the black bowl between the plate and the ramekin and place it on the plate.” Both DINOv2 and SigLIP exhibit varying degrees of focused attention on patch tokens relevant to the instruction.",
                "position": 1633
            }
        ]
    },
    {
        "header": "Appendix EDiscussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21046/app_figs/appx_d4.jpg",
                "caption": "Figure 8:Manipulation Workflows and Visualizations in the LIBERO Simulation Environment.We present the execution processes of CogVLA across LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, and LIBERO-Long, demonstrating its strong performance under diverse instructions and a wide range of tasks.",
                "position": 1688
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]