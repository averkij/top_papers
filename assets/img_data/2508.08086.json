[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.08086/x1.png",
                "caption": "Figure 1:Matrix-3D can generate omnidirectional explorable 3D world from image or text input.",
                "position": 111
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.08086/x2.png",
                "caption": "Figure 2:Comparison between perspective and panoramic images. Panoramic images can capture a significantly wider field of view than perspective images.",
                "position": 194
            }
        ]
    },
    {
        "header": "3Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.08086/x3.png",
                "caption": "Figure 3:Core components of our framework. Given trajectory guidance in the form of scene mesh renderings and corresponding masks, obtained by rendering an estimated mesh along a user-defined camera trajectory, we train an image-to-video diffusion model to generate high-quality panoramic videos that precisely follow the specified trajectory. The generated 2D panoramic content is then lifted into an omnidirectional, explorable 3D world using a large-scale panorama reconstruction model.",
                "position": 230
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.08086/x4.png",
                "caption": "Figure 4:Comparison of trajectory guidance derived from mesh and point cloud representations. Results guided by point clouds suffer from noticeable artifacts, which degrade generation quality.",
                "position": 255
            },
            {
                "img": "https://arxiv.org/html/2508.08086/x5.png",
                "caption": "Figure 5:Dataset Illustration. We present a scene from the dataset and the data collection process. Two points are firstly randomly sampled on the route and connected by their shortest path as the red line segments shows. Then, a Laplacian smoothing algorithm is applied on the initial path to reach the smooth greed path, where paranoma videos and depths will be recorded along with their camera poses. We also show the captured rgb image and depth map of three different frames on the bottom of the figure.",
                "position": 334
            }
        ]
    },
    {
        "header": "5Matrix-Pano Dataset",
        "images": []
    },
    {
        "header": "6Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.08086/x6.png",
                "caption": "Figure 6:Qualitative comparison of panorama video generation methods. For each generated video, we extract panoramic images from the first frame (t=0t=0), the middle frame (t=T/2t=T/2), and the last frame (t=Tt=T), along with their four orthogonal perspective views listed below. Matrix-3D produces panoramic videos with exceptional visual quality and geometric consistency.",
                "position": 498
            },
            {
                "img": "https://arxiv.org/html/2508.08086/x7.png",
                "caption": "Figure 7:Qualitative comparison of 3D world reconstruction.",
                "position": 650
            },
            {
                "img": "https://arxiv.org/html/2508.08086/x8.png",
                "caption": "Figure 8:Comparison with WorldLabsworldlabs2025. Both methods generate 3D scenes from the same input image, and we show the renderings of the 3D scene at the furthest reached positions, all under the same field of view. Matrix-3D can generate 3D scenes with greater range than WorldLabs.",
                "position": 692
            },
            {
                "img": "https://arxiv.org/html/2508.08086/x9.png",
                "caption": "Figure 9:Endless Exploration. With an input image and initial trajectory, users can generate the first 3D scene segment and then explore further by changing direction and following a second trajectory. For ease of viewing, we present the perspective projection results from the center of each panorama.",
                "position": 696
            },
            {
                "img": "https://arxiv.org/html/2508.08086/x10.png",
                "caption": "Figure 10:Qualitative comparison between trajectory guidance from point cloud and mesh renderings. Our method, which utilizes scene mesh renderings, significantly outperforms point cloud-based guidance in both geometric and textural consistency, camera controllability and visual quality.",
                "position": 707
            },
            {
                "img": "https://arxiv.org/html/2508.08086/x11.png",
                "caption": "Figure 11:Comparison between depth predictions from the\nDPT head and the 3D deconvolution head. When depth-related parameters are not frozen, depth degrades during second-stage training.",
                "position": 773
            }
        ]
    },
    {
        "header": "7Conclusions and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APanorama Image Generation",
        "images": []
    },
    {
        "header": "Appendix BLRM Network Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.08086/x12.png",
                "caption": "Figure 12:The network architecture of our large panorama reconstruction model.",
                "position": 1815
            }
        ]
    },
    {
        "header": "Appendix CDataset Construction Details",
        "images": []
    }
]