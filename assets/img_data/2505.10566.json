[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.10566/x1.png",
                "caption": "Figure 1.3D-aware photo editing. Given a source image with user-specified 3D transformations, our model generates a new image that follows the userâ€™s edit while preserving the input identity. The 3D edit is visualized via the transformation between the original mesh (pink) and the edited mesh (cyan).",
                "position": 135
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": []
    },
    {
        "header": "2.Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.10566/x2.png",
                "caption": "Figure 2.Inference pipeline.We assume editing instructions (possibly converted from text prompts) are in the form of 3D operations like rotation and translation. Given a mask indicating the object to be edited, we first perform image-to-3D(Xu etÂ al.,2024)to reconstruct the mesh. We then apply the userâ€™s desired 3D edit to obtain the 3D guidance. Here the 3D edit is visualized as the transformation between original mesh (pink wire-frame) and the edited mesh (cyan wire-frame). Finally, the model outputs the 3D aware editing result.",
                "position": 195
            },
            {
                "img": "https://arxiv.org/html/2505.10566/x3.png",
                "caption": "Figure 3.Data pipeline: Overview.Given a video, we sample two frames, the source frameIsrcsubscriptğ¼srcI_{\\mathrm{src}}italic_I start_POSTSUBSCRIPT roman_src end_POSTSUBSCRIPTand the target frameItgtsubscriptğ¼tgtI_{\\mathrm{tgt}}italic_I start_POSTSUBSCRIPT roman_tgt end_POSTSUBSCRIPT, using optical flow as a cue: we discard videos where the flow indicates little motion through the entire clip. Using Image-to-3D methods, we reconstruct a mesh for the desired object for both frames. We then estimate the 3D transformationğ“ğ“\\mathbf{T}bold_T(see Figure4) between the source frame mesh and the target frame mesh. Availability of the transformationğ“ğ“\\mathbf{T}bold_Tenables two ways\nto create the training data: (1) in â€œTransform Sourceâ€, we paste the rendering of the transformed source mesh onto the target frame; (2) in â€œTransform Targetâ€, we paste the rendering of the target mesh onto the source frame. Data examples are shown in Figure5.",
                "position": 200
            },
            {
                "img": "https://arxiv.org/html/2505.10566/x4.png",
                "caption": "Figure 4.Data pipeline: Estimation of the 3D transformationğ“ğ“\\mathbf{T}bold_T.We estimate the 3D transformationğ“ğ“\\mathbf{T}bold_Tby leveraging correspondences between the source frame and the target frame. Given frames between the source frame and the target frame, we first perform tracking to obtain corresponding points. We then initialize the parameters for the 3D transformationğ“ğ“\\mathbf{T}bold_Tand use an optimization to improveğ“ğ“\\mathbf{T}bold_T: (1) We unproject the 2D correspondences on the source frame to 3D pointclouds and apply the currentğ“ğ“\\mathbf{T}bold_Tto transform points to the target image; (2) we project points back to 2D and compare via an L2 loss with the 2D correspondences of the target frame.",
                "position": 206
            },
            {
                "img": "https://arxiv.org/html/2505.10566/x5.png",
                "caption": "Figure 5.Examples of the training data.Given a video, we use the steps described in Figure3to obtain the training data, i.e., the source imageIsrcsubscriptğ¼srcI_{\\mathrm{src}}italic_I start_POSTSUBSCRIPT roman_src end_POSTSUBSCRIPTand the target imageItgtsubscriptğ¼tgtI_{\\mathrm{tgt}}italic_I start_POSTSUBSCRIPT roman_tgt end_POSTSUBSCRIPT. The guidance image is obtained via the developed data pipeline. The mask has three values: 0 indicates the hole created by the coarse edit and the model needs to inpaint by looking at the details of the reference; 0.5 refers to the rendering of the object; and 1.0 denotes the original background.",
                "position": 210
            },
            {
                "img": "https://arxiv.org/html/2505.10566/x6.png",
                "caption": "Figure 6.Overview of the training pipeline. We develop a conditional diffusion model for 3D-aware image editing. It consists of two networks:fgensubscriptğ‘“genf_{\\mathrm{gen}}italic_f start_POSTSUBSCRIPT roman_gen end_POSTSUBSCRIPTandfdetailsubscriptğ‘“detailf_{\\mathrm{detail}}italic_f start_POSTSUBSCRIPT roman_detail end_POSTSUBSCRIPT. During training, given the inputsâ€”target frameItgtsubscriptğ¼tgtI_{\\mathrm{tgt}}italic_I start_POSTSUBSCRIPT roman_tgt end_POSTSUBSCRIPT, 3D guidanceIguidesubscriptğ¼guideI_{\\mathrm{guide}}italic_I start_POSTSUBSCRIPT roman_guide end_POSTSUBSCRIPT, maskMguidesubscriptğ‘€guideM_{\\mathrm{guide}}italic_M start_POSTSUBSCRIPT roman_guide end_POSTSUBSCRIPT, and detail featureFtsubscriptğ¹ğ‘¡F_{t}italic_F start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTâ€”fgensubscriptğ‘“genf_{\\mathrm{gen}}italic_f start_POSTSUBSCRIPT roman_gen end_POSTSUBSCRIPTlearns the reverse diffusion process to predict the noiseÏµitalic-Ïµ\\epsilonitalic_Ïµand reconstructItgtsubscriptğ¼tgtI_{\\mathrm{tgt}}italic_I start_POSTSUBSCRIPT roman_tgt end_POSTSUBSCRIPT. To better preserve identity and fine-grained details from the source imageIsrcsubscriptğ¼srcI_{\\mathrm{src}}italic_I start_POSTSUBSCRIPT roman_src end_POSTSUBSCRIPT,fdetailsubscriptğ‘“detailf_{\\mathrm{detail}}italic_f start_POSTSUBSCRIPT roman_detail end_POSTSUBSCRIPTtakes as input the source imageIsrcsubscriptğ¼srcI_{\\mathrm{src}}italic_I start_POSTSUBSCRIPT roman_src end_POSTSUBSCRIPT, its noisy counterpartItsubscriptğ¼ğ‘¡I_{t}italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, and the maskMguidesubscriptğ‘€guideM_{\\mathrm{guide}}italic_M start_POSTSUBSCRIPT roman_guide end_POSTSUBSCRIPT, and extracts detail featuresFtsubscriptğ¹ğ‘¡F_{t}italic_F start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. We apply cross-attention betweenFtsubscriptğ¹ğ‘¡F_{t}italic_F start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTand the intermediate features offgensubscriptğ‘“genf_{\\mathrm{gen}}italic_f start_POSTSUBSCRIPT roman_gen end_POSTSUBSCRIPTto incorporate content and details fromIsrcsubscriptğ¼srcI_{\\mathrm{src}}italic_I start_POSTSUBSCRIPT roman_src end_POSTSUBSCRIPTduring the reverse diffusion process.",
                "position": 216
            }
        ]
    },
    {
        "header": "3.Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.10566/x7.png",
                "caption": "Figure 7.Comparison with baselines.We compare several state of the art baselines with different kinds of conditions, such as 3D transforms, drags, inpainting masks, and text prompts. We can see that none of the baselines accurately follow the target 3D transform while preserving identity. Baselines that directly use 3D transforms suffer from a lack of good training data, and using other types of conditions makes it hard to unambiguously specify the 3D transform.",
                "position": 459
            }
        ]
    },
    {
        "header": "4.Experiments",
        "images": []
    },
    {
        "header": "5.Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.10566/x8.png",
                "caption": "Figure 8.3D editing with continuous rotations.We demonstrate that the proposed method can handle extensive 3D edits on common objects. In each scenario, we progressively increase the rotation from 0 to 180 degrees along the y-axis, applying it to the reconstructed mesh to generate the 3D-transformation-based image guidance. The results show that our method can handle large out-of-plane 3D rotations during editing.",
                "position": 1157
            },
            {
                "img": "https://arxiv.org/html/2505.10566/x9.png",
                "caption": "Figure 9.Comparison with Magic Fixup.The results demonstrate that the proposed method achieves more realistic outputs by leveraging the 3D-transformation-based guidance. For instance, our method effectively handles pose changes, such as adjusting the cameraâ€™s viewing direction for the cakes and jaguar, or modifying the poses of the horse and parrot.",
                "position": 1162
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]