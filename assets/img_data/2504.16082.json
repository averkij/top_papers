[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16082/x1.png",
                "caption": "Figure 1:MapReduce Principle.Long video understanding requires bothglobal comprehensionanddetailed perception, as in the motivating example. For such needs, (a) sequence-to-sequence VLMs and (b) video agents are sub-optimal in terms of context lengths, sequential parallelization, and global context information. (c) Instead, we explore and develop a simple “MapReduce” principle via MR. Video and (d) overcome such challenges.",
                "position": 125
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16082/x2.png",
                "caption": "Figure 2:Overview.MR. Video validates the effectiveness of “MapReduce” principle with an LLM agent framework. We demonstrate two distinct types of questions for visual details and reasoning.(a)Captioning(Sec.3.2) first generates detailed captions of individual scenes (Map) and then enhances consistency by merging repeated characters/objects for the scenes (Reduce).(b)Question Intention Analysis(Sec.3.3) investigates if a video segment contributes useful information (Map) and then forms a unified analysis at the video level (Reduce).(c)Goal-Aware Analysis(Sec.3.4) delves deep into detailed perception and reasoning with available contexts, guided by the intention analysis (Map), then unifies them into a final answer (Reduce). For clarity, MR. Video’s intermediate texts are simplified.",
                "position": 167
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16082/x3.png",
                "caption": "Figure 3:Key Characters/Objects in Captioning.(a) The “map” step extracts the salient characters/objects along with a description, which is useful for frames with multiple characters (the 3rd frame). (b) Then, the “reduce” step uses VLM to associate the repeated characters, enhancing the consistency of captions.",
                "position": 211
            },
            {
                "img": "https://arxiv.org/html/2504.16082/x4.png",
                "caption": "Figure 4:Question Intention Analysis.Long video questions generally require the model to recover certain information,e.g., the meaning of “protagonist”, what a “utility room” looks like, and confounding relevant segments, as in (a). This motivates MR. Video’sexplicitly understanding the question’s intentionsby reasoning both the video contents and questions.",
                "position": 232
            },
            {
                "img": "https://arxiv.org/html/2504.16082/x5.png",
                "caption": "Figure 5:Customized Queries for Perception.With this question requiring detailed visual perception, MR. Video proposes objective-aware queries for the VLMs, confirming the criteria.",
                "position": 269
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16082/x6.png",
                "caption": "Figure 6:Analytical Experiments of MR. Video. (a) We investigate the benefits of MR. Video components and the influence of LLMs. (b) The comparison between our question intention analysis and key frame retrieval suggests the necessity of combining more video contexts for understanding.",
                "position": 696
            },
            {
                "img": "https://arxiv.org/html/2504.16082/x7.png",
                "caption": "Figure 7:Referring back to the motivating example(Fig.1), MR. Video demonstrates the necessity of “MapReduce” principle: checking every scene in detail (map) and aggregating information for the whole video (reduce). Although the model misses some goals due to strict criteria (the full action of shooting, goal, and celebration) and frame sampling, MR. Video shows the desired behavior ofcounting exhaustivelyand speculates at least 78 goals.",
                "position": 721
            },
            {
                "img": "https://arxiv.org/html/2504.16082/x8.png",
                "caption": "Figure 8:Failure Case Analysis.MR. Video’s failure largely comes from (1) the LLMsnot aligned with human’s way of watching videos, so that it cannot convincingly reason about the scene transitions (example 1); and (2) VLMs failing to capture certain details, where the LLM does not have sufficient information (example 2). (Noodles and the blurry face are pointed with arrows.)",
                "position": 734
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "ADelving into Long Video Benchmarks",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16082/x9.png",
                "caption": "Figure A:Examples of Different Long Video Benchmarks.",
                "position": 1585
            }
        ]
    },
    {
        "header": "BPrompts and Implementation Details",
        "images": []
    }
]