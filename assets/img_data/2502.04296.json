[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04296/x1.png",
                "caption": "Figure 1:Action-Video Dynamics Model from Heterogeneous Robot Interactions.HMAutilizes heterogeneous datasets comprising over 3 million trajectories (videos) from 40 distinct embodiments to pre-train a full dynamics model with next-set-of-token predictions using masked autoregression. After pre-training, the resulting action-video dynamics model is versatile, supporting applications such as video simulation, policy evaluation, synthetic data generation, and direct adoption as an imitation policy.",
                "position": 104
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Heterogeneous Masked Autoregression",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04296/x2.png",
                "caption": "Figure 2:Dynamics Model.Masked autoregression in the dynamics model generalizes multiple problem settings including policy learning, forward and passive dynamics, and full dynamics.",
                "position": 152
            },
            {
                "img": "https://arxiv.org/html/2502.04296/x3.png",
                "caption": "Figure 3:Network Architecture.TheHMAmodel architecture maps low-level video and action sequences across different embodiments into a shared latent space. For actions, embodiment projectors are activated based on the training sample. The spatial-temporal Transformer produces the output video and action tokens for future frames.",
                "position": 241
            }
        ]
    },
    {
        "header": "4Pre-Training Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04296/x4.png",
                "caption": "Figure 4:Pre-trained Video Model Generation.We show that a single unifiedHMAmodel can generate realistic (left 3 columns) and diverse (right 3 columns) videos across multiple embodiment datasets with heterogeneous action spaces. Each group shows three generated frames from a single sequence.",
                "position": 268
            },
            {
                "img": "https://arxiv.org/html/2502.04296/x5.png",
                "caption": "Figure 5:Ablation on Pre-training Settings and Architecture.Under the pre-training setting with VQ tokens, we ablate the video generation performance (visual fidelity measured by perplexity and controllability measured by controllability). (a) We find action-conditioned models outperform passive video models. (b) We compare different action conditioning architectures in the masked autoregression framework. The purple color denotes the best model that we use by default.",
                "position": 271
            },
            {
                "img": "https://arxiv.org/html/2502.04296/x6.png",
                "caption": "Figure 6:Experiments on Scaling Behaviors ofHMA.We observe positive trends in the scaling performance of heterogeneous video models across axes including the number of datasets, number of trajectories, and model sizes. The evaluation metrics on fidelity (perplexity) and controllability (ΔΔ\\Deltaroman_ΔPSNR) are averaged across validation datasets.",
                "position": 291
            }
        ]
    },
    {
        "header": "5Post-Training Applications",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04296/x7.png",
                "caption": "Figure 7:Qualitative Comparisons Between Tokenizers and Models.Despite longer convergence time, diffusion-based methods (Eq.3) on soft tokens generate better visual quality than on VQ tokens (Eq.2), qualitatively and measured by PSNR.",
                "position": 466
            },
            {
                "img": "https://arxiv.org/html/2502.04296/x8.png",
                "caption": "Figure 8:Video Controllability.HMAcan follow user action inputs to generate physically plausible object permanence (top row) and block pushing interactions (bottom row). These video predictions are both at out-of-distribution settings and at a much longer horizon than training (over 100 frames).",
                "position": 469
            },
            {
                "img": "https://arxiv.org/html/2502.04296/x9.png",
                "caption": "Figure 9:Policy Evaluation withHMA.By learning the action-video dynamics over both successful and failed examples,HMAcan be used to evaluate policies, similar to a traditional simulator[46]. The autoregressive horizon at inference time is 10 times more than the training time horizon.",
                "position": 513
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]