[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17258/x1.png",
                "caption": "Figure 1:VideoGrain enables multi-grained video editing across class, instance, and part levels.",
                "position": 85
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17258/x2.png",
                "caption": "Figure 2:Definition of multi-grained video editing and comparison on instance editing",
                "position": 91
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17258/x3.png",
                "caption": "Figure 3:Analysis of why the diffusion model failed in instance-level video editing. Our goal is to edit left man into ‚ÄúIron Man,‚Äù right man into ‚ÄúSpiderman,‚Äù and trees into ‚Äúcherry blossoms.‚Äù In (b), we apply K-Means on self-attention, and in (d), we visualize the 32x32 cross-attention map.",
                "position": 180
            },
            {
                "img": "https://arxiv.org/html/2502.17258/x4.png",
                "caption": "Figure 4:VideoGrain pipeline.\n(1) we integrate ST-Layout Attn into the frozen SD for multi-grained editing, where we modulate self- and cross-attention in a unified manner.\n(2) In cross-attention, we view each local prompt and its location as positive pairs, while the prompt and outside-location areas are negative pairs, enabling text-to-region control.\n(3) In self-attention,\nwe enhance positive awareness within intra-regions and restrict negative interactions between inter-regions across frames, making each query only attend to the target region and keep feature separation. In the bottom two figures,pùëùpitalic_pdenotes original attention score andw,iùë§ùëñw,iitalic_w , italic_idenotes the word and frame index.",
                "position": 235
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17258/x5.png",
                "caption": "Figure 5:Qualitative results. VideoGrain achieves multi-grained video editing, including class-level, instance-level, and part-level. We refer the reader to ourproject pagefor full-video results.",
                "position": 387
            },
            {
                "img": "https://arxiv.org/html/2502.17258/x6.png",
                "caption": "Figure 6:Qualitative comparisons.\nWe refer the reader to ourproject pagefor detailed assessment.",
                "position": 405
            },
            {
                "img": "https://arxiv.org/html/2502.17258/x7.png",
                "caption": "Table 2:Efficiency comparison.",
                "position": 513
            },
            {
                "img": "https://arxiv.org/html/2502.17258/x7.png",
                "caption": "Figure 7:Attention weight distribution.",
                "position": 580
            },
            {
                "img": "https://arxiv.org/html/2502.17258/x8.png",
                "caption": "Figure 8:Ablation of cross- and self-modulation in ST-Layout Attn.",
                "position": 598
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Ethics statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AEvaluate SAM-Track masks‚Äô impact",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17258/x9.png",
                "caption": "Figure 9:VideoP2P joint and sequential edit with SAM-Track masks",
                "position": 1340
            },
            {
                "img": "https://arxiv.org/html/2502.17258/x10.png",
                "caption": "Figure 10:Ground-A-Video joint edit with instance information",
                "position": 1351
            }
        ]
    },
    {
        "header": "Appendix BVideoGrain can work without SAM-Track masks",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17258/x11.png",
                "caption": "Figure 11:Our method without additional SAM-Track masks",
                "position": 1364
            }
        ]
    },
    {
        "header": "Appendix CSolely edit on specific subjects, without background changed",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17258/x12.png",
                "caption": "Figure 12:Soely edit on specific subjects, without background changed",
                "position": 1376
            }
        ]
    },
    {
        "header": "Appendix DPart-Level modification examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17258/x13.png",
                "caption": "Figure 13:Part-level modifications on humans and animals",
                "position": 1386
            }
        ]
    },
    {
        "header": "Appendix ETemporal Focus of ST-Layout Attn",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17258/x14.png",
                "caption": "Figure 14:Temporal Focus of ST-Layout Attn",
                "position": 1396
            }
        ]
    },
    {
        "header": "Appendix FControlNet Ablation",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17258/x15.png",
                "caption": "Figure 15:ControlNet ablation",
                "position": 1406
            }
        ]
    },
    {
        "header": "Appendix GMore general objects and shape editing",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17258/x16.png",
                "caption": "Figure 16:More general objects instance editing (animals) and shape editing (cars) results.",
                "position": 1417
            }
        ]
    },
    {
        "header": "Appendix HMore visualization",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17258/x17.png",
                "caption": "Figure 17:More frames ablation of ST-Layout Attn‚Äôs effects on attention weight distribution.",
                "position": 1424
            }
        ]
    },
    {
        "header": "Appendix ILatent Blend",
        "images": []
    },
    {
        "header": "Appendix JExperimental Details",
        "images": []
    },
    {
        "header": "Appendix KLimitations.",
        "images": []
    }
]