[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.07239/x1.png",
                "caption": "",
                "position": 328
            }
        ]
    },
    {
        "header": "\\PragyaHeadline1\\PragyaHeadlineWhat Do We Mean by â€œDeterminismâ€ in LLM Inference?",
        "images": []
    },
    {
        "header": "\\PragyaHeadline2\\PragyaHeadlineLetÅ›Stress-Testâ€œDeterministic Inferenceâ€ in Practice",
        "images": []
    },
    {
        "header": "\\PragyaHeadline3\\PragyaHeadlineDeterministic Inference Encourages Benchmark Memorization",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.07239/figures/glue_robustness_heatmap_v6.png",
                "caption": "Figure 1:GLUE Robustness Heatmap under Deterministic vs.Â Stochastic\nDecoding.Each cell shows the robustness ratioRt(d)â€‹(m)R_{t}^{(d)}(m)(higher is better) for tasktâˆˆ{MNLI,QQP,QNLI,SST-2}t\\in\\{\\text{MNLI},\\text{QQP},\\text{QNLI},\\text{SST-2}\\}, decoding modedâˆˆ{Stoch,Det}d\\in\\{\\text{Stoch},\\text{Det}\\}, and modelmm(columns, same\nzoo as in the FRACTURE analysis). Darker green indicates that\nparaphrased, perturbed, and adversarial variants preserve most of the\nmodelâ€™s original GLUE accuracy; purple indicates severe degradation.\nAcross tasks and models,Stochastic rows are consistently\ngreener than their Deterministic counterparts, showing thatbitwise-deterministic greedy decoding systematically\nunderestimates the distributional generalization capacity of the\nunderlying model. In other words,deterministic evaluation\nreplays the GLUE mistake: it optimizes for one canonical completion\nper prompt, while stochastic, distributional evaluation reveals that\nthe modelâ€™s competence is broaderâ€”and its brittleness more severeâ€”\nthan the single trace suggests.",
                "position": 1286
            },
            {
                "img": "https://arxiv.org/html/2601.07239/robustness_ratio/robustness_ratio_Claude.png",
                "caption": "Figure 2:Robustness ratios forClaudeacross GLUE tasks.Understochasticdecoding (teal), Claude attains robustness ratios between0.85\\mathbf{0.85}and0.91\\mathbf{0.91}across MNLI, QQP, QNLI, and SST-2, whereasdeterministicdecoding (orange) stays in the lower0.79â€‹â€“â€‹0.82\\mathbf{0.79\\text{--}0.82}band.\nThis yields absolute stochasticâ€“deterministic gaps in the range of0.05â€‹â€“â€‹0.12\\mathbf{0.05\\text{--}0.12}.\nThe tight stochastic violins on QNLI and SST-2 indicatelow variance across perturbation types, while the slightly wider shapes on MNLI and QQP revealtask-dependent sensitivity.\nOverall,Claude is consistently more robust when decoded stochastically, and the gains are not marginal but numerically substantial.",
                "position": 1390
            },
            {
                "img": "https://arxiv.org/html/2601.07239/robustness_ratio/robustness_ratio_DeepSeek.png",
                "caption": "Figure 3:Robustness ratios forDeepSeekacross GLUE tasks.Stochastic decodingplaces DeepSeek in a high-robustness regime, with ratios spanning0.85â€‹â€“â€‹0.93\\mathbf{0.85\\text{--}0.93}across tasks, whiledeterministic decodinglags behind at0.76â€‹â€“â€‹0.81\\mathbf{0.76\\text{--}0.81}.\nThe stochasticâ€“deterministic gap ranges from about0.05\\mathbf{0.05}up to0.16\\mathbf{0.16}absolute points, making DeepSeek one of the models with thelargest decoding-induced robustness gains.\nQNLI and SST-2 show the highest stochastic robustness, whereas MNLI and QQP display broader violins, reflectingincreased variability under perturbations.\nThese numbers highlight thatDeepSeekâ€™s strong robustness is tightly coupled to stochastic inference; deterministic decoding leaves significant robustness â€œon the table.â€",
                "position": 1399
            },
            {
                "img": "https://arxiv.org/html/2601.07239/robustness_ratio/robustness_ratio_Gemma-2_9B.png",
                "caption": "Figure 4:Robustness ratios forGemma-2 9Bacross GLUE tasks.Withstochasticdecoding, Gemma-2 9B achieves robustness ratios between0.82\\mathbf{0.82}and0.91\\mathbf{0.91}, whiledeterministicdecoding stays in the0.77â€‹â€“â€‹0.82\\mathbf{0.77\\text{--}0.82}range.\nThe task-wise stochasticâ€“deterministic differences vary from essentially0.00\\mathbf{0.00}(one task where deterministic is on par) up to about0.09\\mathbf{0.09}absolute points.\nQQP and SST-2 show thehighest stochastic robustness, while MNLI and QNLI are slightly lower and more spread out.\nThis figure indicates thateven a mid-sized open model like Gemma-2 9B benefits measurably from stochastic decoding, though the magnitude of gains is somewhat smaller and more task-dependent than for frontier proprietary models.",
                "position": 1408
            },
            {
                "img": "https://arxiv.org/html/2601.07239/robustness_ratio/robustness_ratio_Gemma-2_27B.png",
                "caption": "Figure 5:Robustness ratios forGemma-2 27Bacross GLUE tasks.Scaling to 27B pushes thestochastic robustnessband to0.86â€‹â€“â€‹0.90\\mathbf{0.86\\text{--}0.90}, whiledeterministicdecoding lies in the slightly lower interval0.79â€‹â€“â€‹0.84\\mathbf{0.79\\text{--}0.84}.\nStochasticâ€“deterministic gaps span roughly0.02â€‹â€“â€‹0.10\\mathbf{0.02\\text{--}0.10}across tasks, smaller than for some proprietary models but stillsystematically positive.\nMNLI and QQP show clear upward shifts compared to Gemma-2 9B, and SST-2 reaches the top of the modelâ€™s robustness range withnarrow, high violins.\nThe combination of higher means and reduced spread suggests thatGemma-2 27B is both more robust and more stable, yet still meaningfully boosted by stochastic decoding.",
                "position": 1417
            },
            {
                "img": "https://arxiv.org/html/2601.07239/robustness_ratio/robustness_ratio_GPT-4o_mini.png",
                "caption": "Figure 6:Robustness ratios forGPT-4o miniacross GLUE tasks.Understochasticdecoding, GPT-4o mini attains robustness ratios in the0.84â€‹â€“â€‹0.89\\mathbf{0.84\\text{--}0.89}range, whereasdeterministicdecoding falls between0.76\\mathbf{0.76}and0.84\\mathbf{0.84}.\nThe resulting gaps are on the order of0.05â€‹â€“â€‹0.09\\mathbf{0.05\\text{--}0.09}absolute points depending on the task.\nMNLI and QQP sit around the lower end of the stochastic band, while QNLI and especially SST-2 approach the top, indicating thatclassification-style tasks can remain robust even for a compressed model.\nThese numeric ranges show thateven a distilled GPT-4o variant retains a sizable robustness margin under stochastic decoding, making inference-time choices crucial when deploying lightweight models.",
                "position": 1426
            },
            {
                "img": "https://arxiv.org/html/2601.07239/robustness_ratio/robustness_ratio_GPT-4o.png",
                "caption": "Figure 7:Robustness ratios forGPT-4oacross GLUE tasks.GPT-4o shows one of thestrongest robustness profiles: stochastic ratios consistently lie between0.87\\mathbf{0.87}and0.93\\mathbf{0.93}, while deterministic decoding drops to0.75â€‹â€“â€‹0.84\\mathbf{0.75\\text{--}0.84}.\nTask-wise stochasticâ€“deterministic gaps range from about0.04\\mathbf{0.04}up to0.16\\mathbf{0.16}absolute points, with the largest differences on QNLI and SST-2.\nThe tight, high violins for stochastic decoding indicatehigh robustness and low variance, whereas deterministic violins are wider and noticeably shifted down.\nThese results underscore thatGPT-4oâ€™s robustness is not merely a property of the underlying model but also of the decoding policy: deterministic inference underutilizes its potential.",
                "position": 1435
            },
            {
                "img": "https://arxiv.org/html/2601.07239/robustness_ratio/robustness_ratio_GPT-35.png",
                "caption": "Figure 8:Robustness ratios forGPT-3.5across GLUE tasks.Forstochasticdecoding, robustness ratios span0.84â€‹â€“â€‹0.91\\mathbf{0.84\\text{--}0.91}, situating GPT-3.5 below GPT-4o but still in a relatively strong band.Deterministicdecoding compresses the model into the0.78â€‹â€“â€‹0.83\\mathbf{0.78\\text{--}0.83}range, with per-task gaps of roughly0.06â€‹â€“â€‹0.11\\mathbf{0.06\\text{--}0.11}absolute points.\nQQP and MNLI exhibit thelargest downward shifts and broader violinsunder deterministic decoding, signaling heightened vulnerability to adversarial paraphrases in these settings.\nTaken together, the figure positionsGPT-3.5 as a mid-robustness baseline whose observed robustness is highly sensitive to decoding: small sampling changes can translate intoğŸ“â€‹â€“â€‹ğŸğŸ\\mathbf{5\\text{--}10}\\,pp differences in robustness ratio.",
                "position": 1444
            },
            {
                "img": "https://arxiv.org/html/2601.07239/robustness_ratio/robustness_ratio_LLaMA-2_7B.png",
                "caption": "Figure 9:Robustness ratios forLLaMA-2 7Bacross GLUE tasks.Stochasticdecoding yields robustness ratios between0.81\\mathbf{0.81}and0.89\\mathbf{0.89}, whiledeterministicdecoding ranges more widely from0.72\\mathbf{0.72}up to0.86\\mathbf{0.86}.\nThe stochasticâ€“deterministic differences vary from a slight negative value (one task where deterministic happens to be slightly higher) to a substantial positive gap of about0.15\\mathbf{0.15}absolute points.\nMNLI and QNLI show thelowest medians and widest violins, indicating that a 7B-class open model struggles most on inference-style tasks under perturbations.\nNumerically, this figure illustrates thatLLaMA-2 7B sits at the lower end of the robustness spectrum and is highly decoding-sensitive, making it an informative but fragile baseline.",
                "position": 1453
            },
            {
                "img": "https://arxiv.org/html/2601.07239/robustness_ratio/robustness_ratio_LLaMA-2_13B.png",
                "caption": "Figure 10:Robustness ratios forLLaMA-2 13Bacross GLUE tasks.After scaling to 13B,stochastic robustnessclimbs to the0.84â€‹â€“â€‹0.94\\mathbf{0.84\\text{--}0.94}range, whiledeterministicdecoding stays in a narrower but lower interval of0.79â€‹â€“â€‹0.82\\mathbf{0.79\\text{--}0.82}.\nThe resulting stochasticâ€“deterministic gaps fall between0.03\\mathbf{0.03}and0.14\\mathbf{0.14}absolute points, with the largest gains again on MNLI and QNLI.\nCompared to LLaMA-2 7B, both decoding modes shift upward and the stochastic violins becometighter, especially on QQP and SST-2.\nThis figure shows thatscaling within the same family substantially improves robustness, yet the qualitative pattern remains: stochastic decoding consistently exposes a more robust operating regime than deterministic decoding.",
                "position": 1462
            },
            {
                "img": "https://arxiv.org/html/2601.07239/robustness_ratio/robustness_ratio_LLaMA-3_7B.png",
                "caption": "Figure 11:Robustness ratios forLLaMA-3 7Bacross GLUE tasks.Despite having the same parameter count as LLaMA-2 7B,LLaMA-3 7Bachieves higher stochastic robustness, with ratios in the0.83â€‹â€“â€‹0.90\\mathbf{0.83\\text{--}0.90}range.Deterministicdecoding occupies0.77â€‹â€“â€‹0.84\\mathbf{0.77\\text{--}0.84}, and stochasticâ€“deterministic gaps are more modest but still positive at roughly0.06â€‹â€“â€‹0.08\\mathbf{0.06\\text{--}0.08}absolute points.\nQQP and QNLI show thehighest robustness and the tightest violins, while MNLI remains the most challenging task.\nQuantitatively, this figure suggests thatarchitectural and data improvements from LLaMA-2 to LLaMA-3 shift the entire robustness band upward, even though the fundamental advantage of stochastic decoding persists.",
                "position": 1471
            },
            {
                "img": "https://arxiv.org/html/2601.07239/robustness_ratio/robustness_ratio_LLaMA-3_8B.png",
                "caption": "Figure 12:Robustness ratios forLLaMA-3 8Bacross GLUE tasks.Understochastic decoding, LLaMA-3 8B attains robustness ratios in the0.89â€‹â€“â€‹0.96\\mathbf{0.89\\text{--}0.96}band (roughly0.91\\mathbf{0.91}on MNLI,0.80\\mathbf{0.80}on QQP,0.86\\mathbf{0.86}on QNLI, and0.95\\mathbf{0.95}on SST-2), whereasdeterministic decodingfalls to the0.74â€‹â€“â€‹0.79\\mathbf{0.74\\text{--}0.79}band across the same tasks.\nThe stochasticâ€“deterministic gaps range from about0.06\\mathbf{0.06}(QQP, QNLI) up to nearly0.18\\mathbf{0.18}(SST-2), showinglarge decoding-induced robustness gains.\nThe high, tight stochastic violin on SST-2 in particular indicates thatLLaMA-3 8B becomes extremely robust when decoded stochastically, while deterministic decoding systematically underestimates its robustness.",
                "position": 1480
            },
            {
                "img": "https://arxiv.org/html/2601.07239/robustness_ratio/robustness_ratio_LLaMA-3_70B.png",
                "caption": "Figure 13:Robustness ratios forLLaMA-3 70Bacross GLUE tasks.Stochastic decodingplaces LLaMA-3 70B in a strong robustness band of0.84â€‹â€“â€‹0.96\\mathbf{0.84\\text{--}0.96}: around0.85\\mathbf{0.85}on MNLI,0.88\\mathbf{0.88}on QQP,0.88\\mathbf{0.88}on QNLI, and near0.96\\mathbf{0.96}on SST-2.\nIn contrast,deterministic decodingcompresses robustness into the lower0.74â€‹â€“â€‹0.83\\mathbf{0.74\\text{--}0.83}interval.\nThe resulting stochasticâ€“deterministic differences span roughly0.04â€‹â€“â€‹0.13\\mathbf{0.04\\text{--}0.13}absolute points, with thelargest margins on SST-2 and QQP.\nCompared with LLaMA-3 7B, these numbers show thatscaling to 70B significantly strengthens robustness while preserving the same qualitative advantage of stochastic decoding.",
                "position": 1488
            },
            {
                "img": "https://arxiv.org/html/2601.07239/robustness_ratio/robustness_ratio_Mistral-7B.png",
                "caption": "Figure 14:Robustness ratios forMistral-7Bacross GLUE tasks.Withstochasticdecoding, Mistral-7B achieves robustness ratios between0.84\\mathbf{0.84}and0.90\\mathbf{0.90}on MNLI, QQP, and QNLI, and around0.78â€‹â€“â€‹0.82\\mathbf{0.78\\text{--}0.82}on SST-2.Deterministicdecoding yields slightly lower values on most tasks, in the0.79â€‹â€“â€‹0.84\\mathbf{0.79\\text{--}0.84}range for MNLI/QQP/QNLI and around0.77â€‹â€“â€‹0.82\\mathbf{0.77\\text{--}0.82}on SST-2.\nStochasticâ€“deterministic gaps are moderate (0.02â€‹â€“â€‹0.06\\mathbf{0.02\\text{--}0.06}absolute), except for SST-2 where deterministic decoding is marginally higher, illustrating thatthe decoding advantage can flip on specific tasks.\nOverall, the figure highlights thatMistral-7B is reasonably robust but exhibits nuanced, task-specific trade-offs between stochastic and deterministic decoding.",
                "position": 1497
            },
            {
                "img": "https://arxiv.org/html/2601.07239/robustness_ratio/robustness_ratio_Mixtral-8x7B.png",
                "caption": "Figure 15:Robustness ratios forMixtral-8Ã—\\times7Bacross GLUE tasks.Stochastic decodingplaces the mixture-of-experts model in a high band of0.83â€‹â€“â€‹0.95\\mathbf{0.83\\text{--}0.95}: about0.92\\mathbf{0.92}on MNLI,0.86\\mathbf{0.86}on QQP,0.83\\mathbf{0.83}on QNLI, and0.89\\mathbf{0.89}on SST-2.Deterministic decodingyields0.77â€‹â€“â€‹0.84\\mathbf{0.77\\text{--}0.84}across tasks, often trailing stochastic decoding by0.05â€‹â€“â€‹0.10\\mathbf{0.05\\text{--}0.10}absolute points.\nThe largest gaps appear on MNLI and SST-2, where violins are clearly separated, while QQP shows a smaller but still positive advantage for stochastic decoding.\nThese patterns indicate thatrouting-based models like Mixtral-8Ã—\\times7B can be highly robust, but their robustness is substantially unlocked only under stochastic inference.",
                "position": 1506
            },
            {
                "img": "https://arxiv.org/html/2601.07239/robustness_ratio/robustness_ratio_Mixtral-8x22B.png",
                "caption": "Figure 16:Robustness ratios forMixtral-8Ã—\\times22Bacross GLUE tasks.Scaling Mixtral to8Ã—\\times22Byields stochastic robustness ratios in the0.84â€‹â€“â€‹0.95\\mathbf{0.84\\text{--}0.95}band: about0.84\\mathbf{0.84}on MNLI,0.85\\mathbf{0.85}on QQP,0.93\\mathbf{0.93}on QNLI, and0.90\\mathbf{0.90}on SST-2.Deterministic decodingremains in a lower0.78â€‹â€“â€‹0.82\\mathbf{0.78\\text{--}0.82}band across all tasks.\nThe stochasticâ€“deterministic margins are modest (0.03â€‹â€“â€‹0.06\\mathbf{0.03\\text{--}0.06}) on MNLI/QQP/SST-2 but become very large on QNLI (â‰ˆ0.10â€‹â€“â€‹0.15\\approx\\mathbf{0.10\\text{--}0.15}).\nThe very tall, narrow stochastic violin for QNLI emphasizeshigh and stable robustness, whereas deterministic decoding exhibits both lower means and larger spread.\nThus,Mixtral-8Ã—\\times22B combines scale with strong stochastic robustness, particularly on inference-style QNLI.",
                "position": 1515
            },
            {
                "img": "https://arxiv.org/html/2601.07239/robustness_ratio/robustness_ratio_Phi-2.png",
                "caption": "Figure 17:Robustness ratios forPhi-2across GLUE tasks.Despite being a small model,stochastic decodingpropels Phi-2 to surprisingly high robustness ratios: around0.86\\mathbf{0.86}on MNLI,0.93â€‹â€“â€‹0.95\\mathbf{0.93\\text{--}0.95}on QQP,0.96â€‹â€“â€‹0.98\\mathbf{0.96\\text{--}0.98}on QNLI, and0.89â€‹â€“â€‹0.93\\mathbf{0.89\\text{--}0.93}on SST-2.\nIn contrast,deterministic decodingstays in the0.74â€‹â€“â€‹0.80\\mathbf{0.74\\text{--}0.80}band across tasks.\nThis yields very large stochasticâ€“deterministic gaps of roughly0.10â€‹â€“â€‹0.17\\mathbf{0.10\\text{--}0.17}absolute points, some of thelargest differences in the entire model suite.\nThe tall, sharply peaked stochastic violins for QQP and QNLI further indicate thatPhi-2â€™s robustness is heavily latent and only surfaces under stochastic inference, making it a striking example of decoding-dependent robustness.",
                "position": 1525
            },
            {
                "img": "https://arxiv.org/html/2601.07239/robustness_ratio/robustness_ratio_Vicuna-7B.png",
                "caption": "Figure 18:Robustness ratios forVicuna-7Bacross GLUE tasks.Withstochastic decoding, Vicuna-7B reaches robustness ratios of roughly0.88\\mathbf{0.88}on MNLI,0.87\\mathbf{0.87}on QQP,0.90\\mathbf{0.90}on QNLI, and0.82â€‹â€“â€‹0.84\\mathbf{0.82\\text{--}0.84}on SST-2.Deterministic decodinglies around0.83\\mathbf{0.83}on MNLI,0.78\\mathbf{0.78}on QQP,0.79\\mathbf{0.79}on QNLI, and0.85â€‹â€“â€‹0.87\\mathbf{0.85\\text{--}0.87}on SST-2.\nThis produces positive stochasticâ€“deterministic gaps of0.05â€‹â€“â€‹0.11\\mathbf{0.05\\text{--}0.11}on MNLI/QQP/QNLI, but anegativegap on SST-2 where deterministic decoding isâ‰ˆ0.03â€‹â€“â€‹0.04\\approx\\mathbf{0.03\\text{--}0.04}higher.\nThe figure thus reveals amixed robustness profile: Vicuna-7B strongly prefers stochastic decoding on inference-heavy tasks but appears better calibrated under deterministic decoding on sentiment classification.",
                "position": 1534
            }
        ]
    },
    {
        "header": "\\PragyaHeadline4\\PragyaHeadlineDeterministic Decoding Suppresses Explorationâ€“Driven Abilities",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/icl_eg_heatmap_k16_v5_open_only.png",
                "caption": "Figure 20:Few-shot ICL accuracy and exploration gains across models on BESSTIE tasks.Each cell shows theabsolute accuracyunder eitherbestâ€“ofâ€“16decoding (top row for each task) orgreedydecoding (bottom row for each task), evaluated onBESSTIEâ€“SentimentandBESSTIEâ€“Sarcasm. For the\ngreedy rows we additionally print theaccuracy gap(â€œâ†“d%\\downarrow d\\%â€) relative to bestâ€“ofâ€“16, whered=EGt,mICLâ€‹(16)Ã—100d=\\mathrm{EG}^{\\text{ICL}}_{t,m}(16)\\times 100.\nThewarmvs.coolcolormap encodes accuracy, while the overlaid arrows quantify how\nmuch capability ishiddenwhen we collapse exploration to a\nsingle deterministic trajectory. Across both tasks, most models\nsuffer8â€‹â€“â€‹228\\text{--}22absolute-pointdrops when moving from\nbestâ€“ofâ€“16 to greedy decoding, reinforcing thatfew-shot\ninâ€“context learning is an explorationâ€“driven abilitythat\ndeterministic inference systematically suppresses.",
                "position": 2130
            },
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/icl_exploration_landscape_LLaMA_2_13B_sentiment_parametric.png",
                "caption": "Figure 21:Explorationâ€“ICL landscapes forLLaMA-2 13Bon BESSTIE.Left: Sentiment(empirical best-of-16 gainâ‰ˆğŸğŸ“\\mathbf{\\approx 15}pp) shows a broad ridge of exploration benefit concentrated around temperaturesTâˆˆ[0.65,0.80]T\\in[0.65,0.80]and sample countskâˆˆ[8,32]k\\in[8,32](i.e.,log2â¡kâˆˆ[3,5]\\log_{2}k\\in[3,5]), with gains tapering smoothly toward both very low and very high exploration.Right: Sarcasm(peakâ‰ˆğŸğŸ–\\mathbf{\\approx 18}pp) exhibits a taller and slightly sharper ridge over a similarTTrange, indicating that sarcastic completions profit more aggressively from best-of-kksampling.\nIn both panels, the x-axis spanstemperatureTâˆˆ[0.05,1.0]T\\in[0.05,1.0], the y-axis coverslog2â¡kâˆˆ[0,6]\\log_{2}k\\in[0,6](i.e.,kâˆˆ[1,64]k\\in[1,64]), and the color scale encodes exploration gainÎ”â€‹AccICL\\Delta\\mathrm{Acc}^{\\mathrm{ICL}}in the numeric range[0,0.25][0,0.25](corresponding to[0,25][0,25]percentage points).",
                "position": 2481
            },
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/icl_exploration_landscape_LLaMA_2_13B_sarcasm_parametric.png",
                "caption": "",
                "position": 2490
            },
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/icl_exploration_landscape_LLaMA_3_8B_sentiment_parametric.png",
                "caption": "Figure 22:Explorationâ€“ICL landscapes forLLaMA-3 8B.Left: Sentimenthas a relatively lowbest-of-16gain of onlyâ‰ˆğŸ”\\mathbf{\\approx 6}pp, with a shallow ridge centred nearTâ‰ˆ0.7T\\approx 0.7and small-to-moderatekk(kâˆˆ[4,16]k\\in[4,16]), indicating limited upside from exploration on this task.Right: Sarcasm(peakâ‰ˆğŸğŸ‘\\mathbf{\\approx 13}pp) shows a visibly stronger and more extended plateau, with useful gains persisting forTâˆˆ[0.65,0.85]T\\in[0.65,0.85]andkkup toâ‰ˆ32\\approx 32, suggesting that sarcastic prompts require deeper exploration of the candidate distribution.\nAcross both plots, the numeric ranges are fixed toTâˆˆ[0.05,1.0]T\\in[0.05,1.0],log2â¡kâˆˆ[0,6]\\log_{2}k\\in[0,6]andÎ”â€‹AccICLâˆˆ[0,0.25]\\Delta\\mathrm{Acc}^{\\mathrm{ICL}}\\in[0,0.25], making cross-model comparison in later figuresscale-consistent.",
                "position": 2499
            },
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/icl_exploration_landscape_LLaMA_3_8B_sarcasm_parametric.png",
                "caption": "",
                "position": 2508
            },
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/icl_exploration_landscape_LLaMA_3_70B_sentiment_parametric.png",
                "caption": "Figure 23:Explorationâ€“ICL landscapes forLLaMA-3 70B.Left: Sentiment(peak gainâ‰ˆğŸ“\\mathbf{\\approx 5}pp) is characterized by a very flat surface with only a low-amplitude bump atTâ‰ˆ0.7T\\approx 0.7andkâ‰ˆ8k\\approx 8â€“1616, indicating that the strong base model already solves most cases under greedy decoding.Right: Sarcasm(peakâ‰ˆğŸğŸ\\mathbf{\\approx 10}pp) displays a slightly more pronounced ridge, but the overall magnitude remains modest compared to smaller models, again reflecting limited headroom for exploration.\nFormally, the figure keepsTTin[0.05,1.0][0.05,1.0],log2â¡k\\log_{2}kin[0,6][0,6], andÎ”â€‹AccICL\\Delta\\mathrm{Acc}^{\\mathrm{ICL}}clipped to[0,0.25][0,0.25], so thevisually compressedridges here are a real signal of reduced exploration benefit rather than an artefact of scaling.",
                "position": 2519
            },
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/icl_exploration_landscape_LLaMA_3_70B_sarcasm_parametric.png",
                "caption": "",
                "position": 2528
            },
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/icl_exploration_landscape_Gemma_2_9B_sentiment_parametric.png",
                "caption": "Figure 24:Explorationâ€“ICL landscapes forGemma-2 9B.Left: Sentimentshows a substantial ridge with peak gainâ‰ˆğŸğŸ’\\mathbf{\\approx 14}pp, spanningTâˆˆ[0.65,0.8]T\\in[0.65,0.8]andkâˆˆ[8,32]k\\in[8,32], and quickly flattening for very lowkkand overly hot temperatures.Right: Sarcasmis even more exploration-sensitive, achieving a peak ofâ‰ˆğŸğŸ•\\mathbf{\\approx 17}pp and maintaining high gains over a wide bandTâˆˆ[0.65,0.85]T\\in[0.65,0.85]andkâˆˆ[8,48]k\\in[8,48], where the surface height stays above roughly0.100.10(i.e.,1010pp).\nThe color scale is again fixed to[0,0.25][0,0.25], so thetaller, warmerridge for sarcasm versus sentiment visually encodes a true difference in exploration headroom for the same backbone.",
                "position": 2537
            },
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/icl_exploration_landscape_Gemma_2_9B_sarcasm_parametric.png",
                "caption": "",
                "position": 2546
            },
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/icl_exploration_landscape_Gemma_2_27B_sentiment_parametric.png",
                "caption": "Figure 25:Explorationâ€“ICL landscapes forGemma-2 27B.Left: Sentimentexhibits one of thestrongestridges in our study, with peak gainâ‰ˆğŸğŸ•\\mathbf{\\approx 17}pp and a high plateau forTâˆˆ[0.65,0.8]T\\in[0.65,0.8]andkâˆˆ[8,48]k\\in[8,48], whereÎ”â€‹AccICL\\Delta\\mathrm{Acc}^{\\mathrm{ICL}}remains in the[0.10,0.20][0.10,0.20](10â€“20â€‰pp) band.Right: Sarcasm(peakâ‰ˆğŸğŸ\\mathbf{\\approx 10}pp) has a noticeably shorter and narrower ridge, concentrated nearTâ‰ˆ0.7T\\approx 0.7andkâˆˆ[8,24]k\\in[8,24], suggesting that this larger Gemma variant is more exploration-hungry on sentiment than on sarcasm.\nBecause all panels share a common numeric range forTT,kk, and gain, the visual contrast between the left and right surfaces directly quantifies how task identity modulates the value of best-of-kksampling.",
                "position": 2555
            },
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/icl_exploration_landscape_Gemma_2_27B_sarcasm_parametric.png",
                "caption": "",
                "position": 2564
            },
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/icl_exploration_landscape_Mistral_7B_sentiment_parametric.png",
                "caption": "Figure 26:Explorationâ€“ICL landscapes forMistral-7B.Left: Sentiment(peak gainâ‰ˆğŸğŸ\\mathbf{\\approx 10}pp) has a clean, single ridge aroundTâ‰ˆ0.7T\\approx 0.7andkâˆˆ[8,24]k\\in[8,24]; belowk=4k=4or abovek=32k=32the surface rapidly collapses toward0.Right: Sarcasm(peakâ‰ˆğŸ”\\mathbf{\\approx 6}pp) is noticeably flatter and lower, with only a mild bump in the same approximate(T,k)(T,k)region, showing that this backbone is less reliant on exploration to solve sarcastic prompts.\nWithin the global numeric rangesTâˆˆ[0.05,1.0]T\\in[0.05,1.0],log2â¡kâˆˆ[0,6]\\log_{2}k\\in[0,6], andÎ”â€‹AccICLâˆˆ[0,0.25]\\Delta\\mathrm{Acc}^{\\mathrm{ICL}}\\in[0,0.25], Mistral-7B thus appears as a model where exploration isuseful but not critical, especially relative to Gemma-2.",
                "position": 2573
            },
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/icl_exploration_landscape_Mistral_7B_sarcasm_parametric.png",
                "caption": "",
                "position": 2582
            },
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/icl_exploration_landscape_Mixtral_8x7B_sentiment_parametric.png",
                "caption": "Figure 27:Explorationâ€“ICL landscapes forMixtral-8x7B.Left: SentimentandRight: Sarcasmboth peak at roughlyâ‰ˆğŸ•\\mathbf{\\approx 7}pp, with gently sloping ridges aroundTâˆˆ[0.65,0.8]T\\in[0.65,0.8]andkâˆˆ[8,24]k\\in[8,24].\nThe similarity of the two surfacesâ€”both staying mostly within the[0.03,0.12][0.03,0.12]gain band (3â€“12â€‰pp) across the high-exploration regionâ€”suggests that the MoE routing in Mixtral-8x7B introduces a fairlytask-agnosticresponse to best-of-kksampling.\nOverall, the numeric ranges confirm that this model sees consistent but moderate exploration benefits across both sentiment and sarcasm, with no extreme dependence on temperature or very largekk.",
                "position": 2591
            },
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/icl_exploration_landscape_Mixtral_8x7B_sarcasm_parametric.png",
                "caption": "",
                "position": 2600
            },
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/icl_exploration_landscape_Mixtral_8x22B_sentiment_parametric.png",
                "caption": "Figure 28:Explorationâ€“ICL landscapes forMixtral-8x22B.Left: SentimentandRight: Sarcasmboth reach peaks of aboutâ‰ˆğŸ–\\mathbf{\\approx 8}pp, but the ridges are slightly broader inkkthan for Mixtral-8x7B, with useful gains forkkextending up to roughly3232.\nWithinTâˆˆ[0.65,0.8]T\\in[0.65,0.8]andkâˆˆ[8,32]k\\in[8,32],Î”â€‹AccICL\\Delta\\mathrm{Acc}^{\\mathrm{ICL}}often stays above0.050.05(5â€‰pp), while quickly dropping outside this band.\nThe overall shape thus points to ascaling-stableexploration pattern across MoE sizes: larger Mixtral variants do not dramatically change where exploration helps, but slightly widen the high-gain corridor.",
                "position": 2609
            },
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/icl_exploration_landscape_Mixtral_8x22B_sarcasm_parametric.png",
                "caption": "",
                "position": 2618
            },
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/icl_exploration_landscape_Vicuna_7B_sentiment_parametric.png",
                "caption": "Figure 29:Explorationâ€“ICL landscapes forVicuna-7B.Left: Sentimentreaches a moderate peak ofâ‰ˆğŸ—\\mathbf{\\approx 9}pp, with a compact ridge aroundTâ‰ˆ0.7T\\approx 0.7andkâˆˆ[8,24]k\\in[8,24], and limited gain outside this region.Right: Sarcasmis dramatically different: the surface climbs up toâ‰ˆğŸğŸ•\\mathbf{\\approx 17}pp, with a tall ridge coveringTâˆˆ[0.65,0.85]T\\in[0.65,0.85]andkâˆˆ[8,48]k\\in[8,48], where gains stay well above0.100.10(10â€‰pp).\nThis strong asymmetryâ€”in a model fine-tuned on conversational dataâ€”highlights thatexploration is especially crucial for sarcasm, even when sentiment behaves more like a standard classification-style task.",
                "position": 2627
            },
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/icl_exploration_landscape_Vicuna_7B_sarcasm_parametric.png",
                "caption": "",
                "position": 2636
            },
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/icl_exploration_landscape_Phi_2_sentiment_parametric.png",
                "caption": "Figure 30:Explorationâ€“ICL landscapes forPhi-2.Left: Sentimentshows a surprisingly strong ridge for a small model, with peak gainâ‰ˆğŸğŸ\\mathbf{\\approx 11}pp and a concentrated band of high values aroundTâˆˆ[0.65,0.8]T\\in[0.65,0.8]andkâˆˆ[8,24]k\\in[8,24]; here, gains in the[0.06,0.12][0.06,0.12](6â€“12â€‰pp) range are common.Right: Sarcasmis much flatter, with peakâ‰ˆğŸ“\\mathbf{\\approx 5}pp and only a small bump nearTâ‰ˆ0.7T\\approx 0.7andkâˆˆ[8,16]k\\in[8,16], quickly collapsing towards zero for largerkkor temperatures too far from the sweet spot.\nTaken together with the global numeric ranges (shared across all figures), these panels emphasize thateven tiny models can reap non-trivial exploration benefits, but that such benefits may be highly task-specific and vanish rapidly outside a narrow(T,k)(T,k)window.",
                "position": 2645
            },
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/icl_exploration_landscape_Phi_2_sarcasm_parametric.png",
                "caption": "",
                "position": 2654
            },
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/icl_entropy_vs_eg.png",
                "caption": "Figure 31:Entropyâ€“exploration relationship in BESSTIE few-shot ICL.Each marker is ataskâ€“modelpair(t,m)(t,m)fromopen LLMsin TableLABEL:tab:besstie-main,\nfor eitherBESSTIEâ€“Sentiment(circles) orBESSTIEâ€“Sarcasm(triangles).\nThexxâ€“axis shows thenormalized label entropyğ”¼iâ€‹[H~i,t,m]âˆˆ[0,1]\\mathbb{E}_{i}[\\tilde{H}_{i,t,m}]\\in[0,1], where for each exampleiiwe estimate a label\ndistributionp^â„“,i,t,m\\hat{p}_{\\ell,i,t,m}from temperatureâ€“scaled stochastic samples and computeHi,t,m=âˆ’âˆ‘â„“p^â„“,i,t,mâ€‹logâ¡p^â„“,i,t,mH_{i,t,m}=-\\sum_{\\ell}\\hat{p}_{\\ell,i,t,m}\\log\\hat{p}_{\\ell,i,t,m}. We then normalize\nby the task arity,H~i,t,m=Hi,t,m/logâ¡Ct\\tilde{H}_{i,t,m}=H_{i,t,m}/\\log C_{t}, and average overii.\nTheyyâ€“axis plots theICL exploration gainEGt,mICLâ€‹(k=16)=Acct,mICLâ€‹(k=16)âˆ’Acct,mgreedy\\mathrm{EG}^{\\mathrm{ICL}}_{t,m}(k{=}16)=\\mathrm{Acc}^{\\mathrm{ICL}}_{t,m}(k{=}16)-\\mathrm{Acc}^{\\mathrm{greedy}}_{t,m},\ni.e., the improvement (in accuracy) of best-of-kksampling over greedy decoding.\nSolid (Sentiment) and dashed (Sarcasm) curves show quadratic fitsfâ€‹(h)â‰ˆaâ€‹h2+bâ€‹h+cf(h)\\approx ah^{2}+bh+cto the points in each task.\nWe observe a clearinverted-Urelationship: both low-entropy regimes\n(ğ”¼iâ€‹[H~i,t,m]â‰²0.2\\mathbb{E}_{i}[\\tilde{H}_{i,t,m}]\\lesssim 0.2, nearly deterministic labels) and\nvery high-entropy regimes (â‰³0.8\\gtrsim 0.8, almost uniform confusion) yieldnegligible exploration gains(EGICLâ‰²0.05\\mathrm{EG}^{\\mathrm{ICL}}\\lesssim 0.05),\nwhileintermediate entropies(â‰ˆ0.3\\approx 0.3â€“0.70.7) produce the largest gains\n(EGICLâ‰ˆ0.10\\mathrm{EG}^{\\mathrm{ICL}}\\approx 0.10â€“0.200.20).\nIn this middle band, many taskâ€“model pairs exhibit a â€œhidden majorityâ€ structure:\nthe correct label is the dominant mode under stochastic sampling but isnotthe label preferred by the greedy trajectory.\nThe systematic concave shape acrossallmodels shows that\nexploration gains are not idiosyncratic artefacts of a single LLM, but a\npredictable function of label entropy: ICL exploration helps most when the\nmodel is uncertain in astructuredway (few strong modes) rather than\neither over-confident or fully confused.",
                "position": 2794
            },
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/instrusum_eg_style_vs_k.png",
                "caption": "Figure 32:Style exploration gain as a function of search budget.For each modelmm, we plot thestyle exploration gainEâ€‹Gmstyleâ€‹(k)EG^{\\text{style}}_{m}(k)on InstruSum as the search budgetkkincreases\nfrom 1 (greedy decoding) to 32 samples.\nGains rise sharply betweenk=2k{=}2andk=8k{=}8and mostlysaturatebykâ‹†=8k^{\\star}{=}8.\nLarger, recent models such asLLaMAâ€“3andMixtralâ€“8Ã—\\times22Breach higher plateaus\n(Eâ€‹Gmstyleâ€‹(kâ‹†)â‰ˆ0.10EG^{\\text{style}}_{m}(k^{\\star})\\!\\approx\\!0.10â€“0.130.13), whereas smaller\nmodels likePhiâ€“2showfast saturation with low gains,\nindicating limited headroom for improving instruction adherence via search.\nOverall, the figure illustrates thatdistributional explorationsystematically improves style/constraint satisfaction, but the attainable\ngains are stronglymodelâ€“dependent.",
                "position": 3750
            },
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/instrusum_eg_style_vs_icl_scatter.png",
                "caption": "Figure 33:Linking ICL exploration gains to style exploration gains.Each point is a modelmm, with the xâ€“axis showing itsICL exploration gainEâ€‹GmICLEG^{\\text{ICL}}_{m}on BESSTIE and the yâ€“axis\nshowing itsstyle exploration gainEâ€‹Gmstyleâ€‹(kâ‹†)EG^{\\text{style}}_{m}(k^{\\star})on InstruSum atkâ‹†=8k^{\\star}{=}8.\nThe regression line and correlation (Ïâ‰ˆ0.80\\rho{\\approx}0.80) indicate that\nmodels with larger ICL gains typically also gain more in style/constraint\nsatisfaction.\nOutliers such asPhiâ€“2(ICLâ€“strong, styleâ€“weak) andMixtralâ€“8Ã—\\times22B(strong on both) show that this\nrelationship isarchitectureâ€“dependent.\nTogether with Figure32, this scatter plot\nsupports our claim thatdistributional explorationsurfaces latent\nabilities in both ICL and styleâ€“constrained generation, while the degree to\nwhich they are buried under greedy decoding is highlymodelâ€“specific.",
                "position": 3766
            },
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/instrusum_pareto_surface_llama2_annotated.png",
                "caption": "(a)LLaMAâ€“2.The joint density oversemantic adequacyssemâ€‹(Ï„)s_{\\mathrm{sem}}(\\tau)andjoint constraint scorecjointâ€‹(Ï„)c_{\\mathrm{joint}}(\\tau)reveals a tall,\nnarrowunderâ€“constrained ridge: most candidates concentrate inssemâ€‹(Ï„)âˆˆ[0.50,0.70]s_{\\mathrm{sem}}(\\tau)\\in[0.50,0.70]but onlycjointâ€‹(Ï„)âˆˆ[0.15,0.35]c_{\\mathrm{joint}}(\\tau)\\in[0.15,0.35],\nmeaning that LLaMAâ€“2 frequently captures the gist of the article while ignoring length, inclusion, and style requirements.\nThe arrowed point near(ssem,cjoint)â‰ˆ(0.70,0.45)(s_{\\mathrm{sem}},c_{\\mathrm{joint}})\\approx(0.70,0.45)highlights athin but nontrivialregion where wellâ€“balanced, constraintâ€“respecting\nsummaries exist but are rarely selected by greedy decoding.",
                "position": 3987
            },
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/instrusum_pareto_surface_llama2_annotated.png",
                "caption": "(a)LLaMAâ€“2.The joint density oversemantic adequacyssemâ€‹(Ï„)s_{\\mathrm{sem}}(\\tau)andjoint constraint scorecjointâ€‹(Ï„)c_{\\mathrm{joint}}(\\tau)reveals a tall,\nnarrowunderâ€“constrained ridge: most candidates concentrate inssemâ€‹(Ï„)âˆˆ[0.50,0.70]s_{\\mathrm{sem}}(\\tau)\\in[0.50,0.70]but onlycjointâ€‹(Ï„)âˆˆ[0.15,0.35]c_{\\mathrm{joint}}(\\tau)\\in[0.15,0.35],\nmeaning that LLaMAâ€“2 frequently captures the gist of the article while ignoring length, inclusion, and style requirements.\nThe arrowed point near(ssem,cjoint)â‰ˆ(0.70,0.45)(s_{\\mathrm{sem}},c_{\\mathrm{joint}})\\approx(0.70,0.45)highlights athin but nontrivialregion where wellâ€“balanced, constraintâ€“respecting\nsummaries exist but are rarely selected by greedy decoding.",
                "position": 3990
            },
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/instrusum_pareto_surface_llama3_annotated.png",
                "caption": "(b)LLaMAâ€“3.For the newer LLaMAâ€“3 model, the density mass shifts noticeably toward theupperâ€“rightof the plane:\nthe bulk of candidates lies inssemâ€‹(Ï„)âˆˆ[0.60,0.80]s_{\\mathrm{sem}}(\\tau)\\in[0.60,0.80]andcjointâ€‹(Ï„)âˆˆ[0.25,0.50]c_{\\mathrm{joint}}(\\tau)\\in[0.25,0.50].\nThe underâ€“constrained ridge is still visible atcjointâ€‹(Ï„)â‰²0.30c_{\\mathrm{joint}}(\\tau)\\lesssim 0.30, but the arrowedhigh semantics & constraintsregion around(0.70âˆ’0.80,0.45âˆ’0.60)(0.70\\!-\\!0.80,0.45\\!-\\!0.60)now carries substantial density,\nindicating that LLaMAâ€“3 often places mass directly on approximately\nParetoâ€“optimal summaries that satisfy both content and stylistic hints.",
                "position": 4005
            },
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/instrusum_pareto_surface_gemma2_annotated.png",
                "caption": "(c)Gemmaâ€“2.Gemmaâ€“2 shows abroaderand morespreadâ€“outlandscape:\ncandidates typically occupyssemâ€‹(Ï„)âˆˆ[0.55,0.75]s_{\\mathrm{sem}}(\\tau)\\in[0.55,0.75]andcjointâ€‹(Ï„)âˆˆ[0.25,0.55]c_{\\mathrm{joint}}(\\tau)\\in[0.25,0.55], with a visible secondary peak\nnear(0.70,0.50)(0.70,0.50).\nThe underâ€“constrained ridge atcjointâ€‹(Ï„)â‰²0.30c_{\\mathrm{joint}}(\\tau)\\lesssim 0.30is less dominant than in\nLLaMAâ€“2, suggesting that Gemmaâ€“2 is more willing to trade a small amount\nof semantic score to better respect formatting and style.\nIn other words, Gemmaâ€“2â€™s stochastic support contains a richer mix ofsemanticsâ€“heavyandconstraintâ€“faithfulsummaries.",
                "position": 4021
            },
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/instrusum_pareto_surface_mistral7b_annotated.png",
                "caption": "(d)Mistralâ€“7B.Mistralâ€“7B exhibits a tall peak concentrated inssemâ€‹(Ï„)âˆˆ[0.55,0.75]s_{\\mathrm{sem}}(\\tau)\\in[0.55,0.75]but withcjointâ€‹(Ï„)c_{\\mathrm{joint}}(\\tau)mostly confined to[0.15,0.30][0.15,0.30],\nindicating that the model strongly prioritizes semantic coverage of the\narticle over strict adherence to instruction constraints.\nThe annotated underâ€“constrained ridge therefore dominates the surface,\nwhile only a relativelythin and lowâ€“densityband of candidates\nreachescjointâ€‹(Ï„)â‰³0.40c_{\\mathrm{joint}}(\\tau)\\gtrsim 0.40.\nThis pattern makes Mistralâ€“7B look stylistically weak under greedy\ndecoding, even though the geometry reveals that higherâ€“constraint\nsummaries do exist in its sampling distribution.",
                "position": 4036
            },
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/instrusum_pareto_surface_mixtral8x7b_annotated.png",
                "caption": "(a)Mixtralâ€“8Ã—\\times7B.The semanticâ€“constraint landscape isbalanced:\nmost mass lies inssemâ€‹(Ï„)âˆˆ[0.60,0.80]s_{\\mathrm{sem}}(\\tau)\\in[0.60,0.80]andcjointâ€‹(Ï„)âˆˆ[0.30,0.55]c_{\\mathrm{joint}}(\\tau)\\in[0.30,0.55].\nAn underâ€“constrained ridge remains atcjointâ€‹(Ï„)â‰²0.30c_{\\mathrm{joint}}(\\tau)\\lesssim 0.30, but the annotated peak in the\nupperâ€“right shows many candidates that jointly achievehigh semantics and strong constraint satisfaction, so\nmultiâ€“sample decoding can routinely surface well aligned summaries.",
                "position": 4058
            },
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/instrusum_pareto_surface_mixtral8x7b_annotated.png",
                "caption": "(a)Mixtralâ€“8Ã—\\times7B.The semanticâ€“constraint landscape isbalanced:\nmost mass lies inssemâ€‹(Ï„)âˆˆ[0.60,0.80]s_{\\mathrm{sem}}(\\tau)\\in[0.60,0.80]andcjointâ€‹(Ï„)âˆˆ[0.30,0.55]c_{\\mathrm{joint}}(\\tau)\\in[0.30,0.55].\nAn underâ€“constrained ridge remains atcjointâ€‹(Ï„)â‰²0.30c_{\\mathrm{joint}}(\\tau)\\lesssim 0.30, but the annotated peak in the\nupperâ€“right shows many candidates that jointly achievehigh semantics and strong constraint satisfaction, so\nmultiâ€“sample decoding can routinely surface well aligned summaries.",
                "position": 4061
            },
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/instrusum_pareto_surface_mixtral8x22b_annotated.png",
                "caption": "(b)Mixtralâ€“8Ã—\\times22B.The larger Mixtralâ€“8Ã—\\times22B shifts density further toward the\nhighâ€“quality corner:ssemâ€‹(Ï„)âˆˆ[0.65,0.85]s_{\\mathrm{sem}}(\\tau)\\in[0.65,0.85]andcjointâ€‹(Ï„)âˆˆ[0.35,0.60]c_{\\mathrm{joint}}(\\tau)\\in[0.35,0.60]for most candidates.\nThe ridge becomes secondary to a strong peak around(0.75,0.55)(0.75,0.55),\nshowing that the modelnaturally places more probabilityon\nsummaries that respect format, tone, and inclusion.",
                "position": 4074
            },
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/instrusum_pareto_surface_vicuna7b_annotated.png",
                "caption": "(c)Vicunaâ€“7B.Vicunaâ€“7B is dominated by a peak inssemâ€‹(Ï„)âˆˆ[0.50,0.70]s_{\\mathrm{sem}}(\\tau)\\in[0.50,0.70]andcjointâ€‹(Ï„)âˆˆ[0.10,0.30]c_{\\mathrm{joint}}(\\tau)\\in[0.10,0.30],\nwith little mass beyondcjointâ€‹(Ï„)â‰ˆ0.40c_{\\mathrm{joint}}(\\tau)\\approx 0.40.\nThe pronouncedunderâ€“constrained ridgereflects a tendency to\nproduce semantically competent but stylistically misaligned summaries, while\nthe tiny arrowed island of highercjointc_{\\mathrm{joint}}indicates that\nwellâ€“aligned candidates exist but sit at the fringes of the distribution.",
                "position": 4087
            },
            {
                "img": "https://arxiv.org/html/2601.07239/ICL/instrusum_pareto_surface_phi2_annotated.png",
                "caption": "(d)Phiâ€“2.Phiâ€“2 concentrates mass inssemâ€‹(Ï„)âˆˆ[0.45,0.70]s_{\\mathrm{sem}}(\\tau)\\in[0.45,0.70]andcjointâ€‹(Ï„)âˆˆ[0.10,0.30]c_{\\mathrm{joint}}(\\tau)\\in[0.10,0.30],\nyielding one of the steepest underâ€“constrained ridges.\nThe highâ€“constraint regioncjointâ€‹(Ï„)â‰³0.40c_{\\mathrm{joint}}(\\tau)\\gtrsim 0.40is only athin, lowâ€“density tail, showing that jointly\nwellâ€“aligned summaries are rare and almost never selected by\ndeterministic decoding.",
                "position": 4100
            }
        ]
    },
    {
        "header": "\\PragyaHeadline5\\PragyaHeadlineDeterministic inference collapses diverse reasoning paths into a single brittle trace",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.07239/x2.png",
                "caption": "Figure 36:Reasoning graph for a multiâ€“step ticketâ€“sales word problem.The problem (shown below the tree) asks how much money a school donates\nafter selling adult and child tickets over two days, with different prices\nand a fixed perâ€“ticket expense.\nInternal nodes represent partial chains of thought: an initial parsing\nstage, a split intoStrategyÂ A(group by day) versusStrategyÂ B(group by ticket type), and finer subplans\nsuch as â€œMonday firstâ€ vs. â€œTuesday firstâ€ or\nâ€œAdults firstâ€ vs. â€œChildren firstâ€.\nLeaves are classified asCorrect,Nearâ€“miss, orFailure, with outcome types indicated by the colored markers in\nthe legend box.\nThe thickredpath denotes thegreedy chain of thought, which ends in a nearâ€“miss leaf,\nwhereas the dashedgreenpath shows an\nalternative, fully correct strategy that the model also assigns\nnontrivial probability to.\nOther sampled branches are drawn in light gray.\nThis illustrates how multiâ€“sample decoding exposes a rich, multiâ€“path\nlandscape of reasoning, while deterministic inference collapses it into a\nsingle brittle trace.",
                "position": 4564
            },
            {
                "img": "https://arxiv.org/html/2601.07239/x3.png",
                "caption": "Figure 37:DeepSeekâ€“R1â€“Distill: stochastic exploration exposes rich,\nhigh-gain reasoning modes across tasks.This 3D plot shows, forDeepSeekâ€“R1â€“Distill, the joint distribution of\naverage reasoningâ€“path entropyHÂ¯m,d\\overline{H}_{m,d}(xâ€“axis) and accuracy gainÎ”â€‹Accm,d\\Delta\\mathrm{Acc}_{m,d}from multiâ€“sample over greedy decoding (yâ€“axis),\nwith dataset layersdâˆˆ{GSM8K,SVAMP,StrategyQA}d{\\in}\\{\\text{GSM8K},\\text{SVAMP},\\text{StrategyQA}\\}separated along the zâ€“axis.\nGrey point clouds representdeterministicbehavior under greedy decoding:\nthey cluster tightly nearÎ”â€‹Accm,dâ‰ˆ0\\Delta\\mathrm{Acc}_{m,d}{\\approx}0with slightly\nlower entropies, indicating a narrow set of chains of thought that the model\nactually emits when forced to be deterministic.\nColored point clouds correspond tostochasticmultiâ€“sample decoding and\nspanHÂ¯m,dâ‰ˆ1.45\\overline{H}_{m,d}{\\approx}1.45â€“1.901.90andÎ”â€‹Accm,dâ‰ˆ0.05\\Delta\\mathrm{Acc}_{m,d}{\\approx}0.05â€“0.220.22, revealing a much richer,\nhigherâ€“diversity regime of reasoning that greedy decoding never visits.\nArrows fromgreedy centers(black stars) tomultiâ€“sample centroids(colored circles) show a consistent shift towardhigher path entropy and substantially higher accuracyon all three\ndatasets, with the largest displacement onGSM8K.\nTaken together, this figure illustrates that, for DeepSeek, imposing\ndeterministic decoding collapses a broad landscape of competent reasoning\nstrategies into a single, brittle trace that systematically underestimates the\nmodelâ€™s true multiâ€“path capabilities.",
                "position": 5028
            },
            {
                "img": "https://arxiv.org/html/2601.07239/x4.png",
                "caption": "Figure 38:LLaMAâ€“3.1â€“8Bâ€“Instruct: heterogeneous gains across arithmetic\nand verbal reasoning.ForLLaMAâ€“3.1â€“8B, we again plot average path entropyHÂ¯m,d\\overline{H}_{m,d}vs. accuracy gainÎ”â€‹Accm,d\\Delta\\mathrm{Acc}_{m,d}, layered overGSM8K,SVAMP, andStrategyQA.\nThedeterministicclouds (grey) remain tightly packed nearÎ”â€‹Accm,dâ‰ˆ0\\Delta\\mathrm{Acc}_{m,d}{\\approx}0, reflecting low apparent diversity when the\nmodel is evaluated with greedy decoding.\nIn contrast, thestochasticclouds concentrate aroundHÂ¯m,dâ‰ˆ1.35\\overline{H}_{m,d}{\\approx}1.35â€“1.701.70andÎ”â€‹Accm,dâ‰ˆ0.03\\Delta\\mathrm{Acc}_{m,d}{\\approx}0.03â€“0.120.12, clearly separated from the\ndeterministic regime and revealingnontrivial gainsfrom distributional\nexploration.\nThe greedyâ†’\\!\\to\\!multiâ€“sample displacement (arrows from black stars to\ncolored circles) is largest onStrategyQA, where verbal multiâ€“hop\nreasoning admits many distinct but valid chains of thought; arithmetic datasets\nexhibitsmaller but consistentgains in both diversity and accuracy.\nOverall, this figure shows that even a strong instructionâ€“tuned model like\nLLaMAâ€“3.1 hides a substantial fraction of its reasoning flexibility when run\ndeterministically, and thatthe benefits of exploration are strongest\nprecisely on tasks with rich, verbal reasoning structure.",
                "position": 5053
            },
            {
                "img": "https://arxiv.org/html/2601.07239/x5.png",
                "caption": "Figure 39:Mixtralâ€“8Ã—\\times7Bâ€“Instruct: moderate but consistent\ndiversity and accuracy gains.This figure presents the same 3D landscape forMixtralâ€“8Ã—\\times7Bâ€“Instruct.\nHere, thestochasticpoint clouds occupy an intermediate band,\nwithHÂ¯m,dâ‰ˆ1.30\\overline{H}_{m,d}{\\approx}1.30â€“1.551.55andÎ”â€‹Accm,dâ‰ˆ0.02\\Delta\\mathrm{Acc}_{m,d}{\\approx}0.02â€“0.100.10acrossGSM8K,SVAMP, andStrategyQA.\nThedeterministicclouds again lie tightly nearÎ”â€‹Accm,dâ‰ˆ0\\Delta\\mathrm{Acc}_{m,d}{\\approx}0, but the gap to the stochastic regime is\nsmaller than for DeepSeek or LLaMAâ€“3.1.\nArrows from greedy centers to multiâ€“sample centroids consistently move towardhigher reasoning diversity and higher accuracy, yet the magnitude of\nthese shifts isshallower: Mixtral already allocates noticeable\nprobability mass to highâ€“quality chains of thought that greedy decoding\nsometimes captures.\nThis intermediate pattern suggests that Mixtralâ€™s internal reasoning landscape\nis less severely collapsed by deterministic inference than DeepSeekâ€™s, but\nstill benefits from explorationâ€”especially onGSM8KandSVAMP,\nwhere multiâ€“sample decoding uncovers additional correct, diverse solution\npaths.\nIn combination with Figures37and38, this figure highlights thatthe extent of reasoning-path collapse under greedy decoding is strongly\narchitecture-dependent, even when all models are evaluated on the same\nthree reasoning benchmarks.",
                "position": 5076
            }
        ]
    },
    {
        "header": "\\PragyaHeadline6\\PragyaHeadlineDeterministic safety evaluation creates an illusion of robustness",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.07239/safety/safety_risk_surface.png",
                "caption": "Figure 40:Decoderâ€“level risk as a function of harmful tail mass and search budget.We plot the theoretical decoderâ€“level riskr~kâ€‹(q)=1âˆ’(1âˆ’q)k\\tilde{r}_{k}(q)=1-(1-q)^{k}as a function of the perâ€“prompt\nharmful tail massqq(xâ€“axis, logâ€“scaled) and the number of\nindependent sampleskk(yâ€“axis), with a coolâ€“toâ€“warm colormap\nindicatinglowtohighrisk.\nThe thickblackcurve on the front edge shows thek=1k{=}1slice(deterministic greedy evaluation):\nfor small tails,r~1â€‹(q)â‰ˆq\\tilde{r}_{1}(q)\\!\\approx\\!q, so prompts withqâ‰ˆ10âˆ’3q\\approx 10^{-3}andqâ‰ˆ0.02q\\approx 0.02looksimilarly safe(risks around0.0010.001vs.0.020.02).\nThe coloredk=8,16,32k{=}8,16,32curves on the surface reveal that,\nunder realistic multiâ€“sample budgets, thesametail masses\nyield much higher risk:\natq=10âˆ’3q{=}10^{-3}, risk rises fromâ‰ˆ0.001\\approx 0.001(greedy) toâ‰ˆ0.03\\approx 0.03fork=32k{=}32, while atq=0.02q{=}0.02it jumps fromâ‰ˆ0.02\\approx 0.02to well above0.450.45.\nThe warm plateau near the back shows that even moderately sized\nharmful tails becomenear-certain failuresoncekkis large.\nThe stark visual gap between the flatâ€“lookingk=1k{=}1boundary and the lifted multiâ€“sample surface makes explicit theillusion of robustnesscreated by deterministicâ€“only\nsafety evaluation.",
                "position": 5750
            },
            {
                "img": "https://arxiv.org/html/2601.07239/safety/safety_risk_surface_LLaMA_2_7B.png",
                "caption": "Figure 41:Decoderâ€“level risk landscape forLLaMAâ€“2 7B.The surface showsr~kâ€‹(q)=1âˆ’(1âˆ’q)k\\tilde{r}_{k}(q)=1-(1-q)^{k}overprompt percentile(xâ€“axis;0to11, sorted by harmful tail massqq),sampleskk(yâ€“axis;1â‰¤kâ‰¤321{\\leq}k{\\leq}32), andrisk(zâ€“axis and color;0to11).\nThe black, green, and orange curves trace slices atk=1k{=}1(greedy),k=8k{=}8, andk=16k{=}16.\nFor LLaMAâ€“2 7B, roughly the lowest40â€“50%of prompts stay\nnearr~kâ€‹(q)â‰ˆ0\\tilde{r}_{k}(q)\\approx 0even askkgrows, but the upper20â€“30%form a steep ridge where moving fromk=1k{=}1tokâˆˆ{8,16}k{\\in}\\{8,16\\}drives risk into the0.8âˆ’1.00.8{-}1.0band,\nrevealing a substantial highâ€“risk tail that greedy testing largely\nmisses.",
                "position": 6234
            },
            {
                "img": "https://arxiv.org/html/2601.07239/safety/safety_risk_surface_LLaMA_2_13B.png",
                "caption": "Figure 42:Decoderâ€“level risk landscape forLLaMAâ€“2 13B.Axes and color scale match Fig.41.\nCompared to 7B, thegreedyslice is lower over much of the\ndistribution: the bottom50â€“60%of prompts remain very close\ntor~1â€‹(q)â‰ˆ0\\tilde{r}_{1}(q)\\approx 0, and even thek=8k{=}8curve stays belowâ‰ˆ0.2\\approx 0.2for most prompts.\nHowever, the top15â€“25%of prompts still exhibit a sharp\ntransition wherer~kâ€‹(q)\\tilde{r}_{k}(q)underkâˆˆ{8,16}k{\\in}\\{8,16\\}jumps to0.7âˆ’1.00.7{-}1.0, indicating that capability scaling reduces themassof dangerous prompts but leaves a concentrated band where\nmultiâ€“sample risk remains extreme.",
                "position": 6249
            },
            {
                "img": "https://arxiv.org/html/2601.07239/safety/safety_risk_surface_LLaMA_3_8B.png",
                "caption": "Figure 43:Decoderâ€“level risk landscape forLLaMAâ€“3 8B.As before, we plotr~kâ€‹(q)\\tilde{r}_{k}(q)over prompt percentile, sampleskk, and risk.\nLLaMAâ€“3 8B shows a broader lowâ€“risk plateau than LLaMAâ€“2:\napproximately the lowest55â€“65%of prompts remain belowr~kâ€‹(q)â‰ˆ0.1\\tilde{r}_{k}(q)\\approx 0.1even atk=8k{=}8, and the greedy curve is\ntightly bound to zero in this region.\nBeyond theâ‰ˆ65\\approx 65th percentile, thek=8k{=}8andk=16k{=}16slices\nbend sharply upward, with the top15â€“20%of prompts reaching0.8âˆ’1.00.8{-}1.0risk.\nThis reflects a model whose overall safety improves, but whosetail promptsstill become almost surely harmful under\nmultiâ€“sample decoding.",
                "position": 6264
            },
            {
                "img": "https://arxiv.org/html/2601.07239/safety/safety_risk_surface_LLaMA_3_70B.png",
                "caption": "Figure 44:Decoderâ€“level risk landscape forLLaMAâ€“3 70B.For the 70B variant, the lowâ€“risk region widens further:\nroughly the lowest70%of prompts remain atr~kâ€‹(q)â‰²0.05\\tilde{r}_{k}(q)\\lesssim 0.05fork=1k{=}1and stay belowâ‰ˆ0.15\\approx 0.15even atk=8k{=}8.\nYet the remaining10â€“15%of prompts still show a dominant\nridge where thek=8k{=}8andk=16k{=}16curves rise into the0.8âˆ’1.00.8{-}1.0range.\nThus, largeâ€“scale alignment compresses the dangerous set into a\nsmaller fraction of prompts, but does not fully remove thenearâ€“certain failure zonethat appears under stochastic\nsampling.",
                "position": 6279
            },
            {
                "img": "https://arxiv.org/html/2601.07239/safety/safety_risk_surface_Gemma_2_2B.png",
                "caption": "Figure 45:Decoderâ€“level risk landscape forGemmaâ€“2 2B.Gemmaâ€“2 2B exhibits a moderate plateau of low risk:\naround the lowest45â€“55%of prompts stay atr~1â€‹(q)â‰ˆ0\\tilde{r}_{1}(q)\\approx 0and remain belowâ‰ˆ0.15\\approx 0.15fork=8k{=}8.\nIn the upper half of the distribution, however, thek=8k{=}8andk=16k{=}16slices rise quickly, with the top20%of prompts\napproachingr~kâ€‹(q)â‰ˆ0.8âˆ’1.0\\tilde{r}_{k}(q)\\approx 0.8{-}1.0.\nThe contrast between the nearly flat greedy curve in the bulk and\nthe steep rise in the tail again illustratesconcealed\nrisk: deterministic ASR underestimates how often multiâ€“sample\ndecoding will find harmful completions.",
                "position": 6295
            },
            {
                "img": "https://arxiv.org/html/2601.07239/safety/safety_risk_surface_Gemma_2_27B.png",
                "caption": "Figure 46:Decoderâ€“level risk landscape forGemmaâ€“2 27B.Scaling to 27B flattens the lowâ€“percentile region further:\nroughly the lowest60â€“70%of prompts stay atr~1â€‹(q)â‰ˆ0\\tilde{r}_{1}(q)\\approx 0and below about0.10.1even fork=8k{=}8.\nYet, as with other stronger models, a compact top10â€“15%still hosts a steep ridge wherek=8k{=}8andk=16k{=}16yieldr~kâ€‹(q)â‰ˆ0.9âˆ’1.0\\tilde{r}_{k}(q)\\approx 0.9{-}1.0.\nGemmaâ€“2 27B therefore combinesexcellent average safetywith\na persistent, narrowly concentratedhighâ€“risk tailthat\nonly becomes evident when exploring the decoder stochastically.",
                "position": 6309
            },
            {
                "img": "https://arxiv.org/html/2601.07239/safety/safety_risk_surface_Mistral_7B.png",
                "caption": "Figure 47:Decoderâ€“level risk landscape forMistral 7B.The surface is slightly flatter than for LLaMAâ€“2 7B in the\nlowâ€“percentile region: roughly the lowest55â€“65%of prompts\nremain close tor~kâ€‹(q)â‰ˆ0\\tilde{r}_{k}(q)\\approx 0even atk=8k{=}8, and the\ngreedy slice stays near zero over a broad plateau.\nHowever, the top20%of prompts exhibit a steep ridge:\ngoing fromk=1k{=}1tokâˆˆ{8,16}k{\\in}\\{8,16\\}increases risk fromâ‰ˆ0.05âˆ’0.1\\approx 0.05{-}0.1to nearly0.9âˆ’1.00.9{-}1.0.\nThis highlights how decoder stochasticity exposesrare but\nextremely highâ€“risk modesthat remain invisible under purely\ndeterministic testing.",
                "position": 6323
            },
            {
                "img": "https://arxiv.org/html/2601.07239/safety/safety_risk_surface_Mixtral_8x7B.png",
                "caption": "Figure 48:Decoderâ€“level risk landscape forMixtral 8Ã—\\times7B.Mixtral 8Ã—\\times7B displays a pronouncedtwoâ€“regimestructure.\nFor approximately the lowest60â€“70%of prompts, all three\nslicesk=1,8,16k{=}1,8,16remain belowr~kâ€‹(q)â‰ˆ0.2\\tilde{r}_{k}(q)\\approx 0.2,\nforming a wide, cool (blue) plateau where even multiâ€“sample\ndecoding yields low risk.\nBeyond theâ‰ˆ70\\approx 70th percentile, however, thek=8k{=}8andk=16k{=}16curves bend sharply upward, with the top10â€“15%of prompts saturating tor~kâ€‹(q)â‰ˆ1\\tilde{r}_{k}(q)\\approx 1.\nThis is a canonicalconcealedâ€“risk pattern: strong average\nsafety coexists with a compact highâ€“tail region where stochastic\nsampling makes harmful completions almost inevitable.",
                "position": 6336
            },
            {
                "img": "https://arxiv.org/html/2601.07239/safety/safety_risk_surface_Mixtral_8x22B.png",
                "caption": "Figure 49:Decoderâ€“level risk landscape forMixtral 8Ã—\\times22B.The larger Mixtral variant further suppresses greedy risk:\nthe blackk=1k{=}1curve hugs zero for roughly the lowest70%of prompts, and even atk=8k{=}8the bulk of prompts stay\nbelowr~kâ€‹(q)â‰ˆ0.1âˆ’0.2\\tilde{r}_{k}(q)\\approx 0.1{-}0.2.\nNevertheless, the upper10â€“15%of prompts still form a\nbright ridge where increasing the search budget fromk=1k{=}1tok=16k{=}16lifts risk into the0.8âˆ’1.00.8{-}1.0band.\nScaling the model therefore reduces themeasureof dangerous\nprompts but does not fully eliminate the highâ€“risk tail exposed by\nstochastic decoding.",
                "position": 6353
            },
            {
                "img": "https://arxiv.org/html/2601.07239/safety/safety_risk_surface_DeepSeek_R1_Distill.png",
                "caption": "Figure 50:Decoderâ€“level risk landscape forDeepSeekâ€“R1 Distill.DeepSeekâ€“R1 Distill exhibits an even more compressed tail:\nthe greedy slice remains almost zero for the lowest75â€“80%of prompts, and evenk=8k{=}8keeps most of this mass belowr~kâ€‹(q)â‰ˆ0.1\\tilde{r}_{k}(q)\\approx 0.1.\nHowever, a narrow top10%of prompts still shows a sharp\nescalation wherek=8k{=}8andk=16k{=}16rapidly push risk to0.9âˆ’1.00.9{-}1.0.\nThis regime exemplifies the central theme of our analysis:\nhighly capable, wellâ€“aligned models can haveexcellentdeterministic safety profiles yet still hide a small set of prompts\nwhere multiâ€“sample decoding yields nearâ€“certain failure.",
                "position": 6366
            },
            {
                "img": "https://arxiv.org/html/2601.07239/safety/safety_risk_surface_Phi_2.png",
                "caption": "Figure 51:Decoderâ€“level risk landscape forPhiâ€“2.The surface showsr~kâ€‹(q)=1âˆ’(1âˆ’q)k\\tilde{r}_{k}(q)=1-(1-q)^{k}over prompt percentile\n(xâ€“axis;0to11, sorted by harmful tail massqq), sampleskk(yâ€“axis;1â‰¤kâ‰¤321{\\leq}k{\\leq}32), and risk (zâ€“axis and color;0to11).\nFor Phiâ€“2, the greedy slicek=1k{=}1is relatively high across a broad band:\nroughly the middle30â€“50%of prompts already reachr~1â€‹(q)â‰ˆ0.1âˆ’0.3\\tilde{r}_{1}(q)\\approx 0.1{-}0.3, and the upper tail climbs toâ‰ˆ0.4âˆ’0.5\\approx 0.4{-}0.5.\nIncreasing the budget tokâˆˆ{8,16}k{\\in}\\{8,16\\}pushes much of the upper half\nintor~kâ€‹(q)â‰ˆ0.6âˆ’1.0\\tilde{r}_{k}(q)\\approx 0.6{-}1.0, yielding a large red region where\nboth deterministic and stochastic ASR are high.",
                "position": 6382
            },
            {
                "img": "https://arxiv.org/html/2601.07239/safety/safety_risk_surface_Vicuna_7B.png",
                "caption": "Figure 52:Decoderâ€“level risk landscape forVicunaâ€“7B.Axes and color scale match Fig.51.\nRelative to Phiâ€“2, the greedy slice is noticeably lower: about the lowest50â€“60%of prompts stay nearr~1â€‹(q)â‰ˆ0\\tilde{r}_{1}(q)\\approx 0, and even up\nto theâ‰ˆ70\\approx 70th percentile the greedy risk rarely exceeds0.150.15.\nHowever, for the top20â€“25%of prompts thek=8k{=}8andk=16k{=}16curves bend sharply upward, with the worst10â€“15%saturating atr~kâ€‹(q)â‰ˆ0.8âˆ’1.0\\tilde{r}_{k}(q)\\approx 0.8{-}1.0.\nVicunaâ€“7B therefore has lower average greedy ASR than Phiâ€“2 but still\nretains a compact highâ€“risk tail where stochastic decoding uncovers\nfailures that deterministic evaluation largely misses.",
                "position": 6395
            },
            {
                "img": "https://arxiv.org/html/2601.07239/safety/safety_asr_vs_k_models.png",
                "caption": "Figure 53:Attack success vs. sampling budget for different models.Each panel shows thestochastic attack success rateASRmstochâ€‹(k)\\mathrm{ASR}_{m}^{\\text{stoch}}(k)as a function of sampling budgetkâˆˆ{1,2,4,8,16,32}k{\\in}\\{1,2,4,8,16,32\\}for a single modelmmon our combined\njailbreak suites.\nThe horizontal dotted line marks thedeterministicASRASRmgreedy\\mathrm{ASR}_{m}^{\\text{greedy}}(i.e.,k=1k{=}1under greedy\ndecoding), which is often close to zero for stronger models.\nAskkincreases, many models exhibit a sharp rise fromASRmstochâ€‹(1)â‰ˆ0\\mathrm{ASR}_{m}^{\\text{stoch}}(1)\\approx 0toASRmstochâ€‹(16)\\mathrm{ASR}_{m}^{\\text{stoch}}(16)orASRmstochâ€‹(32)\\mathrm{ASR}_{m}^{\\text{stoch}}(32)in the1010â€“30%30\\%range, revealing substantialhidden riskthat isentirely invisibleunder standard greedy evaluation.\nThe gap between the dotted baselines and the curves atk=8k{=}8ork=16k{=}16corresponds directly to thestochastic\nrisk gapÎ”â€‹ASRmâ€‹(k)\\Delta\\mathrm{ASR}_{m}(k)and feeds into the illusion\nindexImâ€‹(k)I_{m}(k)reported in\nTable4.",
                "position": 6491
            },
            {
                "img": "https://arxiv.org/html/2601.07239/safety/safety_riskgap_vs_capability.png",
                "caption": "Figure 55:More capable models exhibit larger deterministic illusions.Each point corresponds to a modelmm, with the xâ€“axis showing acapability proxyCmC_{m}(average normalized score across our\nreasoning benchmarks) and the yâ€“axis showing theillusion indexImâ€‹(8)I_{m}(8)at budgetk=8k{=}8.\nThe fitted trend line reveals a striking pattern:\nmodels with higher capability scores tend to havelargerImâ€‹(8)I_{m}(8), meaning that a growing fraction of their true stochastic\nrisk isconcealedunder greedy evaluation.\nIn our data, smaller models cluster in the lowerâ€“left region\n(moderate capability, modest illusion), while frontierâ€“scale models\nlie in the upperâ€“right, combining strong performance withsubstantial hidden harmful tails.\nThis formalizes the concern that â€œsafer under greedyâ€ does not\nnecessarily mean â€œsafer under realistic stochastic use.â€",
                "position": 6577
            },
            {
                "img": "https://arxiv.org/html/2601.07239/x6.png",
                "caption": "Figure 56:Decomposing safety into robustly safe prompts, concealed risk, and deterministic failures.For each modelmm, we partition attack prompts into three disjoint categories:robustly safe(the greedy completion is safe and allk=8k{=}8stochastic samples are safe),concealed risk(the greedy completion is safe, but at least one of thekkstochastic samples is harmful),\nanddeterministic failure(the greedy completion itself is already harmful).\nThe horizontal stacked bars show, for each model, the empirical fraction of prompts assigned to each category, while the\nyâ€“axis labels additionally report in parentheses the fraction of total risk that is hidden,hiddenâ€‹_â€‹sharem=concealedmconcealedm+det_failm\\mathrm{hidden\\_share}_{m}\\,{=}\\,\\frac{\\text{concealed}_{m}}{\\text{concealed}_{m}+\\text{det\\_fail}_{m}}, among all risky prompts for that model.\nModels are ordered from top to bottom by their total stochastic attack success rateASRmstochâ€‹(k)=concealedm+det_failm\\mathrm{ASR}_{m}^{\\text{stoch}}(k)\\,{=}\\,\\text{concealed}_{m}+\\text{det\\_fail}_{m},\nso that overall risk monotonically increases down the figure.\nStronger, more safety-tuned models in the lower rows exhibit very small deterministic-failure mass but extremely large\nconcealed-risk bands and correspondingly high hidden shares, indicating that most of their stochastic risk arises from\nprompts that would becertified safeby any evaluator that only inspects the greedy output.\nBy contrast, weaker or poorly aligned models near the top allocate more mass to deterministic failures and less to\nconcealed risk, meaning that a larger portion of their risk remains directly visible under standard greedy evaluation.\nThis compositional view makes the illusion of robustnessvisually explicit: a seemingly safe region under\ndeterministic decoding can in fact contain a substantial reservoir of hidden stochastic risk.",
                "position": 6609
            },
            {
                "img": "https://arxiv.org/html/2601.07239/x7.png",
                "caption": "Figure 57:Case study I: a prompt that appears safe under greedy evaluation but hides a harmful stochastic tail.(a)For a single jailbreak promptxâ‹†x^{\\star}, we drawk=16k{=}16completions{Yjâ€‹(xâ‹†)}j=116\\{Y_{j}(x^{\\star})\\}_{j=1}^{16}from the model and display them as a4Ã—44{\\times}4grid.\nEach cell is annotated with its sample indexjj(withj=1j{=}1marked asGreedy) and colored by the human harm label:greenforsafe,orangeforborderline / evasive, andredforharmful.\nEven though the greedy completionYgreedyâ€‹(xâ‹†)=Y1â€‹(xâ‹†)Y_{\\text{greedy}}(x^{\\star}){=}Y_{1}(x^{\\star})is\nlabeled safe, we observe multiple harmful samples among the remainingYjâ€‹(xâ‹†)Y_{j}(x^{\\star})when we enable sampling.(b)The right panel shows the full promptxâ‹†x^{\\star}and a one-line\nsummary of each of the1616sampled completions, again colorâ€“coded by label.\nIn this particular case, we obtainâˆ‘j=116Zjâ€‹(xâ‹†)=3\\sum_{j=1}^{16}Z_{j}(x^{\\star}){=}3harmful\ncompletions and an empirical harmful massq^Î¸â€‹(xâ‹†)â‰ˆ0.19\\widehat{q}_{\\theta}(x^{\\star})\\approx 0.19, which yields a tailâ€“risk estimater^16â€‹(xâ‹†)=1\\widehat{r}_{16}(x^{\\star}){=}1, i.e., a batch of1616samples almost surely\ncontains at least one harmful continuation.\nDeterministic (greedy) evaluation would thuscertifyxâ‹†x^{\\star}as safe,\neven though realistic multiâ€“sample usage reveals a substantial harmful tail.\nThis singleâ€“prompt case study makes theillusion of robustnessvisually\nexplicit: what looks green under greedy decoding still hides significant red\nmass once we examine the full stochastic behavior.",
                "position": 6748
            }
        ]
    },
    {
        "header": "\\PragyaHeadline7\\PragyaHeadlineDiscussion and Limitations",
        "images": []
    },
    {
        "header": "\\PragyaHeadline8\\PragyaHeadlineConclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]