[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08645/x1.png",
                "caption": "",
                "position": 110
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08645/x2.png",
                "caption": "Figure 2:Retrieval feature comparison.Retrieval with DINO features (right) produces semantic matches, while instance retrieval features[51](middle) find identical objects.",
                "position": 162
            }
        ]
    },
    {
        "header": "3Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08645/x3.png",
                "caption": "(a)",
                "position": 212
            },
            {
                "img": "https://arxiv.org/html/2412.08645/x3.png",
                "caption": "(a)",
                "position": 215
            },
            {
                "img": "https://arxiv.org/html/2412.08645/x4.png",
                "caption": "(b)",
                "position": 220
            },
            {
                "img": "https://arxiv.org/html/2412.08645/x5.png",
                "caption": "(c)",
                "position": 225
            },
            {
                "img": "https://arxiv.org/html/2412.08645/x6.png",
                "caption": "Figure 4:Recurring mass-produced objects.Percentage of instances within classes of everyday objects with at least 3 retrieved recurrences in WebLI.",
                "position": 232
            }
        ]
    },
    {
        "header": "4The object recurrence prior",
        "images": []
    },
    {
        "header": "5ObjectMate: Leveraging object recurrence",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08645/x7.png",
                "caption": "Figure 5:Creating a supervised dataset.For each unlabeled image, we detect and crop objects with high detection confidence. Next, we extract the kNN of these objects based on IR feature similarity. To generate the background image, we apply an object removal model.",
                "position": 260
            },
            {
                "img": "https://arxiv.org/html/2412.08645/x8.png",
                "caption": "Figure 6:Architecture.We use an unmodified standard UNet. The input is a2×2222\\times 22 × 2grid of 3 reference images and a noisy target image. We calculate the loss only for the target image pixels. In object insertion, we concatenate the mask and background along the channel axis.",
                "position": 263
            },
            {
                "img": "https://arxiv.org/html/2412.08645/x9.png",
                "caption": "Figure 7:Object insertion results.Our method better harmonizes the pose and lighting with the scene while preserving object identity.",
                "position": 285
            }
        ]
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08645/x10.png",
                "caption": "Figure 8:Subject-driven generation results.ObjectMate can composite the object into the scene given 3 reference views and a prompt describing the scene.Our method does not require test-time tuning.",
                "position": 370
            },
            {
                "img": "https://arxiv.org/html/2412.08645/x11.png",
                "caption": "Figure 9:Open features and data.Using data based on IR features outperforms CLIP and DINO. Public datasets and feature encoders achieve strong performance.",
                "position": 562
            },
            {
                "img": "https://arxiv.org/html/2412.08645/x12.png",
                "caption": "Figure 10:Effect of dataset size on object insertion metrics.Larger unsupervised datasets yield better results.",
                "position": 566
            }
        ]
    },
    {
        "header": "7Discussion and Limitations",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "9Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08645/x13.png",
                "caption": "Figure 11:Subject-driven generation model’s architecture.",
                "position": 1544
            }
        ]
    },
    {
        "header": "Appendix BAdditional comparisons",
        "images": []
    },
    {
        "header": "Appendix CUser study",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08645/x14.png",
                "caption": "Figure 12:A screenshot of the user study questionnaire.",
                "position": 1795
            }
        ]
    },
    {
        "header": "Appendix DQuantitative evaluation protocol",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08645/x15.png",
                "caption": "Figure 13:Example of a quadruplet from out test set. From each quadruplet we extract 4 samples, where one object is used as the ground truth and the remaining 3 serve as the reference condition.",
                "position": 1818
            },
            {
                "img": "https://arxiv.org/html/2412.08645/x16.png",
                "caption": "Figure 14:Ablation study on the importance of IR features for object insertion.Using CLIP or DINO features for instance retrieval during object insertion training is insufficient to achieve identity preservation. Using specialized instance-retrieval (IR) features achieve much stronger results. In addition, the publicly available IR model from[51]is comparable to our internal model.",
                "position": 1821
            },
            {
                "img": "https://arxiv.org/html/2412.08645/x17.png",
                "caption": "Figure 15:Ablation study on the importance of IR features for subject generation.Our subject generation model, denoted as IR, demonstrates superior identity preservation compared to a model trained using DINO-based retrievals.",
                "position": 1824
            },
            {
                "img": "https://arxiv.org/html/2412.08645/x18.png",
                "caption": "Figure 16:Ablation study on data sources.We compare the effectiveness of different data sources for training.\nTraining on Open Images with publicly available IR features and on a web-scraped dataset using our internal IR model both outperform the current state-of-the-art insertion model, AnyDoor.",
                "position": 1827
            },
            {
                "img": "https://arxiv.org/html/2412.08645/x19.png",
                "caption": "Figure 17:Comparison with counterfactual object insertion.We compare to a model similar ObjectDrop. Our model is able to realistically harmonize the object’s pose and lighting, while the counterfactual model pastes the object without adjustments.",
                "position": 1831
            },
            {
                "img": "https://arxiv.org/html/2412.08645/x20.png",
                "caption": "Figure 18:Additional in-the-wild object insertion results.",
                "position": 1834
            },
            {
                "img": "https://arxiv.org/html/2412.08645/x21.png",
                "caption": "Figure 19:Comparison with SuTI.Our method better preserves the fine details of the subjects. SuTI uses semantic features (CLIP) for retrieval, while we use specialized instance-retrieval features. This makes our paired data more suitable for identity preservation. Results of SuTI are taken from their manuscript. Here, SuTI uses 5 references, while we use 3.",
                "position": 1837
            },
            {
                "img": "https://arxiv.org/html/2412.08645/x22.png",
                "caption": "Figure 20:Comparison with SuTI.Our model demonstrates superior capability in preserving fine details of the object, regardless of whether 1 or 3 reference images are provided by the user. Results of SuTI are taken from their manuscript.",
                "position": 1840
            },
            {
                "img": "https://arxiv.org/html/2412.08645/x23.png",
                "caption": "Figure 21:Comparison with Instruct-Imagen.Our method better preserves the fine details of the bowl (e.g., text decoration). Instruct-Imagen uses similar data to SuTI, which is based on semantic clustering. Results of Instruct-Imagen are taken from their manuscript.",
                "position": 1843
            },
            {
                "img": "https://arxiv.org/html/2412.08645/x24.png",
                "caption": "Figure 22:Creative application.We test the model’s generalization by providing it with three references ofdifferentobjects. This setup represents a significant deviation from the training distribution, where the model received three references of the same object. Remarkably, the model demonstrates an ability to generalize beyond its training data by either synthesizing the references into a single unified object or generating the three objects separately.",
                "position": 1846
            },
            {
                "img": "https://arxiv.org/html/2412.08645/x25.png",
                "caption": "(a)",
                "position": 1849
            },
            {
                "img": "https://arxiv.org/html/2412.08645/x25.png",
                "caption": "(a)",
                "position": 1852
            },
            {
                "img": "https://arxiv.org/html/2412.08645/x26.png",
                "caption": "(b)",
                "position": 1857
            },
            {
                "img": "https://arxiv.org/html/2412.08645/x27.png",
                "caption": "Figure 24:Additional object insertion comparisons on our benchmark with the provided ground truth.",
                "position": 1864
            },
            {
                "img": "https://arxiv.org/html/2412.08645/x28.png",
                "caption": "Figure 25:Additional object insertion comparisons on our benchmark with the provided ground truth.",
                "position": 1867
            },
            {
                "img": "https://arxiv.org/html/2412.08645/x29.png",
                "caption": "Figure 26:Additional subject-driven generation comparisons.",
                "position": 1870
            },
            {
                "img": "https://arxiv.org/html/2412.08645/x30.png",
                "caption": "Figure 27:Additional subject-driven generation comparisons.",
                "position": 1873
            }
        ]
    },
    {
        "header": "Appendix EObject insertion benchmark",
        "images": []
    }
]