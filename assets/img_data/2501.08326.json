[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08326/x1.png",
                "caption": "",
                "position": 113
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08326/x2.png",
                "caption": "Figure 2:Method comparison.(a) RoI-based methods generate visual region prompts using RoI-aligned visual features, potentially leading to temporal drift in the visual features of the target object in the video domain. (b) In contrast, our Token Mark is assigned to the corresponding region, preserving a consistent spatio-temporal target reference.",
                "position": 153
            },
            {
                "img": "https://arxiv.org/html/2501.08326/x3.png",
                "caption": "Figure 3:(a) Overview:Omni-RGPT enables region-level understanding across image and video inputs. Given region prompts (e.g.boxes or masks) in a single image or the initial frame of a video, we assignToken Mark— a set of vectors serving as spatio-temporal region indicators — to the region.\nThese vectors are embedded into the spatial region localized by the region prompt and directly injected into both visual and text prompts to indicate the target.(b) Auxiliary Head:We further introduceTemporal Region Guide Headto achieve robust region understanding in videos without relying on tracklet prompts. Building on Token Mark’s consistent representation of target objects across frames, this auxiliary task classifies the target Token Mark for visual tokens in subsequent frames.",
                "position": 158
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Omni-RGPT",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08326/x4.png",
                "caption": "Figure 4:Overview of our instruction sample generation pipeline.From a video with region masklets and nouns, the region-level captions, which contain contextual and temporal information about regions, are generated from GPT4o (left). Then, the hallucinations in the captions are mitigated (middle). Lastly, the instruction samples that cover diverse aspects of the regions are generated (right).",
                "position": 273
            }
        ]
    },
    {
        "header": "4Region-level Video Instruction Dataset",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08326/x5.png",
                "caption": "Figure 5:Heatmap of Temporal Region Guide Head outputs.",
                "position": 1021
            }
        ]
    },
    {
        "header": "6Limitation",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "8Implementation Details",
        "images": []
    },
    {
        "header": "9More Quantitative Results",
        "images": []
    },
    {
        "header": "10More Qualitative Results",
        "images": []
    },
    {
        "header": "11Visualization Analysis",
        "images": []
    },
    {
        "header": "12RegVID-300k",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08326/x6.png",
                "caption": "Figure 6:RegVID-300k statistics.(a) The dataset contains multiple regions in videos. (b) The captions primarily range from 30 to 60 words, resulting in diverse instruction-following samples.",
                "position": 3065
            },
            {
                "img": "https://arxiv.org/html/2501.08326/x7.png",
                "caption": "",
                "position": 3075
            },
            {
                "img": "https://arxiv.org/html/2501.08326/x8.png",
                "caption": "Table 14:Qualitative visualization of brief region-level video captioning capability.",
                "position": 3133
            },
            {
                "img": "https://arxiv.org/html/2501.08326/x9.png",
                "caption": "",
                "position": 3197
            },
            {
                "img": "https://arxiv.org/html/2501.08326/x10.png",
                "caption": "",
                "position": 3244
            },
            {
                "img": "https://arxiv.org/html/2501.08326/x11.png",
                "caption": "Table 15:Qualitative visualization of detailed region-level video captioning capability.Specifically, the sentences highlighted in orange indicate that our model effectively captures the temporal motion (or dynamics) of the targeted objects.",
                "position": 3293
            },
            {
                "img": "https://arxiv.org/html/2501.08326/x12.png",
                "caption": "",
                "position": 3342
            },
            {
                "img": "https://arxiv.org/html/2501.08326/x13.png",
                "caption": "Table 16:Qualitative result of region-level video QA.",
                "position": 3376
            },
            {
                "img": "https://arxiv.org/html/2501.08326/x14.png",
                "caption": "",
                "position": 3470
            },
            {
                "img": "https://arxiv.org/html/2501.08326/extracted/6132114/fig/supp/vis-img-cap.jpg",
                "caption": "Table 17:Qualitative result of region-level image captioning.",
                "position": 3549
            },
            {
                "img": "https://arxiv.org/html/2501.08326/extracted/6132114/fig/supp/vis-img-qa-1.jpg",
                "caption": "Table 18:Qualitative result of region-level image QA.",
                "position": 3624
            },
            {
                "img": "https://arxiv.org/html/2501.08326/extracted/6132114/fig/supp/REC_examples/00610.jpg",
                "caption": "Table 19:Qualitative comparison of image REC.",
                "position": 3684
            },
            {
                "img": "https://arxiv.org/html/2501.08326/extracted/6132114/fig/supp/REC_examples/00610_ours.jpg",
                "caption": "",
                "position": 3722
            },
            {
                "img": "https://arxiv.org/html/2501.08326/extracted/6132114/fig/supp/REC_examples/00610_rgpt.jpg",
                "caption": "",
                "position": 3740
            },
            {
                "img": "https://arxiv.org/html/2501.08326/extracted/6132114/fig/supp/REC_examples/00610_groma.jpg",
                "caption": "",
                "position": 3758
            },
            {
                "img": "https://arxiv.org/html/2501.08326/extracted/6132114/fig/supp/REC_examples/02684.jpg",
                "caption": "Table 20:Qualitative comparison of image REC.",
                "position": 3766
            },
            {
                "img": "https://arxiv.org/html/2501.08326/extracted/6132114/fig/supp/REC_examples/02684_ours.jpg",
                "caption": "",
                "position": 3804
            },
            {
                "img": "https://arxiv.org/html/2501.08326/extracted/6132114/fig/supp/REC_examples/02684_rgpt.jpg",
                "caption": "",
                "position": 3822
            },
            {
                "img": "https://arxiv.org/html/2501.08326/extracted/6132114/fig/supp/REC_examples/02684_groma.jpg",
                "caption": "",
                "position": 3840
            },
            {
                "img": "https://arxiv.org/html/2501.08326/x15.png",
                "caption": "Table 21:Visualized analysis of Elysium dataset.Top:The original Elysium annotations are short and predominantly noun-centric.Bottom:We present Extended-Elysium, refined annotations created using our dataset curation pipeline.",
                "position": 3848
            },
            {
                "img": "https://arxiv.org/html/2501.08326/x16.png",
                "caption": "",
                "position": 3904
            },
            {
                "img": "https://arxiv.org/html/2501.08326/x17.png",
                "caption": "Table 22:Failure Cases.Top:Omni-RGPT occasionally exhibits unstable performance on small objects. The model mistakenly identifes the black chair as a backpack in the second example.Bottom:Omni-RGPT shows limited capacity for understanding the direction of objects. The model answers that the cat is climbing upward, even though it is actually moving downward.",
                "position": 3960
            },
            {
                "img": "https://arxiv.org/html/2501.08326/x18.png",
                "caption": "",
                "position": 3996
            },
            {
                "img": "https://arxiv.org/html/2501.08326/x19.png",
                "caption": "",
                "position": 4026
            },
            {
                "img": "https://arxiv.org/html/2501.08326/x20.png",
                "caption": "Table 23:Visualized instruction-following sample and the video in RegVID-300k.The object masks are overlayed onto the video frames. The video is from the BDD100k dataset[78].",
                "position": 4050
            },
            {
                "img": "https://arxiv.org/html/2501.08326/x21.png",
                "caption": "Table 24:Visualized instruction-following sample and the video in RegVID-300k.The object masks are overlayed onto the video frames. The video is from the Action Genome dataset[23].",
                "position": 4221
            }
        ]
    },
    {
        "header": "13Ethics Concerns",
        "images": []
    }
]