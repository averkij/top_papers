[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01053/figure/cosim.png",
                "caption": "Figure 1:(Left)Relationship between the full cache, base cache, and adapter output.(Right)Layer-wise pairwise cosine similarity of the base and full caches, measured on the same context across three agent pairs using 128 samples of 2k tokens from the HotpotQA dataset.",
                "position": 237
            },
            {
                "img": "https://arxiv.org/html/2602.01053/figure/schemes.png",
                "caption": "Figure 2:Agent iteration and cache accumulation for Non-Shared,BaseShared, andBaseLRShared.T0\\mathrm{T0}denotes the system prompt shared across agents, andT​i\\mathrm{T}idenotes trajectory context blocks, formed by concatenating model-generated tokens and retrieved context from external sources.BaseSharedshares only the base cache and maintains a separate LR cache per agent, whereasBaseLRSharedshares both the base and LR caches.",
                "position": 293
            },
            {
                "img": "https://arxiv.org/html/2602.01053/figure/baselrcache.png",
                "caption": "Figure 3:Diagram of base and LR cache computation with an initial context of lengthLpL_{p}prefilled by agentii, followed by an additional context of lengthLcL_{c}processed by agentjj, under (a)BaseSharedand (b)BaseLRShared.BaseSharedmaintains per-agent LR caches and computes the LR cache using hidden states for all context tokens not yet processed by the current agent, whereasBaseLRSharedshares a single LR cache and uses hidden states only for newly appended tokens.\nBoth methods first compute the base cache from the pretrained weights (\\scriptsize1⃝, \\scriptsize4⃝).\nThey then compute the LR cache via the LoRA down-projection (\\scriptsize2⃝, \\scriptsize5⃝),\nand later expand it to the full dimension via the LoRA up-projection over the full sequence (\\scriptsize3⃝, \\scriptsize6⃝).\nEfficient LR cache expansion is described in Section3.3.",
                "position": 296
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01053/figure/fla.png",
                "caption": "Figure 4:System throughput (tokens per second) ofBaseSharedandBaseLRShared, with Flash-LoRA-Attention (FLA).",
                "position": 952
            },
            {
                "img": "https://arxiv.org/html/2602.01053/figure/memory.png",
                "caption": "Figure 5:Memory usage (GB) of cache sharing methods on total sequence length of 66.4k on Ministral-8B-Instruct.",
                "position": 971
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Software and Data",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ABase Cache and Adapter Output",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01053/figure/appendix/cache_norm.png",
                "caption": "Figure 6:L1 norm of the base cache and adapter output across model layers.",
                "position": 1625
            },
            {
                "img": "https://arxiv.org/html/2602.01053/figure/appendix/k_cosim.png",
                "caption": "Figure 7:K cache Cosine similarity of each agent pairs across the model layers.",
                "position": 1746
            }
        ]
    },
    {
        "header": "Appendix BFlash-LoRA-Attention",
        "images": []
    },
    {
        "header": "Appendix CImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01053/figure/appendix/trajectory.png",
                "caption": "Figure 8:Agent prompts and an example of an accumulated trajectory on HotpotQA.",
                "position": 1870
            },
            {
                "img": "https://arxiv.org/html/2602.01053/figure/appendix/loss.png",
                "caption": "Figure 9:Train loss and L2 norm of the gradient for each agent types.",
                "position": 2022
            }
        ]
    },
    {
        "header": "Appendix DExperiments",
        "images": []
    }
]