[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08828/x1.png",
                "caption": "Figure 1.MMDocIRcomprises 313 lengthy documents across 10 different domains, along with 1,685 questions. For each question, page-level annotations are provided via selected screenshots. Red boundary boxes represent layout-level annotations.",
                "position": 157
            },
            {
                "img": "https://arxiv.org/html/2501.08828/x2.png",
                "caption": "Figure 2.Area ratio of different modalities (1) in overall and (2) by domains in MMLongBench-Doc benchmark(Ma et al.,2024b). Note that the white spaces, headers, and footers are removed from the area counting.",
                "position": 160
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": []
    },
    {
        "header": "2.Dual-Task Retrieval Definition",
        "images": []
    },
    {
        "header": "3.MMDocIR: Evaluation Set",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08828/x3.png",
                "caption": "Table 2.Detailed statistics forMMDocIRevaluation set. “#Lay/Page” is the averaging layouts per page, reflecting page’s layout complexity. “%Lay” refers to the area ratio of useful layouts (excluding white spaces, headers, and footers) over entire page.",
                "position": 375
            },
            {
                "img": "https://arxiv.org/html/2501.08828/x3.png",
                "caption": "Table 4.Document statistics for Training Datasets collected.",
                "position": 581
            }
        ]
    },
    {
        "header": "4.MMDocIR: Training Set",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08828/x3.png",
                "caption": "Table 5.Main results for page-level retrieval. “OCR-text” and “VLM-text” refer to converting multi-modal content in the document page using OCR and VLM respectively. “Image” refers to processing document page as screenshot image.",
                "position": 744
            }
        ]
    },
    {
        "header": "5.Model Training: DPR-Phi3&Col-Phi3",
        "images": []
    },
    {
        "header": "6.Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08828/x3.png",
                "caption": "Table 6.Main results for layout-level retrieval. “OCR-text” and “VLM-text” refer to converting multi-modal layouts using OCR and VLM respectively. “Pure-Image” and “Hybrid” refer to reading textual layouts in image and text format respectively.",
                "position": 1058
            },
            {
                "img": "https://arxiv.org/html/2501.08828/x3.png",
                "caption": "(a)Avg word length",
                "position": 1466
            },
            {
                "img": "https://arxiv.org/html/2501.08828/x3.png",
                "caption": "(a)Avg word length",
                "position": 1469
            },
            {
                "img": "https://arxiv.org/html/2501.08828/x4.png",
                "caption": "(b)Distribution density of word length",
                "position": 1474
            }
        ]
    },
    {
        "header": "7.Related Work",
        "images": []
    },
    {
        "header": "8.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]