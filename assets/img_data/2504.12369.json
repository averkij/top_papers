[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.12369/x1.png",
                "caption": "",
                "position": 76
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3WorldMem",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.12369/x2.png",
                "caption": "Figure 2:Comprehensive overview ofWorldMem.The framework comprises a conditional diffusion transformer integrated with memory blocks, with a dedicated memory bank storing memory units from previously generated content. By retrieving these memory units from the memory bank and incorporating the information by memory blocks to guide generation, our approach ensures long-term consistency in world simulation.",
                "position": 149
            },
            {
                "img": "https://arxiv.org/html/2504.12369/x3.png",
                "caption": "Figure 3:Two-view FOV overlapping visualization",
                "position": 409
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.12369/x4.png",
                "caption": "Figure 4:Qualitative results.We showcaseWorldMem’s capabilities through two sets of examples.Top: A comparison with Ground Truth (GT).WorldMemaccurately models diverse dynamics (e.g., rain) by conditioning on 600 past frames, ensuring temporal consistency.Bottom: Interaction with the world. Objects like hay in the desert or wheat in the plains persist over time, with wheat visibly growing. For the best experience, see the supplementary videos.",
                "position": 440
            },
            {
                "img": "https://arxiv.org/html/2504.12369/x5.png",
                "caption": "Figure 5:Within context window evaluation examples.It illustrates an example where the motion sequence first involves turning right and then returning to the original position, demonstrating methods’ ability to maintain self-contained consistency.",
                "position": 465
            },
            {
                "img": "https://arxiv.org/html/2504.12369/x6.png",
                "caption": "Figure 6:Beyond context window evaluation examples.It shows that Diffusion-Forcing suffers from inconsistency and quality degradation after generating a certain number of frames. In contrast, our method maintains high quality and faithfully reconstructs previously observed scenarios.",
                "position": 547
            },
            {
                "img": "https://arxiv.org/html/2504.12369/x7.png",
                "caption": "Figure 7:Examples on RealEstate[51].DFoT[32]discards content beyond its context window, losing 360-degree consistency. In contrast, our method preserves details and accurately returns to the original location.",
                "position": 557
            },
            {
                "img": "https://arxiv.org/html/2504.12369/x8.png",
                "caption": "Figure 8:Long-term Generation Comparison.This figure presents the PSNR of different ablation methods compared to the ground truth over a 300-frame sequence. The results show that our method without memory blocks or using random memory retrieval exhibits immediate inconsistencies with the ground truth. Additionally, the model lacking relative embeddings begins to degrade significantly beyond 100 frames. In contrast, our full method maintains strong consistency even beyond 300 frames.",
                "position": 604
            },
            {
                "img": "https://arxiv.org/html/2504.12369/x9.png",
                "caption": "Figure 9:Results w/o and w/ time condition.Without timestamps, the model fails to differentiate memory units from the same location at different times, causing errors. With time conditioning, it aligns with the updated world state, ensuring consistency.",
                "position": 654
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.12369/x10.png",
                "caption": "Figure 10:Illustration of different embeddings.",
                "position": 1480
            },
            {
                "img": "https://arxiv.org/html/2504.12369/x11.png",
                "caption": "Figure 11:Structure of pose predictor.",
                "position": 1605
            },
            {
                "img": "https://arxiv.org/html/2504.12369/x12.png",
                "caption": "Figure 12:Training Examples.Our training environments encompass diverse terrains, action spaces, and weather conditions, providing a comprehensive setting for learning.",
                "position": 1775
            },
            {
                "img": "https://arxiv.org/html/2504.12369/x13.png",
                "caption": "Figure 13:Visualization of Trajectory Examples in the X-Z Space.The axis scales represent distances within the Minecraft environment.",
                "position": 1781
            },
            {
                "img": "https://arxiv.org/html/2504.12369/x14.png",
                "caption": "Figure 14:Visualization of Relative Pose Distribution for Training in X-Z Space.Red dots indicate positions, while yellow arrows represent directions.",
                "position": 1787
            }
        ]
    },
    {
        "header": "6Supplementary Materials",
        "images": []
    }
]