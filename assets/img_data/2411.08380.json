[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.08380/x1.png",
                "caption": "",
                "position": 87
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.08380/x2.png",
                "caption": "Figure 2:Data annotation pipeline and cleansing metadata ofEgoVid-5M.",
                "position": 276
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3EgoVid-5M",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.08380/x3.png",
                "caption": "Figure 3:Data annotation distribution ofEgoVid-5M. (a) and (b) describe the quantities of the top 20 verbs and nouns. (c) Text-video action alignment is assessed using the EgoVideo score. (d) and (e) measure the semantic similarity between text and frames and between frames and the first frame using the average CLIP score.\n(f) Motion smoothness is quantified by the variance of translation and rotation.\n(g) Motion strength is represented by the average global optical flow.\n(h) Video clarity is determined by the DOVER score.",
                "position": 361
            },
            {
                "img": "https://arxiv.org/html/2411.08380/x4.png",
                "caption": "Figure 4:The overall framework ofEgoDreamer.EgoDreamerintroduces (a) the Unified Action Encoder to embed different action inputs simultaneously, and it utilizes (b) the Adaptive Alignment to integrate action conditions into the egocentric video generation branch (c).",
                "position": 381
            },
            {
                "img": "https://arxiv.org/html/2411.08380/x5.png",
                "caption": "Figure 5:The video visualization comparison across different data cleaning strategies reveals distinct outcomes, where thebluebox highlights the difference. Videos generated by strategy-1 fail to capture local motion and tend to be stationary. In contrast, videos produced by strategy-2 exhibit excessive motion, compromising semantic coherence. Meanwhile, videos generated by strategy-3 effectively model intricate hand movements, striking a balance between motion strength and semantic fidelity.",
                "position": 409
            },
            {
                "img": "https://arxiv.org/html/2411.08380/x6.png",
                "caption": "Figure 6:Video generation quantitative comparisons between different data cleaning strategies, where the baseline is DynamiCrafter[65]initialized with its original weights.",
                "position": 414
            }
        ]
    },
    {
        "header": "4EgoDreamer",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.08380/x7.png",
                "caption": "Figure 7:Visualizations demonstrate thatEgoVid-fintuned baselines (OpenSora[81], SVD[3], DynamiCrafter[65]) generate egocentric videos with stronger frame-consistency and better semantic-alignment.",
                "position": 472
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.08380/x8.png",
                "caption": "Figure 8:Visualizations show thatEgoDreamercan realize distinct action controls based on different text descriptions.",
                "position": 598
            },
            {
                "img": "https://arxiv.org/html/2411.08380/x9.png",
                "caption": "Figure 9:Visualizations demonstrate thatEgoDreamercan generate various egocentric videos based on different low-level commands.",
                "position": 603
            }
        ]
    },
    {
        "header": "6Discussion and Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.08380/x10.png",
                "caption": "Figure 10:Videos cleaned from thefive-pointoptical flow strategy (average optical flow below 3, and the proportion of optical flow (≥\\geq≥12 pixels) is greater than 3%). This strategy retains videos with a static background while capturing detailed and extensive motion in hands.",
                "position": 1813
            },
            {
                "img": "https://arxiv.org/html/2411.08380/x11.png",
                "caption": "Figure 11:Five-pointoptical flow distribution.",
                "position": 1818
            }
        ]
    },
    {
        "header": "7Annotation and Cleaning Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.08380/x12.png",
                "caption": "Figure 12:Semantic consistency comparison between our text annotation and the original human narration.",
                "position": 1926
            }
        ]
    },
    {
        "header": "8Training Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.08380/x13.png",
                "caption": "Figure 13:Visualizations showing thatEgoDreamercan generate action-driven egocentric videos based on high-level text descriptions.",
                "position": 1945
            },
            {
                "img": "https://arxiv.org/html/2411.08380/x14.png",
                "caption": "Figure 14:Visualizations showing thatEgoDreamercan generate action-driven egocentric videos based on low-level kinematic control.",
                "position": 1950
            },
            {
                "img": "https://arxiv.org/html/2411.08380/x15.png",
                "caption": "Figure 15:Visualizations verifying thatEgoDreamercan generate diverse egocentric videos based on action descriptions.",
                "position": 1955
            }
        ]
    },
    {
        "header": "9Evaluation Details",
        "images": []
    },
    {
        "header": "10Visualizations",
        "images": []
    }
]