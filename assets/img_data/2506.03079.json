[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03079/extracted/6509325/figures/fig_teaser.png",
                "caption": "Figure 1:Our ORV generates action-conditioned robot manipulation videos under the guidance of the 4D occupancy (top) with higher control precision, performs multiview videos generation to build realistic 4D embodied world (middle) and conducts simulation-to-real videos transfer (bottom).",
                "position": 111
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3ORV: Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03079/x1.png",
                "caption": "Figure 2:3D Semantics Occupancy Samples of Dataset BridgeV2[88]and RT-1[89]. Better to zoom in. Refer to Supplementary Materials for more examples.",
                "position": 204
            },
            {
                "img": "https://arxiv.org/html/2506.03079/x2.png",
                "caption": "Figure 3:Overview ofDataset Curation Pipeline,which consists of four parts: semantics space construction, occupancy construction, equip occupancy with semantics, and bullet-time occupancy-to-gaussian renderings in practical usage.",
                "position": 207
            },
            {
                "img": "https://arxiv.org/html/2506.03079/x3.png",
                "caption": "Figure 4:Overview ofORV. For training purposes, we start from Dataset Curation (Sec.3.1) to produce high-quality semantic occupancy data. Leveraging pixel-level aligned condition maps from such 3D representation, we generate robot videos that precisely follow the motion instructions (Sec.3.2). Furthermore, we introduce ORV-MV (Sec.3.3) and ORV-S2R (Sec.3.4), which simultaneously produce multi-view robot videos and effectively convert the simulation data to real-world videos.",
                "position": 219
            },
            {
                "img": "https://arxiv.org/html/2506.03079/extracted/6509325/figures/fig_mv.png",
                "caption": "Figure 5:Overview of ORV-MV, which generates multi-view robot manipulation videos following various controls.",
                "position": 253
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03079/extracted/6509325/figures/fig_recon_results.png",
                "caption": "Figure 6:Qualitative Results ofControllable Video Generationwith full conditions. Given one-frame observation, ORV predict subsequent 15 frames on validation split of Bridge[88], Droid[90], RT-1[89]datasets.Red boxesdenotes the first frame input of the video generation;Orange boxesdenotes the ground-truth of the subsequence frames.",
                "position": 314
            },
            {
                "img": "https://arxiv.org/html/2506.03079/extracted/6509325/figures/fig_sim2real_demo.png",
                "caption": "Figure 7:Qualitative Results ofSim-to-Real Transfer.Given raw dynamic data (e.g., a tabletop manipulation scene, which consists of various mesh components) in the simulation environment, we can transfer them into real-world data, which possesses better visual quality and leads to higher efficiency than that in original physical simulators.",
                "position": 317
            },
            {
                "img": "https://arxiv.org/html/2506.03079/extracted/6509325/figures/fig_multiview.png",
                "caption": "Figure 8:Multiview Videos Generation Results.ORV-MV supports generating multiview videos with high cross-view consistency from initial frames. We illustrate here ORV-MV generates both three-view video and two-view video.",
                "position": 329
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADatasets Details",
        "images": []
    },
    {
        "header": "Appendix BORV-MV Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03079/extracted/6509325/figures/fig_mv_cam.png",
                "caption": "Figure A:Illustration of ORV aligning multiview cameras from VGGT[101]under the frame of Monst3R[28]to get the multiview conditioning sequences.",
                "position": 2387
            },
            {
                "img": "https://arxiv.org/html/2506.03079/extracted/6509325/figures/fig_demo_cam.png",
                "caption": "Figure B:Example of transferring multiview poses from VGGT[101]to Monst3R[28]. The comparison of calibrated side views and the side views demonstrates the efficiency.",
                "position": 2411
            }
        ]
    },
    {
        "header": "Appendix CORV-S2R Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03079/extracted/6509325/figures/fig_supp_s2r.png",
                "caption": "Figure C:Illustration of our simulation-to-real pipeline. We build simulated dynamics in popular simulation tools (e.g., ManiSkill[104], and produce plausible geometries with generated motions. After that, with ORV-S2R, we transforme them into real-world videos.",
                "position": 2421
            }
        ]
    },
    {
        "header": "Appendix DAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03079/extracted/6509325/figures/fig_recon_results_2.png",
                "caption": "Figure D:Qualitative Results ofORVwith full conditions.Red boxesdenote the first frame input of the video generation;Orange boxesdenote the ground-truth of the subsequence frames.",
                "position": 2443
            },
            {
                "img": "https://arxiv.org/html/2506.03079/extracted/6509325/figures/fig_abla_depth.png",
                "caption": "Figure E:Ablation Results ofDepth Condition Map.Without any physical controls, the robot gripper fails to act accurately aligned with the 3D action instructions, due to the accumulation of errors. While ours performs correctly, along with the entire sequence.",
                "position": 2449
            },
            {
                "img": "https://arxiv.org/html/2506.03079/extracted/6509325/figures/fig_psnr_curve.png",
                "caption": "Figure F:Improvement curves of PSNR (left) and SSIM (right) metrics across ordered evaluation samples from BridgeV2[88].",
                "position": 2455
            },
            {
                "img": "https://arxiv.org/html/2506.03079/extracted/6509325/figures/fig_ssim_curve.png",
                "caption": "",
                "position": 2464
            },
            {
                "img": "https://arxiv.org/html/2506.03079/extracted/6509325/figures/fig_abla_sem.png",
                "caption": "Figure G:Ablation Results ofSemantics Condition Map.Without the guidance of our rendered semantics maps, the model fails to accurately predict the shape deformation of the knife during its motion, whereas ours produce outputs that align well with the real-world appearance.",
                "position": 2473
            },
            {
                "img": "https://arxiv.org/html/2506.03079/extracted/6509325/figures/fig_gen_app_traj.png",
                "caption": "Figure H:Appearance & TrajectoryAdaptation Results. Better to zoom in.",
                "position": 2487
            },
            {
                "img": "https://arxiv.org/html/2506.03079/extracted/6509325/figures/fig_mv_cond.png",
                "caption": "Figure I:Qualitative Comparison Results ofMultiview Videos Generation. With our from-reference-view rendered visual conditionings, generated videos under side views achieve better geometric consistency under other side views. Better to zoom in.",
                "position": 2496
            }
        ]
    },
    {
        "header": "Appendix EImplementation Details",
        "images": []
    },
    {
        "header": "Appendix FAdditional Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03079/x4.png",
                "caption": "Figure J:Additional Qualitative Results of ORV #1.",
                "position": 2820
            },
            {
                "img": "https://arxiv.org/html/2506.03079/x5.png",
                "caption": "Figure K:Additional Qualitative Results of ORV #2.",
                "position": 2823
            },
            {
                "img": "https://arxiv.org/html/2506.03079/x6.png",
                "caption": "Figure L:Additional Qualitative Results of ORV #3.",
                "position": 2826
            }
        ]
    },
    {
        "header": "Appendix GLimitations and Future Work",
        "images": []
    },
    {
        "header": "Appendix HSocial Impact",
        "images": []
    }
]