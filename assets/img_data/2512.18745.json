[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.18745/x1.png",
                "caption": "Figure 1:A multi-step visual reasoning example ofInSight-o3onO3-Bench. For clarity, the internal reasoning processes are omitted. More examples can be found in AppendixD.2.",
                "position": 208
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3O3-Bench",
        "images": []
    },
    {
        "header": "4InSight-o3",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.18745/x2.png",
                "caption": "Figure 2:Training pipeline.We use a hybrid RL algorithm to train vSearcher.(a)In the in-loop component, vReasoner generates visual search tasks on-the-fly during training as it tries to answer a user query. We use vReasoner’s feedback and final answer correctness as supervision (denoted by dashed arrows) for vSearcher.(b)In the out-of-loop component, we use pre-generated descriptions with ground-truth bounding boxes, allowing us to train vSearcher efficiently via IoU supervision.",
                "position": 394
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.18745/x3.png",
                "caption": "Figure 3:Training dynamics ofInSight-o3.The rightmost chart, “# of vReasoner calls”, shows the average number of times vReasoner calls vSearcher per QA.∗{}^{\\ast\\text{ }}For fair comparison, the reward curves are plotted under the same setting (“w/o feedback”) for all the settings.",
                "position": 1006
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix AAdditional Discussion on Related Work",
        "images": []
    },
    {
        "header": "Appendix BAdditional Information onO3-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.18745/x4.png",
                "caption": "Figure 4:Distribution of layout numbers inO3-Bench.",
                "position": 1202
            },
            {
                "img": "https://arxiv.org/html/2512.18745/x4.png",
                "caption": "Figure 4:Distribution of layout numbers inO3-Bench.",
                "position": 1205
            },
            {
                "img": "https://arxiv.org/html/2512.18745/x5.png",
                "caption": "Figure 5:Resolution distribution.",
                "position": 1210
            },
            {
                "img": "https://arxiv.org/html/2512.18745/x6.png",
                "caption": "Figure 6:Example fromO3-Bench(Map-1). Each annotation comprises a six-choice QA and a brief explanation with highlighted target layouts for quick verification; additionally, we also provide step-wise close-ups (outside the annotation) to reveal the evidence chain in large images where fine details may be hard to see.",
                "position": 1303
            },
            {
                "img": "https://arxiv.org/html/2512.18745/x7.png",
                "caption": "Figure 7:Example fromO3-Bench(Map-2). Each annotation comprises a six-choice QA and a brief explanation with highlighted target layouts for quick verification; additionally, we also provide step-wise close-ups (outside the annotation) to reveal the evidence chain in large images where fine details may be hard to see.",
                "position": 1306
            },
            {
                "img": "https://arxiv.org/html/2512.18745/x8.png",
                "caption": "Figure 8:Example fromO3-Bench(Map-3). Each annotation comprises a six-choice QA and a brief explanation with highlighted target layouts for quick verification; additionally, we also provide step-wise close-ups (outside the annotation) to reveal the evidence chain in large images where fine details may be hard to see.",
                "position": 1309
            },
            {
                "img": "https://arxiv.org/html/2512.18745/x9.png",
                "caption": "Figure 9:Example fromO3-Bench(Map-4). Each annotation comprises a six-choice QA and a brief explanation with highlighted target layouts for quick verification; additionally, we also provide step-wise close-ups (outside the annotation) to reveal the evidence chain in large images where fine details may be hard to see.",
                "position": 1312
            },
            {
                "img": "https://arxiv.org/html/2512.18745/x10.png",
                "caption": "Figure 10:Example fromO3-Bench(Chart-1). Each annotation comprises a six-choice QA and a brief explanation with highlighted target layouts for quick verification; additionally, we also provide step-wise close-ups (outside the annotation) to reveal the evidence chain in large images where fine details may be hard to see.",
                "position": 1315
            },
            {
                "img": "https://arxiv.org/html/2512.18745/x11.png",
                "caption": "Figure 11:Example fromO3-Bench(Chart-2). Each annotation comprises a six-choice QA and a brief explanation with highlighted target layouts for quick verification; additionally, we also provide step-wise close-ups (outside the annotation) to reveal the evidence chain in large images where fine details may be hard to see.",
                "position": 1318
            }
        ]
    },
    {
        "header": "Appendix CTraining Data Construction Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.18745/x12.png",
                "caption": "Figure 12:Example of synthesized collage for the in-loop RL. Multiple low-resolution images are stitched to raise visual density. The blue dashed box highlights the target tile; the magenta box marks the target bbox. Remaining tiles are distractors.",
                "position": 1673
            },
            {
                "img": "https://arxiv.org/html/2512.18745/x13.png",
                "caption": "Figure 13:Examples of InfographicVQA images with pre-generated layout boxes and region descriptions for the out-of-loop RL.",
                "position": 1676
            }
        ]
    },
    {
        "header": "Appendix DAdditional Information onInSight-o3",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.18745/x14.png",
                "caption": "Figure 14:Qualitative example 1 with GPT-5-mini as vReasoner andInSight-o3-vSas vSearcher. The reasoner requests venue-level cues (e.g., legend/index lookups); the searcher returns localized regions and snippets, iterating to a correct answer.",
                "position": 1789
            },
            {
                "img": "https://arxiv.org/html/2512.18745/x15.png",
                "caption": "Figure 15:Qualitative example 2 with GPT-5-mini as vReasoner andInSight-o3-vSas vSearcher. The reasoner requests venue-level cues (e.g., legend/index lookups); the searcher returns localized regions and snippets, iterating to a correct answer.",
                "position": 1793
            },
            {
                "img": "https://arxiv.org/html/2512.18745/x16.png",
                "caption": "Figure 16:Example 1 (mapcase 1) of complete reasoning trace of our model (GPT-5-mini as vReasoner andInSight-o3-vSas vSearcher). The vReasoner iteratively guides vSearcher through natural-language region descriptions.InSight-o3-vSprecisely retrieves high-quality image crops that match the described regions, effectively supporting vReasoner in locating the target area and producing the correct answer.",
                "position": 1900
            },
            {
                "img": "https://arxiv.org/html/2512.18745/x17.png",
                "caption": "Figure 17:Example 1 (mapcase 1, continuation of Fig.16): reasoning traces of GPT-5-mini and GPT-5-mini + Qwen2.5-VL-7B-Instruct.\nGPT-5-mini exhibits misperception and reasoning drift, while Qwen2.5-VL-7B-Instruct frequently fails to follow vReasoner’s instructions, producing low-quality crops misaligned with the described regions. Consequently, both baselines yield incorrect answers.",
                "position": 1903
            },
            {
                "img": "https://arxiv.org/html/2512.18745/x18.png",
                "caption": "Figure 18:Example 2 (mapcase 2): reasoning traces of our model, GPT-5-mini, and GPT-5-mini + Qwen2.5-VL-7B-Instruct.\nOurInSight-o3-vSaccurately follows vReasoner’s instructions and returns high-quality crops aligned with the described regions, leading to a correct answer. In contrast, Qwen2.5-VL-7B fails to return a valid crop in the final reasoning round, resulting in an incorrect answer.",
                "position": 1907
            },
            {
                "img": "https://arxiv.org/html/2512.18745/x19.png",
                "caption": "Figure 19:Example 3 (chartcase): reasoning traces of our model, GPT-5-mini, and GPT-5-mini + Qwen2.5-VL-7B-Instruct.InSight-o3-vSeffectively follows vReasoner’s guidance and retrieves high-quality crops that fully capture the described regions. In contrast, Qwen2.5-VL-7B returns only partial crops in Round 2 and, in the Round 3, fails to produce a valid crop as requested by vReasoner, incorrectly concluding that the target region is absent, which leads to an incorrect answer.",
                "position": 1911
            },
            {
                "img": "https://arxiv.org/html/2512.18745/x20.png",
                "caption": "Figure 20:Failure cases 1 & 2 ofInSight-o3(GPT-5-mini + InSight-o3-vS).",
                "position": 1926
            },
            {
                "img": "https://arxiv.org/html/2512.18745/x21.png",
                "caption": "Figure 21:Failure cases 3 & 4 ofInSight-o3(GPT-5-mini + InSight-o3-vS).",
                "position": 1929
            }
        ]
    },
    {
        "header": "Appendix EPrompts forO3-Bench",
        "images": []
    },
    {
        "header": "Appendix FPrompts for InSight-o3",
        "images": []
    },
    {
        "header": "Appendix GEvaluation Benchmarks",
        "images": []
    }
]