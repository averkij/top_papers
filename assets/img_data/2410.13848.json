[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13848/x1.png",
                "caption": "",
                "position": 71
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13848/x2.png",
                "caption": "(a)Benchmark Performance.",
                "position": 99
            },
            {
                "img": "https://arxiv.org/html/2410.13848/x2.png",
                "caption": "(a)Benchmark Performance.",
                "position": 102
            },
            {
                "img": "https://arxiv.org/html/2410.13848/x3.png",
                "caption": "(b)Visual Generation Results.",
                "position": 107
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Janus: A Simple, Unified and Flexible Multimodal Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13848/x4.png",
                "caption": "Figure 2:Architecture of our Janus.Different from previous approaches[77,85]that typically assume visual understanding and generation require the same visual encoder, our Janus decouples visual encoding for visual understanding and visual generation. “Und. Encoder” and “Gen. Encoder” are abbreviations for “Understanding Encoder” and “Generation Encoder”, respectively. Best viewed in color.",
                "position": 164
            },
            {
                "img": "https://arxiv.org/html/2410.13848/x5.png",
                "caption": "Figure 3:Our Janus adopts a three-stage training procedure.We use flame symbols/snowflake symbols in the diagram to indicate the module updates/does not update its parameters.",
                "position": 185
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13848/x6.png",
                "caption": "Figure 4:Qualitative comparisons of visual generation with LlamaGen and SDXL. The images generated by Janus show better consistency with the user’s prompts. The image resolutions for SDXL, LlamaGen, and ours are1024×1024102410241024\\times 10241024 × 1024,512×512512512512\\times 512512 × 512, and384×384384384384\\times 384384 × 384, respectively. Best viewed on screen.",
                "position": 1338
            },
            {
                "img": "https://arxiv.org/html/2410.13848/x7.png",
                "caption": "Figure 5:Qualitative results of multimodal understanding on humorous memes. We compare the response with Chameleon-7777B[77]and Show-o[86]. We emphasize the key-points in the response. Best viewed on screen.",
                "position": 1346
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ADetails of Semantic Tokenizer Mentioned in Ablation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13848/x8.png",
                "caption": "Figure 6:Architecture and usage of the semantic tokenizer.(a) Architecture used during training of the semantic tokenizer. We use pre-trained SigLIP[92]to supervise the reconstruction of semantic information, while using raw image to supervise the reconstruction of RGB values. (b) Integrating LLM with the semantic decoder. The semantic decoder outputs continuous features with high-level semantics, which are passed through an adaptor and then used as input for the LLM. Please note that the semantic tokenizer is only used in the ablation study, not in the main experiment.",
                "position": 1375
            }
        ]
    },
    {
        "header": "Appendix BAdditional Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13848/x9.png",
                "caption": "Figure 7:More text-to-image generation results. We upsample the images to1024×1024102410241024\\times 10241024 × 1024for better visualization.",
                "position": 1413
            },
            {
                "img": "https://arxiv.org/html/2410.13848/x10.png",
                "caption": "Figure 8:Multilingual text-to-image generation samples compared to LlamaGen[73]. Note that we only use English text-to-image data in training, and this is an emergent capability of our model. The languages used in the prompt, from left to right, are: English, Chinese, French, Japanese, and English with emoji.",
                "position": 1416
            },
            {
                "img": "https://arxiv.org/html/2410.13848/x11.png",
                "caption": "Figure 9:More multimodal understanding results. Janus has a strong multimodal understanding capability and can handle inputs from various contexts, such as scientific charts, artwork images, LaTeX formula images, and more.",
                "position": 1419
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]