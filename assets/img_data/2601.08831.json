[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08831/x1.png",
                "caption": "",
                "position": 148
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08831/x2.png",
                "caption": "Figure 2:Limitations of traditional VOS and 3D segmentation approaches, and an overview of our capability.(a) Traditional VOS methods such as SAM2[46]lose track when the camera undergoes large viewpoint changes, causing masks to drift or disappear.\n(b) 3D segmentation approaches rely on accurate camera poses and explicit 3D mask merging; they often propagate errors when the 3D reconstruction is incomplete or noisy.\n(c) Our3AMconsistently tracks object instances across drastic viewpoint changes without requiring camera poses or 3D ground-truth masks, demonstrating robust cross-view correspondence purely from geometry-aware 2D tracking.",
                "position": 161
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08831/x3.png",
                "caption": "Figure 3:3AM Pipeline Overview.Our Feature Merger fuses multi-level MUSt3R features, learned from multi-view consistency to encode implicit geometric correspondence, with SAM2’s appearance features via cross-attention and convolutional refinement. These merged geometry-aware representations then undergo memory attention with previous frames and mask decoding, enabling spatially-consistent object recognition that maintains identity across large viewpoint changes without requiring camera poses at inference.",
                "position": 219
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08831/x4.png",
                "caption": "Figure 4:Illustration of Features for Feature Merging.The heat map is computed using the cosine similarity between the red query point and the target frame. As illustrated in the lower row, vanilla SAM2 fails under large viewpoint changes. In contrast, as the MUSt3R feature hierarchy gradually shifts from semantic correspondence toward the point-cloud domain, we select intermediate layers to preserve both semantic relevance and geometric structure. By combining MUSt3R’s geometric cues with SAM2’s visual semantics, the merged featureFm​e​r​g​e​dF_{{merged}}provides a significantly more reliable localization of the target object.",
                "position": 260
            },
            {
                "img": "https://arxiv.org/html/2601.08831/x5.png",
                "caption": "(a)Vanilla continuous sampling strategy",
                "position": 284
            },
            {
                "img": "https://arxiv.org/html/2601.08831/x5.png",
                "caption": "(a)Vanilla continuous sampling strategy",
                "position": 287
            },
            {
                "img": "https://arxiv.org/html/2601.08831/x6.png",
                "caption": "(b)Naive random sampling without considering field-of-view",
                "position": 293
            },
            {
                "img": "https://arxiv.org/html/2601.08831/x7.png",
                "caption": "(c)Field-of-view–aware sampling strategy (Ours)",
                "position": 299
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08831/x8.png",
                "caption": "Figure 6:Visual comparison of VOS methods.The leftmost frame is used as the conditioned frame and provides the reference mask.",
                "position": 548
            },
            {
                "img": "https://arxiv.org/html/2601.08831/x9.png",
                "caption": "Figure 7:Visual comparison of different 3D reconstruction backbones.(Top) CUT3R’s reconstruction lacks stable object alignment; the same table appears at inconsistent 3D locations. Such geometric instability weakens feature distinctiveness, making reliable tracking difficult. (Bottom) In contrast, MUSt3R provides coherent and stable object alignment across viewpoints, yielding features that preserve object identity and enable robust tracking",
                "position": 909
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AOverview",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    },
    {
        "header": "Appendix CTraining Details",
        "images": []
    },
    {
        "header": "Appendix DClass-Agnostic Instance Segmentation",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08831/x10.png",
                "caption": "Figure 8:Visual comparison of different VOS methods",
                "position": 2185
            },
            {
                "img": "https://arxiv.org/html/2601.08831/figs/fig-vis/1ada7a0617/pred_masks_instance_512_7.jpg",
                "caption": "",
                "position": 2189
            },
            {
                "img": "https://arxiv.org/html/2601.08831/figs/fig-vis/1ada7a0617/pred_masks_instance_512_11.jpg",
                "caption": "Figure 9:Visual comparison of different VOS methods",
                "position": 2195
            },
            {
                "img": "https://arxiv.org/html/2601.08831/figs/fig-vis/1ada7a0617/pred_masks_instance_512_36.jpg",
                "caption": "",
                "position": 2199
            },
            {
                "img": "https://arxiv.org/html/2601.08831/figs/fig-vis/7b6477cb95/pred_masks_instance_512_10.jpg",
                "caption": "Figure 10:Visual comparison of different VOS methods",
                "position": 2205
            },
            {
                "img": "https://arxiv.org/html/2601.08831/figs/fig-vis/7b6477cb95/pred_masks_instance_512_11.jpg",
                "caption": "",
                "position": 2209
            },
            {
                "img": "https://arxiv.org/html/2601.08831/figs/fig-vis/7b6477cb95/pred_masks_instance_512_45.jpg",
                "caption": "Figure 11:Visual comparison of different VOS methods",
                "position": 2215
            },
            {
                "img": "https://arxiv.org/html/2601.08831/figs/fig-vis/7b6477cb95/pred_masks_instance_512_53.jpg",
                "caption": "",
                "position": 2219
            },
            {
                "img": "https://arxiv.org/html/2601.08831/figs/fig-vis/a24f64f7fb/pred_masks_instance_512_22.jpg",
                "caption": "Figure 12:Visual comparison of different VOS methods",
                "position": 2225
            },
            {
                "img": "https://arxiv.org/html/2601.08831/figs/fig-vis/acd95847c5/pred_masks_instance_512_10.jpg",
                "caption": "",
                "position": 2229
            },
            {
                "img": "https://arxiv.org/html/2601.08831/figs/fig-vis/acd95847c5/pred_masks_instance_512_14.jpg",
                "caption": "Figure 13:Visual comparison of different VOS methods",
                "position": 2235
            },
            {
                "img": "https://arxiv.org/html/2601.08831/figs/fig-vis/acd95847c5/pred_masks_instance_512_33.jpg",
                "caption": "",
                "position": 2239
            },
            {
                "img": "https://arxiv.org/html/2601.08831/figs/fig-vis/acd95847c5/pred_masks_instance_512_43.jpg",
                "caption": "Figure 14:Visual comparison of different VOS methods",
                "position": 2245
            },
            {
                "img": "https://arxiv.org/html/2601.08831/figs/fig-vis/acd95847c5/pred_masks_instance_512_59.jpg",
                "caption": "",
                "position": 2249
            },
            {
                "img": "https://arxiv.org/html/2601.08831/figs/3D-Visual/1_1.png",
                "caption": "Figure 15:Visual results on class-agnostic instance segmentation.",
                "position": 2270
            },
            {
                "img": "https://arxiv.org/html/2601.08831/figs/3D-Visual/1_2.png",
                "caption": "",
                "position": 2276
            },
            {
                "img": "https://arxiv.org/html/2601.08831/figs/3D-Visual/1_3.png",
                "caption": "",
                "position": 2277
            },
            {
                "img": "https://arxiv.org/html/2601.08831/figs/3D-Visual/2_1.png",
                "caption": "",
                "position": 2280
            },
            {
                "img": "https://arxiv.org/html/2601.08831/figs/3D-Visual/2_2.png",
                "caption": "",
                "position": 2281
            },
            {
                "img": "https://arxiv.org/html/2601.08831/figs/3D-Visual/2_3.png",
                "caption": "",
                "position": 2282
            },
            {
                "img": "https://arxiv.org/html/2601.08831/figs/3D-Visual/3_1.png",
                "caption": "",
                "position": 2285
            },
            {
                "img": "https://arxiv.org/html/2601.08831/figs/3D-Visual/3_2.png",
                "caption": "",
                "position": 2286
            },
            {
                "img": "https://arxiv.org/html/2601.08831/figs/3D-Visual/3_3.png",
                "caption": "",
                "position": 2287
            }
        ]
    },
    {
        "header": "Appendix EQualitative Comparison",
        "images": []
    }
]