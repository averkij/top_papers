[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05040/seed/git.png",
                "caption": "",
                "position": 89
            },
            {
                "img": "https://arxiv.org/html/2510.05040/x1.png",
                "caption": "Figure 1:Overview of our proposed HEX framework.Left: HEX leverages multiple semi-autoregressive hidden experts, guided by different masking schedules, to produce concatenated outputs and a final answer.Right: HEX outperforms Top-K, Top-K margin[1]and Random expert selection strategies[2]on reasoning tasks (GSM8K, MATH, ARC-C), surpassing the training-based GRPO baseline (d1)[3].",
                "position": 117
            },
            {
                "img": "https://arxiv.org/html/2510.05040/x2.png",
                "caption": "",
                "position": 125
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Problem Formulation",
        "images": []
    },
    {
        "header": "4Limitations of SoTA and Our Key Insight",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05040/x3.png",
                "caption": "Figure 2:Random vs. Top-K margin inference on GSM8K.Left: Random decoding achieves 50.87% accuracy, whileRight: Top‑K margin only 24.72%. For each method, the text box shows the result at the last unmasking step. Top-K margin generates output tokens in reverse, from the end toward the beginning, and exhibits a catastrophic collapse in which all tokens are [AfterEoT] (shown in red). Over 55.5% of top‑K margin runs suffered this collapse, yielding very low accuracy. These failures cast doubt on methods that rely solely on token confidence.",
                "position": 261
            },
            {
                "img": "https://arxiv.org/html/2510.05040/x4.png",
                "caption": "Figure 3:The distribution of the 4th token‘Bell’in the output sequence changes significantly depending on the232^{3}masking conditions applied to the previous three tokens:‘The’,‘inventor’,‘was’. The star mark indicates the highest confidence for each distribution generated byUU. Some masking conditions (violetandgreen) produce collapsed distribution:\"Bell Bell was invented.\"(ungrammatical sentence),\"The telephone was invented.\"(missing target information), respectively.",
                "position": 303
            }
        ]
    },
    {
        "header": "5Hidden Semi-autoregressive Experts for Test-time Scaling",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05040/x5.png",
                "caption": "Figure 4:When asked about the 2024 Turing Award winners, names other than the actual recipients (such as Michael or David) might be generated due to different block sizes, which in turn risks producing incorrect information in the subsequent token sequence. However, if we generate outputs with various block sizes and then select the most frequently produced answer, that answer is more likely to be correct, since it was probably derived through a valid reasoning (Andrew) during the inference process.",
                "position": 435
            }
        ]
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05040/x6.png",
                "caption": "Figure 5:HEX improves reasoning accuracy.On LLaDA-8B-Instruct, HEX outperforms training-free baselines (Random, Top-kk, Top-kk-margin) on GSM8K, MATH, ARC-C, and TruthfulQA. In GSM8K, MATH, ARC-C, it even outperforms the model trained with GRPO without any training.",
                "position": 463
            },
            {
                "img": "https://arxiv.org/html/2510.05040/x7.png",
                "caption": "Figure 6:As the number of majority voting samples in HEX increases, accuracy improves and the tie rate decreases. The block sizes used are [8, 16, 32, 64, 128], and sampling was performed while increasing the number of seeds (1-6).",
                "position": 485
            }
        ]
    },
    {
        "header": "7Conclusion and Limitation",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AQualitative results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05040/x8.png",
                "caption": "Figure 7:An instance of generated text responses of different decoding strategies.",
                "position": 1313
            },
            {
                "img": "https://arxiv.org/html/2510.05040/x9.png",
                "caption": "Figure 8:Blue denotes mask tokens, red denotes [AfterEoT] tokens, white denotes text tokens, and green denotes [EoT] tokens (note that in the LLaDA-8-Instruct model, [EoT] and [AfterEoT] are represented as<|e​o​t​_​i​d|><|eot\\_id|>and<|e​n​d​o​f​t​e​x​t|><|endoftext|>, respectively[28]). As unmasking proceeds, two mask tokens are unmasked at each step (output length = 256, unmasking steps = 128). Under a semi-AR regime with block size = 32, positional constraints force reasoning to progress left-to-right while still allowing diffusion-style generation within each block. By contrast, when the positional constraint is removed with block size = 256 (non-semi-AR), the model starts from the last token with the highest confidence—[AfterEoT]—and, due to the inertia of repeatedly generating the same token backward, ultimately collapses into a catastrophic output in which all tokens become [AfterEoT].",
                "position": 1329
            },
            {
                "img": "https://arxiv.org/html/2510.05040/x10.png",
                "caption": "Figure 9:Few cases (2.96%) where the block size of output length hits and semi-AR fails. However, in those cases, the unmasking order converges from both ends toward the center. It means the model\ncommits to an answer before engaging in a full reasoning process—i.e., it states the answer first and\nonly then provides a derivation. This is not only unintuitive;[26]shows\nthat the answer the dLLM settles on canfliprepeatedly during the reasoning process. Consequently,\nwhen the model locks in an answer first, it is hard to claim that it truly “knows” the answer with\ncertainty.",
                "position": 1333
            },
            {
                "img": "https://arxiv.org/html/2510.05040/x11.png",
                "caption": "Figure 10:Examples of the block sizes and counts used in the dynamic HEX block settings. Block sizes and counts were randomly\nchosen and adjusted to match the total output length. The output length is 256 and the number of unmasking steps is 128, meaning that each step unmasks 2 tokens. Accordingly, all block sizes are multiples of 2, and decoding was performed in a semi-autoregressive manner.",
                "position": 1396
            },
            {
                "img": "https://arxiv.org/html/2510.05040/x12.png",
                "caption": "Figure 11:Accuracy across tasks when the block size (B) is increased in fixed increments of 16. Here, block count = (256 // B) when 256 % B is 0 otherwise (256 // B) + 1. Despite the uniform increase in block size, the performance drops drastically in the 128–256 range.",
                "position": 1468
            },
            {
                "img": "https://arxiv.org/html/2510.05040/x13.png",
                "caption": "Figure 12:Visualization of the correct (blue) and incorrect (white) answers for each data point in GSM8K when reasoning with various block sizes. Apart from the case where the block size is equal to the output size, i.e., non–semi-AR generation (256), there is no consistent pattern across block sizes in this figure that would allow us to conclude which block size is most suitable for semi-AR reasoning for given tasks. However, the observation that it is rare for all semi-AR outputs to fail on a given data point suggests that the model has the potential to arrive at the correct answer even without additional training, simply by selecting the appropriate decoding policy for the question.",
                "position": 1527
            },
            {
                "img": "https://arxiv.org/html/2510.05040/x14.png",
                "caption": "Figure 13:For the same input prompt (blue), the output (orange) varies depending on the block size, because the accumulated context changes at each unmasking step of the reasoning process. Refer to Figure4.",
                "position": 1531
            }
        ]
    },
    {
        "header": "Appendix BAdditional Details, Examples, and Results Which Can be Useful",
        "images": []
    }
]