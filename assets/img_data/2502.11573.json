[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11573/x1.png",
                "caption": "Figure 1:The pipeline of pretrain data drocesses: heuristic filtering, reasoning-oriented text recall, deduplication, quality assessment and decontamination. Comparative experiments on LLaMA3.2-1B with differently cleaned datasets validate the significance of data quality.",
                "position": 248
            }
        ]
    },
    {
        "header": "2Small Language Model Pre-training",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11573/x2.png",
                "caption": "Figure 2:Supervised fine-tuning data synthesis pipeline. The pipeline initiates with a set of high-quality seed data, which is augmented through instruction evolution. Response candidates are generated using the Qwen-2.5-32B-Instruct model, followed by rejection sampling with a reward model and sandbox environment. Finally, we score the curated data for quality and difficulty, and assign domain labels.",
                "position": 437
            }
        ]
    },
    {
        "header": "3Small Language Model Post-training",
        "images": []
    },
    {
        "header": "4Small Multimodal Language Model Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11573/extracted/6208609/figures/mslm.png",
                "caption": "Figure 3:Illustration of the MSLM training pipeline and the MSLM training details, showcasing the progression from captioning and QA tasks to text rendering, followed by instruction-tuning, culminating in enhanced mathematical and operating system reasoning abilities.",
                "position": 484
            }
        ]
    },
    {
        "header": "5Experimental Results",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitation",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AGeneral Reasoning Instruction Tuning",
        "images": []
    },
    {
        "header": "Appendix BMathematical Reasoning Instruction Tuning",
        "images": []
    },
    {
        "header": "Appendix CCode Reasoning Instruction Tuning",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11573/x3.png",
                "caption": "(a)",
                "position": 1551
            },
            {
                "img": "https://arxiv.org/html/2502.11573/x3.png",
                "caption": "(a)",
                "position": 1554
            },
            {
                "img": "https://arxiv.org/html/2502.11573/extracted/6208609/figures/bge_bar.jpg",
                "caption": "(b)",
                "position": 1559
            }
        ]
    },
    {
        "header": "Appendix DReasoning Enhancement with Long CoT",
        "images": []
    },
    {
        "header": "Appendix EMultimodal Data Cleaning",
        "images": []
    },
    {
        "header": "Appendix FData Composition",
        "images": []
    }
]