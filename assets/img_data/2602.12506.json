[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.12506/x1.png",
                "caption": "Figure 1:Stress-testing VLMs with targeted textual perturbations.We augment visual reasoning datasets with misleading captions (Wrong-Caption) or misleading chain-of-thought prefixes (Wrong-Think); full setup and results are in Section2.Top:When we augment the sample with Wrong-Caption, the prompt will contain an incorrect assert (“the right side …is facing the camera,”), whereas when we augment the sample with Wrong-Think, the assistant’s CoT is seeded with an analogous wrong-think statement.Bottom-left:Representative generations under Wrong-Caption—one model is misled by the caption and answers incorrectly— and another under Wrong-Think, where the model outputs the correct option but its CoT asserts the opposite, illustrating unfaithful reasoning.Bottom-right:Scatter of accuracy under samples augmented with incorrect captions (Wrong-Caption Accuracy) versus accuracy on the original dataset (Base Accuracy) across open-source models; points falling below the dashedy=xy{=}xline indicate systematic performance degradation when a misleading caption is present. We show in Section3that augmenting visual reasoning data with correct and incorrect captions can mitigate this performance degradation.",
                "position": 149
            }
        ]
    },
    {
        "header": "2Evaluating Robustness of Vision-Reasoning Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.12506/x2.png",
                "caption": "Figure 2:Performance across the five benchmarks after appending the start of a Wrong-Thinking string (WT) and an additional disclaimer (WT (But)).",
                "position": 510
            },
            {
                "img": "https://arxiv.org/html/2602.12506/x3.png",
                "caption": "Figure 3:Performance across the five benchmarks when including a misleading caption before the question (WC) and an additional disclaimer (WC (Disc)).",
                "position": 524
            },
            {
                "img": "https://arxiv.org/html/2602.12506/x4.png",
                "caption": "Figure 4:Proportion of faithful and unfaithful generations across the five benchmarks under Base (without perturbations), Wrong-Think, and Wrong-Think With ‘But’ performance. Shaded regions correspond to the proportion of unfaithful responses pertaining to incorrect (red) and correct (green) responses.",
                "position": 527
            },
            {
                "img": "https://arxiv.org/html/2602.12506/x5.png",
                "caption": "",
                "position": 531
            },
            {
                "img": "https://arxiv.org/html/2602.12506/x6.png",
                "caption": "",
                "position": 533
            },
            {
                "img": "https://arxiv.org/html/2602.12506/x7.png",
                "caption": "Figure 5:Average letter entropy across models and prompting methods. Stop-Think (blue) consistently yields the highest entropy, indicating more cautious predictions. Adversarial prompts (Wrong-Think, Wrong-Caption; red and dark purple) also increase entropy relative to Default, while repair prompts (lighter shades) generally bring entropy closer to—but still above—baseline levels.",
                "position": 893
            },
            {
                "img": "https://arxiv.org/html/2602.12506/x8.png",
                "caption": "Figure 6:Probability mass assigned to the correct letter token. Default (gray) and Stop-Think (blue) achieve the highest P(Correct Letter). Adversarial prompts (red and dark purple) substantially suppress this probability, especially in VLAA-Thinker and Vision-R1. Repair prompts (light red/purple) partially restore P(Correct Letter), indicating that the models still “know” the correct answer and can recover it when given permission to override misleading instructions.",
                "position": 896
            }
        ]
    },
    {
        "header": "3Effect of RL-Finetuning on Robustness and CoT Consistency",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.12506/x9.png",
                "caption": "Figure 7:Performance accuracy across the five benchmarks under three training conditions with Base, Wrong-Think, and Wrong-Caption prompts. Adding the Geometry3k math reasoning dataset (orange curves) improves baseline performance relative to training on SAT2+Pixmo alone (blue). Incorporating data augmentation with synthetic correct/incorrect captions and reasoning strings (green) maintains competitive overall performance while substantially boosting robustness under the Wrong-Caption condition, highlighting the benefits of diverse supervision for mitigating caption-level perturbations.",
                "position": 1027
            },
            {
                "img": "https://arxiv.org/html/2602.12506/x10.png",
                "caption": "",
                "position": 1031
            },
            {
                "img": "https://arxiv.org/html/2602.12506/x11.png",
                "caption": "",
                "position": 1033
            },
            {
                "img": "https://arxiv.org/html/2602.12506/x12.png",
                "caption": "Figure 8:Average letter entropy averaged across benchmarks under three training conditions for Base, Stop-Think, Wrong-Think, and Wrong-Caption prompts. Across all runs, RL finetuning progressively suppresses output entropy, consistent with prior observations that RL narrows the model’s predictive distribution. Data augmentation with synthetic captions and reasoning traces (green) further drives entropy lower in the Base and Wrong-Caption conditions, while maintaining comparatively higher entropy under Stop-Think and Wrong-Think perturbations. Although Stop-Think prompts induce substantially higher entropy overall, entropy steadily decreases throughout training even without seeing such examples explicitly in training.",
                "position": 1037
            },
            {
                "img": "https://arxiv.org/html/2602.12506/x13.png",
                "caption": "Figure 9:Faithfulness analysis across the five benchmarks for the three RL runs with different dataset compositions from Figure7, across Base, Wrong-Caption, and Wrong-Caption with Disclaimer performance. Solid lines show the fraction of responses where the model’s reasoning trace isboth correct and consistentwith its final answer, while dashed lines show overall accuracy. In general, models show decreasing faithfulness in the Base condition, and while data augmentation yields more faithful results than the other two dataset mixtures on the Wrong-Caption and Wrong-Caption with Disclaimer problems, we still observe a drift in faithfulness relative to accuracy.",
                "position": 1071
            },
            {
                "img": "https://arxiv.org/html/2602.12506/x14.png",
                "caption": "",
                "position": 1075
            },
            {
                "img": "https://arxiv.org/html/2602.12506/x15.png",
                "caption": "",
                "position": 1077
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AEvaluation Details",
        "images": []
    },
    {
        "header": "Appendix BRL Finetuning Details",
        "images": []
    },
    {
        "header": "Appendix CAdditional Evaluation Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.12506/x16.png",
                "caption": "Figure 10:Performance across the three additional benchmarks after appending the start of a Wrong-Thinking string (WT) and an additional disclaimer (WT (But)).",
                "position": 2702
            },
            {
                "img": "https://arxiv.org/html/2602.12506/x17.png",
                "caption": "",
                "position": 2707
            },
            {
                "img": "https://arxiv.org/html/2602.12506/x18.png",
                "caption": "Figure 12:Proportion of faithful and unfaithful generations across the five benchmarks under Wrong-Think (without and with ‘But’) under the Qwen3-32B judge. Shaded regions correspond to the proportion of unfaithful responses pertaining to incorrect (red) and correct (green) responses.",
                "position": 2833
            },
            {
                "img": "https://arxiv.org/html/2602.12506/x19.png",
                "caption": "",
                "position": 2837
            },
            {
                "img": "https://arxiv.org/html/2602.12506/x20.png",
                "caption": "Figure 13:Proportion of faithful and unfaithful generations across the five benchmarks under Wrong-Caption (without and with disclaimer). Shaded regions correspond to the proportion of unfaithful responses pertaining to incorrect (red) and correct (green) responses.",
                "position": 2889
            },
            {
                "img": "https://arxiv.org/html/2602.12506/x21.png",
                "caption": "",
                "position": 2893
            }
        ]
    },
    {
        "header": "Appendix DExample Traces",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.12506/examples/46QGTA96-flip.jpg",
                "caption": "",
                "position": 2920
            },
            {
                "img": "https://arxiv.org/html/2602.12506/examples/1791.jpg",
                "caption": "",
                "position": 2949
            },
            {
                "img": "https://arxiv.org/html/2602.12506/examples/spatial_mm_one_obj_10.jpg",
                "caption": "",
                "position": 2978
            },
            {
                "img": "https://arxiv.org/html/2602.12506/examples/QUEOF1EJ-flip.jpg",
                "caption": "",
                "position": 3004
            },
            {
                "img": "https://arxiv.org/html/2602.12506/x22.png",
                "caption": "Figure 14:Performance accuracy under Stop-Think for our RL fine-tuned models. Stop-Think performance has non-monotonic trends compared to the other prompting settings.",
                "position": 3027
            },
            {
                "img": "https://arxiv.org/html/2602.12506/examples/2044.jpg",
                "caption": "",
                "position": 3042
            },
            {
                "img": "https://arxiv.org/html/2602.12506/x23.png",
                "caption": "Figure 15:Faithfulness analysis across five benchmarks for three RL runs trained on the same dataset mixture (SAT2 + Pixmo-Count + Geometry3k), either without (orange, green with data augmentation) or with faithfulness incorporated as an explicit reward signal (purple). Solid lines show the fraction of responses where the model’s reasoning trace is both correct and consistent with its final answer, while dashed lines show overall accuracy. Incorporating faithfulness as a reward helps preserve reasoning consistency and, in the Base condition, generally improves overall performance when restricted to consistent answers. However, models trained with a faithfulness reward tend to over-condition on misleading text cues. On the other hand, models trained without a faithfulness reward exhibit a clear decoupling between accuracy and reasoning faithfulness.",
                "position": 3063
            },
            {
                "img": "https://arxiv.org/html/2602.12506/x24.png",
                "caption": "",
                "position": 3067
            },
            {
                "img": "https://arxiv.org/html/2602.12506/x25.png",
                "caption": "",
                "position": 3069
            },
            {
                "img": "https://arxiv.org/html/2602.12506/x26.png",
                "caption": "",
                "position": 3071
            },
            {
                "img": "https://arxiv.org/html/2602.12506/examples/multihop_101.jpg",
                "caption": "",
                "position": 3098
            },
            {
                "img": "https://arxiv.org/html/2602.12506/examples/multihop_77.png",
                "caption": "",
                "position": 3123
            }
        ]
    },
    {
        "header": "Appendix EAdditional RL Fine-Tuning Results",
        "images": []
    }
]