[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18940/x1.png",
                "caption": "Figure 1:Overview of NeuroAda. For each neuron, top-11weights are adapted, while the rest remain frozen. Bold dark indicates selected pretrained weights; red dashed edges represent newly introduced trainable parameters.",
                "position": 236
            },
            {
                "img": "https://arxiv.org/html/2510.18940/x2.png",
                "caption": "Figure 2:Mask-based sparse tuning employs a binary mask matrix to suppress gradient updates for unselected parameters. However, this approach incurs significant memory overhead, as gradients for the entire original parameter matrix must still be computed and retained by the optimizer.",
                "position": 249
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18940/x3.png",
                "caption": "Figure 3:Neuron-wise Top-kkWeight Selection and Gradient Computation.\n(a) Pretrained weight matrix of sizedo​u​t×di​nd_{out}\\times d_{in}, where for each neuron (row), only the top-kkweights(i.e., highest-magnitude) are selected for adaptation (colored), and the rest remain frozen (white).\n(b) Corresponding gradient matrix restricted to the top-kkweights per neuron (herek=1k=1), showing gradients only for trainable entries.\nThis strategy enables fine-grained, neuron-level adaptation while preserving most of the pretrained model, effectively activating each neuron’s potential through less-invasive tuning.",
                "position": 302
            }
        ]
    },
    {
        "header": "4Neuron-wise Sparse Adaptation: Comparative Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18940/x4.png",
                "caption": "Figure 4:Performance comparison between our NeuroAda and mask-based methods on LLaMA-7B. Top-kkmeans selecting top-kkinput connections per neuron in the neural network.",
                "position": 519
            },
            {
                "img": "https://arxiv.org/html/2510.18940/x5.png",
                "caption": "",
                "position": 523
            },
            {
                "img": "https://arxiv.org/html/2510.18940/x6.png",
                "caption": "",
                "position": 524
            },
            {
                "img": "https://arxiv.org/html/2510.18940/x7.png",
                "caption": "Figure 5:Training GPU memory and training efficiency on different models (RoBERTa-base, RoBERTa-large, LLaMA-7B, LLaMA3-8B) with NeuroAda, mask-based and full fine-tuning method.",
                "position": 547
            },
            {
                "img": "https://arxiv.org/html/2510.18940/x8.png",
                "caption": "",
                "position": 551
            },
            {
                "img": "https://arxiv.org/html/2510.18940/x9.png",
                "caption": "",
                "position": 552
            },
            {
                "img": "https://arxiv.org/html/2510.18940/x10.png",
                "caption": "Figure 6:Comparison across different proportions of neurons involved in the fine-tuning process.",
                "position": 574
            },
            {
                "img": "https://arxiv.org/html/2510.18940/x11.png",
                "caption": "",
                "position": 577
            },
            {
                "img": "https://arxiv.org/html/2510.18940/x12.png",
                "caption": "Figure 7:Comparison of different parameter selection strategies for involving neurons for the fine-tuning process.\nAmong all input connections for each neuron in the network, Top-kkconnection with highest magnitude (Magnitude), highest gradient absolute value (Gradient), lowest magnitude (Reverse) are selected for training using addition-based method.Randommeans randomly selecting Top-kkinput connections per neuron.",
                "position": 599
            },
            {
                "img": "https://arxiv.org/html/2510.18940/x13.png",
                "caption": "",
                "position": 602
            },
            {
                "img": "https://arxiv.org/html/2510.18940/x14.png",
                "caption": "",
                "position": 603
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADatasets",
        "images": []
    },
    {
        "header": "Appendix BResults on natural language understanding tasks",
        "images": []
    },
    {
        "header": "Appendix CHyperparameters",
        "images": []
    },
    {
        "header": "Appendix DAdvantages of NeuroAda",
        "images": []
    },
    {
        "header": "Appendix EAlgorithm",
        "images": []
    }
]