[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01624/x1.png",
                "caption": "",
                "position": 90
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01624/x2.png",
                "caption": "Figure 2:PISCEST2V Post-Training.We introduce a Dual OT-aligned Rewards module: (i) a distributional OT mapùêì‚ãÜ\\mathbf{T}^{\\star}for Quality Reward via[CLS]representation similarity, and (ii) a discrete OT planùêè‚ãÜ\\mathbf{P}^{\\star}with spatio-temporal constraints for Semantic Reward via a Video-Text Matching (VTM) classifier. The rewards module provides supervision for fine-tuning the T2V denoiser and is applicable with direct backpropagation and RL fine-tuning (GRPO).",
                "position": 155
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01624/x3.png",
                "caption": "Figure 3:Human preference study.PISCESoutperforms HunyuanVideo, T2V-Turbo-v2, VideoReward-DPO in visual quality, motion and semantic alignment, validating its effectiveness in T2V.",
                "position": 634
            },
            {
                "img": "https://arxiv.org/html/2602.01624/x4.png",
                "caption": "Figure 4:Qualitative comparison of T2V models.PISCESproduces videos with better semantic fidelity and visual quality, accurately capturing key details such as the reflective wet pavement and vibrant neon lighting.",
                "position": 641
            },
            {
                "img": "https://arxiv.org/html/2602.01624/x5.png",
                "caption": "Figure 5:Cross-attention maps (left) are diffuse, OT plan without spatio-temporal constraints (middle) misaligns tokens, while our constrained OT plan (right) produces accurate correspondences.",
                "position": 743
            },
            {
                "img": "https://arxiv.org/html/2602.01624/x6.png",
                "caption": "Figure 6:t-SNE shows OT aligns text embeddings (green) closer to video embeddings distribution (orange). Pairwise distance distribution indicates OT preserves text embedding structure.",
                "position": 839
            },
            {
                "img": "https://arxiv.org/html/2602.01624/x7.png",
                "caption": "Figure 7:Post-training with OT-aligned rewards (left) yields consistent outputs with expected color changes, while L2 loss mapping (right) causes sampling inconsistencies and visual artifacts.",
                "position": 842
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AConsistency Distillation",
        "images": []
    },
    {
        "header": "Appendix BDistributional Alignment with OT",
        "images": []
    },
    {
        "header": "Appendix CDiscrete OT for Semantic Alignment",
        "images": []
    },
    {
        "header": "Appendix DAutomatic Evaluation with Different Optimization Paradigms",
        "images": []
    },
    {
        "header": "Appendix EComprehensive Ablation Study",
        "images": []
    },
    {
        "header": "Appendix FOptimal Transport Plan Analysis",
        "images": []
    },
    {
        "header": "Appendix GMotion Guidance in T2V Post-Training",
        "images": []
    },
    {
        "header": "Appendix HPISCESwith ViCLIP Evaluator",
        "images": []
    },
    {
        "header": "Appendix IScope of Generalization",
        "images": []
    },
    {
        "header": "Appendix JHyperparameters Sensitivity Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01624/x8.png",
                "caption": "Figure 8:VTM accuracy under(Œ≥,Œ∑)(\\gamma,\\eta)sweep. Despite configuration variations, the accuracy remains stable (¬±\\pm1.48% std).",
                "position": 2447
            }
        ]
    },
    {
        "header": "Appendix KFailure Example of Discrete Partial OT",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01624/x9.png",
                "caption": "Figure 9:Effect of OT mass parameter.Low OT mass (m=0.5m=0.5) fails to retain the‚Äúglasses‚Äùtoken, while higher mass (m=0.9m=0.9) improves alignment.",
                "position": 2457
            }
        ]
    },
    {
        "header": "Appendix LHuman Preference Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01624/x10.png",
                "caption": "Figure 10:Reward and gradient trends in post-training.",
                "position": 2473
            }
        ]
    },
    {
        "header": "Appendix MReward Fusion",
        "images": []
    },
    {
        "header": "Appendix NDual Rewards Interaction",
        "images": []
    },
    {
        "header": "Appendix OReward Hacking and Mitigation Strategy",
        "images": []
    },
    {
        "header": "Appendix PTraining Cost and Efficiency Analysis",
        "images": []
    }
]