[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10741/x1.png",
                "caption": "Figure 1.Aesthetic posters generated by PosterCraft demonstrate that backgrounds, layouts, and typographic designs are produced directly from textual input without modular designs, highlighting its ability to employ a unified framework to generate posters with visual consistency and compelling aesthetic appeal.",
                "position": 74
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10741/x2.png",
                "caption": "Figure 2.The four datasets of PosterCraftacross its four stages: (1) Text-Render-2M for text rendering optimization in the initial phase, (2) HQ-Poster-100K, comprising over 100K high-quality posters with masks and captions, (3) Poster-Preference-100K, yielding 6K high-quality preference pairs from 100K generated samples, and (4) Poster-Reflect-120K, constructing 64K feedback pairs from 120K generated posters.",
                "position": 162
            },
            {
                "img": "https://arxiv.org/html/2506.10741/x3.png",
                "caption": "Figure 3.Comparison of text rendering on poster typography, plain-text scenes, and long-form text posters. Each pair shows the Flux.1 dev baseline (left), exhibiting missing, repeated, or error text, alongside the optimized output (right) after our scalable text rendering optimization, demonstrating marked gains in text fidelity, alignment, and accuracy.",
                "position": 165
            }
        ]
    },
    {
        "header": "2.Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10741/x4.png",
                "caption": "Figure 4.The pipeline of PosterCraft, which has four stages: (1) Text Rendering Optimization to improve text accuracy and fidelity; (2) High-Quality Poster Fine-Tuning with region-aware calibration to poster styling across text and non-text regions; (3) Aesthetic-Text Reinforcement Learning to instill detailed aesthetic and content preferences; and (4) Joint Visionâ€“Language Feedback, integrating multimodal reflections for refined outputs. At inference, the fine-tuned model generates high-quality aesthetic posters end-to-end from a single prompt, with an optional VLM-driven critique loop.",
                "position": 188
            }
        ]
    },
    {
        "header": "3.Unified Workflow and Specific Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10741/x5.png",
                "caption": "Figure 5.User study comparisonsbetween PosterCraft and both SOTA open-source and closed-source models. PosterCraft consistently outperforms all open-source baselines and several proprietary systems cross multiple dimensions, with performance marginally below that of the leading closed-source model, Gemini2.0-Flash-Gen.",
                "position": 380
            },
            {
                "img": "https://arxiv.org/html/2506.10741/x6.png",
                "caption": "Figure 6.Gemini serves as an authoritative evaluator for human preference comparisonsacross our method and other baselines. PosterCraft outperforms most state-of-the-art generative models in aesthetic coherence, prompt alignment, text rendering, and overall preference. It achieves performance nearly on par with the leading commercial model Gemini2.0-Flash-Gen in text rendering, while showing only a slight gap in other aspects.",
                "position": 383
            },
            {
                "img": "https://arxiv.org/html/2506.10741/x7.png",
                "caption": "Figure 7.Visual comparisonof different model outputs.Red boxeshighlight misspelled or distorted text, whileyellow boxesindicate redundant or missing text elements. Within the prompts,orange textdenotes content and style requirements, andred textindicates textual elements. From the visual comparison, it is evident that our method achieves superior aesthetic appeal compared to existing state-of-the-art approaches. In terms of text presentation, our rendered fonts blend more naturally with the scene content, and text rendering errors are nearly eliminated.",
                "position": 386
            },
            {
                "img": "https://arxiv.org/html/2506.10741/x8.png",
                "caption": "Figure 8.Visual comparisonof different model outputs.Red boxeshighlight misspelled or distorted text, whileyellow boxesindicate redundant or missing text elements. Within the prompts,orange textdenotes content and style requirements, andred textindicates textual elements. It indicates that our method significantly outperforms existing SOTA approaches in generating high-quality posters under long-prompt conditions, with notably improved prompt alignment. In terms of text rendering, our model produces fonts that align closely with the visual context of the scene, with minimal rendering errors.",
                "position": 389
            }
        ]
    },
    {
        "header": "4.Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10741/x9.png",
                "caption": "Figure 9.Visual comparisonof different model outputs.Red boxeshighlight misspelled or distorted text, whileyellow boxesindicate redundant or missing text elements. Within the prompts,orange textdenotes content and style requirements, andred textindicates textual elements. Compared to other methods, our approach produces cleaner layouts, better theme alignment, and more accurate text rendering under short prompts.",
                "position": 403
            },
            {
                "img": "https://arxiv.org/html/2506.10741/x10.png",
                "caption": "Figure 10.Aesthetic-text reinforcement learning (top row) and vision-language feedback (bottom row) qualitative comparisons.orange boxesdenote the text biases. Different color fonts represent key feedback information from VLM. The top examples demonstrate that our reinforcement learning stage effectively improves overall aesthetic quality and partially corrects text rendering errors after supervised fine-tuning. The bottom examples illustrate the impact of vision-language reflection, where feedback from a VLM is integrated into the generation loop. This results in noticeable enhancements to both visual aesthetics and semantic coherence in the final poster outputs.",
                "position": 406
            }
        ]
    },
    {
        "header": "5.Ablation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10741/x11.png",
                "caption": "Figure 11.Ablation experiments on the core components of our workflow: removing Text Rendering Optimization, Region-aware Calibration, Aesthetic-Text RL, or Reflection leads to sustained declines in OCR accuracy (purple) and human preference (peach), demonstrating the effectiveness of our overall design, targeted optimizations and validating our motivation.",
                "position": 524
            }
        ]
    },
    {
        "header": "6.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7.Text-Render-2M Construction Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10741/x12.png",
                "caption": "Figure 12.More samples are shown, which are high-quality paired samples in Text-Render-2M.",
                "position": 1308
            }
        ]
    },
    {
        "header": "8.Automatic Processing Pipeline for HQ-Poster-100K",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10741/x13.png",
                "caption": "Figure 15.More pictures are shown, which are high-quality paired samples with masks in HQ-Poster-100K.",
                "position": 1377
            }
        ]
    },
    {
        "header": "9.Explanations of the Poster-Preference-100K",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10741/x14.png",
                "caption": "Figure 18.Additional Preference Pairsin Poster-Preference-100K. The images on the left are Rejected Samples, while those on the right are Preferred Samples.Orange textindicates textual content, andred textcorresponds to content, style, or layout requirements.",
                "position": 1625
            }
        ]
    },
    {
        "header": "10.Illustration of the Poster-Reflect-120K",
        "images": []
    },
    {
        "header": "11.Gemini for OCR calculation and preference evaluator",
        "images": []
    },
    {
        "header": "12.Additional Visual Examples",
        "images": []
    },
    {
        "header": "13.Limitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10741/x15.png",
                "caption": "Figure 25.Examples generated by our PosterCraftdemonstrating high diversity and aesthetic quality across themes including education, entertainment, and science fiction. All generation results showcase genre-specific fidelity, text rendering, and layout aesthetic.",
                "position": 2493
            },
            {
                "img": "https://arxiv.org/html/2506.10741/x16.png",
                "caption": "Figure 26.Examples generated by our PosterCraftdemonstrating high diversity and aesthetic quality across themes including movies, product, and virtual reality. All generation results showcase genre-specific fidelity, text rendering, and layout aesthetic.",
                "position": 2496
            }
        ]
    },
    {
        "header": "14.Future work",
        "images": []
    }
]