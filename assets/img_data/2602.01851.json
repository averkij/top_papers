[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01851/x1.png",
                "caption": "Figure 1:Motivation and scope of the VIBE benchmark.Traditional image editing is largely text-guided, where conveying spatial intent relies on verbose descriptions and incurs high cognitive load.\nIn contrast, visual instructions enable precise and explicit grounding, providing a more human-aligned interaction paradigm.\nVIBE is designed to fill the evaluation gap by systematically benchmarking this visual intruction-guided multi-modal image editing.",
                "position": 159
            }
        ]
    },
    {
        "header": "2VIBE",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01851/x2.png",
                "caption": "Figure 2:Composition of VIBE.VIBE comprises 1,034 samples across 10 tasks, organized into a three-level hierarchy that reflects increasing interaction and reasoning complexity, from deictic grounding and morphological manipulation to causal reasoning.",
                "position": 247
            },
            {
                "img": "https://arxiv.org/html/2602.01851/x3.png",
                "caption": "Figure 3:Overview of VIBE.VIBE organizes visual instruction-guided image editing into a three-level interaction hierarchy with increasing task complexity.\nTheDeicticLevel treats visual instructions as selectors that specify localized regions or objects for basic spatial operations.\nTheMorphologicalLevel interprets visual instructions as blueprints that define abstract structural constraints.\nTheCausalLevel views visual instructions as catalysts that encode underlying physical or logical dynamics.",
                "position": 250
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01851/x4.png",
                "caption": "Figure 4:Performance across image styles on the Deictic Level.Left: Average Deictic Level scores across real-world, animation, and sketch images for four proprietary models.\nRight: Metric-level heatmaps for Seedream 4.5 and GPT-Image-1, illustrating style-dependent variations in Instruction Adherence, Contextual Preservation, and Visual Coherence.",
                "position": 832
            }
        ]
    },
    {
        "header": "4Discussion and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01851/x5.png",
                "caption": "Figure 5:Style-wise performance on Draft Instantiation.",
                "position": 854
            },
            {
                "img": "https://arxiv.org/html/2602.01851/x6.png",
                "caption": "Figure 6:Qualitative examples of multi-task visual instruction following.Examples illustrating model behavior under composed visual instructions.\nIn the first two rows, models correctly execute individual instructions in isolation but fail when the same instructions are combined.\nThe third row shows a case where a model succeeds with two instructions but fails when an additional instruction is introduced.",
                "position": 927
            },
            {
                "img": "https://arxiv.org/html/2602.01851/x7.png",
                "caption": "Figure 7:Pearson correlation between human expert scores and LMM-based evaluation scores for Nano Banana Pro and GPT-Image-1, demonstrating a strong alignment between human judgments and the LMM-as-a-Judge evaluator.",
                "position": 932
            },
            {
                "img": "https://arxiv.org/html/2602.01851/x8.png",
                "caption": "Figure 8:Two representative cases illustrating how textual and visual instructions interact.\nThe first case shows that visual instructions can resolve target ambiguity that detailed text alone fails to address.\nThe second case demonstrates that complex semantic constraints require the joint use of detailed textual and visual instructions.",
                "position": 935
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelated Work",
        "images": []
    },
    {
        "header": "Appendix BData Collection and Annotation",
        "images": []
    },
    {
        "header": "Appendix CAdditional Experiments and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01851/x9.png",
                "caption": "Figure 9:Examples of editing results from Seedream 4.5 across different image styles",
                "position": 1964
            },
            {
                "img": "https://arxiv.org/html/2602.01851/x10.png",
                "caption": "Figure 10:Qualitative incorrect examples on the Deictic and Morphological Level",
                "position": 1973
            },
            {
                "img": "https://arxiv.org/html/2602.01851/x11.png",
                "caption": "Figure 11:Qualitative incorrect examples on the Causal Level",
                "position": 1976
            },
            {
                "img": "https://arxiv.org/html/2602.01851/x12.png",
                "caption": "Figure 12:Qualitative examples with visually embedded instructions.All examples use the same minimal textual prompt, “Edit this image following the instructions annotated on this picture.”\nTask specifications are conveyed through text and symbols embedded directly in the input image.\nNano Banana Pro correctly executes single-task, multi-task, and causal editing operations based on these visually embedded instructions.",
                "position": 2021
            },
            {
                "img": "https://arxiv.org/html/2602.01851/x13.png",
                "caption": "Figure 13:Screenshot of the developed data annotation system used in section4.3.",
                "position": 2043
            }
        ]
    },
    {
        "header": "Appendix DEvaluation",
        "images": []
    }
]