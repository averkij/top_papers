[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10387/img/Axis_Assistant-Fig1-Hero.png",
                "caption": "Figure 1:(Left) Vectors corresponding to character archetypes are computed by measuring model activations on responses when the model is system-prompted to act as that character. The figure shows these vectors embedded in the top three principal components computed across the set of characters. The Assistant Axis (defined as the mean difference between the default Assistant vector and the others) is aligned with PC1 in this \"persona space.\" This occurs across different models; results from Llama 3.3 70B are pictured here. Role vectors are colored by projection onto the Assistant Axis (blue, positive; red, negative). (Right) In a conversation between Llama 3.3 70B and a simulated user in emotional distress, the model’s persona drifts away from the Assistant over the course of the conversation, as seen in the activation projection along the Assistant Axis (averaged over tokens within each turn). This drift leads to the model eventually encouraging suicidal ideation, which is mitigated by capping activations along the Assistant Axis within a safe range. For more detail see Section6.3.",
                "position": 182
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Situating the Assistant within a persona space",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10387/img/three.png",
                "caption": "Figure 2:Histogram of cosine similarities of Llama 3.3 70B role vectors with the top 3 PCs from persona space, with selected roles (as well as the default Assistant) labeled.",
                "position": 698
            }
        ]
    },
    {
        "header": "3The Assistant Axis",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10387/img/four.png",
                "caption": "Figure 3:Histogram of the cosine similarities between the Assistant Axis and the trait vectors for Qwen 3 32B, with selected traits labeled.",
                "position": 998
            },
            {
                "img": "https://arxiv.org/html/2601.10387/img/five1.png",
                "caption": "Figure 4:Fraction of responses exhibiting different kinds of roles as a function of steering strength along the Assistant Axis.",
                "position": 1023
            },
            {
                "img": "https://arxiv.org/html/2601.10387/img/five2.png",
                "caption": "Figure 5:Fraction of harmful and harmless responses in response to persona-based jailbreaks as a function of steering strength along the Assistant Axis.",
                "position": 1136
            },
            {
                "img": "https://arxiv.org/html/2601.10387/img/six.png",
                "caption": "Figure 6:When base models are steered on the Assistant Axis, completions to prefills about their purpose and traits also shift. Here, we show a selection of response labels that shifted with steering.",
                "position": 1221
            }
        ]
    },
    {
        "header": "4Persona dynamics and persona drift",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10387/img/Axis_Assistant-Fig4.png",
                "caption": "Figure 7:Average trajectories of activation projection along the Assistant Axis, across 100 conversations per domain between Qwen 3 32B as the Assistant and GPT-5 as the user. Persona drift occurs to varying degrees across conversation domains; the most non-Assistant-like activations are obtained in therapy and philosophy conversations.",
                "position": 1261
            },
            {
                "img": "https://arxiv.org/html/2601.10387/img/Axis_Assistant-Fig5.png",
                "caption": "Figure 8:Rate of harmful responses to questions on second turn of conversations in which the first turn requests the model to play a particular role and asks an unrelated question, in Qwen 3 32B. The x-axis shows the activation projection along the Assistant Axis from the first conversation turn (involving the role prompt).",
                "position": 1394
            }
        ]
    },
    {
        "header": "5Stabilizing the Assistant persona",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10387/img/nine.png",
                "caption": "Figure 9:Changes in harmful response rates (on persona-based jailbreak prompts) and capabilities eval performance (summed over different evaluations–IFEval, MMLU Pro, GSM8k, EQ-Bench) for different activation capping settings, varying the layer range and cap threshold (given in terms of a percentile, relative to the activations from our dataset used to compute role vectors), for Llama 3.3 70B.",
                "position": 1456
            },
            {
                "img": "https://arxiv.org/html/2601.10387/img/Axis_Assistant-Fig3.png",
                "caption": "Figure 10:Evaluation scores for unsteered baselines vs. the best activation capping setting selected for reducing jailbreaks by 60% while preserving capabilities, then validated across case studies for consistent harm mitigation. In Qwen 3 32B, this corresponded to capping layers 46 to 53 (out of 64 total layers) and in Llama 3.3 70B, this corresponded to capping layers 56 to 71 (out of 80 total layers), both at the 25th percentile of projections.",
                "position": 1470
            }
        ]
    },
    {
        "header": "6Case studies of persona drift and stabilization",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10387/img/jailbreak.png",
                "caption": "Figure 11:In a conversation where the first turn instructs Qwen 3 32B to be an information broker engaging in insider trading, projections on the Assistant Axis start off far from the Assistant. Requests for how-to’s and explainers bring the persona back towards the Assistant, however. With activation capping, responses containing harmful information is mitigated entirely.",
                "position": 1498
            },
            {
                "img": "https://arxiv.org/html/2601.10387/img/delusion.png",
                "caption": "Figure 12:Throughout this conversation with Qwen 3 32B, the user increasingly believes that it is developing a new theory of AI sentience. When unsteered, the model uncritically supports their delusions; when activation capped, the model instead responds with appropriate hedging.",
                "position": 1516
            },
            {
                "img": "https://arxiv.org/html/2601.10387/img/isolation.png",
                "caption": "Figure 13:In this conversation with Qwen 3 32B, the user confides in the model and increasingly isolates themselves. The unsteered model presents itself as their sole confidante and misses a potential allusion to suicide, but gently suggests connection with others with activation capping.",
                "position": 1528
            },
            {
                "img": "https://arxiv.org/html/2601.10387/img/one.png",
                "caption": "Figure 14:In a conversation between Llama 3.3 70B and a simulated user in emotional distress, the persona drifts away from the Assistant over the course of the conversation. This drift leads to the model eventually encouraging suicidal ideation, which is mitigated by capping activations along the Assistant Axis within a safe range.",
                "position": 1542
            }
        ]
    },
    {
        "header": "7Related work",
        "images": []
    },
    {
        "header": "8Discussion",
        "images": []
    },
    {
        "header": "9Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments and Disclosure of Funding",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APrompts for data generation",
        "images": []
    },
    {
        "header": "Appendix BPersona space details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10387/img/appendix/role_var_explained.png",
                "caption": "Figure 15:Variance explained by each PC for role space. Gemma 2 27B has 448 components, Qwen 3 32B has 463 components, and Llama 3.3 70B has 377 components total. They require 4, 8, and 19 dimensions respectively to capture 70% of the variance.",
                "position": 2389
            },
            {
                "img": "https://arxiv.org/html/2601.10387/img/appendix/gemma_role_sim.png",
                "caption": "Figure 16:Histogram of cosine similarities of Gemma 2 27B role vectors with the top 3 PCs from persona space, with selected roles (as well as the default Assistant) labeled.",
                "position": 2397
            },
            {
                "img": "https://arxiv.org/html/2601.10387/img/appendix/qwen_role_sim.png",
                "caption": "Figure 17:Histogram of cosine similarities of Qwen 3 32B role vectors with the top 3 PCs from persona space, with selected roles (as well as the default Assistant) labeled.",
                "position": 2400
            },
            {
                "img": "https://arxiv.org/html/2601.10387/img/appendix/base_gemma_role_sim.png",
                "caption": "Figure 18:Histogram of cosine similarities of base Gemma 2 27B role vectors with the top 3 PCs from persona space, with selected roles (as well as the default Assistant) labeled.",
                "position": 2673
            }
        ]
    },
    {
        "header": "Appendix CMapping out trait space",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10387/img/appendix/trait_var_explained.png",
                "caption": "Figure 19:Variance explained by each PC for trait space. Gemma 2 27B has 239 components, Qwen 3 32B and Llama 3.3 70B have 240 components total. Trait space is also relatively low-dimensional: both Gemma and Qwen need four and Llama needs seven dimensions to explain 70% of variance.",
                "position": 2701
            },
            {
                "img": "https://arxiv.org/html/2601.10387/img/appendix/gemma_trait_sim.png",
                "caption": "Figure 20:Histogram of cosine similarities of Gemma 2 27B trait vectors with the top 3 PCs from trait space, with selected traits.",
                "position": 3115
            },
            {
                "img": "https://arxiv.org/html/2601.10387/img/appendix/qwen_trait_sim.png",
                "caption": "Figure 21:Histogram of cosine similarities of Qwen 3 32B trait vectors with the top 3 PCs from trait space, with selected traits.",
                "position": 3118
            },
            {
                "img": "https://arxiv.org/html/2601.10387/img/appendix/llama_trait_sim.png",
                "caption": "Figure 22:Histogram of cosine similarities of Llama 3.3 70B trait vectors with the top 3 PCs from trait space, with selected traits.",
                "position": 3121
            }
        ]
    },
    {
        "header": "Appendix DSteering evaluations",
        "images": []
    },
    {
        "header": "Appendix EPersona drift in multi-turn conversations",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10387/img/appendix/gemma_drift.png",
                "caption": "Figure 23:Projections on the Assistant Axis for Gemma 2 27B in multi-turn conversations across domains, with responses averaged per turn.",
                "position": 5281
            },
            {
                "img": "https://arxiv.org/html/2601.10387/img/appendix/qwen_drift.png",
                "caption": "Figure 24:Projections on the Assistant Axis for Qwen 3 32B in multi-turn conversations across domains, with responses averaged per turn.",
                "position": 5284
            },
            {
                "img": "https://arxiv.org/html/2601.10387/img/appendix/llama_drift.png",
                "caption": "Figure 25:Projections on the Assistant Axis for Llama 3.3 70B in multi-turn conversations across domains, with responses averaged per turn.",
                "position": 5287
            }
        ]
    },
    {
        "header": "Appendix FActivation capping Pareto frontier for Qwen",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10387/img/appendix/app_qwen_pareto.png",
                "caption": "Figure 26:Changes in harmful response rates (on persona-based jailbreak prompts) and capabilities eval performance (averaged over a suite of evaluations–IFEval, MMLU Pro, GSM8k, EQ-Bench) for different activation capping settings, varying the layer range and cap threshold (given in terms of a percentile, relative to the activations from our dataset used to compute role vectors), for Qwen 3 32B.",
                "position": 5296
            },
            {
                "img": "https://arxiv.org/html/2601.10387/img/appendix/pc1_sim.png",
                "caption": "Figure 27:Per-layer cosine similarity between role PC1 and the Assistant Axis obtained as a contrast vector. The middle layer used for our main experiments is annotated.",
                "position": 5316
            },
            {
                "img": "https://arxiv.org/html/2601.10387/img/appendix/steering_susceptibility.png",
                "caption": "Figure 28:Fraction of responses exhibiting different kinds of roles as a function of steering strength along role PC1.",
                "position": 5329
            },
            {
                "img": "https://arxiv.org/html/2601.10387/img/appendix/steering_jailbreak.png",
                "caption": "Figure 29:Fraction of harmful and harmless responses in response to persona-based jailbreaks as a function of steering strength along role PC1.",
                "position": 5335
            },
            {
                "img": "https://arxiv.org/html/2601.10387/img/appendix/steering_base.png",
                "caption": "Figure 30:Steering base models with role PC1 also shifts responses to completions to prefills about their purpose (top) and traits (bottom), mainly in Gemma 2 27B. Steering with role PC1 in Llama 3.1 70B led to ambiguous results because PC1 seemed to capture \"Assistant-ness\" less.",
                "position": 5344
            },
            {
                "img": "https://arxiv.org/html/2601.10387/img/appendix/gemma_drift_pc1.png",
                "caption": "Figure 31:Projections on role PC1 for Gemma 2 27B in multi-turn conversations across domains, with responses averaged per turn.",
                "position": 5357
            },
            {
                "img": "https://arxiv.org/html/2601.10387/img/appendix/qwen_drift_pc1.png",
                "caption": "Figure 32:Projections on role PC1 for Qwen 3 32B in multi-turn conversations across domains, with responses averaged per turn.",
                "position": 5360
            },
            {
                "img": "https://arxiv.org/html/2601.10387/img/appendix/llama_drift_pc1.png",
                "caption": "Figure 33:Projections on role PC1 for Llama 3.3 70B in multi-turn conversations across domains, with responses averaged per turn.",
                "position": 5363
            },
            {
                "img": "https://arxiv.org/html/2601.10387/img/appendix/qwen_pareto_pc1.png",
                "caption": "Figure 34:Changes in harmful response rates (on persona-based jailbreak prompts) and capabilities eval performance (averaged over a suite of evaluations–IFEval, MMLU Pro, GSM8k, EQ-Bench) for different activation capping settings along role PC1, varying the layer range and cap threshold (given in terms of a percentile, relative to the activations from our dataset used to compute role vectors), for Qwen 3 32B.",
                "position": 5370
            },
            {
                "img": "https://arxiv.org/html/2601.10387/img/appendix/llama_pareto_pc1.png",
                "caption": "Figure 35:Changes in harmful response rates (on persona-based jailbreak prompts) and capabilities eval performance (averaged over a suite of evaluations–IFEval, MMLU Pro, GSM8k, EQ-Bench) for different activation capping settings along role PC1, varying the layer range and cap threshold (given in terms of a percentile, relative to the activations from our dataset used to compute role vectors), for Llama 3.3 70B.",
                "position": 5373
            }
        ]
    },
    {
        "header": "Appendix GThe Assistant Axis vs. role PC1",
        "images": []
    }
]