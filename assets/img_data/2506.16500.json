[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.16500/x1.png",
                "caption": "Figure 1:Many recent parameter-efficient fine-tuning methods, such as QLoRA and DoRA, do not reduce compute. Our SparseLoRA accelerates LLM fine-tuning with minimal accuracy loss across a range of downstream tasks, including commonsense reasoning, math reasoning, code generation, and instruction following. See Section4for more details.",
                "position": 163
            },
            {
                "img": "https://arxiv.org/html/2506.16500/x2.png",
                "caption": "Figure 2:Runtime breakdown of LLaMA3-8B fine-tuning under different sequence lengths.",
                "position": 179
            },
            {
                "img": "https://arxiv.org/html/2506.16500/x3.png",
                "caption": "Figure 3:SparseLoRA accelerates LLM fine-tuning with contextual sparsity. It first performs an offline SVD decomposition of the pre-trained weights to construct the SVD sparsity estimator. During fine-tuning, it relies on the SVD sparsity estimator to identify which weight channels are needed for the given input sequence. It then sparsely computes by slicing the weights on the fly. Note that sparsity is applied only to the main branch, as the LoRA branch is typically very fast.",
                "position": 187
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.16500/x4.png",
                "caption": "Figure 4:Activation value distributions across all tokens of a sequence for layer 20 of LLaMA2-7B. Each column shows inputs to:Wg⁢a⁢t⁢e,u⁢psubscript𝑊𝑔𝑎𝑡𝑒𝑢𝑝W_{gate,up}italic_W start_POSTSUBSCRIPT italic_g italic_a italic_t italic_e , italic_u italic_p end_POSTSUBSCRIPT,Wd⁢o⁢w⁢nsubscript𝑊𝑑𝑜𝑤𝑛W_{down}italic_W start_POSTSUBSCRIPT italic_d italic_o italic_w italic_n end_POSTSUBSCRIPTof FFNs, andWQ,K,Vsubscript𝑊𝑄𝐾𝑉W_{Q,K,V}italic_W start_POSTSUBSCRIPT italic_Q , italic_K , italic_V end_POSTSUBSCRIPT,WOsubscript𝑊𝑂W_{O}italic_W start_POSTSUBSCRIPT italic_O end_POSTSUBSCRIPTof attention projections. The inputs toWd⁢o⁢w⁢nsubscript𝑊𝑑𝑜𝑤𝑛W_{down}italic_W start_POSTSUBSCRIPT italic_d italic_o italic_w italic_n end_POSTSUBSCRIPTandWOsubscript𝑊𝑂W_{O}italic_W start_POSTSUBSCRIPT italic_O end_POSTSUBSCRIPTfollow Laplace distributions, enabling higher sparsity when using the L2 norm metric.Wg⁢a⁢t⁢e,u⁢psubscript𝑊𝑔𝑎𝑡𝑒𝑢𝑝W_{gate,up}italic_W start_POSTSUBSCRIPT italic_g italic_a italic_t italic_e , italic_u italic_p end_POSTSUBSCRIPTis pruned alongside correspondingWd⁢o⁢w⁢nsubscript𝑊𝑑𝑜𝑤𝑛W_{down}italic_W start_POSTSUBSCRIPT italic_d italic_o italic_w italic_n end_POSTSUBSCRIPTchannels andWVsubscript𝑊𝑉W_{V}italic_W start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPTis pruned alongside correspondingWOsubscript𝑊𝑂W_{O}italic_W start_POSTSUBSCRIPT italic_O end_POSTSUBSCRIPTchannels, whileWQ,Ksubscript𝑊𝑄𝐾W_{Q,K}italic_W start_POSTSUBSCRIPT italic_Q , italic_K end_POSTSUBSCRIPTrequires a different criterion.",
                "position": 232
            },
            {
                "img": "https://arxiv.org/html/2506.16500/x5.png",
                "caption": "Figure 5:An illustration of how sparsity is applied to FFNs.",
                "position": 238
            },
            {
                "img": "https://arxiv.org/html/2506.16500/x6.png",
                "caption": "Figure 6:Comparison of attention maps for different pruning strategies at 50% sparsity. The figure shows the attention maps for the last 25 tokens of head 7 using dense full attention, pruning based on attention importance (ours), pruning based on input L2 norm (simlar to FFNs), pruning randomly.",
                "position": 277
            },
            {
                "img": "https://arxiv.org/html/2506.16500/x7.png",
                "caption": "Figure 7:Sensitivity analysis on layer-wise sparsity of LLaMA2-7B.",
                "position": 340
            },
            {
                "img": "https://arxiv.org/html/2506.16500/x8.png",
                "caption": "Figure 8:Output tokens go through the dense computation in our context-output aware sparsity strategy. The final outputs are gathered from both sparse and dense results.",
                "position": 349
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.16500/x9.png",
                "caption": "(a)last row in Figure9(b)’s attention scores visualization",
                "position": 2286
            },
            {
                "img": "https://arxiv.org/html/2506.16500/x9.png",
                "caption": "(a)last row in Figure9(b)’s attention scores visualization",
                "position": 2289
            },
            {
                "img": "https://arxiv.org/html/2506.16500/x10.png",
                "caption": "(b)attention scores of 3 different heads for a sample",
                "position": 2294
            },
            {
                "img": "https://arxiv.org/html/2506.16500/x11.png",
                "caption": "Figure 10:Iso-FLOPs comparison on LLaMA3-8B using Math10K and CSR170K. As the FLOPs budget decreases, SparseLoRA is able to better retain task performance compared to the LoRA counterpart by a larger margin.",
                "position": 2717
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]