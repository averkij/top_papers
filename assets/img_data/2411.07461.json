[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.07461/x1.png",
                "caption": "Figure 1:Overview ofKALE dataset creation and performance.Top:Example showing how KALE combines web alt-text with synthetic captions to produce knowledge-rich dense captions.Bottom left:Two-stage generation pipeline for KALE, using CogVLM and Mistral to create an initial set of knowledge augmented captions, followed by training a distilled VLM to scale up to 218M samples.Bottom right:Evaluation results comparing KALE’s average performance against popular synthetic image-text datasets.",
                "position": 61
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.07461/extracted/5989662/figs/kale.png",
                "caption": "Table 1:Comparison of open-source synthetic image-text datasets:We compare various datasets in terms of scale (number of samples), density (average number of words per sample), whether they are knowledge-augmented (meaning that the caption includes information found in image’s web scraped alt-text), and the size of the captioning model used to generate the descriptions. For KALE, we create an initial pool of 100M captions from a 17B parameter model and use it to distill a 2B parameter model that matches the performance of the larger 17B model.",
                "position": 67
            }
        ]
    },
    {
        "header": "2Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.07461/x2.png",
                "caption": "Figure 2:We generateKALE in a two stage process.Stage 1:We first create an initial pool of 100M knowledge-augmented dense captions using CogVLM-17B to generate dense captions for Datacomp-1B images and then augmenting these captions with real world knowledge by prompting Mistral.Stage 2:We use the knowledge-augmented captions from Stage 1 to train a VLM that takes image patch embeddings and Datacomp-1B captions as inputs and outputs knowledge-augmented captions. This VLM is then used to efficiently caption 118M more images from Datacomp-1B.",
                "position": 130
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.07461/extracted/5989662/figs/kale.png",
                "caption": "Table 2:Downstream performance:To measure the quality of KALE in comparison to other datasets, we evaluate the instruction-tuned models across vision-language tasks. KALE maintains a slight edge in overall performance, while our CogVLM synthetic captions shows strong performance in tasks like MMBench. Both subsets of our KALE data outperform existing synthetic image-text datasets.",
                "position": 196
            },
            {
                "img": "https://arxiv.org/html/2411.07461/extracted/5989662/figs/kale.png",
                "caption": "Table 3:Average performance shows little difference between training on stage 1 captions and a mixture of stage 1 and stage 2 (i.e.KALE).",
                "position": 311
            }
        ]
    },
    {
        "header": "4Related Works",
        "images": []
    },
    {
        "header": "5Limitations and conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]