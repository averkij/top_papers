[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00979/x1.png",
                "caption": "Figure 1:Overview of theIvy-Fakeframework: By conducting in-depth analysis of temporal and spatial artifacts, the framework enables explainable detection of AI-generated content.",
                "position": 106
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00979/x2.png",
                "caption": "Figure 2:Overview of the proposed unified and explainable IVY-FAKE dataset. Input images or videos from diverse domains are processed alongside specific prompts by an MLLM. The model leverages temporal and spatial analysis to produce structured, explainable annotations.",
                "position": 163
            }
        ]
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00979/x3.png",
                "caption": "Figure 3:Overview of the three-stage training pipeline for Ivy-Detector, including general video understanding, detection instruction tuning, and interpretability instruction tuning.",
                "position": 309
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00979/x4.png",
                "caption": "Figure 4:Showcase of the Video artifact detection.",
                "position": 564
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APerformance Evaluation of Explainability Datasets",
        "images": []
    },
    {
        "header": "Appendix BVideo Understanding Models and Evaluation on General Benchmarks",
        "images": []
    },
    {
        "header": "Appendix CEffect of Incorporating Human-Annotated Labels viagemini 2.5 proon Accuracy",
        "images": []
    },
    {
        "header": "Appendix DPrompts",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00979/x5.png",
                "caption": "Figure 10:Imageexample 1 whereIvy-xDetectorsuccessfully detects subtle spatial anomalies missed by baselines.",
                "position": 1934
            },
            {
                "img": "https://arxiv.org/html/2506.00979/x6.png",
                "caption": "Figure 11:Imageexample 2 illustrating improved robustness ofIvy-xDetectoragainst visually deceptive artifacts.",
                "position": 1937
            },
            {
                "img": "https://arxiv.org/html/2506.00979/x7.png",
                "caption": "Figure 12:Videoexample 1 demonstrating thatIvy-xDetectoreffectively captures temporal inconsistencies overlooked by baseline models.",
                "position": 1940
            },
            {
                "img": "https://arxiv.org/html/2506.00979/x8.png",
                "caption": "Figure 13:Videoexample 2 showcasingIvy-xDetectorâ€™s superior ability to detect subtle cross-frame temporal artifacts.",
                "position": 1943
            }
        ]
    },
    {
        "header": "Appendix ECase Study: Qualitative Comparison of Methods",
        "images": []
    }
]