[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04228/x1.png",
                "caption": "Figure 1.Demonstration for the applications of LayerFlow.Given layer-wise prompts, our method produces videos for a transparent foreground, a clean background, and a blended scenario. It also supports different user-provided conditions, enabling users to decompose and recompose videos creatively.",
                "position": 151
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04228/x2.png",
                "caption": "Figure 2.Overall pipeline of LayerFlow, which allows for the production of multi-layer videos including transparent foreground, undisturbed background and blended sequences. We organize videos of different layers as sub-clips and concatenate them to form a whole sequence to be encoded by VAE encoder. At the same time, index modification is conducted before prompts are processed by theT‚Å¢5ùëá5T5italic_T 5encoder, then layer embedding is added to text embeddings to impart layer awareness. All the visual patches and text embeddings are fed into transformer blocks as a long tensor. In the process of training, a base model is firstly trained on crudely made multi-layer video data for initial layered generation ability. Motion LoRA tuning prepares the model with accommodations for frozen video and Content LoRA can then borrow knowledge from both high-quality duplicated multi-layer images and copy-pasted video data for improving layer-aware synthesis quality as well as maintenance of motion dynamics.",
                "position": 188
            }
        ]
    },
    {
        "header": "2.Related Work",
        "images": []
    },
    {
        "header": "3.Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04228/x3.png",
                "caption": "Figure 3.Ablation for training data. We visualize the results for models trained on purely video data and joint image and video data.\nWithout high-quality image data, the model tends to generate a fuzzy background with obvious blur and low fidelity, while joint image-video data training contributes to undisturbed background synthesis and a higher level of text alignment (e.g., ‚Äùcolorful flowers‚Äù) and generation quality.",
                "position": 285
            },
            {
                "img": "https://arxiv.org/html/2506.04228/extracted/6512540/fig/comp_v5.jpg",
                "caption": "Figure 4.Qualitative comparison for multi-layer video generationwith generation then animation pipeline, i.e., composition of LayerDiffuse(Zhang and Agrawala,2024)and motion module(Guo et¬†al.,2023), where LayerFlow achieves better layer-level coherence and clearer separation of layers.",
                "position": 398
            }
        ]
    },
    {
        "header": "4.Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04228/x4.png",
                "caption": "Figure 5.Qualitative results for iterative multi-layer video generation, which iteratively implement multi-layer decomposition and conditioned layer generation to recompose video assets.",
                "position": 408
            }
        ]
    },
    {
        "header": "5.Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04228/x5.png",
                "caption": "Figure 6.Demonstrations for background-conditioned layer generation, where we take three layer-wise descriptions and a background sequence as input (top row) and show generated results of foreground (middle row) and blended video (bottom row).",
                "position": 1240
            },
            {
                "img": "https://arxiv.org/html/2506.04228/x6.png",
                "caption": "Figure 7.Demonstrations for multi-layer video generation.For each example, we take three layer-wise descriptions as input and show generated results of foreground (top row), background (middle row)\nand blended video (bottom row).",
                "position": 1244
            },
            {
                "img": "https://arxiv.org/html/2506.04228/x7.png",
                "caption": "Figure 8.Demonstrations for foreground-conditioned layer generation, where we take three layer-wise descriptions and a foreground sequence as input (top row) and show generated results of background (middle row) and blended video (bottom row).",
                "position": 1250
            },
            {
                "img": "https://arxiv.org/html/2506.04228/x8.png",
                "caption": "Figure 9.Demonstrations for multi-layer video decomposition, where we take three layer-wise descriptions and a blended sequence as input (top row) and show generated results of background (middle row) and foreground video (bottom row).",
                "position": 1255
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]