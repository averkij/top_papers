[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": []
    },
    {
        "header": "IIRelated Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05545/images/PII_R_LM.png",
                "caption": "Figure 1:End-to-end decision workflow for training or deploying a PII-aware redaction language model. The diagram outlines multiple adaptation paths: Fine-Tuning, Instruction Tuning, and Retrieval-Augmented Generation (RAG) and model selection spanning both proprietary (P) and open-source (OS) architectures.",
                "position": 224
            }
        ]
    },
    {
        "header": "IIIMethodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05545/images/RAG_image.png",
                "caption": "Figure 2:Overview of the RAG-based redaction pipeline: retrieved examples inform context-aware masking.",
                "position": 560
            }
        ]
    },
    {
        "header": "IVExperimental setup",
        "images": []
    },
    {
        "header": "VEvaluation",
        "images": []
    },
    {
        "header": "VIResults and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05545/images/TimevF1.png",
                "caption": "Figure 3:This graph visualizes the trade-off between GPU usage and model performance. All experiments were run on two 48GB NVIDIA RTX 6000 GPUs. The plot is divided into quadrants: the top-left represents the optimal trade-off—high performance with low GPU usage. The top-right indicates high performance at high computational cost, while the bottom-left reflects both low resource usage and low performance.",
                "position": 1325
            },
            {
                "img": "https://arxiv.org/html/2508.05545/images/latencyvsF1.png",
                "caption": "Figure 4:This plot illustrates the trade-off between inference latency (ms) and F1 score for generating 150 tokens. Models are categorized by training strategy (FT: fine-tuned, IT: instruction-tuned). The top-left quadrant indicates the optimal balance—high F1 with low latency. The top-right captures high-performing but slower models, while the bottom-right reflects both high latency and low performance, notably T5(FT).",
                "position": 1336
            },
            {
                "img": "https://arxiv.org/html/2508.05545/images/SizevF1.png",
                "caption": "Figure 5:This plot illustrates the trade-off between model size (in billions of parameters) and F1 score. The x-axis uses a log scale. Models in the top-left quadrant achieve high F1 with compact architectures, including DeepSeek-Q1(IT), LLaMA 3.2–3B(FT), and DeepSeek-Q1(FT). The top-right quadrant includes larger, high-performing models such as GPT-4 and Mixtral(FT). T5(FT) and instruction-tuned LLaMA variants lie in the lower quadrants, reflecting reduced performance despite varied model size.",
                "position": 1347
            }
        ]
    },
    {
        "header": "VIIConclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "VIIIAppendix",
        "images": []
    },
    {
        "header": "List of Supported PII Labels",
        "images": []
    }
]