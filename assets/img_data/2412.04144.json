[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04144/extracted/6047768/figures/recycling.png",
                "caption": "",
                "position": 90
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04144/x1.png",
                "caption": "Figure 1:An overview of our setup. Given models obtained from different LLM training runs, we optimize linear merging weightings (Œ±1,Œ±2,Œ±3subscriptùõº1subscriptùõº2subscriptùõº3\\alpha_{1},\\alpha_{2},\\alpha_{3}italic_Œ± start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_Œ± start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_Œ± start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT) via iterative search to obtain a model with minimal task tradeoffs. Eachrepresents a single model, with ato designate its performance on the two tasks. The purple color indicates a Pareto-optimal model, achieving a good balance between the two tasks without being dominated by other models. We show tradeoffs between only two tasks since it is easier to visualize.",
                "position": 149
            },
            {
                "img": "https://arxiv.org/html/2412.04144/x4.png",
                "caption": "",
                "position": 150
            },
            {
                "img": "https://arxiv.org/html/2412.04144/x5.png",
                "caption": "",
                "position": 150
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Optimizing LLM Merging",
        "images": []
    },
    {
        "header": "4Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04144/x6.png",
                "caption": "Figure 2:Performance of individual models over the seven tasks covering different capabilities. Models 1-8 are the result of supervised finetuning runs, while 8-16 from preference optimization. Held-out tasks (MT-Bench and LBPP) are used to evaluate the resulting merges to make sure the merge optimization process does not overfit to the held-in tasks that we aim to minimize tradeoffs over. MT-Bench rating is scaled by a factor of 10 for better visualization. The exact numbers are inTable2inAppendixA.",
                "position": 281
            }
        ]
    },
    {
        "header": "5Results and Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04144/x7.png",
                "caption": "Figure 3:Performance tradeoffs with different merging approaches. Shaded areas represent 95% confidence interval of the best-fit line computed over individual checkpoint scores (shown in green).",
                "position": 377
            },
            {
                "img": "https://arxiv.org/html/2412.04144/x8.png",
                "caption": "",
                "position": 380
            },
            {
                "img": "https://arxiv.org/html/2412.04144/x9.png",
                "caption": "",
                "position": 381
            },
            {
                "img": "https://arxiv.org/html/2412.04144/x10.png",
                "caption": "Table 1:Performance of different baselines compared to search optimized merge. Held-in tasks refer to tasks included in the fitness function (¬ß3.2) and held-out tasks are used to validate the quality of the search optimized models. Search yields models with the lowest tradeoffs over the held-in tasks without sacrificing performance on held-out tasks. We highlightsingle modelsandmergesdifferently. Evaluations on MMLU Pro use only 2K test examples.",
                "position": 385
            },
            {
                "img": "https://arxiv.org/html/2412.04144/x10.png",
                "caption": "Figure 4:Spearman‚Äôs rank correlation between task pairs. It is easy to see how some tasks exhibit strong performance tradeoffs, such as MBPP-IFEval and MMLU-Pro/MUSR.",
                "position": 427
            },
            {
                "img": "https://arxiv.org/html/2412.04144/x11.png",
                "caption": "Figure 5:Performance of different merge approaches when minimizing the tradeoffs across three tasks: MBPP, IFEVal, and GSM8K. It is clear that search-optimized merging can well balance the performance over the three tasks. Bars corresponding to merging are hatched to differentiate from individual models.",
                "position": 461
            },
            {
                "img": "https://arxiv.org/html/2412.04144/x12.png",
                "caption": "Figure 6:Best solutions found via CMA-ES search when optimizing tradeoffs over the pairs MBPP-MUSR (left) MBPP-IFEval (mid) and MBPP-IFEval-GSM8K. We order the weightings over the x-axis based on the fitness of the individual model they correspond to. We observe that top-solutions do not necessarily assign high weights to high-fitness individual checkpoints. For instance, the top solution on MBPP-IFEval assigns considerably high weight to model #9, which exhibits a relatively bad tradeoff on the task pair.",
                "position": 483
            },
            {
                "img": "https://arxiv.org/html/2412.04144/x13.png",
                "caption": "",
                "position": 486
            },
            {
                "img": "https://arxiv.org/html/2412.04144/x14.png",
                "caption": "",
                "position": 487
            },
            {
                "img": "https://arxiv.org/html/2412.04144/x15.png",
                "caption": "Figure 7:Fitness vs. CMA-ES iterations when optimizing tradeoffs over task pairs (seeSection5.1). CMA-ES explores the search space to find merge weightings with high fitness or low task tradeoffs.",
                "position": 491
            },
            {
                "img": "https://arxiv.org/html/2412.04144/x16.png",
                "caption": "",
                "position": 494
            },
            {
                "img": "https://arxiv.org/html/2412.04144/x17.png",
                "caption": "",
                "position": 495
            },
            {
                "img": "https://arxiv.org/html/2412.04144/x18.png",
                "caption": "Figure 8:Merges found via CMA-ES when optimizing MBPP-MUSR tradeoffs over 2, 4, 8, and 16 checkpoints. We also show the centroid of each set of experiments (in large markers). Optimizing over more checkpoints (8 and 16) tends to yield less tradeoffs comapred to fewer checkpoints (2,4), showing how recycling more models can outperform recycling fewer checkpoints.",
                "position": 517
            }
        ]
    },
    {
        "header": "Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ACheckpoint details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04144/x19.png",
                "caption": "Table 3:Comparison of model performance across different task pairs. Held-in tasks refer to tasks included in the fitness function (¬ßLABEL:sec:method).",
                "position": 1505
            },
            {
                "img": "https://arxiv.org/html/2412.04144/x19.png",
                "caption": "Figure 9:Merges found via CMA-ES when optimizing MBPP-MUSR tradeoffs over 2, 4, 8, and 16 checkpoints. We also show the centroid of each set of experiments. We find that optimizing over more checkpoints (8 and 16) outperforms optimization over fewer checkpoints (2,4), showing how recycling more models can outperform recycling fewer checkpoints.",
                "position": 1528
            }
        ]
    },
    {
        "header": "Appendix BAdditional Results",
        "images": []
    }
]