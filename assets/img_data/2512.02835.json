[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02835/x1.png",
                "caption": "",
                "position": 77
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02835/x2.png",
                "caption": "Figure 2:Overview of ReVSeg. The model runs a two-turn reasoning chain over the input video and query. Round one analyzes the scene and selects an informative keyframe with a concise object description. Round two grounds the target on that keyframe by predicting a bounding box. The keyframe-bbox pair conditions a video tracker to produce full segmentation sequence. A reward manager provides concise signals to post-train the VLM via reinforcement learning, improving keyframe selection, grounding accuracy, and overall robustness.",
                "position": 151
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02835/x3.png",
                "caption": "Figure 3:Qualitative cases of ReVSeg on ReasonVOS[bai2024one]. The frame highlighted in red indicates the selected keyframe. The green bounding box within the enlarged keyframe on the right size represents the grounding result. Zoom in to view visual details.",
                "position": 1368
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ATraining Curves",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02835/x4.png",
                "caption": "(a)",
                "position": 1640
            },
            {
                "img": "https://arxiv.org/html/2512.02835/x4.png",
                "caption": "(a)",
                "position": 1643
            },
            {
                "img": "https://arxiv.org/html/2512.02835/x5.png",
                "caption": "(b)",
                "position": 1648
            },
            {
                "img": "https://arxiv.org/html/2512.02835/x6.png",
                "caption": "(c)",
                "position": 1654
            },
            {
                "img": "https://arxiv.org/html/2512.02835/x7.png",
                "caption": "(d)",
                "position": 1660
            },
            {
                "img": "https://arxiv.org/html/2512.02835/x8.png",
                "caption": "(e)",
                "position": 1665
            },
            {
                "img": "https://arxiv.org/html/2512.02835/x9.png",
                "caption": "(f)",
                "position": 1671
            },
            {
                "img": "https://arxiv.org/html/2512.02835/x10.png",
                "caption": "Figure 5:Additional qualitative cases of ReVSeg. The frame highlighted in red indicates the selected keyframe. The green bounding box within the enlarged keyframe on the right side represents the grounding result. Zoom in to view visual details.",
                "position": 1678
            }
        ]
    },
    {
        "header": "Appendix BQualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02835/x11.png",
                "caption": "(a)",
                "position": 1710
            },
            {
                "img": "https://arxiv.org/html/2512.02835/x11.png",
                "caption": "(a)",
                "position": 1713
            },
            {
                "img": "https://arxiv.org/html/2512.02835/x12.png",
                "caption": "(b)",
                "position": 1718
            }
        ]
    },
    {
        "header": "Appendix CImplementation Details",
        "images": []
    }
]