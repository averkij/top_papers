[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05336/extracted/6515880/figures/comparison.png",
                "caption": "Figure 1:Given complex referring expressions in natural language,VideoMolmodemonstrates improved spatio-temporal reasoning in visual grounding.\nBy decomposing the visual grounding task into sequential steps‚Äîpointing (denoted by star) followed by generating masks (in red) -VideoMolmoproduces more accurate and coherent segmentation masks compared to prior approaches.",
                "position": 85
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3VideoMolmo",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05336/x1.png",
                "caption": "Figure 2:VideoMolmoArchitecture.The visual encoder extracts multi-crop features from the current frame and the pastlùëôlitalic_lframes. These temporal features provide contextual cues and are processed by the Temporal Module‚Ñ≥‚Ñ≥\\mathcal{M}caligraphic_Mvia multi-head cross-attention, where the query comes from the current frame, and key and value from the mean of previous frames. The output is fused with the original features to enrich temporal cues while preserving the spatial details of the current frame. The combined visual-textual representations are then passed to the LLM to predict grounded points. These points are converted into masks using our Bidirectional Temporal Mask Fusion module, ensuring temporally consistent pixel-level grounding.",
                "position": 135
            },
            {
                "img": "https://arxiv.org/html/2506.05336/x2.png",
                "caption": "Figure 3:VideoMolmoannotation pipeline:We construct point-level supervision from frame-level masks using a semi-automatic process. For each frame,kùëòkitalic_kpoints are sampled on the mask and passed to SAM2 to generate candidate masks. The point with the highest-IoU candidate mask (w.r.t. ground truth) is selected as the optimal annotation.",
                "position": 259
            }
        ]
    },
    {
        "header": "4VideoMolmodataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05336/extracted/6515880/figures/benchmark_diagram.png",
                "caption": "Figure 4:VideoMolmodemonstrates robust generalization and fine-grained spatio-temporal grounding across diverse out-of-distribution scenarios from our proposed benchmark, for instance, correctly pointing to traffic lights (2ndrow) in challenging driving scenes despite never encountering such scenarios during training. (Please refer to AppendixA.4for additional qualitative results.)",
                "position": 298
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05336/x3.png",
                "caption": "Figure 5:Performance comparison of VideoMolmo on point grounding.",
                "position": 680
            },
            {
                "img": "https://arxiv.org/html/2506.05336/x3.png",
                "caption": "Figure 5:Performance comparison of VideoMolmo on point grounding.",
                "position": 683
            },
            {
                "img": "https://arxiv.org/html/2506.05336/x4.png",
                "caption": "Figure 6:Effect of temporal module context length on segmentation accuracy in Refer-DAVIS benchmark.",
                "position": 688
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05336/x5.png",
                "caption": "Figure 7:Effect ofBidirectional temporal mask fusionon Molmo+SAM2 baseline.",
                "position": 1715
            },
            {
                "img": "https://arxiv.org/html/2506.05336/x6.png",
                "caption": "Figure 8:VPoS-Bench qualitative examples.",
                "position": 1763
            },
            {
                "img": "https://arxiv.org/html/2506.05336/x7.png",
                "caption": "Figure 9:MeVis qualitative examples.",
                "position": 1766
            },
            {
                "img": "https://arxiv.org/html/2506.05336/x8.png",
                "caption": "Figure 10:Refer-YouTube-VOS qualitative examples.",
                "position": 1769
            },
            {
                "img": "https://arxiv.org/html/2506.05336/x9.png",
                "caption": "Figure 11:Refer-DAVIS qualitative examples.",
                "position": 1772
            },
            {
                "img": "https://arxiv.org/html/2506.05336/x10.png",
                "caption": "Figure 12:ReasonVOS qualitative examples.",
                "position": 1775
            },
            {
                "img": "https://arxiv.org/html/2506.05336/x11.png",
                "caption": "Figure 13:Qualitative failure cases ofVideoMolmo.",
                "position": 1778
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]