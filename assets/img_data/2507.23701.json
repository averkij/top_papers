[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23701/x1.png",
                "caption": "Figure 1:Examples showing the diverse reasoning challenges inTextQuests.denotes LLM thinking.denotes the action.",
                "position": 110
            },
            {
                "img": "https://arxiv.org/html/2507.23701/x4.png",
                "caption": "",
                "position": 111
            },
            {
                "img": "https://arxiv.org/html/2507.23701/x5.png",
                "caption": "",
                "position": 111
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2TextQuests",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23701/images/openai_logo.png",
                "caption": "Table 1:LLMs performance onTextQuests. All reasoning models are evaluated with high-reasoning budget. For complete results and more models, seeTable˜4.",
                "position": 153
            },
            {
                "img": "https://arxiv.org/html/2507.23701/images/claude_logo.png",
                "caption": "",
                "position": 184
            },
            {
                "img": "https://arxiv.org/html/2507.23701/images/grok_logo.png",
                "caption": "",
                "position": 192
            },
            {
                "img": "https://arxiv.org/html/2507.23701/images/gemini_logo.png",
                "caption": "",
                "position": 208
            },
            {
                "img": "https://arxiv.org/html/2507.23701/images/qwen_logo.png",
                "caption": "",
                "position": 240
            },
            {
                "img": "https://arxiv.org/html/2507.23701/images/deepseek_logo.png",
                "caption": "",
                "position": 256
            }
        ]
    },
    {
        "header": "3Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23701/x6.png",
                "caption": "Figure 2:Game progress for various model scales versus an optimal human walkthrough. Capable models sustain progress longer, suggesting better long-horizon reasoning.",
                "position": 288
            }
        ]
    },
    {
        "header": "4Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23701/x7.png",
                "caption": "Figure 3:Comparing mini and standard models from different closed-source providers, highlighting the importance of model scale for exploratory tasks.",
                "position": 329
            },
            {
                "img": "https://arxiv.org/html/2507.23701/x8.png",
                "caption": "Figure 4:Examples of long context reasoning failures inTextQuests.Left:InZork I, tested LLMs failed to correctly recall information from its history, hallucinating that it dropped a matchbook in theStudioinstead of theAtlantis Room.Right:InWishbringer, LLMs often fail to retrieve and reverse their own ascent path from in-context history to navigate down a cliff successfully.",
                "position": 345
            },
            {
                "img": "https://arxiv.org/html/2507.23701/x9.png",
                "caption": "Figure 5:A comparison of output and reasoning token efficiency across state-of-the-art LLMs onTextQuests. Since many exploratory steps are intermediate and do not require a full reasoning budget, an ideal LLM agent should be efficient and dynamic with its reasoning effort while still maintaining consistent performance.",
                "position": 360
            }
        ]
    },
    {
        "header": "5Related Work and Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATextQuestsEnvironments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23701/x10.png",
                "caption": "Figure 6:Adding an AutoSave mechanism to the game environment improves the agent’s exploration efficiency.Left:An example of evaluated LLMs makes use of the autosave and restore features to experiment with different approaches to solve an in-game puzzle.Right:As LLMs’ capabilities increase, the performance difference between runs with and without the Autosave feature widens, leading to a difference of more than 10% after 500 steps on Gemini 2.5 Pro and Claude Sonnet 4.0 and 6% on Grok 3 Mini.",
                "position": 989
            },
            {
                "img": "https://arxiv.org/html/2507.23701/x11.png",
                "caption": "",
                "position": 998
            }
        ]
    },
    {
        "header": "Appendix BBeyond 500 Run Steps",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23701/x12.png",
                "caption": "Figure 7:Game progress trajectories onTextQuestsfor selected frontier LLMs evaluated up to800800steps. The visualization shows game progress saturating after approximately500500steps. For detailed metrics, seeTable˜3.",
                "position": 1014
            },
            {
                "img": "https://arxiv.org/html/2507.23701/images/openai_logo.png",
                "caption": "Table 3:Game progress and harm for several LLMs at500500and800800run steps.",
                "position": 1017
            }
        ]
    },
    {
        "header": "Appendix CFull Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23701/images/openai_logo.png",
                "caption": "Table 4:LLMs performance onTextQuests. Reasoning models are evaluated with high reasoning setting.",
                "position": 1121
            },
            {
                "img": "https://arxiv.org/html/2507.23701/images/kimi_logo.png",
                "caption": "",
                "position": 1298
            },
            {
                "img": "https://arxiv.org/html/2507.23701/images/meta_logo.png",
                "caption": "",
                "position": 1348
            }
        ]
    },
    {
        "header": "Appendix DSystem Prompt and Environment Interaction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23701/x13.png",
                "caption": "Figure 8:An illustration of an agent’s turn. From the model’s output, the brief reasoning and action are extracted and added to the context history, while any intermediate ‘thinking’ is discarded",
                "position": 1407
            }
        ]
    },
    {
        "header": "Appendix EToken Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23701/x14.png",
                "caption": "Figure 9:A comparison of ourGame Progressmetric against the in-gameGame Score.Left:The trajectory for an optimal walkthrough of a sample game shows that ourGame Progressprovides a more representative signal of advancement than the built-in score.Right:The final scores for games likeMoonmistandWitnessdemonstrate that game completion (100% progress) is often independent of achieving the maximum possible game score.",
                "position": 1549
            }
        ]
    },
    {
        "header": "Appendix FComparing Game Progress and Game Score",
        "images": []
    }
]