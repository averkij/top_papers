[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05975/x1.png",
                "caption": "",
                "position": 167
            },
            {
                "img": "https://arxiv.org/html/2602.05975/x2.png",
                "caption": "",
                "position": 189
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05975/x3.png",
                "caption": "Figure 1:Sagetask overview. Given a complex question, the deep research agent (e.g.,DR Tulu) iteratively reasons, generates keyword-based sub-queries, searches for relevant papers, and outputs a final answer. We first evaluate the agents with their native web-search tool, and then modify DR Tulu’s MCP service to replace web search with retrievers that performs corpus search over our paper collection.",
                "position": 197
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3SageBenchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05975/x4.png",
                "caption": "Table 1:Our Benchmark statistics. Domains: Computer Science (Com. Sci.), Natural Science (Nat. Sci.), Healthcare (Health.), and Humanities (Human.). Query length is in tokens. GT Documents = average ground truth papers per query.",
                "position": 290
            },
            {
                "img": "https://arxiv.org/html/2602.05975/x4.png",
                "caption": "Figure 2:Overview ofshort-formquestions that require intensive reasoning over metadata, paper details and inter-paper relationships. Each question consists of three parts and has only one ground-truth answer.",
                "position": 386
            },
            {
                "img": "https://arxiv.org/html/2602.05975/x5.png",
                "caption": "Figure 3:Overview ofopen-endedquestions that are grounded on real-world scenarios. Each question consists of three parts and has multiple ground-truth papers weighted by their relevance.",
                "position": 406
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05975/x6.png",
                "caption": "Figure 4:An illustrative case where LLM-based retrieval fails due to semantic drift. The query seeks a paper that uses physics-informed heuristics. ReasonIR over-emphasizes title-level keywords (highlighted in red) and thus retrieves wrong papers. The retrieved content then reinforces this focus in subsequent retrieval steps, creating a feedback loop that increasingly prioritizes “physics-informed” in title. In contrast, BM25 remains anchored by lexical matching in similar sub-queries and avoids this drift.",
                "position": 775
            }
        ]
    },
    {
        "header": "5Test-Time Corpus Scaling",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations and Future Work",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05975/x7.png",
                "caption": "Figure 9:Distribution of markdown length (in tokens) for 1,000 randomly sampled documents from the Computer Science domain.",
                "position": 1712
            },
            {
                "img": "https://arxiv.org/html/2602.05975/x8.png",
                "caption": "Figure 10:Distribution of markdown length (in tokens) for 1,000 randomly sampled documents from the Healthcare domain.",
                "position": 1715
            },
            {
                "img": "https://arxiv.org/html/2602.05975/x9.png",
                "caption": "Figure 11:Distribution of markdown length (in tokens) for 1,000 randomly sampled documents from the Humanities domain.",
                "position": 1718
            },
            {
                "img": "https://arxiv.org/html/2602.05975/x10.png",
                "caption": "Figure 12:Distribution of markdown length (in tokens) for 1,000 randomly sampled documents from the Natural Science domain.",
                "position": 1721
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]