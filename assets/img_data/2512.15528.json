[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.15528/x1.png",
                "caption": "Figure 1:In addition to a structured Chain-of-Thought (CoT) and the derived answer, EmoCaliber also produces a self-evaluated confidence level. It enables users to adopt the output selectively, significantly enhancing the reliability of the VEC system.",
                "position": 86
            },
            {
                "img": "https://arxiv.org/html/2512.15528/x2.png",
                "caption": "Figure 2:Illustration of the model’s evolution across the three training stages. Through these stages, the model is successively endowed with structured reasoning, taught to verbalize confidence, and finally calibrated to express confidence accurately.",
                "position": 95
            },
            {
                "img": "https://arxiv.org/html/2512.15528/x3.png",
                "caption": "Figure 3:Task composition of VECBench. The training split comprises Visual Sentiment Analysis (VSA) and Visual Emotion Recognition (VER) tasks, with each subtask denoted as “source-granularity (#sample)”. In addition to retaining corresponding subtasks for in-domain (ID) evaluation, the test split also includes out-of-domain (OOD) VER tasks to verify generalization ability.",
                "position": 107
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Training Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.15528/x4.png",
                "caption": "Figure 4:Construction pipeline of the VEC-CoT dataset from the training split of VECBench. Image–label pairs are first templatized and fed into proprietary MLLMs to synthesize structured CoTs, which are then subjected to strict quality evaluation. Finally, image–text pairs with high-quality CoTs are retained and grouped into the VEC-CoT dataset.",
                "position": 194
            },
            {
                "img": "https://arxiv.org/html/2512.15528/x5.png",
                "caption": "Figure 5:Illustration of the 2nd training stage. The scaffold model first performs inference on unseen data, producing a CoT and an emotion prediction. This prediction is then mapped onto a VAD lexicon-based emotion loop to measure its normalized distance from the ground-truth label. The estimated confidence score, derived from this distance, is directly appended to the original CoT and prediction, forming the supervision data used for SFT.",
                "position": 284
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.15528/x6.png",
                "caption": "Figure 6:Distribution of verbalized confidences of models derived from different RL configurations on ID VER tasks. Error bars represent 95% confidence intervals.",
                "position": 1469
            },
            {
                "img": "https://arxiv.org/html/2512.15528/x7.png",
                "caption": "Figure 7:Representative cases of EmoCaliber performing ID VER, ID VSA, and OOD VER tasks on VECBench.",
                "position": 1508
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    }
]