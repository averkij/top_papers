[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16318/x1.png",
                "caption": "",
                "position": 113
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16318/x2.png",
                "caption": "Figure 2:Illustration of training and inference pipeline forOneDiffusion. We encode the desired task for each sample via a special task token. During training we independently sample different diffusion timesteps for each view and add noise to them accordingly. In inference, we replace input image(s) with Gaussian noises while setting timesteps of conditions to00.",
                "position": 183
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16318/x3.png",
                "caption": "Figure 3:High-resolution samplesfrom textof ourOneDiffusionmodel, showcasing its capabilities in precise prompt adherence, attention to fine details, and high image quality across a wide variety of styles.",
                "position": 299
            },
            {
                "img": "https://arxiv.org/html/2411.16318/x4.png",
                "caption": "Figure 4:Illustration of our model capability to generate HED, depth, human pose, semantic mask, and bounding box from input image. For semantic segmentation, we segment thesword(highlighted in yellow) and themoon(highlighted in cyan) the first example, while segmentingroad(yellow),sky(cyan) in the second. For object detection, We localize theheadandmoon(both highlighted in cyan). Leveraging these conditions, we can reverse the process to recreate a variant of the input image based on the same caption. Additionally, we can edit the image by modifying specific elements, such as replacing the moon with Saturn (last example).",
                "position": 340
            }
        ]
    },
    {
        "header": "4One-Gen Datasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16318/x5.png",
                "caption": "Figure 5:Illustration of the multiview generation with single input image. We equally slice the azimuth in range of[−45,60]4560[-45,60][ - 45 , 60 ]and elevation in range of[−15,45]1545[-15,45][ - 15 , 45 ]for the left scenes. For the right scene, the azimuth range is set to[0;360]0360[0;360][ 0 ; 360 ]and elevation range is set to[−15;15]1515[-15;15][ - 15 ; 15 ].",
                "position": 408
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16318/x6.png",
                "caption": "Figure 6:Illustration of ID customization using reference images. Unlike prior methods that rely on face embeddings and often fail to generalize, our model demonstrates superior generalization. It effectively adjusts facial expressions and gaze directions (first row), changes viewpoints (second row), and even customizes non-human IDs (third row). All results in the third row are generated from a single reference image, while InstantID fails as its face detector cannot detect faces in the input.",
                "position": 527
            },
            {
                "img": "https://arxiv.org/html/2411.16318/extracted/6023202/figure/depth_comparison.jpg",
                "caption": "Figure 7:Qualitative comparison between a diffusion-based depth estimation - Marigold[22]and our methods.",
                "position": 671
            },
            {
                "img": "https://arxiv.org/html/2411.16318/extracted/6023202/figure/poses.png",
                "caption": "Figure 8:Qualitative comparison between RayDiffusion andOneDiffusionon GSO dataset.OneDiffusionyields better prediction.",
                "position": 783
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Additional Qualitative results",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16318/x7.png",
                "caption": "Figure 9:Distribution of training datasets for all tasks. Segments proportional to sampling rates. The inner section shows the super-category of target tasks, it can be observed that we train the model with equal budget for text-to-image, image-to-image and multiview generation. The outer section shows datasets used for each super-category.",
                "position": 1812
            },
            {
                "img": "https://arxiv.org/html/2411.16318/extracted/6023202/figure/onediffusion_appendix_multiview.jpg",
                "caption": "Figure 10:Qualitative results of image-to-multiview generation. The left most images are input. We equally slice the azimuth in range of[−45,60]4560[-45,60][ - 45 , 60 ]and elevation in range of[−15,45]1545[-15,45][ - 15 , 45 ]for all scenes.",
                "position": 1823
            },
            {
                "img": "https://arxiv.org/html/2411.16318/extracted/6023202/figure/onediffusion_appendix_multiview_2.jpg",
                "caption": "Figure 11:Qualitative results of image-to-multiview generation. We equally slice the azimuth in range of[−45,60]4560[-45,60][ - 45 , 60 ]and elevation in range of[−15,45]1545[-15,45][ - 15 , 45 ]for the first 3 scenes. For the last scene, the azimuth range is set to[0;360]0360[0;360][ 0 ; 360 ]and elevation range is set to[−15;15]1515[-15;15][ - 15 ; 15 ].",
                "position": 1826
            },
            {
                "img": "https://arxiv.org/html/2411.16318/x8.png",
                "caption": "Figure 12:Qualitative results of text-to-multiview generation. The azimuth and elevation of left to right columns are[0,30,60,90]0306090[0,30,60,90][ 0 , 30 , 60 , 90 ]and[0,10,20,30]0102030[0,10,20,30][ 0 , 10 , 20 , 30 ], respectively. We use following prefix for all prompts to improve the quality and realism of generated images: “photorealistic, masterpiece, highly detail, score_9, score_8_up”.",
                "position": 1829
            },
            {
                "img": "https://arxiv.org/html/2411.16318/extracted/6023202/figure/onediffusion_appendix_faceid.jpg",
                "caption": "Figure 13:Qualitative results ofOneDiffusionfor (single reference) ID Customization task with photo of human faces. The left most images are input, target prompts for left to right columns are: 1) “Photo of a man/woman wearing suit at Shibuya at night. He/She is looking at the camera”, 2) “pixarstyle, cartoon, a person in pixar style sitting on a crowded street”, 3) “watercolor drawing of a man/woman with Space Needle in background”",
                "position": 1832
            },
            {
                "img": "https://arxiv.org/html/2411.16318/extracted/6023202/figure/onediffusion_appendix_faceid_3.jpg",
                "caption": "Figure 14:Qualitative results ofOneDiffusionfor (single reference) ID Customization task with photo of of non-human subjects or cartoon style input.OneDiffusionis highly versatile and can produce good results for all kind of input and not limited to photorealistic human images. Since we rely on attention, the model can attend to the condition view and preserve intricate details and is not limited by any bottlenecke.g.latent representation.",
                "position": 1835
            },
            {
                "img": "https://arxiv.org/html/2411.16318/extracted/6023202/figure/onediffusion_appendix_depth.jpg",
                "caption": "Figure 15:Qualitative comparison for depth estimation betweenOneDiffusion, Marigold[22]and DepthAnything-v2[62]",
                "position": 1838
            },
            {
                "img": "https://arxiv.org/html/2411.16318/extracted/6023202/figure/onediffusion_appendix_depth_2.jpg",
                "caption": "Figure 16:Qualitative comparison for depth estimation betweenOneDiffusion, Marigold[22]and DepthAnything-v2[62]",
                "position": 1841
            },
            {
                "img": "https://arxiv.org/html/2411.16318/x9.png",
                "caption": "Figure 17:Qualitative examples of human pose estimation on COCO datasets.",
                "position": 1844
            },
            {
                "img": "https://arxiv.org/html/2411.16318/x10.png",
                "caption": "Figure 18:Qualitative examples of semantic segmentation on COCO datasets. The target class for each image (from left to right, from top to bottom) are\n(sheep,grass,mountain,sky),\n(apple,person,building),\n(vase,flower, ),\n(dog,book,sheet),\n(umbrella,person,building,gate),\n(boat,dock,drum).",
                "position": 1847
            },
            {
                "img": "https://arxiv.org/html/2411.16318/x11.png",
                "caption": "Figure 19:Illustration of our model capability to generate semantic mask, detection, human pose, depth, and canny edge from input image.\nFor semantic segmentation, we segment the flower (highlighted in yellow) and the rock (highlighted in green). For object detection, We localize the backpack (highlighted in yellow) and butterfly (highlighted in cyan).\nLeveraging these conditions, we can reverse the process to recreate a variant of the input image based on the same caption.",
                "position": 1857
            }
        ]
    },
    {
        "header": "8Summary Datasets",
        "images": []
    }
]