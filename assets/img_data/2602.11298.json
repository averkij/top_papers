[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11298/images/header.jpeg",
                "caption": "",
                "position": 81
            },
            {
                "img": "https://arxiv.org/html/2602.11298/x1.png",
                "caption": "Figure 1:Voxtral Realtime approaches offline accuracy at sub-second latency.Macro-average word error-rate (WER) vs. delay on the FLEURS multilingual benchmark for realtime and offline models. Lower is better. At 480 ms delay, Voxtral Realtime is competitive with Scribe v2 Realtime, the leading realtime API model, as well as Whisper, the most popular open-source offline model. It surpasses both baselines at 960 ms delay, approaching the performance of Voxtral Mini Transcribe V2, a state-of-the-art offline transcription model.",
                "position": 95
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Modeling",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11298/figures/streaming.004.jpeg",
                "caption": "Figure 2:Voxtral Realtime architecture and decoding scheme for a target delayτ=80\\tau=80ms.Voxtral Realtime consists of a causal audio encoder to embed the input audio stream, an MLP adapter layer to temporally downsample the audio embeddings, and a text decoder to auto-regressively generate the output text stream. The downsampled audio embeddings from the adapter and the embeddings of previously generated tokens have the same frame-rate of 12.5Hz, with each frame representing 80ms of audio. These are summed and processed by the text decoder, which predicts one token per frame. The decoder emits a padding token[P]while waiting for sufficient acoustic evidence. Once a word is acoustically complete and the target delayτ\\tauhas elapsed, a word-boundary token[W]is emitted to initiate generation, followed by the corresponding subword tokens.",
                "position": 146
            }
        ]
    },
    {
        "header": "3Training",
        "images": []
    },
    {
        "header": "4Inference and Serving in vLLM",
        "images": []
    },
    {
        "header": "5Results",
        "images": []
    },
    {
        "header": "6Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11298/x2.png",
                "caption": "(a)English",
                "position": 532
            },
            {
                "img": "https://arxiv.org/html/2602.11298/x2.png",
                "caption": "(a)English",
                "position": 535
            },
            {
                "img": "https://arxiv.org/html/2602.11298/x3.png",
                "caption": "(b)French",
                "position": 540
            },
            {
                "img": "https://arxiv.org/html/2602.11298/x4.png",
                "caption": "(c)German",
                "position": 545
            },
            {
                "img": "https://arxiv.org/html/2602.11298/x5.png",
                "caption": "(a)English",
                "position": 560
            },
            {
                "img": "https://arxiv.org/html/2602.11298/x5.png",
                "caption": "(a)English",
                "position": 563
            },
            {
                "img": "https://arxiv.org/html/2602.11298/x6.png",
                "caption": "(b)French",
                "position": 568
            },
            {
                "img": "https://arxiv.org/html/2602.11298/x7.png",
                "caption": "(c)German",
                "position": 573
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11298/figures/vllm_session.png",
                "caption": "Figure 5:Voxtral streaming session via vLLM resumable requests.A session is created with an anchor request that includes the initial buffered audio (e.g., the firstτ\\taums plus padding tokens to enforce the target delay) and runs a one-token decoder step. Each subsequent update is sent as a resumable request that appends the next 80 ms audio chunk together with the previously emitted token ID, allowing the engine to reuse cached KV states and emit the next token incrementally. This request–decode–update loop enables low-latency, continuous transcription with full-duplex streaming-input/streaming-output.",
                "position": 1881
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]