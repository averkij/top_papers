[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background and Related Work",
        "images": []
    },
    {
        "header": "3Principled approach toμ\\muP for MoE",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.09752/x1.png",
                "caption": "Figure 1:The plots present MoE performance for varying learning rates in the following set-ups: standard parametrization (SP) with no scaling on the left.simpleP - treating each Expert like a FeedForward layer in the middle.μ\\muP - our theory applied to MoE layer on the right. While in the case of SP, the optimal learning rate is different for different model sizes, both reparameterizations achieve learning rate transfer across model widths.",
                "position": 324
            }
        ]
    },
    {
        "header": "4Experimental results and alternative views on hyperparameter transfer in MoE",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.09752/x2.png",
                "caption": "Figure 2:(a) Varying the number of experts. Given our muP parametrization the optimal learning rate is preserved across varied number of experts. (b) Varying granularity. Learning rate is not preserved across different granularities.",
                "position": 453
            },
            {
                "img": "https://arxiv.org/html/2508.09752/x2.png",
                "caption": "",
                "position": 456
            },
            {
                "img": "https://arxiv.org/html/2508.09752/x3.png",
                "caption": "",
                "position": 460
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMuP for Dense Transformer",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.09752/x4.png",
                "caption": "Figure 3:This figure shows our experiments on learning rate transfer indensemodels. Standard Parameterization (SP) on the lft has different optimal learning rate for each model width, whileμ\\muP has stable optimum.",
                "position": 815
            }
        ]
    },
    {
        "header": "Appendix BExpert–gradient covariance lemma",
        "images": []
    },
    {
        "header": "Appendix CExperimental setup",
        "images": []
    }
]