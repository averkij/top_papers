[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02231/x1.png",
                "caption": "Figure 1:Motivation of AV-SpeakerBench.Existing video benchmarks often contain visually solvable questions—such as counting visible people—where state-of-the-art multimodal models can answer correctly even when the audio stream is muted (left; examples from Video-MME[fu2025video]).\nIn contrast, questions in AV-SpeakerBench (right) are explicitly designed to require audiovisual fusion: the correct answer depends on who speaks, when they speak, and how speech events unfold over time.",
                "position": 141
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3AV-SpeakerBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02231/x2.png",
                "caption": "Figure 2:Top: Examples of audiovisual reasoning questions in AV-SpeakerBench.Each question illustrates a distinct way in which audiovisual dependency is enforced—through spoken‐phrase grounding, visual event conditioning, cross-modal temporal localization, or multi-speaker coordination—ensuring that the correct answer cannot be inferred from a single modality.Bottom: Dataset Distribution.We present the distribution of videos by duration, task category, and visual complexity (measured by the number of unique visible people). Together, these statistics highlight the diversity of conversational scenes and reasoning types represented inAV-SpeakerBench.",
                "position": 431
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02231/x3.png",
                "caption": "(a)Modality ablation across task types.Gemini 2.5 Pro demonstrates consistent multimodal gains across most tasks, whereas Qwen3-Omni 30B exhibits limited or even negative audio contributions in certain tasks.",
                "position": 1020
            },
            {
                "img": "https://arxiv.org/html/2512.02231/x3.png",
                "caption": "(a)Modality ablation across task types.Gemini 2.5 Pro demonstrates consistent multimodal gains across most tasks, whereas Qwen3-Omni 30B exhibits limited or even negative audio contributions in certain tasks.",
                "position": 1023
            },
            {
                "img": "https://arxiv.org/html/2512.02231/x4.png",
                "caption": "(b)Error type distribution across benchmark categories.Most errors occur in audio perception and temporal reasoning tasks.",
                "position": 1029
            },
            {
                "img": "https://arxiv.org/html/2512.02231/x5.png",
                "caption": "Figure 4:Qualitative examples of Gemini 2.5 Proreasoning traces onAV-SpeakerBench.Greenandredhighlight colors indicate the model’s correct and incorrect reasoning, respectively.(a) Vision-only example answered correctly:the model identifies the correct speaker by tracking the duration and consistency of mouth movement and conversational gestures, which serve as natural visual cues for inferring who is speaking.(b) Vision-only example answered incorrectly:the model incorrectly associates slower gestures with slower speech, leading to a wrong prediction.(c) The same example as (b) but with audio input:the model correctly identifies the faster speaker once speech-rate evidence becomes available, confirming that the question requires true audiovisual fusion.(d) Vision + audio example answered incorrectly:the model predicts that only one woman speaks while both women say “Okay” after the event. Eventually, all three speakers talk after the event, showing residual difficulty in temporal alignment and speaker disambiguation.",
                "position": 1062
            }
        ]
    },
    {
        "header": "5Ethical Statement",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix AAnnotation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02231/pics/annotationinterface.png",
                "caption": "Figure 5:Annotation interface for rate–comparison tasks.The interface presents annotators with the video clip, metadata (video ID, category, task type), the question, all answer choices, and the selected response.\nAnnotators also specify the temporal window used for judgment and provide a brief justification.\nThe examples shown correspond to (left) lowest rate of speech, (middle) highest rate of speech, and (right) lowest rate of speech for a different time span within the same video.\nThese examples illustrate how annotators validate temporal reasoning by explicitly grounding answers in the video timeline.",
                "position": 1255
            },
            {
                "img": "https://arxiv.org/html/2512.02231/x6.png",
                "caption": "Figure 6:Examples of trivially solvable questions removed during filtering.(Top) A moment-specific visibility question becomes trivial because only one person is visible throughout the entire clip, making the answer recoverable without grounding to the referenced utterance.\n(Bottom) A speech-content question becomes trivial because the spoken line appears as burned-in captions, allowing the answer to be selected without performing audio-based reasoning.",
                "position": 1288
            }
        ]
    },
    {
        "header": "Appendix BDetailed Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02231/x7.png",
                "caption": "Figure 7:Evaluation prompt used for multimodal LLMs.\nFor each example we show a short video (represented here by keyframes and the waveform), a fixed natural-language instruction, and a multiple-choice question with four options (A–D).\nModels must answer by outputting only the letter of the correct option.",
                "position": 1346
            },
            {
                "img": "https://arxiv.org/html/2512.02231/pics/human_interface.png",
                "caption": "Figure 8:Human evaluation interface.Evaluators watch the video clip, then answer the corresponding multiple-choice question (A–D).\nNo transcript or subtitle is provided.\nThe interface also includes an optional refinement tag and a control question asking for the total number of people visible in the video.\nThis setup ensures that human performance is independent of annotation and directly comparable to model outputs.",
                "position": 1368
            }
        ]
    },
    {
        "header": "Appendix CQualitative Analysis by Error Pattern",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02231/x8.png",
                "caption": "Figure 9:Qualitative examples of Gemini 2.5 Proreasoning traces onAV-SpeakerBench.Greenandredhighlight colors indicate the model’s correct and incorrect reasoning, respectively.\nThe figure above contains representative failure cases spanning four key error patterns: (a) cross-modality attribution, (b) audio and visual perception, (c) temporal grounding, and (d) temporal localization.\nDetailed analyses are provided in the subsection.",
                "position": 1447
            }
        ]
    },
    {
        "header": "Appendix DQuestion Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02231/x9.png",
                "caption": "Figure 10:Visualization of speaker tasks.Top:Speaker Detection.Middle:Speaker Recognition.Bottom:Speaker Counting.",
                "position": 1564
            },
            {
                "img": "https://arxiv.org/html/2512.02231/x10.png",
                "caption": "Figure 11:Visualization of speaker-visual tasks.Top:Activity Recognition.Middle:Visual Counting.Bottom:Attribute Recognition.",
                "position": 1567
            },
            {
                "img": "https://arxiv.org/html/2512.02231/x11.png",
                "caption": "Figure 12:Visualization of speech tasks.Top:Speech Counting.Middle:Speech Duration.Bottom:Speech Recognition.",
                "position": 1570
            },
            {
                "img": "https://arxiv.org/html/2512.02231/x12.png",
                "caption": "Figure 13:Visualization of speech attribute tasks.Top:Speech Intensity.Middle:Speech Pitch.Bottom:Speech Rate.",
                "position": 1573
            }
        ]
    },
    {
        "header": "Appendix EPerformance of Gemini 3 Pro on AV-SpeakerBench",
        "images": []
    }
]