[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11052/x1.png",
                "caption": "Figure 1:Comparison between the existing decoding strategy and the proposed method. Differentcolours represent distinct tokens, while gradient colours indicate predicted token representations.Top:In the existing strategy, all[MASK]tokens share the same embedding and are repeatedly remasked if not selected.Bottom:In LRD, Phase 1 refines each[MASK]embedding, and Phase 2 progressively commits confident tokens while keeping uncertain ones soft for context-aware decoding.",
                "position": 117
            }
        ]
    },
    {
        "header": "2Preliminary",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11052/images/kl.png",
                "caption": "Figure 2:KL divergence between step-wise predictive distributions and final decoded results for LLaDA-1.5 and Dream-Ins across benchmarks. The red vertical line marks where decoding begins after a fixed 20-step latent refinement.",
                "position": 722
            },
            {
                "img": "https://arxiv.org/html/2510.11052/images/kl.png",
                "caption": "Figure 2:KL divergence between step-wise predictive distributions and final decoded results for LLaDA-1.5 and Dream-Ins across benchmarks. The red vertical line marks where decoding begins after a fixed 20-step latent refinement.",
                "position": 725
            },
            {
                "img": "https://arxiv.org/html/2510.11052/images/hotstart.png",
                "caption": "Figure 3:Convergence ratios across latent refinement steps for LLaDA-1.5 and Dream-Ins on four benchmarks. Since computing the difference in KL divergence requires at least three consecutive steps, the curves are plotted starting from step 2.",
                "position": 730
            },
            {
                "img": "https://arxiv.org/html/2510.11052/images/base_rate.png",
                "caption": "Figure 4:Accuracy of Dream-Ins on four benchmarks under different Maximum token proportion, whererfr_{f}=0 corresponds to the no mixing.",
                "position": 1320
            },
            {
                "img": "https://arxiv.org/html/2510.11052/images/base_rate.png",
                "caption": "Figure 4:Accuracy of Dream-Ins on four benchmarks under different Maximum token proportion, whererfr_{f}=0 corresponds to the no mixing.",
                "position": 1323
            },
            {
                "img": "https://arxiv.org/html/2510.11052/images/topp.png",
                "caption": "Figure 5:Effect of top-ppmixing on Dream-Ins across four benchmarks. The purple curve shows the log fraction of tokens included in the mixture.",
                "position": 1328
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Reproducibility statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "The Use of LLMs",
        "images": []
    },
    {
        "header": "Appendix AExperiment Details",
        "images": []
    },
    {
        "header": "Appendix BDerivation of the True Posterior in the Masking Process",
        "images": []
    },
    {
        "header": "Appendix CIntegration with Semi-AR Framework",
        "images": []
    },
    {
        "header": "Appendix DStability Analysis of Mixed Embedding Updates",
        "images": []
    }
]