[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.10316/x1.png",
                "caption": "",
                "position": 134
            }
        ]
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.10316/x2.png",
                "caption": "Figure 1:BrushEditcan achieve all-in-one inpainting for arbitrary mask shapes without requiring separate model training for each mask type. This flexibility in handling arbitrary shapes also enhances user-driven editing, as user-provided masks often combine segmentation-based structural details with random mask noise. By supporting arbitrary mask shapes,BrushEditavoids the artifacts introduced by the random-mask version of BrushNet-Ran and the edge inconsistencies caused by the segmentation-mask version BrushNet-Seg‚Äôs strong reliance on boundary shapes.",
                "position": 150
            }
        ]
    },
    {
        "header": "IIRelated Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.10316/x3.png",
                "caption": "Figure 2:Model overview.Our model outputs an inpainted image given the mask and masked image input. Firstly, we downsample the mask to accommodate the size of the latent, and input the masked image to the VAE encoder to align the distribution of latent space. Then, noisy latent, masked image latent, and downsampled mask are concatenated as the input ofBrushEdit. The feature extracted fromBrushEditis added to pretrained UNet layer by layer after a zero convolution block[33]. After denoising, the generated image and masked image are blended with a blurred mask.",
                "position": 358
            }
        ]
    },
    {
        "header": "IIIPreliminaries and Motivation",
        "images": []
    },
    {
        "header": "IVMethod",
        "images": []
    },
    {
        "header": "VExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.10316/x4.png",
                "caption": "Figure 3:Benchmark overview.I and II separately show natural and artificial images, masks, and caption ofBrushBench. (a) to (d) show images of humans, animals, indoor scenarios, and outdoor scenarios. Each group of images shows the original image, inside-inpainting mask, and outside-inpainting mask, with an image caption on the top. III show image, mask, and caption from EditBench[32], with (e) for generated images and (f) for natural images. The images are randomly selected from both benchmarks.",
                "position": 595
            },
            {
                "img": "https://arxiv.org/html/2412.10316/x5.png",
                "caption": "Figure 4:Comparison of previous editing methods andBrushEditon natural and synthetic images, covering image editing operations such as removing objects¬†(I), adding objects¬†(II), modifying attributes¬†(III), and swapping objects¬†(IV).",
                "position": 666
            },
            {
                "img": "https://arxiv.org/html/2412.10316/x6.png",
                "caption": "Figure 5:Performance comparisons ofBrushEditand previous image inpainting methodsacross various inpainting tasks: (I) Random Mask Inpainting (II) Segmentation Mask Inpainting. Each group of results contains7777inpainting methods: (b) Blended Latent Diffusion (BLD)[27], (c) Stable Diffusion Inpainting (SDI)[5], (d) HD-Painter (HDP)[30], (e) PowerPaint (PP)[29], (f) ControlNet-Inpainting (CNI)[33], (g) Our Previous BrushNet and (h) Ours.",
                "position": 1145
            },
            {
                "img": "https://arxiv.org/html/2412.10316/x7.png",
                "caption": "Figure 6:IntegratingBrushEditto community fine-tuned diffusion models.We use five popular community diffusion models fine-tuned from stable diffusion v1.5: DreamShaper (DS)[99], epiCRealism (ER)[100], Henmix_Real (HR)[101], MeinaMix (MM)[102], and Realistic Vision (RV)[103]. MM is specifically designed for anime images.",
                "position": 1157
            },
            {
                "img": "https://arxiv.org/html/2412.10316/x8.png",
                "caption": "Figure 7:Flexible control scale ofBrushEdit.(a) shows the given masked image, (b)-(h) show adding control scalewùë§witalic_wfrom1.01.01.01.0to0.20.20.20.2. Results show a gradually diminishing controllable ability from precise to rough control.",
                "position": 1160
            }
        ]
    },
    {
        "header": "VIDiscussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]