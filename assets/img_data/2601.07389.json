[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.07389/training-pipeline.png",
                "caption": "Figure 1:Training pipeline for modern LLMs. This work focuses on two post-training methods, Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), that refine a pretrained base model after its initial pretraining phase.",
                "position": 96
            },
            {
                "img": "https://arxiv.org/html/2601.07389/two-stage.png",
                "caption": "Figure 2:Any combination of SFT and RL in post-training reduces to the two canonical pipelines:(a)SFT-then-RL and(b)RL-then-SFT.",
                "position": 120
            }
        ]
    },
    {
        "header": "2Problem Setup",
        "images": []
    },
    {
        "header": "3SFT-then-RL Coupling",
        "images": []
    },
    {
        "header": "4RL-then-SFT Coupling",
        "images": []
    },
    {
        "header": "5Empirical Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.07389/x1.png",
                "caption": "(a)",
                "position": 683
            },
            {
                "img": "https://arxiv.org/html/2601.07389/x1.png",
                "caption": "(a)",
                "position": 686
            },
            {
                "img": "https://arxiv.org/html/2601.07389/x2.png",
                "caption": "(b)",
                "position": 691
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]