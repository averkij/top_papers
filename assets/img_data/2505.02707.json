[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02707/extracted/6412745/Figures/voila-logo.png",
                "caption": "",
                "position": 121
            },
            {
                "img": "https://arxiv.org/html/2505.02707/extracted/6412745/Figures/hf-logo.png",
                "caption": "",
                "position": 136
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02707/x1.png",
                "caption": "Figure 1:Different paradigms of voice conversation systems:(a)Traditional pipeline systems, such as Apple Siri, Amazon Alexa, and Google Assistant, launched in the 2010s;(b)Simplified pipeline systems using LLMs to handle text-based understanding and response generation;(c)End-to-end audio-in, audio-out systems that offer low latency and rich vocal nuances;(d)Autonomous systems that further enable dynamic, proactive interactions.",
                "position": 184
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02707/x2.png",
                "caption": "Figure 2:Voilamodels:(a)Voila-e2efor end-to-end voice conversation,(b)Voila-autonomousfor autonomous interaction. Both models allow easy customization of speaker characteristics and voice via text instructions and audio samples.",
                "position": 278
            }
        ]
    },
    {
        "header": "3Voila: Voice-Language Foundation Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02707/extracted/6412745/Figures/text_audio_interleave_v4.png",
                "caption": "Figure 3:Text and audio interleaved alignment.",
                "position": 297
            },
            {
                "img": "https://arxiv.org/html/2505.02707/extracted/6412745/Figures/input_embedding_and_output_decoding_v7.png",
                "caption": "Figure 4:Input embedding and output decoding inVoila.",
                "position": 310
            },
            {
                "img": "https://arxiv.org/html/2505.02707/extracted/6412745/Figures/expand2full_v2.png",
                "caption": "Figure 5:Voila-autonomoustwo-stream inputs, including user’s audio stream andVoila’s own audio stream.",
                "position": 313
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02707/extracted/6412745/Figures/domain_circular_packing.png",
                "caption": "Figure 6:Domain distribution inVoilaBenchmark",
                "position": 352
            },
            {
                "img": "https://arxiv.org/html/2505.02707/extracted/6412745/Figures/model_scores_radar_v2.png",
                "caption": "Figure 7:Performance comparison across the diverse domains inVoilaBenchmark.",
                "position": 366
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]