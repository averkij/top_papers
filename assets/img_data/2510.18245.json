[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18245/x1.png",
                "caption": "Figure 1:Although larger models generally achieve lower inference throughput than smaller ones, Qwen2.5-1.5B outperforms Qwen3-0.6B. Despite having the same number of layers, Qwen2.5-1.5B benefits from a higher hidden size, GQA, and mlp-to-attention ratio.",
                "position": 123
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Model Architecture-Aware Scaling Laws",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18245/x2.png",
                "caption": "Figure 2:Inference throughputvs (left) hidden sized=dmodeld=d_{\\text{model}}and (right) mlp-to-attention ratior=rmlp/attnr=r_{\\text{mlp}/\\text{attn}}on the 8B model. Under a fixed parameter budgetNnon-embedN_{\\text{non-embed}}, larger hidden sizes and higher mlp-to-attention ratios improve inference throughput for varying batch sizes.",
                "position": 179
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x2.png",
                "caption": "",
                "position": 182
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x3.png",
                "caption": "",
                "position": 187
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x4.png",
                "caption": "Figure 3:Loss vs. hidden size:(Left) 80M model variants; (Center) 145M model variants; (Right) 297M model variants. Across model sizes, the relationship between training loss anddmodel/Nd_{\\text{model}}/\\sqrt{N}exhibits a consistent U-shaped curve when architectural factors such as GQA and the MLP-to-attention ratio are held fixed. The legend denotes the MLP-to-attention ratior=rmlp/attnr=r_{\\text{mlp}/\\text{attn}}for each model.",
                "position": 245
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x4.png",
                "caption": "",
                "position": 248
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x5.png",
                "caption": "",
                "position": 253
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x6.png",
                "caption": "",
                "position": 258
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x7.png",
                "caption": "Figure 4:Loss vs. MLP-to-attention ratio:(Left) 80M model variants; (Center) 145M model variants; (Right) 297M model variants. Across model sizes, the relationship between training loss andrmlp/attnr_{\\text{mlp}/\\text{attn}}exhibits a consistent U-shaped curve when architectural factors such as GQA and hidden size are held fixed. The legend denotes the hidden sized=dmodeld=d_{\\text{model}}for each model.",
                "position": 264
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x7.png",
                "caption": "",
                "position": 267
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x8.png",
                "caption": "",
                "position": 271
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x9.png",
                "caption": "",
                "position": 276
            }
        ]
    },
    {
        "header": "4Experiment Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18245/x10.png",
                "caption": "Figure 5:Predictive performancesof the fitted conditional scaling law on: (left) Task 1: Fit on8080M, evaluate on145145M; (center) Task 2: Fit on80,14580,145M, evaluate on297297M; (right) Task 3: Fit on80,145,29780,145,297M, evaluate on11B. Orange dots denote fitting data points, and purple crosses indicate the test data points.\nWe compare scaling-law predicted loss with actual pretraining loss of architectures and observed a consistently low MSE and high Spearman correlation across model scales.",
                "position": 430
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x10.png",
                "caption": "",
                "position": 433
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x11.png",
                "caption": "",
                "position": 438
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x12.png",
                "caption": "",
                "position": 443
            }
        ]
    },
    {
        "header": "5Experiment Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18245/x13.png",
                "caption": "Figure 6:Results for 1B and 3B models:(left) Panda-1B closely follows the scaling law predictions for minimizing training loss. (center) Inference throughput comparison between LLaMA-3.2-1B and Surefire-1B, showing that Surefire-1B consistently achieves higher efficiency across batch sizes. (right) Inference throughput comparison between LLaMA-3.2-3B and Surefire-3B, demonstrating that Surefire-3B consistently delivers higher efficiency across all batch sizes.",
                "position": 563
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x13.png",
                "caption": "",
                "position": 566
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x14.png",
                "caption": "",
                "position": 571
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x15.png",
                "caption": "",
                "position": 576
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x16.png",
                "caption": "Figure 7:Effect of the Fitting Dataset on Predictive Performancevs (left) Fit on 80, 145, 297M, 1B, evaluate on 3B; (right) Fit on 1B, evaluate on 3B. Orange dots denote fitting data points, and purple crosses indicate the test data points. We compare scaling-law predicted loss with actual pretraining loss of architectures and we observe that fitting the scaling laws with only 1B model data yields lower MSE and higher Spearman correlation for the 3B model loss prediction.",
                "position": 659
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x16.png",
                "caption": "",
                "position": 662
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x17.png",
                "caption": "",
                "position": 666
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Limitations and Future Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALLM Usage",
        "images": []
    },
    {
        "header": "Appendix BOpen-Weighted Model Architectures",
        "images": []
    },
    {
        "header": "Appendix CModel Architectures",
        "images": []
    },
    {
        "header": "Appendix DHyper-parameters",
        "images": []
    },
    {
        "header": "Appendix EAdditional Inference Evaluation Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18245/x18.png",
                "caption": "Figure 8:Hidden size on Inference Throughput:(left) 1B model variants; (center) 3B model variants; (right) 8B model variants. Across varying batch sizes and model scales, larger hidden sizes yield higher inference throughput under a fixed parameter budget. The legend indicates the hidden size of the models, whered=dmodeld=d_{\\text{model}}.",
                "position": 3398
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x18.png",
                "caption": "",
                "position": 3401
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x19.png",
                "caption": "",
                "position": 3405
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x20.png",
                "caption": "",
                "position": 3410
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x21.png",
                "caption": "Figure 9:MLP-to-Attention ratio on Inference Throughput:(left) 1B model variants; (center) 3B model variants; (right) 8B model variants. Across varying batch sizes and model scales, a larger MLP-to-Attention ratio increases inference throughput under a fixed parameter budget. The legend indicates the MLP-to-Attention ratio of the models, wherer=rmlp/attnr=r_{\\text{mlp}/\\text{attn}}.",
                "position": 3417
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x21.png",
                "caption": "",
                "position": 3420
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x22.png",
                "caption": "",
                "position": 3425
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x23.png",
                "caption": "",
                "position": 3430
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x24.png",
                "caption": "Figure 10:GQA on Inference Throughput:(left) 1B model variants; (center) 3B model variants; (right) 8B model variants. This figure shows the impact of GQA on inference throughput. With the total parameter count fixed, hidden size is set to 2048 (1B), 3072 (3B), and 4096 (8B), and the MLP-to-Attention ratio is 4.0, 2.67, and 4.2, respectively. Across varying batch sizes, models with larger GQA achieve higher throughput. All evaluations are performed using the vLLM frameworkKwon et al. (2023)on a single NVIDIA Ampere 40GB A100 GPU with 4096 input and 1024 output tokens.",
                "position": 3436
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x24.png",
                "caption": "",
                "position": 3439
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x25.png",
                "caption": "",
                "position": 3443
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x26.png",
                "caption": "",
                "position": 3448
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x27.png",
                "caption": "Figure 11:Hidden size on Inference Throughput (Qwen3):(left) Qwen3-0.6B model variants; (center) Qwen3-1.7B model variants; (right) Qwen3-4B model variants. Across varying batch sizes and model scales, larger hidden sizes yield higher inference throughput under a fixed parameter budget. The legend indicates the hidden size of the models, whered=dmodeld=d_{\\text{model}}. All evaluations are performed using the vLLM frameworkKwon et al. (2023)on a single NVIDIA Ampere 40GB A100 GPU with 4096 input and 1024 output tokens.",
                "position": 3457
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x27.png",
                "caption": "",
                "position": 3460
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x28.png",
                "caption": "",
                "position": 3465
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x29.png",
                "caption": "",
                "position": 3470
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x30.png",
                "caption": "Figure 12:MLP-to-Attention ratio on Inference Throughput (Qwen3):(left) Qwen3-0.6B model variants; (center) Qwen3-1.7B model variants; (right) Qwen3-4B model variants. Across varying batch sizes and model scales, a larger MLP-to-Attention ratio increases inference throughput under a fixed parameter budget. The legend indicates the MLP-to-Attention ratio of the models, wherer=rmlp/attnr=r_{\\text{mlp}/\\text{attn}}. All evaluations are performed using the vLLM frameworkKwon et al. (2023)on a single NVIDIA Ampere 40GB A100 GPU with 4096 input and 1024 output tokens.",
                "position": 3477
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x30.png",
                "caption": "",
                "position": 3480
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x31.png",
                "caption": "",
                "position": 3485
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x32.png",
                "caption": "",
                "position": 3490
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x33.png",
                "caption": "Figure 13:GQA on Inference Throughput (Qwen3):(left) Qwen3-0.6B model variants; (center) Qwen3-1.7B model variants; (right) Qwen3-4B model variants. This figure shows the impact of GQA on inference throughput. With the total parameter count fixed, hidden size is set to 1024 (0.6B), 2048 (1.7B), and 2560 (4B), and the MLP-to-Attention ratio is 1.5, 3.0, and 2.85, respectively. Across varying batch sizes, models with larger GQA achieve higher throughput. All evaluations are performed using the vLLM frameworkKwon et al. (2023)on a single NVIDIA Ampere 40GB A100 GPU with 4096 input and 1024 output tokens.",
                "position": 3496
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x33.png",
                "caption": "",
                "position": 3499
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x34.png",
                "caption": "",
                "position": 3504
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x35.png",
                "caption": "",
                "position": 3508
            }
        ]
    },
    {
        "header": "Appendix FAdditional Results: Loss vs. Model Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18245/x36.png",
                "caption": "Figure 14:Loss vs. GQA:(left) 80M model variants; (center) 145M model variants; (right) 297M model variants. Across different model sizes, the relationship between training loss and GQA varies substantially when hidden size and the mlp-to-attention ratio are fixed. The legend denotes the hidden size of each trained model.",
                "position": 3522
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x36.png",
                "caption": "",
                "position": 3525
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x37.png",
                "caption": "",
                "position": 3529
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x38.png",
                "caption": "",
                "position": 3534
            }
        ]
    },
    {
        "header": "Appendix GMore Ablation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18245/x39.png",
                "caption": "Figure 15:Ablation Study:(left) use multiplicative calibrations without outliers; (center) use multiplicative calibrations with outliers; (right) use additive calibrations without outliers. The outlier refers to models trained with an mlp-to-attention ratio below 0.5 or above 5. We observe that outlier data points harm the scaling law fit. Moreover, while multiplicative and additive calibrations differ in formulation, their MSE and Spearman values remain nearly identical. Dots denote the data points used for fitting, while crosses indicate the test data points.",
                "position": 3559
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x39.png",
                "caption": "",
                "position": 3562
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x40.png",
                "caption": "",
                "position": 3566
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x41.png",
                "caption": "",
                "position": 3571
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x42.png",
                "caption": "Figure 16:Joint and non-separable calibrations:(left) use multiplicative calibrations; (right) use joint and non-separable calibrations. We observe that joint and non-separable calibrations yield higher MSE and lower Spearman scores than multiplicative calibrations, indicating inferior performance. Dots denote the data points used for fitting, while crosses indicate the test data points.",
                "position": 3577
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x42.png",
                "caption": "",
                "position": 3580
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x43.png",
                "caption": "",
                "position": 3584
            }
        ]
    },
    {
        "header": "Appendix HInference FLOPs Analysis",
        "images": []
    },
    {
        "header": "Appendix IMore Large-scale Training Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18245/x44.png",
                "caption": "Figure 17:Active-Experts-to-Attn on Inference Throughput:(left) 3B-A1.1B model variants; (center) 5.3B-A1.7B model variants; (right) 8.3B-A1.5B model variants. We study the effect of the Active-Experts-to-Attention ratio on inference throughput by fixing the total number of active parameters, setting GQA to 4, and using a batch size of 2048 to reduce MoE inference variance in this figure. All evaluations are performed using the vLLM frameworkKwon et al. (2023)on a single NVIDIA Ampere 40GB A100 GPU with 1024 input and 256 output tokens.",
                "position": 3879
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x44.png",
                "caption": "",
                "position": 3882
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x45.png",
                "caption": "",
                "position": 3886
            },
            {
                "img": "https://arxiv.org/html/2510.18245/x46.png",
                "caption": "",
                "position": 3891
            }
        ]
    },
    {
        "header": "Appendix JMoE Inference",
        "images": []
    }
]