[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09201/x1.png",
                "caption": "Figure 1:Concept Figure.(A) Existing prompt optimization approaches restrict the optimization to the textual space, leaving MLLMs underutilized by failing to provide rich contextual signals. (B) Our multimodal prompt optimization expands the optimization space into multimodality, allowing the discovery of salient multimodal context and fully leveraging the expressive capacity of MLLMs.",
                "position": 161
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09201/x2.png",
                "caption": "Figure 2:Overview of MPO, consisting of two components. (A) Alignment-preserving exploration analyzes a failure set to generate feedback, which is then used both to refine the textual prompt and to guide a modality-specific generator to create a new non-textual prompt with one of three operators. (B) Prior-Inherited Bayesian UCB Selection leverages the parent‚Äôs performance as an informative prior, warm-starting the search to effectively identify high-performing prompts among candidates.",
                "position": 191
            }
        ]
    },
    {
        "header": "3Methodology: Multimodal Prompt Optimizer",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09201/x3.png",
                "caption": "Figure 3:Correlation of parent and child scores.",
                "position": 323
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09201/x4.png",
                "caption": "Table 2:Generalizability results of MPOacross components with different backbones: (Top) base models; (Bottom Left) optimizer models; (Bottom Right) modality-specific generators.",
                "position": 652
            },
            {
                "img": "https://arxiv.org/html/2510.09201/x4.png",
                "caption": "Figure 4:Relationship between cross-modal alignment and performance gain. We report median values alongside Q1 and Q3.",
                "position": 799
            },
            {
                "img": "https://arxiv.org/html/2510.09201/x5.png",
                "caption": "Table 3:Ablation on the contribution of each modality in the optimized multimodal prompt.",
                "position": 823
            },
            {
                "img": "https://arxiv.org/html/2510.09201/x6.png",
                "caption": "Figure 6:Image prompt optimization processof the best-performing multimodal prompt on a subtask (i.e., grosbeak species classification) of CUB. ‚ÄúTask Classes‚Äù box contains the examples of four species: Rose Breasted Grosbeak, Pine Grosbeak, Blue Grosbeak, and Evening Grosbeak.",
                "position": 934
            },
            {
                "img": "https://arxiv.org/html/2510.09201/x7.png",
                "caption": "Figure 7:Train Curve of MPOcompared to ProTeGi on CUB.",
                "position": 955
            },
            {
                "img": "https://arxiv.org/html/2510.09201/x7.png",
                "caption": "Figure 7:Train Curve of MPOcompared to ProTeGi on CUB.",
                "position": 958
            },
            {
                "img": "https://arxiv.org/html/2510.09201/x8.png",
                "caption": "Figure 8:Visualization of hidden statesin MLLMs by PCA.",
                "position": 963
            },
            {
                "img": "https://arxiv.org/html/2510.09201/x9.png",
                "caption": "Figure 9:Analysis of the prior strength(SS) on performance.",
                "position": 968
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Experimental Details",
        "images": []
    },
    {
        "header": "Appendix BTheoretical Analysis on Prior-Inherited Bayesian UCB",
        "images": []
    },
    {
        "header": "Appendix CAdditional Experimental Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09201/x10.png",
                "caption": "Figure 14:The optimization process for the best multimodal prompt on the BBBP task. Inherited substructures from the parent molecule are marked with the same colored circles.",
                "position": 2952
            },
            {
                "img": "https://arxiv.org/html/2510.09201/x11.png",
                "caption": "Table 8:Operation examples for the image prompt update, including parent image prompts, resulting child image prompts, and the textual conditionùíÑ\\bm{c}to the modality-specific generator, i.e., GPT-Image.",
                "position": 2955
            },
            {
                "img": "https://arxiv.org/html/2510.09201/x12.png",
                "caption": "",
                "position": 3020
            },
            {
                "img": "https://arxiv.org/html/2510.09201/x13.png",
                "caption": "",
                "position": 3095
            },
            {
                "img": "https://arxiv.org/html/2510.09201/Figures/qualitative/grape.jpg",
                "caption": "Table 9:Qualitative examples of the optimized multimodal (image and text) prompts.",
                "position": 3128
            },
            {
                "img": "https://arxiv.org/html/2510.09201/Figures/qualitative/grosbeak.jpg",
                "caption": "",
                "position": 3206
            },
            {
                "img": "https://arxiv.org/html/2510.09201/Figures/qualitative/mri.jpg",
                "caption": "",
                "position": 3321
            },
            {
                "img": "https://arxiv.org/html/2510.09201/Figures/qualitative/drivingvqa.jpg",
                "caption": "",
                "position": 3348
            },
            {
                "img": "https://arxiv.org/html/2510.09201/Figures/qualitative/rsvqa.jpg",
                "caption": "",
                "position": 3407
            },
            {
                "img": "https://arxiv.org/html/2510.09201/Figures/qualitative/drivact.jpg",
                "caption": "Table 10:Qualitative examples of the optimized multimodal (video and text) prompts.",
                "position": 3466
            },
            {
                "img": "https://arxiv.org/html/2510.09201/Figures/qualitative/vanebench.jpg",
                "caption": "",
                "position": 3510
            },
            {
                "img": "https://arxiv.org/html/2510.09201/Figures/qualitative/hia.jpg",
                "caption": "Table 11:Qualitative examples of the optimized multimodal (molecule and text) prompts.",
                "position": 3541
            },
            {
                "img": "https://arxiv.org/html/2510.09201/Figures/qualitative/bbb.jpg",
                "caption": "Table 12:Qualitative examples of the optimized multimodal (molecule and text) prompts.",
                "position": 3638
            },
            {
                "img": "https://arxiv.org/html/2510.09201/Figures/qualitative/cyp2c19.jpg",
                "caption": "",
                "position": 3819
            }
        ]
    },
    {
        "header": "Appendix DUse of Large Language Models (LLMs)",
        "images": []
    }
]