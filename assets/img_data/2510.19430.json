[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19430/x1.png",
                "caption": "",
                "position": 70
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3GigaBrain-0 Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19430/x2.png",
                "caption": "Figure 2:The framework ofGigaBrain-0.GigaBrain-0takes RGB-D input to enhance spatial perception and outputs Embodied Chain-of-Thought (Embodied CoT) as an intermediate representation to strengthen embodied reasoning for manipulation. During training,GigaBrain-0employs Knowledge Insulation(Driess et al.,2025)to prevent interference between the optimization processes of action prediction and Embodied CoT generation.",
                "position": 121
            }
        ]
    },
    {
        "header": "4GigaBrain-0 Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19430/x3.png",
                "caption": "Figure 3:GigaBrain-0’s self-collected real-world robot data is gathered from PiPER arms and the AgiBot G1 platform, spanning diverse environments including homes, supermarkets, factories, and office settings.",
                "position": 468
            },
            {
                "img": "https://arxiv.org/html/2510.19430/x4.png",
                "caption": "Figure 4:GigaWorld enables Real2Real apperance transfer by taking real-world captured data and generating generalized variations in texture, color, lighting, and material properties.",
                "position": 494
            },
            {
                "img": "https://arxiv.org/html/2510.19430/x5.png",
                "caption": "Figure 5:GigaWorld supports view transfer by re-rendering real-world captured data from diverse viewpoints, thereby enriching the dataset with varied perspective changes.",
                "position": 500
            },
            {
                "img": "https://arxiv.org/html/2510.19430/x6.png",
                "caption": "Figure 6:GigaWorld enables Sim2Real transfer by generalizing simulation-collected data in terms of texture, color, lighting, and material properties to better bridge the domain gap and enhance realism.",
                "position": 511
            },
            {
                "img": "https://arxiv.org/html/2510.19430/x7.png",
                "caption": "Figure 7:GigaWorld supports egocentric human video transfer by transforming first-person human hand actions into robotic manipulation scenarios, effectively mapping human demonstrations to robot-executable tasks.",
                "position": 520
            },
            {
                "img": "https://arxiv.org/html/2510.19430/x8.png",
                "caption": "Figure 8:GigaWorld can generate diverse future trajectories from the same initial frame under different text prompts, thereby augmenting the dataset with novel manipulation sequences.",
                "position": 533
            },
            {
                "img": "https://arxiv.org/html/2510.19430/images/mv.png",
                "caption": "Figure 9:GigaWorld can generate multi-view consistent videos, thereby enabling 3D-aware training and improving spatial reasoning in downstream tasks.",
                "position": 536
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19430/x9.png",
                "caption": "Figure 10:Performance comparison betweenGigaBrain-0andπ0\\pi_{0}across six tasks on G1 and PiPER robot platforms, where (a)Laundry Foldingand (b)Paper Towel Preparationare dexterous manipulation tasks; (c)Juice Preparationand (d)Table Bussingare long-horizon tasks; (e)Boxes Movingand (f)Laundry Baskets Movingare mobile manipulation tasks.",
                "position": 559
            },
            {
                "img": "https://arxiv.org/html/2510.19430/x10.png",
                "caption": "Figure 11:Deployment ofGigaBrain-0on the G1 humanoid robot for real-worldlaundry folding.",
                "position": 570
            },
            {
                "img": "https://arxiv.org/html/2510.19430/x11.png",
                "caption": "Figure 12:Deployment ofGigaBrain-0on the PiPER arms for real-worldpaper towel preparation.",
                "position": 573
            },
            {
                "img": "https://arxiv.org/html/2510.19430/x12.png",
                "caption": "Figure 13:Deployment ofGigaBrain-0on PiPER arms for real-worldtable bussing.",
                "position": 588
            },
            {
                "img": "https://arxiv.org/html/2510.19430/x13.png",
                "caption": "Figure 14:Deployment ofGigaBrain-0on G1 humanoid robot for real-worldjuice preparation.",
                "position": 591
            },
            {
                "img": "https://arxiv.org/html/2510.19430/x14.png",
                "caption": "Figure 15:Deployment ofGigaBrain-0on the G1 humanoid robot for real-worldpaper towel preparation.",
                "position": 606
            },
            {
                "img": "https://arxiv.org/html/2510.19430/x15.png",
                "caption": "Figure 16:Deployment ofGigaBrain-0on the PiPER arms for real-worldlaundry baskets moving.",
                "position": 609
            },
            {
                "img": "https://arxiv.org/html/2510.19430/images/generalization_exp.png",
                "caption": "Figure 17:Generalization performance ofGigaBrain-0under appearance, placement, and viewpoint shifts. The horizontal axis denotes the sampling probabilityα\\alphaof world model-generated data used during training.",
                "position": 616
            },
            {
                "img": "https://arxiv.org/html/2510.19430/x16.png",
                "caption": "Figure 18:Illustration of the appearance generalization experiment onlaundry folding: one white garment and nine items with varied colors and textures.",
                "position": 626
            },
            {
                "img": "https://arxiv.org/html/2510.19430/x17.png",
                "caption": "Figure 19:Illustration of the object placement generalization experiment ontable bussing: one collected placement and nine novel object placements.",
                "position": 634
            },
            {
                "img": "https://arxiv.org/html/2510.19430/x18.png",
                "caption": "Figure 20:Illustration of the camera viewpoint generalization experiment ontable bussing: one collected viewpoint and eight novel camera viewpoints.",
                "position": 647
            }
        ]
    },
    {
        "header": "6Conclusion and Future Work",
        "images": []
    },
    {
        "header": "7Contributors",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]