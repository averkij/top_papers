[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14868/x1.png",
                "caption": "(a)GPU memory breakdown across various personalization methods.",
                "position": 129
            },
            {
                "img": "https://arxiv.org/html/2503.14868/x1.png",
                "caption": "(a)GPU memory breakdown across various personalization methods.",
                "position": 132
            },
            {
                "img": "https://arxiv.org/html/2503.14868/x2.png",
                "caption": "(b)VRAM usage versus image and text alignment scores.",
                "position": 137
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14868/x3.png",
                "caption": "(a)Overall illustration of ZOODiP training framework.",
                "position": 181
            },
            {
                "img": "https://arxiv.org/html/2503.14868/x3.png",
                "caption": "(a)Overall illustration of ZOODiP training framework.",
                "position": 184
            },
            {
                "img": "https://arxiv.org/html/2503.14868/x4.png",
                "caption": "(b)Illustration of Subspace Gradient (SG) updates.",
                "position": 189
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14868/x5.png",
                "caption": "Figure 3:Sparse effective dimension in the token trained with Textual Inversion. Notably, the concept was preserved even when retaining only one-third of the optimized dimensions (k=256ùëò256k=256italic_k = 256).",
                "position": 358
            },
            {
                "img": "https://arxiv.org/html/2503.14868/x6.png",
                "caption": "Figure 4:Textual Inversion[17]with various timestep sampling. When the timesteptùë°titalic_tfor training is sampled fromU‚Å¢(0,500)ùëà0500U(0,500)italic_U ( 0 , 500 ), key conceptual features such as color and body shape of the reference image are not effectively trained. In contrast, sampling fromU‚Å¢(500,1000)ùëà5001000U(500,1000)italic_U ( 500 , 1000 )results in successful learning of these features.",
                "position": 443
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14868/x7.png",
                "caption": "Figure 5:Qualitative comparison of image and text alignment.This figure shows how well each method generates images that match the input text prompt while preserving the identity of the personalized subject. ZOODiP generates images that faithfully reflect the prompt while maintaining the concept of the reference image, demonstrating strong image-text alignment.",
                "position": 627
            },
            {
                "img": "https://arxiv.org/html/2503.14868/extracted/6288943/figure/fig6_style.png",
                "caption": "Figure 6:Qualitative results on style personalization.This figure showcases the results of style personalization achieved through ZOODiP, using few reference images with a consistent style. The outcome highlights ability of ZOODiP to personalize not only the subject but also the style with a high degree of accuracy. This demonstrates the versatility and extensive personalization capabilities of ZOODiP, effectively adapting both stylistic elements and subject details to match the reference images.",
                "position": 630
            },
            {
                "img": "https://arxiv.org/html/2503.14868/extracted/6288943/figure/clip_t_PUTS.png",
                "caption": "(a)CLIP-T scores.",
                "position": 805
            },
            {
                "img": "https://arxiv.org/html/2503.14868/extracted/6288943/figure/clip_t_PUTS.png",
                "caption": "(a)CLIP-T scores.",
                "position": 808
            },
            {
                "img": "https://arxiv.org/html/2503.14868/extracted/6288943/figure/clip_i_PUTS.png",
                "caption": "(b)CLIP-I scores.",
                "position": 814
            },
            {
                "img": "https://arxiv.org/html/2503.14868/extracted/6288943/figure/dino_PUTS.png",
                "caption": "(c)DINO scores.",
                "position": 820
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14868/x8.png",
                "caption": "Figure 8:Histogram ofi‚àó/œÑsuperscriptùëñùúèi^{*}/\\tauitalic_i start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT / italic_œÑratios for<dog6>and<shiny_sneaker>dataset with hyperparameterœÑ=128ùúè128\\tau=128italic_œÑ = 128,ŒΩ=10‚àí3ùúàsuperscript103\\nu=10^{-3}italic_ŒΩ = 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPTduring training with SG. Despite the smallŒΩùúà\\nuitalic_ŒΩ, a significant portion (>80%absentpercent80>80\\%> 80 %) of dimensions are projected out.",
                "position": 836
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "S1Additional Experimental Details",
        "images": []
    },
    {
        "header": "S2Additional Ablation Study",
        "images": []
    },
    {
        "header": "S3Additional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14868/x9.png",
                "caption": "Figure S1:Generated images with the prompt‚Äúa photo of a dog‚Äùwith various weight precision. While INT8 precision produces results nearly equivalent to full-precision performance, INT4 precision exhibits noticeable degradation in image quality, highlighting the trade-off between lower precision and fidelity.",
                "position": 1811
            },
            {
                "img": "https://arxiv.org/html/2503.14868/x10.png",
                "caption": "Figure S2:Qualitative results of U-Net precision at INT8 and INT4 in<dog6>dataset. ZOODiP works on INT4 and INT8, but performance diminishes due to degradation caused by INT4 quantization.",
                "position": 1821
            },
            {
                "img": "https://arxiv.org/html/2503.14868/x11.png",
                "caption": "Figure S3:Qualitative comparison of the diversity of generated imagesThis figure compares the diversity achieved by different personalization methods. ZOODiP demonstrates the ability to generate highly diverse images while utilizing minimal memory resources.",
                "position": 1865
            },
            {
                "img": "https://arxiv.org/html/2503.14868/x12.png",
                "caption": "Figure S4:Qualitative results for personalizing SD2.1 and SDXL with ZOODiP. The figure demonstrate that ZOODiP can be applied not only to SD1.5, as discussed in the main paper, but also to various other models. For SD2.1, inference were conducted with images at a resolution of768√ó768768768768\\times 768768 √ó 768, while for SDXL, image generation was performed with resolution of1024√ó1024102410241024\\times 10241024 √ó 1024. However, for SDXL, it was observed that the model‚Äôs inherent color interpretation prevents the subject‚Äôs colors from being completely replicated. This indicates that the model‚Äôs color rendering can vary depending on the environmental context, leading to shifts in the perceived color scheme.",
                "position": 1868
            },
            {
                "img": "https://arxiv.org/html/2503.14868/x13.png",
                "caption": "Figure S5:Qualitative comparisons on na√Øve ZO textual inversion without SG and PUTS to ZOODiP (Ours) over iterations. The na√Øve approach exhibits slower training and tends to produce images that are less aligned with the reference image. In contrast, ZOODiP achieves faster training and generates images that are closely aligned with the reference subject.",
                "position": 1937
            }
        ]
    },
    {
        "header": "S4Additional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14868/x14.png",
                "caption": "Figure S6:Qualitative comparison of image and text alignment on the<cat>subset of DB dataset.",
                "position": 1959
            },
            {
                "img": "https://arxiv.org/html/2503.14868/x15.png",
                "caption": "Figure S7:Qualitative comparison of image and text alignment on the<cat2>subset of DB dataset.",
                "position": 1962
            },
            {
                "img": "https://arxiv.org/html/2503.14868/x16.png",
                "caption": "Figure S8:Qualitative comparison of image and text alignment on the<dog6>subset of DB dataset.",
                "position": 1965
            },
            {
                "img": "https://arxiv.org/html/2503.14868/x17.png",
                "caption": "Figure S9:Qualitative comparison of image and text alignment on the<pink_sunglasses>subset of DB dataset.",
                "position": 1968
            },
            {
                "img": "https://arxiv.org/html/2503.14868/x18.png",
                "caption": "Figure S10:Qualitative comparison of image and text alignment on the<shiny_sneaker>subset of DB dataset.",
                "position": 1971
            }
        ]
    },
    {
        "header": "S5Limitation",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]