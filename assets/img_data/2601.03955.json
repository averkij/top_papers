[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.03955/x1.png",
                "caption": "(a)",
                "position": 75
            },
            {
                "img": "https://arxiv.org/html/2601.03955/x1.png",
                "caption": "(a)",
                "position": 78
            },
            {
                "img": "https://arxiv.org/html/2601.03955/x2.png",
                "caption": "(b)",
                "position": 84
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.03955/x3.png",
                "caption": "Figure 2:Overview of ResTok. (a) Pipeline of encoding and decoding processes. There areS‚àí1S-1residual merging blocks uniformly replacing the original transformer blocks in the encoder, whereSSdenotes the number of scales. (b) Residual 1D latent token initialization. When increasing the target size of pooling, we first double the width, and then alternately double the height and width in subsequent steps. (c) Residual merging block. Average pooling is used as the merging method in our experiments.",
                "position": 137
            }
        ]
    },
    {
        "header": "3Residual Tokenizer",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.03955/x4.png",
                "caption": "Figure 3:Representation alignment. The imageùíô\\bm{x}is processed by a VF model to get the[CLS]tokenùíávf[CLS]\\bm{f}^{\\texttt{[CLS]}}_{\\text{vf}}and the visual tokens of image patchesùíávfpatch\\bm{f}^{\\text{patch}}_{\\text{vf}}. The coarsest image tokensùíë1(N)\\bm{p}^{(N)}_{1}and mask VF tokensùíévf(N)\\bm{m}^{(N)}_{\\text{vf}}are aligned withùíávf[CLS]\\bm{f}^{\\texttt{[CLS]}}_{\\text{vf}}andùíávfpatch\\bm{f}^{\\text{patch}}_{\\text{vf}}, respectively.",
                "position": 174
            },
            {
                "img": "https://arxiv.org/html/2601.03955/x5.png",
                "caption": "Figure 4:Hierarchical autoregressive generator. The numbers in the colored tokens stand for the indices of the latent tokens.[Mi]denotes the mask token filled at thei-th missing position.",
                "position": 187
            },
            {
                "img": "https://arxiv.org/html/2601.03955/x6.png",
                "caption": "Figure 5:Visualizations of reconstructions with various token lengths and attention weights in the encoder. The first 16 latent tokens are more closely associated with the coarser image scales S1 and S2, capturing high-level semantics (e.g., object, position, color, etc.). In contrast, the subsequent latent tokens progressively refine fine-grained details, primarily querying the finer image tokens from S3 and S4.",
                "position": 582
            }
        ]
    },
    {
        "header": "4Hierarchical Autoregressive Generation",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.03955/figures/grid.png",
                "caption": "Figure 6:Visualizations of generated 256√ó\\times256 samples on ImageNet-1K. By enhancing the representation capabilities of the tokenizer and constraining the causal dependencies among latent tokens, ResTok enables the AR generator to produce high-quality and diverse images.",
                "position": 609
            },
            {
                "img": "https://arxiv.org/html/2601.03955/x7.png",
                "caption": "Figure 7:Reconstruction and generation performance versus tokenizer training iterations.",
                "position": 868
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.03955/x8.png",
                "caption": "(a)",
                "position": 1734
            },
            {
                "img": "https://arxiv.org/html/2601.03955/x8.png",
                "caption": "(a)",
                "position": 1737
            },
            {
                "img": "https://arxiv.org/html/2601.03955/x9.png",
                "caption": "(b)",
                "position": 1743
            },
            {
                "img": "https://arxiv.org/html/2601.03955/x10.png",
                "caption": "(a)",
                "position": 1750
            },
            {
                "img": "https://arxiv.org/html/2601.03955/x10.png",
                "caption": "(a)",
                "position": 1753
            },
            {
                "img": "https://arxiv.org/html/2601.03955/x11.png",
                "caption": "(b)",
                "position": 1759
            }
        ]
    },
    {
        "header": "Appendix AMore Implementation Details",
        "images": []
    },
    {
        "header": "Appendix BAdditional Results",
        "images": []
    },
    {
        "header": "Appendix CLicenses for Released Assets",
        "images": []
    }
]