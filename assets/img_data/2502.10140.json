[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Results: Small mLMs",
        "images": []
    },
    {
        "header": "6Results: Small mLMs vs. SoTA LLMs",
        "images": []
    },
    {
        "header": "7General Findings and Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.10140/extracted/6204374/assets/data_downstream.png",
                "caption": "Figure 1:Correlation between the pre-training data sizes for mBERT and XLM-R and downstream task results for the pre-adaptation and post-adaptation results. The vertical bars indicate the amounts of adaptation data. The improvements in downstream performance for both models are primarily concentrated in languages with smaller pre-training data sizes, which are positioned on the left side of the plots. Conversely, for languages with substantial representation in the pre-training data, the improvements are less pronounced or nonexistent (Section7.4).",
                "position": 800
            }
        ]
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AConceptNet Tripple Conversion Mapping",
        "images": []
    },
    {
        "header": "Appendix BLanguage Details",
        "images": []
    },
    {
        "header": "Appendix CSentiment Analysis Data Details",
        "images": []
    },
    {
        "header": "Appendix DNamed Entity Recognition Data Details",
        "images": []
    },
    {
        "header": "Appendix ELanguage Adapters Evaluation Losses",
        "images": []
    },
    {
        "header": "Appendix FLanguage Adapter Hyperparameters",
        "images": []
    },
    {
        "header": "Appendix GMasked Language Modeling Pseudo-Perplexity - Part I",
        "images": []
    },
    {
        "header": "Appendix HMasked Language Modeling Pseudo-Perplexity - Part II",
        "images": []
    },
    {
        "header": "Appendix ICorrelation Between Pseudo-Perplexity Pre- and Post-training Data Sizes",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.10140/extracted/6204374/assets/ppl_vs_size.png",
                "caption": "Figure 2:Correlation between the pre-training data sizes for mBERT and XLM-R and the pseudo-perplexities with the values fit in the log-space for the pre-adaptation and post-adaptation results.",
                "position": 4920
            }
        ]
    },
    {
        "header": "Appendix JComparison of XLM-R-base with Glot500 and XLM-R-large",
        "images": []
    },
    {
        "header": "Appendix KCorrelation Between Pseudo-Perplexity and Downstream Tasks",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.10140/extracted/6204374/assets/base_cor_tasks.png",
                "caption": "Figure 3:Correlation between the downstream performance for mBERT and XLM-R pre- and post-adaptation and the pseudo-perplexities.",
                "position": 5226
            }
        ]
    },
    {
        "header": "Appendix LTopic Classification Results - Part I",
        "images": []
    },
    {
        "header": "Appendix MTopic Classification Results - Part II",
        "images": []
    },
    {
        "header": "Appendix NCorrelation Between Topic Classification and Pre- and Post-training Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.10140/extracted/6204374/assets/tc_vs_data.png",
                "caption": "Figure 4:Correlation between the downstream performance for mBERT and XLM-R and the pre-training data and adaptation data.",
                "position": 6096
            }
        ]
    },
    {
        "header": "Appendix ONamed Entity Recognition Results - Part I",
        "images": []
    },
    {
        "header": "Appendix PNamed Entity Recognition Results - Part II",
        "images": []
    },
    {
        "header": "Appendix QCorrelation Between Named Entity Recognition and Pre- and Post-training data",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.10140/extracted/6204374/assets/ner_vs_data.png",
                "caption": "Figure 5:Correlation between the downstream performance for mBERT and XLM-R and the pre-training data and adaptation data.",
                "position": 7015
            }
        ]
    },
    {
        "header": "Appendix RSentiment Analysis Results - Part I",
        "images": []
    },
    {
        "header": "Appendix SSentiment Analysis Results - Part II",
        "images": []
    },
    {
        "header": "Appendix TCorrelation Between Sentiment Analysis and Pre- and Post-training data",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.10140/extracted/6204374/assets/sa_vs_data.png",
                "caption": "Figure 6:Correlation between the downstream performance for mBERT and XLM-R and the pre-training data and adaptation data.",
                "position": 7864
            }
        ]
    },
    {
        "header": "Appendix UResults for Large-Scale Models for TC and NER",
        "images": []
    }
]