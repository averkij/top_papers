[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16375/x1.png",
                "caption": "",
                "position": 102
            },
            {
                "img": "https://arxiv.org/html/2503.16375/x2.png",
                "caption": "Figure 2:A scene generated by our model, trained on multiple scenes. We texture the scene using SceneTex. As shown, our model combines elements from different scenes in the dataset such as castles and skyscrapers. For the untextured mesh, please see the supplement (Figure16).",
                "position": 107
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16375/x3.png",
                "caption": "Figure 3:To accommodate scene chunks of varying heights, vector sets offer a compact and uniform representation, whereas prior work relies on spatially structured latents that require scaling either the latent resolution or the chunk itself for compatibility, resulting in high memory usage or loss of detail.",
                "position": 119
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16375/x4.png",
                "caption": "Figure 4:Overview of our VAE and diffusion models. For the VAE we use vector sets[49]for latent compression. For the diffusion model we train a conditional outpainting model with four different settings to allow for fast generation in a raster scan pattern during inference.",
                "position": 137
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16375/x5.png",
                "caption": "Figure 5:A sample scene processed using our curation pipeline. The occupancy is processed with marching cubes to get the mesh.",
                "position": 178
            },
            {
                "img": "https://arxiv.org/html/2503.16375/x6.png",
                "caption": "Figure 6:We show generation results for the vector set and triplane baseline trained on a single scene.Orangeboxes and zoomed-in renders highlight chunk outpainting coherence, whileblueboxes illustrate scene geometry (zoom in for details). Zoomed-in renders may also have adjusted camera angles for clarity. The triplane model struggles with building details and introduces noisy artifacts, whereas the vector set model has better fidelity. While models can generate continuous chunks with smoothly connected roads and pipes (orange box, right), they sometimes fail to maintain coherence across chunks, leading to broken bridges (orange box, left).",
                "position": 187
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16375/x7.png",
                "caption": "Figure 7:We compare generation results using RePaint[31]for outpainting for resampling stepsr=1ùëü1r=1italic_r = 1andr=5ùëü5r=5italic_r = 5.",
                "position": 507
            },
            {
                "img": "https://arxiv.org/html/2503.16375/x8.png",
                "caption": "Figure 8:We present results from our NuiScene model trained on multiple scenes. In Figure (a), the top-right shows the generated geometry, while the top-left displays the textured model using SceneTex[6]. We render these models and feed them into Trellis[45]for reconstruction (zoom in to see details). It can be seen that our method showcases better geometric detail. Figure (b) presents a large scene generated by our model, showcasing its ability to blend elements from different scenes in the dataset.",
                "position": 510
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16375/x9.png",
                "caption": "Figure 9:Sample and depth mean deviation maps are calculated for sampling chunks from scenes. The sample map is a binary map defining valid x and z coordinates where quad chunks are fully occupied. And the depth mean variation map is used to filter out overly flat regions.",
                "position": 1239
            },
            {
                "img": "https://arxiv.org/html/2503.16375/x10.png",
                "caption": "Figure 10:Three additional scenes used to train our multi-scene model. The top two sub-scenes were split from the same scene in Objaverse for occupancy calculation. The shown scenes have been processed with fixed ground geometries, and their meshes were extracted using the marching cubes algorithm on the occupancy grid.",
                "position": 1242
            }
        ]
    },
    {
        "header": "Appendix S1NuiScene43 Dataset",
        "images": []
    },
    {
        "header": "Appendix S2Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16375/extracted/6295304/figtab/Figures/Supplementary/fig-texture-overview.png",
                "caption": "(a)An illustration of our choice for the camera trajectory, indicated by red arrows.",
                "position": 1445
            },
            {
                "img": "https://arxiv.org/html/2503.16375/extracted/6295304/figtab/Figures/Supplementary/fig-texture-overview.png",
                "caption": "(a)An illustration of our choice for the camera trajectory, indicated by red arrows.",
                "position": 1448
            },
            {
                "img": "https://arxiv.org/html/2503.16375/extracted/6295304/figtab/Figures/Supplementary/fig-texture-100.png",
                "caption": "(b)Image rendered at position (1).",
                "position": 1454
            },
            {
                "img": "https://arxiv.org/html/2503.16375/extracted/6295304/figtab/Figures/Supplementary/fig-texture-200.png",
                "caption": "(c)Image rendered at position (2).",
                "position": 1459
            },
            {
                "img": "https://arxiv.org/html/2503.16375/extracted/6295304/figtab/Figures/Supplementary/fig-texture-250.png",
                "caption": "(d)Image rendered at position (3).",
                "position": 1464
            },
            {
                "img": "https://arxiv.org/html/2503.16375/extracted/6295304/figtab/Figures/Supplementary/fig-texture-450.png",
                "caption": "(e)Image rendered at position (4).",
                "position": 1469
            },
            {
                "img": "https://arxiv.org/html/2503.16375/extracted/6295304/figtab/Figures/Supplementary/fig-scene-1.png",
                "caption": "(a)Generated scene (1).",
                "position": 1476
            },
            {
                "img": "https://arxiv.org/html/2503.16375/extracted/6295304/figtab/Figures/Supplementary/fig-scene-1.png",
                "caption": "(a)Generated scene (1).",
                "position": 1479
            },
            {
                "img": "https://arxiv.org/html/2503.16375/extracted/6295304/figtab/Figures/Supplementary/fig-textured-1.png",
                "caption": "(b)Textured scene (1).",
                "position": 1484
            },
            {
                "img": "https://arxiv.org/html/2503.16375/extracted/6295304/figtab/Figures/Supplementary/fig-scene-2.png",
                "caption": "(c)Generated scene (2).",
                "position": 1489
            },
            {
                "img": "https://arxiv.org/html/2503.16375/extracted/6295304/figtab/Figures/Supplementary/fig-textured-2.png",
                "caption": "(d)Textured scene (2).",
                "position": 1494
            },
            {
                "img": "https://arxiv.org/html/2503.16375/extracted/6295304/figtab/Figures/Supplementary/fig-scene-3.png",
                "caption": "(e)Generated scene (3).",
                "position": 1500
            },
            {
                "img": "https://arxiv.org/html/2503.16375/extracted/6295304/figtab/Figures/Supplementary/fig-textured-3.png",
                "caption": "(f)Textured scene (3).",
                "position": 1505
            },
            {
                "img": "https://arxiv.org/html/2503.16375/extracted/6295304/figtab/Figures/Supplementary/fig-scene-4.png",
                "caption": "(g)Generated scene (4).",
                "position": 1510
            },
            {
                "img": "https://arxiv.org/html/2503.16375/extracted/6295304/figtab/Figures/Supplementary/fig-textured-4.png",
                "caption": "(h)Textured scene (4).",
                "position": 1515
            },
            {
                "img": "https://arxiv.org/html/2503.16375/x11.png",
                "caption": "Figure 13:Additional results for our vector set diffusion model trained on a single scene. The scenes shown here are of size21√ó21212121\\times 2121 √ó 21.",
                "position": 1559
            },
            {
                "img": "https://arxiv.org/html/2503.16375/x12.png",
                "caption": "Figure 14:Additional RePaint results using our vector set diffusion model trained on a single scene. The scenes shown here are of size16√ó16161616\\times 1616 √ó 16. We show results for resampling stepsr=5ùëü5r=5italic_r = 5andr=10ùëü10r=10italic_r = 10. Compared to our outpainting model, RePaint struggles with inter-chunk coherence and sometimes collapses, producing broken chunks in larger scenes.",
                "position": 1562
            },
            {
                "img": "https://arxiv.org/html/2503.16375/x13.png",
                "caption": "Figure 15:Additional results for our vector set diffusion model trained on multiple scenes. The scenes shown here are of size21√ó21212121\\times 2121 √ó 21.",
                "position": 1565
            },
            {
                "img": "https://arxiv.org/html/2503.16375/x14.png",
                "caption": "Figure 16:Additional results for our vector set diffusion model trained on a 4 scenes. The scenes shown here are of size16√ó46164616\\times 4616 √ó 46. The scene in the middle is the untextured mesh ofFigure2.",
                "position": 1568
            }
        ]
    },
    {
        "header": "Appendix S3Additional Results",
        "images": []
    }
]