[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14614/x1.png",
                "caption": "",
                "position": 81
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14614/x2.png",
                "caption": "Figure 2:Method overview.Given a single image or text prompt to describe a world,WorldPlayperforms a next chunk (16 video frames) prediction task to generate future videos conditioned on action from users. For the generation of each chunk, we dynamically reconstitute context memory from past chunks to enforce long-term temporal and geometric consistency.",
                "position": 432
            },
            {
                "img": "https://arxiv.org/html/2512.14614/x3.png",
                "caption": "Figure 3:Detailed architecture of our autoregressive diffusion transformer. The discrete key is incorporated with time embedding, while the continuous camera pose is injected into causal self-attention through PRoPE[li2025cameras].",
                "position": 454
            },
            {
                "img": "https://arxiv.org/html/2512.14614/x4.png",
                "caption": "(a)Full context",
                "position": 492
            },
            {
                "img": "https://arxiv.org/html/2512.14614/x4.png",
                "caption": "(a)Full context",
                "position": 495
            },
            {
                "img": "https://arxiv.org/html/2512.14614/x5.png",
                "caption": "(b)Absolute indices",
                "position": 501
            },
            {
                "img": "https://arxiv.org/html/2512.14614/x6.png",
                "caption": "(c)Relative indices",
                "position": 507
            },
            {
                "img": "https://arxiv.org/html/2512.14614/x7.png",
                "caption": "Figure 5:Context forcingis a novel distillation method that employs memory-augmented self-rollout and memory-augmented bidirectional video diffusion to preserve long-term consistency, enable real-time interaction, and mitigate error accumulation.",
                "position": 538
            },
            {
                "img": "https://arxiv.org/html/2512.14614/x8.png",
                "caption": "Figure 6:Qualitative comparisons with existing methods.WorldPlay achieves the state-of-the-art long-term consistency (shown in red boxes) and visual quality across diverse scenes, including both first- and third-person real and stylized worlds.",
                "position": 545
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14614/x9.png",
                "caption": "Figure 7:RoPE design comparisons. Upper:Our reframed RoPE avoids exceeding the the positional range in standard RoPE, alleviating error accumulation.Bottom:By maintaining a small relative distance to long-range spatial memory, it achieves better long-term consistency.",
                "position": 858
            },
            {
                "img": "https://arxiv.org/html/2512.14614/x10.png",
                "caption": "Figure 8:Ablation for context forcing.a)When the teacher and student have misaligned context, it leads to distillation failure, resulting in collapsed outputs.b)Self-rollout historical context can introduce artifacts. Zoom in for details.",
                "position": 900
            },
            {
                "img": "https://arxiv.org/html/2512.14614/x11.png",
                "caption": "Figure 9:Promptable event.Our method supports text-based manipulation during streaming.",
                "position": 915
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix ATraining and Inference Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14614/x12.png",
                "caption": "Figure 10:Camera trajectories included in our collected dataset.",
                "position": 1262
            }
        ]
    },
    {
        "header": "Appendix BDataset",
        "images": []
    },
    {
        "header": "Appendix CAdditional Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14614/figures/appendix/supp_action.png",
                "caption": "Figure 11:More qualitative results.",
                "position": 1293
            },
            {
                "img": "https://arxiv.org/html/2512.14614/figures/appendix/supp_long_video.png",
                "caption": "Figure 12:Long video generation.",
                "position": 1297
            },
            {
                "img": "https://arxiv.org/html/2512.14614/x13.png",
                "caption": "Figure 13:Visualization of different models under context forcing.",
                "position": 1319
            },
            {
                "img": "https://arxiv.org/html/2512.14614/x14.png",
                "caption": "Figure 14:VBench evaluation.",
                "position": 1427
            },
            {
                "img": "https://arxiv.org/html/2512.14614/x15.png",
                "caption": "Figure 15:Human evaluation.",
                "position": 1431
            },
            {
                "img": "https://arxiv.org/html/2512.14614/x16.png",
                "caption": "Figure 16:Visualization of promptable event and video continuation.",
                "position": 1435
            },
            {
                "img": "https://arxiv.org/html/2512.14614/x17.png",
                "caption": "Figure 17:3D reconstruction results.",
                "position": 1439
            }
        ]
    },
    {
        "header": "Appendix DUser Study",
        "images": []
    },
    {
        "header": "Appendix EAdditional Applications",
        "images": []
    },
    {
        "header": "Appendix FLimitations",
        "images": []
    }
]