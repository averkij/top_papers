[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08030/figures/huggingface_logo.png",
                "caption": "",
                "position": 144
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08030/figures/Figure1_new.png",
                "caption": "Figure 1:Empirical observation of Qwen3-8B reasoning on AIME benchmarks.Left:Standard LLMs passively accumulate tokens, causing the reasoning process to eventually “crash” (degenerate).Right:Free()LMintegrates an intrinsicfree()mechanism. By periodically identifying and pruning redundant reasoning steps, it actively maintains a compact, noise-free state, enabling sustainable long-chain reasoning.",
                "position": 157
            }
        ]
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08030/x1.png",
                "caption": "Figure 2:TheFree()LMInference Framework.The model operates on a cyclic Reasoning-Cleaning mechanism.Reasoning:The Main model generates tokens normally with the Free-Module unmerged.Cleaning:Upon reaching a chunk limit, the Free-Module is merged to identify and prune redundant chunks.Resumed Reasoning:The module is unmerged, and reasoning resumes on the cleaned context.",
                "position": 205
            },
            {
                "img": "https://arxiv.org/html/2602.08030/x2.png",
                "caption": "Figure 3:The Data Construction Pipeline.(a) Data Synthesis:We segment raw trajectories into 1k-token chunks and employ Gemini-2.5-Pro tosequentiallygenerate candidate training instances.(b) Reward Mechanism:By executingK=8K=8parallel rollouts, we retain an instance only if the pruned contextCnewC_{\\text{new}}maintains or improves accuracy compared to the original (Acc​(Cnew)≥Acc​(Craw)\\text{Acc}(C_{\\text{new}})\\geq\\text{Acc}(C_{\\text{raw}})).",
                "position": 280
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08030/x3.png",
                "caption": "Figure 4:Performance vs. Reasoning Length on HLE.While standard Qwen3-235B-A22B suffers from Degradation on trajectories longer than 80k tokens,Free()LMexhibits a striking rebound in accuracy. On these cases, the Free-Module reduces the context length by∼\\sim45%, effectively mitigating context pollution.",
                "position": 787
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperimental Details",
        "images": []
    }
]