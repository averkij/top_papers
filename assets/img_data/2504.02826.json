[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02826/x1.png",
                "caption": "Figure 1:Comparison of leading models on our Reasoning-Informed viSual Editing tasks.",
                "position": 85
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2RISEBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02826/x2.png",
                "caption": "Figure 2:Overview of RISEBench.We present illustrative example questions from each of the four problem categories, each demanding profound image understanding and reasoning capabilities.",
                "position": 187
            },
            {
                "img": "https://arxiv.org/html/2504.02826/x3.png",
                "caption": "Figure 3:Evaluation metrics of RISEBench.RISEBench assesses the quality of generated images along three key dimensions:Instruction Following,Appearance Consistency, andVisual Plausibility. For each dimension, carefully crafted prompts are provided to the evaluator model (GPT-4o in this study), which analyzes various inputs and returns scores for each corresponding sub-dimension.",
                "position": 274
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02826/x4.png",
                "caption": "Figure 4:Comparison across models on three evaluation sub-dimensions.The native generation models, GPT-4o-Native and Gemini-2-Flash, demonstrate strong performance across all three evaluation dimensions. GPT-4o∗performs well in instruction reasoning but struggles with appearance consistency. The remaining models fail to follow instructions, highlighting the gap in reasoning-informed visual editing.",
                "position": 513
            },
            {
                "img": "https://arxiv.org/html/2504.02826/x5.png",
                "caption": "Figure 5:Examples of models’ outputs on RISEBench.The complete outputs of the five evaluated models are provided inAppx.Bfor comprehensive comparison.",
                "position": 540
            },
            {
                "img": "https://arxiv.org/html/2504.02826/x6.png",
                "caption": "Figure 6:Correlation between human and model-based judgments.(a) illustrates the score distributions from human annotators and the model-based evaluator, while (b) presents the MAE between the two, with scores ranging from 0 to 4. The close alignment in score distributions and the low MAE values together indicate a strong correlation between human and model judgments.",
                "position": 564
            }
        ]
    },
    {
        "header": "4Conclusion",
        "images": []
    },
    {
        "header": "5Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExample of Generation from GPT-4o∗",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02826/x7.png",
                "caption": "Figure 7:Example of cascade generation pipeline in GPT-4o∗.",
                "position": 895
            }
        ]
    },
    {
        "header": "Appendix BDetailed Outputs of All Evaluated Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02826/x8.png",
                "caption": "Figure 8:Logical Reasoning Outputs – Part 1.",
                "position": 905
            },
            {
                "img": "https://arxiv.org/html/2504.02826/x9.png",
                "caption": "Figure 9:Logical Reasoning Outputs – Part 2.",
                "position": 908
            },
            {
                "img": "https://arxiv.org/html/2504.02826/x10.png",
                "caption": "Figure 10:Spatial Reasoning Outputs – Part 1.",
                "position": 911
            },
            {
                "img": "https://arxiv.org/html/2504.02826/x11.png",
                "caption": "Figure 11:Spatial Reasoning Outputs – Part 2.",
                "position": 914
            },
            {
                "img": "https://arxiv.org/html/2504.02826/x12.png",
                "caption": "Figure 12:Temporal Reasoning Outputs – Part 1.",
                "position": 917
            },
            {
                "img": "https://arxiv.org/html/2504.02826/x13.png",
                "caption": "Figure 13:Temporal Reasoning Outputs – Part 2.",
                "position": 920
            },
            {
                "img": "https://arxiv.org/html/2504.02826/x14.png",
                "caption": "Figure 14:Causal Reasoning Outputs – Part 1.",
                "position": 923
            },
            {
                "img": "https://arxiv.org/html/2504.02826/x9.png",
                "caption": "Figure 15:Causal Reasoning Outputs – Part 2.",
                "position": 926
            }
        ]
    },
    {
        "header": "Appendix CPrompt for Judgement",
        "images": []
    }
]