[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.16141/x1.png",
                "caption": "Figure 1:(a)SEED-Bench-R1(SB-R1) provides a systematic, three-level evaluation of post-training methods for MLLMs in video understanding, encompassing tasks that require both perception and reasoning to tackle complex real-world scenarios. (b) Our analysis identifies a key limitation of standard outcome-supervised GRPO: while it improves answer accuracy, it often compromises logical consistency between reasoning and answers. By introducing an adaptive, group-relative consistency bonus via reference-likelihood calibration, ourGRPO-CAREachieves higher answer accuracy across all difficulty levels and improves interpretability, as reflected by increased consistency rates.",
                "position": 65
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Pilot Study with SEED-Bench-R1",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.16141/x2.png",
                "caption": "Figure 2:Case study of an L3 question from SEED-Bench-R1, showing a video of task progress, a final observation image, and attention maps (output-to-visual tokens).\nTheSFTmodel tends to memorize reasoning patterns and exhibits perceptual hallucinations.\nTheGRPOmodel attends more comprehensively to the highlighted key visual observation while lacking logical consistency in the generated content.\nTheGRPO-CAREmodel further balances visual perception and logical reasoning.",
                "position": 343
            }
        ]
    },
    {
        "header": "4Consistency-Aware Reward-Enhanced GRPO for MLLMs (GRPO-CARE)",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.16141/x3.png",
                "caption": "Figure 3:GRPO-CARE uses a two-tier reward system: a base reward for answer correctness (r‚àóbsubscriptsuperscriptùëüùëèr^{b}_{*}italic_r start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ‚àó end_POSTSUBSCRIPT) and an adaptive consistency bonus (r‚àócsubscriptsuperscriptùëüùëêr^{c}_{*}italic_r start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ‚àó end_POSTSUBSCRIPT). The consistency bonus is given to high-accuracy samples whose reasoning-to-answer likelihood‚Äîestimated by a slowly updated (EMA) reference model‚Äîis higher than that of their group peers, conditioned on the multimodal question. The total reward, the sum of base and consistency rewards, is then used to compute advantages for updating the online model.",
                "position": 376
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]