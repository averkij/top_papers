[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.13029/figs/teaser.jpg",
                "caption": "Figure 1:Comparison between prior “think with image”[74]and our proposed “think with space“. While the former reasons over 2D content by manipulating images, our method operates directly within 3D point cloud space for spatial understanding.",
                "position": 127
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Think3D for Spatial Reasoning",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.13029/figs/pipeline1.png",
                "caption": "Figure 2:TheThink3Dpipeline. The VLM interacts with the 3D scene through iterative calls to the 3D Manipulation Toolkit, issuing viewpoint-manipulation actions that control camera pose and rendering parameters.\nEach rendered image is appended to the agent’s memory and informs the next reasoning step, forming a repeated cycle of observe → manipulate → reflect.",
                "position": 226
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.13029/figs/example.png",
                "caption": "Figure 3:Spatial exploration behavior of Think3D. The agent autonomously selects viewpoints and switches between global and ego-centric views; after RL training, it explores angles more systematically than the untuned baseline.",
                "position": 895
            }
        ]
    },
    {
        "header": "5Ablation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.13029/x1.png",
                "caption": "Figure 4:Task level spatial exploration patterns. This figure shows the distribution of viewpoint selections made by GPT-4.1 across different tasks. The exploration patterns vary substantially: for instance, tasks such as route planning exhibit a strong preference for top-down views (0,60), while others rely on more diverse or oblique perspectives.",
                "position": 977
            },
            {
                "img": "https://arxiv.org/html/2601.13029/x2.png",
                "caption": "Figure 5:Reinforcement Learning Dynamics. As RL fine-tuning progresses, the model learns when extra 3D tool calls are worthwhile, shifting from shorter but less accurate trajectories to more informative explorations with higher reward.",
                "position": 989
            },
            {
                "img": "https://arxiv.org/html/2601.13029/x3.png",
                "caption": "Figure 6:model-level spatial exploration patterns. Strong models concentrate on informative angles such as oblique and top-down views; after RL fine-tuning, Qwen3-VL-4B shifts its angle distribution toward a similar pattern.",
                "position": 1005
            },
            {
                "img": "https://arxiv.org/html/2601.13029/x4.png",
                "caption": "Figure 7:Performance of different exploration rounds. After RL, the smaller model benefits more from additional steps and follows the same upward trend as the stronger models.",
                "position": 1016
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APrompts and Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.13029/figs/prompt_pi31.png",
                "caption": "Figure 8:The Pi3 Tool Prompt. The prompt specifies the tool’s capabilities, key control parameters, and multi-angle query usage strategies to support comprehensive spatial understanding.",
                "position": 1994
            },
            {
                "img": "https://arxiv.org/html/2601.13029/figs/prompt_continue3.png",
                "caption": "Figure 9:Multi-step prompt for iterative 3D viewpoint exploration. Including angle selection, camera rotation controls, tool invocation rules to refine spatial reasoning.",
                "position": 1997
            },
            {
                "img": "https://arxiv.org/html/2601.13029/figs/prompt_sys1.png",
                "caption": "Figure 10:The system prompt. Instruction prompt detailing tool invocation rules and the multi-step workflow for iterative 3D viewpoint exploration, including tool-call format, recommended angles, and guidelines for reasoning with reconstructed camera poses.",
                "position": 2000
            },
            {
                "img": "https://arxiv.org/html/2601.13029/figs/promptrl_sys1.png",
                "caption": "Figure 11:The RL system prompt. Instruction prompt defining the constrained 3-view 3D analysis workflow, including tool-call format, angle selection rules (left, right, top), and iterative reasoning steps for viewpoint-guided spatial understanding.",
                "position": 2009
            },
            {
                "img": "https://arxiv.org/html/2601.13029/figs/promptrl_continue12.png",
                "caption": "Figure 12:The RL continuation prompt used during non-final turns. Iterative-step instruction prompt outlining allowed viewpoint choices (left/right/top), tool-call rules, and the decision process for progressing or concluding 3D spatial analysis.",
                "position": 2012
            },
            {
                "img": "https://arxiv.org/html/2601.13029/figs/promptrl_continue22.png",
                "caption": "Figure 13:The RL continuation prompt used in the final turn. Final-turn instruction prompt specifying the no-tool phase, requiring explicit reasoning and a final answer based solely on previously generated 3D views and the original image.",
                "position": 2015
            },
            {
                "img": "https://arxiv.org/html/2601.13029/figs/promptnotool_.png",
                "caption": "Figure 14:The prompt without tools. Base instruction prompt for direct image-question analysis, requiring explicit reasoning and final answer formatting without tool interactions.",
                "position": 2018
            }
        ]
    },
    {
        "header": "Appendix BFurther Experiment Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.13029/x5.png",
                "caption": "Figure 15:Ego view usage ratio across different tasks. Distribution of GPT-4.1’s reliance on ego-view versus global-view across tasks. Fine-grained tasks emphasize ego-centric information, whereas tasks requiring broad context predominantly utilize global-view.",
                "position": 2037
            },
            {
                "img": "https://arxiv.org/html/2601.13029/x6.png",
                "caption": "Figure 16:Tool calling iteration ratio across different tasks. GPT-4.1 rarely uses tools for route planning, while conducting multiple rounds of tool calls for other tasks to acquire richer spatial information.",
                "position": 2046
            }
        ]
    },
    {
        "header": "Appendix CThink3D-RL Training Parameters",
        "images": []
    },
    {
        "header": "Appendix DReproducibility Across Different Temperatures",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.13029/x7.png",
                "caption": "Figure 17:The Mindcube example.",
                "position": 2275
            },
            {
                "img": "https://arxiv.org/html/2601.13029/x8.png",
                "caption": "Figure 18:The Mindcube example.",
                "position": 2278
            },
            {
                "img": "https://arxiv.org/html/2601.13029/x9.png",
                "caption": "Figure 19:The BLINK example.",
                "position": 2281
            },
            {
                "img": "https://arxiv.org/html/2601.13029/x10.png",
                "caption": "Figure 20:The BLINK example.",
                "position": 2284
            },
            {
                "img": "https://arxiv.org/html/2601.13029/x11.png",
                "caption": "Figure 21:The VSI-Bench example.",
                "position": 2287
            },
            {
                "img": "https://arxiv.org/html/2601.13029/figs/vsi_eg2.png",
                "caption": "Figure 22:The VSI-Bench example.",
                "position": 2290
            }
        ]
    },
    {
        "header": "Appendix EInteraction Visualization",
        "images": []
    }
]