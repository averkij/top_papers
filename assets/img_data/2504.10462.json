[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.10462/x1.png",
                "caption": "Figure 1:(A) Data scaling curvefor Modular Multimodal Large Language Model (MLLM) andSail, our Single Transformer-based MLLM. As pretraining data increases, the single transformerSailshows a sharper performance gain, demonstrating its superior data scalability.(B) Comparison to existing Single Transformer-based MLLMs: ourSailpushes the performance boundaries on both vision tasks and vision-language tasks.",
                "position": 412
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Sail: Training a Single Transformer for Vision and Language",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.10462/x2.png",
                "caption": "Figure 2:Model architecture and micro-designs forSail.(A) Model Architecture:Sailis a unified transformer that processes both images and texts without extra module designs.(B) Mixed Attention Mechanism:we adopt bidirectional attention for image patches from the same image and causal attention for text tokens. Examples for a multimodal sequence and a text sequence are provided. Colored squares represent “allow to attend” and white squares indicate “prevent from attending”.(C) Multimodal RoPE:an illustration of the multimodal rotary position embedding forSail, with examples for a multimodal sequence and a text sequence.",
                "position": 504
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.10462/x3.png",
                "caption": "Figure 3:Model scaling ofSail.Left: As the model size increases, the training language modeling loss gradually decreases. Right: As the model size increases, performance on downstream VLM tasks progressively improves.",
                "position": 1270
            },
            {
                "img": "https://arxiv.org/html/2504.10462/x4.png",
                "caption": "Figure 4:Image Attention Score Allocation: The figure shows the proportion of image attention scores across different transformer layers for Single Transformer-based MLLM and modular MLLM when predicting tokens. Single Transformer-based MLLM generally allocates higher attention weights to image tokens compared to modular MLLM.",
                "position": 1284
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.10462/x5.png",
                "caption": "Figure 5:Comparison ofSailand LLaVA1.5 on MMVP examples.Saildemonstrates better performance in perceiving minor regions and objects, as well as more accurately distinguishing object states.",
                "position": 1794
            },
            {
                "img": "https://arxiv.org/html/2504.10462/x6.png",
                "caption": "Figure 6:Image attention score allocation forSailand its modular MLLM counterpart.We compared the attention score allocation distribution for shallow layers, medium layers, and deep layers between these two models.\nThe Single Transformer-based MLLM model significantly allocates a higher proportion of attention score to image tokens during prediction than the modular MLLM.",
                "position": 1797
            },
            {
                "img": "https://arxiv.org/html/2504.10462/x7.png",
                "caption": "Figure 7:Visualization ofSail’s attention distribution across image regions during token prediction.In early transformer layers, attention primarily focuses on the salient regions of the image. As the model progresses to deeper layers, attention shifts to areas more relevant to the predicted tokens.",
                "position": 1827
            },
            {
                "img": "https://arxiv.org/html/2504.10462/extracted/6360030/figure/arxiv.jpg",
                "caption": "Table 11:Saildemonstrates a strong capability to extract information in OCR-rich scenarios.",
                "position": 1838
            },
            {
                "img": "https://arxiv.org/html/2504.10462/extracted/6360030/figure/realworld.jpg",
                "caption": "Table 12:Saildemonstrates a strong capability about understanding the real-world scene.",
                "position": 1886
            },
            {
                "img": "https://arxiv.org/html/2504.10462/extracted/6360030/figure/ai2d_img.jpg",
                "caption": "Table 13:Saildemonstrates a strong capability about understanding the scientific charts.",
                "position": 1934
            },
            {
                "img": "https://arxiv.org/html/2504.10462/extracted/6360030/figure/poster1.jpg",
                "caption": "Table 14:Saildemonstrates a strong capability about understanding the poster content.",
                "position": 1973
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]