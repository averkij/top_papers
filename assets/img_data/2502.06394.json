[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06394/x1.png",
                "caption": "Рис. 1:An illustration of the proposed approach for collecting and generating the multilingual text detoxification datasetSynthDetoxM.",
                "position": 226
            }
        ]
    },
    {
        "header": "2Background and Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06394/x2.png",
                "caption": "Рис. 2:Number of accepted samples in the finalSynthDetoxMdataset with respect to the LLM by language.",
                "position": 316
            },
            {
                "img": "https://arxiv.org/html/2502.06394/x3.png",
                "caption": "Рис. 3:Distribution of STA toxicity scores of toxic and neutral examples in the dataset. The original toxic texts are in orange, while detoxified texts are in blue. For readability we apply Gaussian smoothing.",
                "position": 409
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06394/",
                "caption": "Рис. 4:Side-by-side comparison of model outputs across all languages, evaluated by GPT-4o. The results highlight the relative performance of the models in generating detoxified text for German, Russian, and Spanish. The notation is similar to the notation from Table3.",
                "position": 802
            }
        ]
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "8Ethical Considerations",
        "images": []
    },
    {
        "header": "Список литературы",
        "images": []
    },
    {
        "header": "Приложение APrompts",
        "images": []
    },
    {
        "header": "Приложение BAutomatic evaluation results",
        "images": []
    },
    {
        "header": "Приложение CLimitations of ChrF1 as a Fluency Metric",
        "images": []
    },
    {
        "header": "Приложение DRefusal classifier training",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06394/x5.png",
                "caption": "Рис. 8:Side-by-side comparison of model outputs across all languages, evaluated by GPT-4o. The results highlight the relative performance of the models in generating detoxified text for German. The notation is similar to the notation from Table3.",
                "position": 2065
            },
            {
                "img": "https://arxiv.org/html/2502.06394/x6.png",
                "caption": "Рис. 9:Side-by-side comparison of model outputs across all languages, evaluated by GPT-4o. The results highlight the relative performance of the models in generating detoxified text for Spanish. The notation is similar to the notation from Table3.",
                "position": 2068
            },
            {
                "img": "https://arxiv.org/html/2502.06394/x7.png",
                "caption": "Рис. 10:Side-by-side comparison of model outputs across all languages, evaluated by GPT-4o. The results highlight the relative performance of the models in generating detoxified text for Russian. The notation is similar to the notation from Table3.",
                "position": 2071
            }
        ]
    },
    {
        "header": "Приложение EAdditional linguistic analysis of the dataset",
        "images": []
    },
    {
        "header": "Приложение FSide-by-side comparison of trained mT0 models",
        "images": []
    },
    {
        "header": "Приложение GPer-Language Few-Shot Examples",
        "images": []
    }
]