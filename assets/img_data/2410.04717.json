[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.04717/x1.png",
                "caption": "(a)Semantic clustering of relevant datasets.",
                "position": 484
            },
            {
                "img": "https://arxiv.org/html/2410.04717/x1.png",
                "caption": "(a)Semantic clustering of relevant datasets.",
                "position": 487
            },
            {
                "img": "https://arxiv.org/html/2410.04717/x2.png",
                "caption": "(b)OSS-Alpaca mixture and test instructions.",
                "position": 492
            },
            {
                "img": "https://arxiv.org/html/2410.04717/x3.png",
                "caption": "(c)OSS-COT-Alpaca mixture and test instructions.",
                "position": 497
            }
        ]
    },
    {
        "header": "2Experiments with String Replacements",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.04717/x4.png",
                "caption": "(a)Re-writing accuracy against the number of instructions with a fixed-size training set.",
                "position": 556
            },
            {
                "img": "https://arxiv.org/html/2410.04717/x4.png",
                "caption": "(a)Re-writing accuracy against the number of instructions with a fixed-size training set.",
                "position": 559
            },
            {
                "img": "https://arxiv.org/html/2410.04717/x5.png",
                "caption": "(b)Rewriting with no-op situation included.",
                "position": 564
            },
            {
                "img": "https://arxiv.org/html/2410.04717/x6.png",
                "caption": "Figure 3:Effect of long-tail task distributions on model’s generalization ability.",
                "position": 583
            },
            {
                "img": "https://arxiv.org/html/2410.04717/x7.png",
                "caption": "Figure 4:Model’s performance onk<3𝑘3k<3italic_k < 3when trained on the three classes of restricted semantics as in2.3.4. Models trained on 500 or less instructions never generalize to smaller k.",
                "position": 633
            }
        ]
    },
    {
        "header": "3Rewriting with Abstraction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.04717/x8.png",
                "caption": "(a)Accuracy On Unseen Deduction Rules vs. Abstract Rule Diversity.D3:pattern with tree depth 3.ZS:Zero shot.FS: Few shot.",
                "position": 710
            },
            {
                "img": "https://arxiv.org/html/2410.04717/x8.png",
                "caption": "(a)Accuracy On Unseen Deduction Rules vs. Abstract Rule Diversity.D3:pattern with tree depth 3.ZS:Zero shot.FS: Few shot.",
                "position": 713
            },
            {
                "img": "https://arxiv.org/html/2410.04717/x9.png",
                "caption": "(b)Accuracy on unseen depths against number of diversification rules with the same in-domain / out-of-domain mixture.",
                "position": 718
            },
            {
                "img": "https://arxiv.org/html/2410.04717/x10.png",
                "caption": "(c)Accuracy on unseen depths vs. In-domain/out-of-domain combination.Diver-means number of diversification rules. X axis ticks are |ℛd⁢i⁢v⁢e⁢rsubscriptℛ𝑑𝑖𝑣𝑒𝑟\\mathcal{R}_{diver}caligraphic_R start_POSTSUBSCRIPT italic_d italic_i italic_v italic_e italic_r end_POSTSUBSCRIPT|: |ℛs⁢p⁢e⁢csubscriptℛ𝑠𝑝𝑒𝑐\\mathcal{R}_{spec}caligraphic_R start_POSTSUBSCRIPT italic_s italic_p italic_e italic_c end_POSTSUBSCRIPT|.",
                "position": 723
            }
        ]
    },
    {
        "header": "4Fine-Tuning A Sepcialist Instruction-Follower: Case of Code Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.04717/x11.png",
                "caption": "Figure 6:Sweet spots of Pass@1 with data mixture. Baseline is marked with dotted lines.",
                "position": 1073
            }
        ]
    },
    {
        "header": "5Fine-tuning Generalist LLMs",
        "images": []
    },
    {
        "header": "6Related works",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AComplement on Markov algorithms",
        "images": []
    },
    {
        "header": "Appendix BExperimental set-up",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.04717/x12.png",
                "caption": "Figure 7:The sorted percentage of each instruction following power-law distribution with different shape parameters. The y-axis is the percentage of the rules in the training mixture. The x-axis is the ranked index (by proportion of examples) of instructions.",
                "position": 2492
            }
        ]
    },
    {
        "header": "Appendix CMore Details On Evaluation",
        "images": []
    }
]