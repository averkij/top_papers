[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24317/x1.png",
                "caption": "Figure 1:(Left)\\ourmethodStage 2:Frozen-teacher, learnable student and predictor. The frozen teacher encoder is obtained viaStage 1(not pictured above) by training using a pixel reconstruction objective. The student and predictor are jointly optimized to learn representation from video in Stage 2 using a latent space prediction objective.(Right):SALT’s compute-accuracy curve dominates V-JEPA 2.",
                "position": 187
            }
        ]
    },
    {
        "header": "2Method Overview",
        "images": []
    },
    {
        "header": "3Experimental Setup",
        "images": []
    },
    {
        "header": "4Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24317/x2.png",
                "caption": "(a)Per-benchmark accuracy of ViT-L.",
                "position": 456
            },
            {
                "img": "https://arxiv.org/html/2509.24317/x2.png",
                "caption": "(a)Per-benchmark accuracy of ViT-L.",
                "position": 459
            },
            {
                "img": "https://arxiv.org/html/2509.24317/x3.png",
                "caption": "(b)Scaling model-size.",
                "position": 464
            },
            {
                "img": "https://arxiv.org/html/2509.24317/figs/ablation/student_loss_vs_acc.png",
                "caption": "(a)\\ourmethod-teacher 80k.R2=0.951R^{2}=0.951",
                "position": 488
            },
            {
                "img": "https://arxiv.org/html/2509.24317/figs/ablation/student_loss_vs_acc.png",
                "caption": "(a)\\ourmethod-teacher 80k.R2=0.951R^{2}=0.951",
                "position": 491
            },
            {
                "img": "https://arxiv.org/html/2509.24317/x4.png",
                "caption": "(b)\\ourmethod-teacher 40k.R2=0.972R^{2}=0.972",
                "position": 497
            },
            {
                "img": "https://arxiv.org/html/2509.24317/x5.png",
                "caption": "(c)\\ourmethod-teacher 20k.R2=0.984R^{2}=0.984",
                "position": 503
            },
            {
                "img": "https://arxiv.org/html/2509.24317/x6.png",
                "caption": "Figure 4:Training data of static-teacher.We ablate the impact of training data of teacher, thus fixed the student’s training data as the whole data-mix by default.Table˜13provides a detailed breakdown of the results show above.",
                "position": 524
            }
        ]
    },
    {
        "header": "5Teacher Design Choice Ablation",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24317/x7.png",
                "caption": "Figure 5:Masking Strategy of static-teacher.We study the impact of random vs multi-block masking strategy influences a student’s performance.Table˜15includes hyperparameters and results information.",
                "position": 544
            },
            {
                "img": "https://arxiv.org/html/2509.24317/x8.png",
                "caption": "Figure 6:Teacher model size ablation.We train ViT-B, ViT-L, ViT-H and ViT-G based teacher and use the teacher to train a ViT-L and ViT-G student. Observe that the best performing student is obtained from ViT-L teacher with modest performance.Table˜14provides a detailed breakdown of results on downstream benchmarks.",
                "position": 561
            },
            {
                "img": "https://arxiv.org/html/2509.24317/x9.png",
                "caption": "Figure 7:Comparison of compute allocation in SALT.We show average Top-1 accuracy across benchmarks against total training FLOPs. Our SALT curves dominate V-JEPA-2 at matched budgets. SeeTable˜16for additional details.",
                "position": 573
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Limitations and Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining Dataset",
        "images": []
    },
    {
        "header": "Appendix BArchitecture Details",
        "images": []
    },
    {
        "header": "Appendix CTraining Details",
        "images": []
    },
    {
        "header": "Appendix DEvaluation Details",
        "images": []
    },
    {
        "header": "Appendix EAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24317/x10.png",
                "caption": "Figure 8:\\ourmethodtrained with a compute budget of (a) 120K steps (b) 160K steps and (c) 240K steps. The X-axis shows the number of steps allocated to the teacher with the rest used to optimize the student. Observe that the optimal allocation favors training the student longer than the teacher.",
                "position": 2108
            },
            {
                "img": "https://arxiv.org/html/2509.24317/x11.png",
                "caption": "Figure 9:Teacher quality vs. student performance. We take all the teachers trained in\\ourmethodand measure the RankME(Garrido et al.,2023)of the embedding and pretraining loss and analyze the corelation between them and student’s downstream performance. Each point represents a single\\ourmethodrun. We control the total training budgets of 160k for both stages to the be same for all models in this comparison.",
                "position": 2118
            }
        ]
    },
    {
        "header": "Appendix FFloating Point Operations (FLOPs) Estimation",
        "images": []
    },
    {
        "header": "Appendix GCompute Budget specified via FLOPs and optimization steps",
        "images": []
    },
    {
        "header": "Appendix HAdditional Tables",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24317/x12.png",
                "caption": "Figure 10:Correlation between RankME and downstream accuracy. We use the same SALT-Stage-1-80k teacher checkpoint.",
                "position": 3117
            }
        ]
    },
    {
        "header": "Appendix IAdditional Figures",
        "images": []
    }
]