[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09283/x1.png",
                "caption": "Figure 1:Top:Comparison of the reconstruction-via-recaption results betweenğ™¸ğš—ğšœğšğšŠğš—ğšŒğšğ™²ğšŠğš™ğ™¸ğš—ğšœğšğšŠğš—ğšŒğšğ™²ğšŠğš™\\mathtt{InstanceCap}typewriter_InstanceCapand state-of-the-art captioning methods for annotating the ground truth video.ğ™¸ğš—ğšœğšğšŠğš—ğšŒğšğ™²ğšŠğš™ğ™¸ğš—ğšœğšğšŠğš—ğšŒğšğ™²ğšŠğš™\\mathtt{InstanceCap}typewriter_InstanceCapproduces results that more closely resemble the original video, showing greater detail fidelity (highlighted by the red circle).Bottom:The corresponding captions generated byğ™¸ğš—ğšœğšğšŠğš—ğšŒğšğ™²ğšŠğš™ğ™¸ğš—ğšœğšğšŠğš—ğšŒğšğ™²ğšŠğš™\\mathtt{InstanceCap}typewriter_InstanceCapand others.Reddenotes incorrect captions,bluerepresents ambiguous captions, andgreenindicates detailed and accurate descriptions of video. Specific visual hints are marked as A, B, and C for clarity.\nAll videos are generated using thesamevideo generation product,Hailuo AI222https://hailuoai.com/video, which has robust prompt-following capabilities, clearly highlighting the effectiveness ofğ™¸ğš—ğšœğšğšŠğš—ğšŒğšğ™²ğšŠğš™ğ™¸ğš—ğšœğšğšŠğš—ğšŒğšğ™²ğšŠğš™\\mathtt{InstanceCap}typewriter_InstanceCap.",
                "position": 161
            }
        ]
    },
    {
        "header": "2Related works",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09283/x2.png",
                "caption": "Figure 2:Overview of InstanceCap pipeline. Details of â€œfrom dense prompts to structured phrasesâ€ design are shown in Figure3.",
                "position": 223
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09283/x3.png",
                "caption": "Figure 3:Details on â€œfrom dense prompts to structured phrasesâ€ design. We propose an improved CoT pipeline with carefully designed information interactions (red arrow), which facilitates MLLMs to accurately capture instances with precise descriptions on attributes.",
                "position": 269
            },
            {
                "img": "https://arxiv.org/html/2412.09283/x4.png",
                "caption": "Figure 4:ğ™¸ğš—ğšœğšğšŠğš—ğšŒğšğš…ğš’ğšğ™¸ğš—ğšœğšğšŠğš—ğšŒğšğš…ğš’ğš\\mathtt{InstanceVid}typewriter_InstanceVidprovides structured captions for videos in open-domain scenarios, featuring diverse instance, expansive scenes, precise and instance-aware captions, and video-generation-friendly durations.",
                "position": 305
            },
            {
                "img": "https://arxiv.org/html/2412.09283/x5.png",
                "caption": "Figure 5:High-level overview ofÂ InstanceEnhancer, illustrating the data flow and the partitioning of stages. For a detailed implementation, refer to the supplemental materials, which provide an in-depth description of the enhancer pipeline design and the interdependencies between the stages.",
                "position": 334
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09283/x6.png",
                "caption": "Figure 6:Comparison on reconstruction-via-recaption betweenğ™¸ğš—ğšœğšğšŠğš—ğšŒğšğ™²ğšŠğš™ğ™¸ğš—ğšœğšğšŠğš—ğšŒğšğ™²ğšŠğš™\\mathtt{InstanceCap}typewriter_InstanceCapand MiraData.\nCorresponding 3DVAE scores are also indicated.\nSimilar semantics shared betweenğ™¸ğš—ğšœğšğšŠğš—ğšŒğšğ™²ğšŠğš™ğ™¸ğš—ğšœğšğšŠğš—ğšŒğšğ™²ğšŠğš™\\mathtt{InstanceCap}typewriter_InstanceCapand GT are indicated by red circles and lines.",
                "position": 342
            },
            {
                "img": "https://arxiv.org/html/2412.09283/x7.png",
                "caption": "Figure 7:Visual comparison ofğ™¸ğš—ğšœğšğšŠğš—ğšŒğšğ™²ğšŠğš™ğ™¸ğš—ğšœğšğšŠğš—ğšŒğšğ™²ğšŠğš™\\mathtt{InstanceCap}typewriter_InstanceCapand Opensora on Single and Multiple Action Score. In terms of the dynamic degree of video generation, we show better consistency and enhanced multi-instance dynamic generation effect.",
                "position": 347
            },
            {
                "img": "https://arxiv.org/html/2412.09283/x8.png",
                "caption": "Table 1:Quantitative comparisons on reconstruction-via-recaption results. The best results are marked inbold, and the second-best areunderscored.\nAs a reference, CogVideoX-5b accepts226226226226text tokens, with any excess being truncated.",
                "position": 368
            },
            {
                "img": "https://arxiv.org/html/2412.09283/x8.png",
                "caption": "Figure 8:User study on instance detail and hallucination scores.\nOur instance-aware structured caption shows clear advantages compared to the coarse-structured MiraData[9].",
                "position": 434
            },
            {
                "img": "https://arxiv.org/html/2412.09283/x9.png",
                "caption": "Figure 9:Visual comparison ofğ™¸ğš—ğšœğšğšŠğš—ğšŒğšğ™²ğšŠğš™ğ™¸ğš—ğšœğšğšŠğš—ğšŒğšğ™²ğšŠğš™\\mathtt{InstanceCap}typewriter_InstanceCapand Open-Sora oninstance-level attributes.ğ™¸ğš—ğšœğšğšŠğš—ğšŒğšğ™²ğšŠğš™ğ™¸ğš—ğšœğšğšŠğš—ğšŒğšğ™²ğšŠğš™\\mathtt{InstanceCap}typewriter_InstanceCapexcels in precise instance detail fidelity and instruction-following capabilities, even with complex multi-instance and multi-attribute scenarios.",
                "position": 600
            },
            {
                "img": "https://arxiv.org/html/2412.09283/x10.png",
                "caption": "Figure 10:(a) Ablation study on the effect of camera movement hints on the accuracy of MLLM labeling. (b) Impact of human-designed class hints on the details of instance labeling.",
                "position": 619
            },
            {
                "img": "https://arxiv.org/html/2412.09283/x11.png",
                "caption": "Figure 11:(a) Comparison against the weak visual prompt for reconstruction-via-caption visualization on multi-instance targets. (b) Comparison against color screen backgrounds (red), which may negatively affect MLLM labeling performance.",
                "position": 622
            }
        ]
    },
    {
        "header": "5Conclusions and Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "1Positive/Negative Lexicon",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09283/x12.png",
                "caption": "Figure S1:The detail of Positive/Negative Lexicon",
                "position": 1215
            }
        ]
    },
    {
        "header": "2Human-designed Class Hints",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09283/x13.png",
                "caption": "Figure S2:Detailed overview of theÂ InstanceEnhancerÂ pipeline. Example No.1 as shown in FigureS9.",
                "position": 1349
            }
        ]
    },
    {
        "header": "3Prompt Design of Figure3",
        "images": []
    },
    {
        "header": "4Design of InstanceEnhancer",
        "images": []
    },
    {
        "header": "5Evaluation metrics for video reconstruction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09283/x14.png",
                "caption": "Figure S3:Inference examples of Inseval.",
                "position": 1473
            },
            {
                "img": "https://arxiv.org/html/2412.09283/x15.png",
                "caption": "Figure S4:Visualization comparing open-source models and commercial models on prompts with poorer performance.",
                "position": 1476
            }
        ]
    },
    {
        "header": "6Inseval",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09283/x16.png",
                "caption": "Figure S5:System prompt ofğ™¸ğš—ğšœğšğšŠğš—ğšŒğšğ™²ğšŠğš™ğ™¸ğš—ğšœğšğšŠğš—ğšŒğšğ™²ğšŠğš™\\mathtt{InstanceCap}typewriter_InstanceCap.",
                "position": 1689
            },
            {
                "img": "https://arxiv.org/html/2412.09283/x17.png",
                "caption": "Figure S6:Code of getting video temporal metadata.",
                "position": 1692
            },
            {
                "img": "https://arxiv.org/html/2412.09283/x18.png",
                "caption": "Figure S7:Prompt of camera movement.",
                "position": 1695
            },
            {
                "img": "https://arxiv.org/html/2412.09283/x19.png",
                "caption": "Figure S8:Prompt of actions and motion.",
                "position": 1698
            },
            {
                "img": "https://arxiv.org/html/2412.09283/x20.png",
                "caption": "Figure S9:Designed example for LLMs.",
                "position": 1701
            },
            {
                "img": "https://arxiv.org/html/2412.09283/x21.png",
                "caption": "Figure S10:Evaluation prompts of Inseval.",
                "position": 1704
            },
            {
                "img": "https://arxiv.org/html/2412.09283/x22.png",
                "caption": "Figure S11:Aligning prompt used during alignment with the open source model.",
                "position": 1707
            }
        ]
    },
    {
        "header": "7Analysis on Commercial Products vs. Open-source Models",
        "images": []
    }
]