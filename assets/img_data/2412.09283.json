[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09283/x1.png",
                "caption": "Figure 1:Top:Comparison of the reconstruction-via-recaption results between𝙸𝚗𝚜𝚝𝚊𝚗𝚌𝚎𝙲𝚊𝚙𝙸𝚗𝚜𝚝𝚊𝚗𝚌𝚎𝙲𝚊𝚙\\mathtt{InstanceCap}typewriter_InstanceCapand state-of-the-art captioning methods for annotating the ground truth video.𝙸𝚗𝚜𝚝𝚊𝚗𝚌𝚎𝙲𝚊𝚙𝙸𝚗𝚜𝚝𝚊𝚗𝚌𝚎𝙲𝚊𝚙\\mathtt{InstanceCap}typewriter_InstanceCapproduces results that more closely resemble the original video, showing greater detail fidelity (highlighted by the red circle).Bottom:The corresponding captions generated by𝙸𝚗𝚜𝚝𝚊𝚗𝚌𝚎𝙲𝚊𝚙𝙸𝚗𝚜𝚝𝚊𝚗𝚌𝚎𝙲𝚊𝚙\\mathtt{InstanceCap}typewriter_InstanceCapand others.Reddenotes incorrect captions,bluerepresents ambiguous captions, andgreenindicates detailed and accurate descriptions of video. Specific visual hints are marked as A, B, and C for clarity.\nAll videos are generated using thesamevideo generation product,Hailuo AI222https://hailuoai.com/video, which has robust prompt-following capabilities, clearly highlighting the effectiveness of𝙸𝚗𝚜𝚝𝚊𝚗𝚌𝚎𝙲𝚊𝚙𝙸𝚗𝚜𝚝𝚊𝚗𝚌𝚎𝙲𝚊𝚙\\mathtt{InstanceCap}typewriter_InstanceCap.",
                "position": 161
            }
        ]
    },
    {
        "header": "2Related works",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09283/x2.png",
                "caption": "Figure 2:Overview of InstanceCap pipeline. Details of “from dense prompts to structured phrases” design are shown in Figure3.",
                "position": 223
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09283/x3.png",
                "caption": "Figure 3:Details on “from dense prompts to structured phrases” design. We propose an improved CoT pipeline with carefully designed information interactions (red arrow), which facilitates MLLMs to accurately capture instances with precise descriptions on attributes.",
                "position": 269
            },
            {
                "img": "https://arxiv.org/html/2412.09283/x4.png",
                "caption": "Figure 4:𝙸𝚗𝚜𝚝𝚊𝚗𝚌𝚎𝚅𝚒𝚍𝙸𝚗𝚜𝚝𝚊𝚗𝚌𝚎𝚅𝚒𝚍\\mathtt{InstanceVid}typewriter_InstanceVidprovides structured captions for videos in open-domain scenarios, featuring diverse instance, expansive scenes, precise and instance-aware captions, and video-generation-friendly durations.",
                "position": 305
            },
            {
                "img": "https://arxiv.org/html/2412.09283/x5.png",
                "caption": "Figure 5:High-level overview of InstanceEnhancer, illustrating the data flow and the partitioning of stages. For a detailed implementation, refer to the supplemental materials, which provide an in-depth description of the enhancer pipeline design and the interdependencies between the stages.",
                "position": 334
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09283/x6.png",
                "caption": "Figure 6:Comparison on reconstruction-via-recaption between𝙸𝚗𝚜𝚝𝚊𝚗𝚌𝚎𝙲𝚊𝚙𝙸𝚗𝚜𝚝𝚊𝚗𝚌𝚎𝙲𝚊𝚙\\mathtt{InstanceCap}typewriter_InstanceCapand MiraData.\nCorresponding 3DVAE scores are also indicated.\nSimilar semantics shared between𝙸𝚗𝚜𝚝𝚊𝚗𝚌𝚎𝙲𝚊𝚙𝙸𝚗𝚜𝚝𝚊𝚗𝚌𝚎𝙲𝚊𝚙\\mathtt{InstanceCap}typewriter_InstanceCapand GT are indicated by red circles and lines.",
                "position": 342
            },
            {
                "img": "https://arxiv.org/html/2412.09283/x7.png",
                "caption": "Figure 7:Visual comparison of𝙸𝚗𝚜𝚝𝚊𝚗𝚌𝚎𝙲𝚊𝚙𝙸𝚗𝚜𝚝𝚊𝚗𝚌𝚎𝙲𝚊𝚙\\mathtt{InstanceCap}typewriter_InstanceCapand Opensora on Single and Multiple Action Score. In terms of the dynamic degree of video generation, we show better consistency and enhanced multi-instance dynamic generation effect.",
                "position": 347
            },
            {
                "img": "https://arxiv.org/html/2412.09283/x8.png",
                "caption": "Table 1:Quantitative comparisons on reconstruction-via-recaption results. The best results are marked inbold, and the second-best areunderscored.\nAs a reference, CogVideoX-5b accepts226226226226text tokens, with any excess being truncated.",
                "position": 368
            },
            {
                "img": "https://arxiv.org/html/2412.09283/x8.png",
                "caption": "Figure 8:User study on instance detail and hallucination scores.\nOur instance-aware structured caption shows clear advantages compared to the coarse-structured MiraData[9].",
                "position": 434
            },
            {
                "img": "https://arxiv.org/html/2412.09283/x9.png",
                "caption": "Figure 9:Visual comparison of𝙸𝚗𝚜𝚝𝚊𝚗𝚌𝚎𝙲𝚊𝚙𝙸𝚗𝚜𝚝𝚊𝚗𝚌𝚎𝙲𝚊𝚙\\mathtt{InstanceCap}typewriter_InstanceCapand Open-Sora oninstance-level attributes.𝙸𝚗𝚜𝚝𝚊𝚗𝚌𝚎𝙲𝚊𝚙𝙸𝚗𝚜𝚝𝚊𝚗𝚌𝚎𝙲𝚊𝚙\\mathtt{InstanceCap}typewriter_InstanceCapexcels in precise instance detail fidelity and instruction-following capabilities, even with complex multi-instance and multi-attribute scenarios.",
                "position": 600
            },
            {
                "img": "https://arxiv.org/html/2412.09283/x10.png",
                "caption": "Figure 10:(a) Ablation study on the effect of camera movement hints on the accuracy of MLLM labeling. (b) Impact of human-designed class hints on the details of instance labeling.",
                "position": 619
            },
            {
                "img": "https://arxiv.org/html/2412.09283/x11.png",
                "caption": "Figure 11:(a) Comparison against the weak visual prompt for reconstruction-via-caption visualization on multi-instance targets. (b) Comparison against color screen backgrounds (red), which may negatively affect MLLM labeling performance.",
                "position": 622
            }
        ]
    },
    {
        "header": "5Conclusions and Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "1Positive/Negative Lexicon",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09283/x12.png",
                "caption": "Figure S1:The detail of Positive/Negative Lexicon",
                "position": 1215
            }
        ]
    },
    {
        "header": "2Human-designed Class Hints",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09283/x13.png",
                "caption": "Figure S2:Detailed overview of the InstanceEnhancer pipeline. Example No.1 as shown in FigureS9.",
                "position": 1349
            }
        ]
    },
    {
        "header": "3Prompt Design of Figure3",
        "images": []
    },
    {
        "header": "4Design of InstanceEnhancer",
        "images": []
    },
    {
        "header": "5Evaluation metrics for video reconstruction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09283/x14.png",
                "caption": "Figure S3:Inference examples of Inseval.",
                "position": 1473
            },
            {
                "img": "https://arxiv.org/html/2412.09283/x15.png",
                "caption": "Figure S4:Visualization comparing open-source models and commercial models on prompts with poorer performance.",
                "position": 1476
            }
        ]
    },
    {
        "header": "6Inseval",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09283/x16.png",
                "caption": "Figure S5:System prompt of𝙸𝚗𝚜𝚝𝚊𝚗𝚌𝚎𝙲𝚊𝚙𝙸𝚗𝚜𝚝𝚊𝚗𝚌𝚎𝙲𝚊𝚙\\mathtt{InstanceCap}typewriter_InstanceCap.",
                "position": 1689
            },
            {
                "img": "https://arxiv.org/html/2412.09283/x17.png",
                "caption": "Figure S6:Code of getting video temporal metadata.",
                "position": 1692
            },
            {
                "img": "https://arxiv.org/html/2412.09283/x18.png",
                "caption": "Figure S7:Prompt of camera movement.",
                "position": 1695
            },
            {
                "img": "https://arxiv.org/html/2412.09283/x19.png",
                "caption": "Figure S8:Prompt of actions and motion.",
                "position": 1698
            },
            {
                "img": "https://arxiv.org/html/2412.09283/x20.png",
                "caption": "Figure S9:Designed example for LLMs.",
                "position": 1701
            },
            {
                "img": "https://arxiv.org/html/2412.09283/x21.png",
                "caption": "Figure S10:Evaluation prompts of Inseval.",
                "position": 1704
            },
            {
                "img": "https://arxiv.org/html/2412.09283/x22.png",
                "caption": "Figure S11:Aligning prompt used during alignment with the open source model.",
                "position": 1707
            }
        ]
    },
    {
        "header": "7Analysis on Commercial Products vs. Open-source Models",
        "images": []
    }
]