[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21334/x1.png",
                "caption": "",
                "position": 76
            },
            {
                "img": "https://arxiv.org/html/2505.21334/x2.png",
                "caption": "",
                "position": 111
            },
            {
                "img": "https://arxiv.org/html/2505.21334/x3.png",
                "caption": "",
                "position": 112
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21334/extracted/6485173/fig/cross.png",
                "caption": "Table 1:Compression scope of vision-language model acceleration methods.This table outlines where different methods apply compression.SpatialandTemporalrefer to compression of the input visual data, whileInner-LLMindicates compression mechanisms applied within the model‚Äôs processing.",
                "position": 133
            },
            {
                "img": "https://arxiv.org/html/2505.21334/extracted/6485173/fig/checked.png",
                "caption": "",
                "position": 151
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21334/x4.png",
                "caption": "Figure 2:Overview of ourHoliTommethod.HoliTomcompresses video LLMs across three scopes; the first two are outer-LLM pruning.Temporal Mergingmaximizes temporal compression via global redundancy-aware segmentation, merging similar tokens into their first occurrence.Spatial Mergingfurther reduces redundancy by applying tailored spatial compression based on the characteristics of remaining temporal variations.Inner-LLM Mergingleverages attention within the LLM to identify key tokens and merges less important, similar tokens, streamlining information within the LLM.",
                "position": 312
            }
        ]
    },
    {
        "header": "4Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21334/x5.png",
                "caption": "Figure 3:Left:Performance of our methodvs.FastV when pruning various layers at rate R=50%.Right:Performance comparison with varying pruning rates at a fixed layer (K=14).",
                "position": 1134
            },
            {
                "img": "https://arxiv.org/html/2505.21334/x5.png",
                "caption": "Figure 3:Left:Performance of our methodvs.FastV when pruning various layers at rate R=50%.Right:Performance comparison with varying pruning rates at a fixed layer (K=14).",
                "position": 1137
            },
            {
                "img": "https://arxiv.org/html/2505.21334/x6.png",
                "caption": "Figure 4:Performancevs.number of frames for our method and other token compression methods.",
                "position": 1142
            },
            {
                "img": "https://arxiv.org/html/2505.21334/x7.png",
                "caption": "Figure 5:Achieving superior inference.\"Other\" indicates token pre-processing time (e.g., pooling). Our proposed method reduces Time-To-First-Token (TTFT) by 2.28√ó\\times√óand achieves 1.32√ó\\times√óhigher decoding throughput, outperforming all other token compression methods and the vanilla model.",
                "position": 1177
            },
            {
                "img": "https://arxiv.org/html/2505.21334/x8.png",
                "caption": "Figure 6:Histogram of temporal pruning rates across four benchmarks (œÑ=0.80ùúè0.80\\tau=0.80italic_œÑ = 0.80).\nThe average pruning ratio for each benchmark is annotated in the top right.\nMVBench (16s duration) exhibits the highest ratio, reflecting greater temporal redundancy, while EgoSchema is the least.",
                "position": 1325
            },
            {
                "img": "https://arxiv.org/html/2505.21334/x9.png",
                "caption": "Figure 7:Ablation study on video segmentation methods.This table compares different video segmentation strategies: Fixed-interval segmentation partitions the video at equal intervals; DySeg adaptively segments based on transition similarity; and our proposed global redundancy-aware segmentation.",
                "position": 1342
            },
            {
                "img": "https://arxiv.org/html/2505.21334/x9.png",
                "caption": "Figure 8:Ablation study onœÑùúè\\tauitalic_œÑ.Performance of our method is analyzed with varyingœÑùúè\\tauitalic_œÑat a target before LLM retained ratio of 15%.",
                "position": 1393
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASupplemental Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21334/x10.png",
                "caption": "Figure 9:Histogram of temporal pruning rates across four benchmarks (œÑ=0.65ùúè0.65\\tau=0.65italic_œÑ = 0.65).\nThe average pruning ratio for each benchmark is annotated in the top right.\nMVBench (16s duration) exhibits the highest ratio, reflecting greater temporal redundancy, while VideoMME is the least (œÑ=0.65ùúè0.65\\tau=0.65italic_œÑ = 0.65).",
                "position": 2132
            }
        ]
    },
    {
        "header": "Appendix BSupplemental Ablation Study onœÑùúè\\tauitalic_œÑ",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21334/x11.png",
                "caption": "Figure 10:Ablation study onœÑùúè\\tauitalic_œÑ.Performance of our method is analyzed with varyingœÑùúè\\tauitalic_œÑat a target before LLM retained ratio of 10%.",
                "position": 2142
            }
        ]
    },
    {
        "header": "Appendix CCompatible with Flash Attention",
        "images": []
    },
    {
        "header": "Appendix DLimitations and Future Work",
        "images": []
    },
    {
        "header": "Appendix EBroader impacts",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21334/x12.png",
                "caption": "Figure 11:Comparison on Challenging Video Understanding.Green: correct results,Red: incorrect results. Our method is able to produce correct answers on challenging video tasks.",
                "position": 2183
            },
            {
                "img": "https://arxiv.org/html/2505.21334/x13.png",
                "caption": "Figure 12:Qualitative generation comparison.Greenindicates correctly detailed descriptions. Our method achieves high-quality, accurate text generation even when retaining only 15% of input tokens.",
                "position": 2187
            }
        ]
    },
    {
        "header": "Appendix FMore Visualizations",
        "images": []
    }
]