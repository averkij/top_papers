[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04178/extracted/6510823/figures/open_thoughts_full2.png",
                "caption": "",
                "position": 161
            },
            {
                "img": "https://arxiv.org/html/2506.04178/x1.png",
                "caption": "Figure 1:OpenThoughts3 outperforms existing SFT reasoning datasets across data scales.All models are finetuned from Qwen-2.5-7B-Instruct. We compare to large SFT datasets (AM, Nemotron Nano) and small curated datasets (s1.1, LIMO) on AIME 2025 (left), LiveCodeBench 06/24-01/25 (middle), and GPQA Diamond (right).\nScaling curves for all evaluation benchmarks are inFigure8.",
                "position": 248
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04178/extracted/6510823/figures/qwen_logo.jpeg",
                "caption": "Table 1:OpenThinker3-7B outperforms all open-data 7B and 8B reasoning models across domains.Our model also performs well on held out benchmarks which are not measured during our main experimentation,\nsuch as HMMT and AIME25.\nIn our table,denotes a model trained from Qwen-2.5-7B-Instruct,Mfor Qwen-2.5-Math-Base,for Llama-3.1-8B-Instruct,\nandfor DeepSeek-R1-Distill-Qwen-7B.“Base Model” denotes the starting checkpoint of the training strategy. “Method” denotes the model’s optimization algorithm.\nIn each row, we bold values within two standard errors of the highest-scoring model.",
                "position": 263
            },
            {
                "img": "https://arxiv.org/html/2506.04178/extracted/6510823/figures/meta.png",
                "caption": "",
                "position": 390
            },
            {
                "img": "https://arxiv.org/html/2506.04178/extracted/6510823/figures/deepseek.png",
                "caption": "",
                "position": 395
            },
            {
                "img": "https://arxiv.org/html/2506.04178/extracted/6510823/figures/realgreen.jpeg",
                "caption": "",
                "position": 575
            },
            {
                "img": "https://arxiv.org/html/2506.04178/extracted/6510823/figures/red_circle.png",
                "caption": "",
                "position": 580
            },
            {
                "img": "https://arxiv.org/html/2506.04178/extracted/6510823/figures/yellow_circle.png",
                "caption": "",
                "position": 607
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3The OpenThoughts project",
        "images": []
    },
    {
        "header": "4OpenThoughts3 Data Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04178/x2.png",
                "caption": "Figure 2:The OpenThoughts experiment pipeline aims to build the strongest reasoning dataset recipe.We investigate (1) sourcing questions from existing and newly generated datasets, (2) mixing questions from the top-performing sources, (3) filtering for high-quality questions usingfastTextor LLMs, (4) deduplicating questions and sampling multiple answers per question, (5) filtering out low-quality answers using LLM verification or majority consensus, and (6) selecting the best teacher model.",
                "position": 1482
            }
        ]
    },
    {
        "header": "5Scaling our pipeline to OpenThoughts3-1.2M",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04178/x3.png",
                "caption": "Figure 3:Scaling the top strategies from each pipeline step.Across dataset scales, the datasets created by subsequent stages in the pipeline shift the scaling curve upwards. The largest gains come from the selection of question sources, question filtering strategies, and teacher model selection. The average performance on math and code has a clearer scaling trend than the performance on science – the final \"Teacher Model\" curve not being the top science performer is a consequence of our design choice, where we select winning strategies based on average performance across domains.",
                "position": 2287
            },
            {
                "img": "https://arxiv.org/html/2506.04178/x4.png",
                "caption": "Figure 4:The OpenThoughts3-1.2M Full Data Pipeline.Our full dataset begins with sourcing many questions in math, code, and science domains. The next step is filtering those questions, deduplicating the math and science questions, randomly sampling questions, and then generating multiple answers for each question. Our final dataset contains 1.2 million datapoints.",
                "position": 2306
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALinks to Assets",
        "images": []
    },
    {
        "header": "Appendix BContributions",
        "images": []
    },
    {
        "header": "Appendix CBespoke-Stratos and OpenThoughts 1 & 2",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04178/extracted/6510823/figures/openthoughts1_data_diagram.png",
                "caption": "Figure 5:OpenThoughts-114K data recipe.OpenThoughts1 was constructed by sourcing questions from each of the four domains, completing answers with DeepSeek-R1, verifying (Math, Puzzle, and Code – not Science) and mixing.",
                "position": 3400
            },
            {
                "img": "https://arxiv.org/html/2506.04178/x5.png",
                "caption": "Figure 6:OpenThoughts2-1M data recipe.OpenThoughts2-1M improved upon OpenThoughts-114K by adding new question generation strategies. This included preexisting datasets such as OpenR1 and CodeFeedback and also ones we generated ourselves, such as AutoMathText. Combining this with deduplication led us to our1111million-sized dataset.",
                "position": 3403
            }
        ]
    },
    {
        "header": "Appendix DTraining Details",
        "images": []
    },
    {
        "header": "Appendix EEvaluation Details",
        "images": []
    },
    {
        "header": "Appendix FDecontamination",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04178/x6.png",
                "caption": "Figure 7:Our decontamination algorithm accurately identifies contaminated samples.Our decontamination algorithm has a99.6%percent99.699.6\\%99.6 %true negative accuracy rate. The algorithm also throws out minimal amounts of noncontaminated samples.",
                "position": 3909
            }
        ]
    },
    {
        "header": "Appendix GAdditional Scaling Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04178/x7.png",
                "caption": "Figure 8:Downstream model performance after finetuning Qwen-2.5-7B-Instruct on increasingly larger subsets from OpenThoughts3-1.2M. Across a wide variety of math (AIME, AMC, MATH, HMMT), code (LiveCodeBench, CodeElo, CodeForces), and science (JEEBench, HLE) benchmarks, OpenThoughts3-1.2M outperforms existing reasoning datasets. HMMT, AIME 2025, LiveCodeBench 06/24-01/25, and HLE are \"held out\", which means that we did not use them to evaluate any intermediate models during our experiments to inform our data recipe.",
                "position": 3917
            },
            {
                "img": "https://arxiv.org/html/2506.04178/x8.png",
                "caption": "Figure 9:The OpenThoughts3 data recipe within each domain shows strong scaling over baselines. Math performance is averaged over AIME24, AMC32, and MATH500. Code performance is averaged over LCB 05/23-05/24, CodeElo, and CodeForces. Science is averaged over GPQA Diamond and JEEBench. The largest scale for the OpenThoughts3 math and science subsets are 250K and 100K, respectively.",
                "position": 3940
            }
        ]
    },
    {
        "header": "Appendix HAdditional Data Recipe Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04178/x9.png",
                "caption": "(a)AIME 24",
                "position": 4219
            },
            {
                "img": "https://arxiv.org/html/2506.04178/x9.png",
                "caption": "(a)AIME 24",
                "position": 4222
            },
            {
                "img": "https://arxiv.org/html/2506.04178/x10.png",
                "caption": "(b)LiveCodeBench",
                "position": 4227
            },
            {
                "img": "https://arxiv.org/html/2506.04178/x11.png",
                "caption": "(c)GPQA Diamond",
                "position": 4232
            }
        ]
    },
    {
        "header": "Appendix IModel Reasoning Performance Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04178/x12.png",
                "caption": "Figure 11:GPT-4o-mini can reliably determine the relative difficulty of questions. DeepSeek R1 performs substantially worse on the hardest questions (difficulty ten).",
                "position": 5119
            },
            {
                "img": "https://arxiv.org/html/2506.04178/x13.png",
                "caption": "Figure 12:OpenThoughts3 performance scaling on LiveCodeBench 05/23-05/24 as the dataset size is increased. The contribution of the performance on each category of problem is represented.",
                "position": 5129
            }
        ]
    },
    {
        "header": "Appendix JSurpassing the Teacher with Distillation for Legal Reasoning",
        "images": []
    },
    {
        "header": "Appendix KAll Teachers Ablations",
        "images": []
    },
    {
        "header": "Appendix LSafety Analysis of OpenThinker Models",
        "images": []
    },
    {
        "header": "Appendix MExisting Frontier Model Evaluations",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04178/extracted/6510823/figures/gemini.png",
                "caption": "Table 31:Performance of current API-based frontier models.A significant gap remains between current open-source reasoning models and frontier reasoning models. The vertical line denotes the division between models that underperform and overperform OpenThinker3-7B according to average benchmark performance. The largest gaps arise from benchmarks such as CodeElo, CodeForces, and GPQA Diamond. Gemini-2.5-pro is the most performant model.",
                "position": 5811
            },
            {
                "img": "https://arxiv.org/html/2506.04178/extracted/6510823/figures/openai.png",
                "caption": "",
                "position": 5908
            },
            {
                "img": "https://arxiv.org/html/2506.04178/extracted/6510823/figures/claude.png",
                "caption": "",
                "position": 5923
            },
            {
                "img": "https://arxiv.org/html/2506.04178/extracted/6510823/figures/grok.png",
                "caption": "",
                "position": 5935
            }
        ]
    },
    {
        "header": "Appendix NTesting reasoning robustness: Alice in Wonderland evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04178/x14.png",
                "caption": "Figure 13:Distilled reasoning models show deficits in generalization.All distilled reasoning models exhibit strong performance fluctuations on AIW Friends variations 1-6Nezhurina et al. (2024), despite the variations not changing problem structure at all. This points to generalization deficits. The fluctuations affect to same extent SFT only (eg S1.1 32B, LIMO-32B) and SFT+RL (eg Light-R1-32B, QwQ 32B) reasoning models. Smaller scale models, eg OpenThinker3-7B, perform worse than larger scale ones, eg OpenThinker-32B, showing overall lower correct response rates. Larger scale 32B models, while having higher overall correct response rate, show strong fluctuations, e.g. OpenThinker-32B going from close to 1 on variations 2 and 3 down to close to 0 on variation 1. For reference, o1-preview and o3-mini are shown, which have much smaller fluctuations and higher overall correct response rates. Distilled models still do not possess robust zero-shot generalization on simple problems. Numbers in the legend are prompt IDs, seeNezhurina et al. (2024)andAIW repo",
                "position": 6656
            },
            {
                "img": "https://arxiv.org/html/2506.04178/x15.png",
                "caption": "Figure 14:Reasoning models outperform conventional LLMs.Albeit suffering from strong fluctuations, larger-scale distilled reasoning models set themselves apart from the conventional language models from which they were distilled. Shown are correct response rates averaged across all variations of AIW Friends, AIW Plus, and AIW Circles Colleagues problemsNezhurina et al. (2024). Larger reasoning models on a 32B scale are populating the upper correct response rate range (the only exception being R1-Qwen-32B). Conventional LLMs, including the largest scale Llama 3.1 405B and DeepSeek v3 671B, stay confined to the low correct response rate region below 0.2. As a reference, we show closed reasoning models o3-mini and o1-preview that show only weak fluctuations and settle in the upper performance region above 0.7%",
                "position": 6659
            }
        ]
    },
    {
        "header": "Appendix OCompute Requirements",
        "images": []
    },
    {
        "header": "Appendix PSourcing reasoning traces from the Web",
        "images": []
    },
    {
        "header": "Appendix QLicenses of Existing Assets",
        "images": []
    },
    {
        "header": "Appendix RPipeline Details",
        "images": []
    },
    {
        "header": "Appendix SPipeline Experiments Expanded Results",
        "images": []
    }
]