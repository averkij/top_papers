[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17190/extracted/6028160/fig_main/teaser2.jpg",
                "caption": "Figure 1:Overview of SelfSplat. Given unposed multi-view images as input, we predict depth and Gaussian attributes from the images, as well as the relative camera poses between them. We unify a self-supervised depth estimation framework with explicit 3D representation achieving accurate scene reconstruction.",
                "position": 147
            }
        ]
    },
    {
        "header": "3Preliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17190/extracted/6028160/fig_main/sub_figure_architecture_font_up.jpg",
                "caption": "Figure 2:Matching-aware pose network (a) and depth refinement module (b). We leverage cross-view features from input images to achieve accurate camera pose estimation, and use these estimated poses to further refine the depth maps with spatial awareness.",
                "position": 200
            }
        ]
    },
    {
        "header": "4Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17190/extracted/6028160/fig_main/main_result.jpg",
                "caption": "Figure 3:Qualitative comparison of novel view synthesis on RE10k (top two rows) and ACID (bottom row) datasets.",
                "position": 697
            },
            {
                "img": "https://arxiv.org/html/2411.17190/extracted/6028160/fig_main/dl3dv.jpg",
                "caption": "Figure 4:Qualitative comparison of novel view synthesis on DL3DV dataset.",
                "position": 1034
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17190/extracted/6028160/fig_main/epi.jpg",
                "caption": "Figure 5:Epipolar lines visualization. We draw the lines from reference to target frame using relative camera pose.",
                "position": 1276
            },
            {
                "img": "https://arxiv.org/html/2411.17190/extracted/6028160/fig_main/ablation.jpg",
                "caption": "Figure 6:Ablation studies on our proposed component.",
                "position": 1311
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]