[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07104/x1.png",
                "caption": "Figure 1:VLV matches GPT-4o‚Äôs descriptive fidelity atthree orders of magnitudelower cost.Left:VLV captures all salient objects, matching GPT-4o in coverage without hallucinations, yet better preserving their spatial layout.Right:On the FID‚Äìcost‚Äìthroughput plane,VLVreaches comparable FID, trains for orders-of-magnitude less, and delivers vastly higher captions-per-dollar at inference‚Äîproving that detail-rich descriptions need not demand massive budgets.",
                "position": 143
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07104/x2.png",
                "caption": "Figure 2:Method Overview.Our method has two stages: 1) vision-language-vision autoencoding for learning language semantics, 2) representation decoding into discrete language tokens through multi-modal LLM alignment. Our model has three major modules(i) VLV Encoder: a visual backbone augmented with a lightweight multi-modal adapter maps an input image into continuous caption embedding with compact semantic information;(ii) Diffusion Decoder: afrozentext-to-image diffusion model reconstructs the image;(iii) Caption Decoder: a pretrained large language model with an MLP projector decodes language-centric representations into comprehensive captions.",
                "position": 234
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07104/x3.png",
                "caption": "Figure 3:Reconstruction with language semantics.For each original input image (top), we feed itscaption embeddingdirectly to the frozen diffusion decoder and obtain a reconstruction (middle) that preserveshigh-level semanticsandfine-grained appearance cues.\nThe same embedding is then decoded by the LLM; prompting Midjourney with that caption yields an image of high fidelity.",
                "position": 337
            },
            {
                "img": "https://arxiv.org/html/2507.07104/x4.png",
                "caption": "Figure 4:Representation Learning Beyond Text: Spatial Preservation.The figure compares the original images (left) with those reconstructed by our embeddings. The accurate 6D poses of individual objects and the relative spatial configurations among multiple objects demonstrate the method‚Äôs strong capability in capturing spatial structure.",
                "position": 735
            },
            {
                "img": "https://arxiv.org/html/2507.07104/x5.png",
                "caption": "Figure 5:Continual Spatial Representation LearningVLV enables continual 3D spatial representation learning.",
                "position": 738
            },
            {
                "img": "https://arxiv.org/html/2507.07104/x5.png",
                "caption": "Figure 5:Continual Spatial Representation LearningVLV enables continual 3D spatial representation learning.",
                "position": 741
            },
            {
                "img": "https://arxiv.org/html/2507.07104/x6.png",
                "caption": "Figure 6:Emerging compositionality with multi-image semantics.Given two input images‚Äîa Siberian cat at theleftedge of the frame and either (above) a Van Gogh-style painting or (bottom) a Mount Fuji landscape‚Äîwe truncate and concatenate their caption embeddings and feed the composite vector toStable Diffusion 2.1.\nThe generated outputs faithfully preserve the cat‚Äôs spatial layout while transferring the desired artistic style or background,without any extra fine-tuning or text prompts.",
                "position": 830
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AData Processing",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07104/x7.png",
                "caption": "Figure 7:Data Filtering Principles.We filter and collect 40M images from LAION-2B-en-aesthetic. We apply filtering based on the image resolution and aspect ratio to ensure the image quality and then prompt Gemini 2.0 Flash with image-conditioned templates to generate rich, descriptive captions.",
                "position": 2078
            },
            {
                "img": "https://arxiv.org/html/2507.07104/extracted/6614125/figures/sup/Stage_2_Captions_distribution.png",
                "caption": "Figure 8:VLV Captions‚Äô Length Statistics.Histogram of token counts for all captions (our‚àº6‚Å¢Msimilar-toabsent6M\\sim\\!6\\text{M}‚àº 6 Mimage-text paired data, used for stage-2 captioning).\nMost captions fall in the170‚àí280170280170\\!-\\!280170 - 280token band, with meanŒº=226.82ùúá226.82\\mu\\!=\\!226.82italic_Œº = 226.82(red dashed) and medianx~=226~ùë•226\\tilde{x}\\!=\\!226over~ start_ARG italic_x end_ARG = 226(green dashed).",
                "position": 2081
            }
        ]
    },
    {
        "header": "Appendix BVQA Analysis: Are ‚ÄúGround Truth\" labels really ground truth?",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07104/x8.png",
                "caption": "Figure 9:OK-VQA Example.Both our caption and Gemini caption do not mention the states information. But our caption not only capture the oranges but also the number of oranges. Our answers contain the right ones highlighting inLimeGreen.",
                "position": 2097
            }
        ]
    },
    {
        "header": "Appendix CVision-Language-Vision Autoencoding Does Help",
        "images": []
    },
    {
        "header": "Appendix DCaption Evaluation with SoTA Multi-modal LLM (Gemini)",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07104/x9.png",
                "caption": "Figure 10:Captioner Arena Example.All captions show the correct objects without hallucinations. Both our caption and GPT-4o caption show the spatial relationship while Qwen-2.5 VL does not.",
                "position": 2161
            }
        ]
    },
    {
        "header": "Appendix EQualitative Results: Reconstruction from Captions",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07104/x10.png",
                "caption": "Figure 11:VLV can capture spatial layout.The caption shows bear‚Äôs layout(in the center of the frame)in this image as well as the bear‚Äôs posture(head turned towards the right side), showing VLV‚Äôs ability of capturing spatial layout.",
                "position": 2239
            },
            {
                "img": "https://arxiv.org/html/2507.07104/x11.png",
                "caption": "Figure 12:VLV can capture text (OCR).VLV has reasonable OCR ability, even though the training set is heavily filtered (we filter the data by watermark probability less than 0.5). There is still potential to improve OCR performance with further training on more OCR-oriented data.",
                "position": 2243
            },
            {
                "img": "https://arxiv.org/html/2507.07104/x12.png",
                "caption": "Figure 13:VLV can capture complex objects.Caption enumerates almost every object and correctly describe their spatial relationships, highlighting VLV‚Äôs comprehensive scene understanding.",
                "position": 2247
            },
            {
                "img": "https://arxiv.org/html/2507.07104/x13.png",
                "caption": "Figure 14:VLV can capture human posture.Captions show details of human as well as his posture, demonstrating VLV‚Äôs fine-grained posture awareness.",
                "position": 2251
            }
        ]
    },
    {
        "header": "Appendix FDataset & Model License",
        "images": []
    }
]