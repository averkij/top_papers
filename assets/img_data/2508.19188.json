[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.19188/figure/Teaser.png",
                "caption": "",
                "position": 114
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.19188/x1.png",
                "caption": "Figure 2:(a)Overall architecture ofFastMesh. Note that our pipeline consists of two stages, where we first generate the vertices from the shape condition and then construct the faces to complete the mesh.(b)Visualization of the block-wise indexing scheme introduced by BPT[49], which we adopt for vertex tokenization.(c)Structure of the fidelity enhancer in the first stage. The 7-bit discretized vertices and shape condition are fed into the network to estimate the offset that can make the coordinate a continuous value.(d)Details of face reconstruction. The generated vertices are embedded to capture inter-vertex relationships in a multi-head manner. Each head computes a matrix, where the output represents one feature dimension used in edge prediction.",
                "position": 176
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Proposed Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.19188/figure/discretize.png",
                "caption": "Figure 3:(a)Example of high-resolution mesh containing 7,694 vertices.(b)Mesh obtained by discretizing (a) in 7-bit coordinate space, resulting in 3,636 vertices.(c)Mesh reconstructed using the same 3,636 vertices as in (b), with continuous coordinates refined by our fidelity enhancer.",
                "position": 213
            },
            {
                "img": "https://arxiv.org/html/2508.19188/figure/candidate_reduction.png",
                "caption": "Figure 4:The detailed structure of the prediction filtering. We use the initial adjacency matrix from the first face generation to perform BFS reordering. Based on this reordering, we apply the maximum bandwidth mask and the minimum candidate mask as attention masks.",
                "position": 273
            },
            {
                "img": "https://arxiv.org/html/2508.19188/figure/comparision.png",
                "caption": "Figure 5:Qualitative comparison of shape-conditioned mesh generation on the Toys4K dataset[38]. All meshes were generated from the same input point clouds that were sampled from the original meshes.",
                "position": 286
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.19188/figure/objaverse.png",
                "caption": "Figure 6:Diverse view of mesh results from the proposed method in the Objaverse dataset[10].",
                "position": 412
            },
            {
                "img": "https://arxiv.org/html/2508.19188/figure/fidelity_enhancer.png",
                "caption": "Figure 7:Comparison of the mesh results fromFastMesh-V4K according to the usage of the fidelity enhancer.",
                "position": 532
            },
            {
                "img": "https://arxiv.org/html/2508.19188/figure/fidelity_enhancer.png",
                "caption": "Figure 7:Comparison of the mesh results fromFastMesh-V4K according to the usage of the fidelity enhancer.",
                "position": 535
            },
            {
                "img": "https://arxiv.org/html/2508.19188/figure/V1KV4K.png",
                "caption": "Figure 8:Comparison results of the two variants of the proposed method. The meshes are generated from input point clouds extracted from the original meshes for each model.",
                "position": 540
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.19188/figure/bad_data.png",
                "caption": "Figure 9:Examples from the dataset before and after filtering. (a) A non-manifold mesh with only minor defects is retained for training due to its limited impact on model performance. (b) A non-manifold mesh that is removed during filtering. (c) An undesirable structure with excessive faces used to represent a coplanar surface, also removed through filtering.",
                "position": 1353
            },
            {
                "img": "https://arxiv.org/html/2508.19188/figure/V10.png",
                "caption": "Figure 10:Results with and without prediction filtering. The post-processing reduces face count while preserving mesh structure.",
                "position": 1475
            }
        ]
    },
    {
        "header": "8Detail Analysis in Toys4K dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.19188/figure/application.png",
                "caption": "Figure 11:Examples of Image- and Text-to-3D Generation. The initial meshes are generated by TRELLIS[50], from which we extract point clouds to serve as shape conditions. (left: inputs, right: our results)",
                "position": 1542
            },
            {
                "img": "https://arxiv.org/html/2508.19188/figure/supp_toys4K.png",
                "caption": "Figure 12:Additional qualitative comparison in Toys4K[38]dataset",
                "position": 1554
            },
            {
                "img": "https://arxiv.org/html/2508.19188/figure/supp_ojbaverse.png",
                "caption": "Figure 13:Additional qualitative comparison in Objaverse[10]and ObjaverseXL[11]dataset",
                "position": 1560
            }
        ]
    },
    {
        "header": "9Applications and Additional Results",
        "images": []
    }
]