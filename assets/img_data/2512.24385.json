[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.24385/figs/icons/worldbench.png",
                "caption": "",
                "position": 115
            },
            {
                "img": "https://arxiv.org/html/2512.24385/figs/icons/car1.png",
                "caption": "",
                "position": 118
            },
            {
                "img": "https://arxiv.org/html/2512.24385/figs/icons/car2.png",
                "caption": "",
                "position": 119
            },
            {
                "img": "https://arxiv.org/html/2512.24385/figs/icons/github.png",
                "caption": "",
                "position": 156
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.24385/x1.png",
                "caption": "Figure 1:Overview of the paper structure.We systematically structure the landscape of multi-modal data pre-training for forging Spatial Intelligence. This work is organized into four key pillars: (1)Background, introducing onboard sensors and foundational learning paradigms; (2)Platforms & Datasets, analyzing benchmarks across autonomous vehicles, drones, and other robotic systems; (3)Pre-Training Methodologies, categorized into single-modality, cross-modal (Camera/LiDAR-centric), and unified frameworks; and (4)Applications, highlighting downstream tasks from 3D perception to open-world planning.",
                "position": 179
            }
        ]
    },
    {
        "header": "2Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.24385/x2.png",
                "caption": "Figure 2:Chronological evolution of representative pre-training methods (2020–2025).The timeline illustrates the paradigm shift in representation learning for autonomous systems. Early approaches predominantly focused on single-modality self-supervision (e.g.,LiDAR-onlycontrastive learning). In contrast, recent advancements (2023–present) demonstrate a surge in cross-modal synergy, characterized byCamera/LiDAR-centricmethods andUnifiedpre-training frameworks, ultimately paving the way for generative world models and comprehensive spatial intelligence.",
                "position": 307
            }
        ]
    },
    {
        "header": "3Platform-specific Datasets",
        "images": []
    },
    {
        "header": "4Pre-Training Techniques for Perception",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.24385/x3.png",
                "caption": "Figure 4:Schematic illustration of representative LiDAR-only pre-training paradigms.To learn robust geometric representations from sparse point clouds without annotations, methods typically adopt three strategies:(a) Masked Autoencoding (MAE), which reconstructs missing structures to learn local geometry;(b) Contrastive Learning, which enforces view-invariant feature discrimination; and(c) Temporal Forecasting, which predicts future frames to capture dynamic scene evolution.",
                "position": 1914
            },
            {
                "img": "https://arxiv.org/html/2512.24385/x4.png",
                "caption": "Figure 5:Taxonomy of LiDAR-centric pre-training methodologies.To bridge the semantic gap of point clouds, these approaches leverage images as privileged information during training. The main paradigms involve:(a) Cross-modal MAE-based Pre-Training, which incorporates2D-guided maskingstrategies to enhance geometric reconstruction and structural understanding;(b) Cross-modal Contrastive/Distillation Pre-Training, which either enforces feature alignment between modalities or directly transfers rich open-vocabulary semantics from pre-trained Vision Foundation Models (VFMs) to 3D encoders; and(c) Temporal Pre-Training, which exploits video-LiDAR sequences to capture motion dynamics and enforce spatiotemporal consistency.",
                "position": 1999
            },
            {
                "img": "https://arxiv.org/html/2512.24385/x5.png",
                "caption": "Figure 6:Overview of camera-centric pre-training paradigms (LiDAR-to-Vision).These methods aim to inject 3D geometric priors into 2D visual backbones using LiDAR as a supervisor. Key approaches include:\n(a)Depth Estimationfor explicit geometry learning;\n(b)Feature Distillationto align 2D-3D latent spaces; and\n(c)Forecastingand (d)Generative Rendering, which empower vision models to hallucinate 3D structures and predict future dynamics from monocular inputs.",
                "position": 2188
            },
            {
                "img": "https://arxiv.org/html/2512.24385/x6.png",
                "caption": "Figure 7:Illustration of unified multi-modal pre-training frameworks.Unlike asymmetric distillation, unified approaches jointly optimize Camera and LiDAR encoders within a shared representation space.\nThis paradigm facilitates the learning ofmodality-agnosticfeatures that integrate both semantic richness and geometric precision, forging a holistic basis for Spatial Intelligence.",
                "position": 2230
            }
        ]
    },
    {
        "header": "5Open-World Perception and Planning",
        "images": []
    },
    {
        "header": "6Challenges and Future Directions",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]