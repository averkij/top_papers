[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01061/extracted/6173608/imgs/omni1.png",
                "caption": "",
                "position": 71
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01061/extracted/6173608/imgs/framework.jpg",
                "caption": "Figure 2:The framework of OmniHuman.It consists of two parts: (1) the OmniHuman model, which is based on the DiT architecture and supports simultaneous conditioning with multiple modalities including text, image, audio, and pose; (2) the omni-conditions training strategy, which employs progressive, multi-stage training based on the motion-related extent of the conditions. The mixed condition training allows the OmniHuman model to benefit from the scaling up of mixed data.",
                "position": 137
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01061/x1.png",
                "caption": "Figure 6:Ablation study on different audio condition ratios.The models are trained with different audio ratios (top: 10%, middle: 50%, bottom: 100%) and tested in an audio-driven setting with the same input image and audio.",
                "position": 486
            },
            {
                "img": "https://arxiv.org/html/2502.01061/extracted/6173608/imgs/pose_ratio0.jpeg",
                "caption": "Figure 7:Ablation study on different pose condition ratios.The models are trained with different pose ratios (top: 20%, middle: 50%, bottom: 80%) and tested in an audio-driven setting with the same input image and audio.",
                "position": 489
            },
            {
                "img": "https://arxiv.org/html/2502.01061/extracted/6173608/imgs/pose_ratio1.jpeg",
                "caption": "Figure 8:Ablation study on different pose condition ratios.The models are trained with different pose ratios (top: 20%, middle: 50%, bottom: 80%) and tested in an audio-driven setting with the same input image and audio.",
                "position": 492
            },
            {
                "img": "https://arxiv.org/html/2502.01061/x2.png",
                "caption": "Figure 9:Ablation study on reference condition ratios.Comparisons of visualization results for 30s videos at different reference ratios.",
                "position": 495
            },
            {
                "img": "https://arxiv.org/html/2502.01061/extracted/6173608/imgs/omni2.jpg",
                "caption": "Figure 10:The videos generated by OmniHuman based on input audio and images.OmniHuman is compatible with stylized humanoid and 2D cartoon characters, and can even animate non-human images in an anthropomorphic manner.",
                "position": 506
            },
            {
                "img": "https://arxiv.org/html/2502.01061/extracted/6173608/imgs/omni3.jpg",
                "caption": "Figure 11:The videos generated by OmniHuman based on input audio and images.These demonstrates OmniHumanâ€™s compatibility with various environments, objects, and camera angles, producing satisfactory results.",
                "position": 532
            },
            {
                "img": "https://arxiv.org/html/2502.01061/extracted/6173608/imgs/omni4.jpg",
                "caption": "Figure 12:The videos generated by OmniHuman based on input audio and images.OmniHuman can generate highly realistic human motion videos, whether portrait or full-body. In these relatively standard scenarios, OmniHuman still performs satisfactorily.",
                "position": 535
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]