[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.06993/x1.png",
                "caption": "Figure 1:Performance degradation with CoT and reasoning collapse in RFT.In zero-shot evaluation (top), MLLMs predict the correct label directly, but adding CoT reasoning leads to a wrong answer. During RFT (bottom), reasoning length steadily shrinks while accuracy improves, indicating a reasoning collapse.",
                "position": 107
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Cost of Thinking in FGVC",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.06993/x2.png",
                "caption": "Figure 2:Dynamics of reasoning length during RFT across FGVC datasets.The dark green lines denote the running average of completion lengths throughout RFT FGVC tasks. Across all datasets, the reasoning content length rapidly decreases and stabilizes at a shorter range, suggesting that RFT discourages excessive reasoning generation and promotes concise, decision-focused responses. [Zero-shot: average content length of base model on the evaluation set; Step: the cumulative number of gradient update steps.]",
                "position": 341
            },
            {
                "img": "https://arxiv.org/html/2601.06993/x3.png",
                "caption": "Figure 3:Impact of reasoning length on FGVC performance.We analyze the relationship between average reasoning (thinking) length and classification accuracy across FGVC datasets. As the average thinking length increases, performance consistently declines, indicating that excessive reasoning generation introduces noise or distracting the model from key discriminative visual cues.",
                "position": 364
            },
            {
                "img": "https://arxiv.org/html/2601.06993/x4.png",
                "caption": "Figure 4:Overview of ReFine-RFT.Given a question, the model generates multiple candidate responses, each evaluated using an ensemble reward that combines rule-based rewards and model-based rewards like MLLM-based accuracy reward and embedding similarity reward. The proposed MRN then normalizes the rewards for each function to compute the final advantages used to update the MLLM.",
                "position": 367
            }
        ]
    },
    {
        "header": "4Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.06993/x5.png",
                "caption": "Figure 5:Differences among rewards during training.Each reward exhibits distinct convergence speed, value range, and saturation point, reflecting the heterogeneity of different rewards.",
                "position": 463
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.06993/x6.png",
                "caption": "Figure 6:Reward curves of ReFine-RFT on Flowers-102.Rewards consistently increase over training, demonstrating the effectiveness of our reward design.",
                "position": 857
            },
            {
                "img": "https://arxiv.org/html/2601.06993/x7.png",
                "caption": "Figure 7:Training reward and its standard deviation comparison on Aircrafts-102.MRN+GRPO\\text{MRN}+\\text{GRPO}achieves consistently higher reward values and lower variance throughout training, indicating improved stability and optimization efficiency.",
                "position": 872
            },
            {
                "img": "https://arxiv.org/html/2601.06993/x8.png",
                "caption": "Figure 8:Comparison of responses.SFT-CoT and Visual-RFT produce long reasoning with incorrect answers, while ReFine-RFT achieves concise reasoning and higher accuracy. More results and analyses are in the supplementary.",
                "position": 875
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]