[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3The Gated Associative Memory (GAM) Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.00605/gam_block_diagram.png",
                "caption": "Figure 1:The GAM Block.The inputxxfirst passes through a Layer Normalization. It then splits into three branches. The first branch is a residual connection. The second branch computes local and global context in parallel. The local context is generated by a Causal 1D Convolution. The global context is generated by querying a learnable Memory Bank. The outputs of these two pathways are combined via a learned gate. The gated output is added to the residual connection. This is followed by another Layer Normalization and a standard Feed-Forward Network, also with a residual connection.",
                "position": 191
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.00605/scaling.png",
                "caption": "Figure 2:GAM vs. Transformer Scaling Comparison.(Left) Average forward-backward pass time vs. sequence length. (Right) Peak GPU memory usage vs. sequence length. Both axes are on a logarithmic scale. The Transformerâ€™s quadratic growth is evident in the steep upward curve, while GAM exhibits clear linear scaling. The Transformer fails due to out-of-memory errors beyond a sequence length of 2048 in this setup.",
                "position": 571
            },
            {
                "img": "https://arxiv.org/html/2509.00605/wikitext2_dynamics.png",
                "caption": "Figure 3:Training dynamics on the WikiText-2 dataset.(a) Validation perplexity, (b) validation loss, and (c) wall-clock time per epoch. The plots show GAM (in blue) achieving a lower final perplexity and consistently faster epoch times compared to both the Transformer (in red) and Mamba (in green) baselines.",
                "position": 599
            },
            {
                "img": "https://arxiv.org/html/2509.00605/tinystories_dynamics.png",
                "caption": "Figure 4:Training dynamics on the TinyStories dataset.(a) Validation perplexity, (b) validation loss, and (c) wall-clock time per epoch. GAM (in green) demonstrates a faster learning trajectory and maintains a significant efficiency advantage throughout the 5 epochs compared to the Transformer (in violet).",
                "position": 602
            }
        ]
    },
    {
        "header": "6Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]