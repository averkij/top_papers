[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13472/x1.png",
                "caption": "Figure 1:Evaluation loss comparison showing that the proposed multilingual scaling law achieves consistently lower loss than the baseline across model sizes and token budgets.",
                "position": 148
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Scaling Laws for Code Pre-training",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13472/x2.png",
                "caption": "Figure 2:Scaling Laws for each PL independently. It shows a clear ordering of intrinsic predictability across PLs: C#<<Java≈\\approxRust<<Go<<TypeScript<<JavaScript<<Python.",
                "position": 215
            }
        ]
    },
    {
        "header": "3Language-specific Scaling Law",
        "images": []
    },
    {
        "header": "4Language Mixture for Data Scarcity",
        "images": []
    },
    {
        "header": "5Cross-Lingual Scaling Laws",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13472/x3.png",
                "caption": "Figure 3:Validation loss on unseen translation directions (non-Python language pairs). Each entry is the average loss across 30 translation pairs not seen during pre-training.",
                "position": 723
            },
            {
                "img": "https://arxiv.org/html/2512.13472/x4.png",
                "caption": "Figure 4:Translation scores across 3 strategies, 7 programming languages (PLs), and 42 directions. We aggregated the results by averaging based on language and direction into three categories: from each language to others, from others to a specific language, and between other languages, excluding Python. Across different model sizes, bothprompt-based concatenationanddirect concatenationsignificantly outperform the Baseline. Furthermore, we observe that scores for translations from other languages to Python are significantly lower than for other directions.",
                "position": 732
            }
        ]
    },
    {
        "header": "6Guideline for Code Pre-training",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13472/x5.png",
                "caption": "Figure 5:Token allocation comparison between baseline and optimized strategies. Both use 350B total code tokens but with different distributions. The optimized allocation is derived from fitted scaling laws (αN\\alpha_{N},αD\\alpha_{D},L∞L_{\\infty}), optimalNN:DDratios, and synergy gain analysis.",
                "position": 766
            },
            {
                "img": "https://arxiv.org/html/2512.13472/x6.png",
                "caption": "((a))Multilingual translation performance",
                "position": 821
            },
            {
                "img": "https://arxiv.org/html/2512.13472/x6.png",
                "caption": "((a))Multilingual translation performance",
                "position": 824
            },
            {
                "img": "https://arxiv.org/html/2512.13472/x7.png",
                "caption": "((b))Multilingual code generation performance",
                "position": 829
            }
        ]
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    }
]