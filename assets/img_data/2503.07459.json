[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07459/x1.png",
                "caption": "Figure 1:Performance analysis of large language models on medical tasks.OverallPass@1accuracy comparison across models in zero-shot setting.\nThe score is an average of seven test sets’ results (MedQA, PubMedQA, MedMCQA, MedBullets, MMLU, MMLU-Pro, MedExQA, and MedXpertQA).",
                "position": 109
            },
            {
                "img": "https://arxiv.org/html/2503.07459/x2.png",
                "caption": "Figure 2:Performance analysis of agents and models onMedAgentsBench.Cost-performance trade-off analysis showingPass@1accuracy versus cost per sample (in log scale), with marker sizes indicating inference time. Different markers represent various prompting methods\n, while colors distinguish different models. The Pareto frontier (red dashed line) indicates optimal cost-performance trade-offs.",
                "position": 249
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07459/x3.png",
                "caption": "Figure 3:Distribution of model performance across eight medical datasets (MedQA, MedMCQA, PubMedQA, MedBullets, MMLU-Pro, MMLU, MedExQA, and MedXpertQA.Each subplot shows the number of questions answered correctly by different proportions of models (x-axis: k/N, where k is the number of correct models and N is the total number of models). Questions are categorized as either hard (left of the dashed line,<<<50% of models correct) or easy (right of the dashed line,≥\\geq≥50% of models correct), with selected questions highlighted in darker shades. The total question count for each dataset is indicated in the subplot titles.",
                "position": 270
            }
        ]
    },
    {
        "header": "3MedAgentsBench",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07459/x4.png",
                "caption": "Figure 4:Data contamination analysis across medical question-answering datasets using MELD.The boxplots display similarity percentages between model-generated text and original question text, with higher values potentially indicating memorization of training data.\nLower similarity scores suggest minimal data contamination, while higher values may indicate potential contamination in model training data.",
                "position": 866
            },
            {
                "img": "https://arxiv.org/html/2503.07459/x5.png",
                "caption": "Figure 5:Cost-performance analysis across seven medical datasets, comparing open and closed-source language models.Each subplot shows Pass@1 accuracy (%) versus cost per sample (USD, log scale). Marker shapes distinguish thinking models from non-thinking models, while colors indicate open-source (blue) versus closed-source (red) models. Marker sizes represent inference time, and the red dashed line shows the Pareto frontier of optimal cost-performance trade-offs.",
                "position": 870
            }
        ]
    },
    {
        "header": "5Analysis",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetailed Description of Datasets",
        "images": []
    }
]