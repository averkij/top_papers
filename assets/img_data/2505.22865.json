[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22865/x1.png",
                "caption": "Figure 1:Overview of our BinauralFlow framework. (a) shows the causal U-Net architecture. Our causal U-Net takes as input the flowœït‚Å¢(ùê≥)subscriptitalic-œïùë°ùê≥\\phi_{t}(\\mathbf{z})italic_œï start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_z )as well as four conditionstùë°titalic_t,prxsubscriptùëùrxp_{\\mathrm{rx}}italic_p start_POSTSUBSCRIPT roman_rx end_POSTSUBSCRIPT,ptxsubscriptùëùtxp_{\\mathrm{tx}}italic_p start_POSTSUBSCRIPT roman_tx end_POSTSUBSCRIPT, andùê±ùê±\\mathbf{x}bold_x, and outputs a predicted vector field. The U-Net consists of several Causal 2D Conv Blocks in the contracting and expanding parts. (b) displays the Causal 2D Conv Block. We design fully causal convolution, down/up-sampling, and normalization layers to ensure temporal causality.",
                "position": 231
            },
            {
                "img": "https://arxiv.org/html/2505.22865/x2.png",
                "caption": "Figure 2:Continuous inference pipeline. Starting with a mono audio chunk (top left, black solid-line box), we compute its spectrogram via streaming STFT, add noise, and duplicate the channel to form the noisy spectrogramœï0‚Å¢(ùê≥)subscriptitalic-œï0ùê≥\\phi_{0}(\\mathbf{z})italic_œï start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( bold_z ). The trained model progressively removes the noise with a buffer bank. Finally, streaming ISTFT converts the predicted binaural spectrogramœï1‚Å¢(ùê≥)subscriptitalic-œï1ùê≥\\phi_{1}(\\mathbf{z})italic_œï start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( bold_z )into binaural audio. When the next audio chunk appears (black dashed-line box), we repeat the process and synthesize seamlessly continuous binaural speech.",
                "position": 363
            },
            {
                "img": "https://arxiv.org/html/2505.22865/x3.png",
                "caption": "Figure 3:The early skip time schedule. The use of an early skip strategy effectively reduces the inference steps and retains the generation performance.",
                "position": 411
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22865/x4.png",
                "caption": "Figure 4:Performance with respect to the NFE. We evaluate all generative models using the same NFE for a fair comparison.",
                "position": 517
            },
            {
                "img": "https://arxiv.org/html/2505.22865/x5.png",
                "caption": "Figure 5:Qualitative comparison between different baselines. We display waveforms of rendered spatial audio.",
                "position": 537
            },
            {
                "img": "https://arxiv.org/html/2505.22865/x6.png",
                "caption": "Figure 6:Output spectrograms using different inference pipelines.",
                "position": 706
            },
            {
                "img": "https://arxiv.org/html/2505.22865/x7.png",
                "caption": "Figure 7:Large-scale pretraining strategy. We propose pretraining our model using massive data to improve data efficiency and enhance generalization in downstream tasks.",
                "position": 762
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADemo Videos",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    },
    {
        "header": "Appendix CMidpoint Solver",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22865/extracted/6490858/fig/capture_setup.jpg",
                "caption": "(a)We collect data in a standard room without significant soundproofing or sound-absorbing materials.\nThe background noise from multiple air conditioning vents and electronic equipment is recorded.",
                "position": 1654
            },
            {
                "img": "https://arxiv.org/html/2505.22865/extracted/6490858/fig/capture_setup.jpg",
                "caption": "(a)We collect data in a standard room without significant soundproofing or sound-absorbing materials.\nThe background noise from multiple air conditioning vents and electronic equipment is recorded.",
                "position": 1657
            },
            {
                "img": "https://arxiv.org/html/2505.22865/extracted/6490858/fig/evaluation_setup.jpg",
                "caption": "(b)We perform the perceptual study in a quiet, acoustically treated room, with carefully calibrated playback levels and equalized headphones.",
                "position": 1663
            },
            {
                "img": "https://arxiv.org/html/2505.22865/extracted/6490858/fig/hearsay_2.jpg",
                "caption": "Figure 9:The large-scale binaural data capture system with artificial binaural heads.",
                "position": 1670
            }
        ]
    },
    {
        "header": "Appendix DData Collection Setup",
        "images": []
    },
    {
        "header": "Appendix EPerceptual Study Setup",
        "images": []
    },
    {
        "header": "Appendix FPre-training Dataset",
        "images": []
    },
    {
        "header": "Appendix GComparison with Other Flow Matching-Based Audio Models",
        "images": []
    },
    {
        "header": "Appendix HImpact of Different Numerical Solvers",
        "images": []
    },
    {
        "header": "Appendix ISway Sampling Schedule",
        "images": []
    },
    {
        "header": "Appendix JResults on a Public Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22865/x8.png",
                "caption": "Figure 10:Qualitative comparison between different baselines. We display waveforms of rendered spatial audio.",
                "position": 1923
            },
            {
                "img": "https://arxiv.org/html/2505.22865/x9.png",
                "caption": "Figure 11:Qualitative comparison between different baselines. We display waveforms of rendered spatial audio.",
                "position": 1926
            },
            {
                "img": "https://arxiv.org/html/2505.22865/x10.png",
                "caption": "Figure 12:Qualitative comparison between different baselines. We display waveforms of rendered spatial audio.",
                "position": 1929
            }
        ]
    },
    {
        "header": "Appendix KMore Qualitative Results",
        "images": []
    }
]