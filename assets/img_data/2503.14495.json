[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14495/x1.png",
                "caption": "Figure 1:Performance improvements for various models on process error identification benchmarks.",
                "position": 132
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14495/x2.png",
                "caption": "Figure 2:Overview of our Temporal Consistency approach, where each LLM iteratively examines its own verification results until reaching a stable result (stopping criteria defined in Section2). The self-checking mechanism allows LLMs to refine their judgments based on previous verifications, potentially correcting initial misidentification.",
                "position": 144
            },
            {
                "img": "https://arxiv.org/html/2503.14495/x3.png",
                "caption": "Figure 3:Cost v.s. Performance across different methods and models on ProcessBench. The x-axis (logarithmic scale) shows the cost per problem in dollars (based on OpenRouter pricing333https://openrouter.ai), while the y-axis shows the F1 Score percentage.",
                "position": 159
            }
        ]
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14495/x4.png",
                "caption": "Figure 4:Example of the self-checking process: The first error occurred in step 1. Initially, two LLMs incorrectly identified the first incorrect step, while one correctly located the first incorrect step.\nAfter self-checking, all LLMs achieve the correct identification.",
                "position": 178
            },
            {
                "img": "https://arxiv.org/html/2503.14495/x5.png",
                "caption": "Figure 5:Performance comparison across three datasets (Mathcheckâˆ—, ProcessBench, and PRM800K). Our Temporal Consistency approach (green) consistently outperforms baseline methods, including greedy decoding (yellow), majority voting (orange), and multi-model debate (red).",
                "position": 345
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14495/x6.png",
                "caption": "Figure 6:Performance comparison across different consistency requirements on ProcessBench for Deepseek-R1-Llama-8B. Higher consistency requirements, indicating stricter stability requirements, correlate with improved F1 scores.",
                "position": 597
            },
            {
                "img": "https://arxiv.org/html/2503.14495/x6.png",
                "caption": "Figure 6:Performance comparison across different consistency requirements on ProcessBench for Deepseek-R1-Llama-8B. Higher consistency requirements, indicating stricter stability requirements, correlate with improved F1 scores.",
                "position": 600
            },
            {
                "img": "https://arxiv.org/html/2503.14495/x7.png",
                "caption": "Figure 7:Cost-performance analysis of our method with different parameter configurations (max rounds and consistency requirement) on ProcessBench for Deepseek-R1-Llama-8B. The horizontal axis shows the cost per problem, while the vertical axis shows the average F1 score. As the computational budget increases, we observe improved performance, demonstrating the effectiveness of additional test-time scaling computation resources.",
                "position": 605
            },
            {
                "img": "https://arxiv.org/html/2503.14495/x8.png",
                "caption": "Figure 8:Performance comparison across problem difficulty levels. Problems are categorized as Easy (from GSM8K and MATH) or Hard (from OlympiadBench and Omni-MATH). Our method shows particular advantages on harder problems, maintaining more stable performance than baseline approaches.",
                "position": 625
            },
            {
                "img": "https://arxiv.org/html/2503.14495/x8.png",
                "caption": "Figure 8:Performance comparison across problem difficulty levels. Problems are categorized as Easy (from GSM8K and MATH) or Hard (from OlympiadBench and Omni-MATH). Our method shows particular advantages on harder problems, maintaining more stable performance than baseline approaches.",
                "position": 628
            },
            {
                "img": "https://arxiv.org/html/2503.14495/x9.png",
                "caption": "Figure 9:Ablation study results for ProcessBench demonstrating the effectiveness of both iterative generation and multi-agent components, with their combination yielding the best performance.",
                "position": 633
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BExamples for Various Methods",
        "images": []
    },
    {
        "header": "Appendix CBreak Down of Evaluation Results",
        "images": []
    }
]