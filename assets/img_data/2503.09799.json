[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09799/x1.png",
                "caption": "Figure 1:DiLoCo. Each DiLoCo model replica trains independently forHùêªHitalic_Hinner optimization steps. These models are synchronized via an outer optimization step, usually involving momentum across outer optimization steps. In this figure, there areM=4ùëÄ4M=4italic_M = 4replicas.",
                "position": 292
            }
        ]
    },
    {
        "header": "3Experimental Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09799/x2.png",
                "caption": "(a)Evaluation loss for various algorithms, as a function ofNùëÅNitalic_N.",
                "position": 530
            },
            {
                "img": "https://arxiv.org/html/2503.09799/x2.png",
                "caption": "(a)Evaluation loss for various algorithms, as a function ofNùëÅNitalic_N.",
                "position": 533
            },
            {
                "img": "https://arxiv.org/html/2503.09799/x3.png",
                "caption": "(b)Percentage difference in evaluation loss, relative to Data-Parallel.",
                "position": 538
            }
        ]
    },
    {
        "header": "4Empirical Findings",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09799/x4.png",
                "caption": "(a)Evaluation loss.",
                "position": 747
            },
            {
                "img": "https://arxiv.org/html/2503.09799/x4.png",
                "caption": "(a)Evaluation loss.",
                "position": 750
            },
            {
                "img": "https://arxiv.org/html/2503.09799/x5.png",
                "caption": "(b)Zero-shot accuracy on HellaSwag.",
                "position": 755
            },
            {
                "img": "https://arxiv.org/html/2503.09799/x6.png",
                "caption": "Figure 4:DiLoCo increases optimal batch size, part 1.Evaluation loss of Data-Parallel and DiLoCo as a function of global batch size (in tokens). For allMùëÄMitalic_M, DiLoCo exhibits larger optimal batch size than Data-Parallel. Moreover, the optimal batch size increases as a function ofMùëÄMitalic_M. We see similar results for other model sizes, but omit for conciseness.",
                "position": 772
            },
            {
                "img": "https://arxiv.org/html/2503.09799/x7.png",
                "caption": "Figure 5:DiLoCo increases optimal batch size, part 2.Zero-shot accuracy on HellaSwag of Data-Parallel and DiLoCo as a function of global batch size (in tokens). Even at smaller model sizes, DiLoCo withM=2ùëÄ2M=2italic_M = 2attains higher accuracy for larger global batch sizes. We see similar results for other model sizes, but omit for conciseness.",
                "position": 775
            },
            {
                "img": "https://arxiv.org/html/2503.09799/x8.png",
                "caption": "(a)Network with a bandwidth of 10 gigabits/s and a latency of10‚àí2superscript10210^{-2}10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPTseconds (low-bandwidth).",
                "position": 781
            },
            {
                "img": "https://arxiv.org/html/2503.09799/x8.png",
                "caption": "(a)Network with a bandwidth of 10 gigabits/s and a latency of10‚àí2superscript10210^{-2}10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPTseconds (low-bandwidth).",
                "position": 784
            },
            {
                "img": "https://arxiv.org/html/2503.09799/x9.png",
                "caption": "(b)Network with a bandwidth of 100 gigabits/s and a latency of10‚àí3superscript10310^{-3}10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPTseconds (medium-bandwidth).",
                "position": 789
            },
            {
                "img": "https://arxiv.org/html/2503.09799/x10.png",
                "caption": "(c)Network with a bandwidth of 400 gigabits/s and a latency of10‚àí4superscript10410^{-4}10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPTseconds (high-bandwidth).",
                "position": 794
            },
            {
                "img": "https://arxiv.org/html/2503.09799/x11.png",
                "caption": "Figure 7:Optimal outer learning rate is independent of model size.We present the best outer learning rate for DiLoCo for varyingMùëÄMitalic_Mand model sizesNùëÅNitalic_N. We select the best outer learning rate over{0.2,0.4,0.6,0.8,1.0}0.20.40.60.81.0\\{0.2,0.4,0.6,0.8,1.0\\}{ 0.2 , 0.4 , 0.6 , 0.8 , 1.0 }, optimizing over inner learning rateŒ≥ùõæ\\gammaitalic_Œ≥and global batch sizeBùêµBitalic_B. For sufficiently large models, the best outer learning rate is clearly constant.",
                "position": 811
            }
        ]
    },
    {
        "header": "5Ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09799/x12.png",
                "caption": "(a)Optimal outer learning rate for each synchronization cadence. Shaded regions represent the variance across model sizes.",
                "position": 821
            },
            {
                "img": "https://arxiv.org/html/2503.09799/x12.png",
                "caption": "(a)Optimal outer learning rate for each synchronization cadence. Shaded regions represent the variance across model sizes.",
                "position": 824
            },
            {
                "img": "https://arxiv.org/html/2503.09799/x13.png",
                "caption": "(b)Optimal outer learning rate for each model size. Shaded regions represent the variance across synchronization cadences.",
                "position": 829
            },
            {
                "img": "https://arxiv.org/html/2503.09799/x14.png",
                "caption": "Figure 9:Infrequent synchronization works better for larger models.Outside ofH=1ùêª1H=1italic_H = 1, which performs the worst, evaluation loss increases withHùêªHitalic_H. However, the rate of increase is less pronounced for DiLoCo withM=1ùëÄ1M=1italic_M = 1and for larger models, suggesting that large models can be synchronized quite infrequently.",
                "position": 836
            },
            {
                "img": "https://arxiv.org/html/2503.09799/extracted/6275542/figures/bw_chinchilla_10b.png",
                "caption": "(a)10B Chinchilla",
                "position": 855
            },
            {
                "img": "https://arxiv.org/html/2503.09799/extracted/6275542/figures/bw_chinchilla_10b.png",
                "caption": "(a)10B Chinchilla",
                "position": 858
            },
            {
                "img": "https://arxiv.org/html/2503.09799/extracted/6275542/figures/bw_llama_405b.png",
                "caption": "(b)405B Llama3",
                "position": 863
            },
            {
                "img": "https://arxiv.org/html/2503.09799/extracted/6275542/figures/bw_deepseek_671b.png",
                "caption": "(c)671B DeepSeek-V3",
                "position": 868
            },
            {
                "img": "https://arxiv.org/html/2503.09799/x15.png",
                "caption": "Figure 11:DiLoCo scales reliably with overtraining.For each overtrain multiplier, the curves of loss with respect to FLOPs are all essentially parallel lines in log-space. We see that variations in loss from changing the algorithm is dominated by changing the model size or amount of overtraining. Moreover, DiLoCo withM=1ùëÄ1M=1italic_M = 1is slightly better than all other algorithms, including Data-Parallel, at all overtraining amounts and model sizes.",
                "position": 1098
            },
            {
                "img": "https://arxiv.org/html/2503.09799/x16.png",
                "caption": "(a)Network with a bandwidth of 10 gigabits/s and a latency of10‚àí2superscript10210^{-2}10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPTseconds (low-bandwidth).",
                "position": 1110
            },
            {
                "img": "https://arxiv.org/html/2503.09799/x16.png",
                "caption": "(a)Network with a bandwidth of 10 gigabits/s and a latency of10‚àí2superscript10210^{-2}10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPTseconds (low-bandwidth).",
                "position": 1113
            },
            {
                "img": "https://arxiv.org/html/2503.09799/x17.png",
                "caption": "(b)Network with a bandwidth of 100 gigabits/s and a latency of10‚àí3superscript10310^{-3}10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPTseconds (medium-bandwidth).",
                "position": 1118
            },
            {
                "img": "https://arxiv.org/html/2503.09799/x18.png",
                "caption": "(c)Network with a bandwidth of 400 gigabits/s and a latency of10‚àí4superscript10410^{-4}10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPTseconds (high-bandwidth).",
                "position": 1123
            }
        ]
    },
    {
        "header": "6Scaling Laws",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09799/x19.png",
                "caption": "Figure 13:DiLoCo scaling laws extrapolate well to larger models.We present loss scaling laws for Data-Parallel and DiLoCo. Pictured are both the loss values to form the scaling law (by training models up to a scale of 2.4B) and loss values attained on larger models (4B and 10B). While we present the individual-fit scaling laws for simplicity, the joint fit also predicts loss well.",
                "position": 1513
            }
        ]
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Conclusions and Future Work",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AWall-Clock Time Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09799/x20.png",
                "caption": "Figure 14:Evaluation loss of Data-Parallel and DiLoCo as a function of global batch size (in tokens).",
                "position": 2625
            },
            {
                "img": "https://arxiv.org/html/2503.09799/x21.png",
                "caption": "Figure 15:Zero-shot evaluation accuracy on HellaSwag of Data-Parallel and DiLoCo as a function of global batch size (in tokens).",
                "position": 2628
            },
            {
                "img": "https://arxiv.org/html/2503.09799/x22.png",
                "caption": "Figure 16:Zero-shot evaluation accuracy on Piqa of Data-Parallel and DiLoCo as a function of global batch size (in tokens).",
                "position": 2631
            },
            {
                "img": "https://arxiv.org/html/2503.09799/x23.png",
                "caption": "Figure 17:Zero-shot evaluation accuracy on Arc-Easy of Data-Parallel and DiLoCo as a function of global batch size (in tokens).",
                "position": 2634
            },
            {
                "img": "https://arxiv.org/html/2503.09799/x24.png",
                "caption": "(a)Evaluation loss.",
                "position": 2640
            },
            {
                "img": "https://arxiv.org/html/2503.09799/x24.png",
                "caption": "(a)Evaluation loss.",
                "position": 2643
            },
            {
                "img": "https://arxiv.org/html/2503.09799/x25.png",
                "caption": "(b)Zero-shot accuracy on HellaSwag.",
                "position": 2648
            },
            {
                "img": "https://arxiv.org/html/2503.09799/x26.png",
                "caption": "(a)Zero-shot accuracy on Arc-Easy.",
                "position": 2655
            },
            {
                "img": "https://arxiv.org/html/2503.09799/x26.png",
                "caption": "(a)Zero-shot accuracy on Arc-Easy.",
                "position": 2658
            },
            {
                "img": "https://arxiv.org/html/2503.09799/x27.png",
                "caption": "(b)Zero-shot accuracy on Arc-Easy.",
                "position": 2663
            }
        ]
    },
    {
        "header": "Appendix BAdditional Experimental Results",
        "images": []
    }
]