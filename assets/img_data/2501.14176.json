[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.14176/extracted/6152569/img/improvement/1.png",
                "caption": "(a)Early inference",
                "position": 86
            },
            {
                "img": "https://arxiv.org/html/2501.14176/extracted/6152569/img/improvement/1.png",
                "caption": "(a)Early inference",
                "position": 89
            },
            {
                "img": "https://arxiv.org/html/2501.14176/extracted/6152569/img/improvement/2.png",
                "caption": "(b)Mid inference",
                "position": 94
            },
            {
                "img": "https://arxiv.org/html/2501.14176/extracted/6152569/img/improvement/3.png",
                "caption": "(c)Late inference",
                "position": 99
            }
        ]
    },
    {
        "header": "2Background and Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.14176/extracted/6152569/img/Transformer.png",
                "caption": "Figure 2:Fine-tuning LLaMA 3.1 8B Instruct with IA3 Adapters and a reinforcement learning objective. The model is fed sequences of states, actions, and (if nonzero) rewards, with every episode prefixed by the<|begin_of_text|>(BOT) token and terminated by the<|end_of_text|>(EOT) token. Tokens like<|start_header_id|>(SHI),<|end_header_id|>(EHI), and<|eot_id|>(EID) separate thestate,action, andreward, mirroring how instruct models delineateuserandassistantroles. The model predicts the Q-value of the current state for every action, updating the Q-values during training using the Bellman backup equation.",
                "position": 182
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.14176/extracted/6152569/img/chaining/1.png",
                "caption": "(a)Example 1",
                "position": 422
            },
            {
                "img": "https://arxiv.org/html/2501.14176/extracted/6152569/img/chaining/1.png",
                "caption": "(a)Example 1",
                "position": 425
            },
            {
                "img": "https://arxiv.org/html/2501.14176/extracted/6152569/img/chaining/2.png",
                "caption": "(b)Example 2",
                "position": 430
            },
            {
                "img": "https://arxiv.org/html/2501.14176/extracted/6152569/img/chaining/3.png",
                "caption": "(c)Trial 1",
                "position": 435
            },
            {
                "img": "https://arxiv.org/html/2501.14176/extracted/6152569/img/chaining/4.png",
                "caption": "(d)Trial 2",
                "position": 440
            },
            {
                "img": "https://arxiv.org/html/2501.14176/extracted/6152569/img/chaining/5.png",
                "caption": "(e)Trial 3",
                "position": 445
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Ethical Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]