[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.23913/x1.png",
                "caption": "Figure 1:Comparison between the traditional self-training pipeline and EAST. The LLM generatesn𝑛nitalic_nresponses per question, clustered by final answers. Questions with all incorrect answers are discarded. Self-training fine-tunes uniformly on the rest, while EAST assigns higher weights to questions with diverse (uncertain) answers and lower weights to consistent (confident) ones.",
                "position": 108
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.23913/x2.png",
                "caption": "Figure 2:The framework of EAST. For each training question, the LLM generatesn𝑛nitalic_nresponses, clustered by final answers. Entropy value is computed from the cluster distribution, transformed via mapping function, and integrated as weight into the loss objective.",
                "position": 217
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.23913/x3.png",
                "caption": "Table 1:Experimental results in terms of accuracy(%) on GSM8K and MATH benchmarks. The best performance under each loss category is highlighted inbold. Significant boosts (≥\\geq≥1%) of EAST over both the vanilla method and baselines areunderlined.",
                "position": 400
            },
            {
                "img": "https://arxiv.org/html/2503.23913/x3.png",
                "caption": "Figure 3:Performance(accuracy (%)) of various exponent parametersa𝑎aitalic_aon GSM8K and MATH datasets using LLaMA-3.2-1B.",
                "position": 450
            },
            {
                "img": "https://arxiv.org/html/2503.23913/x4.png",
                "caption": "Figure 4:Comparison of iterative learning performance (accuracy (%)) between vanilla SFT and EAST on LLaMA-3.2-1B.",
                "position": 464
            },
            {
                "img": "https://arxiv.org/html/2503.23913/x5.png",
                "caption": "Figure 5:The figure illustrates the distribution of training data in entropy-based, accuracy-based, and rejected-based values. Each point represents a training example (xisubscript𝑥𝑖x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT), with coordinates (H⁢(xi),1−A⁢(xi)𝐻subscript𝑥𝑖1𝐴subscript𝑥𝑖H(x_{i}),1-A(x_{i})italic_H ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) , 1 - italic_A ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )) for entropy-based and accuracy-based values, and color indicating the rejected-based value (R⁢(xi)𝑅subscript𝑥𝑖R(x_{i})italic_R ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )). The accompanying table reports the performance (accuracy(%)) of three weighting strategies on the GSM8K and MATH datasets.",
                "position": 483
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AReproducibility",
        "images": []
    },
    {
        "header": "Appendix BExperiment Setup",
        "images": []
    }
]