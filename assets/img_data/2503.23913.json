[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.23913/x1.png",
                "caption": "Figure 1:Comparison between the traditional self-training pipeline and EAST. The LLM generatesnğ‘›nitalic_nresponses per question, clustered by final answers. Questions with all incorrect answers are discarded. Self-training fine-tunes uniformly on the rest, while EAST assigns higher weights to questions with diverse (uncertain) answers and lower weights to consistent (confident) ones.",
                "position": 108
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.23913/x2.png",
                "caption": "Figure 2:The framework of EAST. For each training question, the LLM generatesnğ‘›nitalic_nresponses, clustered by final answers. Entropy value is computed from the cluster distribution, transformed via mapping function, and integrated as weight into the loss objective.",
                "position": 217
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.23913/x3.png",
                "caption": "Table 1:Experimental results in terms of accuracy(%) on GSM8K and MATH benchmarks. The best performance under each loss category is highlighted inbold. Significant boosts (â‰¥\\geqâ‰¥1%) of EAST over both the vanilla method and baselines areunderlined.",
                "position": 400
            },
            {
                "img": "https://arxiv.org/html/2503.23913/x3.png",
                "caption": "Figure 3:Performance(accuracy (%)) of various exponent parametersağ‘aitalic_aon GSM8K and MATH datasets using LLaMA-3.2-1B.",
                "position": 450
            },
            {
                "img": "https://arxiv.org/html/2503.23913/x4.png",
                "caption": "Figure 4:Comparison of iterative learning performance (accuracy (%)) between vanilla SFT and EAST on LLaMA-3.2-1B.",
                "position": 464
            },
            {
                "img": "https://arxiv.org/html/2503.23913/x5.png",
                "caption": "Figure 5:The figure illustrates the distribution of training data in entropy-based, accuracy-based, and rejected-based values. Each point represents a training example (xisubscriptğ‘¥ğ‘–x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT), with coordinates (Hâ¢(xi),1âˆ’Aâ¢(xi)ğ»subscriptğ‘¥ğ‘–1ğ´subscriptğ‘¥ğ‘–H(x_{i}),1-A(x_{i})italic_H ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) , 1 - italic_A ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )) for entropy-based and accuracy-based values, and color indicating the rejected-based value (Râ¢(xi)ğ‘…subscriptğ‘¥ğ‘–R(x_{i})italic_R ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )). The accompanying table reports the performance (accuracy(%)) of three weighting strategies on the GSM8K and MATH datasets.",
                "position": 483
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AReproducibility",
        "images": []
    },
    {
        "header": "Appendix BExperiment Setup",
        "images": []
    }
]