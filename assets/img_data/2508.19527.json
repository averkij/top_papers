[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.19527/x1.png",
                "caption": "",
                "position": 62
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.19527/x2.png",
                "caption": "Figure 2:Overview of MotionFlux. In the first stage, we begin by utilizing a pre-trained VAE (with frozen parameters) to compress the raw motion sequenceX1:NframeX_{1:N_{\\text{frame}}}into the latent space. The compressed representation, along with the text embedding and timestep, is then fed into the vector estimator to obtain the vector field predictionvv. In the second stage, we freeze the model parameters trained in the first stage and use them as a reference model. We then select random texts from the initial training set to generate an online dataset. The optimization objective in the second stage is to iteratively generate high-quality outputs that align with the expected targets by comparing the predictions of the main model and the reference model.",
                "position": 74
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.19527/x3.png",
                "caption": "Figure 3:Overview of the sampling pipeline employed in our rectified-flow–based text-to-motion framework",
                "position": 196
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.19527/x4.png",
                "caption": "Figure 4:Qualitative comparison of the state-of-the-art methods in the text-to-motiontask,the darker the color, the later the time. We employed ChatGPT-o3 to randomly generate three prompts—none of which had appeared in the dataset—for inference. The visualization results show that MotionFlux exhibits strong semantic alignment on critical events (e.g., “left and right,” “glance”) and demonstrates robust generalization performance.",
                "position": 626
            },
            {
                "img": "https://arxiv.org/html/2508.19527/x5.png",
                "caption": "Figure 5:Trajectory of FID and TMR++ scores over training iterations. Offline training peaks by the second iteration with rising FID, while online training continues to improve, showing lower FID and higher TMR++ scores.",
                "position": 645
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]