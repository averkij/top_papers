[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01744/extracted/5897795/images/leopard.png",
                "caption": "",
                "position": 56
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01744/x1.png",
                "caption": "Figure 1:Left: A demonstration of a text-rich multi-image task. Models need to reason about the textual content across multiple images to answer the question correctly.Leopardsuccessfully generates the right answer while baselines fail. Right: Evaluation results ofLeopardand three baselines. Our model surpasses its counterparts across text-rich multi-image benchmarks by a large margin, maintaining comparable performance on single and general evaluations.",
                "position": 88
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01744/x2.png",
                "caption": "Figure 2:The overall model pipeline. Given ‚ë† raw image inputs, ‚ë° we first compute the optimal allocation of sub-image numbers and splitting strategy for all images based on their resolution and aspect ratio. ‚ë¢ The images undergo padding, resizing, and splitting operations. ‚ë£ Both sub-images and resized original images are then encoded into a sequence of visual features. These sequences subsequently undergo a pixel shuffle operation that concatenates every four features. ‚ë§ The visual features are projected into the language embedding space via a vision-language connector. Finally, the large language model then integrates these visual and language embeddings to generate responses.",
                "position": 140
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01744/x3.png",
                "caption": "Figure 3:Impact of the sub-image budgetMùëÄMitalic_Mon the resulting model across four benchmarks.w/oindicates no partitioning into sub-images.",
                "position": 992
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01744/x4.png",
                "caption": "Figure 4:An illustration of the proportion of sub-datasets and domains in the proposed dataset.",
                "position": 2329
            },
            {
                "img": "https://arxiv.org/html/2410.01744/x5.png",
                "caption": "Figure 5:The prompt used for generating Q-A pairs with rationales for slide decks data.",
                "position": 2340
            },
            {
                "img": "https://arxiv.org/html/2410.01744/x6.png",
                "caption": "Figure 6:The prompt used for generating Q-A pairs with rationales for webpage data.",
                "position": 2343
            },
            {
                "img": "https://arxiv.org/html/2410.01744/x7.png",
                "caption": "Figure 7:We use this prompt for the generation of chain-of-thought rationales given original question, answer, and images.",
                "position": 2346
            },
            {
                "img": "https://arxiv.org/html/2410.01744/x8.png",
                "caption": "Figure 8:An example of multi-table reasoning ofLeopard.",
                "position": 2363
            },
            {
                "img": "https://arxiv.org/html/2410.01744/x9.png",
                "caption": "Figure 9:An example of multi-page document question answering ofLeopard.",
                "position": 2366
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]