[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.03978/x1.png",
                "caption": "Figure 1:(A) Distribution of BIOMEDICA-6M caption token usage with a cutoff of 77 tokens. The blue histogram represents tokens visible to the model, while the pink histogram represents wasted tokens truncated beyond the cutoff (corresponding to 434 million tokens or 55% of total tokens ). (B) Distribution with a cutoff of 512 tokens, showing substantially reduced token waste of 2.2% (17M tokens).\n(C) Qualitative examples of BIOMEDICA-6M and BIOMEDICA-LongCAP captions, showing truncated vs. full captions, as well as our enhanced captions.",
                "position": 107
            },
            {
                "img": "https://arxiv.org/html/2510.03978/images/performance_summary.png",
                "caption": "Figure 2:Context-length ablation results of BMC-LongCLIP trained with 77, 154, and 512 tokens.(Left) Average retrieval performance (Recall@K) on the PMC long-caption benchmark. (Middle) Average retrieval performance on the CXR benchmark. (Right) Average zero-shot classification accuracy across biomedical datasets. Longer context improves retrieval and classification, with the largest gains on PMC.",
                "position": 118
            }
        ]
    },
    {
        "header": "2Methods",
        "images": []
    },
    {
        "header": "3Experiments",
        "images": []
    },
    {
        "header": "4Results",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ABIOMEDICA-LongCAP details",
        "images": []
    },
    {
        "header": "Appendix BCXR Benchmark",
        "images": []
    },
    {
        "header": "Appendix CPMC Benchmark",
        "images": []
    },
    {
        "header": "Appendix DZero-shot classification benchmark",
        "images": []
    },
    {
        "header": "Appendix ECompute details",
        "images": []
    },
    {
        "header": "Appendix FTraining parameters",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.03978/images/training_loss_converge.png",
                "caption": "Figure 3:Training loss curves across context lengths, illustrating that longer text windows accelerate convergence.",
                "position": 1069
            }
        ]
    },
    {
        "header": "Appendix GTraining loss curves by training context length",
        "images": []
    }
]