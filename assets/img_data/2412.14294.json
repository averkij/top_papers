[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14294/x1.png",
                "caption": "",
                "position": 116
            },
            {
                "img": "https://arxiv.org/html/2412.14294/x2.png",
                "caption": "",
                "position": 119
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3TRecViTÂ Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14294/extracted/6080313/img/eigs.png",
                "caption": "Figure 2:Distribution of the eigenvalues of the recurrent matrix at the beginning and end of training on long video memorisation task (see subsection5.3) for different initialisation ranges.",
                "position": 253
            },
            {
                "img": "https://arxiv.org/html/2412.14294/extracted/6080313/img/memory.png",
                "caption": "(a)Memory comparison",
                "position": 259
            },
            {
                "img": "https://arxiv.org/html/2412.14294/extracted/6080313/img/memory.png",
                "caption": "(a)Memory comparison",
                "position": 262
            },
            {
                "img": "https://arxiv.org/html/2412.14294/extracted/6080313/img/flops.png",
                "caption": "(b)FLOPs comparison",
                "position": 267
            }
        ]
    },
    {
        "header": "4Training TRecViT",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14294/extracted/6080313/img/scratch.png",
                "caption": "Figure 4:TRecViTÂ compared to baselines on supervised video classification on SSv2 dataset, trained from scratch. The plot shows the evolution of the evaluation accuracy as training progresses.",
                "position": 338
            },
            {
                "img": "https://arxiv.org/html/2412.14294/extracted/6080313/img/davis.png",
                "caption": "Figure 5:Qualitative results obtained by TRecViTÂ for point tracking on DAVIS dataset compared to VideoMAE. The leftmost image indicates the point to track in the original frame, and the images towards the right show zoom-ins on subsequent frames. Green plus (+) marker indicates the ground truth, yellow circle indicates TRecViTâ€™s predictions and red circles indicate VideoMAEâ€™s predictions.",
                "position": 342
            },
            {
                "img": "https://arxiv.org/html/2412.14294/extracted/6080313/img/wtlong.png",
                "caption": "Figure 6:Qualitative results obtained by TRecViTÂ on the dense memorisation task compared to ViViT-L. Both models are trained using Imagenet pre-trained weights, on video sequences ofT=64ð‘‡64T=64italic_T = 64frames and they reconstruct the(Tâˆ’48)thsuperscriptð‘‡48th(T-48)^{\\text{th}}( italic_T - 48 ) start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPTframe.",
                "position": 517
            },
            {
                "img": "https://arxiv.org/html/2412.14294/extracted/6080313/img/psnr.png",
                "caption": "(a)PSNR comparison",
                "position": 520
            },
            {
                "img": "https://arxiv.org/html/2412.14294/extracted/6080313/img/psnr.png",
                "caption": "(a)PSNR comparison",
                "position": 523
            },
            {
                "img": "https://arxiv.org/html/2412.14294/extracted/6080313/img/sps.png",
                "caption": "(b)Steps-per-second comparison",
                "position": 529
            },
            {
                "img": "https://arxiv.org/html/2412.14294/extracted/6080313/img/genlong.png",
                "caption": "Figure 8:Generalisation to longer sequences. Both models are trained using Imagenet pre-trained weights, on video sequences ofT=64ð‘‡64T=64italic_T = 64frames to reconstruct the(Tâˆ’48)thsuperscriptð‘‡48th(T-48)^{\\text{th}}( italic_T - 48 ) start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPTframe; during evaluation, the models receive sequences ofT=96ð‘‡96T=96italic_T = 96frames.",
                "position": 552
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Training hyperparameters",
        "images": []
    },
    {
        "header": "8Point tracking qualitative results",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14294/extracted/6080313/img/leg.png",
                "caption": "Figure 9:Qualitative results obtained by TRecViTÂ for point tracking on DAVIS dataset (rows 1-2) and Perception Test (rows 3-4) compared to VideoMAE. The leftmost image indicates the point to track in the original frame, and the images towards the right show zoom-ins on subsequent frames. Green plus (+) marker indicates the ground truth, yellow circle indicates TRecViTâ€™s predictions and red circles indicate VideoMAEâ€™s predictions.",
                "position": 1455
            },
            {
                "img": "https://arxiv.org/html/2412.14294/extracted/6080313/img/goat.png",
                "caption": "",
                "position": 1459
            },
            {
                "img": "https://arxiv.org/html/2412.14294/extracted/6080313/img/ptbike.png",
                "caption": "",
                "position": 1461
            },
            {
                "img": "https://arxiv.org/html/2412.14294/extracted/6080313/img/ptcard.png",
                "caption": "",
                "position": 1463
            },
            {
                "img": "https://arxiv.org/html/2412.14294/extracted/6080313/img/longtaskmany.png",
                "caption": "Figure 10:Qualitative results for the task of reconstructing a frame from the past, for increasing distancekð‘˜kitalic_kto the frame to reconstruct from left to right.First row: last frame seen by the model.Second row: TRecViTÂ output.Third row: ViViT-L output; ViViT-L goes OOM fork>80ð‘˜80k>80italic_k > 80, so no predictions are shown.",
                "position": 1474
            }
        ]
    },
    {
        "header": "9Long video memorisation task",
        "images": []
    }
]