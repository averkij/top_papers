[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14294/x1.png",
                "caption": "",
                "position": 116
            },
            {
                "img": "https://arxiv.org/html/2412.14294/x2.png",
                "caption": "",
                "position": 119
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3TRecViT Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14294/extracted/6080313/img/eigs.png",
                "caption": "Figure 2:Distribution of the eigenvalues of the recurrent matrix at the beginning and end of training on long video memorisation task (see subsection5.3) for different initialisation ranges.",
                "position": 253
            },
            {
                "img": "https://arxiv.org/html/2412.14294/extracted/6080313/img/memory.png",
                "caption": "(a)Memory comparison",
                "position": 259
            },
            {
                "img": "https://arxiv.org/html/2412.14294/extracted/6080313/img/memory.png",
                "caption": "(a)Memory comparison",
                "position": 262
            },
            {
                "img": "https://arxiv.org/html/2412.14294/extracted/6080313/img/flops.png",
                "caption": "(b)FLOPs comparison",
                "position": 267
            }
        ]
    },
    {
        "header": "4Training TRecViT",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14294/extracted/6080313/img/scratch.png",
                "caption": "Figure 4:TRecViT compared to baselines on supervised video classification on SSv2 dataset, trained from scratch. The plot shows the evolution of the evaluation accuracy as training progresses.",
                "position": 338
            },
            {
                "img": "https://arxiv.org/html/2412.14294/extracted/6080313/img/davis.png",
                "caption": "Figure 5:Qualitative results obtained by TRecViT for point tracking on DAVIS dataset compared to VideoMAE. The leftmost image indicates the point to track in the original frame, and the images towards the right show zoom-ins on subsequent frames. Green plus (+) marker indicates the ground truth, yellow circle indicates TRecViT’s predictions and red circles indicate VideoMAE’s predictions.",
                "position": 342
            },
            {
                "img": "https://arxiv.org/html/2412.14294/extracted/6080313/img/wtlong.png",
                "caption": "Figure 6:Qualitative results obtained by TRecViT on the dense memorisation task compared to ViViT-L. Both models are trained using Imagenet pre-trained weights, on video sequences ofT=64𝑇64T=64italic_T = 64frames and they reconstruct the(T−48)thsuperscript𝑇48th(T-48)^{\\text{th}}( italic_T - 48 ) start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPTframe.",
                "position": 517
            },
            {
                "img": "https://arxiv.org/html/2412.14294/extracted/6080313/img/psnr.png",
                "caption": "(a)PSNR comparison",
                "position": 520
            },
            {
                "img": "https://arxiv.org/html/2412.14294/extracted/6080313/img/psnr.png",
                "caption": "(a)PSNR comparison",
                "position": 523
            },
            {
                "img": "https://arxiv.org/html/2412.14294/extracted/6080313/img/sps.png",
                "caption": "(b)Steps-per-second comparison",
                "position": 529
            },
            {
                "img": "https://arxiv.org/html/2412.14294/extracted/6080313/img/genlong.png",
                "caption": "Figure 8:Generalisation to longer sequences. Both models are trained using Imagenet pre-trained weights, on video sequences ofT=64𝑇64T=64italic_T = 64frames to reconstruct the(T−48)thsuperscript𝑇48th(T-48)^{\\text{th}}( italic_T - 48 ) start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPTframe; during evaluation, the models receive sequences ofT=96𝑇96T=96italic_T = 96frames.",
                "position": 552
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Training hyperparameters",
        "images": []
    },
    {
        "header": "8Point tracking qualitative results",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14294/extracted/6080313/img/leg.png",
                "caption": "Figure 9:Qualitative results obtained by TRecViT for point tracking on DAVIS dataset (rows 1-2) and Perception Test (rows 3-4) compared to VideoMAE. The leftmost image indicates the point to track in the original frame, and the images towards the right show zoom-ins on subsequent frames. Green plus (+) marker indicates the ground truth, yellow circle indicates TRecViT’s predictions and red circles indicate VideoMAE’s predictions.",
                "position": 1455
            },
            {
                "img": "https://arxiv.org/html/2412.14294/extracted/6080313/img/goat.png",
                "caption": "",
                "position": 1459
            },
            {
                "img": "https://arxiv.org/html/2412.14294/extracted/6080313/img/ptbike.png",
                "caption": "",
                "position": 1461
            },
            {
                "img": "https://arxiv.org/html/2412.14294/extracted/6080313/img/ptcard.png",
                "caption": "",
                "position": 1463
            },
            {
                "img": "https://arxiv.org/html/2412.14294/extracted/6080313/img/longtaskmany.png",
                "caption": "Figure 10:Qualitative results for the task of reconstructing a frame from the past, for increasing distancek𝑘kitalic_kto the frame to reconstruct from left to right.First row: last frame seen by the model.Second row: TRecViT output.Third row: ViViT-L output; ViViT-L goes OOM fork>80𝑘80k>80italic_k > 80, so no predictions are shown.",
                "position": 1474
            }
        ]
    },
    {
        "header": "9Long video memorisation task",
        "images": []
    }
]