[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14762/x1.png",
                "caption": "(a)Maximum batch-size when training video tokenizerson128×128128128128\\times 128128 × 128resolution videos with varying lengths, measured with a single NVIDIA 4090 24GB GPU.",
                "position": 151
            },
            {
                "img": "https://arxiv.org/html/2411.14762/x1.png",
                "caption": "(a)Maximum batch-size when training video tokenizerson128×128128128128\\times 128128 × 128resolution videos with varying lengths, measured with a single NVIDIA 4090 24GB GPU.",
                "position": 154
            },
            {
                "img": "https://arxiv.org/html/2411.14762/x2.png",
                "caption": "(b)Inter-clip reconstruction consistency of video tokenizers.Existing video tokenizers[9,64,50]show the pixel-value inconsistency between short clips (16 frames). In contrast, Our tokenizer shows the temporally consistent reconstruction.",
                "position": 159
            },
            {
                "img": "https://arxiv.org/html/2411.14762/x3.png",
                "caption": "Figure 2:Overview of CoordTok.We design our encoder to encode a video𝐱𝐱{\\mathbf{x}}bold_xinto factorized triplane representations𝐳=[𝐳x⁢y,𝐳y⁢t,𝐳x⁢t]𝐳superscript𝐳𝑥𝑦superscript𝐳𝑦𝑡superscript𝐳𝑥𝑡{\\mathbf{z}}=[{\\mathbf{z}}^{xy},{\\mathbf{z}}^{yt},{\\mathbf{z}}^{xt}]bold_z = [ bold_z start_POSTSUPERSCRIPT italic_x italic_y end_POSTSUPERSCRIPT , bold_z start_POSTSUPERSCRIPT italic_y italic_t end_POSTSUPERSCRIPT , bold_z start_POSTSUPERSCRIPT italic_x italic_t end_POSTSUPERSCRIPT ]which can efficiently represent the video with three 2D latent planes.\nGiven the triplane representations𝐳𝐳\\mathbf{z}bold_z, our decoder learns a mapping from(x,y,t)𝑥𝑦𝑡(x,y,t)( italic_x , italic_y , italic_t )coordinates to RGB pixels within the corresponding patches.\nIn particular, we extract coordinate-based representations ofN𝑁Nitalic_Nsampled coordinates by querying the coordinates from triplane representations via bilinear interpolation.\nThen the decoder aggregates and fuses information from different coordinates with self-attention layers and project outputs into corresponding patches.\nThis design enables us to train tokenizers on long videos in a compute-efficient manner by avoiding reconstruction of entire frames at once.",
                "position": 208
            }
        ]
    },
    {
        "header": "2Method",
        "images": []
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14762/x4.png",
                "caption": "Figure 3:128-frame, 128×\\times×128 resolution video reconstruction resultsfrom CoordTok (Ours) and baselines[64,50]trained on the UCF-101 dataset[40].\nFor each frame, we visualize the ground-truth (GT) and reconstructed pixels within the region highlighted in the red box, where CoordTok achieves noticeably better reconstruction quality than other baselines.",
                "position": 311
            },
            {
                "img": "https://arxiv.org/html/2411.14762/x5.png",
                "caption": "Figure 4:CoordTok can efficiently encode long videos.rFVD scores of video tokenizers, evaluated on 128-frame videos, with respect to the token size.↓↓\\downarrow↓indicates lower values are better.",
                "position": 356
            },
            {
                "img": "https://arxiv.org/html/2411.14762/x6.png",
                "caption": "Table 2:FVDs of video generation modelson the UCF-101 dataset (128-frame, 128×\\times×128 resolution).↓↓\\downarrow↓indicates lower values are better.",
                "position": 438
            },
            {
                "img": "https://arxiv.org/html/2411.14762/x7.png",
                "caption": "Figure 6:Unconditional 128-frame, 128×\\times×128 resolution video generation resultsfrom CoordTok-SiT-L/2 trained on 128-frame videos from the UCF-101 dataset[40].\nWe provide more examples of generated videos inAppendixD.",
                "position": 607
            },
            {
                "img": "https://arxiv.org/html/2411.14762/x8.png",
                "caption": "(a)Effect of Model size",
                "position": 611
            },
            {
                "img": "https://arxiv.org/html/2411.14762/x8.png",
                "caption": "(a)Effect of Model size",
                "position": 614
            },
            {
                "img": "https://arxiv.org/html/2411.14762/x9.png",
                "caption": "(b)Effect of Triplane size (spatial)",
                "position": 619
            },
            {
                "img": "https://arxiv.org/html/2411.14762/x10.png",
                "caption": "(c)Effect of Triplane size (temporal)",
                "position": 624
            },
            {
                "img": "https://arxiv.org/html/2411.14762/x11.png",
                "caption": "(a)Effect of triplane representations.",
                "position": 671
            },
            {
                "img": "https://arxiv.org/html/2411.14762/x11.png",
                "caption": "(a)Effect of triplane representations.",
                "position": 674
            },
            {
                "img": "https://arxiv.org/html/2411.14762/x12.png",
                "caption": "(b)Effect of coordinate-based representations",
                "position": 679
            },
            {
                "img": "https://arxiv.org/html/2411.14762/x13.png",
                "caption": "Figure 9:Illustration of factorized triplane representations𝐳=[𝐳x⁢y,𝐳y⁢t,𝐳x⁢t]𝐳superscript𝐳𝑥𝑦superscript𝐳𝑦𝑡superscript𝐳𝑥𝑡{\\mathbf{z}}=[{\\mathbf{z}}^{xy},{\\mathbf{z}}^{yt},{\\mathbf{z}}^{xt}]bold_z = [ bold_z start_POSTSUPERSCRIPT italic_x italic_y end_POSTSUPERSCRIPT , bold_z start_POSTSUPERSCRIPT italic_y italic_t end_POSTSUPERSCRIPT , bold_z start_POSTSUPERSCRIPT italic_x italic_t end_POSTSUPERSCRIPT ]of CoordTok trained on the UCF-101 dataset[40].\nWe note that𝐳x⁢ysuperscript𝐳𝑥𝑦{\\mathbf{z}}^{xy}bold_z start_POSTSUPERSCRIPT italic_x italic_y end_POSTSUPERSCRIPTcaptures the global content in the video across time,e.g., layout and appearance of the scene or object, and𝐳y⁢tsuperscript𝐳𝑦𝑡{\\mathbf{z}}^{yt}bold_z start_POSTSUPERSCRIPT italic_y italic_t end_POSTSUPERSCRIPT,𝐳x⁢tsuperscript𝐳𝑥𝑡{\\mathbf{z}}^{xt}bold_z start_POSTSUPERSCRIPT italic_x italic_t end_POSTSUPERSCRIPTcapture the underlying motion in the video across two spatial axes.",
                "position": 754
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BEvaluation Details",
        "images": []
    },
    {
        "header": "Appendix CBaselines",
        "images": []
    },
    {
        "header": "Appendix DAdditional Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14762/x14.png",
                "caption": "Figure 10:Additional 128-frame, 128×\\times×128 resolution video reconstruction results from CoordTok (Ours) trained on the UCF-101 dataset[40]. For each frame, we visualize the ground-truth (GT) and reconstructed pixels from CoordTok.",
                "position": 2173
            },
            {
                "img": "https://arxiv.org/html/2411.14762/x15.png",
                "caption": "Figure 11:Additional unconditional 128-frame, 128×\\times×128 resolution video generation results from CoordTok-SiT-L/2 trained on 128-frame videos from the UCF-101 dataset[40].",
                "position": 2176
            }
        ]
    },
    {
        "header": "Appendix EAdditional Quantitative Results",
        "images": []
    }
]