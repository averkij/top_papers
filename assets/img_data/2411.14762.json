[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14762/x1.png",
                "caption": "(a)Maximum batch-size when training video tokenizerson128Ã—128128128128\\times 128128 Ã— 128resolution videos with varying lengths, measured with a single NVIDIA 4090 24GB GPU.",
                "position": 151
            },
            {
                "img": "https://arxiv.org/html/2411.14762/x1.png",
                "caption": "(a)Maximum batch-size when training video tokenizerson128Ã—128128128128\\times 128128 Ã— 128resolution videos with varying lengths, measured with a single NVIDIA 4090 24GB GPU.",
                "position": 154
            },
            {
                "img": "https://arxiv.org/html/2411.14762/x2.png",
                "caption": "(b)Inter-clip reconstruction consistency of video tokenizers.Existing video tokenizers[9,64,50]show the pixel-value inconsistency between short clips (16 frames). In contrast, Our tokenizer shows the temporally consistent reconstruction.",
                "position": 159
            },
            {
                "img": "https://arxiv.org/html/2411.14762/x3.png",
                "caption": "Figure 2:Overview of CoordTok.We design our encoder to encode a videoğ±ğ±{\\mathbf{x}}bold_xinto factorized triplane representationsğ³=[ğ³xâ¢y,ğ³yâ¢t,ğ³xâ¢t]ğ³superscriptğ³ğ‘¥ğ‘¦superscriptğ³ğ‘¦ğ‘¡superscriptğ³ğ‘¥ğ‘¡{\\mathbf{z}}=[{\\mathbf{z}}^{xy},{\\mathbf{z}}^{yt},{\\mathbf{z}}^{xt}]bold_z = [ bold_z start_POSTSUPERSCRIPT italic_x italic_y end_POSTSUPERSCRIPT , bold_z start_POSTSUPERSCRIPT italic_y italic_t end_POSTSUPERSCRIPT , bold_z start_POSTSUPERSCRIPT italic_x italic_t end_POSTSUPERSCRIPT ]which can efficiently represent the video with three 2D latent planes.\nGiven the triplane representationsğ³ğ³\\mathbf{z}bold_z, our decoder learns a mapping from(x,y,t)ğ‘¥ğ‘¦ğ‘¡(x,y,t)( italic_x , italic_y , italic_t )coordinates to RGB pixels within the corresponding patches.\nIn particular, we extract coordinate-based representations ofNğ‘Nitalic_Nsampled coordinates by querying the coordinates from triplane representations via bilinear interpolation.\nThen the decoder aggregates and fuses information from different coordinates with self-attention layers and project outputs into corresponding patches.\nThis design enables us to train tokenizers on long videos in a compute-efficient manner by avoiding reconstruction of entire frames at once.",
                "position": 208
            }
        ]
    },
    {
        "header": "2Method",
        "images": []
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14762/x4.png",
                "caption": "Figure 3:128-frame, 128Ã—\\timesÃ—128 resolution video reconstruction resultsfrom CoordTok (Ours) and baselines[64,50]trained on the UCF-101 dataset[40].\nFor each frame, we visualize the ground-truth (GT) and reconstructed pixels within the region highlighted in the red box, where CoordTok achieves noticeably better reconstruction quality than other baselines.",
                "position": 311
            },
            {
                "img": "https://arxiv.org/html/2411.14762/x5.png",
                "caption": "Figure 4:CoordTok can efficiently encode long videos.rFVD scores of video tokenizers, evaluated on 128-frame videos, with respect to the token size.â†“â†“\\downarrowâ†“indicates lower values are better.",
                "position": 356
            },
            {
                "img": "https://arxiv.org/html/2411.14762/x6.png",
                "caption": "Table 2:FVDs of video generation modelson the UCF-101 dataset (128-frame, 128Ã—\\timesÃ—128 resolution).â†“â†“\\downarrowâ†“indicates lower values are better.",
                "position": 438
            },
            {
                "img": "https://arxiv.org/html/2411.14762/x7.png",
                "caption": "Figure 6:Unconditional 128-frame, 128Ã—\\timesÃ—128 resolution video generation resultsfrom CoordTok-SiT-L/2 trained on 128-frame videos from the UCF-101 dataset[40].\nWe provide more examples of generated videos inAppendixD.",
                "position": 607
            },
            {
                "img": "https://arxiv.org/html/2411.14762/x8.png",
                "caption": "(a)Effect of Model size",
                "position": 611
            },
            {
                "img": "https://arxiv.org/html/2411.14762/x8.png",
                "caption": "(a)Effect of Model size",
                "position": 614
            },
            {
                "img": "https://arxiv.org/html/2411.14762/x9.png",
                "caption": "(b)Effect of Triplane size (spatial)",
                "position": 619
            },
            {
                "img": "https://arxiv.org/html/2411.14762/x10.png",
                "caption": "(c)Effect of Triplane size (temporal)",
                "position": 624
            },
            {
                "img": "https://arxiv.org/html/2411.14762/x11.png",
                "caption": "(a)Effect of triplane representations.",
                "position": 671
            },
            {
                "img": "https://arxiv.org/html/2411.14762/x11.png",
                "caption": "(a)Effect of triplane representations.",
                "position": 674
            },
            {
                "img": "https://arxiv.org/html/2411.14762/x12.png",
                "caption": "(b)Effect of coordinate-based representations",
                "position": 679
            },
            {
                "img": "https://arxiv.org/html/2411.14762/x13.png",
                "caption": "Figure 9:Illustration of factorized triplane representationsğ³=[ğ³xâ¢y,ğ³yâ¢t,ğ³xâ¢t]ğ³superscriptğ³ğ‘¥ğ‘¦superscriptğ³ğ‘¦ğ‘¡superscriptğ³ğ‘¥ğ‘¡{\\mathbf{z}}=[{\\mathbf{z}}^{xy},{\\mathbf{z}}^{yt},{\\mathbf{z}}^{xt}]bold_z = [ bold_z start_POSTSUPERSCRIPT italic_x italic_y end_POSTSUPERSCRIPT , bold_z start_POSTSUPERSCRIPT italic_y italic_t end_POSTSUPERSCRIPT , bold_z start_POSTSUPERSCRIPT italic_x italic_t end_POSTSUPERSCRIPT ]of CoordTok trained on the UCF-101 dataset[40].\nWe note thatğ³xâ¢ysuperscriptğ³ğ‘¥ğ‘¦{\\mathbf{z}}^{xy}bold_z start_POSTSUPERSCRIPT italic_x italic_y end_POSTSUPERSCRIPTcaptures the global content in the video across time,e.g., layout and appearance of the scene or object, andğ³yâ¢tsuperscriptğ³ğ‘¦ğ‘¡{\\mathbf{z}}^{yt}bold_z start_POSTSUPERSCRIPT italic_y italic_t end_POSTSUPERSCRIPT,ğ³xâ¢tsuperscriptğ³ğ‘¥ğ‘¡{\\mathbf{z}}^{xt}bold_z start_POSTSUPERSCRIPT italic_x italic_t end_POSTSUPERSCRIPTcapture the underlying motion in the video across two spatial axes.",
                "position": 754
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BEvaluation Details",
        "images": []
    },
    {
        "header": "Appendix CBaselines",
        "images": []
    },
    {
        "header": "Appendix DAdditional Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14762/x14.png",
                "caption": "Figure 10:Additional 128-frame, 128Ã—\\timesÃ—128 resolution video reconstruction results from CoordTok (Ours) trained on the UCF-101 dataset[40]. For each frame, we visualize the ground-truth (GT) and reconstructed pixels from CoordTok.",
                "position": 2173
            },
            {
                "img": "https://arxiv.org/html/2411.14762/x15.png",
                "caption": "Figure 11:Additional unconditional 128-frame, 128Ã—\\timesÃ—128 resolution video generation results from CoordTok-SiT-L/2 trained on 128-frame videos from the UCF-101 dataset[40].",
                "position": 2176
            }
        ]
    },
    {
        "header": "Appendix EAdditional Quantitative Results",
        "images": []
    }
]