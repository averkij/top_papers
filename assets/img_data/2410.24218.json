[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.24218/x1.png",
                "caption": "Figure 1:An overview of four environments used for experiments. It shows tasks to be learned in each environment; examples of hindsight (markedH) and foresight (F) language feedback (next to the gear icon are hand-crafted templates and next to the GPT icon are GPT-4 generated feedback); as well as low-level actions in each environment.",
                "position": 213
            }
        ]
    },
    {
        "header": "3Problem Setting",
        "images": []
    },
    {
        "header": "4Data Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.24218/x2.png",
                "caption": "Figure 2:A demonstration of hindsight and foresight language feedback generation. In our framework, the agentœÄùúã\\piitalic_œÄexecutes the trajectory, while the expert agentœÄ‚àósuperscriptùúã\\pi^{*}italic_œÄ start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT, with access to privileged ground truth knowledge, is used solely to provide information for generating language feedback toœÄùúã\\piitalic_œÄ. At time steptùë°titalic_t,hindsight languageis generated by comparing the agent‚Äôs actionat‚àí1subscriptùëéùë°1a_{t-1}italic_a start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPTwith the expert agent‚Äôs actionat‚àí1‚àósuperscriptsubscriptùëéùë°1a_{t-1}^{*}italic_a start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT, whereasforesight languageis generated by referring to the expert agent‚Äôs actionat‚àósuperscriptsubscriptùëéùë°a_{t}^{*}italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPTto guide the agent on the next step. To increase the diversity of language feedback, we construct a pool of language templates comprising GPT-augmented languages, and sample candidate instructions as online language feedback.",
                "position": 298
            }
        ]
    },
    {
        "header": "5Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.24218/x3.png",
                "caption": "Figure 3:Language-Teachable Decision Transformer.",
                "position": 406
            }
        ]
    },
    {
        "header": "6Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.24218/x4.png",
                "caption": "Figure 4:Comparison of agent performance in four environments (averaged across 100 seeds in each environment) under varying levels of language feedback informativeness and diversity. Agents trained with more informative language feedback exhibit progressively higher performance. Furthermore, given the same informativeness (Hindsight + Foresight), increasing diversity with the GPT-augmented language pool leads to the highest performance.",
                "position": 446
            },
            {
                "img": "https://arxiv.org/html/2410.24218/x5.png",
                "caption": "Figure 5:Comparison of agent performance onunseen tasksin four environments (averaged across 100 seeds in each environment) under varying language informativeness in agent pre-training. Agent trained with more informative language adapts to new tasks faster and better.",
                "position": 449
            },
            {
                "img": "https://arxiv.org/html/2410.24218/x6.png",
                "caption": "Figure 6:Efficiency gain vs. task difficulty. We fit the scatter plots with a second-degree polynomial to visualize the overall trend. As task difficulty increases, the general trend of the efficiency gain is to rise initially and then decline, suggesting: (1) for tasks that are too easy or too hard, language feedback does not improve efficiency; (2) language feedback is most helpful in increasing efficiency for moderate tasks.",
                "position": 478
            },
            {
                "img": "https://arxiv.org/html/2410.24218/x7.png",
                "caption": "Figure 7:Performance vs. language frequency. Agents perform better with more frequent language feedback across four environments.",
                "position": 490
            },
            {
                "img": "https://arxiv.org/html/2410.24218/x8.png",
                "caption": "Figure 8:We investigate two special evaluation settings: (1) no language feedback is provided during evaluation and (2) disturbed language feedback is given at every step. Results show that agents trained with the GPT-augmented language still outperform the no-language agent (the black dotted line) in the disturbed setting, and also achieve better performance in some environments while no language is given.",
                "position": 521
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethical Impacts",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AEnvironments and Language Feedback",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.24218/x9.png",
                "caption": "",
                "position": 1446
            },
            {
                "img": "https://arxiv.org/html/2410.24218/x10.png",
                "caption": "",
                "position": 1466
            },
            {
                "img": "https://arxiv.org/html/2410.24218/x12.png",
                "caption": "",
                "position": 1514
            },
            {
                "img": "https://arxiv.org/html/2410.24218/x14.png",
                "caption": "",
                "position": 1575
            },
            {
                "img": "https://arxiv.org/html/2410.24218/x16.png",
                "caption": "",
                "position": 1623
            },
            {
                "img": "https://arxiv.org/html/2410.24218/x18.png",
                "caption": "",
                "position": 1661
            },
            {
                "img": "https://arxiv.org/html/2410.24218/x19.png",
                "caption": "",
                "position": 1681
            },
            {
                "img": "https://arxiv.org/html/2410.24218/x21.png",
                "caption": "",
                "position": 1729
            },
            {
                "img": "https://arxiv.org/html/2410.24218/x23.png",
                "caption": "",
                "position": 1787
            },
            {
                "img": "https://arxiv.org/html/2410.24218/x25.png",
                "caption": "",
                "position": 1835
            }
        ]
    },
    {
        "header": "Appendix BAgent for Offline Data Collection and Language Feedback Generation",
        "images": []
    },
    {
        "header": "Appendix CTask Settings for RQ 1 and 2",
        "images": []
    },
    {
        "header": "Appendix DPerformance under aligned language type with training.",
        "images": []
    },
    {
        "header": "Appendix EImpact of hindsight on future steps",
        "images": []
    },
    {
        "header": "Appendix FMore results on the Messenger environment",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.24218/x27.png",
                "caption": "Figure 9:In the Messenger environment, when trained with more diverse foresight and hindsight languages, the agents can perform better than those trained without languages. Furthermore, agents trained with more informative languages demonstrate stronger performance.",
                "position": 2080
            }
        ]
    },
    {
        "header": "Appendix GModels and Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.24218/x28.png",
                "caption": "Figure 10:Examples for language feedback generated by online GPT in evaluation.",
                "position": 2396
            }
        ]
    },
    {
        "header": "Appendix HExamples for Language Feedback in Evaluation",
        "images": []
    }
]