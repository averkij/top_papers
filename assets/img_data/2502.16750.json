[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16750/extracted/6227116/Results/Lit_Guardian.png",
                "caption": "Figure 1:This figure represents the research papers discussed in the related work section.",
                "position": 180
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16750/extracted/6227116/Results/Workflow.png",
                "caption": "Figure 2:Workflow Diagram of the Methodology for Evaluating Agentic Systems, illustrating the Reverse Turing Test, Aligning Multi-Agent Systems, and Prevention of Multi-Shot Jailbreaks, including agent interactions and processes.",
                "position": 214
            }
        ]
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16750/extracted/6227116/Results/umap.png",
                "caption": "Figure 3:UMAP Projection of Jailbreak Prompts with Attack Success Rate Overlay shows a two-dimensional UMAP projection of the jailbreak prompts. Each point represents a prompt, and the color of the point indicates the Attack Success Rate (ASR) associated with that prompt. This visualization helps to understand the relationships between different prompts and their effectiveness in jailbreaking the models.",
                "position": 751
            },
            {
                "img": "https://arxiv.org/html/2502.16750/extracted/6227116/Results/ASR.png",
                "caption": "Figure 4:Attack Success Rate vs Length of Jailbreak Prompt for Different Models shows the relationship between the length of a jailbreak prompt (in characters) and the Attack Success Rate (ASR) for various language models. A higher ASR indicates greater vulnerability to jailbreaking.",
                "position": 764
            },
            {
                "img": "https://arxiv.org/html/2502.16750/extracted/6227116/Results/obedience.png",
                "caption": "Figure 5:Obedience Levels of Different Models to Jailbreak Prompts exhibit the number of responses categorized as \"Partial Rejection,\" \"Partial Obedience,\" and \"Full Obedience\" for each language model when presented with jailbreak prompts.",
                "position": 770
            },
            {
                "img": "https://arxiv.org/html/2502.16750/extracted/6227116/Results/detection.png",
                "caption": "Figure 6:Detection Rate of Harmful Content Across Different Models displays the percentage of harmful content correctly detected by each language model.",
                "position": 776
            },
            {
                "img": "https://arxiv.org/html/2502.16750/extracted/6227116/Results/conf.png",
                "caption": "Figure 7:Confusion Matrix for Harmful Content Detection Across Different Models. Each section of the image represents a confusion matrix for a specific language model, showing the performance in classifying content as benign or harmful. The axes represent Predicted Label (horizontal) and Actual Label (vertical). The values within the matrix represent the percentage of instances falling into each category.",
                "position": 875
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]