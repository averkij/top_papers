[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13778/x1.png",
                "caption": "Figure 1:InternVLA-M1 integrates spatial grounding into the vision–language–action training pipeline. Given a task instruction, the VLM planner produces latent plans through explicit spatial prompting, which then effectively guides the action expert to generate control signals.",
                "position": 98
            }
        ]
    },
    {
        "header": "2InternVLA-M1",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13778/x2.png",
                "caption": "Figure 2:Overview of InternVLA-M1.InternVLA-M1 adopts a spatially guided two-stage training pipeline. Stage 1 (spatial grounding pre-training): the VLM is trained on large-scale multisource multimodal spatial grounding data to learn embodiment-agnostic spatial priors. Stage 2 (spatially guided action post-training): the VLM Planner, functioning as a slow but reliable System 2 reasoner, generates latent planning tokens via spatial prompting as the condition to the action expert (instantiated as a DiT Actor) to execute as a fast System 1 controller.",
                "position": 144
            }
        ]
    },
    {
        "header": "3Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13778/x3.png",
                "caption": "Figure 3:Overview of the pre-training data for the vision-language model. The data comprises two main parts: general VQA data to maintain the model’s general multimodal capabilities, and spatial VQA data focusing on robotic-related grounding and spatial perception in a VQA format.",
                "position": 245
            },
            {
                "img": "https://arxiv.org/html/2510.13778/x4.png",
                "caption": "Figure 4:Simulation data synthesis pipeline. The pipeline generates diverse robotic manipulation data from a large asset library, converts intermediate representations into VQA data, and separates physics from rendering to reduce wasted failures and improve efficiency.",
                "position": 260
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13778/x5.png",
                "caption": "Figure 5:Ablation study on the effect of auxiliary spatial prompting for co-training robot manipulation with spatial grounding.\nFrom left to right: (a) spatial grounding performance (IoU@0.5 on RefCOCO-g);\n(b) manipulation performance (SimplerEnv-WidowX SR);\n(c) shows the gradient similarity of the spatial grounding and manipulation objectives.",
                "position": 764
            },
            {
                "img": "https://arxiv.org/html/2510.13778/x6.png",
                "caption": "Figure 6:Evaluation settings for generalizable pick-and-place in large-scale simulation.",
                "position": 1058
            },
            {
                "img": "https://arxiv.org/html/2510.13778/x7.png",
                "caption": "Figure 7:Result comparison of 200 simulated benchmarks in instruction-following pick-and-place.",
                "position": 1061
            },
            {
                "img": "https://arxiv.org/html/2510.13778/x8.png",
                "caption": "Figure 8:Overview of objects and containers used in instruction-following pick-and-place.",
                "position": 1078
            },
            {
                "img": "https://arxiv.org/html/2510.13778/x9.png",
                "caption": "Figure 9:Evaluation settings showcase for real-world instruction-following manipulations.",
                "position": 1081
            },
            {
                "img": "https://arxiv.org/html/2510.13778/x10.png",
                "caption": "Figure 10:Result comparison in real-world instruction-following pick-and-place.",
                "position": 1094
            },
            {
                "img": "https://arxiv.org/html/2510.13778/x11.png",
                "caption": "Figure 11:Showcase for long-horizon instruction-following manipulation.",
                "position": 1145
            },
            {
                "img": "https://arxiv.org/html/2510.13778/x12.png",
                "caption": "Figure 12:Result comparison in real-world long-horizon task planning for manipulation.",
                "position": 1148
            }
        ]
    },
    {
        "header": "5Related work",
        "images": []
    },
    {
        "header": "6Discussion and conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAuthor contributions",
        "images": []
    }
]