[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26328/x1.png",
                "caption": "(a)",
                "position": 116
            },
            {
                "img": "https://arxiv.org/html/2509.26328/x1.png",
                "caption": "(a)",
                "position": 119
            },
            {
                "img": "https://arxiv.org/html/2509.26328/x2.png",
                "caption": "(b)",
                "position": 124
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26328/x3.png",
                "caption": "Figure 2:Training process of Fast-dLLM-v2. The input sequence is decoded block by block. Within each block, the model performs next-token prediction with partial masking. To ensure every token is trained, complementary masks are introduced so that masked tokens in one view can be predicted from the other. We only apply loss to predicted tokens that are highlighted in green, and dashed curves connect Mask tokens to their corresponding predictions.",
                "position": 229
            },
            {
                "img": "https://arxiv.org/html/2509.26328/x4.png",
                "caption": "Figure 3:Illustration of the inference process. The sequence is decoded block-by-block. The decoded blocks are cached to speed up inference. Within each block, we adopt the parallel decoding and DualCache in Fast-dLLM to further accelerate inference.",
                "position": 292
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26328/x5.png",
                "caption": "Figure 4:Accuracy and throughput under different thresholds on GSM8K. Threshold 0.9 is selected, offering a 2.6Ã— speedup with minimal accuracy drop.",
                "position": 552
            },
            {
                "img": "https://arxiv.org/html/2509.26328/x6.png",
                "caption": "Figure 5:Throughput comparison between autoregressive and diffusion generation methods on NVIDIA A100 and H100 GPUs across varying batch sizes.\nDiffusion generation consistently outperforms autoregressive on both GPUs.",
                "position": 579
            },
            {
                "img": "https://arxiv.org/html/2509.26328/x7.png",
                "caption": "(a)",
                "position": 660
            },
            {
                "img": "https://arxiv.org/html/2509.26328/x7.png",
                "caption": "(a)",
                "position": 663
            },
            {
                "img": "https://arxiv.org/html/2509.26328/x8.png",
                "caption": "(b)",
                "position": 668
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26328/x9.png",
                "caption": "(a) Training-time attention mask.",
                "position": 1317
            },
            {
                "img": "https://arxiv.org/html/2509.26328/x9.png",
                "caption": "(a) Training-time attention mask.",
                "position": 1320
            },
            {
                "img": "https://arxiv.org/html/2509.26328/x10.png",
                "caption": "(b) Inference-time attention mask.",
                "position": 1325
            }
        ]
    },
    {
        "header": "Appendix BCase Study",
        "images": []
    }
]