[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.10176/x1.png",
                "caption": "Figure 1:Illustration of multisensory integration and the role of inverse effectiveness in IEMF (Inverse Effectiveness driven Multimodal Fusion).(a)Comparison between unimodal sensory input and multisensory integration: integrating visual and auditory cues reduces ambiguity and uncertainty compared to relying on a single modality.(b)Neural basis of audiovisual integration in the human brain, focusing on the superior temporal sulcus (STS) where visual and auditory inputs converge onto multisensory neurons.(c)Biological principle of inverse effectiveness: multisensory integration is strengthened when unimodal signals are weak. Visual and auditory stimuli are processed through distinct sensory pathways and converge at multisensory synapses. The inset illustrates the inverse relationship between unimodal strength and integrative gain.(d)The proposed inverse effectiveness driven multimodal fusion strategy inspired by biological multisensory fusion mechanisms. Visual and auditory inputs are processed by respective encoders, fused via a dynamic fusion module regulated by inverse effectiveness principles, and evaluated using modality strength score estimation. The fusion module weights are dynamically adjusted according to the computed scores.\n(This figure was created withhttps://BioRender.com.)",
                "position": 67
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.10176/x2.png",
                "caption": "Figure 2:Comprehensive evaluation of the proposed inverse effectiveness driven multimodal fusion (IEMF).(a)Overall performance on ANNs.\nBar charts compare the vanilla method (blue) with the method augmented by IEMF (khaki) on three audiovisual classification benchmarks‚ÄîCREMA-D(a1), Kinetics-Sounds(a2)and UrbanSound8K-AV(a3)‚Äîunder four representative fusion schemes (Concat, MSLR, OGM_GE and LFM).(b)Overall performance on SNNs.\nSame layout as (a) but using spiking neural networks, demonstrating that IEMF consistently boosts accuracy across network paradigms and datasets(b1‚Äìb3).(c)Mechanism ablation on the Kinetics-Sounds dataset.(c1)Effect of the inverse gain coefficientŒ≥ùõæ\\gammaitalic_Œ≥: the baseline without IEMF (grey dashed line on the colour bar) scores below every IEMF setting; model accuracy peaks atŒ≥=1ùõæ1\\gamma{=}1italic_Œ≥ = 1.(c2)Removing the IEMF term (‚ÄúJoint Learning only‚Äù) leads to a clear performance drop, highlighting the essential role of the inverse effectiveness multimodal fusion component.(d)Unimodal gain analysis.\nTest accuracy for (i) a unimodal audio model trained alone (‚ÄúUnimodal‚Äù), (ii) the audio branch extracted from a multimodal model without IEMF (‚ÄúUnimodal branch (MM) w/o IEMF‚Äù), and (iii) the audio branch with IEMF (‚ÄúUnimodal branch (MM) w/ IEMF‚Äù). IEMF yields a persistent relative gain for the unimodal branch.(e)Dynamics of the IEMF coefficient.\nEvolution of the learnt IMEF coefficientŒæùúâ\\xiitalic_Œæ(dashed orange, right y-axis) alongside the test accuracy (solid blue, left y-axis) during training. At the early stage, the value ofŒæùúâ\\xiitalic_Œæis large to accelerate the multimodal integration, while as the network converges,Œæùúâ\\xiitalic_Œæfalls back and remains stable to maintain the fusion stability.(f)Loss landscape visualization.(f1)3D loss surface comparison: vanilla method (left) versus IEMF-enhanced method (right).(f2-f3)2D contour plots: without IEMF (f2) versus with IEMF (f3). The IEMF method leads to broader and flatter minima.(g)Computational cost analysis.\nComparison of computational cost between standard models (blue) and IEMF-enhanced models (khaki). IEMF significantly reduces computational costs across all fusion methods, with reductions ranging from 15.2% to 50.0% (highlighted in yellow percentages).",
                "position": 124
            }
        ]
    },
    {
        "header": "2Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.10176/x3.png",
                "caption": "Figure 3:Inverse effectiveness driven multimodal fusion boosts audio visual continual learning.(a) Task schematic.A single audiovisual model is incrementally updated as new classes arrive; the goal is to absorb the new knowledge while preserving performance on previously learned classes‚Äîachieving ‚Äúlearning without forgetting‚Äù.(b) Results on AVE-CI,(c) K-S-CIand(d) VS100-CI.\nFor three representative class incremental learning baselines‚ÄîLwF, SSIL and AV-CIL‚Äîwe compare the vanilla method (blue) with the method augmented by IEMF (khaki).\nEach sub-panel is split into top and bottom:\nthe top bar chart reports theoverall performance(mean accuracy across all tasks, error bars denote one standard deviation),\nwhile the bottom line plot traces theincremental accuracyafter each successive task.\nAcross all datasets and baselines, IEMF consistently increases mean accuracy and yields a flatter accuracy-decay curve, indicating the better knowledge transfer.",
                "position": 194
            },
            {
                "img": "https://arxiv.org/html/2505.10176/x4.png",
                "caption": "Figure 4:Quantitative and qualitative impact of IEMF on audio visual question answering task.(a)Baseline model.\nThe three radar charts (top) report accuracy on audio-only, visual-only, and audio-visual questions, respectively.Orange= vanilla,Blue= w/ IEMF.\nThe bottom row shows a representative sample‚Äîwaveform, video frames, and question/answer‚Äîwhere the vanilla fusion miscounts the instruments (‚ÄúTwo‚Äù), whereas IEMF answers correctly (‚ÄúOne‚Äù).(b)ST-AVQA model.\nThe same layout as (a), but using the stronger ST-AVQA model.\nIEMF again enlarges the radar area for every question type and corrects the localization query in the illustrated example (vanilla: ‚ÄúLeft‚Äù; w/ IEMF: ‚ÄúRight‚Äù).\nAcross both models, the blue polygons consistently enclose the orange ones, confirming that the inverse effectiveness driven multimodal fusion mechanism improves all question categories while providing intuitive per-sample gains.",
                "position": 232
            }
        ]
    },
    {
        "header": "3Discussion",
        "images": []
    },
    {
        "header": "4Materials and methods",
        "images": []
    },
    {
        "header": "5Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix Aappendix",
        "images": []
    }
]