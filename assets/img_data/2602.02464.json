[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02464/x1.png",
                "caption": "Figure 1:MFA decomposes each activation into a region assignment and a within-region offset.Left: the region structure is modeled by Gaussian components (centroidsùùÅk\\boldsymbol{\\mu}_{k}), with complex concepts typically spanning multiple Gaussians ‚Äì here, the broaderEmotionsneighborhood is spanned by several interpretable Gaussians. Right: each component is equipped with a low-dimensional subspace that parameterizes structured within-region variation.",
                "position": 163
            }
        ]
    },
    {
        "header": "2Preliminaries and Notation",
        "images": []
    },
    {
        "header": "3Mapping the Activation Space with Mixtures of Factor Analyzers",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02464/x2.png",
                "caption": "Figure 2:Example MFA Gaussians in the activation space of Llama-3.1-8B, visualized in 3D using three loadings as axes. (Left) A broad region spanning multiple movie genres, where the loadings separate genre-related themes. (Right) A narrow region centered on the tokenNational, where the loadings capture context-dependent usage.",
                "position": 432
            }
        ]
    },
    {
        "header": "4Activation Structures Discovered by MFA",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02464/x3.png",
                "caption": "Figure 3:Characterizing MFA regions.(a) Broad vs. narrow regions differ across model families (Gemma skews narrow/token-driven; Llama stays mostly broad/semantic). (b) Semantic vs. syntactic loadings, split by broad/narrow components, become more semantic asKKincreases, indicating more context-dependent within-region variation.",
                "position": 464
            },
            {
                "img": "https://arxiv.org/html/2602.02464/x3.png",
                "caption": "",
                "position": 467
            },
            {
                "img": "https://arxiv.org/html/2602.02464/x4.png",
                "caption": "",
                "position": 472
            }
        ]
    },
    {
        "header": "5MFA vs. Dictionary Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02464/x5.png",
                "caption": "Figure 4:MFA vs. SAE reconstructions.MFA reconstructs an activation by anchoring it to a region (centroid) and refining it with a region-specific direction, whereas SAEs reconstruct by accumulating many global dictionary features. Left: Llama-3.1-8B, layer 22; right: Gemma-2-2B, layer 18.",
                "position": 604
            }
        ]
    },
    {
        "header": "6Evaluations",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02464/x6.png",
                "caption": "Figure 5:Steering results across layers in Gemma-2-2B and Llama-3.1-8B of state-of-the-art SAEs, DiffMeans and 1K, 8K, 32K Gaussian MFAs. Across the majority of settings MFA significantly outperforms DiffMeans and SAEs.",
                "position": 772
            }
        ]
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMFA Initialization and Training",
        "images": []
    },
    {
        "header": "Appendix BAnnotation Statistical Testing",
        "images": []
    },
    {
        "header": "Appendix CLabeling Loadings",
        "images": []
    },
    {
        "header": "Appendix DReconstruction Analysis",
        "images": []
    },
    {
        "header": "Appendix EBenchmarking: Additional Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02464/x7.png",
                "caption": "Figure 6:Concept Score steering results across layers in Gemma-2-2B and Llama-3.1-8B of state-of-the-art SAEs, DiffMeans and 1K, 8K, 32K Gaussian MFAs. Across all settings MFA significantly outperforms DiffMeans and SAEs. Strongly promoting its associated concepts.",
                "position": 1851
            },
            {
                "img": "https://arxiv.org/html/2602.02464/x8.png",
                "caption": "Figure 7:Fluency Score steering results across layers in Gemma-2-2B and Llama-3.1-8B of state-of-the-art SAEs, DiffMeans and 1K, 8K, 32K Gaussian MFAs. Across all settings scores are consistent for all methods, with MFA showing a slight decline.",
                "position": 1854
            }
        ]
    },
    {
        "header": "Appendix FExamples",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02464/lg1.png",
                "caption": "",
                "position": 1868
            },
            {
                "img": "https://arxiv.org/html/2602.02464/lg1.png",
                "caption": "",
                "position": 1871
            },
            {
                "img": "https://arxiv.org/html/2602.02464/lg2.png",
                "caption": "",
                "position": 1879
            },
            {
                "img": "https://arxiv.org/html/2602.02464/lg3.png",
                "caption": "",
                "position": 1881
            },
            {
                "img": "https://arxiv.org/html/2602.02464/gg1.png",
                "caption": "",
                "position": 1886
            },
            {
                "img": "https://arxiv.org/html/2602.02464/gg2.png",
                "caption": "",
                "position": 1894
            },
            {
                "img": "https://arxiv.org/html/2602.02464/gg3.png",
                "caption": "",
                "position": 1896
            },
            {
                "img": "https://arxiv.org/html/2602.02464/lg4.png",
                "caption": "",
                "position": 1902
            },
            {
                "img": "https://arxiv.org/html/2602.02464/lg4.png",
                "caption": "",
                "position": 1905
            },
            {
                "img": "https://arxiv.org/html/2602.02464/lg5.png",
                "caption": "",
                "position": 1913
            },
            {
                "img": "https://arxiv.org/html/2602.02464/gg4.png",
                "caption": "",
                "position": 1918
            },
            {
                "img": "https://arxiv.org/html/2602.02464/gg5.png",
                "caption": "",
                "position": 1926
            },
            {
                "img": "https://arxiv.org/html/2602.02464/x9.png",
                "caption": "Figure 10:Annotation instructions provided to graduate NLP students for labeling the loadings as either semantic or syntactic, for the analysis of ¬ß4.",
                "position": 2385
            },
            {
                "img": "https://arxiv.org/html/2602.02464/x10.png",
                "caption": "Figure 11:Annotation instructions provided to annotators for labeling the Gaussians as either broad or narrow, for the analysis of ¬ß4.",
                "position": 2388
            }
        ]
    },
    {
        "header": "Appendix GPrompts and Annotation Instructions",
        "images": []
    }
]