[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Open problems in mechanistic interpretability methods and foundations",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.16496/x1.png",
                "caption": "Figure1:Two approaches to neural network interpretability. (Left) Reverse Engineering is characterized by decomposing networks into functional components and describing how those components interact to produce the network’s behavior. It thus aims to ‘identify the roles of network components’ (Section2.1). (Right) Concept-based interpretability on the other hand attempts to discover human concepts within neural network internals. It thus aims to ‘identify the network components for given roles’ (Section2.2).",
                "position": 394
            },
            {
                "img": "https://arxiv.org/html/2501.16496/x2.png",
                "caption": "Figure2:The steps of reverse engineering neural networks. (1) Decomposing a network into simpler components. This decomposition might not necessarily use architecturally-defined bases, such as individual neurons or layers (Section2.1.2). (2) Hypothesizing about the functional roles of some or all components (Section2.1.3). (3) Validating whether our hypotheses are correct, creating a cycle in which we iteratively refine our decompositions and hypotheses to improve our understanding of the network (Section2.1.4).",
                "position": 452
            },
            {
                "img": "https://arxiv.org/html/2501.16496/x3.png",
                "caption": "Figure3:Three ideas underlying the sparse dictionary learning (SDL) paradigm in mechanistic interpretability. (Left) The linear representation hypothesis states that the map from ‘concepts’ to neural activations is linear. (Middle) Superposition is the hypothesis that models represent many more concepts than they have dimensions by representing them both sparsely and linearly in activation spaces. (Right) SDL attempts to recover an overcomplete basis of concepts represented in superposition in activation space.",
                "position": 480
            },
            {
                "img": "https://arxiv.org/html/2501.16496/x4.png",
                "caption": "Figure4:Sparse dictionary learning has a number of practical and conceptual limitations that cause issues when using it to reverse engineer neural networks (Section2.1.2).",
                "position": 496
            },
            {
                "img": "https://arxiv.org/html/2501.16496/x5.png",
                "caption": "Figure5:To study the functional role ofthe blue componentnumerous approaches are possible. We could study its causes:the purple input componentsorthe red intermediate componentsvia e.g. feature synthesis(Olah et al.,2017a;2020b), maximum activating examples(Olah et al.,2017a; Bricken,2023), or attributions(Sundararajan et al.,2017). Or we could study its effects:the orange output componentsorthe green intermediate componentsvia e.g. the logit lens(Nostalgebraist,2020), activation steering(Turner et al.,2024), or attributions(Marks et al.,2024).",
                "position": 569
            }
        ]
    },
    {
        "header": "3Open problems in applications of mechanistic interpretability",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.16496/x6.png",
                "caption": "Figure6:A summary of problem areas for applications of mechanistic interpretability.",
                "position": 865
            }
        ]
    },
    {
        "header": "4Open socio-technical problems in mechanistic interpretability",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASummary of open questions",
        "images": []
    }
]