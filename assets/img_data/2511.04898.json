[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.04898/x1.png",
                "caption": "",
                "position": 95
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Real-Time Reasoning Gym",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.04898/x2.png",
                "caption": "Figure 2:Agent loops in OpenAI Gym(openaigym)and Real-Time Reasoning Gym. ConstantsT_EandDEFAULT_ACTIONwill be explained in the following ‘time pressure’ paragraph.",
                "position": 178
            },
            {
                "img": "https://arxiv.org/html/2511.04898/x3.png",
                "caption": "Figure 3:Existing evaluation setups for LLM Agents often assume astaticsetting, where the environment halts while the agent completes reasoning\nwith unlimited computation. In Real-Time Reasoning Gym, environments aredynamic,\nevolving regardless of agents’ computation state. As illustrated in theFreewaysetting,Planning Agent, which\nperforms extended reasoning without interruption, fails to act timely whileReactive\nAgent, which performs reasoning strictly within environment update period,\nlacks foresight and collides.AgileThinkercombines both timely reaction and long-term planning to navigate such environments\neffectively.",
                "position": 234
            }
        ]
    },
    {
        "header": "3Real-time Reasoning Agents",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.04898/x4.png",
                "caption": "Figure 4:Two parallel threads in AgileThinker",
                "position": 280
            },
            {
                "img": "https://arxiv.org/html/2511.04898/x5.png",
                "caption": "Figure 5:Performance of reasoning agents in Real-Time Reasoning Gym under varying cognitive\nloads and time pressures. Upper: we fix time pressure at 8k tokens per step\nand vary cognitive load. Lower: we fix cognitive load at medium level\nand vary time pressure. Full data and significance test at App. §C.1and §C.2.",
                "position": 289
            }
        ]
    },
    {
        "header": "4Is Single Paradigm Enough for Real-Time Reasoning?",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.04898/x6.png",
                "caption": "Figure 6:Thinking trajectories of different paradigms at critical\nstepsAt step 3,Reactive Agent (V3)greedily pursues the nearest food and collides inevitably after three steps.Planning Agent (R1),\nstill reasoning over the outdated step-1 state, defaults left.\nHowever, it correctly identifies that eating the nearest food would result\nin a future collision, and that its lifespan is sufficient to delay consumption.\nGuided by the reasoning of Reactive Thread, Planning Thread in theAgileThinkeranticipates the trap and chooses to move upward toward a safer food target.",
                "position": 308
            }
        ]
    },
    {
        "header": "5How to Manage Resources Between Reaction and Planning?",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.04898/x7.png",
                "caption": "Figure 7:Performance of AgileThinker under different reactive thread token budgetsNTℛN_{T_{\\mathcal{R}}}. The cumulative distribution function (CDF) shows the natural token usage ofℛ\\mathcal{R}across all game trajectories when generation is not truncated, indicating inherent computational requirements ofℛ\\mathcal{R}.",
                "position": 344
            }
        ]
    },
    {
        "header": "6Performance Improvement Under Wall-Clock Time",
        "images": []
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "9Limitations",
        "images": []
    },
    {
        "header": "Appendix AEnvironment Details",
        "images": []
    },
    {
        "header": "Appendix BPrompt",
        "images": []
    },
    {
        "header": "Appendix CAdditional Experiment Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.04898/x8.png",
                "caption": "Figure 8:Statistical significance of AgileThinker’s advantage over single-paradigm agents. Upper: Advantage over reactive agent (V3). Lower: Advantage over planning agent (R1). Numbers represent p-values under varying cognitive loads and time pressures, with red indicating statistical significance (p<0.05p<0.05). The advantage of AgileThinker generally increases with both cognitive load and time pressure.",
                "position": 1755
            },
            {
                "img": "https://arxiv.org/html/2511.04898/figures/token_usage_vs_budget.png",
                "caption": "Figure 9:To test the effectiveness of thinking budget control, we set the thinking budget of Gemini-2.5-Flash at 4k, 8k, 16k and 32k to investigate the distribution of response length.\nThe distribution shows that even Gemini-2.5-Flash is aware of the budget, it cannot precisely regulate response token count, often generating excessive tokens.",
                "position": 2004
            },
            {
                "img": "https://arxiv.org/html/2511.04898/x9.png",
                "caption": "Figure 10:Almost linear correlation between generated token count and wall-clock time\nusing DeepSeek official API, demonstrating the validity of our token-as-time\nabstraction. Here the numbers after agent methods, e.g. 4k, 8k, refer to the corresponding environment time pressure budgets.",
                "position": 2303
            }
        ]
    },
    {
        "header": "Appendix DWalltime Experiments",
        "images": []
    }
]