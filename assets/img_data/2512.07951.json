[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07951/x1.png",
                "caption": "",
                "position": 92
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07951/x2.png",
                "caption": "Figure 2:(a) GAN-based approaches process videos in a frame-by-frame manner, and therefore often struggle with realism and suffer from temporal inconsistency.\n(b) Inpainting-based methods focus on generating the facial region based on sparse conditions, which inevitably leads to a loss of fidelity and unnatural visual artifacts.\n(c) Recent reference-based generation methods enable faithful utilization of rich visual attributes contained in references and demonstrate remarkable capability in preserving them.",
                "position": 105
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary: Video Generation with DiT and Rectified Flow",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07951/x3.png",
                "caption": "Figure 3:Overview of the proposedLivingSwapframework for video face swapping.\n(1) Keyframes are used as temporal anchors to ensure consistent identity injection across long sequences.\n(2) We feed the source video as a reference, enabling high-fidelity reconstruction of non-identity attributes such as lighting and expressions.\n(3) By sequentially generating chunks and propagating the final frame of the previous chunk as guidance,LivingSwapachieves seamless transitions in long videos.\n(4) We use Per-frame Edit method to generate the data and reverse data roles to construct paired samples, ensuring reliable and artifact-free learning.",
                "position": 236
            },
            {
                "img": "https://arxiv.org/html/2512.07951/x4.png",
                "caption": "Figure 4:Qualitative comparison with state-of-the-art face-swapping methods.LivingSwapachieves the best overall performance, outperforming both GAN-based and diffusion-based approaches in video consistency, visual fidelity, and identity similarity.\nAlthough our keyframes are generated using Inswapper, the final results produced byLivingSwapare more stable and better preserve source attributes, even in challenging scenarios such as side profiles, occlusions, facial makeup, and complex lighting.",
                "position": 409
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07951/x5.png",
                "caption": "Figure 5:Visualization of theFace2Facedataset.\nThe central plot shows the distribution of identity similarity scores between each swapped video and its corresponding original video, with the lowest 30% (red) and highest 30% (blue) highlighted.\nLow-similarity pairs often contain artifacts and distortions as significant identity discrepancies (left), while high-similarity pairs may contain failed swap frames, causing identity inconsistencies and flickering (right).",
                "position": 734
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07951/x6.png",
                "caption": "Figure 6:Qualitative comparison between the data pairs inFace2Face(by Inswapper[facefusion2025]) and corresponding results generated byLivingSwap.\nBenefiting from reversing the role in data pair and strong priors in pretrained model,LivingSwapsurpasses the quality of its training data, achieving better expression consistency and overall realism.\nUnlike Inswapper-based results, our method avoids local failure cases—such as incomplete swaps, mismatched regions, and occlusion-induced artifacts—demonstrating its strong generalization beyond the training dataset.",
                "position": 996
            }
        ]
    },
    {
        "header": "Appendix AGeneralization Beyond Train Data Quality",
        "images": []
    },
    {
        "header": "Appendix BKeyframe Identity Injection for Accumulated Identity Errors",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07951/x7.png",
                "caption": "Figure 7:Keyframes Identity Injection for resolving Accumulated ID Errors.\nKeyframe Identity Injection for Resolving Accumulated ID Errors.\nWhen using only the first frame for ID injection, face-swapping results suffer from gradually accumulating ID errors over time.\nIn contrast, with Keyframe Identity Injection, each video chunk is corrected individually by swapped keyframe, ensuring better ID consistency throughout the entire long video sequence.",
                "position": 1025
            }
        ]
    },
    {
        "header": "Appendix CRobustness to Keyframe Quality",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07951/x8.png",
                "caption": "Figure 8:Qualitative comparison of using different image-level face swapping models as Per-frame Edit module.\nInjected keyframes often exhibit flaws including artifacts and expression misalignment.\nIn contrast, by directly referencing the source video,LivingSwapsuccessfully refines these flaws using the corresponding source attributes, demonstrating strong robustness to imperfect or corrupted keyframes.",
                "position": 1046
            }
        ]
    },
    {
        "header": "Appendix DRobustness to Identity Differences",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07951/x9.png",
                "caption": "Figure 9:Identity swapping results on the same source video with different target identities.\nOur method produces consistent and high-fidelity face swaps regardless of large or small identity differences, demonstrating strong robustness to identity variations.",
                "position": 1065
            }
        ]
    },
    {
        "header": "Appendix ERobustness to Attribute Variations in Source Video",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07951/x10.png",
                "caption": "Figure 10:Face swapping results on diverse source videos with the same target identity.\nOur method consistently preserves target identity and produces high-fidelity outputs across challenging conditions, including occlusions, side profiles, and complex lighting.",
                "position": 1078
            }
        ]
    },
    {
        "header": "Appendix FGrayscale Keyframe Guidance for Robust Color Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07951/x11.png",
                "caption": "Figure 11:Grayscale keyframe guidance.To avoid incorrect color propagation from imperfect edited keyframes, we modify Video Reference Completion module and convert each keyframe to a grayscale image before VAE encoding.\nThis preserves structural cues (identity, pose, shading) while removing misleading chromatic information, allowing the model to recover accurate colors from the reference video.",
                "position": 1091
            },
            {
                "img": "https://arxiv.org/html/2512.07951/x12.png",
                "caption": "Figure 12:Compared with the originalLivingSwap, using grayscale keyframes effectively suppresses color bleeding (e.g., the blue tint near the ear in the first example) and reduces temporal flickering artifacts (e.g., the dark patches on the head in the second example), leading to more stable and faithful video face swapping results.",
                "position": 1097
            }
        ]
    },
    {
        "header": "Appendix GFace2FaceConstruction Details",
        "images": []
    },
    {
        "header": "Appendix HCineFaceBenchConstruction Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07951/x13.png",
                "caption": "Figure 13:Additional Qualitative Comparison of Different Methods onCineFaceBench.LivingSwapproduces results with higher fidelity and realism compared to other methods.",
                "position": 1143
            },
            {
                "img": "https://arxiv.org/html/2512.07951/x14.png",
                "caption": "Figure 14:Qualitative comparison with recent inpainting-based video face swapping methods[chen2024hifivfs,han2024face]shows that our approach better preserves source video attributes (e.g., lighting and expression) and achieves greater stability under occlusions.",
                "position": 1170
            }
        ]
    },
    {
        "header": "Appendix IComparison with Close-Source Methods",
        "images": []
    }
]