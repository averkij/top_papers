[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.06964/ViPro_Illustration_v2.png",
                "caption": "Figure 1:A simplified illustration for video suppression (left) and video promotion (right). The circle represents the retrieval boundary in which videos can be retrieved by the query (text). The yellow shape indicates the targeted area in which the perturbed videos must reach for successful attacks.Video promotion is more challenging, as it seeks an intersection of the retrieval boundaries for all target queries, while video suppression only pushes videos outside the retrieval boundary.",
                "position": 73
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.06964/overview_v2.png",
                "caption": "Figure 2:An illustration of our ViPro with MoRe. (1)Temporal Clipping:Video frames are clustered into video clips‚ÑÇ=[ùêÇ1,‚Ä¶,ùêÇM]\\mathbb{C}=[\\mathbf{C}_{1},...,\\mathbf{C}_{M}]based on frame-to-frame similarityùêñùêó\\mathbf{W}_{\\mathbf{X}}. (2)Semantical Weighting:For each clip and each query, we calculate its frame-to-frame similarityùêñùêÇi\\mathbf{W}_{\\mathbf{C}_{i}}and frame-to-query similarityùêñùêÇi,q\\mathbf{W}_{\\mathbf{C}_{i},q}using cosine similarity between all framesxj‚ààùêÇix_{j}\\in\\mathbf{C}_{i}and all query tokensti‚ààqt_{i}\\in{q}. Frames and queries with low similarity are suppressed by their corresponding weights during optimization. 3)Clip-wise Optimization:Perturbation are optimized as per clip before outputting the finalŒ¥Ci\\delta_{C_{i}}for adversarial videoùêó‚Ä≤\\mathbf{X^{\\prime}}.",
                "position": 153
            },
            {
                "img": "https://arxiv.org/html/2508.06964/qw_visualization_v2.png",
                "caption": "Figure 3:An illustration of the effectiveness of MoRe in guiding optimization. Due to the conflicting gradients,Œ¥\\deltain the left figure leads to suboptimal results without entering any boundary. After applying MoRe, the anomalous query (q3q_{3}) is suppressed,yielding a more focused perturbationŒ¥~\\tilde{\\delta}to push videos towards the target area(yellow shape).",
                "position": 405
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.06964/x1.png",
                "caption": "Figure 4:The impact of varying numbers of training queries for white-box (solid lines) and grey-box (dashed lines) ViPro on all datasets (top row) and all models (bottom row). We presentŒî\\DeltaR@1 (red) andŒî\\DeltaR@5 (orange).",
                "position": 1498
            },
            {
                "img": "https://arxiv.org/html/2508.06964/x2.png",
                "caption": "Figure 5:Ablation studies on PGD hyperparameters, showing results for max PGD stepsŒ∑\\eta, step sizeŒ±\\alpha, and perturbation boundœµ\\epsilon, respectively. Results are presented in R@1 changes after attacks, denoted asŒî\\DeltaR@1. R@5 results are not presented for better visibility.",
                "position": 1502
            },
            {
                "img": "https://arxiv.org/html/2508.06964/Visulization.png",
                "caption": "Figure 6:Visualization of original frames and manipulated frames for all attacks (white-box). From top to bottom:Clean, SGA, Co-Attack, ViPro (ours). SGA has the most obvious patterns and artifacts. Co-Attack has on-par stealthiness as our ViPro with minor visual clues, whileour ViPro remains stealthy with the best performance.",
                "position": 1512
            },
            {
                "img": "https://arxiv.org/html/2508.06964/data_sim_dist.png",
                "caption": "Figure 7:Visualization of white-box top 1 text-to-video similarity scores for all datasets. All results are normalized between 0 to 1 for better visibility.",
                "position": 1524
            },
            {
                "img": "https://arxiv.org/html/2508.06964/model_sim_dist.png",
                "caption": "Figure 8:Visualization of white-box top 1 text-to-video similarity scores for all models. Because the embeddings vary significantly across models, we only present normalized top 1 scores for better visibility.",
                "position": 1537
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.06964/query_stealing.png",
                "caption": "Figure 9:An illustration of data stealing:i. Attackers can obtain a diverse subset of training videos and corresponding texts by querying the victim model.ii. Attackers can perform a candidate-wise sortation foreachcandidate video by querying the candidate video with the texts from stepi, checking the rank of the candidate video, and categorizing the texts intoRelevantandIrrelevant.",
                "position": 2525
            },
            {
                "img": "https://arxiv.org/html/2508.06964/clip_visual.png",
                "caption": "Figure 10:An illustration of the clipped video frames through temporal clipping. All temporally related frames are grouped into clips without creating too fragmented videos, i.e., only 1-2 frames per clip.",
                "position": 2529
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]