[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06771/0EKYU1QY4.jpeg",
                "caption": "(a)Clean humor",
                "position": 117
            },
            {
                "img": "https://arxiv.org/html/2509.06771/0EKYU1QY4.jpeg",
                "caption": "(a)Clean humor",
                "position": 120
            },
            {
                "img": "https://arxiv.org/html/2509.06771/IZ800O2CA.jpeg",
                "caption": "(b)Clean humor",
                "position": 125
            },
            {
                "img": "https://arxiv.org/html/2509.06771/345908.jpeg",
                "caption": "(c)Dark humor",
                "position": 131
            },
            {
                "img": "https://arxiv.org/html/2509.06771/0BX203L6M.jpeg",
                "caption": "(d)Dark humor",
                "position": 136
            }
        ]
    },
    {
        "header": "IILiterature Review",
        "images": []
    },
    {
        "header": "IIIDataset Description and Annotation Process",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06771/6UW4W2IX9.jpeg",
                "caption": "(a)Dark Humor: NoTarget: N/AIntensity: N/A",
                "position": 203
            },
            {
                "img": "https://arxiv.org/html/2509.06771/6UW4W2IX9.jpeg",
                "caption": "(a)Dark Humor: NoTarget: N/AIntensity: N/A",
                "position": 206
            },
            {
                "img": "https://arxiv.org/html/2509.06771/2NNR4FTY4.jpeg",
                "caption": "(b)Dark Humor: YesTarget: DisabilityIntensity: 2",
                "position": 213
            },
            {
                "img": "https://arxiv.org/html/2509.06771/349286.jpeg",
                "caption": "(c)Dark Humor: YesTarget: Gender/SexIntensity: 3",
                "position": 220
            },
            {
                "img": "https://arxiv.org/html/2509.06771/506006.jpeg",
                "caption": "(d)Dark Humor: YesTarget: Race/EthnicityIntensity: 3",
                "position": 227
            },
            {
                "img": "https://arxiv.org/html/2509.06771/825996.jpeg",
                "caption": "(e)Dark Humor: YesTarget: Gender/SexIntensity: 2",
                "position": 235
            },
            {
                "img": "https://arxiv.org/html/2509.06771/UZO6DZH5A.jpeg",
                "caption": "(f)Dark Humor: YesTarget: Mental HealthIntensity: 2",
                "position": 242
            },
            {
                "img": "https://arxiv.org/html/2509.06771/MUSDLJVI6.jpeg",
                "caption": "(g)Dark Humor: YesTarget: DisabilityIntensity: 2",
                "position": 249
            },
            {
                "img": "https://arxiv.org/html/2509.06771/vasdbf.jpg",
                "caption": "(h)Dark Humor: YesTarget: Violence/DeathIntensity: 3",
                "position": 256
            }
        ]
    },
    {
        "header": "IVMethodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06771/Frame_38.png",
                "caption": "Figure 3:Overview of the proposed reasoning‐augmented multimodal architecture. Each meme (image + OCR text) is first processed by VLM to generate and refine a structured explanation via the Role-Reversal Self-Loop mechanism. Text encoders encode the OCR transcript and the self-refined explanation into textual embeddings, while the image encoder extracts visual embeddings from the meme image. A Tri‐stream Cross‐Reasoning Network (TCRNet) then applies pairwise scaled dot-product attention across the three modalities, text, image, and reasoning, to produce attended representations. These representations are concatenated into a unified feature vector, which is passed through a classification head to predict dark humor presence, target category, and intensity.",
                "position": 611
            },
            {
                "img": "https://arxiv.org/html/2509.06771/Self_Loop_Final.png",
                "caption": "Figure 4:Self-loop mechanism using Role Reversal (RR) Prompting.The input meme is passed to a language model which generates an initial explanation. This explanation is then refined iteratively through Role Reversal prompting, where the model assumes the role of a reviewer to critique and improve its own output. AfterNNsuch refinement cycles, a final explanation is obtained. This mechanism enables the model to simulate self-awareness and improve explanatory coherence without external supervision.",
                "position": 680
            }
        ]
    },
    {
        "header": "VResults and Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06771/ch1.png",
                "caption": "(a)Dark humor",
                "position": 1214
            },
            {
                "img": "https://arxiv.org/html/2509.06771/ch1.png",
                "caption": "(a)Dark humor",
                "position": 1217
            },
            {
                "img": "https://arxiv.org/html/2509.06771/ch2.png",
                "caption": "(b)Target identification",
                "position": 1222
            },
            {
                "img": "https://arxiv.org/html/2509.06771/ch3.png",
                "caption": "(c)Intensity prediction",
                "position": 1227
            }
        ]
    },
    {
        "header": "VIAblation Study",
        "images": []
    },
    {
        "header": "VIIConclusion",
        "images": []
    },
    {
        "header": "Ethical Consideration",
        "images": []
    },
    {
        "header": "Reddit Ethical policy for scraping the memes",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]