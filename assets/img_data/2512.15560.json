[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.15560/images/logo.png",
                "caption": "",
                "position": 97
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.15560/x1.png",
                "caption": "Figure 1:An overview of our complete framework, integrating our evaluation and development pipelines. Figure (a) illustrates the TED Evaluation Framework, consisting of the TED-6K benchmark and a context aggregator to assess the representational capabilities of text encoders. Figure (b) shows the construction of our TED Encoder, where Qwen3-VL-8B-Instruct is fine-tuned on a curated VQA and captioning dataset to specialize the MLLM. Figure (c) depicts our final GRAN-TED solution, which incorperates a learnable layer-wise weighting module to generate GRAN-TED for diffusion models.",
                "position": 190
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.15560/x2.png",
                "caption": "Figure 2:Left.The data construction pipeline for TED-6K, consisting of four stages: (1) Data Curation and Filtering; (2) Base Caption Generation; (3) Semantic Pair Construction; (4) Human Verification.Right.The data Composition of the TED-6K dataset.",
                "position": 209
            },
            {
                "img": "https://arxiv.org/html/2512.15560/x3.png",
                "caption": "Figure 3:The context aggregator architecture and its training&inference process.\n(a) training process of the context aggregator. (b) the inference process during evaluation on TED-6K.",
                "position": 278
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.15560/x4.png",
                "caption": "Table 3:Correlation between our benchmark scores and T2I generation performance. All evaluated encoders are instruction models, except for UMT5-XXL.",
                "position": 592
            },
            {
                "img": "https://arxiv.org/html/2512.15560/x4.png",
                "caption": "Table 4:Correlation between our benchmark scores and T2V generation performance.",
                "position": 650
            },
            {
                "img": "https://arxiv.org/html/2512.15560/x4.png",
                "caption": "Figure 4:Dynamics of the learnable layer weights over the course of continuous training (i.e., without the two-step strategy). The weight values shown are normalized via Softmax.",
                "position": 709
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix AAggregator Architecture",
        "images": []
    },
    {
        "header": "Appendix BTheoretical Justification for the Two-Step Training Strategy",
        "images": []
    },
    {
        "header": "Appendix CExperimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.15560/x5.png",
                "caption": "Figure 5:Examples of visual question answering (VQA) training samples,\ncovering both image-based and video-based settings.",
                "position": 1148
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]