[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.22668/extracted/6316146/images/teaser.png",
                "caption": "Figure 1:Co-speech gestures supplement the spoken language – we show examples for three phrases here, with common words. Learning to associate gestures with the uttered phrases is essential for a holistic understanding of human communication.",
                "position": 108
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.22668/extracted/6316146/images/arch.png",
                "caption": "Figure 2:TheJEGALarchitecture. The three input modalities (video, text, speech) are each encoded with a modality-specific encoder, followed by a fusion block to merge speech and text representations. The encoder outputs are average-pooled to obtain global (phrase-level) gesture and speech-text embeddings. During training, these provide the inputs for the global ‘phrase contrastive loss’. The gesture alignment module aggregates the relevant video frames to obtain a local word-level gesture embedding for each speech-text word. During training, these provide the inputs for the local ‘gesture-word coupling loss’. The two losses encourage the learning of global and local correspondences between the three modalities.",
                "position": 127
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Downstream Tasks and Evaluation",
        "images": []
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.22668/extracted/6316146/images/spotting.png",
                "caption": "Figure 3:JEGAL can spot the gestured words in a video clip. Here, we show a similarity heatmap of words vs video frames. The vertical yellow lines indicate the speech-based word boundaries of ‘beautiful’ and ‘energy’. The red triangles zoom into the corresponding frames where JEGAL detects the words, clearly aligning with the gestures. For the word “beautiful” the gestured segment is smaller than the spoken word boundary and for the word “energy” it extends well beyond to the right of the word boundary. The alignment layer (Sec3.6) allows the model to look beyond just the speech-based boundaries. Note that our model learns to perform gestured-word spotting without using any training labels on which words are gestured.",
                "position": 827
            }
        ]
    },
    {
        "header": "6Insights and Ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.22668/extracted/6316146/images/stress_sample.png",
                "caption": "Figure 4:Audio and text heatmap examples when words are stressed.",
                "position": 1029
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMost Gestured Words",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.22668/extracted/6316146/images/wordcloud.png",
                "caption": "Figure 5:Word cloud for the most commonly gestured words.",
                "position": 2019
            }
        ]
    },
    {
        "header": "Appendix BModel Details",
        "images": []
    },
    {
        "header": "Appendix CDataset Visualization",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.22668/extracted/6316146/images/spotting_samples.png",
                "caption": "Figure 6:Visualization of theAVS-Spotdataset, showcasing video frames from different samples. Each row corresponds to a single video, with the highlighted keyword indicating the annotated gestured word for spotting. The figure illustrates the dataset’s diversity, featuring a wide range of unique keywords, various speakers, and distinct gestures.",
                "position": 2267
            }
        ]
    },
    {
        "header": "Appendix DQualitative Results",
        "images": []
    },
    {
        "header": "Appendix ELimitations and Areas of Improvement",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.22668/extracted/6316146/images/additional_spotting_results.png",
                "caption": "Figure 7:Additional gestured word spotting results on AVS-Spot dataset. Keywords are highlighted in red on the left panel and the speech-based force alignment word boundaries are marked by yellow lines. JEGAL successfully spots the gestured keywords, demonstrating its robustness across diverse gestures and speakers. The red triangles zoom into the corresponding frames where JEGAL detects the keywords, clearly aligning with the gestures. Note that in some cases (e.g., rows2222and3333), ground-truth boundaries may slightly differ, as the speaker can gesture and utter the same word at slightly different times. JEGAL effectively estimates the approximate intervals where the target word is gestured.",
                "position": 2294
            },
            {
                "img": "https://arxiv.org/html/2503.22668/extracted/6316146/images/additional_stress_results.png",
                "caption": "Figure 8:Examples highlighting the role of stressed speech regions in audio-based gesture spotting. The audio-only model successfully detects the stressed keywords “every” and “respond”, whereas the text-only model misses these words. Evidently, the audio-only model looks for word emphasis cues (indicated by high pitch) as such words are more likely to be gestured. This would be difficult to infer from text modality alone. These examples illustrate the advantages of leveraging audio cues for gesture spotting.",
                "position": 2297
            }
        ]
    },
    {
        "header": "Appendix FPotential Negative Societal Impacts",
        "images": []
    }
]