[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23747/x1.png",
                "caption": "Figure 1:We proposeSpatial-MLLM, a method that significantly enhances the visual-based spatial intelligence of existing video MLLMs. As shown, Spatial-MLLM is capable of understanding and reasoning about the underlying scene from video input, achieving state-of-the-art performance across a wide range of tasks.",
                "position": 94
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23747/x2.png",
                "caption": "Figure 2:Overview of Spatial-MLLM. Our model is composed of a 2D visual encoder‚Ñ∞2Dsubscript‚Ñ∞2D\\mathcal{E}_{\\text{2D}}caligraphic_E start_POSTSUBSCRIPT 2D end_POSTSUBSCRIPT, a spatial encoder‚Ñ∞Spatialsubscript‚Ñ∞Spatial\\mathcal{E}_{\\text{Spatial}}caligraphic_E start_POSTSUBSCRIPT Spatial end_POSTSUBSCRIPT, which is initialized from a feed-forward visual geometry foundation model, a connector, and a large language model backbone. At inference time, we incorporate a space-aware frame sampling strategy to select spatially informative frames when the number of input frames is limited due to GPU memory constraints.",
                "position": 179
            },
            {
                "img": "https://arxiv.org/html/2505.23747/x3.png",
                "caption": "Figure 3:Basic statistic of our constucted Spatial-MLLM-120K dataset.",
                "position": 306
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23747/x4.png",
                "caption": "Figure 4:Visualization of Training Curves in the SFT and RL Stages. For the SFT stage, we present the mean token accuracy and loss curves. For the RL stage, we show the dynamics of completion length and reward.",
                "position": 833
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Technical Appendices and Supplementary Material",
        "images": []
    },
    {
        "header": "Appendix AAdditional Method Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23747/x5.png",
                "caption": "Figure 5:Illustration of the prompts used in the SFT and GRPO stages.We use the default system prompt of Qwen2.5-VL[14](i.e.,, \"You are a helpful assistant\") for both stages. In the SFT stage, the user prompt consists of a question and a type template. In the GRPO stage, the user prompt includes a question, a question post string, and a type template.",
                "position": 1256
            }
        ]
    },
    {
        "header": "Appendix BAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23747/x6.png",
                "caption": "Figure 6:Visualization of different frame sampling strategies.For clarity of visualization, we setNm=128subscriptùëÅùëö128N_{m}=128italic_N start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT = 128andNk=8subscriptùëÅùëò8N_{k}=8italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = 8in this example. Uniform frame sampling often overlooks transient regions that appear briefly in the video. Furthermore, when the camera remains static for extended periods, this strategy tends to yield redundant viewpoints. In contrast, our proposed space-aware frame sampling strategy achieves more comprehensive spatial coverage.",
                "position": 1346
            },
            {
                "img": "https://arxiv.org/html/2505.23747/x7.png",
                "caption": "Figure 7:Qualitative example on VSI-Bench[18].",
                "position": 1748
            },
            {
                "img": "https://arxiv.org/html/2505.23747/x8.png",
                "caption": "Figure 8:Qualitative example on VSI-Bench[18].",
                "position": 1751
            },
            {
                "img": "https://arxiv.org/html/2505.23747/x9.png",
                "caption": "Figure 9:Qualitative example on VSI-Bench[18].",
                "position": 1754
            },
            {
                "img": "https://arxiv.org/html/2505.23747/x10.png",
                "caption": "Figure 10:Qualitative example on VSI-Bench[18].",
                "position": 1757
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]