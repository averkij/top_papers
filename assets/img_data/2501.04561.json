[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04561/extracted/6119121/figs/logo.png",
                "caption": "",
                "position": 86
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04561/x1.png",
                "caption": "Figure 1:Overview of the motivation and architecture of OpenOmni. For simplicity, our core architecture is presented without the connectors between modules.",
                "position": 131
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04561/x2.png",
                "caption": "Figure 2:Training process of OpenOmni.To facilitate the omnimodal learining and real-time emotional speech generation of OpenOmni, we employ a three-stage training process: (1)Speech-Text Generation: In this stage, we utilize a speech encoder to extract continuous speech features and text features for alignment learning, which enables the large language model to develop speech understanding capabilities. (2)Image-Text Generation: This stage involves the use of an image encoder to extract continuous image features and text features from OpenOmni, facilitating alignment learning that enhances OpenOmniâ€™s image comprehension and instruction-following abilities. Additionally, this process achieves implicit omnimodal alignment, granting the model the capacity for omni-understanding. (3)Speech Generation: In the final stage, we train a lightweight speech decoder using high-quality synthesized speech dialogue data, focusing on optimizing emotional speech preferences directly. This enables OpenOmni to generate real-time self-aware emotional speech.",
                "position": 185
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04561/x3.png",
                "caption": "Figure 3:Structure of speech decoder. The speech decoder consists of a mixture of expert module and multiple transformer layers, achieving end-to-end speech unit learning through connectionist temporal classification (CTC) loss.",
                "position": 296
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Data Construction",
        "images": []
    },
    {
        "header": "7Additional Experiments",
        "images": []
    },
    {
        "header": "8Additional Ablation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04561/extracted/6119121/figs/openomni_add.jpg",
                "caption": "Figure 4:Overview of Speech Decoder Mode.OpenOmni supports both autoregressive (AR) and non-autoregressive speech (NAR) generation. The NAR mode uses CTC loss modeling and a 6K speech vocabulary size to enable real-time parallel speech decoding generation. The AR mode uses NTP loss modeling and a speech vocabulary size of 16K to support streaming decoding and higher-quality speech generation. In order to make the training of speech generator more stable and easy, we design a text-guided output feature fusion method to ensure the correctness of semantic alignment in speech generation modeling.",
                "position": 2066
            }
        ]
    },
    {
        "header": "9Additional Implementation Details",
        "images": []
    }
]