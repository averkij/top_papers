[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13082/x1.png",
                "caption": "",
                "position": 84
            }
        ]
    },
    {
        "header": "IIntroduction",
        "images": []
    },
    {
        "header": "IIRelated works",
        "images": []
    },
    {
        "header": "IIIOur method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13082/x2.png",
                "caption": "Figure 2:FreeGrasppipeline. (a) The setup considered for the robotic reasoning and grasping task and (b) the proposed pipeline that leverages pre-trained VLMs in a zero-shot manner without additional training.",
                "position": 190
            },
            {
                "img": "https://arxiv.org/html/2503.13082/x3.png",
                "caption": "Figure 3:Object localization performance with different VLM-based method on MetaGrasNetv2[16].",
                "position": 236
            }
        ]
    },
    {
        "header": "IVFree-form language grasping dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13082/x4.png",
                "caption": "Figure 4:Examples ofFreeGraspDataat different task difficulties with three user-provided instructions.★indicates the target object, and●indicates the ground-truth objects to pick.",
                "position": 365
            },
            {
                "img": "https://arxiv.org/html/2503.13082/x5.png",
                "caption": "Figure 5:Similarity distribution among the three user-defined instructions used in theFreeGraspDatascenarios.",
                "position": 368
            }
        ]
    },
    {
        "header": "VExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13082/x6.png",
                "caption": "Figure 6:Samples from real-world experiments for different task difficulties.★indicates the user-described target object, and●are the GT objects to pick.",
                "position": 1270
            },
            {
                "img": "https://arxiv.org/html/2503.13082/x7.png",
                "caption": "Figure 7:Examples of three successful scenarios withFreeGrasp:\nAt the top, we show a real-world case where segmentation, pose, and motion were all correctly identified. In the middle, we present another real-world scenario where segmentation was incorrect, but GraspNet was still able to identify the correct pose to remove the obstacle. At the bottom, we display a successful example from aFreeGraspDatascenario, featuring a highly cluttered environment.",
                "position": 1273
            },
            {
                "img": "https://arxiv.org/html/2503.13082/x8.png",
                "caption": "",
                "position": 1277
            }
        ]
    },
    {
        "header": "VIConclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]