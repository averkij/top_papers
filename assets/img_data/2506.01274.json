[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Proposed Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01274/x1.png",
                "caption": "Figure 1:Pipeline overview ofReFoCUS. The policy modelπθsubscript𝜋𝜃\\pi_{\\theta}italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPTsamplesN𝑁Nitalic_Ncandidate frame subsetsF𝐹Fitalic_Ffrom the input videov𝑣vitalic_vand questionq𝑞qitalic_q, and the reward modelrφsubscript𝑟𝜑r_{\\varphi}italic_r start_POSTSUBSCRIPT italic_φ end_POSTSUBSCRIPTevaluates each subset using its prediction confidence, producing reward signals to trainπθsubscript𝜋𝜃\\pi_{\\theta}italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPTvia policy gradient.",
                "position": 211
            },
            {
                "img": "https://arxiv.org/html/2506.01274/x2.png",
                "caption": "Figure 2:Distribution of reward varianceVar⁢(m)Var𝑚\\mathrm{Var}(m)roman_Var ( italic_m )(prediction margin) across 962K QA pairs. We observe that many samples yield low variance, indicating weak sensitivity to visual input. We filter out such cases (<τ=0.21absent𝜏0.21<\\tau=0.21< italic_τ = 0.21) to retain a high-quality subset for policy learning.",
                "position": 238
            },
            {
                "img": "https://arxiv.org/html/2506.01274/x3.png",
                "caption": "Figure 3:Overview of theReFoCUSframework. Given a video and query,πθsubscript𝜋𝜃\\pi_{\\theta}italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPTautoregressively selectsN𝑁Nitalic_Nframe subsets, which are then scored byrφsubscript𝑟𝜑r_{\\varphi}italic_r start_POSTSUBSCRIPT italic_φ end_POSTSUBSCRIPTbased on their answer prediction margins. The resulting rewards guide frame-level policy optimization via reinforcement learning.",
                "position": 273
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01274/x4.png",
                "caption": "Figure 4:Prediction ratio relative to the baseline accuracy, across over-k% (dashed) and under-k% (solid) frame subsets from each bin.",
                "position": 745
            },
            {
                "img": "https://arxiv.org/html/2506.01274/x5.png",
                "caption": "Figure 5:Result of V-NIAH. (a) Uniform sampling (b) Frame selection ofReFoCUS. Thex𝑥xitalic_x-axis denotes the total #video frames, and they𝑦yitalic_y-axis indicates the relative position of the needle frame.",
                "position": 768
            },
            {
                "img": "https://arxiv.org/html/2506.01274/x6.png",
                "caption": "Figure 6:Temporal entropy of selected frames over training steps for different selectionT′∈{4,8,32}superscript𝑇′4832T^{\\prime}\\in\\{4,8,32\\}italic_T start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ∈ { 4 , 8 , 32 }. SmallerT′superscript𝑇′T^{\\prime}italic_T start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPTlead to more focused selections.",
                "position": 902
            }
        ]
    },
    {
        "header": "5Discussion and Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AVideo Data Processing for ReFoCUS",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01274/x7.png",
                "caption": "Figure 7:Visualization of the temporal segmentation and sampling strategy. We divide each video into8888overlapping windowsW1subscript𝑊1W_{1}italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTtoW8subscript𝑊8W_{8}italic_W start_POSTSUBSCRIPT 8 end_POSTSUBSCRIPT(top), and for each window, define a complementary regionCisubscript𝐶𝑖C_{i}italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT(bottom). We uniformly sample frames from both regions to construct16161616candidate subsets per QA pair.",
                "position": 1807
            }
        ]
    },
    {
        "header": "Appendix BImplementation & Training Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01274/x8.png",
                "caption": "Figure 8:Overview of a simplified policy model (omitting the vision encoder) with initialization constants. Here,γ𝛾\\gammaitalic_γindicates the RMSNorm weight parameter, andα𝛼\\alphaitalic_αdenotes the orthogonal initialization gain for the linear projections in the Key, Query, and Value heads.",
                "position": 1927
            }
        ]
    },
    {
        "header": "Appendix CAnalysis of Diversity in Policy Frame Selection",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01274/x9.png",
                "caption": "Figure 9:Red dashed line indicates the visual cues to answer the given question.",
                "position": 2215
            },
            {
                "img": "https://arxiv.org/html/2506.01274/x10.png",
                "caption": "Figure 10:Red dashed line indicates the visual cues to answer the given question.",
                "position": 2219
            },
            {
                "img": "https://arxiv.org/html/2506.01274/x11.png",
                "caption": "Figure 11:Red dashed line indicates the visual cues to answer the given question.",
                "position": 2223
            },
            {
                "img": "https://arxiv.org/html/2506.01274/x12.png",
                "caption": "Figure 12:Red dashed line indicates the visual cues to answer the given question.",
                "position": 2227
            },
            {
                "img": "https://arxiv.org/html/2506.01274/x13.png",
                "caption": "Figure 13:Red dashed line indicates the visual cues to answer the given question.",
                "position": 2231
            }
        ]
    },
    {
        "header": "Appendix DQualitative Results",
        "images": []
    }
]