[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Proposed Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01274/x1.png",
                "caption": "Figure 1:Pipeline overview ofReFoCUS. The policy modelÏ€Î¸subscriptğœ‹ğœƒ\\pi_{\\theta}italic_Ï€ start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPTsamplesNğ‘Nitalic_Ncandidate frame subsetsFğ¹Fitalic_Ffrom the input videovğ‘£vitalic_vand questionqğ‘qitalic_q, and the reward modelrÏ†subscriptğ‘Ÿğœ‘r_{\\varphi}italic_r start_POSTSUBSCRIPT italic_Ï† end_POSTSUBSCRIPTevaluates each subset using its prediction confidence, producing reward signals to trainÏ€Î¸subscriptğœ‹ğœƒ\\pi_{\\theta}italic_Ï€ start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPTvia policy gradient.",
                "position": 211
            },
            {
                "img": "https://arxiv.org/html/2506.01274/x2.png",
                "caption": "Figure 2:Distribution of reward varianceVarâ¢(m)Varğ‘š\\mathrm{Var}(m)roman_Var ( italic_m )(prediction margin) across 962K QA pairs. We observe that many samples yield low variance, indicating weak sensitivity to visual input. We filter out such cases (<Ï„=0.21absentğœ0.21<\\tau=0.21< italic_Ï„ = 0.21) to retain a high-quality subset for policy learning.",
                "position": 238
            },
            {
                "img": "https://arxiv.org/html/2506.01274/x3.png",
                "caption": "Figure 3:Overview of theReFoCUSframework. Given a video and query,Ï€Î¸subscriptğœ‹ğœƒ\\pi_{\\theta}italic_Ï€ start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPTautoregressively selectsNğ‘Nitalic_Nframe subsets, which are then scored byrÏ†subscriptğ‘Ÿğœ‘r_{\\varphi}italic_r start_POSTSUBSCRIPT italic_Ï† end_POSTSUBSCRIPTbased on their answer prediction margins. The resulting rewards guide frame-level policy optimization via reinforcement learning.",
                "position": 273
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01274/x4.png",
                "caption": "Figure 4:Prediction ratio relative to the baseline accuracy, across over-k% (dashed) and under-k% (solid) frame subsets from each bin.",
                "position": 745
            },
            {
                "img": "https://arxiv.org/html/2506.01274/x5.png",
                "caption": "Figure 5:Result of V-NIAH. (a) Uniform sampling (b) Frame selection ofReFoCUS. Thexğ‘¥xitalic_x-axis denotes the total #video frames, and theyğ‘¦yitalic_y-axis indicates the relative position of the needle frame.",
                "position": 768
            },
            {
                "img": "https://arxiv.org/html/2506.01274/x6.png",
                "caption": "Figure 6:Temporal entropy of selected frames over training steps for different selectionTâ€²âˆˆ{4,8,32}superscriptğ‘‡â€²4832T^{\\prime}\\in\\{4,8,32\\}italic_T start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT âˆˆ { 4 , 8 , 32 }. SmallerTâ€²superscriptğ‘‡â€²T^{\\prime}italic_T start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPTlead to more focused selections.",
                "position": 902
            }
        ]
    },
    {
        "header": "5Discussion and Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AVideo Data Processing for ReFoCUS",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01274/x7.png",
                "caption": "Figure 7:Visualization of the temporal segmentation and sampling strategy. We divide each video into8888overlapping windowsW1subscriptğ‘Š1W_{1}italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTtoW8subscriptğ‘Š8W_{8}italic_W start_POSTSUBSCRIPT 8 end_POSTSUBSCRIPT(top), and for each window, define a complementary regionCisubscriptğ¶ğ‘–C_{i}italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT(bottom). We uniformly sample frames from both regions to construct16161616candidate subsets per QA pair.",
                "position": 1807
            }
        ]
    },
    {
        "header": "Appendix BImplementation & Training Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01274/x8.png",
                "caption": "Figure 8:Overview of a simplified policy model (omitting the vision encoder) with initialization constants. Here,Î³ğ›¾\\gammaitalic_Î³indicates the RMSNorm weight parameter, andÎ±ğ›¼\\alphaitalic_Î±denotes the orthogonal initialization gain for the linear projections in the Key, Query, and Value heads.",
                "position": 1927
            }
        ]
    },
    {
        "header": "Appendix CAnalysis of Diversity in Policy Frame Selection",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01274/x9.png",
                "caption": "Figure 9:Red dashed line indicates the visual cues to answer the given question.",
                "position": 2215
            },
            {
                "img": "https://arxiv.org/html/2506.01274/x10.png",
                "caption": "Figure 10:Red dashed line indicates the visual cues to answer the given question.",
                "position": 2219
            },
            {
                "img": "https://arxiv.org/html/2506.01274/x11.png",
                "caption": "Figure 11:Red dashed line indicates the visual cues to answer the given question.",
                "position": 2223
            },
            {
                "img": "https://arxiv.org/html/2506.01274/x12.png",
                "caption": "Figure 12:Red dashed line indicates the visual cues to answer the given question.",
                "position": 2227
            },
            {
                "img": "https://arxiv.org/html/2506.01274/x13.png",
                "caption": "Figure 13:Red dashed line indicates the visual cues to answer the given question.",
                "position": 2231
            }
        ]
    },
    {
        "header": "Appendix DQualitative Results",
        "images": []
    }
]