[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.21850/x1.png",
                "caption": "Figure 1:Compositional Complexity Comparison.Comparison between visual instruction tuning data (LLaVA-665K[24]VIT) and our compositional tuning data (COMPACT). The VIT data is dominated by simple queries (k=1ùëò1k=1italic_k = 1), while ourCOMPACTdata is balanced across compositional complexity levels (k=1,2,3ùëò123k=1,2,3italic_k = 1 , 2 , 3).",
                "position": 484
            },
            {
                "img": "https://arxiv.org/html/2504.21850/x2.png",
                "caption": "Figure 2:COMPACT‚Äôs Data Generation Pipeline.(Left):We samplek‚àà{1,2,3}ùëò123k\\in\\{1,2,3\\}italic_k ‚àà { 1 , 2 , 3 }atomic capabilities such as color, object recognition, and spatial relationship.(Center):We generate questions that integrate allkùëòkitalic_ksampled capabilities.(Right):We verify the quality of generated conversations and combine them with instruction tuning data to maintain instruction following capability. This structured data recipe explicitly models atomic-to-complex learning procedure, in contrast to standardLLaVA-665K[24]VIT that promotes learning from simple queries.",
                "position": 514
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.21850/x3.png",
                "caption": "Figure 3:Performance Across Compositional Tuning Data Scales.We fix the VIT subset (5% ofLLaVA-665K[24]) and scale the compositional tuning data inCOMPACTfrom 2K to 32K. For comparison, we remove the compositional tuning data and add more VIT data (2K-32K) instead to prepare VIT only recipes with equal data budgets.COMPACT(solid lines) consistently outperformsLLaVA-665K[24]VIT (dashed lines) with fewer data. The performance gap is pronounced for complex reasoning benchmarks such as MM-Vet and MMStar, where the 8KCOMPACTmodel often exceeds theLLaVA-665K[24]VIT baseline at 32K. This demonstrates the data efficiency ofCOMPACT, requiring substantially less data thanLLaVA-665K[24]VIT to achieve comparable or better results.",
                "position": 823
            },
            {
                "img": "https://arxiv.org/html/2504.21850/x4.png",
                "caption": "Figure 4:Compositional Generalization to Higher-Complexities.Performance comparison across compositional complexities (kùëòkitalic_k).COMPACTshows competitive performance againstLLaVA-665K[24]VIT training. It exceeds theLLaVA-665K[24]baseline at higher compositional complexity tasks (k=4ùëò4k=4italic_k = 4andk=5ùëò5k=5italic_k = 5) while using significantly less training data. Thekùëòkitalic_k-distribution rows show the distribution of compositional complexities in each benchmark.",
                "position": 853
            },
            {
                "img": "https://arxiv.org/html/2504.21850/extracted/6402656/fig/8_sec45_C_ours.png",
                "caption": "Figure 5:Comparison of Capability Distribution.The heatmaps show the frequency of each atomic capability in LLaVA (left) andCOMPACT(right) samples. The capabilities are sorted by frequency based on the LLaVA capability distribution, with more common capabilities appearing closer to the top. In LLaVA, the distribution is notably imbalanced: object recognition and scene understanding are some of the most frequent, while shape and spatial recognition are less prevalent. In contrast, ourCOMPACTexhibits a more balanced distribution across all capability categories.",
                "position": 892
            },
            {
                "img": "https://arxiv.org/html/2504.21850/x5.png",
                "caption": "Figure 6:Leave-One-Out Analysis on Atomic Capabilities.We measure the average performance degradation across benchmarks by excluding an atomic capability from training. Higher drop indicates higher importance of the atomic capability. Excluding scene understanding and spatial relationships have the largest impact, while that of excluding shape and action recognition are modest.",
                "position": 980
            },
            {
                "img": "https://arxiv.org/html/2504.21850/x6.png",
                "caption": "Figure 7:Compositional Complexity Analysis:Performance comparison of models trained with different compositional complexities.k=1ùëò1k=1italic_k = 1refers to only one atomic capability per question,k=1,2ùëò12k=1,2italic_k = 1 , 2to both single and dual capabilities, andk=1,2,3ùëò123k=1,2,3italic_k = 1 , 2 , 3to single, dual, and triple capabilities. Results show consistent improvements as the range of compositional complexities increases.",
                "position": 996
            },
            {
                "img": "https://arxiv.org/html/2504.21850/x7.png",
                "caption": "Figure 8:Impact of Instruction Tuning Data\nRatio on Performance.Relative performance of models trained onCOMPACTmixed with different ratios instruction tuning data fromLLaVA-665K[24]. The x-axis is the percentage ofLLaVA-665K[24]used as instruction tuning data, and the y-axis is the average relative score across benchmarks. The performance improves significantly with a small percentage of instruction tuning data and stabilizes around¬†5%.",
                "position": 1011
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.21850/extracted/6402656/fig/2_conv_per_img.png",
                "caption": "Figure 9:Distribution of conversations per image inLLaVA-665K.The overwhelming majority of images (97.69%) have‚â§\\leq‚â§20 conversation pairs. The average of number of conversations per image is 5.18 (œÉ=5.62ùúé5.62\\sigma=5.62italic_œÉ = 5.62).\nA small subset (2.31%) exceeds 20 conversations, which includes a sample with the maximum length of 275. Total conversations:3,444,24634442463,\\!444,\\!2463 , 444 , 246.",
                "position": 1656
            },
            {
                "img": "https://arxiv.org/html/2504.21850/x8.png",
                "caption": "Figure 10:Distribution of Compositional Complexities inLLaVA-665Ksamples.Majority of questions (59.2%) use one atomic capability, followed by 30.9% using two.",
                "position": 1662
            }
        ]
    },
    {
        "header": "Appendix BAdditional Experiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.21850/x9.png",
                "caption": "Figure 11:Qualitative comparison of model outputs.Examples showing responses from our compositionally-tunedCOMPACTmodel andLLaVA-665K[24]VIT model on complex queries that require multiple capabilities (k‚â•3ùëò3k\\geq 3italic_k ‚â• 3). Our model demonstrates better integration of visual capabilities which leads to more accurate responses.",
                "position": 1912
            },
            {
                "img": "https://arxiv.org/html/2504.21850/x10.png",
                "caption": "Figure 12:Visualization ofCOMPACTCompositional Tuning Samples.",
                "position": 2019
            },
            {
                "img": "https://arxiv.org/html/2504.21850/x11.png",
                "caption": "Figure 13:Visualization ofCOMPACTCompositional Tuning Samples.",
                "position": 2023
            }
        ]
    },
    {
        "header": "Appendix CVisualizations",
        "images": []
    }
]