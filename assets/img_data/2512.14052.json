[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14052/Figure/trend.png",
                "caption": "Figure 1:OpenCompass Average Score vs. Model Parameters. We compare HyperVL with open‑source models released after October 2024, based on OpenCompass benchmarks. HyperVL (marked by the star) achieves superior performance comparable to larger models while maintaining a compact parameter footprint.",
                "position": 101
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3HyperVL",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14052/Figure/model_architecture.png",
                "caption": "Figure 2:The overall architecture of HyperVL mainly includes a visual resolution compressor, two visual encoders of different sizes, a vision-language projector, and a shared LLM.",
                "position": 153
            },
            {
                "img": "https://arxiv.org/html/2512.14052/Figure/visual_resolution_compressor.png",
                "caption": "Figure 3:Architecture of the Visual Resolution Compressor. The top panel illustrates the data construction pipeline used for training the compressor, while the lower shows the inference process. The bottom panel depicts the inference workflow, where the lightweight compressor dynamically predicts the optimal resolution to accelerate processing.",
                "position": 224
            },
            {
                "img": "https://arxiv.org/html/2512.14052/Figure/data_pipeline.png",
                "caption": "Figure 4:Data pipeline.We adopt a three-stage data governance pipeline consisting of data preparation and categorization, data cleaning and normalization, and data quality filtering and mixed packaging, enabling large-scale construction of high-quality multimodal training samples with strong controllability and consistency.",
                "position": 335
            }
        ]
    },
    {
        "header": "4Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14052/Figure/memory_usage.png",
                "caption": "Figure 5:Efficiency comparison of on-device ViTs cross different image resolutions on Qualcomm 8750 Platform. Left: memory consumption (MB) comparison of diffenet ViTs. Right: inference latency (ms) comparison of different ViTs.",
                "position": 1441
            },
            {
                "img": "https://arxiv.org/html/2512.14052/Figure/memory_usage.png",
                "caption": "",
                "position": 1444
            },
            {
                "img": "https://arxiv.org/html/2512.14052/Figure/inference_time.png",
                "caption": "",
                "position": 1448
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix AContributor",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14052/Figure/appendixes_images/math.png",
                "caption": "Table 8:Mathematical reasoning results.",
                "position": 1539
            },
            {
                "img": "https://arxiv.org/html/2512.14052/Figure/appendixes_images/chart.png",
                "caption": "Table 9:Visual chart interpretation and data extraction.",
                "position": 1594
            },
            {
                "img": "https://arxiv.org/html/2512.14052/Figure/appendixes_images/scientific.png",
                "caption": "Table 10:An example of geometric reasoning.",
                "position": 1622
            },
            {
                "img": "https://arxiv.org/html/2512.14052/Figure/appendixes_images/gui.png",
                "caption": "Table 11:GUI understanding and task planning performed.",
                "position": 1643
            },
            {
                "img": "https://arxiv.org/html/2512.14052/Figure/appendixes_images/text4.jpg",
                "caption": "Table 12:Image description and text extraction.",
                "position": 1665
            },
            {
                "img": "https://arxiv.org/html/2512.14052/Figure/appendixes_images/common_sense.jpg",
                "caption": "Table 13:Common‑sense reasoning on real‑world scenarios.",
                "position": 1724
            },
            {
                "img": "https://arxiv.org/html/2512.14052/Figure/appendixes_images/knowledge.jpg",
                "caption": "Table 14:Knowledge‑based visual reasoning.",
                "position": 1746
            }
        ]
    },
    {
        "header": "Appendix BQualitative Examples",
        "images": []
    }
]