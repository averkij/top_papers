[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.13807/x1.png",
                "caption": "Figure 1:MagicDriveDiTgenerates high-resolution and long videos with multi-view and control supports, significantly exceeding the limitation of previous works[35,15,11,12].",
                "position": 96
            },
            {
                "img": "https://arxiv.org/html/2411.13807/x2.png",
                "caption": "Figure 2:Different from spatial latent[11,39,38,27,18], spatial-temporal latent (ours) requires spatial-temporal condition injection for geometry controls (we omit the text condition here).",
                "position": 103
            },
            {
                "img": "https://arxiv.org/html/2411.13807/x3.png",
                "caption": "Figure 3:Architecture Overview ofMagicDriveDiT. To incorporate different conditions for video generation,MagicDriveDiTadopts a two-branch architecture as in[44]with basic STDiT3 blocks from[6,47]. We also propose the MVDiT block for multi-view consistency and Spatial-Temporal (Box/Traj.) Encoder to inject condition into the Spatial-Temporal (SP) Latent.",
                "position": 142
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary",
        "images": []
    },
    {
        "header": "4Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.13807/x4.png",
                "caption": "Figure 4:Spatial-Temporal Encoder for Maps(a)and Boxes(b). Our spatial encoding module follows[11], and temporal encoding integrates the downsampling strategy in our 3D VAE[43], resulting in temporally aligned embedding between the control signals and video latents.",
                "position": 335
            },
            {
                "img": "https://arxiv.org/html/2411.13807/x5.png",
                "caption": "Figure 5:Progressive Bootstrap Training inMagicDriveDiT. For high-resolution long video generation, we train the model to progressively scale up from both resolution and frame count.",
                "position": 357
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.13807/x6.png",
                "caption": "Figure 6:Qualitative Comparison betweenMagicDriveDiTand MagicDrive[11].\nFrames are extracted from the generated videos.\nTo conserve space, we only present the 3 (out of 6) perspectives that include the front view.\nTwo crops from the generated views are magnified and shown on the right.\nBy generating 4×\\times×resolution of MagicDrive,\nthe synthesized street view of our model contains finer details.",
                "position": 428
            },
            {
                "img": "https://arxiv.org/html/2411.13807/x7.png",
                "caption": "Figure 7:Reconstruction Visualization from Different VAEs.\nCogVAE[43]maintains most details compared with others and exhibits better performance for high-resolution contents.",
                "position": 546
            },
            {
                "img": "https://arxiv.org/html/2411.13807/x8.png",
                "caption": "Figure 8:MagicDriveDiTgenerates high-resolution (e.g., 424×\\times×800) street-view videos for 241 frames (i.e., the full length of nuScenes videos, approximately 20 seconds at 12 FPS) with multiple controls (i.e., road map, object boxes, ego trajectory, and text). Notably, the 241-frame length at 424×\\times×800 are unseen during training, demonstrating our method’s generalization capability to video length. We annotate the ego-vehicle trajectory and selected objects to aid localization, with same-color boxes denoting the same object. Due to space constraints, the “rainy” example includes only two frames; additional examples can be found in the AppendixH.",
                "position": 552
            },
            {
                "img": "https://arxiv.org/html/2411.13807/x9.png",
                "caption": "Figure 9:Validation Loss through Training with Different SP Encodings.4×4\\times4 ×down (our methods inMagicDriveDiT) can help the model converge, performing the best among all the encodings.",
                "position": 573
            },
            {
                "img": "https://arxiv.org/html/2411.13807/x10.png",
                "caption": "Figure 10:Visual Effect of Spatial-Temporal Encoding for Boxes.\nFor videos with spatial-temporal latent encoding, a simple global reduce baseline can cause artifacts and trailing effects in object trajectories across viewpoints (we highlight them withred boxes). Our spatial-temporal encoding effectively resolves this, maintaining object clarity and accurate motion trajectories.",
                "position": 577
            }
        ]
    },
    {
        "header": "6Applications",
        "images": []
    },
    {
        "header": "7Conclusion and Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "ASequence Parallel Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.13807/x11.png",
                "caption": "Figure I:Diagram for Sequence Parallel.Left: We split the spatial dimension before the first block and gather them after the last block.Right: For each attention module, we use all-to-all communication, changing the splitting dimension to attention heads.\nB: batch; T: temporal dimension; S: spatial dimension; D: latent dimension; HD: number of heads; CH: per-head dimension; SP: sequence parallel size.",
                "position": 1388
            }
        ]
    },
    {
        "header": "BMore Details for Mixed Resolution and Frames Training",
        "images": []
    },
    {
        "header": "CMore Experimental Details",
        "images": []
    },
    {
        "header": "DMore Training Details",
        "images": []
    },
    {
        "header": "EMore Comparison among VAEs",
        "images": []
    },
    {
        "header": "FReason for Using CogVAE without the Pre-trained Diffusion Model",
        "images": []
    },
    {
        "header": "GSingle Inference v.s. Rollout Inference",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.13807/x12.png",
                "caption": "(a)Generation from Vista. It takes the first frame as input and generates the following (only support the front view).",
                "position": 1591
            },
            {
                "img": "https://arxiv.org/html/2411.13807/x12.png",
                "caption": "(a)Generation from Vista. It takes the first frame as input and generates the following (only support the front view).",
                "position": 1594
            },
            {
                "img": "https://arxiv.org/html/2411.13807/x13.png",
                "caption": "(b)Generation fromMagicDriveDiT. We take conditions as inputs and generate the full video (only show the first 9s for comparison).",
                "position": 1600
            },
            {
                "img": "https://arxiv.org/html/2411.13807/x14.png",
                "caption": "Figure III:We show some frames from the generated6×848×1600×241684816002416\\times 848\\times 1600\\times 2416 × 848 × 1600 × 241videos with the same scene configuration (i.e., boxes, maps, cameras, and ego trajectory) but under different weather conditions. Conditions are from the nuScenes validation set.",
                "position": 1618
            },
            {
                "img": "https://arxiv.org/html/2411.13807/x15.png",
                "caption": "Figure IV:We show some frames from the generated6×848×1600×241684816002416\\times 848\\times 1600\\times 2416 × 848 × 1600 × 241videos with the same scene configuration (i.e., boxes, maps, cameras, and ego trajectory) but under different time-of-day conditions. Conditions are from the nuScenes validation set.",
                "position": 1621
            }
        ]
    },
    {
        "header": "HMore Visualization",
        "images": []
    }
]