[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06830/",
                "caption": "Figure 1:Overview of Curia.Curia is a radiological foundation model for CT and MRI images, trained with self-supervised learning with the DINOv2 algorithm on 200M images, based on the vision transformer architecture.(a)Pre-training methodology and statistics - All reported numbers correspond to the number of 2D images. PACS = Picture Archiving and Communication System.(b)List of tasks and pathology areas evaluated in the benchmark. We evaluated Curia on classification, regression, survival prediction, and segmentation tasks, and also explore generalization in few-shot and cross-modal settings.(c)Method for supervised evaluations: image-level prediction, object-level prediction, and volume-level prediction.(d)Radar plot of Curia-L’s performance against MedImageInsight and BioMedCLIP. Metrics are detailed in Fig.2. To provide robust estimates, we report the mean performance over 1000 bootstrap samples for each task.(e)Performance comparison of Curia-L against resident radiologists. We report the mean performance with 95% confidence intervals, calculated over 1000 bootstrap samples, along with the statistical significance using a paired bootstrap hypothesis test.",
                "position": 316
            }
        ]
    },
    {
        "header": "2Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06830/x2.png",
                "caption": "Figure 2:List of downstream tasks considered in the CuriaBench benchmark.For each task, we report the modality (CT/MRI), the type (2D/3D), the metric (Accuracy, AUC, Balanced Accuracy,r2r^{2}), the number of classes for classification tasks, and the sizes of the training, validation, and test sets. The registration task and the prompted segmentation task are showcased in Fig.3.",
                "position": 381
            },
            {
                "img": "https://arxiv.org/html/2509.06830/x3.png",
                "caption": "Figure 3:Performance on anatomical tasks(a)Comparison of foundation models on the CuriaBench anatomical subset. Metrics are defined in Fig.2. The error bars represent the 95% confidence interval derived from 1000 bootstrap samples.(b)Performance on Imaging Registration on three datasets: XCAT, Learn2Reg Brain, Learn2Reg Abdomen.(c)Cross-modality generalization on organ classification. We report the gap between the two modalities for each model.(d)Data Efficiency: performance of FMs and model from scratch with varying number of labeled samples on theAnatomy - CTtask. “All” are models trained on the full dataset.(e)First two lines: Principal Component Analysis (PCA) visualization of feature maps from Curia, MedImageInsight, BiomedCLIP, and DINOv2 on a CT and an MRI image. Last line: Image registration results using image features. A displacement field was computed, and used to project themovingimage to match thefixedimage. We display the projected image for Curia, MedImageInsight, BioMedCLIP and DINOv2. We also show in red the positions of kidneys from the fixed image for reference.",
                "position": 390
            },
            {
                "img": "https://arxiv.org/html/2509.06830/x4.png",
                "caption": "Figure 4:Performance of Curia, SAM and RadSAM on the Prompted Segmentation benchmark.(a)Performance with a bounding box prompt. For each structure, we report the average Dice Score over all samples.(b)Performance with a point prompt. For each structure, we report the average Dice Score over all samples.(c)Visualization of the segmentation maps predicted by Curia-B and Curia-L with the SAM decoder. We compare the results with SAM and RadSAM on the same image and prompt. Top row: with a bounding box prompt; Bottom row: with a point prompt.",
                "position": 446
            },
            {
                "img": "https://arxiv.org/html/2509.06830/x5.png",
                "caption": "Figure 5:Performance of Curia on Oncology-related tasks(a)Results of CuriaBench Oncology subset – Tumor anatomical site, kidney tumor and lung nodule malignancy, and renal malignancy survival. We compared Curia against BioMedCLIP and MedImageInsight, as well as Harvard Onco-FM[22]for lung nodule malignancy.\nThe 95% CI of the scores obtained with 1000 bootstrap samples is shown by error bars, except for Harvard-RT for which we report the score from the original publication.(b)Performance (AUC) of Curia, BioMedCLIP, MedImageInsight, and a ViT-B without any pre-training, with varying number of examples in the training set of the Kidney lesion malignancy task. The error bars represent the variance over 5 runs for each point.(c)ROC curves for Lung nodule and Kidney lesion malignancy tasks. All models were evaluated 5 times, and we aggregated the predictions using the pooling method[23,24]to display the final ROC curve.(d)Kaplan–Meier curves for groups, stratified by local stage (T1/T2 vs T3/T4), and by model’s risk prediction. The error bars represent the 95% CI of the estimates.",
                "position": 461
            },
            {
                "img": "https://arxiv.org/html/2509.06830/x6.png",
                "caption": "Figure 6:Performance of Curia, BioMedCLIP and MedImageInsight on four subsets of CuriaBench.Metrics are described in Fig.2. The metrics reported are the average across 1000 bootstrap samples for 5 runs, and we show the 95% confidence intervals.(a)Musculoskeletal (MSK) conditions(b)Infectious conditions(c)Emergency-related conditions(d)Neurodegenerative conditions",
                "position": 504
            },
            {
                "img": "https://arxiv.org/html/2509.06830/x7.png",
                "caption": "Figure 7:(a)Visualization of the attention maps. Images displayed are windowed to brain standard viewing parameters (level=40, width=80) with a varying blue-to-red colormap corresponding to increasing attention scores. The attention maps were computed between the final patches of each model and a single learnable query vector, highlighting the areas used in the decision-making process.(b)Visualization of keypoint matching. The first row illustrates keypoint matching between two MRI images from different patients in the OASIS dataset. The second row presents matches between two CT images from different patients in the Learn2Reg CT-Abdomen dataset. The third and fourth rows demonstrate cross-modality matching between MRI (source) and CT (target) images from the same patient in the Learn2Reg MR-CT dataset.",
                "position": 584
            }
        ]
    },
    {
        "header": "3Discussion",
        "images": []
    },
    {
        "header": "4Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06830/x8.png",
                "caption": "Figure 8:The Vision Transformer architecture. The image is tokenized into 16x16 patches, then converted into tokens and fed into the vision transformer. An additional class token is added, to perform image-level pre-training tasks.",
                "position": 661
            },
            {
                "img": "https://arxiv.org/html/2509.06830/x9.png",
                "caption": "(a)",
                "position": 672
            },
            {
                "img": "https://arxiv.org/html/2509.06830/x9.png",
                "caption": "(a)",
                "position": 675
            },
            {
                "img": "https://arxiv.org/html/2509.06830/x10.png",
                "caption": "(b)",
                "position": 681
            }
        ]
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExtended Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06830/x11.png",
                "caption": "Figure 10:Receiver Operating Characteristic (ROC) curves for all binary classification tasks in our benchmark. The curves were computed on 5 runs for each models, and were aggregated using the pooling method[23,24]. All predictions from the 5 runs were concatenated into a single ensemble of predictions, that was used to plot the ROC curve.",
                "position": 1641
            },
            {
                "img": "https://arxiv.org/html/2509.06830/x12.png",
                "caption": "Figure 11:Scaling curves for two architectures: ViT-B and ViT-L. We train the two models with various dataset sizes (30K, 200K, 2M, 20M, 200M), for different number of training steps, and show all results in the top 5 plots.\nThe average is computed on all our 19 benchmarks. On the bottom, we plot, for each dataset size, the best performing model.",
                "position": 1653
            }
        ]
    },
    {
        "header": "Appendix BModel Parameters",
        "images": []
    },
    {
        "header": "Appendix CPrompted Segmentation",
        "images": []
    },
    {
        "header": "Appendix DDetailed Results",
        "images": []
    },
    {
        "header": "Appendix EDetailed Scores by Benchmark",
        "images": []
    }
]