[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.23469/x1.png",
                "caption": "Figure 1:An overview of our VGT paradigm, which transforms a pre-trained Vision-Language Model (VLM) into a powerful generative model.(a) Pre-trained VLMs excel at visual understanding by aligning semantic vision encoders with Language Models (LLMs), hinting at their latent image generation capabilities. (b) Our VGT-AE (Visual Generation Tuning-AutoEncoder) aligns the VLM’s semantic encoder through reconstruction training. (c) We perform visual generation tuning(VGT) on the VLM by predicting these semantically aligned image latents using a lightweight flow matching head, enabling efficient continuous autoregressive image generation.",
                "position": 131
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.23469/x2.png",
                "caption": "Figure 2:Visualization of clustering results with different visual representations.Our VGT-AE is capable of preserving semantic structure while retaining fine-grained textures.",
                "position": 226
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.23469/x3.png",
                "caption": "Figure 3:Visualization of reconstruction results from our VGT-AE-InternViT.Left: input image;Right: reconstructed image.",
                "position": 513
            },
            {
                "img": "https://arxiv.org/html/2511.23469/x4.png",
                "caption": "Figure 4:OurVGT-1.6Bbased on Qwen2.5-VL and InternVL3, endow pretrained vision–language models trained on multimodal understanding tasks with high-quality visual generation, enabling them to produce diverse and realistic images.",
                "position": 921
            },
            {
                "img": "https://arxiv.org/html/2511.23469/x5.png",
                "caption": "Figure 5:Impact of noise regularization on the reconstruction-generation trade-off. While reconstruction quality (rFID, PSNR) degrades with increasing noise, generation performance (GenEval) peaks at moderate noise intensities (σ=0.4\\sigma=0.4–0.60.6).",
                "position": 1086
            },
            {
                "img": "https://arxiv.org/html/2511.23469/x6.png",
                "caption": "Figure 6:t-SNE of latent tokens from three VGT-AE variants. Generation-oriented models (Qwen2.5-VL and InternVL3 Low Rec.) yield dispersed, semantically structured manifolds, while the reconstruction-oriented InternVL3 forms compact, entangled clusters, reflecting different requirements on the latent space for reconstruction vs. autoregressive generation.",
                "position": 1143
            },
            {
                "img": "https://arxiv.org/html/2511.23469/x7.png",
                "caption": "Figure 7:Qualitative comparison between InternVL3-based VGT (left) and Qwen2.5-VL-based VGT (right).\nThe InternVL3 variant, despite lower generation metrics, produces noticeably sharper illumination and more detailed textures.",
                "position": 1149
            },
            {
                "img": "https://arxiv.org/html/2511.23469/x8.png",
                "caption": "Figure 8:QueryARsimultaneously generates 4 or 16 tokens while maintaining high-quality image outputs.",
                "position": 1265
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledgements",
        "images": []
    }
]