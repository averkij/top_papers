[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01396/x1.png",
                "caption": "Figure 1:Overview of seminar domains and task structures in MAHTG.Left:Distribution of academic seminars across diverse domains such as Science & Technology, Health, Finance, and others. The outer arc further decomposes each domain into representative research tasks. For instance, Science & Technology includes tasks such asHypothesis Generation,Empirical Test,Prototype Specification, andTrend Scan.Right:Illustration of MAHTGâ€™s multi-agent pipeline, where seminar content is transformed into structured research tasks via intermediate inspirations (e.g.,Methodology,Transdisciplinarity). Example outputs are shown for both stages.",
                "position": 124
            }
        ]
    },
    {
        "header": "Introduction",
        "images": []
    },
    {
        "header": "Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01396/x2.png",
                "caption": "Figure 2:Overview of our benchmark construction pipeline, including four stages: (a) Data generation from transcribed seminar videos, (b) extraction of research inspirations, (c) multi-phase task design, and (d) evaluation using both KAE and ACE metrics.",
                "position": 263
            }
        ]
    },
    {
        "header": "Multi-Agent Hierarchical Task Generation.",
        "images": []
    },
    {
        "header": "Evaluation Methodology",
        "images": []
    },
    {
        "header": "Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01396/x3.png",
                "caption": "Figure 3:Comparison of current mainstream models on the DeepResearch Arena benchmark. (a) Performance across 12 research disciplines (e.g., Science & Technology, Art, Finance). (b) Performance across 10 research task types (e.g., Hypothesis Generation, Method Blueprint, Evaluation Metric Design), highlighting task-specific capabilities.",
                "position": 1051
            },
            {
                "img": "https://arxiv.org/html/2509.01396/x4.png",
                "caption": "Figure 4:Comparison of DeepResearch agents in terms of Keypoint-Aligned Evaluation (KAE) metrics and efficiency.",
                "position": 1054
            }
        ]
    },
    {
        "header": "Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AData Leakage Detection",
        "images": []
    },
    {
        "header": "Appendix BAlignment Between Automated Evaluation and Human Judgment",
        "images": []
    },
    {
        "header": "Appendix CSample Checklist Generated by ACE",
        "images": []
    },
    {
        "header": "Appendix DPrompt Templates",
        "images": []
    }
]