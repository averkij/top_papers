[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03437/extracted/5909665/figures/zebra_completely_white_background-2.png",
                "caption": "",
                "position": 161
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Problem setting",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03437/x1.png",
                "caption": "Figure 1:ZebraFramework for solving parametric PDEs. 1) A finite vocabulary of physical phenomena is learned by training a VQ-VAE on spatial representations. 2) During the pretraining, multiple trajectories sharing the same dynamics are tokenized and concatenated into a common sequenceSùëÜSitalic_S. A transformer is used to predict the next tokens in these sequences, conditioned on the context. This enables the model to perform both zero-shot and few-shot generation, without gradient-based updates.",
                "position": 258
            }
        ]
    },
    {
        "header": "3Zebra Framework",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03437/x2.png",
                "caption": "Figure 2:Influence of the number of examples.Zebra‚Äôs rollout loss for a different number of trajectory examples. The x-axis is the # of context examples and the y-axis is the RelativeL2superscriptùêø2L^{2}italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT.",
                "position": 660
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x3.png",
                "caption": "Figure 3:Zero-shot vs one-shot performanceofZebrawith 2 frames.",
                "position": 754
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x4.png",
                "caption": "Figure 4:Uncertainty quantificationwithZebrain a one-shot setting onHeat.",
                "position": 773
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x5.png",
                "caption": "Figure 5:Uncertainty quantificationwithZebra",
                "position": 811
            }
        ]
    },
    {
        "header": "5Limitations",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Reproducibility Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelated Work",
        "images": []
    },
    {
        "header": "Appendix BDataset details",
        "images": []
    },
    {
        "header": "Appendix CArchitecture details",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03437/x6.png",
                "caption": "Figure 6:Zebra‚Äôs inference pipeline from context trajectory. The context trajectory and initial conditions are tokenized into index sequences that are concatenated according to the sequence design adopted during pretraining. The transformer then generates the next tokens to complete the sequence. We detokenize these indices to get back to the physical space.",
                "position": 1578
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x7.png",
                "caption": "Figure 7:Zebra‚Äôs inference pipeline from observations of several initial frames. The initial timestamps are tokenized into index sequences that are concatenated according to the sequence design adopted during pretraining. The transformer then generates the next tokens to complete the sequence. We detokenize these indices to get back to the physical space.",
                "position": 1581
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x8.png",
                "caption": "Figure 8:Generation possibilities withZebra.",
                "position": 1584
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x9.png",
                "caption": "Figure 9:Zebra‚Äôs transformer architecture is based on Llama(Touvron et¬†al.,2023).",
                "position": 1594
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x10.png",
                "caption": "Figure 10:Zebra‚Äôs VQVAE is used to obtain compressed and discretized latent representation. By retrieving the codebok index for each discrete representation, we can obtain discrete tokens encoding physical observations that can be mapped back to the physical space with high fidelity.",
                "position": 1756
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x11.png",
                "caption": "Figure 11:Architecture of Zebra‚Äôs VQVAE for 1D datasets. Each convolution acts only on the spatial dimension and uses a kernel of size 3. The Residual Blocks are used to process information and increase or decrease the channel dimensions, while the Up and Down blocks respectively up-sample and down-sample the resolution of the inputs. In 1D, we used a spatial compression factor of 16 on all datasets. Every downsampling results in a doubling of the number of channels, and likewise, every upsampling is followed by a reduction of the number of channels by 2. We choose a maximum number of channels of 256.",
                "position": 1759
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x12.png",
                "caption": "Figure 12:Architecture of Zebra‚Äôs VQVAE for 2D datasets. Each convolution acts only on the spatial dimensions and uses a kernel of size 3. The Residual Blocks are used to process information and increase or decrease the channel dimensions, while the Up and Down blocks respectively up-sample and down-sample the resolution of the inputs. In 2D, we used a spatial compression factor of 4 forVorticity, and 8 forWave2D. Every downsampling results in a doubling of the number of channels, and likewise, every upsampling is followed by a reduction of the number of channels by 2. We choose a maximum number of channels of 1024.",
                "position": 1762
            }
        ]
    },
    {
        "header": "Appendix DAdditional Quantitative results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03437/x13.png",
                "caption": "(a)Analysis of the distribution of the generated initial conditions (t=0).",
                "position": 1962
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x13.png",
                "caption": "(a)Analysis of the distribution of the generated initial conditions (t=0).",
                "position": 1965
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x14.png",
                "caption": "(b)Analysis of the distribution of the generated trajectories (t=9).",
                "position": 1970
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x15.png",
                "caption": "Figure 14:One-shotadaptation on Advection",
                "position": 2015
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x16.png",
                "caption": "Figure 15:Zero-shotprediction on Advection",
                "position": 2018
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x17.png",
                "caption": "Figure 16:Five-shotadaptation on Advection",
                "position": 2021
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x18.png",
                "caption": "Figure 17:Uncertainty quantificationon Advection",
                "position": 2024
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x19.png",
                "caption": "Figure 18:One-shotadaptation on Burgers",
                "position": 2031
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x20.png",
                "caption": "Figure 19:Zero-shotprediction on Burgers",
                "position": 2034
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x21.png",
                "caption": "Figure 20:Five-shotadaptation on burgers",
                "position": 2037
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x22.png",
                "caption": "Figure 21:Uncertainty quantificationon Burgers",
                "position": 2040
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x23.png",
                "caption": "Figure 22:One-shotadaptation on Heat",
                "position": 2047
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x24.png",
                "caption": "Figure 23:Five-shotprediction on Heat",
                "position": 2050
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x25.png",
                "caption": "Figure 24:Zero-shotadaptation on Heat",
                "position": 2053
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x26.png",
                "caption": "Figure 25:Uncertainty quantificationon Heat",
                "position": 2056
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x27.png",
                "caption": "Figure 26:One-shotadaptation on Wave b",
                "position": 2063
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x28.png",
                "caption": "Figure 27:Zero-shotprediction on Wave b",
                "position": 2066
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x29.png",
                "caption": "Figure 28:Uncertainty quantificationon Wave b",
                "position": 2069
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x30.png",
                "caption": "Figure 29:One-shotadaptation on Combined",
                "position": 2076
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x31.png",
                "caption": "Figure 30:Zero-shotprediction on Combined",
                "position": 2079
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x32.png",
                "caption": "Figure 31:Five-shotadaptation on Combined",
                "position": 2082
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x33.png",
                "caption": "Figure 32:Uncertainty quantificationon Combined equation",
                "position": 2085
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x34.png",
                "caption": "Figure 33:One-shotadaptation on Vorticity. Example 1.",
                "position": 2092
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x35.png",
                "caption": "Figure 34:One-shotadaptation on Vorticity. Example 2.",
                "position": 2095
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x36.png",
                "caption": "Figure 35:One-shotadaptation on Vorticity. Example 3.",
                "position": 2098
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x37.png",
                "caption": "Figure 36:Zero-shotprediction on Vorticity. Example 1.",
                "position": 2101
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x38.png",
                "caption": "Figure 37:Zero-shotprediction on Vorticity. Example 2.",
                "position": 2104
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x39.png",
                "caption": "Figure 38:Zero-shotprediction on Vorticity. Example 3.",
                "position": 2107
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x40.png",
                "caption": "Figure 39:One-shotadaptation on Vorticity. Example 1.",
                "position": 2114
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x41.png",
                "caption": "Figure 40:One-shotadaptation on Wave2d. Example 2.",
                "position": 2117
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x42.png",
                "caption": "Figure 41:One-shotadaptation on Wave2d. Example 3.",
                "position": 2120
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x43.png",
                "caption": "Figure 42:Zero-shotprediction on Wave2d. Example 1.",
                "position": 2123
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x44.png",
                "caption": "Figure 43:Zero-shotprediction on Wave2d. Example 2.",
                "position": 2126
            },
            {
                "img": "https://arxiv.org/html/2410.03437/x45.png",
                "caption": "Figure 44:Zero-shotprediction on Wave2d. Example 3.",
                "position": 2129
            }
        ]
    },
    {
        "header": "Appendix EQualitative results",
        "images": []
    }
]