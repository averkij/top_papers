[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.02379/x1.png",
                "caption": "Figure 1:DIST2Loss is a training framework for foundational models that leverages distance relationships among output tokens, enabling improvements across diverse tasks such as alignment reward modeling, visual grounding, robotic manipulation, and autoregressive image generation.",
                "position": 181
            },
            {
                "img": "https://arxiv.org/html/2503.02379/x2.png",
                "caption": "Figure 2:Left: DIST2Loss demonstrates broad applicability for tasks where foundational models need to generate tokens with embedded distance semantics. Right: Illustration of the DIST2Loss framework applied to a visual grounding task. Here, for an input subsequence with spatial metrics (e.g., coordinates(x1,y1,x2,y2)subscript𝑥1subscript𝑦1subscript𝑥2subscript𝑦2(x_{1},y_{1},x_{2},y_{2})( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT )in visual grounding), the process is as follows: (a) calculate the distanced⁢(x,x′)𝑑𝑥superscript𝑥′d(x,x^{\\prime})italic_d ( italic_x , italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT )between the target tokenx𝑥xitalic_xand each tokenx′superscript𝑥′x^{\\prime}italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPTbased on the defined metric; (b) transform this distance into a continuous probabilistic distributionp⁢(x,x′)𝑝𝑥superscript𝑥′p(x,x^{\\prime})italic_p ( italic_x , italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT )within the exponential family; (c) discretize the continuous distribution into a categorical distributionpd⁢(x,x′)subscript𝑝𝑑𝑥superscript𝑥′p_{d}(x,x^{\\prime})italic_p start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( italic_x , italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT )compatible with the discrete foundational model; (d) optimize the foundational model by minimizing the KL divergence between the model’s likelihoodpθsubscript𝑝𝜃p_{\\theta}italic_p start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPTand thedistance-awaretarget distributionpdsubscript𝑝𝑑p_{d}italic_p start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPTat each timestep.",
                "position": 187
            }
        ]
    },
    {
        "header": "2Method",
        "images": []
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.02379/x3.png",
                "caption": "Figure 3:Top: Overview of the task setup in the meta linear regression experiment, where the model learns to perform linear regression based on provided data points. Bottom: Experimental results showing MAE and RMSE across varying numbers of training samples. The y-axis is inverted for visualization.",
                "position": 420
            },
            {
                "img": "https://arxiv.org/html/2503.02379/x4.png",
                "caption": "Figure 4:Illustration of token distance impact on image semantics. Each row shows an image encoded and decoded by the VQ model, with four central tokens modified by: the original token, a random close token (top-10), a random token, and a random distant token (bottom-10). Tokens close to the original mostly maintain the image’s semantics, while random and distant tokens lead to visual distortions or introduce entirely new elements.",
                "position": 856
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BExtended Quantitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.02379/x5.png",
                "caption": "Figure 8:Qualitative examples from the visual grounding experiment. Top: our proposed DIST2Loss loss demonstrates higher visual grounding accuracy compared to the standard cross-entropy loss.\nBottom: A manual examination of inference results reveals that a substantial portion of the RefCOCO dataset[28,38,61]contains labels that are ambiguous, even for human annotators, which may lead to underestimation of model performance.",
                "position": 3047
            }
        ]
    },
    {
        "header": "Appendix CAdditional Qualitative Samples",
        "images": []
    }
]