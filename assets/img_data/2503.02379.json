[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.02379/x1.png",
                "caption": "Figure 1:DIST2Loss is a training framework for foundational models that leverages distance relationships among output tokens, enabling improvements across diverse tasks such as alignment reward modeling, visual grounding, robotic manipulation, and autoregressive image generation.",
                "position": 181
            },
            {
                "img": "https://arxiv.org/html/2503.02379/x2.png",
                "caption": "Figure 2:Left: DIST2Loss demonstrates broad applicability for tasks where foundational models need to generate tokens with embedded distance semantics. Right: Illustration of the DIST2Loss framework applied to a visual grounding task. Here, for an input subsequence with spatial metrics (e.g., coordinates(x1,y1,x2,y2)subscriptğ‘¥1subscriptğ‘¦1subscriptğ‘¥2subscriptğ‘¦2(x_{1},y_{1},x_{2},y_{2})( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT )in visual grounding), the process is as follows: (a) calculate the distancedâ¢(x,xâ€²)ğ‘‘ğ‘¥superscriptğ‘¥â€²d(x,x^{\\prime})italic_d ( italic_x , italic_x start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT )between the target tokenxğ‘¥xitalic_xand each tokenxâ€²superscriptğ‘¥â€²x^{\\prime}italic_x start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPTbased on the defined metric; (b) transform this distance into a continuous probabilistic distributionpâ¢(x,xâ€²)ğ‘ğ‘¥superscriptğ‘¥â€²p(x,x^{\\prime})italic_p ( italic_x , italic_x start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT )within the exponential family; (c) discretize the continuous distribution into a categorical distributionpdâ¢(x,xâ€²)subscriptğ‘ğ‘‘ğ‘¥superscriptğ‘¥â€²p_{d}(x,x^{\\prime})italic_p start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( italic_x , italic_x start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT )compatible with the discrete foundational model; (d) optimize the foundational model by minimizing the KL divergence between the modelâ€™s likelihoodpÎ¸subscriptğ‘ğœƒp_{\\theta}italic_p start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPTand thedistance-awaretarget distributionpdsubscriptğ‘ğ‘‘p_{d}italic_p start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPTat each timestep.",
                "position": 187
            }
        ]
    },
    {
        "header": "2Method",
        "images": []
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.02379/x3.png",
                "caption": "Figure 3:Top: Overview of the task setup in the meta linear regression experiment, where the model learns to perform linear regression based on provided data points. Bottom: Experimental results showing MAE and RMSE across varying numbers of training samples. The y-axis is inverted for visualization.",
                "position": 420
            },
            {
                "img": "https://arxiv.org/html/2503.02379/x4.png",
                "caption": "Figure 4:Illustration of token distance impact on image semantics. Each row shows an image encoded and decoded by the VQ model, with four central tokens modified by: the original token, a random close token (top-10), a random token, and a random distant token (bottom-10). Tokens close to the original mostly maintain the imageâ€™s semantics, while random and distant tokens lead to visual distortions or introduce entirely new elements.",
                "position": 856
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BExtended Quantitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.02379/x5.png",
                "caption": "Figure 8:Qualitative examples from the visual grounding experiment. Top: our proposed DIST2Loss loss demonstrates higher visual grounding accuracy compared to the standard cross-entropy loss.\nBottom: A manual examination of inference results reveals that a substantial portion of the RefCOCO dataset[28,38,61]contains labels that are ambiguous, even for human annotators, which may lead to underestimation of model performance.",
                "position": 3047
            }
        ]
    },
    {
        "header": "Appendix CAdditional Qualitative Samples",
        "images": []
    }
]