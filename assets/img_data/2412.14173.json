[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14173/x1.png",
                "caption": "",
                "position": 121
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14173/x2.png",
                "caption": "Figure 2:Illustration of the workflow of 2D animation production.",
                "position": 136
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14173/x3.png",
                "caption": "Figure 3:Overview ofAniDocpipeline. We adopt a two-stage training strategy. In thedense-sketchtraining stage, we explicitly extract matching keypoints pairs between the reference image and each frame of the training video, constructing point maps to represent the correspondences. In thesparse-sketchtraining stage, we remove the intermediate frame sketches and use the matching points from the start and end frames to interpolate point trajectories, which guide the generation of the intermediate frames.",
                "position": 174
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14173/x4.png",
                "caption": "Figure 4:Illustration of color leakage issue in non-binarized sketch. For previous video colorization method[21], when given non-binarized sketch, even if the reference is an empty image, it can still generate colorized results with similar color pattern to the ground truth. After binarizing the sketch, the colorization results degrade significantly.",
                "position": 225
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14173/x5.png",
                "caption": "Figure 5:Visual comparison of reference-based colorization with four methods LVCD[21], LVCD+IP-Adapter[55], ID-animator[17], ToonCrafter[52].",
                "position": 344
            },
            {
                "img": "https://arxiv.org/html/2412.14173/x6.png",
                "caption": "Figure 6:Ablations on each component. ‚Äúw/omatching‚Äù indicates without the corresponding matching module, ‚Äúw/obinarize‚Äù indicates without binarization and background augmentation.",
                "position": 347
            },
            {
                "img": "https://arxiv.org/html/2412.14173/x7.png",
                "caption": "Figure 7:Illustration of the flexible usage of our model. Figure (a) shows the ability of using same reference to colorize different sketches. Figure (b) demonstrates the robustness to different references. Figure (c) shows the sparse-sketch generation results.",
                "position": 398
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AReference with Different Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14173/x8.png",
                "caption": "Figure S1:Illustration of reference with different backgrounds.",
                "position": 1350
            }
        ]
    },
    {
        "header": "Appendix BMultiple Characters",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14173/x9.png",
                "caption": "Figure S2:Illustration of the multiple characters situation. When the reference image contains multiple characters, our method can correctly infer the correspondence and apply colorization to each character accordingly.",
                "position": 1360
            }
        ]
    },
    {
        "header": "Appendix CDifferent Line Art Extraction Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14173/x10.png",
                "caption": "Figure S3:Impact of different line art extraction methods.",
                "position": 1373
            }
        ]
    },
    {
        "header": "Appendix DMotivation for Correspondence Matching",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14173/x11.png",
                "caption": "Figure S4:In the early training stage (10k step), the video generation model produces static videos that closely resemble the given reference design.",
                "position": 1380
            }
        ]
    },
    {
        "header": "Appendix EIllustration of DIFT Matching",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14173/x12.png",
                "caption": "Figure S5:Semantic feature can effectively find matching keypoints between reference color image and binarized sketch.",
                "position": 1396
            },
            {
                "img": "https://arxiv.org/html/2412.14173/x13.png",
                "caption": "Figure S6:Limitations: in the1s‚Å¢tsuperscript1ùë†ùë°1^{st}1 start_POSTSUPERSCRIPT italic_s italic_t end_POSTSUPERSCRIPTrow, the cartoon bear highlighted within the red square is not present in the reference image. Consequently, the model can only infer the bear‚Äôs color as purple, based on the main color of the reference character, which deviates from its actual appearance. In the2n‚Å¢dsuperscript2ùëõùëë2^{nd}2 start_POSTSUPERSCRIPT italic_n italic_d end_POSTSUPERSCRIPTrow, the character‚Äôs clothing in the line art clip is different from the reference. Therefore, our model can only infer the color of the dress and scarf based on the dominant color patterns identified in the reference image.",
                "position": 1415
            }
        ]
    },
    {
        "header": "Appendix FLimitation",
        "images": []
    }
]