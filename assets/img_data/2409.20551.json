[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.20551/x1.png",
                "caption": "Figure 1:UniAff demonstrates its ability to unify tool usage and articulation understanding in a VQA format, predicting part bounding boxes, 6D poses, grasp affordances, functional affordances, and manipulation types, etc for effective robotic manipulation tasks.",
                "position": 80
            }
        ]
    },
    {
        "header": "IIRelated Work",
        "images": []
    },
    {
        "header": "IIIMethod",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.20551/x2.png",
                "caption": "Figure 2:The architecture of UniAff. The image features are first extracted using a Mixed Visual Encoder, such as DINOv2, CLIP, or Q-Former, followed by an MLP projector. Next, language instructions are used to extract features with the Llama Tokenizer. Finally, the output of the structured manipulation tasks, such as Part BBOX, Affordance, and Revolute Parts, is used to execute robotic instructions.",
                "position": 134
            },
            {
                "img": "https://arxiv.org/html/2409.20551/x3.png",
                "caption": "Figure 3:Illustration of tools. The blue box indicates grasp affordance, the red box indicates functional affordance and the orientation axis illustrates the objectâ€™s pose.",
                "position": 180
            },
            {
                "img": "https://arxiv.org/html/2409.20551/x4.png",
                "caption": "Figure 4:Illustration of manipulation types.(a) bottle cap, (b) revolute part, (c) sliding lid, (d) prismatic part. The yellow box represents the object part, the blue box indicates grasp affordance, the red arrow marks the joint parameter, and the green arrow illustrates the manipulation trajectory.",
                "position": 183
            }
        ]
    },
    {
        "header": "IVExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.20551/x5.png",
                "caption": "Figure 5:Implementation of UniAff in real-world experiments progressed from tool manipulation to articulated object interaction, encompassing tasks such as striking a designated target with a hammer, opening a drawer, refrigerator, microwave, pot, and lifting a bucket handle.",
                "position": 796
            }
        ]
    },
    {
        "header": "VConclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]