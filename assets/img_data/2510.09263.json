[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09263/x1.png",
                "caption": "Figure 1:Quality as evaluated by human raters against FNR at 0.1% FPR averaged across various image transformations. Each model is calibrated to obtain 0.1% FPR (more details to follow in Section8). The quality is measured on the y-axis by looking at the difference in artifact rates between watermarked and non-watermarked content.SynthID-O(our external variant ofSynthID-Imageavailable via partnerships) achieves the highest quality (i.e., lowest perceptibility) and robustness (i.e., lowest brittleness) compared to other baselines.",
                "position": 417
            }
        ]
    },
    {
        "header": "2Our approach",
        "images": []
    },
    {
        "header": "3Invisibility",
        "images": []
    },
    {
        "header": "4Transformation robustness",
        "images": []
    },
    {
        "header": "5Payload",
        "images": []
    },
    {
        "header": "6Ensuring security",
        "images": []
    },
    {
        "header": "7Internet-scale deployment",
        "images": []
    },
    {
        "header": "8Experimental results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09263/x2.png",
                "caption": "Figure 4:Quality metrics for all methods computed on 1,000 watermarked and unwatermarked images. The top-left panel shows the average difference in identified artifact rate between watermarked and non-watermarked samples (lower is better). A 5 percent point increase indicates that the watermarking method creates visible artifacts in 5% of the images. For FID, CMMD and LPIPS the lower is better and for PSNR and SSIM the higher is better. Overall, we see little correlation between computational metrics and human ratings.",
                "position": 1192
            }
        ]
    },
    {
        "header": "9Related work",
        "images": []
    },
    {
        "header": "10Limitations and future work",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional robustness evaluations",
        "images": []
    },
    {
        "header": "Appendix BHuman evaluation prompts",
        "images": []
    }
]