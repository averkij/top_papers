[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02425/x1.png",
                "caption": "Figure 1:Concept Figure.(a) A day-long video sampled at 1 fps has frames that exceed the context limits of video LLMs. (b) M3-Agent[m3-agent]relies on textual representation of video, which can underrepresent visual information. (c) EgoRAG[egolife]retrieves both captions and the corresponding visual frames, but irrelevant frames may distract model. (d) WorldMM (Ours) constructs multiple memories, incorporating both textual and visual representations, and uses adaptive memory retrieval to effectively leverage multimodal information.",
                "position": 163
            },
            {
                "img": "https://arxiv.org/html/2512.02425/x2.png",
                "caption": "Figure 2:Overview of WorldMM.(Left) Multimodal Memory Construction: WorldMM builds three complementary memories (episodic, semantic, and visual memory) that capture temporal events, long-term relations, and visual details from video streams. (Middle) Adaptive Memory Retrieval: A retrieval agent iteratively selects and integrates relevant information from diverse memories for a given query. (Right) Response Generation: The retrieved content and reasoning history are used by a response agent to produce a grounded response.",
                "position": 176
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3WorldMM",
        "images": []
    },
    {
        "header": "4Experiment Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02425/x3.png",
                "caption": "Figure 3:Memory type utilization of WorldMM on five distinctive categories in EgoLifeQA.",
                "position": 649
            },
            {
                "img": "https://arxiv.org/html/2512.02425/x4.png",
                "caption": "Figure 4:Qualitative results.(a) Episodic memory alone cannot capture detailed visual context. The retrieval agent dynamically retrieves from visual memory, enabling access to fine-grained visual details. (b) To address the limitations of episodic memory in representing relationships or habitual behaviors, the retrieval agent proactively accesses semantic memory, allowing it to incorporate habitual knowledge.",
                "position": 808
            },
            {
                "img": "https://arxiv.org/html/2512.02425/x5.png",
                "caption": "Figure 7:Average tIoU and performance of WorldMM and baselines.",
                "position": 998
            },
            {
                "img": "https://arxiv.org/html/2512.02425/x5.png",
                "caption": "Figure 7:Average tIoU and performance of WorldMM and baselines.",
                "position": 1001
            },
            {
                "img": "https://arxiv.org/html/2512.02425/x6.png",
                "caption": "Figure 8:Average latency and performance of WorldMM and baselines.",
                "position": 1006
            },
            {
                "img": "https://arxiv.org/html/2512.02425/x7.png",
                "caption": "Figure 9:Accuracy of WorldMM with different maximum retrieval steps.",
                "position": 1011
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Additional Details on Dataset",
        "images": []
    },
    {
        "header": "Additional Implementation Details",
        "images": []
    },
    {
        "header": "Additional Description on Experiments",
        "images": []
    },
    {
        "header": "Detailed Experimental Results",
        "images": []
    },
    {
        "header": "Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02425/x8.png",
                "caption": "Figure 10:Memory type utilization of WorldMM on four distinctive categories in HippoVlog.",
                "position": 2106
            },
            {
                "img": "https://arxiv.org/html/2512.02425/tab/qualitative/frames/frame_000001_5x3.png",
                "caption": "Table 12:Example of the multi-turn refinement of WorldMM.",
                "position": 2897
            },
            {
                "img": "https://arxiv.org/html/2512.02425/tab/qualitative/frames/frame_000002_5x3.png",
                "caption": "",
                "position": 3090
            },
            {
                "img": "https://arxiv.org/html/2512.02425/tab/qualitative/frames/frame_000003_5x3.png",
                "caption": "",
                "position": 3097
            },
            {
                "img": "https://arxiv.org/html/2512.02425/tab/qualitative/frames/frame_000004_5x3.png",
                "caption": "",
                "position": 3104
            },
            {
                "img": "https://arxiv.org/html/2512.02425/tab/qualitative/frames/frame_000005_5x3.png",
                "caption": "",
                "position": 3111
            }
        ]
    },
    {
        "header": "Limitation and Broader Impact",
        "images": []
    }
]