[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22891/x1.png",
                "caption": "Figure 1:Performance-efficiency trade-offs of various model families across six mathematical reasoning benchmarks (including AIME2025). The dotted curve indicates the Pareto frontier, which illustrates the trade-off between higher compression rates and loss in accuracy. Our proposed method, combiningMentalese alignmentwith SLPO, consistently lies on this frontier, identifying an optimal operating point that achieves a balance between accuracy and efficiency.",
                "position": 71
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22891/x2.png",
                "caption": "Figure 2:Contrast between human and machine reasoning (response from DeepSeek-R1). While humans arrive at intuitive and concise solutions, LLMs often produce verbose and redundant reasoning chains even for simple problems. We bridge this gap by developing methods that encourage models to reason more like humans—clear, efficient, and direct—while preserving accuracy. Grounded in the Language of Thought hypothesis, human reasoning compresses complex ideas into minimal symbolic steps, reflecting cognitive efficiency. Emulating this compact reasoning reduces redundancy in machine outputs, improving both interpretability and token efficiency.",
                "position": 80
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22891/x3.png",
                "caption": "Figure 3:Illustration of symbolic, logic-based chain of thought (mentalese). This figure shows the\ndefinition (top), an example of symbolic reasoning steps (left) with rules governing the\nreasoning style (right).",
                "position": 157
            },
            {
                "img": "https://arxiv.org/html/2511.22891/x4.png",
                "caption": "Figure 4:Comparison of reasoning traces on AIME 2024. Agentica-24k model use approximately 7800 tokens,ORION-AG150 tokens, andORION-AG-SLPO300 tokens, achieving similar accuracy.",
                "position": 304
            }
        ]
    },
    {
        "header": "4Experiment Design",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22891/x5.png",
                "caption": "Figure 5:This figure compares direct SLPO on the base model with Intermediate SFT followed by RLHF methods (SLPO/GRPO) on theMentaleseR-40kdataset across five metrics. TheMentalesealignment yields greater training stability and efficiency: (1) Response Length reveals direct SLPO collapses due to gradient instability, whileORIONmodels stay stable; (2) Clip Ratio indicates more controlled updates inMentalesemethods, driven by reduced response truncation.; (3) Entropy Loss reflects better exploration-exploitation balance; (4) Training Time per RL Step shows higher computational efficiency; (5) Test Performance on AIME 2024 (∼\\sim22% Pass@1) confirmsORIONmodels outperform direct SLPO on the base model. Shaded regions denote min-max ranges across runs. These results highlight the importance of structured intermediate representations (Mentalese) for stable, efficient RL in large language models.",
                "position": 877
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22891/x6.png",
                "caption": "Figure 6:Violin plots of token usage in Agentica-24k responses across six benchmarks before and after fine-tuning. The Base model generates very long responses, while Direct SLPO provides only limited compression. Mentalese-based methods (SFT, SFT+GRPO, SFT+SLPO) achieve 10-20× reduction in response length, approaching an optimal reasoning length that balances efficiency with performance. Although some performance degradation occurs, theMentalesetraining pipeline with RLVR methods offers the best trade-off between token efficiency and problem-solving ability.",
                "position": 1667
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]