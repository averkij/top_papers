[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3The Massive Legal Embedding Benchmark (MLEB)",
        "images": []
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19365/x1.png",
                "caption": "Figure 1:Performance of the top 10 embedding models on MLEB, broken down by domain type. Scores represent NDCG@10 averaged across all datasets within each domain (Judicial, Contractual, Regulatory). Legal domain-adapted models (Kanon 2 Embedder, Voyage 3 Large, Voyage 3.5, Voyage Law 2) show particularly strong performance.",
                "position": 841
            },
            {
                "img": "https://arxiv.org/html/2510.19365/x2.png",
                "caption": "Figure 2:NDCG@10 score versus total evaluation time for commercial embedding models on MLEB. Evaluation time includes network latency and reflects real-world conditions (batch size of 16 for documents, 1 for queries). The plot demonstrates the speed-accuracy tradeoff among commercial offerings.",
                "position": 857
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Data Availability",
        "images": []
    },
    {
        "header": "7Disclosures",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AContractual Clause Types",
        "images": []
    }
]