[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02267/x1.png",
                "caption": "",
                "position": 65
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Overview",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02267/x2.png",
                "caption": "Figure 2:Method overview. The figure illustrates our complete pipeline from multi-view images to final mesh recovery, which proceeds as follows: (a) given multi-view images and cameras parameters, the proxy generator produces per-view SMPL-X proxiesğv\\mathbf{P}_{v}; (b) hand-focused regions inferred from the body proxies are incorporated as additional views for hand refinement; (c) test-time scaling runsKKstochastic inference attempts, aggregates predictions through median (UV) and majority voting (segmentation), and computes pixel-wise uncertainty to produce a weight mapğ–v\\mathbf{W}_{v}that guides fitting; (d) the body is fitted and then refined with hand-specific proxies to recover the final human mesh.",
                "position": 144
            },
            {
                "img": "https://arxiv.org/html/2601.02267/x3.png",
                "caption": "Figure 3:Diffusion-based proxy generator architecture. Our model is built on Stable DiffusionÂ 2.1 with a frozen UNet backbone, equipped with three conditioning signals (ğœtxt\\mathbf{c}_{\\text{txt}},ğœT2I\\mathbf{c}_{\\text{T2I}},ğœDINO\\mathbf{c}_{\\text{DINO}}) and four trainable attention modules (ğ’œtext\\mathcal{A}_{\\mathrm{text}},ğ’œimg\\mathcal{A}_{\\mathrm{img}},ğ’œcm\\mathcal{A}_{\\mathrm{cm}},ğ’œepi\\mathcal{A}_{\\mathrm{epi}}) for multi-view consistent proxy generation.",
                "position": 147
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02267/x4.png",
                "caption": "Figure 4:Qualitative comparison with baseline methods. Our method demonstrates: (i) bias-free predictions avoiding real-data annotation artifacts; (ii) strong generalization despite synthetic-only training; (iii) robustness to partial observations.",
                "position": 628
            },
            {
                "img": "https://arxiv.org/html/2601.02267/x5.png",
                "caption": "Figure 5:Qualitative comparison of hand refinement. Hand refinement improves fitting quality and produces accurate finger details.",
                "position": 685
            },
            {
                "img": "https://arxiv.org/html/2601.02267/x6.png",
                "caption": "Figure 6:Our method benefits from increasing view counts, with performance improving from single-view to multi-view.",
                "position": 734
            },
            {
                "img": "https://arxiv.org/html/2601.02267/x7.png",
                "caption": "Figure 7:Test-time scaling with uncertainty weighting improves robustness by down-weighting unreliable predictions and recovering correct poses from erroneous proxy outputs.",
                "position": 789
            }
        ]
    },
    {
        "header": "6Limitation and Future Works",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Acknowledgments",
        "images": []
    }
]