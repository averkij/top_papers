[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03073/all-twemojis.pdf",
                "caption": "",
                "position": 151
            },
            {
                "img": "https://arxiv.org/html/2512.03073/figures/orgs-countries/world-map.png",
                "caption": "Figure 1:Top:TheTop 12 Nation Mapranked by the all-timedownloads of their models.Bottom:The top 10downloadsLeaderboardfor countries, developers, and models, with their download percentages.\nBoth the map and the leaderboard use Rolling Window Filter to mitigate inauthentic downloads.\nTheAll-timesection reflects all time downloads, whereas theAug. 2024 - Aug. 2025reflects all downloads for models created within the last year (August 2024 to August 2025).\nSymbols indicate details about the model:= embedding and classification models;= text generation,= image generation,= speech generation,= video generation,= international/online organization,= unaffiliated user.",
                "position": 216
            },
            {
                "img": "https://arxiv.org/html/2512.03073/custom_emojis/google.png",
                "caption": "",
                "position": 241
            },
            {
                "img": "https://arxiv.org/html/2512.03073/custom_emojis/stable-diffusion.png",
                "caption": "",
                "position": 251
            },
            {
                "img": "https://arxiv.org/html/2512.03073/custom_emojis/open-ai.png",
                "caption": "",
                "position": 274
            },
            {
                "img": "https://arxiv.org/html/2512.03073/custom_emojis/meta.png",
                "caption": "",
                "position": 281
            },
            {
                "img": "https://arxiv.org/html/2512.03073/custom_emojis/lmstudio-community.png",
                "caption": "",
                "position": 291
            },
            {
                "img": "https://arxiv.org/html/2512.03073/custom_emojis/hugging-face.png",
                "caption": "",
                "position": 294
            },
            {
                "img": "https://arxiv.org/html/2512.03073/custom_emojis/timm.png",
                "caption": "",
                "position": 321
            },
            {
                "img": "https://arxiv.org/html/2512.03073/custom_emojis/deepseek-ai.png",
                "caption": "",
                "position": 331
            },
            {
                "img": "https://arxiv.org/html/2512.03073/custom_emojis/comfy.png",
                "caption": "",
                "position": 365
            },
            {
                "img": "https://arxiv.org/html/2512.03073/custom_emojis/qwen.png",
                "caption": "",
                "position": 375
            },
            {
                "img": "https://arxiv.org/html/2512.03073/custom_emojis/stranger-zone.png",
                "caption": "",
                "position": 395
            },
            {
                "img": "https://arxiv.org/html/2512.03073/custom_emojis/mlc-ai.png",
                "caption": "",
                "position": 398
            },
            {
                "img": "https://arxiv.org/html/2512.03073/custom_emojis/mlx-community.png",
                "caption": "",
                "position": 415
            },
            {
                "img": "https://arxiv.org/html/2512.03073/custom_emojis/shakker-labs.png",
                "caption": "",
                "position": 425
            },
            {
                "img": "https://arxiv.org/html/2512.03073/custom_emojis/onnx.png",
                "caption": "",
                "position": 428
            },
            {
                "img": "https://arxiv.org/html/2512.03073/x1.png",
                "caption": "Figure 2:Top:Developer Download Share over time, using the Rolling Window Filter and applying Recursive Model Attribution. Where Google, Meta, and OpenAI once dominated market share (2021-2024), their influence has subsided for other developers beyond the Top 20 have gradually increased from 20% to >50% share.Bottom:National Download Share over time, using the Rolling Window Filter and applying Recursive Model Attribution.\nWhere the US and Europe once dominated market share (2021-2023), now Users, China, and Germany have become prominent contributors.\nBoth plots use a 1-year Rolling Window Filter to better estimate authentic usage.",
                "position": 499
            },
            {
                "img": "https://arxiv.org/html/2512.03073/x1.png",
                "caption": "",
                "position": 502
            },
            {
                "img": "https://arxiv.org/html/2512.03073/x2.png",
                "caption": "",
                "position": 507
            },
            {
                "img": "https://arxiv.org/html/2512.03073/x3.png",
                "caption": "Figure 3:Top:Model economic concentration over time.Middle:Developer economic concentration over time.Bottom:National economic concentration over time.\nIn each plot we measure the share of downloads allocated to ranked segments of the open model economy.\nEconomic measures of concentration are also displayed in purple (the Gini coefficient) and pink (the Hherfindahl-Hirschman Index from 0-1).Across levels of abstraction (model, developer, nation) economic concentration first declined significantly, but has started to rise again in 2025.",
                "position": 518
            },
            {
                "img": "https://arxiv.org/html/2512.03073/x3.png",
                "caption": "",
                "position": 521
            },
            {
                "img": "https://arxiv.org/html/2512.03073/x4.png",
                "caption": "",
                "position": 526
            },
            {
                "img": "https://arxiv.org/html/2512.03073/x5.png",
                "caption": "",
                "position": 531
            },
            {
                "img": "https://arxiv.org/html/2512.03073/x6.png",
                "caption": "Figure 4:The proportion of downloads allocated by developer organization type in each country.We find US, China, and UK development is skewed heavily to industry,whereas Germany, France, and the rest of Asia, Europe, and Online development is more balanced towards non-profits, universities, and community contributors.",
                "position": 605
            },
            {
                "img": "https://arxiv.org/html/2512.03073/x7.png",
                "caption": "Figure 5:The distribution of model sizes downloaded in each year (left—pink), and created in each year (right—green) is shifting over time.The created statistics only begin in 2022, as Hugging Face did not record prior model creation times.\nWe log-scale the downloads distribution prior to the violin plots’ kernel density estimation to smooth and improve the visibility of the various model size peaks.\nThe lines within each distribution represent the 25, 50, and 75 percentiles.\nWe find the mean size of model download and creation are both rising, though the medians far less quickly as seen inTable˜1.",
                "position": 973
            },
            {
                "img": "https://arxiv.org/html/2512.03073/x8.png",
                "caption": "Figure 6:We illustrate theHead Sample’s aggregate model modality distributions, weighted by allocation of downloads. The model input modalities are depicted on the left, and the model output modalities on the right.\nWe find model inputs are predominantly text (61%), followed by text and images together (20%), then images (9%), audio (7%), followed by other combinations.\nThe outputs are ranked as text encoding (33%), text generation (22%), image generation (19%), image encodings (18%), audio generation (3%), and video generation (1%).We find the output modalities are more heterogeneously distributed than that input modalities on aggregate.",
                "position": 1004
            },
            {
                "img": "https://arxiv.org/html/2512.03073/custom_emojis/hf-color.png",
                "caption": "Table 2:A list of the data collected, their sources, and coverage.We list the source(s), whether from Hugging Face APIs (), automatic crawling (), or manual collection (), and whether the metadata coversAllmodels, or just theHeadsample.",
                "position": 1204
            },
            {
                "img": "https://arxiv.org/html/2512.03073/x9.png",
                "caption": "Figure 7:These plots measure the portion of downloads associated with different model attributes.Top:Model architectures over time.Middle:Training and inference methods over time.Bottom:The disclosure and availability of a models’ training data.",
                "position": 1329
            },
            {
                "img": "https://arxiv.org/html/2512.03073/x9.png",
                "caption": "",
                "position": 1332
            },
            {
                "img": "https://arxiv.org/html/2512.03073/x10.png",
                "caption": "",
                "position": 1337
            },
            {
                "img": "https://arxiv.org/html/2512.03073/x11.png",
                "caption": "",
                "position": 1342
            },
            {
                "img": "https://arxiv.org/html/2512.03073/x12.png",
                "caption": "Figure 8:The portion of downloads associated with models of a given license type.Open Use licenses are on the decline, mainly replaced by licenses with Acceptable Usage Policies (AUPs).",
                "position": 1353
            },
            {
                "img": "https://arxiv.org/html/2512.03073/x13.png",
                "caption": "Figure 9:We illustrate a network derivation graph, showing the portion of downloads allocated to models derived according to different methods.\nEach column sums to 100%.\nWe find that supervised finetuning is the most popular type of derivation (21%), followed by continued pretraining (16%), and quantization (4%).\nModels that undergoe a second round of derivations see additional pretraining (7%), parameter-efficient finetuning (5%), and finetuning (4%).\nAnd a full 7% of model downloads are allocated to systems with 2+ series of derivations.",
                "position": 1356
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]