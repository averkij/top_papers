[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19415/extracted/6477685/figures/teaser2.png",
                "caption": "Figure 1:Overview of MMIG-Bench. We present a unified multi-modal benchmark which contains 1,750 multi-view reference images with 4,850 richly annotated text prompts, covering both text-only and image-text-conditioned generation.\nWe also propose a comprehensive three-level evaluation framework: low-level of artifacts and identity preservation, mid-level of VQA-based Aspect Matching Score, and high-level of aesthetics and human preferences—delivers holistic and interpretable scores.",
                "position": 121
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19415/extracted/6477685/figures/stat.png",
                "caption": "Figure 2:Statistics of the tags in MMIG-Bench.Top-left: Data distribution of compositional categories and high-level categories for text in T2I task.Bottom-left: Data distribution of text prompts in customization task.Right: Statistics of classes for the reference images.",
                "position": 185
            }
        ]
    },
    {
        "header": "3Data Curation",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19415/extracted/6477685/figures/data_curation_v3.png",
                "caption": "Figure 3:Our data curation pipeline for multi-modal image generation benchmarking. We begin by extracting 207 frequent entities from public T2I datasets. Using these entities, we generate diverse prompts with GPT-4o by prompting it with a set of carefully designed instruction templates, which control the structure and style of the prompts (left). Simultaneously, we collect grouped reference images for each entity from free stock sources, with human annotators selecting 3–5 object-centric images per group that vary in pose or view (right). We further collect artistic images in 12 visual styles to support style transfer. The resulting dataset includes high-quality, structured text-image pairs for both T2I and customization.",
                "position": 214
            }
        ]
    },
    {
        "header": "4Proposed Metrics - MMIG-Bench",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19415/extracted/6477685/figures/qual.png",
                "caption": "Figure 4:A qualitative study of text-only (top) and text-image-conditioned (bottom) generation methods on MMIG-Bench.",
                "position": 540
            }
        ]
    },
    {
        "header": "6Discussions and Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19415/extracted/6477685/figures/wc.png",
                "caption": "Figure 5:Word clouds of text prompts for the text-only generation (T2I) task (left) and the multimodal generation task (right).",
                "position": 1821
            },
            {
                "img": "https://arxiv.org/html/2505.19415/extracted/6477685/figures/aspects.png",
                "caption": "Figure 6:Aspect Distribution of the QA pairs ofAMS.",
                "position": 1831
            },
            {
                "img": "https://arxiv.org/html/2505.19415/extracted/6477685/figures/radar1.png",
                "caption": "Figure 7:The AMS of different models on the text-only generation (T2I) task (left) and the multimodal generation task (right).",
                "position": 1834
            },
            {
                "img": "https://arxiv.org/html/2505.19415/extracted/6477685/figures/radar2.png",
                "caption": "",
                "position": 1837
            },
            {
                "img": "https://arxiv.org/html/2505.19415/extracted/6477685/figures/dataset_overview.png",
                "caption": "Figure 8:Overview of MMIG-Bench.",
                "position": 2099
            },
            {
                "img": "https://arxiv.org/html/2505.19415/extracted/6477685/figures/qual_supp1.png",
                "caption": "Figure 9:More qualitative results of text-only generation methods on MMIG-Bench.",
                "position": 2109
            },
            {
                "img": "https://arxiv.org/html/2505.19415/extracted/6477685/figures/qual_supp2.png",
                "caption": "Figure 10:More qualitative results of text-only generation methods on MMIG-Bench.",
                "position": 2112
            },
            {
                "img": "https://arxiv.org/html/2505.19415/extracted/6477685/figures/qual_supp3.png",
                "caption": "Figure 11:More qualitative results of text-image-conditioned generation methods on MMIG-Bench.",
                "position": 2115
            },
            {
                "img": "https://arxiv.org/html/2505.19415/extracted/6477685/figures/user_study_1.png",
                "caption": "Figure 12:The interface of user study for general prompt following.",
                "position": 2125
            },
            {
                "img": "https://arxiv.org/html/2505.19415/extracted/6477685/figures/user_study_2.png",
                "caption": "Figure 13:The interface of user study for prompt following onObject.",
                "position": 2128
            },
            {
                "img": "https://arxiv.org/html/2505.19415/extracted/6477685/figures/user_study_3.png",
                "caption": "Figure 14:The interface of user study for prompt following onAttributes.",
                "position": 2131
            },
            {
                "img": "https://arxiv.org/html/2505.19415/extracted/6477685/figures/user_study_4.png",
                "caption": "Figure 15:The interface of user study for prompt following onRelations.",
                "position": 2134
            },
            {
                "img": "https://arxiv.org/html/2505.19415/extracted/6477685/figures/user_study_5.png",
                "caption": "Figure 16:The interface of user study for prompt following onNumeracy.",
                "position": 2137
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]