[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Preface",
        "images": []
    },
    {
        "header": "2Introduction",
        "images": []
    },
    {
        "header": "3Related Work",
        "images": []
    },
    {
        "header": "4Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.10248/extracted/6204622/figure/model_architecture.png",
                "caption": "Figure 1:Architecture overview of Step-Video-T2V. Videos are represented by a high-compression Video-VAE, achieving 16x16 spatial and 8x temporal compression ratios. User prompts are encoded using two bilingual pre-trained text encoders to handle both English and Chinese. A DiT with 3D full attention is trained using Flow Matching and is employed to denoise input noise into latent frames, with text embeddings and timesteps serving as conditioning factors. To further enhance the visual quality of the generated videos, a video-based DPO approach is applied, which effectively reduces artifacts and ensures smoother, more realistic video outputs.",
                "position": 289
            },
            {
                "img": "https://arxiv.org/html/2502.10248/x1.png",
                "caption": "Figure 2:Architecture overview of Video-VAE.",
                "position": 314
            },
            {
                "img": "https://arxiv.org/html/2502.10248/extracted/6204622/figure/dit-arch.png",
                "caption": "Figure 3:The model architecture of our bilingual text encoder and DiT with 3D Attention.",
                "position": 450
            },
            {
                "img": "https://arxiv.org/html/2502.10248/x2.png",
                "caption": "Figure 4:Overall pipeline of incorporating human feedback.",
                "position": 553
            },
            {
                "img": "https://arxiv.org/html/2502.10248/x3.png",
                "caption": "Figure 5:We generate different samples with same prompt (\"A ballet dancer practicing in the dance studio\" in this case), and annotate these samples as non-preferred (a) or preferred (b).",
                "position": 592
            }
        ]
    },
    {
        "header": "5Distillation for Step-Video-T2V Turbo",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.10248/x4.jpg",
                "caption": "Figure 6:Generated samples with Step-Video-T2V Turbo with 10 NFE.",
                "position": 641
            },
            {
                "img": "https://arxiv.org/html/2502.10248/x5.jpg",
                "caption": "",
                "position": 645
            },
            {
                "img": "https://arxiv.org/html/2502.10248/x6.jpg",
                "caption": "",
                "position": 647
            },
            {
                "img": "https://arxiv.org/html/2502.10248/x7.jpg",
                "caption": "",
                "position": 649
            },
            {
                "img": "https://arxiv.org/html/2502.10248/x8.jpg",
                "caption": "",
                "position": 651
            },
            {
                "img": "https://arxiv.org/html/2502.10248/x9.jpg",
                "caption": "",
                "position": 653
            }
        ]
    },
    {
        "header": "6System",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.10248/x10.png",
                "caption": "Figure 7:The workflow of Step-Video-T2V training system.",
                "position": 667
            },
            {
                "img": "https://arxiv.org/html/2502.10248/x11.png",
                "caption": "Figure 8:Load balancing with hybrid granularity.",
                "position": 880
            }
        ]
    },
    {
        "header": "7Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.10248/x12.png",
                "caption": "Figure 9:The pipeline of Step-Video-T2V data process.",
                "position": 1201
            }
        ]
    },
    {
        "header": "8Training Strategy",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.10248/extracted/6204622/figure/v2_training_loss.png",
                "caption": "Figure 10:Training curve of different training stages, wheresisubscriptùë†ùëñs_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTdenotes theit‚Å¢hsuperscriptùëñùë°‚Ñéi^{th}italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPTdataset used in the corresponding stage.",
                "position": 1498
            },
            {
                "img": "https://arxiv.org/html/2502.10248/x13.png",
                "caption": "Figure 11:Hierarchical data filtering for pre-training and post-training.",
                "position": 1533
            }
        ]
    },
    {
        "header": "9Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.10248/x14.png",
                "caption": "Figure 12:Four frames sampled from the video generated based on the prompt \"In the video, a Chinese girl is dressed in an exquisite traditional outfit, smiling with a confident and graceful expression. She holds a piece of paper with the words \"we will open source\" clearly written on it. The background features an ancient and elegant setting, complementing the girl‚Äôs demeanor. The entire scene is clear and has a realistic style.\".",
                "position": 2267
            },
            {
                "img": "https://arxiv.org/html/2502.10248/x15.png",
                "caption": "Figure 13:Video reconstruction results compared with public available models, in scenarios including high-motion (first row), text (second row), texture (third row), high-motion combined with text (fourth row), and high-motion combined with texture (fifth row).",
                "position": 2344
            },
            {
                "img": "https://arxiv.org/html/2502.10248/x16.png",
                "caption": "",
                "position": 2348
            },
            {
                "img": "https://arxiv.org/html/2502.10248/x17.png",
                "caption": "",
                "position": 2350
            },
            {
                "img": "https://arxiv.org/html/2502.10248/x18.png",
                "caption": "",
                "position": 2352
            },
            {
                "img": "https://arxiv.org/html/2502.10248/x19.png",
                "caption": "",
                "position": 2354
            },
            {
                "img": "https://arxiv.org/html/2502.10248/x20.png",
                "caption": "Figure 14:Visual comparison of video generation with and without the DPO baseline.",
                "position": 2389
            }
        ]
    },
    {
        "header": "10Discussion",
        "images": []
    },
    {
        "header": "11Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Contributors and Acknowledgments",
        "images": []
    }
]