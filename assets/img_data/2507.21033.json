[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21033/logo_browser.png",
                "caption": "",
                "position": 93
            },
            {
                "img": "https://arxiv.org/html/2507.21033/github.png",
                "caption": "",
                "position": 94
            },
            {
                "img": "https://arxiv.org/html/2507.21033/logo_hf.png",
                "caption": "",
                "position": 95
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21033/x1.png",
                "caption": "Figure 1:An overview of theGPT-Image-Edit-1.5Mdataset. The figure presents qualitative examples from our dataset, showcasing its ability to handle complex and diverse instruction-guided edits. The bar chart on the right demonstrates the effectiveness of our data; a model fine-tuned onGPT-Image-Edit-1.5Machieves a new state-of-the-art score of 7.24 on the GEdit-EN-full benchmark, outperforming existing open-source methods.",
                "position": 133
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Data Curation",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21033/x2.png",
                "caption": "Figure 2:An overview ofGPT-Image-Edit-1.5Mdata curation pipeline. We applied mutiple methods to collect high-quality image-editing data. We used GPT-4o to re-write 10% instructions of the original OmniEdit dataset to make them more accurate, and the input images originally generated by DALL-E in HQ-Edit were re-synthesized by GPT-Image-1 for higher alignment.",
                "position": 196
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21033/x3.png",
                "caption": "Figure 3:The qualitative results of our method on G-Edit-Benchmark-EN.",
                "position": 1398
            },
            {
                "img": "https://arxiv.org/html/2507.21033/x4.png",
                "caption": "Figure 4:The qualitative results of our method on Img-Edit.",
                "position": 1401
            },
            {
                "img": "https://arxiv.org/html/2507.21033/x5.png",
                "caption": "Figure 5:The qualitative results of our method on OmniContext.",
                "position": 1404
            },
            {
                "img": "https://arxiv.org/html/2507.21033/x6.png",
                "caption": "Figure 6:The qualitative results of our method on Complex-Edit.",
                "position": 1407
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledge",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADataset-Specific Processing Details",
        "images": []
    }
]