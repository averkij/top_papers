[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21457/extracted/6484181/o3.png",
                "caption": "",
                "position": 150
            },
            {
                "img": "https://arxiv.org/html/2505.21457/x1.png",
                "caption": "Figure 1:Zero-shot reasoning on theV‚àósuperscriptùëâV^{*}italic_V start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPTbenchmark. When asked ‚ÄúTell me the number on the traffic light‚Äù, Qwen2.5 VL incorrectly refers to unrelated text. In contrast,Active-o3locates and magnifies the precise area on the traffic light, accurately answering 10 through effective spatial localization.",
                "position": 180
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21457/x2.png",
                "caption": "Figure 2:Overview of the proposed Active-O3 framework.\nGiven a multimodal query (e.g., ‚Äúfind all coins‚Äù), traditional task models often miss or misidentify target objects due to limited perceptual coverage. Active-O3 enhances perception by allowing the model to actively propose informative subregions (zoom-in regions) based on a learnable sensing policy.",
                "position": 235
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3MLLM-based Active Perception: Definition and Analysis",
        "images": []
    },
    {
        "header": "4Active-o3",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21457/x3.png",
                "caption": "Figure 4:Visualization details of our proposed method on three datasets.",
                "position": 695
            },
            {
                "img": "https://arxiv.org/html/2505.21457/x4.png",
                "caption": "Figure 5:Comparison of segmentation performance (mIoU) under different zoom-in budgets.",
                "position": 934
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "Appendix AAppendix Overview",
        "images": []
    },
    {
        "header": "Appendix BHeuristic Reward Formulations",
        "images": []
    },
    {
        "header": "Appendix CTask-Aware Reward Formulation",
        "images": []
    },
    {
        "header": "Appendix DDiscussion: Framework Considerations and Insights",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21457/x5.png",
                "caption": "Figure 6:A failure case of GPT-o3 in answering the question: What animal is drawn on that red signicade?. The reasoning trajectory reveals two key limitations: inaccurate region selection (left), and inefficient, near-exhaustive search patterns (right).",
                "position": 1270
            }
        ]
    },
    {
        "header": "Appendix EMethod Details",
        "images": []
    },
    {
        "header": "Appendix FAblation Studies",
        "images": []
    },
    {
        "header": "Appendix GQualitative Visualization",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21457/x6.png",
                "caption": "Figure 10:Zero-shot reasoning on theV‚àósuperscriptùëâV^{*}italic_V start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPTbenchmark (Example 2). Given the question ‚ÄúTell me the number on the police car‚Äù, the baseline model (Qwen2.5 VL) fails to locate the relevant visual evidence due to limited resolution and reasoning capability. In contrast, our method (Active-o3) identifies the appropriate region through contextual reasoning and zoom-in selection. It successfully locates the number 102 on the police car, demonstrating strong spatial inference and fine-grained visual understanding.",
                "position": 1596
            },
            {
                "img": "https://arxiv.org/html/2505.21457/x7.png",
                "caption": "Figure 11:Zero-shot reasoning on theV‚àósuperscriptùëâV^{*}italic_V start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPTbenchmark (Example 3). For the question ‚ÄúWhat is the color of the van?‚Äù, the baseline model (Qwen2.5 VL) fails to detect the presence of the van and incorrectly claims that no such object is visible. In contrast,Active-o3accurately identifies the small red van in the background and correctly answers red, demonstrating its ability to localize and reason over subtle visual cues that are easily overlooked.",
                "position": 1607
            },
            {
                "img": "https://arxiv.org/html/2505.21457/x8.png",
                "caption": "Figure 12:Zero-shot reasoning on theV‚àósuperscriptùëâV^{*}italic_V start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPTbenchmark (Example 4). Given the question ‚ÄúWhat is the color of the watchband?‚Äù, baseline predictions are inconsistent.Active-o3focuses on the wrist of the foreground figure, providing the accurate answer (purple) by effectively zooming in on the fine-grained detail.",
                "position": 1618
            },
            {
                "img": "https://arxiv.org/html/2505.21457/x9.png",
                "caption": "Figure 13:Visualization of Small Object Detection results on SODA-A and SODA-D datasets.\nEach row shows a different example from either SODA-A (top two rows) or SODA-D (remaining rows). The second column illustrates the candidate regions selected by our sensing model. Zoom in for better visibility of fine details and small objects.",
                "position": 1629
            },
            {
                "img": "https://arxiv.org/html/2505.21457/x10.png",
                "caption": "Figure 14:Visualization of object detection results on various scenes from the LVIS dataset. The left column shows the candidate regions selected by our sensing model.",
                "position": 1641
            },
            {
                "img": "https://arxiv.org/html/2505.21457/x11.png",
                "caption": "Figure 15:Interactive segmentation analysis on ThinObjects.Active-o3identifies specific regions with segmentation inaccuracies by reasoning over visual cues. The left example (helicopter) reveals both over-segmentation (e.g., mask spilling beyond the nose and tail) and under-segmentation (e.g., missing rotor parts). The right example (harp) similarly highlights areas where the mask exceeds or misses the object boundary. These results demonstrateActive-o3‚Äôs capability to localize fine-grained segmentation errors, facilitating efficient and targeted mask refinement.",
                "position": 1652
            },
            {
                "img": "https://arxiv.org/html/2505.21457/x12.png",
                "caption": "Figure 16:Failure cases.Left (LVIS): When objects are densely packed, the model fails to distinguish between them, resulting in inaccurate segmentation.Right (SODA-A): For small objects in aerial images, domain gap issues lead to poor localization‚Äîeven if the object is roughly boxed, the model can fail\nto identify it correctly.",
                "position": 1663
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]