[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3The Data Quality Bottleneck in Recommendation CPT",
        "images": []
    },
    {
        "header": "4A Layered Framework for High-Fidelity Synthetic Data",
        "images": []
    },
    {
        "header": "5Empirical Validation of Synthetic Data Quality",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07298/figure/tstr_vs_trtr.png",
                "caption": "Figure 1:TSTR vs TRTR: Recall@K Comparison Across Models. TSTR (Train on Synthetic, Test on Real) consistently outperforms TRTR (Train on Real, Test on Real) across all models (GRU4Rec, NARM, SASRec, STAMP) and K-values. Note: Real UIHs are filtered to the same set of items as Synthetic UIHs.",
                "position": 527
            }
        ]
    },
    {
        "header": "6Principled Scaling Laws for Recommendation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07298/x1.png",
                "caption": "Figure 2:Scaling laws for different domains across different model scales. UIH exhibits strong scaling (αUIH=0.63\\alpha_{\\text{UIH}}=0.63–0.990.99), indicating continued improvement with additional training tokens. General domain shows near-saturation (α<0.1\\alpha<0.1) as expected for pretrained models. The dashed lines are the fitted scaling law curves and parameters are provided as legend of the curve and Table5",
                "position": 881
            },
            {
                "img": "https://arxiv.org/html/2602.07298/x1.png",
                "caption": "",
                "position": 884
            },
            {
                "img": "https://arxiv.org/html/2602.07298/x2.png",
                "caption": "",
                "position": 888
            },
            {
                "img": "https://arxiv.org/html/2602.07298/x3.png",
                "caption": "",
                "position": 893
            },
            {
                "img": "https://arxiv.org/html/2602.07298/x4.png",
                "caption": "",
                "position": 897
            },
            {
                "img": "https://arxiv.org/html/2602.07298/x5.png",
                "caption": "",
                "position": 901
            },
            {
                "img": "https://arxiv.org/html/2602.07298/x6.png",
                "caption": "",
                "position": 906
            },
            {
                "img": "https://arxiv.org/html/2602.07298/x7.png",
                "caption": "",
                "position": 910
            }
        ]
    },
    {
        "header": "7Scaling Analysis and Future Directions",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07298/x8.png",
                "caption": "Figure 3:Ablation studies on selection of domains for recommendation data. Obviously, excluding a domain will causes degradation on the corresponding domain.",
                "position": 1056
            },
            {
                "img": "https://arxiv.org/html/2602.07298/x8.png",
                "caption": "",
                "position": 1059
            },
            {
                "img": "https://arxiv.org/html/2602.07298/x9.png",
                "caption": "",
                "position": 1063
            },
            {
                "img": "https://arxiv.org/html/2602.07298/x10.png",
                "caption": "",
                "position": 1068
            },
            {
                "img": "https://arxiv.org/html/2602.07298/x11.png",
                "caption": "",
                "position": 1072
            },
            {
                "img": "https://arxiv.org/html/2602.07298/x12.png",
                "caption": "Figure 4:We plot the evaluation perplexity for UIH using number of UIH training tokens as X-axis. This figure indicates including CF data (Item-text + CF + UIH and CF + UIH) enables the model to learn UIH better.",
                "position": 1078
            },
            {
                "img": "https://arxiv.org/html/2602.07298/x13.png",
                "caption": "Figure 5:Scaling laws on 4B models with different mixture ratio on UIH. Obviously the perplexity starts to increase when the UIH mixture ratio is too high (reduced UIH data is repeated too many times), which is a sign of overfitting. The higher mixture ratio, this increase happens earlier in training stage. We omitted the figures for CF Both Seen and CF On Unseen here, as they are very similar to CF Both Unseen.",
                "position": 1088
            },
            {
                "img": "https://arxiv.org/html/2602.07298/x13.png",
                "caption": "",
                "position": 1091
            },
            {
                "img": "https://arxiv.org/html/2602.07298/x14.png",
                "caption": "",
                "position": 1095
            },
            {
                "img": "https://arxiv.org/html/2602.07298/x15.png",
                "caption": "",
                "position": 1100
            },
            {
                "img": "https://arxiv.org/html/2602.07298/x16.png",
                "caption": "",
                "position": 1104
            },
            {
                "img": "https://arxiv.org/html/2602.07298/x17.png",
                "caption": "Figure 6:(a) The training loss curve for experiment in Figure2. All the models are trained smoothly and show a monotonically decreasing training loss. (b) the training loss curve for experiment in Figure5. The final training loss decreases in the order of0.5%>1%≈30​M≈15%>2%>5%0.5\\%>1\\%\\approx 30M\\approx 15\\%>2\\%>5\\%.",
                "position": 1110
            },
            {
                "img": "https://arxiv.org/html/2602.07298/x17.png",
                "caption": "",
                "position": 1113
            },
            {
                "img": "https://arxiv.org/html/2602.07298/x18.png",
                "caption": "",
                "position": 1117
            },
            {
                "img": "https://arxiv.org/html/2602.07298/x19.png",
                "caption": "(a)UIH ratio=2%",
                "position": 1123
            },
            {
                "img": "https://arxiv.org/html/2602.07298/x19.png",
                "caption": "(a)UIH ratio=2%",
                "position": 1126
            },
            {
                "img": "https://arxiv.org/html/2602.07298/x20.png",
                "caption": "(b)UIH ratio=15%",
                "position": 1132
            },
            {
                "img": "https://arxiv.org/html/2602.07298/x21.png",
                "caption": "Figure 8:Compute-optimal analysis for models from 0.6B to 8B. Our scaling experiments train models from 0.6B to 8B parameters on 163.84B tokens, spanning from heavily over-trained (454 tokens/param for 0.6B) to near Chinchilla-optimal (20 tokens/param for 8B). While general-domain perplexity follows expected compute-optimal scaling, recommendation tasks—where domain-specific data is repeated 6–7× due to limited unique tokens—exhibit divergent behavior.",
                "position": 1287
            }
        ]
    },
    {
        "header": "8Case Studies",
        "images": []
    },
    {
        "header": "9Discussion and Conclusion",
        "images": []
    },
    {
        "header": "10Impact Statements",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AHyperparameter Tuning for Synthetic UIH Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07298/x22.png",
                "caption": "Figure 12:Scaling laws on 4B models with two tokenizaton methods SAE vs RQ-Kmeans. The other settings are exactly the same. SAE clearly outperformed SAE across all domains.",
                "position": 1818
            },
            {
                "img": "https://arxiv.org/html/2602.07298/x22.png",
                "caption": "",
                "position": 1821
            },
            {
                "img": "https://arxiv.org/html/2602.07298/x23.png",
                "caption": "",
                "position": 1825
            },
            {
                "img": "https://arxiv.org/html/2602.07298/x24.png",
                "caption": "",
                "position": 1830
            },
            {
                "img": "https://arxiv.org/html/2602.07298/x25.png",
                "caption": "",
                "position": 1834
            },
            {
                "img": "https://arxiv.org/html/2602.07298/x26.png",
                "caption": "",
                "position": 1838
            },
            {
                "img": "https://arxiv.org/html/2602.07298/x27.png",
                "caption": "",
                "position": 1843
            },
            {
                "img": "https://arxiv.org/html/2602.07298/x28.png",
                "caption": "",
                "position": 1847
            }
        ]
    },
    {
        "header": "Appendix BMethodology for Semantic Tokenization",
        "images": []
    }
]