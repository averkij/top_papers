[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13063/x1.png",
                "caption": "Figure 1:How many tokens fit into a single input vector?We estimate maximum number of tokens that can be decoded from a single input vector across various language models.",
                "position": 108
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13063/extracted/6214616/imgs/compression_schema.drawio.png",
                "caption": "Figure 2:Compressing text into a[mem]vector.The pre-trained LLM is frozen, and we only finetune one or multiple[mem]vectors to decode the sequence of tokens[t1,t2,‚Ä¶,tN]subscriptùë°1subscriptùë°2‚Ä¶subscriptùë°ùëÅ[t_{1},t_{2},\\ldots,t_{N}][ italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚Ä¶ , italic_t start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ].[mem]vectors are trained for each text separately.",
                "position": 187
            }
        ]
    },
    {
        "header": "4Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13063/x2.png",
                "caption": "Figure 3:Information gain of text compression to[mem]vector doesn‚Äôt depend on language understanding capabilities of models.Compression results for various language models show the relationship between the cross-entropy (CE) of the original and decompressed texts. If the text CE falls below a model-specific threshold (red line), the text is losslessly compressed. This value is a input vector capacity in terms of entropy (Information Gain,CHsubscriptùê∂ùêªC_{H}italic_C start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT).\nFor texts that are not perfectly compressed, the compression process reduces their CE to a consistent, model-specific value (bias of the black dashed line).\nLarger models (e.g., Llama-3.1-8B) can handle longer texts before reaching the compression threshold, due to their greater capacity compared to smaller models (e.g., Pythia-160M). This behavior holds for both natural texts (PG-19) and unnaturalrandomtexts consisting of random word sequences.",
                "position": 436
            },
            {
                "img": "https://arxiv.org/html/2502.13063/x3.png",
                "caption": "Figure 4:Compression scales linearly with the number of trainable[mem]vectors.Dashed lines represent ideal linear scaling, and shaded regions indicate¬±1plus-or-minus1\\pm 1¬± 1std.\nPythia-160m reaches its maximum input context length of 2048 tokens and can successfully encode texts of up to 2016 tokens into 32[mem]input vectors. Llama-3.2-1B can perfectly decode texts of 7168 tokens from just 16 input vectors.",
                "position": 456
            },
            {
                "img": "https://arxiv.org/html/2502.13063/x4.png",
                "caption": "Figure 5:Only fraction of learned input embedding information capacity can be utilized.Top.Maximum token capacity (seeEq.1) against gain in correctly decoded tokens shows differences in utilization of learned memory embedding for studied models.Bottom.Capacity utilization for natural and random texts.",
                "position": 500
            }
        ]
    },
    {
        "header": "5Discussion and Conclusions",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Broader Impact",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AModels and Training Details",
        "images": []
    },
    {
        "header": "Appendix BCollecting Texts from the Fanfics Library",
        "images": []
    },
    {
        "header": "Appendix CResults of Evaluating Text Compression for All Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13063/x5.png",
                "caption": "Figure 6:Information gain of text compression to[mem]vector doesn‚Äôt depend on language understanding capabilities of models.Compression results for various language models show the relationship between the cross-entropy (CE) of the original and decompressed texts. If the text CE falls below a model-specific threshold (red line), the text is losslessly compressed. This value is a input vector capacity in terms of entropy (Information Gain,CHsubscriptùê∂ùêªC_{H}italic_C start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT). For texts that are not perfectly compressed, the compression process reduces their CE to a consistent, model-specific value (bias of the black dashed line).\nLarger models (e.g., Llama-3.1-8B) can handle longer texts before reaching the compression threshold, due to their greater capacity compared to smaller models (e.g., Pythia-160M). This behavior holds for both natural texts (PG-19) and unnaturalrandomtexts consisting of random word sequences.",
                "position": 1332
            },
            {
                "img": "https://arxiv.org/html/2502.13063/x6.png",
                "caption": "Figure 7:Intra/inter-sample embeddings cosine similarity.Empirical\nprobability densities of cosine similarity between intra-sample and inter-sample\nembeddings. Intra-sample similarities are measured between\nof the same sequence of tokens, while inter-sample between different\nones. Measured on GovReportHuang et¬†al. (2021)and Sheared-Llama-1.3BXia et¬†al. (2024).",
                "position": 1398
            },
            {
                "img": "https://arxiv.org/html/2502.13063/x7.png",
                "caption": "Figure 8:Intra-sample Interpolation Accuracies.Interpolation lines\nare provided for all pairs between 32 embeddings of the same input sequence.\nAll interpolation lines are printed with high transparency\nto show denser regions. Grey lines depict minimums and maximums of the\naccuracy for a given interpolation parameterŒ∏ùúÉ\\thetaitalic_Œ∏.",
                "position": 1406
            }
        ]
    },
    {
        "header": "Appendix DUnderstanding the Structure of Compressed Vectors",
        "images": []
    }
]