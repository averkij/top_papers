[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IINTRODUCTION",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.14674/x1.png",
                "caption": "Figure 1:Comparison between thesimpleandrichlanguage guidance for failure recovery: The robot should approach the Oreo (the blue box on the right) directly to grasp it but instead moved to the wrong object (the black box). To help the visuomotor policy recover from this failure, the rich language instruction provides sufficient details, including afailure analysis (in red),spatial movements (in orange)and theexpected outcome (in purple). In contrast, simple language instructions with limited descriptions may not guide the robot effectively, potentially causing it to continue making mistakes.",
                "position": 127
            },
            {
                "img": "https://arxiv.org/html/2409.14674/x2.png",
                "caption": "Figure 2:An overview of automatic rich language-annotated failure-recovery data augmentation pipeline. Given an expert demo (e.g., task goal:close the olive jar), perturbations are injected to expert actions at crucial keyframes (e.g. aligning to, grasping, and releasing a target object) to induce failures. Then, the expert actions are reused as corrections to collect recovery transitions. Finally, all expert and recovery transitions are labelled with rich instructions through GPT-4-turbo. The input for GPT-4-turbo includes the task description, ground-truth object locations, failure types, and heuristic language describing the change in the end-effector’s pose movement at the current step.",
                "position": 168
            }
        ]
    },
    {
        "header": "IIRelated Works",
        "images": []
    },
    {
        "header": "IIIMETHOD",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.14674/x3.png",
                "caption": "Figure 3:The RACER framework consists of: (1) theSupervisor, a VLM that monitors the robot’s behavior, providing feedback for task execution and error correction with rich instructions; and (2) theActor, a language-conditioned visuomotor policy that generates actions based on visual observations, proprioceptive states, and language guidance that includes a high-level task goal and an instruction.",
                "position": 232
            },
            {
                "img": "https://arxiv.org/html/2409.14674/x4.png",
                "caption": "TABLE I:: Multi-task performance comparision of different models on 18 RLbench tasks. RACER+His RACER with human intervention.",
                "position": 273
            }
        ]
    },
    {
        "header": "IVExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.14674/x4.png",
                "caption": "Figure 4:(a) Comparison of RACER’s performance trained with and without failure recovery across three types of instructions. (b) Cross-evaluation of RACER trained with failure recovery, where training and testing were conducted on varying instruction types.",
                "position": 866
            }
        ]
    },
    {
        "header": "VConclusion and Discussions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]