[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.06920/x1.png",
                "caption": "Figure 1:(a) Verifiers synthesized primarily from LLM-generated data exhibit a high failure rate when testing human-written bugs. (b) PCA analysis reveals that LLM-induced errors are highly clustered, indicating systematic weaknesses, whereas human errors are diverse and dispersed, posing greater challenges to existing verifiers.",
                "position": 138
            },
            {
                "img": "https://arxiv.org/html/2507.06920/x2.png",
                "caption": "Figure 2:The code evaluation pipeline and different TCG paradigms.",
                "position": 187
            }
        ]
    },
    {
        "header": "2Evaluating Verifier Quality: Metrics and TCG Paradigms",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.06920/x3.png",
                "caption": "Figure 3:Direct generation issues: (a) Low quality of LLM-generated tests. (b) High self-pass rates suggest model blind spots.",
                "position": 257
            },
            {
                "img": "https://arxiv.org/html/2507.06920/x4.png",
                "caption": "Figure 4:Experimental validation of Input-Interpreter (random sampling) limitations on TCGBench. (a) Detection rate vs. number of test cases (linear scale), showing clear saturation below 100%. (b) Detection rate vs. log of the number of test cases (semi-log scale), illustrating diminishing returns consistent with the theoretical upper bound1‚àí(1‚àíp¬Ø)neff1superscript1¬Øùëùsubscriptùëõeff1-(1-\\bar{p})^{n_{\\mathrm{eff}}}1 - ( 1 - over¬Ø start_ARG italic_p end_ARG ) start_POSTSUPERSCRIPT italic_n start_POSTSUBSCRIPT roman_eff end_POSTSUBSCRIPT end_POSTSUPERSCRIPT, validating the impact of correlationœÅ¬Ø¬Øùúå\\bar{\\rho}over¬Ø start_ARG italic_œÅ end_ARG.",
                "position": 285
            }
        ]
    },
    {
        "header": "3SAGA: A Human-LLM Collaborative Framework for Advanced TCG",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.06920/x5.png",
                "caption": "Figure 5:Overview of the SAGA framework. SAGA leverages both GroundTruth (correct human solutions) and Human Bugs (incorrect submissions) alongside the Problem description. An LLM performs Multi-Dimensional Analysis and Differential Analysis to generate Python Case Scripts for test input synthesis. These scripts are accompanied by Math Explanations (capturing testing strategies and constraints) and Self-Validation code. The generated Test Inputs are then passed to an Interpreter (ground-truth solution) to produce Test Outputs, forming the final test cases.",
                "position": 364
            },
            {
                "img": "https://arxiv.org/html/2507.06920/x6.png",
                "caption": "Figure 6:SAGA outperforms the Baseline (Random Input-Interpreter) and its individual analytical components (Multidimensional Analysis leveragingùíÆhumansubscriptùíÆhuman\\mathcal{S}_{\\text{human}}caligraphic_S start_POSTSUBSCRIPT human end_POSTSUBSCRIPT; Differential Analysis leveragingùíÆwrongsubscriptùíÆwrong\\mathcal{S}_{\\text{wrong}}caligraphic_S start_POSTSUBSCRIPT wrong end_POSTSUBSCRIPT) on the AtCoder subset of the fullTCGBenchacross: (a) Detection Rate, (b) Verifier Accuracy (with AUC@50 values), (c) Distinct Error Pattern Coverage (DEPC), and (d) Diversity Ratio. Dotted lines in (a) & (b) show baseline performance atn=100ùëõ100n=100italic_n = 100. See AppendixFfor SAGA‚Äôs performance on other platforms within the full TCGBench.",
                "position": 437
            }
        ]
    },
    {
        "header": "4Towards Advanced Applications of SAGA",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.06920/x7.png",
                "caption": "Table 3:Verifier Quality: CodeCompass vs. LCB-v6 (Shared Subset, @40 tests).",
                "position": 633
            },
            {
                "img": "https://arxiv.org/html/2507.06920/x7.png",
                "caption": "Table 3:Verifier Quality: CodeCompass vs. LCB-v6 (Shared Subset, @40 tests).",
                "position": 636
            },
            {
                "img": "https://arxiv.org/html/2507.06920/x8.png",
                "caption": "Figure 8:Model Ranking Changes.",
                "position": 680
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ARelated Works",
        "images": []
    },
    {
        "header": "Appendix BFormulation and Interpretation of Advanced Evaluation Metrics",
        "images": []
    },
    {
        "header": "Appendix CTheoretical Analysis of Detection Rate",
        "images": []
    },
    {
        "header": "Appendix DTCGBench: Foundational Dataset for TCG Research",
        "images": []
    },
    {
        "header": "Appendix ETCGBench-Lite and CodeCompass: Curated Set for Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.06920/x9.png",
                "caption": "Figure 9:SAGA performance on Codeforces (CF) and Nowcoder (NC) problems from the full TCGBench dataset: (a) Detection Rate, (b) Verifier Accuracy (with AUC@50), (c) DEPC, and (d) Diversity Ratio, compared to Baseline (Input-Interpreter) and SAGA‚Äôs analytical components (Multidimensional Analysis and Differential Analysis). Dotted lines in (a) & (b) show respective baseline performance atn=100ùëõ100n=100italic_n = 100.",
                "position": 1812
            }
        ]
    },
    {
        "header": "Appendix FSAGA Performance on Full TCGBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.06920/x10.png",
                "caption": "Figure 10:SAGA performance with different LLM backbones (Qwen-Coder, Qwen-72B, DeepSeek-V3) compared to the Baseline (Input-Interpreter with DeepSeek-V3) on Codeforces and Nowcoder portions of the full TCGBench dataset. Metrics: Detection Rate (DR) and Verifier Accuracy (VAcc) at varying test case sizes. Dashed lines indicate baseline performance.",
                "position": 1825
            }
        ]
    },
    {
        "header": "Appendix GDetailed Performance Analysis on TCGBench-Lite by Difficulty",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.06920/x11.png",
                "caption": "Figure 11:Performance comparison (VAcc@50 and DR@50) of SAGA, its components, and baseline TCG methods across Easy, Medium, and Hard problem subsets of TCGBench-Lite. SAGA consistently outperforms baselines across all difficulties, and its full framework generally surpasses its individual analytical components, especially on harder problems.",
                "position": 1835
            }
        ]
    },
    {
        "header": "Appendix HSupplementary Experimental Analyses",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.06920/x12.png",
                "caption": "Figure 12:Heatmap illustrating AUC@50 performance of mixed random test suites. Diagonal elements: suites from a single model (V3, 72B, Coder). Off-diagonal (i,j): mixing random tests from model i and model j.",
                "position": 1909
            }
        ]
    },
    {
        "header": "Appendix IModel Performance on CodeCompass",
        "images": []
    },
    {
        "header": "Appendix JTCGCoder-7B Training Details",
        "images": []
    }
]