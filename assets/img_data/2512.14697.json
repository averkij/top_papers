[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14697/x1.png",
                "caption": "",
                "position": 118
            },
            {
                "img": "https://arxiv.org/html/2512.14697/x2.png",
                "caption": "",
                "position": 121
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14697/x3.png",
                "caption": "(a)d=3d=3.",
                "position": 533
            },
            {
                "img": "https://arxiv.org/html/2512.14697/x3.png",
                "caption": "(a)d=3d=3.",
                "position": 536
            },
            {
                "img": "https://arxiv.org/html/2512.14697/x4.png",
                "caption": "(b)d=24d=24.",
                "position": 542
            }
        ]
    },
    {
        "header": "4Autoregression with a Very Large Codebook",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14697/x5.png",
                "caption": "Figure 3:Training curve for a 16-layer‚àû\\infty-CC model.The Dion optimizer addresses the problem of exploding gradient norm.\nZ-loss effectively regularizes|log‚Å°Z|2|\\log{Z}|^{2}and smoothens the loss and gradient curve, leading to a lower training loss.",
                "position": 718
            },
            {
                "img": "https://arxiv.org/html/2512.14697/x6.png",
                "caption": "Figure 4:Codebook usage histogram.The imbalance in a huge codebook calls for dedicated training tricks in¬†¬ß4.2.\nUsage is computed on IN-1k val-50k over 10 VAR levels.yy-axis in log scale.‚ãÜ: 4,096 VQ codebook indices and density are normalized for illustrative purposes.",
                "position": 755
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14697/x7.png",
                "caption": "Figure 5:VAR Tokenizer.VAR+Œõùüöùüú\\mathbbold{\\Lambda}_{24}-SQ achieves an rFID of 0.84; VAR+Œõùüöùüú\\mathbbold{\\Lambda}_{24}-SQ (+VF) achieves an rFID of 0.92.\nMore metrics are given inTable12.",
                "position": 1147
            },
            {
                "img": "https://arxiv.org/html/2512.14697/x8.png",
                "caption": "Figure 6:VF alignment improves convergence and final generation results, especially recall.The model has 12 layers (240M).",
                "position": 1464
            },
            {
                "img": "https://arxiv.org/html/2512.14697/x9.png",
                "caption": "Figure 7:Scaling effect of the codebook size.Left: Increasing the codebook size improves gFID when the model is large (0.49B).Right: Increasing the codebook size pushes the Precision-Recall Pareto frontier towards the oracle precision-recall derived from the validation set (see the zoom-in at the bottom left).",
                "position": 1556
            }
        ]
    },
    {
        "header": "6Related work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Appendix AConstructing Fibonacci Lattices",
        "images": []
    },
    {
        "header": "Appendix BDetails of the Leech Lattice",
        "images": []
    },
    {
        "header": "Appendix CExperimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14697/x10.png",
                "caption": "(a)BSQ + BCE",
                "position": 2092
            },
            {
                "img": "https://arxiv.org/html/2512.14697/x10.png",
                "caption": "(a)BSQ + BCE",
                "position": 2095
            },
            {
                "img": "https://arxiv.org/html/2512.14697/x11.png",
                "caption": "(b)BSQ + CE",
                "position": 2100
            },
            {
                "img": "https://arxiv.org/html/2512.14697/x12.png",
                "caption": "(c)Œõùüöùüú\\mathbbold{\\Lambda}_{24}-SQ +dd-way CE",
                "position": 2106
            },
            {
                "img": "https://arxiv.org/html/2512.14697/x13.png",
                "caption": "(d)Œõùüöùüú\\mathbbold{\\Lambda}_{24}-SQ + CE",
                "position": 2111
            },
            {
                "img": "https://arxiv.org/html/2512.14697/x14.png",
                "caption": "Figure 9:More sampled generation results of Infinity-CC +Œõùüöùüú\\mathbbold{\\Lambda}_{24}-SQ (2B).Classes are 323: monarch butterfly; 107: jellyfish; 417: balloon; 279: arctic fox.",
                "position": 2211
            }
        ]
    },
    {
        "header": "Appendix DMore Results",
        "images": []
    }
]