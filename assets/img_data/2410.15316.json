[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.15316/extracted/5932896/luatex.p2haJG/1.png",
                "caption": "",
                "position": 97
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Model Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.15316/extracted/5932896/img/1-ichigo-architecture.png",
                "caption": "Figure 1:Ichigo represents speech and text modalities as discrete tokens and uses a uniform transformer-based architecture. It uses WhisperVQ to quantize speech into discrete tokens in the same manner with original text modality.",
                "position": 170
            }
        ]
    },
    {
        "header": "3Datasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.15316/extracted/5932896/img/2-data-processing-pipeline.png",
                "caption": "Figure 2:Data Processing Pipeline for Speech Instruction Dataset Generation. This chart illustrates the multi-stage filtering and conversion process, starting from 6M samples of multiple open-source instruction text datasets. The data undergoes filtering process results in 2.2M samples. Finally, these samples are converted to speech instruction data using WhisperSpeech (TTS) and WhisperVQ (speech to semantic tokens), creating the 1.3M pairs of Speech instruction and Text answer.",
                "position": 243
            }
        ]
    },
    {
        "header": "4Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.15316/extracted/5932896/img/10-data-dis.png",
                "caption": "Figure 3:a.Distribution of data types in the Instruction Fine-tuning dataset. The goal of this specific distribution was to enhance speech comprehension while maintaining robust general language abilities.b.Distribution of data samples used in the enhancement fine-tuning stage. This specific distribution improves Ichigo robustness in handling multi-turn conversations and inaudible inputs.",
                "position": 389
            }
        ]
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.15316/extracted/5932896/img/5-system-prompt.png",
                "caption": "Figure 4:The system prompt used for Ichigo during inference.",
                "position": 651
            },
            {
                "img": "https://arxiv.org/html/2410.15316/extracted/5932896/img/6-cross-modal-instruction.png",
                "caption": "Figure 5:The model follows text-based system prompts during speech-based conversations with users.",
                "position": 657
            },
            {
                "img": "https://arxiv.org/html/2410.15316/extracted/5932896/img/9-multi-inaudible.png",
                "caption": "Figure 6:a.Transcribed dialogue examples using Ichigo. The user-turn is audio input. The examples illustrate zero-shot multi-turn capabilities.b.Ichigo requests clarification from the user when unable to understand the question clearly.",
                "position": 669
            }
        ]
    },
    {
        "header": "6Related works",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Limitations and Future work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Data and Analysis",
        "images": []
    }
]