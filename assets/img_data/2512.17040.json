[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17040/x1.png",
                "caption": "Figure 2:(a)Infinite homography-based approach (ours).By conditioning\non images warped byùêá‚Äã‚àû\\mathbf{H}{\\infty}, the model focuses on learning the parallax relative to the plane at infinity. This parallax is confined to the region between the epipoleùêû‚Ä≤\\mathbf{e}^{\\prime}andùê±‚àû\\mathbf{x}_{\\infty}, as visualized by the yellow segment on the epipolar linel‚Ä≤l^{\\prime}.\nThis spatial constraint helps the model to achieve\nhigher camera pose fidelity with reduced search space.\nEnd-to-end training enables the network to implicitly refine the 3D geometry, correcting inaccuracies inùêó\\mathbf{X}.\n(b)Reprojection-based approach.Inaccuracies in depth estimation lead to unreliable conditions, causing artifacts in the generated image. Since no gradients flow through the depth estimation network, the incorrect reprojection positionùê±‚Ä≤\\mathbf{x^{\\prime}}remains fixed during training, hindering error correction.",
                "position": 143
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17040/x2.png",
                "caption": "Figure 3:Model Architecture Overview.Our model builds upon Wan2.1,\ntraining only newly introduced parameters while freezing pretrained weights.\n(a)DiT block with homography-guided self-attention layer.Homography-guided self-attention layer takes\nsource, target, and warped latents combined with camera embeddings as input,\nand performs per-frame attention, ensuring temporal alignment.\nBy conditioning on warped latents,\nthe model enables rotation-aware reasoning and constrained parallax estimation.\nOnly source and target latents proceed to the subsequent Wan2.1 layers.\n(b)Warping module.\nThis module warps the input latent with infinite homography to handle rotation, then add camera embeddings for translation.\nThis decomposition simplifies reprojection to parallax estimation relative to plane at infinity, enabling higher camera trajectory fidelity.",
                "position": 241
            }
        ]
    },
    {
        "header": "3Motivation",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17040/x3.png",
                "caption": "Figure 4:Visualization of synchronized multi-view synthetic video datasets.\nDifferent trajectories are visualized in different colors.\n(a)SynCamVideo.Captured with stationary cameras placed at distinct positions.\n(b)MultiCamVideo.Captured with dynamic cameras following diverse trajectories, all sharing the same initial frame.\n(c)AugMCV.An augmented version of MultiCamVideo with varied starting poses and different focal lengths.",
                "position": 385
            },
            {
                "img": "https://arxiv.org/html/2512.17040/x4.png",
                "caption": "Figure 5:Qualitative Comparison.(a) shows results on the test split of the AugMCV dataset, and (b) presents results on the WebVid dataset. In both cases, GEN3C and ReCamMaster fail to perform proper viewpoint transformations, largely preserving the initial frame of the source video. ReCamMaster further fails to reflect pose changes in the initial frame even when trajectory interpolation is applied (ReCamMasterw/ Interp). TrajectoryCrafter introduces artifacts due to inaccurate reprojection (highlighted in the orange box). In contrast, our method achieves natural pose transitions while maintaining high visual quality throughout the sequence. Best viewed in zoom.",
                "position": 394
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17040/x5.png",
                "caption": "Figure 6:Qualitative ablation study of proposed components. From top to bottom, each row incrementally adds proposed components, showing cumulative improvements toward the target video. Best viewed in zoom.",
                "position": 764
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Model Architecture Details",
        "images": []
    },
    {
        "header": "8Data Preparation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17040/x6.png",
                "caption": "Figure 8:(a)Example scene from the MultiCamVideo dataset.\nEach row shows a camera trajectory and its corresponding video, with all videos sharing the same initial frame (highlighted in yellow).(b)Augmented dataset examples.\nRows 1&3 show trajectory augmentation, which preserves temporal alignment among trajectories while introducing variation in the initial frame selection (highlighted in yellow).\nRows 2&4 show focal-length augmentation, enabling the model to learn the relationship between focal-length changes and their visual effects in video generation.",
                "position": 867
            }
        ]
    },
    {
        "header": "9Additional Experimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17040/x7.png",
                "caption": "Figure 9:Additional qualitative results on¬†AugMCV dataset.\nBest viewed in zoom.",
                "position": 1242
            },
            {
                "img": "https://arxiv.org/html/2512.17040/x8.png",
                "caption": "Figure 10:Additional qualitative results on WebVid dataset.\nThe first row illustrates the target trajectory and source video, while the remaining rows present the outputs of the baseline methods and our method.\nNote that both GEN3C and ReCamMaster fail to modify the camera pose of the first frame (highlighted in yellow).\nFor ease of comparison, the key regions for comparison are highlighted in orange.\nBest viewed in zoom.",
                "position": 1247
            },
            {
                "img": "https://arxiv.org/html/2512.17040/x9.png",
                "caption": "Figure 11:Additional qualitative results of our method under eight different camera trajectories. Best viewed in zoom.",
                "position": 1255
            }
        ]
    },
    {
        "header": "10Additional Results",
        "images": []
    }
]