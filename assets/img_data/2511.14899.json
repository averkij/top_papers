[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.14899/x1.png",
                "caption": "Figure 1:I-Mix2Mix overview.\nGiven a set of input images, a randomly chosen reference image is edited by the frozen teacher and encoded to serve as the personalized multi-view student’s input latent (Initialization).\nAt each distillation iteration, noisy multi-view latentsζτ\\zeta_{\\tau}are denoised by the student (Student Query), aligned to the teacher’s latent space (Alignment), and perturbed with our forward schedule (Perturbation).\nThe teacher predicts edits with Random Cross-View Attention (Teacher Prediction), where all frames attend to theκ\\kappa’s frame, and the resulting supervision is distilled back into the student (Student Update). After distillation, the student outputs a set of multi-view consistent edited frames.",
                "position": 191
            }
        ]
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.14899/x2.png",
                "caption": "Figure 2:The five SDS stages.",
                "position": 239
            },
            {
                "img": "https://arxiv.org/html/2511.14899/x3.png",
                "caption": "Figure 3:Random Cross-View Attention effect when used for full teacher sampling.",
                "position": 330
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.14899/x4.png",
                "caption": "Figure 4:Qualitative comparison with prior work. The top row shows the original scenes, and the lower rows present edits from different methods. Matching red or purple rectangles indicate pairs of inconsistent regions, which frequently appear in baselines but not in our edits. Please zoom in electronically for details; enlarged views are provided in AppendixF.",
                "position": 471
            },
            {
                "img": "https://arxiv.org/html/2511.14899/",
                "caption": "",
                "position": 475
            },
            {
                "img": "https://arxiv.org/html/2511.14899/x6.png",
                "caption": "Figure 5:Failure cases from variants of the perturbation and teacher prediction stages.\nRows 1–2: alternative forward schedules collapse to near-identity edits.\nRow 3: removing RCVAttn breaks multi-view coherence.\nRow 4: full method output.",
                "position": 732
            },
            {
                "img": "https://arxiv.org/html/2511.14899/x7.png",
                "caption": "Figure 6:Efficiency analysis.\nLeft: peak GPU memory usage for alternative student update and alignment strategies.\nRight: throughput degradation with extended attention as the number of viewsNNincreases.",
                "position": 751
            }
        ]
    },
    {
        "header": "6Discussion: Parallel to Diffusion Guidance",
        "images": []
    },
    {
        "header": "7Conclusion, Limitations, and Future Work",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.14899/x8.png",
                "caption": "Figure 7:Teacher timestep schedule for different skewness factorsff.",
                "position": 854
            }
        ]
    },
    {
        "header": "Appendix B3D Consistency Evaluation",
        "images": []
    },
    {
        "header": "Appendix CEvaluation Scenes and Edits",
        "images": []
    },
    {
        "header": "Appendix DLimitations of Instruct-NeRF2NeRF in Sparse-View Settings",
        "images": []
    },
    {
        "header": "Appendix EStudent and Teacher Limitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.14899/x9.png",
                "caption": "Figure 8:Examples for I-N2N failures in the sparse-view setting.",
                "position": 1308
            },
            {
                "img": "https://arxiv.org/html/2511.14899/figures/ablation-only-student-or-teacher.png",
                "caption": "Figure 9:Student and Teacher models limitation example, on theBearscene andPandaedit.",
                "position": 1314
            },
            {
                "img": "https://arxiv.org/html/2511.14899/x10.png",
                "caption": "Figure 10:SDEdit failure example, on thePersonscene andKnightedit.",
                "position": 1319
            }
        ]
    },
    {
        "header": "Appendix FExtended Qualitative Comparisons with Baselines",
        "images": []
    },
    {
        "header": "Appendix GAdditional Results on Diverse Scenes",
        "images": []
    },
    {
        "header": "Appendix HResults with More Input Frames",
        "images": []
    },
    {
        "header": "Appendix IBeyond Image Editing",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.14899/x11.png",
                "caption": "Figure 11:Comparison to baselines on Face scene edits.",
                "position": 1365
            },
            {
                "img": "https://arxiv.org/html/2511.14899/x12.png",
                "caption": "",
                "position": 1369
            },
            {
                "img": "https://arxiv.org/html/2511.14899/x13.png",
                "caption": "",
                "position": 1371
            },
            {
                "img": "https://arxiv.org/html/2511.14899/x14.png",
                "caption": "Figure 12:Comparison to baselines on Face scene edits.",
                "position": 1376
            },
            {
                "img": "https://arxiv.org/html/2511.14899/x15.png",
                "caption": "",
                "position": 1380
            },
            {
                "img": "https://arxiv.org/html/2511.14899/x16.png",
                "caption": "",
                "position": 1382
            },
            {
                "img": "https://arxiv.org/html/2511.14899/x17.png",
                "caption": "Figure 13:Comparison to baselines on Person scene edits.",
                "position": 1387
            },
            {
                "img": "https://arxiv.org/html/2511.14899/x18.png",
                "caption": "",
                "position": 1391
            },
            {
                "img": "https://arxiv.org/html/2511.14899/x19.png",
                "caption": "",
                "position": 1393
            },
            {
                "img": "https://arxiv.org/html/2511.14899/x20.png",
                "caption": "Figure 14:Comparison to baselines on Person scene edits.",
                "position": 1398
            },
            {
                "img": "https://arxiv.org/html/2511.14899/x21.png",
                "caption": "",
                "position": 1402
            },
            {
                "img": "https://arxiv.org/html/2511.14899/x22.png",
                "caption": "",
                "position": 1404
            },
            {
                "img": "https://arxiv.org/html/2511.14899/x23.png",
                "caption": "Figure 15:Comparison to baselines on Person scene edits.",
                "position": 1409
            },
            {
                "img": "https://arxiv.org/html/2511.14899/x24.png",
                "caption": "",
                "position": 1413
            },
            {
                "img": "https://arxiv.org/html/2511.14899/x25.png",
                "caption": "",
                "position": 1415
            },
            {
                "img": "https://arxiv.org/html/2511.14899/x26.png",
                "caption": "Figure 16:Comparison to baselines on Person scene edits.",
                "position": 1420
            },
            {
                "img": "https://arxiv.org/html/2511.14899/x27.png",
                "caption": "",
                "position": 1424
            },
            {
                "img": "https://arxiv.org/html/2511.14899/x28.png",
                "caption": "",
                "position": 1426
            },
            {
                "img": "https://arxiv.org/html/2511.14899/x29.png",
                "caption": "Figure 17:Comparison to baselines on Bear scene edits.",
                "position": 1431
            },
            {
                "img": "https://arxiv.org/html/2511.14899/x30.png",
                "caption": "",
                "position": 1435
            },
            {
                "img": "https://arxiv.org/html/2511.14899/x31.png",
                "caption": "",
                "position": 1437
            },
            {
                "img": "https://arxiv.org/html/2511.14899/x32.png",
                "caption": "Figure 18:Comparison to baselines on Bear scene edits.",
                "position": 1442
            },
            {
                "img": "https://arxiv.org/html/2511.14899/x33.png",
                "caption": "",
                "position": 1446
            },
            {
                "img": "https://arxiv.org/html/2511.14899/x34.png",
                "caption": "",
                "position": 1448
            },
            {
                "img": "https://arxiv.org/html/2511.14899/x35.png",
                "caption": "Figure 19:I-Mix2Mix edits on the Car (top three rows) and Garden (bottom rows) scenes.",
                "position": 1453
            },
            {
                "img": "https://arxiv.org/html/2511.14899/x36.png",
                "caption": "Figure 20:I-Mix2Mix edits on the Horse (top three rows) and Ignatius (bottom rows) scenes.",
                "position": 1456
            },
            {
                "img": "https://arxiv.org/html/2511.14899/x37.png",
                "caption": "Figure 21:I-Mix2Mix edits on 8 input frames on Face and Bear scenes.",
                "position": 1459
            },
            {
                "img": "https://arxiv.org/html/2511.14899/x38.png",
                "caption": "Figure 22:Example results of I-Mix2Mix with Canny edge map and Depth maps as input, with corresponding ControlNet teachers.",
                "position": 1462
            },
            {
                "img": "https://arxiv.org/html/2511.14899/x39.png",
                "caption": "Figure 23:Example of inconsistencies in a scene marked by a human rater.",
                "position": 1465
            }
        ]
    },
    {
        "header": "Appendix JUse of Large Language Models",
        "images": []
    }
]