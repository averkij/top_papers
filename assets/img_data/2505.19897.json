[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19897/extracted/6477006/Figures/logos/sci_agent_logo.png",
                "caption": "",
                "position": 164
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19897/x1.png",
                "caption": "Figure 1:ScienceBoardis a pioneering computer environment for scientific discovery agents, integrated with a suite of professional software and tools. It serves as an infrastructure enabling computer-using agents to assist in scientific workflows. Based on instructions, agents autonomously interact with the environment via GUI actions or generated code to complete realistic tasks.",
                "position": 231
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3ScienceBoardEnvironment",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19897/x2.png",
                "caption": "Figure 2:Overview of theScienceBoardinfrastructure.\nThe scalable environment is built upon a VM pre-installed with scientific discovery software.\nIt supports both CLI and GUI interfaces to enable autonomous agent interaction.\nFor each task designed to evaluate the agentâ€™s capability as a research assistant,\nan initialization script, configs, and related files are provided.\nAgents perceive the environment through visual or textual modalities, and are expected to plan and act accordingly.\nAfter the interaction,\nan evaluation function determines completion based on the VM internal states.",
                "position": 334
            },
            {
                "img": "https://arxiv.org/html/2505.19897/extracted/6477006/Figures/demos/chimerax_eval.png",
                "caption": "Table 1:Typical evaluation cases ofScienceBoardinclude exact matching, range-based assessment, and numerical tasks with tolerance. We have tailored appropriate evaluation methods for each task. Additional evaluation strategies are detailed in AppendixB.4.",
                "position": 410
            },
            {
                "img": "https://arxiv.org/html/2505.19897/extracted/6477006/Figures/demos/grass_gis_eval.png",
                "caption": "",
                "position": 479
            },
            {
                "img": "https://arxiv.org/html/2505.19897/extracted/6477006/Figures/demos/celestia_eval.png",
                "caption": "",
                "position": 520
            }
        ]
    },
    {
        "header": "4ScienceBoardBenchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19897/x3.png",
                "caption": "Figure 3:The annotation pipeline of the tasks inScienceBoardbenchmark.",
                "position": 652
            },
            {
                "img": "https://arxiv.org/html/2505.19897/x4.png",
                "caption": "Table 2:Statistics ofScienceBoard.",
                "position": 710
            },
            {
                "img": "https://arxiv.org/html/2505.19897/x4.png",
                "caption": "Figure 4:Distribution of tasks inScienceBoardbenchmark.",
                "position": 794
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19897/x5.png",
                "caption": "Figure 5:GUI + CLI v.s. GUI Only.",
                "position": 2137
            }
        ]
    },
    {
        "header": "7Discussion and Future Directions",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Limitations and Broader Impacts",
        "images": []
    },
    {
        "header": "Appendix ADetails ofScienceBoardEnvironment",
        "images": []
    },
    {
        "header": "Appendix BDetails ofScienceBoardBenchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19897/x6.png",
                "caption": "Figure 6:t-SNE visualization of task instructions distribution. The seeds of t-SNE are randomly sampled for each plot.",
                "position": 3855
            },
            {
                "img": "https://arxiv.org/html/2505.19897/x7.png",
                "caption": "",
                "position": 3858
            },
            {
                "img": "https://arxiv.org/html/2505.19897/x8.png",
                "caption": "",
                "position": 3859
            },
            {
                "img": "https://arxiv.org/html/2505.19897/extracted/6477006/Figures/demos/chimerax_eval_ext_1.png",
                "caption": "Table 8:More evaluation cases ofScienceBoardinclude exact matching, range-based assessment, and numerical tasks with tolerance.",
                "position": 3962
            },
            {
                "img": "https://arxiv.org/html/2505.19897/extracted/6477006/Figures/demos/grass_gis_eval_ext_1.png",
                "caption": "",
                "position": 4046
            },
            {
                "img": "https://arxiv.org/html/2505.19897/extracted/6477006/Figures/demos/celestia_eval_ext_1.png",
                "caption": "",
                "position": 4112
            },
            {
                "img": "https://arxiv.org/html/2505.19897/extracted/6477006/Figures/demos/lean_4_eval_ext_1.png",
                "caption": "",
                "position": 4208
            },
            {
                "img": "https://arxiv.org/html/2505.19897/x9.png",
                "caption": "Figure 7:Stability analysis.",
                "position": 4257
            }
        ]
    },
    {
        "header": "Appendix CDetails of Experiments",
        "images": []
    },
    {
        "header": "Appendix DExtended Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19897/x10.png",
                "caption": "Figure 8:Extended analysis of Vision-Only vs. Hybrid Interface.",
                "position": 4443
            },
            {
                "img": "https://arxiv.org/html/2505.19897/x11.png",
                "caption": "Figure 9:Textual v.s. Interactive",
                "position": 4460
            },
            {
                "img": "https://arxiv.org/html/2505.19897/x12.png",
                "caption": "Figure 10:Comparative analysis of task difficulty solve rates.",
                "position": 4479
            },
            {
                "img": "https://arxiv.org/html/2505.19897/extracted/6477006/Figures/cases/atp_task_fail_wrong_file.png",
                "caption": "Figure 11:Use wrong file.",
                "position": 4498
            },
            {
                "img": "https://arxiv.org/html/2505.19897/extracted/6477006/Figures/cases/ka_task_fail_wrong_invoke.jpg",
                "caption": "Figure 12:Function invocation error.",
                "position": 4509
            },
            {
                "img": "https://arxiv.org/html/2505.19897/extracted/6477006/Figures/cases/chimera_task_fail_wrong_cli.jpg",
                "caption": "Figure 13:CLI code error.",
                "position": 4519
            }
        ]
    },
    {
        "header": "Appendix EPrompts",
        "images": []
    }
]