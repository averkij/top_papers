[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19111/x1.png",
                "caption": "Figure 2:Automatic data collection framework.The pipeline consists of two stages: (1) identifying editable regions using VLMs[Qwen2.5-VL]and Grounded-SAM[groundedsam]to generate object masks; (2) generating context-appropriate editing prompts with VLMs and applying sequential diffusion-based edits to the selected regions.",
                "position": 210
            }
        ]
    },
    {
        "header": "3DiffSeg30k benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19111/x2.png",
                "caption": "Figure 3:DiffSeg30k Statistics. The distributions reflect our balanced data collection strategy: (a) Object category distribution shows Food (14,308) are most frequently edited, with an overall balanced distribution across categories. (b) Mask area ratio distribution reveals a decreasing trend from smaller to larger regions. (c) Editing count per model indicates uniform participation of all eight diffusion models across first, second, and third editing rounds. (d) We manually increased the frequency of multi-turn editing, with the first, second, and third rounds occurring at an approximate 1:4:5 ratio, as multi-turn editing subsumes single-turn cases and presents a more challenging scenario. (e) The base image ratio is approximately balanced between real and AI-generated images.",
                "position": 226
            }
        ]
    },
    {
        "header": "4Benchmarking baselines",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19111/x3.png",
                "caption": "Figure 4:Example edited images.For each image pair, the left shows the original image with red contours marking the regions to be edited, and the right presents the corresponding editing result.",
                "position": 440
            },
            {
                "img": "https://arxiv.org/html/2511.19111/x4.png",
                "caption": "Figure 5:Confusion matrix on the semantic segmentation task.",
                "position": 443
            },
            {
                "img": "https://arxiv.org/html/2511.19111/x5.png",
                "caption": "Figure 6:Segmentation results.“SEG” and “DPL” denote SegFormer and Deeplab v3+, respectively. “-B” and “-S” refer to binary and semantic segmentation, respectively. In each image group, the first column shows edited images, the second column shows the ground truth masks, and the third column shows the predicted masks.\nIn each mask,Crimsoncorresponds toStable Diffusion 2,ForestGreencorresponds toKolors,Yellowcorresponds toStable Diffusion 3.5 Medium,RoyalBluecorresponds toFlux,MediumPurplecorresponds toStable Diffusion XL 1.0,TealBluecorresponds toGlide,Graycorresponds toHunyuan-DiT,\nandredcorresponds toKandinsky 2.2.",
                "position": 669
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Relation to global editing",
        "images": []
    },
    {
        "header": "7Do LoRAs affect localization?",
        "images": []
    },
    {
        "header": "8Quality assessment",
        "images": []
    },
    {
        "header": "9Segmentation on different base images",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19111/x6.png",
                "caption": "Figure 7:Chain-of-thought examples for image quality assessment.",
                "position": 803
            },
            {
                "img": "https://arxiv.org/html/2511.19111/x7.png",
                "caption": "Figure 8:Typical low quality edits and VLM ratings.",
                "position": 806
            },
            {
                "img": "https://arxiv.org/html/2511.19111/x8.png",
                "caption": "Figure 9:More examples in DiffSeg30k.For each pair of images, the left is the original image with red contours highlighting areas to be edited. The right is the editing results.",
                "position": 816
            },
            {
                "img": "https://arxiv.org/html/2511.19111/x9.png",
                "caption": "Figure 10:More examples in DiffSeg30k.For each pair of images, the left is the original image with red contours highlighting areas to be edited. The right is the editing results.",
                "position": 819
            }
        ]
    },
    {
        "header": "10Additional visualization of edited images",
        "images": []
    }
]