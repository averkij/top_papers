[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21692/x1.png",
                "caption": "Figure 1:Comparison of human-defined and IRT difficulty estimates for three datasets.\nEach dot represents one question.Left: MATH question distribution by number of reasoning steps(Hendrycks et al.,2021b).Top right: MMLU-Pro question distribution by grade levelWang et al. (2024), with questions lacking assigned grades grouped as “Other Disciplines”.Bottom right: ARC question distribution by grade level(Clark et al.,2018).\nAll distributions are shown across IRT difficulty score bins.",
                "position": 406
            }
        ]
    },
    {
        "header": "3Cross-Difficulty Generalization with Item Response Theory",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21692/x2.png",
                "caption": "Figure 2:Heatmaps showing Spearman correlations between IRT difficulty scores and human metrics.\nColors indicate correlation strength from negative (red) to positive (blue).\nARC shows weak positive correlations across all metrics, while MMLU-Pro demonstrates mostly no or negative correlation between IRT difficulty and common human metrics for difficulty.",
                "position": 460
            },
            {
                "img": "https://arxiv.org/html/2511.21692/x3.png",
                "caption": "Figure 3:Cross-difficulty generalization heatmaps for Qwen2.5 14B Instruct on MMLU Pro dataset.Left: Performance when training on a difficulty bin (y-axis) and testing on another difficulty bin (x-axis).Right: Improvement from finetuning on each bin compared to the zero-shot performance of the model on that bin.\nDiagonal elements are masked as they represent the same train and test data.",
                "position": 465
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21692/x4.png",
                "caption": "Figure 4:Improvement analysis for Qwen2.5 14B Instruct showing the difference between SFT and zero-shot performance. Blue indicates positive improvements (SFT better than zero-shot), red indicates negative improvements (SFT worse than zero-shot).",
                "position": 628
            }
        ]
    },
    {
        "header": "5Findings",
        "images": []
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining Configuration",
        "images": []
    },
    {
        "header": "Appendix BEvaluation Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21692/x5.png",
                "caption": "Figure 5:Zero-shot performance of Qwen 3 4B Instruct 2507 and Qwen 3 30B-A3B Instruct 2507 on the same benchmarks we evaluate against, divided by IRT difficulty bins. These models exhibit lower performance on more difficult bins, despite not being calibrated using their model responses.",
                "position": 1458
            }
        ]
    },
    {
        "header": "Appendix CIRT Model Selection",
        "images": []
    },
    {
        "header": "Appendix DDatasets",
        "images": []
    },
    {
        "header": "Appendix ESupplementary Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21692/x6.png",
                "caption": "Figure 6:Improvement analysis for Qwen2.5 7B Instruct showing the difference between SFT and zero-shot performance.Blue indicates positive improvements (SFT better than zero-shot), red indicates negative improvements (SFT worse than zero-shot).",
                "position": 1616
            },
            {
                "img": "https://arxiv.org/html/2511.21692/x7.png",
                "caption": "Figure 7:Improvement analysis for Qwen2.5 3B Instruct showing the difference between SFT and zero-shot performance.Blue indicates positive improvements (SFT better than zero-shot), red indicates negative improvements (SFT worse than zero-shot).",
                "position": 1619
            },
            {
                "img": "https://arxiv.org/html/2511.21692/x8.png",
                "caption": "Figure 8:Improvement analysis for Qwen2.5 1.5B Instruct showing the difference between SFT and zero-shot performance.Blue indicates positive improvements (SFT better than zero-shot), red indicates negative improvements (SFT worse than zero-shot).",
                "position": 1622
            },
            {
                "img": "https://arxiv.org/html/2511.21692/x9.png",
                "caption": "Figure 9:Improvement analysis for Llama3.1 8B Instruct showing the difference between SFT and zero-shot performance.Blue indicates positive improvements (SFT better than zero-shot), red indicates negative improvements (SFT worse than zero-shot).",
                "position": 1625
            },
            {
                "img": "https://arxiv.org/html/2511.21692/x10.png",
                "caption": "Figure 10:Improvement analysis for Llama3.2 3B Instruct showing the difference between SFT and zero-shot performance.Blue indicates positive improvements (SFT better than zero-shot), red indicates negative improvements (SFT worse than zero-shot).",
                "position": 1628
            },
            {
                "img": "https://arxiv.org/html/2511.21692/x11.png",
                "caption": "Figure 11:Improvement analysis for Llama3.2 1B Instruct showing the difference between SFT and zero-shot performance.Blue indicates positive improvements (SFT better than zero-shot), red indicates negative improvements (SFT worse than zero-shot).",
                "position": 1631
            },
            {
                "img": "https://arxiv.org/html/2511.21692/x12.png",
                "caption": "Figure 12:Improvement analysis on IFEval and GPQA-Extended for Qwen2.5 14B Instruct showing the difference between SFT and zero-shot performance.Blue indicates positive improvements (SFT better than zero-shot), red indicates negative improvements (SFT worse than zero-shot).",
                "position": 1634
            },
            {
                "img": "https://arxiv.org/html/2511.21692/x13.png",
                "caption": "Figure 13:Improvement analysis on IFEval and GPQA-Extended for Qwen2.5 7B Instruct showing the difference between SFT and zero-shot performance.Blue indicates positive improvements (SFT better than zero-shot), red indicates negative improvements (SFT worse than zero-shot).",
                "position": 1637
            },
            {
                "img": "https://arxiv.org/html/2511.21692/x14.png",
                "caption": "Figure 14:Improvement analysis on IFEval and GPQA-Extended for Qwen2.5 3B Instruct showing the difference between SFT and zero-shot performance.Blue indicates positive improvements (SFT better than zero-shot), red indicates negative improvements (SFT worse than zero-shot).",
                "position": 1640
            },
            {
                "img": "https://arxiv.org/html/2511.21692/x15.png",
                "caption": "Figure 15:Improvement analysis on IFEval and GPQA-Extended for Qwen2.5 1.5B Instruct showing the difference between SFT and zero-shot performance.Blue indicates positive improvements (SFT better than zero-shot), red indicates negative improvements (SFT worse than zero-shot).",
                "position": 1643
            },
            {
                "img": "https://arxiv.org/html/2511.21692/x16.png",
                "caption": "Figure 16:Improvement analysis on IFEval and GPQA-Extended for Llama3.1 8B Instruct showing the difference between SFT and zero-shot performance.Blue indicates positive improvements (SFT better than zero-shot), red indicates negative improvements (SFT worse than zero-shot).",
                "position": 1646
            },
            {
                "img": "https://arxiv.org/html/2511.21692/x17.png",
                "caption": "Figure 17:Improvement analysis on IFEval and GPQA-Extended for Llama3.2 3B Instruct showing the difference between SFT and zero-shot performance.Blue indicates positive improvements (SFT better than zero-shot), red indicates negative improvements (SFT worse than zero-shot).",
                "position": 1649
            },
            {
                "img": "https://arxiv.org/html/2511.21692/x18.png",
                "caption": "Figure 18:Improvement analysis on IFEval and GPQA-Extended for Llama3.2 1B Instruct showing the difference between SFT and zero-shot performance.Blue indicates positive improvements (SFT better than zero-shot), red indicates negative improvements (SFT worse than zero-shot).",
                "position": 1652
            }
        ]
    },
    {
        "header": "Appendix FCorrelation",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21692/x19.png",
                "caption": "Figure 19:Correlation heatmaps between IRT-based difficulty scores and human-defined metrics across all eight evaluation datasets.Each heatmap shows Spearman rank correlation coefficients, with colors ranging from red (negative correlation) to blue (positive correlation). Metrics vary by dataset based on available annotations: ARC includes grade level and Bloom’s taxonomy; GSM8K includes reasoning steps; MMLU-Pro includes high school vs. college labels. Across all datasets, most correlations remain weak (|ρ|<0.3|\\rho|<0.3), with only GSM8k’s reasoning steps (ρ=0.49\\rho=0.49) and question length in BBH (ρ=0.42\\rho=0.42) showing moderate positive correlations. Answer length consistently shows negative correlations, while expert-assigned difficulty ratings and educational levels show minimal alignment with model performance patterns.",
                "position": 1665
            }
        ]
    },
    {
        "header": "Appendix GExamples",
        "images": []
    },
    {
        "header": "Appendix HUse of Large Language Models",
        "images": []
    }
]