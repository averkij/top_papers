[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IINTRODUCTION",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23705/x1.png",
                "caption": "Figure 1:In-the-Wild Qualitative Results.The first and third rows present frames extracted from the input videos, while the second and fourth rows display the predictions. Our method achieves robust depth estimation for transparent objects in arbitrary-length, in-the-wild videos. For the full video, please refer to the Appendix video.",
                "position": 98
            },
            {
                "img": "https://arxiv.org/html/2512.23705/x2.png",
                "caption": "Figure 2:We present DKT, a foundational model for fine-grained, temporally consistent depth estimation of in-the-wild videos featuring transparent objects of arbitrary lengths.",
                "position": 107
            }
        ]
    },
    {
        "header": "IIRELATED WORKS",
        "images": []
    },
    {
        "header": "IIIMETHOD",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23705/x3.png",
                "caption": "Figure 3:Overaview of DKT.DKT starts with a pretrained video diffusion model[wan2025]and is finetuned for video depth estimation by concatenating an extra RGB latent with the input latent using LoRA training strategy.",
                "position": 174
            },
            {
                "img": "https://arxiv.org/html/2512.23705/x4.png",
                "caption": "Figure 4:Rendering Pipeline.Scenarios are constructed using static and parametric 3D assets. RGB, depth, and normal videos are rendered by sampling a circular trajectory within the scene.",
                "position": 246
            },
            {
                "img": "https://arxiv.org/html/2512.23705/x5.png",
                "caption": "Figure 5:Qualitative comparison on the ClearPose[chen2022clearpose].For better visualizing the temporal quality, we show the temporal profiles of each result in green boxes, by slicing the depth values along the time axis at the red line positions.",
                "position": 249
            }
        ]
    },
    {
        "header": "IVEXPERIMENT",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23705/x6.png",
                "caption": "Figure 6:Qualitative comparison for video normal estimation on ClearPose[chen2022clearpose].",
                "position": 583
            },
            {
                "img": "https://arxiv.org/html/2512.23705/x7.png",
                "caption": "Figure 7:The effects of different Inference step.",
                "position": 678
            },
            {
                "img": "https://arxiv.org/html/2512.23705/x8.png",
                "caption": "Figure 8:Real-World Setup.We use the Cobot Magic system, which integrates the PiPER Arm and RealSense D435 for our grasping tasks.",
                "position": 775
            },
            {
                "img": "https://arxiv.org/html/2512.23705/x9.png",
                "caption": "Figure 9:Demonstration of Surface Types.",
                "position": 778
            }
        ]
    },
    {
        "header": "VCONCLUSIONS",
        "images": []
    },
    {
        "header": "VIAcknowledgements",
        "images": []
    }
]