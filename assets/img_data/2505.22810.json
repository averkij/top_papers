[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22810/x1.png",
                "caption": "Figure 1:Statistical overview of our VidText.(Left)Video genres included in VidText.(Top Right)Visual Text Instance Distribution.(Bottom Right)Hierarchical Task type settings.",
                "position": 394
            },
            {
                "img": "https://arxiv.org/html/2505.22810/x2.png",
                "caption": "Figure 2:Examples from VidText.\nThe benchmark includes eight tasks, featuring paired perception and reasoning components designed to evaluate thevideo-level,clip-level, andinstance-levelcapabilities of LMMs.\nGiven the video input and textual prompt, models are required to solve the tasks, with ground-truth answers highlighted in green.",
                "position": 397
            }
        ]
    },
    {
        "header": "3Dataset Construction",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22810/x3.png",
                "caption": "Figure 3:Ablation studies on the multi-granularity design of VidText.",
                "position": 835
            },
            {
                "img": "https://arxiv.org/html/2505.22810/x4.png",
                "caption": "Figure 4:Ablation studies on the joint reasoning of video texts and video contents. “HR”, “LR” and “SR” denote Holistic Reasoning, Local Reasoning and Spatial Reasoning, respectively. We visualize “Video content masking” and “Video Text masking” in the right part.",
                "position": 838
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AOverview of Appendix",
        "images": []
    },
    {
        "header": "Appendix BLimitation",
        "images": []
    },
    {
        "header": "Appendix CBroader Impact",
        "images": []
    },
    {
        "header": "Appendix DCollecting Details ofVidText",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22810/x5.png",
                "caption": "Figure 5:Text quantity distribution across six scene categories.",
                "position": 2113
            },
            {
                "img": "https://arxiv.org/html/2505.22810/x6.png",
                "caption": "Figure 6:Video duration distribution inVidText.",
                "position": 2122
            },
            {
                "img": "https://arxiv.org/html/2505.22810/extracted/6490584/figs/VidText_wordcloud.png",
                "caption": "Figure 7:Word cloud of all questions and answers inVidText.",
                "position": 2131
            }
        ]
    },
    {
        "header": "Appendix EDetails of Annotation",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22810/x7.png",
                "caption": "Figure 8:Instance-Level Annotation Guidelines.",
                "position": 2144
            },
            {
                "img": "https://arxiv.org/html/2505.22810/x8.png",
                "caption": "Figure 9:CLip&Video-Level Annotation Guidelines.",
                "position": 2160
            },
            {
                "img": "https://arxiv.org/html/2505.22810/x9.png",
                "caption": "Figure 10:HolisticReasoning Annotation Guidelines.",
                "position": 2169
            },
            {
                "img": "https://arxiv.org/html/2505.22810/x10.png",
                "caption": "Figure 11:LocalReasoning Annotation Guidelines.",
                "position": 2178
            },
            {
                "img": "https://arxiv.org/html/2505.22810/x11.png",
                "caption": "Figure 12:TemporalCausalReasoning Annotation Guidelines.",
                "position": 2187
            },
            {
                "img": "https://arxiv.org/html/2505.22810/x12.png",
                "caption": "Figure 13:SpatialReasoning Annotation Guidelines.",
                "position": 2196
            }
        ]
    },
    {
        "header": "Appendix FDetails of Experimental Settings",
        "images": []
    },
    {
        "header": "Appendix GModel Prompts",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22810/x13.png",
                "caption": "Figure 14:Prompt template used for Aria to generate frame-level captions.",
                "position": 2366
            },
            {
                "img": "https://arxiv.org/html/2505.22810/x14.png",
                "caption": "Figure 15:(Top) More examples of HolisticOCR. (Middle) More examples of HolisticReasoning. (Bottom) More examples of LocalOCR.",
                "position": 2544
            },
            {
                "img": "https://arxiv.org/html/2505.22810/x15.png",
                "caption": "",
                "position": 2548
            },
            {
                "img": "https://arxiv.org/html/2505.22810/x16.png",
                "caption": "",
                "position": 2550
            },
            {
                "img": "https://arxiv.org/html/2505.22810/x17.png",
                "caption": "Figure 16:(Top) More examples of LocalReasoning. (Middle) More examples of TextLocalization. (Bottom) More examples of TemporalCausalReasoning.",
                "position": 2554
            },
            {
                "img": "https://arxiv.org/html/2505.22810/x18.png",
                "caption": "",
                "position": 2558
            },
            {
                "img": "https://arxiv.org/html/2505.22810/x19.png",
                "caption": "",
                "position": 2560
            },
            {
                "img": "https://arxiv.org/html/2505.22810/x20.png",
                "caption": "Figure 17:(Top) More examples of TextTracking. (Bottom) More examples of SpatialReasoning.",
                "position": 2573
            },
            {
                "img": "https://arxiv.org/html/2505.22810/x21.png",
                "caption": "",
                "position": 2577
            }
        ]
    },
    {
        "header": "Appendix HMore Visualization Results",
        "images": []
    }
]