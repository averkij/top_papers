[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.16704/x1.png",
                "caption": "Figure 1:Comparison of standard NTP andReFINE.Standard NTP (top) computes cross-entropy loss at each token position, providing only token-level supervision to fast weight models.ReFINE(bottom) provides sequence-level supervision by generating multi-token rollouts at high-entropy positions, assigning sequence-level rewards from hidden states, and optimizing with RL.",
                "position": 159
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.16704/figures/fast_weights.png",
                "caption": "Figure 2:Comparison of Standard Transformer and Fast Weight Models, adapted fromZhanget al.(2025). Fast weight models replace attention with a fixed-size memory implemented as a weight matrix (WW), and updated according toEq.1.",
                "position": 199
            }
        ]
    },
    {
        "header": "2Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.16704/figures/main_method.png",
                "caption": "Figure 3:ReFINE.We forward the sequence through the policy model and compute token-level entropy values. Sequences are split into chunks and a target token position is sampled from each chunk based on the entropy (Entropy-Based Token Selection). Prefixes are copied from the original sequence up to each target token. The policy model predicts continuations from the prefixes (Rollout Generation). Reward is computed based on the generated rollouts and ground truth tokens (Reward Assignment). Finally, we update the policy model with GRPO (Optimization with RL).",
                "position": 296
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.16704/x2.png",
                "caption": "(a)DeltaNet-1.3B",
                "position": 1335
            },
            {
                "img": "https://arxiv.org/html/2602.16704/x2.png",
                "caption": "(a)DeltaNet-1.3B",
                "position": 1338
            },
            {
                "img": "https://arxiv.org/html/2602.16704/x3.png",
                "caption": "(b)LaCT-760M",
                "position": 1343
            },
            {
                "img": "https://arxiv.org/html/2602.16704/x4.png",
                "caption": "Figure 5:Ablation onkkandcc.We mid-train models with different numbers of tokens per rolloutkk(left) and numbers of chunks per sequencecc(right). We evaluate on 16K-context samples from 12 tasks in LongBench(Baiet al.,2024). With cosine similarity reward, there is an optimalkk. Higherccleads to more NSP training per sequence, which leads to better overall performance.",
                "position": 1383
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ANotation",
        "images": []
    },
    {
        "header": "Appendix BRelated Work",
        "images": []
    },
    {
        "header": "Appendix CDatasets and Benchmarks",
        "images": []
    },
    {
        "header": "Appendix DTraining Configuration",
        "images": []
    },
    {
        "header": "Appendix EAdditional Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.16704/x5.png",
                "caption": "(a)DeltaNet-1.3B",
                "position": 2961
            },
            {
                "img": "https://arxiv.org/html/2602.16704/x5.png",
                "caption": "(a)DeltaNet-1.3B",
                "position": 2964
            },
            {
                "img": "https://arxiv.org/html/2602.16704/x6.png",
                "caption": "(b)LaCT-760M",
                "position": 2969
            },
            {
                "img": "https://arxiv.org/html/2602.16704/x7.png",
                "caption": "(a)LaCT-760M",
                "position": 2982
            },
            {
                "img": "https://arxiv.org/html/2602.16704/x7.png",
                "caption": "(a)LaCT-760M",
                "position": 2985
            },
            {
                "img": "https://arxiv.org/html/2602.16704/x8.png",
                "caption": "(b)DeltaNet-1.3B",
                "position": 2990
            },
            {
                "img": "https://arxiv.org/html/2602.16704/x9.png",
                "caption": "(a)LaCT-760M (Mean)",
                "position": 3004
            },
            {
                "img": "https://arxiv.org/html/2602.16704/x9.png",
                "caption": "(a)LaCT-760M (Mean)",
                "position": 3007
            },
            {
                "img": "https://arxiv.org/html/2602.16704/x10.png",
                "caption": "(b)DeltaNet-1.3B (Mean)",
                "position": 3012
            },
            {
                "img": "https://arxiv.org/html/2602.16704/x11.png",
                "caption": "(c)LaCT-760M (Std)",
                "position": 3018
            },
            {
                "img": "https://arxiv.org/html/2602.16704/x12.png",
                "caption": "(d)DeltaNet-1.3B (Std)",
                "position": 3023
            }
        ]
    },
    {
        "header": "Appendix FQualitative Examples",
        "images": []
    }
]