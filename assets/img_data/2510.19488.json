[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19488/x1.png",
                "caption": "Figure 1:Overview ofVideoAgentTrek. (1)Video Collection: crawl screen-recorded tutorials and filter GUI footage withScreenFilter. (2)Video2Action: an inverse dynamics module that first performs dense action-event detection to localize clips and assign action types, thenaction parameterization(e.g., click coordinates, typed text) to yield structured(screenshot,action,parameters)(\\text{screenshot},\\text{action},\\text{parameters})trajectories. (3)Agent Training: use the mined trajectories for continued pretraining and supervised finetuning of computer-use agents.",
                "position": 177
            }
        ]
    },
    {
        "header": "2VideoAgentTrek",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19488/x2.png",
                "caption": "Figure 2:Video candidate auto-discovery.From seed keywords and tags, we search and evaluate videos, expand to related videos and high-quality channels (≥\\geq80% pass), and iteratively collect GUI-containing videos for VAT.",
                "position": 212
            },
            {
                "img": "https://arxiv.org/html/2510.19488/x3.png",
                "caption": "Figure 3:Domain distribution.",
                "position": 255
            },
            {
                "img": "https://arxiv.org/html/2510.19488/x4.png",
                "caption": "Figure 4:Overview ofVideo2Action: Given a screen-capture video (with optional subtitles), the module (1) detects GUI action events and segments clips, (2) parameterizes each action (type and arguments), and (3) generates step-level thoughts, yielding training-ready sequences of {action clip, action, thought}",
                "position": 272
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19488/x5.png",
                "caption": "Figure 5:Experimental Results on OSWorld-Verified(Xie et al.,2025b)and AgentNetBench(Wang et al.,2025b). VideoAgentTrek demonstrates significant improvements over baseline models, with test-time scaling providing additional performance gains",
                "position": 405
            }
        ]
    },
    {
        "header": "4Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19488/x6.png",
                "caption": "Figure 6:Performance Scaling",
                "position": 592
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AYouTube Video Quality Standards",
        "images": []
    },
    {
        "header": "Appendix BScreenFilterDetails",
        "images": []
    },
    {
        "header": "Appendix CDense Event Detection",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19488/x7.png",
                "caption": "Figure 7:Example training sample for the dense event detector.",
                "position": 1131
            },
            {
                "img": "https://arxiv.org/html/2510.19488/images/cg_idm_train_loss.png",
                "caption": "Figure 8:Dense event detector: training loss (left) and training configuration (right).",
                "position": 1138
            }
        ]
    },
    {
        "header": "Appendix DAction Identification",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19488/x8.png",
                "caption": "Figure 9:Example training sample for the action parametrization model.",
                "position": 1196
            }
        ]
    },
    {
        "header": "Appendix EInner Monologue Generation",
        "images": []
    },
    {
        "header": "Appendix FComputer Use Agent Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19488/x9.png",
                "caption": "Figure 10:Computer Use Agent Training Data (a) Stage-1 training, (b) Stage-2 training.",
                "position": 1328
            },
            {
                "img": "https://arxiv.org/html/2510.19488/x10.png",
                "caption": "",
                "position": 1338
            },
            {
                "img": "https://arxiv.org/html/2510.19488/images/vat_stage1_loss.png",
                "caption": "Figure 11:CUA Stage-1 Training: training loss (left) and training configuration (right).",
                "position": 1345
            },
            {
                "img": "https://arxiv.org/html/2510.19488/images/vat_stage2_loss.png",
                "caption": "Figure 12:CUA Stage-2 Training: training loss (left) and training configuration (right).",
                "position": 1396
            }
        ]
    },
    {
        "header": "Appendix GComputer Use Agent Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19488/x11.png",
                "caption": "Table 9:Average steps across datasets (as reported in their papers).†Estimated from a 5,416-trajectory sample in our corpus.",
                "position": 1957
            },
            {
                "img": "https://arxiv.org/html/2510.19488/x11.png",
                "caption": "Table 10:VideoAgentTrek data distribution of step number",
                "position": 2011
            }
        ]
    },
    {
        "header": "Appendix HVideoAgentTrek Data Analysis",
        "images": []
    }
]