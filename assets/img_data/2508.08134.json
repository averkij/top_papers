[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.08134/scissors.jpg",
                "caption": "",
                "position": 112
            },
            {
                "img": "https://arxiv.org/html/2508.08134/x1.png",
                "caption": "",
                "position": 143
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.08134/x2.png",
                "caption": "Figure 2:Motivation for Trajectory Divergence Map (TDM) Guided Editing.Top:Standard editing methods (red) often produce unstable trajectories compared to the stable reconstruction path (orange), leading to distorted outputs.Middle:Our staged approach first stabilizes the editing trajectory before using the TDM to guide it toward the target concept. This method supports diverse shape modifications (dashed lines).Bottom:The TDM visualizes the dynamically localized editing region across different timesteps, with different border colors correspond to different stages.",
                "position": 278
            },
            {
                "img": "https://arxiv.org/html/2508.08134/x3.png",
                "caption": "Figure 3:Overview of our proposed pipeline.Given a source image and a corresponding prompt, we first perform inversion to obtain the initial noisy latent codexTx_{T}. The editing process is then divided into three stages.In Stage 1, we stabilize the initial denoising trajectory. This is achieved by injecting key-value (KV) features from the inversion path into the denoising model during its initial steps.In Stage 2, we compute a Trajectory Divergence Map (TDM) by comparing the denoising trajectories generated from the source and edit prompts. This map is then processed to precisely identify the regions intended for editing.In Stage 3, we perform the final edit. Guided by the TDM, blended KV features are injected into the final attention blocks of the denoising model to introduce the new semantics. Simultaneously, ControlNet conditions are supplied to ensure the edited regions conform to the original structure.",
                "position": 285
            },
            {
                "img": "https://arxiv.org/html/2508.08134/x4.png",
                "caption": "Figure 4:Qualitative comparisons on various shape-aware editing cases.Follow-Your-Shape successfully performs large-scale shape transformations while preserving the background, demonstrating advantages in both editing ability and visual consistency over existing baselines.",
                "position": 467
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.08134/x5.png",
                "caption": "Figure 5:The qualitative ablation study onkfrontk_{\\text{front}}. As the number of stabilization steps increases, background is better preserved, but shape transformation becomes less effective.",
                "position": 689
            },
            {
                "img": "https://arxiv.org/html/2508.08134/x6.png",
                "caption": "Figure 6:Ablation on ControlNet conditioning timestep and condition strength.(a) Conditioning is applied within five subranges of the normalized denoising interval[0,1][0,1]. Injection at relatively early stages (e.g.,[0.1,0.3][0.1,0.3]) yields the most stable results.\n(b) Condition strength of the depth and canny branches, denoted as(depth,canny)(\\text{depth},\\text{canny}), is varied across different settings. Excessive strength can overly constrain the edit, while insufficient strength leads to weak guidance.",
                "position": 692
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Preliminaries",
        "images": []
    },
    {
        "header": "7Additional Details on ReShapeBench Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.08134/x7.png",
                "caption": "Figure 7:Visualization of Shape Transformation. The objectâ€™s contour, semantic, structure changed while ensuring its subject continuity.",
                "position": 1676
            },
            {
                "img": "https://arxiv.org/html/2508.08134/x8.png",
                "caption": "Figure 8:TDM of Wan2.1 video editing at a single timestep across different frames.",
                "position": 1708
            },
            {
                "img": "https://arxiv.org/html/2508.08134/x9.png",
                "caption": "Figure 9:Comparisons between original prompts and our refined prompts",
                "position": 1711
            },
            {
                "img": "https://arxiv.org/html/2508.08134/x10.png",
                "caption": "Figure 10:Additional Shape-Aware Editing Results",
                "position": 1714
            }
        ]
    },
    {
        "header": "8Limitations and Future Work",
        "images": []
    },
    {
        "header": "9More Editing Results",
        "images": []
    }
]