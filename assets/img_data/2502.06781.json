[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06781/x1.png",
                "caption": "Figure 1:Overall performance between OREAL-32B and some competitive baselines.",
                "position": 110
            }
        ]
    },
    {
        "header": "2Methods",
        "images": []
    },
    {
        "header": "3Implementation",
        "images": []
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06781/x2.png",
                "caption": "Table 2:Ablation study for 7B models performance on MATH-500 with different reinforcement learning settings.",
                "position": 866
            },
            {
                "img": "https://arxiv.org/html/2502.06781/x2.png",
                "caption": "Figure 2:Average test accuracy of 7B models across different training steps.",
                "position": 912
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AToken Level Reward Model Score Visualization",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06781/extracted/6193043/figures/pos_score.png",
                "caption": "Figure A1:Token-level reward model score visualization for a correct response.",
                "position": 1884
            },
            {
                "img": "https://arxiv.org/html/2502.06781/extracted/6193043/figures/neg_score.png",
                "caption": "",
                "position": 1897
            }
        ]
    },
    {
        "header": "Appendix BPrompt",
        "images": []
    }
]