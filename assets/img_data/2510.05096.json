[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05096/figure/logo_.png",
                "caption": "",
                "position": 81
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05096/x1.png",
                "caption": "Figure 1:This work solves two core problems for academic presentations:Left:how to create a presentation video from a paper?PaperTalker – an agent integrates slide, subtitling, cursor grounding, speech synthesis, and talking-head video rendering.Right:how to evaluate a presentation video?Paper2Video – a benchmark with well-designed metrics to evaluate presentation quality.",
                "position": 120
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Paper2Video Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05096/x2.png",
                "caption": "Figure 2:Statistics of Paper2Video benchmark.It spans diverse topics, with presentations comprising 4–28 slides and lasting 2–14 min, providing a valuable benchmark for the automatic generation and evaluation of academic presentation videos.",
                "position": 327
            },
            {
                "img": "https://arxiv.org/html/2510.05096/x3.png",
                "caption": "",
                "position": 337
            },
            {
                "img": "https://arxiv.org/html/2510.05096/x4.png",
                "caption": "",
                "position": 343
            },
            {
                "img": "https://arxiv.org/html/2510.05096/x5.png",
                "caption": "Figure 3:Overview of evaluation metrics.We propose three metrics that systematically evaluate academic presentation video generation from the perspective of the relationship between the generated video and(i)the original paper and(ii)the human-made video.",
                "position": 382
            }
        ]
    },
    {
        "header": "4PaperTalker Agent",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05096/x6.png",
                "caption": "Figure 4:Overview of PaperTalker.Our pipeline comprises three key modules:(i)tree search visual choice for fine-grained slide layout optimization;(ii)a GUI-grounded model paired with WhisperX for spatiotemporally aligned cursor grounding; and(iii)slide-wise parallel generation for efficiency.",
                "position": 399
            },
            {
                "img": "https://arxiv.org/html/2510.05096/x7.png",
                "caption": "Figure 5:Tree Search Visual Choice.It combines a rule-based proposal mechanism with VLM-based scoring to select the optimal candidate.",
                "position": 416
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05096/x8.png",
                "caption": "Figure 6:Human evaluation. We randomly sample the generated results from ten papers for evaluation.",
                "position": 486
            },
            {
                "img": "https://arxiv.org/html/2510.05096/x9.png",
                "caption": "Figure 7:Visualization of generated results.PaperTalker produces presentation videos with rich, fine-grained slide content, accurate cursor grounding, and an engaging talker; in contrast, Veo3DeepMind [2025]yields blurred text and incomplete information coverage, while PresentAgentShi et al. [2025]produces text-heavy slides and suffers from overfull layout issues and inaccurate information (e.g., title and institutions).",
                "position": 754
            }
        ]
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AChecklist",
        "images": []
    },
    {
        "header": "Appendix BEvaluation Metrics",
        "images": []
    },
    {
        "header": "Appendix CExperiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05096/x10.png",
                "caption": "Figure 8:Slide Visualization of Tree Search Visual Choice.The first row shows slide results before layout refinement, while the second row shows their corresponding slides after refinement.",
                "position": 1383
            }
        ]
    },
    {
        "header": "Appendix DPrompts",
        "images": []
    }
]