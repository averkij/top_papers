[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.02260/x1.png",
                "caption": "Figure 1:MagicFace takes in the AU changes based on the input portrait and edit the portrait to exhibit different expressions. The edited image respects the AU condition and preserve identity, pose, background as well as other facial details.",
                "position": 82
            }
        ]
    },
    {
        "header": "IIRelated Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.02260/x2.png",
                "caption": "Figure 2:A display showcasing various action units and their corresponding intensity scales.Only a set of commonly used AUs are displayed here. For a complete collection of AUs with descriptions see[15].",
                "position": 124
            }
        ]
    },
    {
        "header": "IIIMethod",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.02260/x3.png",
                "caption": "Figure 3:Overview of MagicFace.During training, a pair of images with the same identity but different pose, background as well as expressions is used respectively as the identity image and the target. AU variations are computed by an estimator and then sent into the denoising UNet as AU condition. Pose and background of the target is parsed into an image condition independently, dealed with an Attribute Controller and then inputted to the denoising UNet. ID encoder takes in the encoded identity image to edit for target AUs, where features in each transformer blocks are merged into the corresponding ones of the denoising UNet via self-attention. During inference, the conditional image will be parsed from the identity image.",
                "position": 148
            },
            {
                "img": "https://arxiv.org/html/2501.02260/x4.png",
                "caption": "Figure 4:Qualitative comparison for facial expression editing.Other methods exhibit shortcomings in preserving the identity, pose or background, and they are unable to support continuous control over the intensity of generated expressions, whereas our method excels in maintaining exceptional detail features and meanwhile allows flexible, fine-grained control to the expression intensity. Please zoom in for more detailed observation.",
                "position": 265
            }
        ]
    },
    {
        "header": "IVExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.02260/x5.png",
                "caption": "Figure 5:Results of using laboratory dataset for training.In lab setting, the model cannot generalize to image from natural settings (the second row).",
                "position": 393
            },
            {
                "img": "https://arxiv.org/html/2501.02260/x6.png",
                "caption": "Figure 6:Ablation study for different model architectures.Zoom in to view facial details.",
                "position": 396
            },
            {
                "img": "https://arxiv.org/html/2501.02260/x7.png",
                "caption": "Figure 7:Demonstration of out-of-domain testing for extreme expressions as well as unseen styles.Some AUs are out of the range [0, 5]. The left column displays the editing results of real person photos with extreme expressions and the right column displays that of the cartoon characters.",
                "position": 545
            },
            {
                "img": "https://arxiv.org/html/2501.02260/x8.png",
                "caption": "Figure 8:Qualitative comparison of images edited by MagicFace trained without/with AU dropout and using different values of the guidance scaleŒ±ùõº\\alphaitalic_Œ±.The AU variation to edit is AU4[-6]. Please zoom in for more details.",
                "position": 554
            },
            {
                "img": "https://arxiv.org/html/2501.02260/x9.png",
                "caption": "Figure 9:Visualization of self-attention maps (lst row) and cross-attention maps (2nd row) from the denoising UNet.Please zoom in to observe more details.",
                "position": 557
            }
        ]
    },
    {
        "header": "VImpact Statement",
        "images": []
    },
    {
        "header": "VILimitations and Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]