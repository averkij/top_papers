[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05046/x1.png",
                "caption": "Figure 1:Qualitative results produced by our training-free video editing framework.Given a source video and a target textual prompt, our method performs precise and semantically faithful edits while preserving the spatial content and motion dynamics of unedited regions. The results exhibit strong alignment with the editing instructions, high visual fidelity, and consistent temporal coherence across frames.",
                "position": 74
            },
            {
                "img": "https://arxiv.org/html/2506.05046/x2.png",
                "caption": "Figure 2:Framework overview.(a) Editing Flow Generation:Our method first infers velocity flows, then combines cross-attention maps with our mask generation technique to spatially refine the initial flow.(b) Differential Averaging Guidance:We iteratively generate a high-quality token and multiple baseline tokens, establish a stable evolution direction through their difference calculation, which guides token refinement for the final editing vector field.",
                "position": 148
            },
            {
                "img": "https://arxiv.org/html/2506.05046/x3.png",
                "caption": "Figure 3:Qualitative results of FlowDirector.Our method showcases superior performance across a wide range of video editing challenges. The results highlight high fidelity to prompts, preservation of unedited regions and motion, temporal coherence, and visual plausibility. Best viewed zoomed-in.",
                "position": 292
            },
            {
                "img": "https://arxiv.org/html/2506.05046/x4.png",
                "caption": "Figure 4:Qualitative comparison.Our method outperforms previous methods across diverse editing tasks, demonstrating superior visual quality and temporal consistency. Best viewed zoomed-in.",
                "position": 311
            },
            {
                "img": "https://arxiv.org/html/2506.05046/x5.png",
                "caption": "Figure 5:Ablation study of Spatially Attentive Flow Correction. The “Flow” row displays editing flows from two consecutive late denoising steps. Left: with SAFC. Right: without SAFC.",
                "position": 482
            },
            {
                "img": "https://arxiv.org/html/2506.05046/x5.png",
                "caption": "Figure 5:Ablation study of Spatially Attentive Flow Correction. The “Flow” row displays editing flows from two consecutive late denoising steps. Left: with SAFC. Right: without SAFC.",
                "position": 485
            },
            {
                "img": "https://arxiv.org/html/2506.05046/x6.png",
                "caption": "Figure 6:Ablation study of Differential Averaging Guidance. Best viewed zoomed-in.",
                "position": 490
            },
            {
                "img": "https://arxiv.org/html/2506.05046/x7.png",
                "caption": "Figure 7:Qualitative comparison between the editing results of a multi-round inference averaging strategy and using a DAG.The Sample Average strategy is set to use a regular averaging strategy for 20 rounds of iterative inference at every denoising step to obtain the editing flow. The DAG setting uses 4 rounds of iterative inference to obtain a high-quality estimate and perform reinforcement-guided generation of the editing flow.",
                "position": 1547
            },
            {
                "img": "https://arxiv.org/html/2506.05046/x8.png",
                "caption": "Figure 8:An example of editing failure due to incomplete target text replacement.When attempting to edit a “bear” into a “dinosaur,” if the target prompt erroneously retains descriptions of the “bear” (e.g., “…capturing the bear’s deliberate movements” instead of a full replacement with dinosaur-related descriptions), the edited video exhibits significant residual features of the original “bear.”",
                "position": 1579
            },
            {
                "img": "https://arxiv.org/html/2506.05046/x9.png",
                "caption": "Figure 9:Qualitative comparison between advanced text-to-video editing approaches and FlowDirector.Best viewed zoomed-in.",
                "position": 1595
            },
            {
                "img": "https://arxiv.org/html/2506.05046/x10.png",
                "caption": "Figure 10:Qualitative comparison between advanced text-to-video editing approaches and FlowDirector.Best viewed zoomed-in.",
                "position": 1598
            },
            {
                "img": "https://arxiv.org/html/2506.05046/x11.png",
                "caption": "Figure 11:Qualitative comparison between advanced text-to-video editing approaches and FlowDirector.Best viewed zoomed-in.",
                "position": 1601
            },
            {
                "img": "https://arxiv.org/html/2506.05046/x12.png",
                "caption": "Figure 12:More Qualitative Results.Our method performs precise and semantically faithful edits while preserving the spatial content and motion dynamics of unedited regions. The results exhibit strong alignment with the editing instructions, high visual fidelity, and consistent temporal coherence across frames. Best viewed zoomed-in.",
                "position": 1611
            },
            {
                "img": "https://arxiv.org/html/2506.05046/x13.png",
                "caption": "Figure 13:More Qualitative Results.Our method performs precise and semantically faithful edits while preserving the spatial content and motion dynamics of unedited regions. The results exhibit strong alignment with the editing instructions, high visual fidelity, and consistent temporal coherence across frames. Best viewed zoomed-in.",
                "position": 1614
            },
            {
                "img": "https://arxiv.org/html/2506.05046/x14.png",
                "caption": "Figure 14:More Qualitative Results.Our method performs precise and semantically faithful edits while preserving the spatial content and motion dynamics of unedited regions. The results exhibit strong alignment with the editing instructions, high visual fidelity, and consistent temporal coherence across frames. Best viewed zoomed-in.",
                "position": 1617
            },
            {
                "img": "https://arxiv.org/html/2506.05046/x15.png",
                "caption": "Figure 15:More Qualitative Results.Our method performs precise and semantically faithful edits while preserving the spatial content and motion dynamics of unedited regions. The results exhibit strong alignment with the editing instructions, high visual fidelity, and consistent temporal coherence across frames. Best viewed zoomed-in.",
                "position": 1620
            },
            {
                "img": "https://arxiv.org/html/2506.05046/x16.png",
                "caption": "Figure 16:More Qualitative Results.Our method performs precise and semantically faithful edits while preserving the spatial content and motion dynamics of unedited regions. The results exhibit strong alignment with the editing instructions, high visual fidelity, and consistent temporal coherence across frames. Best viewed zoomed-in.",
                "position": 1623
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]