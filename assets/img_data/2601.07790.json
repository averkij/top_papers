[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Data",
        "images": []
    },
    {
        "header": "4Methodologies",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.07790/figures/workflow.png",
                "caption": "Figure 1:Workflow for evaluating log-severity classification using SLMs and SRLMs. System logs are collected from real-world servers, processed through prompting strategies (zero-shot, few-shot, and RAG with FAISS), evaluated across various open-source models, and assessed by accuracy and inference speed.",
                "position": 492
            },
            {
                "img": "https://arxiv.org/html/2601.07790/figures/rag.png",
                "caption": "Figure 2:Retrieval-Augmented Generation (RAG) Pipeline for Syslog severity classification where incoming logs are embedded using a local LM Studio encoder (768-dimensional embeddings) and indexed with FAISS. At inference, each query log is embedded, and the top-kknearest training vectors (kk=5) are retrieved using L2 similarity and appended to a zero-shot prompt. The LLM then predicts the Syslog severity label conditioned on the retrieved examples.",
                "position": 554
            }
        ]
    },
    {
        "header": "5Results",
        "images": []
    },
    {
        "header": "6Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.07790/figures/top-k.png",
                "caption": "Figure 3:Performance of Qwen3-1.7B across retrieval depths. Reducingkkfrom 5 to 1 decreases latency but does not improve accuracy, indicating limited retrieval integration at this model scale.",
                "position": 952
            },
            {
                "img": "https://arxiv.org/html/2601.07790/figures/acc.png",
                "caption": "Figure 4:Performance of evaluated models under zero-shot, few-shot, and RAG prompting. Retrieval yields the strongest overall gains, with notable variation across architectures.",
                "position": 968
            },
            {
                "img": "https://arxiv.org/html/2601.07790/figures/time.png",
                "caption": "Figure 5:Average per-sample inference time. Several models approach real-time latency, though performance varies significantly by scale and prompting mode.",
                "position": 971
            }
        ]
    },
    {
        "header": "7Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]