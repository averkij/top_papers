[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01786/x1.png",
                "caption": "",
                "position": 167
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01786/x2.png",
                "caption": "Figure 2:VLM system setup used by BlenderGym. It follows a generator-verifier structure, where the generator further contains a brainstormer and a code editor. The generator takes in start-goal image pairs along with Python script of the start scene, and then edits the Python script of the start scene to achieve the goal scene based on their visual differences. The verifier takes a pair of renders of proposed edits and selects a single best edit, which is propagated back to the generator for the next iteration.",
                "position": 250
            },
            {
                "img": "https://arxiv.org/html/2504.01786/x3.png",
                "caption": "Figure 3:Examples of VLM/human generated outputs on lighting and blend shape tasks. Even powerful closed-source VLMs fail to generate lighting settings with accurate colors. In the blend shape task above, only Claude 3.5 Sonnet correctly captures the overall appearance of the police car but still misses the difference of the front light. We report N-CLIP (10âˆ’3superscript10310^{-3}10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT) for all edits above as a calibration.",
                "position": 294
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01786/extracted/6307723/images/verifier_alignment_cut.jpg",
                "caption": "Figure 4:Human-VLM and inter-human verifier alignment rate. All models perform above the random baseline (0.5) yet differ notably, with even the highest-aligned model, Claude-3.5-Sonnet (0.66), falling short of inter-human alignment (0.79).",
                "position": 665
            },
            {
                "img": "https://arxiv.org/html/2504.01786/x4.png",
                "caption": "Figure 5:Verifier scaling results with InternVL2-8B, Claude3.5 Sonnet, and GPT4o. We show that increasing verifier queries brings the selected edit closer to the goal. Bounding boxes of all object instances of interest are shown.",
                "position": 725
            },
            {
                "img": "https://arxiv.org/html/2504.01786/x5.png",
                "caption": "Figure 6:The impact of compute allocation on VLM system performance for 16 blend shape task instances. We show photometric loss at VeriRatios of 0.73, 0.62, and 0.33. We observe that with fewer compute, the generation process dominates the performance of the whole pipeline, while with a large compute budget, increasing verifier compute is more effective. The compute unit is query as every 100 generation/verification queries incur a similar cost of 0.45 USD.",
                "position": 751
            }
        ]
    },
    {
        "header": "5Conclusion and Discussion",
        "images": []
    },
    {
        "header": "Appendix Overview",
        "images": []
    },
    {
        "header": "Appendix S1Verifier Scaling",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01786/extracted/6307723/images/all_metrics_shapekey.png",
                "caption": "Figure 7:Impact of compute allocation in all three metrics onblend shape manipulationtask.",
                "position": 845
            },
            {
                "img": "https://arxiv.org/html/2504.01786/extracted/6307723/images/all_metrics_lighting.png",
                "caption": "Figure 8:Impact of compute allocation in both PL and N-CLIP metrics onlighting adjustmenttask.",
                "position": 848
            }
        ]
    },
    {
        "header": "Appendix S2Generator Failure Cases",
        "images": []
    },
    {
        "header": "Appendix S3Verifier Failure Cases",
        "images": []
    },
    {
        "header": "Appendix S4Calibration of Evaluation Metrics",
        "images": []
    },
    {
        "header": "Appendix S5Camera Viewpoint Selection",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01786/x6.png",
                "caption": "Figure 9:Examples of generator failure for blend shape manipulation. We present the most visually obvious difference observed by the VLM, the code change proposed, and a failure analysis.",
                "position": 911
            },
            {
                "img": "https://arxiv.org/html/2504.01786/x7.png",
                "caption": "Figure 10:Examples of generator failure for lighting and procedural geometry. We present the most visually obvious difference observed by the VLM, the code change proposed, and a failure analysis.",
                "position": 915
            },
            {
                "img": "https://arxiv.org/html/2504.01786/x8.png",
                "caption": "Figure 11:Examples of generator failure for procedural material editing. We present the most visually obvious difference observed by the VLM, the code change proposed, and a failure analysis.",
                "position": 919
            },
            {
                "img": "https://arxiv.org/html/2504.01786/x9.png",
                "caption": "Figure 12:A complete verification process of a 3x4 tree generated by GPT4o on one task instance. We observe that a more human-aligned candidate is generated in edit iteration 2 but is not selected by the verifier.",
                "position": 923
            },
            {
                "img": "https://arxiv.org/html/2504.01786/x10.png",
                "caption": "Figure 13:Examples of verifier decisions for a blend shape instance. N/A indicates that no reasoning is provided by the verifier. The candidates differ across models since they are rendered from edits generated by the model itself.",
                "position": 927
            },
            {
                "img": "https://arxiv.org/html/2504.01786/x11.png",
                "caption": "Figure 14:Examples of verifier decisions for an object placement instance. N/A indicates that no reasoning is provided by the verifier. The candidates differ across models since they are rendered from edits generated by the model itself.",
                "position": 931
            },
            {
                "img": "https://arxiv.org/html/2504.01786/x12.png",
                "caption": "Figure 15:Calibration of metric values with render images of VLM system output edits. We present start scene, goal scene, human user edit, and VLM system edits side by side with their corresponding metric values.",
                "position": 935
            },
            {
                "img": "https://arxiv.org/html/2504.01786/x13.png",
                "caption": "Figure 16:Examples of VLM-input views and evaluation-only views. Images on the first column are all rendered from comprehensive views.",
                "position": 939
            },
            {
                "img": "https://arxiv.org/html/2504.01786/x14.png",
                "caption": "Figure 17:Examples of VLM-input views and evaluation-only views. Images on the first column are all rendered from comprehensive views.",
                "position": 943
            }
        ]
    },
    {
        "header": "Appendix S6Prompts for VLM System",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]