[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04942/x1.png",
                "caption": "Figure 1:Overview of theœÄ\\pi-Distill framework.(1) Successful trajectories (not shown) are collected from a frontier agent that exposes only actions while hiding its Chain-of-Thought.\n(2) These trajectories are transformed into training-time privileged information (PI) and used to sample a PI-conditioned teacher policyœÄŒ∏T‚Äã(ùê®‚à£s,ùêà){\\color[rgb]{0.390625,0.2578125,0.828125}\\definecolor[named]{pgfstrokecolor}{rgb}{0.390625,0.2578125,0.828125}\\pi^{\\mathrm{T}}_{\\theta}(\\mathbf{o}\\mid s,\\mathbf{I})}.\n(3) The PI-conditioned teacher and an unconditioned studentœÄŒ∏S‚Äã(ùê®|s){\\color[rgb]{0.90234375,0.59765625,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0.90234375,0.59765625,0}\\pi^{\\text{S}}_{\\theta}(\\mathbf{o}|s)}share parameters and are trained jointly, enabling transfer of privileged knowledge to a test-time policy that acts without PI.",
                "position": 259
            },
            {
                "img": "https://arxiv.org/html/2602.04942/x1.png",
                "caption": "Figure 1:Overview of theœÄ\\pi-Distill framework.(1) Successful trajectories (not shown) are collected from a frontier agent that exposes only actions while hiding its Chain-of-Thought.\n(2) These trajectories are transformed into training-time privileged information (PI) and used to sample a PI-conditioned teacher policyœÄŒ∏T‚Äã(ùê®‚à£s,ùêà){\\color[rgb]{0.390625,0.2578125,0.828125}\\definecolor[named]{pgfstrokecolor}{rgb}{0.390625,0.2578125,0.828125}\\pi^{\\mathrm{T}}_{\\theta}(\\mathbf{o}\\mid s,\\mathbf{I})}.\n(3) The PI-conditioned teacher and an unconditioned studentœÄŒ∏S‚Äã(ùê®|s){\\color[rgb]{0.90234375,0.59765625,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0.90234375,0.59765625,0}\\pi^{\\text{S}}_{\\theta}(\\mathbf{o}|s)}share parameters and are trained jointly, enabling transfer of privileged knowledge to a test-time policy that acts without PI.",
                "position": 262
            },
            {
                "img": "https://arxiv.org/html/2602.04942/x2.png",
                "caption": "Figure 2:Results forQwen3-8Bon TravelPlanner andœÑ\\tau-Bench retail.The dashed line separating SFT w/ CoT + RL denotes that this method is not a required baseline, as all other methods do not rely on frontier-model CoT traces.\nWe find thatbothœÄ\\pi-Distill and OPSD substantially outperform all baselines in this setting.",
                "position": 272
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Methods",
        "images": []
    },
    {
        "header": "4Experimental Setting",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04942/x3.png",
                "caption": "Figure 3:Deriving PI from frontier model traces.The left panel illustrates sampling trajectories from aclosed-sourcefrontier model, where full CoT reasoning is typically occludedopenai2024openaio1card. We transform these raw traces into three types of PI with varying information density: (1)Tool Calls & Arguments, retaining the exact actions invoked by the frontier model; (2)Tool Calls Only, where arguments are stripped; and (3)Self-Generated Hints, where the student model summarizes the frontier trajectory into a concise hint.",
                "position": 522
            },
            {
                "img": "https://arxiv.org/html/2602.04942/x4.png",
                "caption": "Table 1:Evaluation results on Travel Planner,œÑ\\tau-Bench (Retail), andœÑ\\tau-Bench (Airline). Shaded rows denote our methods.Boldvalues indicate the best performance within each model category, whileunderlinedvalues indicate the second-best. Results show mean¬±\\pmstandard deviation across three random seeds. We find that bothœÄ\\pi-Distill and OPSD effectively leverage PI, consistently outperforming all baselines that lack access to frontier reasoning traces. Furthermore, both methods can surpass SFT w/ CoT + RL on TravelPlanner, withœÄ\\pi-Distill also achieving superior performance onœÑ\\tau-Bench.",
                "position": 578
            }
        ]
    },
    {
        "header": "5Main Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04942/x4.png",
                "caption": "Figure 4:Evaluation on Out-of-Domain Environments.We report Pass@1 and Pass@10 on the GEM search-tool benchmark suite (7 datasets) forQwen 3models andR1-Distill-Llama-8B, using the best checkpoint selected onœÑ\\tau-Bench Retail. Bars show mean¬±\\pmstandard errors over three seeds per dataset, comparingœÄ\\pi-Distill variants (œÄT{\\color[rgb]{0.390625,0.2578125,0.828125}\\definecolor[named]{pgfstrokecolor}{rgb}{0.390625,0.2578125,0.828125}\\pi^{\\mathrm{T}}}(Œ±=1\\alpha=1),œÄS{\\color[rgb]{0.90234375,0.59765625,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0.90234375,0.59765625,0}\\pi^{\\text{S}}}(Œ±=0\\alpha=0),œÄT+œÄS{\\color[rgb]{0.390625,0.2578125,0.828125}\\definecolor[named]{pgfstrokecolor}{rgb}{0.390625,0.2578125,0.828125}\\pi^{\\mathrm{T}}}+{\\color[rgb]{0.90234375,0.59765625,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0.90234375,0.59765625,0}\\pi^{\\text{S}}}(Œ±=0.5\\alpha=0.5)) and OPSD against SFT w/ CoT + RL,œÄBase\\pi_{\\text{Base}}, and standard RL. The dashed line separating SFT w/ CoT + RL denotes that this method is not considered a required baseline, as all PI methods avoid relying on frontier-model CoT traces. We consistently find that both algorithms exhibit substantially less forgetting than standard RL. Moreover, we findœÄ\\pi-Distill and OPSD generalize significantly better than SFT w/ CoT + RL when usingQwen 3-8B.",
                "position": 1018
            }
        ]
    },
    {
        "header": "6Out of Domain Experiments (OOD)",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04942/x5.png",
                "caption": "Figure 5:Impact of PI Types and Algorithms on Performance.\nWe compare held-out performance onœÑ\\tau-Bench (top row) and Travel Planner (bottom row) across three base models and three PI types (colors). The scatter plots map final scores against the initial teacher-student divergence (DKL‚Äã(œÄbaseT‚à•œÄbaseS)D_{\\mathrm{KL}}(\\pi^{T}_{\\text{base}}\\,\\|\\,\\pi^{S}_{\\text{base}})), while the bar-charts display the PI utility (Œî\\Delta) on training tasks. Key observations:\n(1) Higher initial KL divergence generally correlates with decreased final performance.\n(2) Joint training (Œ±=0.5\\alpha=0.5,‚ñ≥\\triangle) is the most stable configuration, performing best in 6/16 scenarios and worst in a single one.\n(3) Student-only training (Œ±=0\\alpha=0,‚ñ°\\square) requires low KL and positive utility (note the failure in Planner QWEN3 8B whereŒî<0\\Delta<0). Conversely, Teacher-only training (Œ±=1\\alpha=1,‚óã\\bigcirc) degrades as KL increases or fails due to policy collapse when KL is negligible.",
                "position": 1053
            }
        ]
    },
    {
        "header": "7What Matters When Using Train-Time PI",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04942/x6.png",
                "caption": "Figure 6:Maximum Improvement (Œîmax\\Delta_{\\text{max}}) across PI Types and Algorithms.We compare the peak performance gain over baselines for different PI variants (x-axis) and training configurations (Œ±\\alpha) on TravelPlanner (bottom) andœÑ\\tau-Bench (top). We find that PI types which initially underperform (e.g., self-generated hints on Planner,Qwen3-8B) can yield substantial gains when the teacher is trained to utilize them (Œ±>0\\alpha>0), confirming that learning to leverage PI is an important factor in transferring from teacherœÄT{\\color[rgb]{0.390625,0.2578125,0.828125}\\definecolor[named]{pgfstrokecolor}{rgb}{0.390625,0.2578125,0.828125}\\pi^{\\mathrm{T}}}to studentœÄS{\\color[rgb]{0.90234375,0.59765625,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0.90234375,0.59765625,0}\\pi^{\\text{S}}}.",
                "position": 1067
            },
            {
                "img": "https://arxiv.org/html/2602.04942/x7.png",
                "caption": "Figure 7:Training KL betweenœÄT{\\color[rgb]{0.390625,0.2578125,0.828125}\\definecolor[named]{pgfstrokecolor}{rgb}{0.390625,0.2578125,0.828125}\\pi^{\\mathrm{T}}}andœÄS{\\color[rgb]{0.90234375,0.59765625,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0.90234375,0.59765625,0}\\pi^{\\text{S}}}during training the teacherœÄS‚Äã(Œ±=1){\\color[rgb]{0.90234375,0.59765625,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0.90234375,0.59765625,0}\\pi^{\\text{S}}}(\\alpha=1)onœÑ\\tau-Bench. We observe an early KL collapse makingœÄS‚âàœÄT{\\color[rgb]{0.90234375,0.59765625,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0.90234375,0.59765625,0}\\pi^{\\text{S}}}\\approx{\\color[rgb]{0.390625,0.2578125,0.828125}\\definecolor[named]{pgfstrokecolor}{rgb}{0.390625,0.2578125,0.828125}\\pi^{\\mathrm{T}}}. We attribute the under performance ofœÄT{\\color[rgb]{0.390625,0.2578125,0.828125}\\definecolor[named]{pgfstrokecolor}{rgb}{0.390625,0.2578125,0.828125}\\pi^{\\mathrm{T}}}on low KL settings to this collapse.",
                "position": 1074
            },
            {
                "img": "https://arxiv.org/html/2602.04942/x8.png",
                "caption": "Figure 8:Performance and Stability Analysis of OPSD.We compare held-out performance onœÑ\\tau-Bench (top row) and Travel Planner (bottom row) across three base models and three PI types (colors). The scatter plots map final scores against the student-teacher KL divergence (DKL‚Äã(œÄS‚à•œÄT)D_{\\text{KL}}({\\color[rgb]{0.90234375,0.59765625,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0.90234375,0.59765625,0}\\pi^{\\text{S}}}\\|{\\color[rgb]{0.390625,0.2578125,0.828125}\\definecolor[named]{pgfstrokecolor}{rgb}{0.390625,0.2578125,0.828125}\\pi^{\\mathrm{T}}})), while the bar-charts display the PI utility (Œî\\Delta). Key observations:\n(1) UnlikeœÄ\\pi-Distill, higher KL are not always detrimental, rather information richness of PI is most important, finding (Tool Calls & Arguments) often performs best (e.g., all results on Travel Planner andQwen3-4BonœÑ‚àí\\tau-Bench ).\n(2) Excessive KL can override positive utility (noteQwen3-8BonœÑ\\tau-Bench, whereŒî>0\\Delta>0but the high KL degrades performance).\n(3)R1-Distill-Llama-8Bconsistently struggles, which we attribute to either extreme KL divergence (œÑ\\tau-Bench) or negative PI utility (TravelPlanner).",
                "position": 1127
            }
        ]
    },
    {
        "header": "8Ablation onŒ≤\\beta",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04942/x9.png",
                "caption": "Figure 9:Evaluation performance throughout training forœÄ\\pi-Distill variants across varying KL penalties (Œ≤\\beta). Runs that deteriorate significantly early are truncated for visual clarity. We observe that for settings involving teacher updates (Œ±>0\\alpha>0), a non-zero penalty (Œ≤>0\\beta>0) is crucial for stabilizing training and achieving peak performance. Here error bars indicate standard errors. Discrepancies between plot and table values are addressed in App.F",
                "position": 1155
            }
        ]
    },
    {
        "header": "9Related Work",
        "images": []
    },
    {
        "header": "10Limitations & Future Work",
        "images": []
    },
    {
        "header": "11Conclusion",
        "images": []
    },
    {
        "header": "12Acknowledgments",
        "images": []
    },
    {
        "header": "13Author Contributions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AConnections To Variational EM",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04942/x10.png",
                "caption": "Figure 10:Illustration of the Variational EM procedure for policy optimization. The initial student policy,œÄŒ∏S\\pi_{\\theta}^{S}, lacks support over trajectories with positive rewards (R‚Äã(ùê®,ùê¨)>0R(\\mathbf{o},\\mathbf{s})>0), preventing direct improvement. To address this: (1) the teacher policyœÄŒ∏T\\pi_{\\theta}^{T}is optimized viaJTeacher‚Äã(Œ∏)J_{\\text{Teacher}}(\\theta)to sample successful traces and approximate the optimal policyœÄ‚àó\\pi^{*}; (2) the student policy is then updated viaJSFT‚Äã(Œ∏)J_{\\text{SFT}}(\\theta)to distill the knowledge from the teacher. While this two-step procedure is principled, it is computationally inefficient due to the requirement of maintaining dual parameter sets and distinct training phases. In contrast,œÄ\\pi-Distill simplifies this pipeline into a single-phase process, providing superior performance with reduced complexity.",
                "position": 1239
            },
            {
                "img": "https://arxiv.org/html/2602.04942/x11.png",
                "caption": "Figure 11:Performance of Sequential EM variants onœÑ\\tau-Bench Retail (Qwen3-8B). We compare these methods against SFT baselines. While standard EM RFT (light blue) fails to match SFT w/ CoT (yellow), replacing RFT with off-policy RL (EM Off-Pol, purple) successfully closes allows for EM to outperform SFT w CoT. The strongest performance comes fromJTeacher‚Äã(Œ∏)J_{\\text{Teacher}}(\\theta)(dark blue), demonstrating that parameter sharing between the teacher and student yields the most effective transfer. Shaded regions indicate standard error across 3 seeds.",
                "position": 1351
            }
        ]
    },
    {
        "header": "Appendix BOn-Policy Self-DistillationJOPSDJ_{\\text{OPSD}}",
        "images": []
    },
    {
        "header": "Appendix CDetailed Algorithms",
        "images": []
    },
    {
        "header": "Appendix DAdditional Ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04942/x12.png",
                "caption": "Figure 12:Performance with and without leakage penalty forœÄT‚Äã(Œ±=1)\\pi^{\\text{T}}(\\alpha=1). We find that although the penalty does reduce the leakage of the privileged information (seeFigureÀú13) it does not affect performance.",
                "position": 1708
            },
            {
                "img": "https://arxiv.org/html/2602.04942/x13.png",
                "caption": "Figure 13:Proportion fo traces leaking privileged information as training progresses, we see that regardless PI type, the leakage increases with more gradient steps, finding using a leakage penalty reduces this proportion, but not substantially.",
                "position": 1711
            },
            {
                "img": "https://arxiv.org/html/2602.04942/x14.png",
                "caption": "Figure 14:Proportion fo traces leaking privileged information whenevaluatingœÄS{\\color[rgb]{0.90234375,0.59765625,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0.90234375,0.59765625,0}\\pi^{\\text{S}}}as training progresses, we see that there is no increase in leakage when evaluated as training goes on.",
                "position": 1719
            },
            {
                "img": "https://arxiv.org/html/2602.04942/x15.png",
                "caption": "Figure 15:Ablation between using reverse-Kl between the teacher and a prior. We ablate over two possible priors,œÄbase\\pi_{\\text{base}}andœÄŒ∏S\\pi^{\\text{S}}_{\\theta}. We find that usingœÄŒ∏S\\pi^{\\text{S}}_{\\theta}to be highly important in obtaining best performance.",
                "position": 1730
            },
            {
                "img": "https://arxiv.org/html/2602.04942/x16.png",
                "caption": "Figure 16:Full sweep forŒ≤\\betaover variants ofœÄ\\pi-Distill. All experiments are given 600 gradient step budget, where here we cutoff experiment types early if they crash or do not continue learning. Consistent performance gains are seen forŒ≤>0\\beta>0across both 8B and 4B scales. We find thatŒ≤>0\\beta>0is important in 14/18 cases. Specifically, we find it most important when trainingœÄT{\\color[rgb]{0.390625,0.2578125,0.828125}\\definecolor[named]{pgfstrokecolor}{rgb}{0.390625,0.2578125,0.828125}\\pi^{\\mathrm{T}}}and less important with student-only trainingœÄS{\\color[rgb]{0.90234375,0.59765625,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0.90234375,0.59765625,0}\\pi^{\\text{S}}}.",
                "position": 1741
            },
            {
                "img": "https://arxiv.org/html/2602.04942/x17.png",
                "caption": "",
                "position": 1752
            }
        ]
    },
    {
        "header": "Appendix EImplementation setup - Further details",
        "images": []
    },
    {
        "header": "Appendix FPlot and Table Discrepancies",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04942/x18.png",
                "caption": "Figure 17:Comparison aggregating trtaining runs versus the individual training runs. The left figure displays the mean score across seeds at each gradient step, while the right panel highlights the individual trajectories where peaks occur at different intervals. The reported table value (31.11%31.11\\%) represents the average of these individual seed peaks.",
                "position": 1946
            }
        ]
    },
    {
        "header": "Appendix GPrompts",
        "images": []
    }
]