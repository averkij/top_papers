[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20475/x1.png",
                "caption": "Figure 1:To answer one-to-many factual queries, we found that LMs first use attention to propagate subject information to the last token, which is used by MLPs to promote all possible answers. Attention then attends to and suppresses the subject and previous answer tokens, while MLPs amplify the suppression and further promote new answers.",
                "position": 147
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Problem Settings",
        "images": []
    },
    {
        "header": "4Decoding the Overall Mechanism",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20475/extracted/6254738/figures/unembed_attn_mlp_outputs/unembed_component_output_logit.png",
                "caption": "Figure 2:Logit of the subject and answer tokens from unembedding attention and MLP outputs. Attention primarily promotes the subject at the middle layers, then promotes new answers and suppresses previous answers at deeper layers. MLPs consistently promote all answers; at deeper layers, they also decrease the logits of previously generated answers. Early layers are omitted as logits are near zero. SeeAppendixCfor full figures.",
                "position": 321
            }
        ]
    },
    {
        "header": "5Which Tokens Matter?",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20475/extracted/6254738/figures/causal_tracing/causal_tracing_for_main.png",
                "caption": "Figure 3:The impact of attention and MLPs‚Äô activations on LMs‚Äô predictions when intervening on the subject (left) and previous answer tokens (right) at step2222. The probability differences all peak around or above0.550.550.550.55, reflecting the importance of both the subject and previous answer tokens. SeeAppendixDfor figures of other answer steps, which have similar patterns.",
                "position": 365
            }
        ]
    },
    {
        "header": "6Analyze Critical Tokens",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20475/extracted/6254738/figures/critical_token_analysis/subject_logits.png",
                "caption": "Figure 4:Token Lens logit values (left) and MLP logit differences (right) of subject and answer tokens when attending to or knocking out the subject tokens. Attention promotes and extracts subject information in the middle layers but suppresses it in later layers. MLPs promote the answers and suppress the subject at deeper layers.",
                "position": 453
            },
            {
                "img": "https://arxiv.org/html/2502.20475/extracted/6254738/figures/critical_token_analysis/prev_ans_logits.png",
                "caption": "Figure 5:Token Lens logits of the attended previous answers (left) are negative at deeper layers, showing that attention suppresses prior answers. Negative MLP logit differences (right) for previous answers and positive differences for new answers suggest that MLPs use previous answer tokens for both repetition avoidance and knowledge recall.",
                "position": 485
            },
            {
                "img": "https://arxiv.org/html/2502.20475/extracted/6254738/figures/critical_token_analysis/last_token_logits.png",
                "caption": "Figure 6:Token Lens logit values (left) and MLP logit differences (right) of subject and answer tokens when attending to or knocking out the last token. Attention promotes all three answers and the subject at the final layers, prioritizingo(i)superscriptùëúùëño^{(i)}italic_o start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPTat each stepiùëñiitalic_i. Late-layer MLP logit differences are negative for the subject and answers, possibly compensating for the absence of direct attention to the last token to encourage correct outputs.",
                "position": 514
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APrompt Templates",
        "images": []
    },
    {
        "header": "Appendix BSample Responses and Example of Analysis Data Creation",
        "images": []
    },
    {
        "header": "Appendix CDecoding Attention and MLP Outputs Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20475/extracted/6254738/figures/unembed_attn_mlp_outputs/full_figures/unembed_attn_output_logit.png",
                "caption": "Figure 7:Logit of the subject and answer tokens from decoding the attention outputs across layers and answer steps. Attention primarily promotes the subject at the middle layers while promoting new answers and suppressing previously generated ones at deeper layers.",
                "position": 1142
            },
            {
                "img": "https://arxiv.org/html/2502.20475/extracted/6254738/figures/unembed_attn_mlp_outputs/full_figures/unembed_mlp_output_logit.png",
                "caption": "Figure 8:Logits of the subject and answer tokens from decoding the MLP outputs across layers and answer steps. The consistently positive logits for all three answers illustrate that MLPs promote multiple answers simultaneously. MLPs also decrease the logits of previously generated answers in deeper layers, contributing to repetition suppression alongside attention.",
                "position": 1145
            },
            {
                "img": "https://arxiv.org/html/2502.20475/extracted/6254738/figures/unembed_attn_mlp_outputs/data_model_specific/omit_early_layers_figures/country_cities/meta-llama/Meta-Llama-3-8B-Instruct/attn_mlp_output_logit.png",
                "caption": "Figure 9:Attention and MLP output logits of Llama-3-8B-Instruct on Country-Cities dataset.",
                "position": 1149
            },
            {
                "img": "https://arxiv.org/html/2502.20475/extracted/6254738/figures/unembed_attn_mlp_outputs/data_model_specific/omit_early_layers_figures/country_cities/mistralai/Mistral-7B-Instruct-v0.2/attn_mlp_output_logit.png",
                "caption": "Figure 10:Attention and MLP output logits of Mistral-7B-Instruct on Country-Cities dataset.",
                "position": 1152
            },
            {
                "img": "https://arxiv.org/html/2502.20475/extracted/6254738/figures/unembed_attn_mlp_outputs/data_model_specific/omit_early_layers_figures/artist_songs/meta-llama/Meta-Llama-3-8B-Instruct/attn_mlp_output_logit.png",
                "caption": "Figure 11:Attention and MLP output logits of Llama-3-8B-Instruct on Artist-Songs dataset.",
                "position": 1155
            },
            {
                "img": "https://arxiv.org/html/2502.20475/extracted/6254738/figures/unembed_attn_mlp_outputs/data_model_specific/omit_early_layers_figures/artist_songs/mistralai/Mistral-7B-Instruct-v0.2/attn_mlp_output_logit.png",
                "caption": "Figure 12:Attention and MLP output logits of Mistral-7B-Instruct on Artist-Songs dataset.",
                "position": 1158
            },
            {
                "img": "https://arxiv.org/html/2502.20475/extracted/6254738/figures/unembed_attn_mlp_outputs/data_model_specific/omit_early_layers_figures/actor_movies/meta-llama/Meta-Llama-3-8B-Instruct/attn_mlp_output_logit.png",
                "caption": "Figure 13:Attention and MLP output logits of Llama-3-8B-Instruct on Actor-Movies dataset.",
                "position": 1161
            },
            {
                "img": "https://arxiv.org/html/2502.20475/extracted/6254738/figures/unembed_attn_mlp_outputs/data_model_specific/omit_early_layers_figures/actor_movies/mistralai/Mistral-7B-Instruct-v0.2/attn_mlp_output_logit.png",
                "caption": "Figure 14:Attention and MLP output logits of Mistral-7B-Instruct on Actor-Movies dataset.",
                "position": 1164
            }
        ]
    },
    {
        "header": "Appendix DCausal Tracing Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20475/extracted/6254738/figures/causal_tracing/subject_causal_tracing.png",
                "caption": "Figure 15:The impact of attention and MLPs‚Äô activations on LMs‚Äô predictions when intervening on the subject tokens across three answer steps macro-averaged across all models and datasets. Attention contributions dominate in the middle layers at the last token, while MLPs are important in early layers at the subject token and in late layers at the last token. The probability differences all peak around or above0.550.550.550.55, reflecting the importance of the subject tokens.",
                "position": 1174
            },
            {
                "img": "https://arxiv.org/html/2502.20475/extracted/6254738/figures/causal_tracing/prev_ans_causal_tracing.png",
                "caption": "Figure 16:The impact of attention and MLPs‚Äô activations on LMs‚Äô predictions when intervening on previous answer tokens at step2222and3333macro-averaged across all models and datasets. Attention is important in both the middle and the last layers at the last token position. MLPs‚Äô contributions are critical in early layers at the previous answer positions and in final layers at the last token. The probability differences all peak around or above0.540.540.540.54, indicating previous answer tokens are critical to models‚Äô predictions.",
                "position": 1178
            }
        ]
    },
    {
        "header": "Appendix ECritical Token Analysis Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20475/extracted/6254738/figures/critical_token_analysis/macro_avg_figures/full_figures/subject/token_lens_logit.png",
                "caption": "Figure 17:Token Lens logit values of subject and answer tokens across layers and answer steps when attending to the subject (macro-averaged across all datasets and models). Attention promotes and extracts subject information in the middle layers while suppressing it in later layers.",
                "position": 1188
            },
            {
                "img": "https://arxiv.org/html/2502.20475/extracted/6254738/figures/critical_token_analysis/macro_avg_figures/full_figures/subject/attn_knockout_logit.png",
                "caption": "Figure 18:Average logit differences of the subject and answer tokens between MLP outputs with and without knocking out attention from the last to the subject tokens. Positive logit differences for the answers and negative differences for the subject in later layers show that MLPs use the subject information to promote answers and suppress the subject.",
                "position": 1191
            },
            {
                "img": "https://arxiv.org/html/2502.20475/extracted/6254738/figures/critical_token_analysis/macro_avg_figures/full_figures/prev_ans/token_lens_logit.png",
                "caption": "Figure 19:Token Lens logit values subject and answer tokens across layers and answer steps2222and3333(macro-averaged across all datasets and models) when attending to previous answers. The logit of the attended answer is negative at later layers, showing that the attention is suppressing previously generated answers.",
                "position": 1194
            },
            {
                "img": "https://arxiv.org/html/2502.20475/extracted/6254738/figures/critical_token_analysis/macro_avg_figures/full_figures/prev_ans/attn_knockout_logit.png",
                "caption": "Figure 20:Average logit differences for subject and answer tokens between MLP outputs with and without knocking attention from the last to previous answer tokens. All previously generated answer tokens have negative logits, and all new answers have positive logits. This result suggests that MLPs use previous answers for both repetition suppression and new answer promotion.",
                "position": 1197
            },
            {
                "img": "https://arxiv.org/html/2502.20475/extracted/6254738/figures/critical_token_analysis/macro_avg_figures/full_figures/last_token/token_lens_logit.png",
                "caption": "Figure 21:Token Lens logit values of subject and answer tokens across layers and answer steps when attending to the last token (macro-averaged across all datasets and models). Attention promotes all three answers and the subject at the final layers, with the answer for the current step having the highest logit.",
                "position": 1200
            },
            {
                "img": "https://arxiv.org/html/2502.20475/extracted/6254738/figures/critical_token_analysis/macro_avg_figures/full_figures/last_token/attn_knockout_logit.png",
                "caption": "Figure 22:Average logit differences for subject and answer tokens between MLP outputs with and without knocking attention from the last token to itself. The logit differences of all three answers and the subject are negative at the late layers, meaning MLPs output higher logits when it does not have information from the last token. This pattern may suggest a compensation behavior for the absence of direct attention to the last token to encourage the outputs to still be correct.",
                "position": 1203
            },
            {
                "img": "https://arxiv.org/html/2502.20475/extracted/6254738/figures/critical_token_analysis/data_model_specific/omit_early_layer_figures/country_cities/meta-llama/Meta-Llama-3-8B-Instruct/subject_logit.png",
                "caption": "Figure 23:Token Lens logit values (left) and MLP logit differences (right) of subject and answer tokens of Llama-3-8B-Instruct on Country-Cities dataset when attending to or knocking out the subject tokens.",
                "position": 1206
            },
            {
                "img": "https://arxiv.org/html/2502.20475/extracted/6254738/figures/critical_token_analysis/data_model_specific/omit_early_layer_figures/country_cities/mistralai/Mistral-7B-Instruct-v0.2/subject_logit.png",
                "caption": "Figure 24:Token Lens logit values (left) and MLP logit differences (right) of subject and answer tokens of Mistral-7B-Instruct on Country-Cities dataset when attending to or knocking out the subject tokens.",
                "position": 1209
            },
            {
                "img": "https://arxiv.org/html/2502.20475/extracted/6254738/figures/critical_token_analysis/data_model_specific/omit_early_layer_figures/artist_songs/meta-llama/Meta-Llama-3-8B-Instruct/subject_logit.png",
                "caption": "Figure 25:Token Lens logit values (left) and MLP logit differences (right) of subject and answer tokens of Llama-3-8B-Instruct on Artist-Songs dataset when attending to or knocking out the subject tokens.",
                "position": 1212
            },
            {
                "img": "https://arxiv.org/html/2502.20475/extracted/6254738/figures/critical_token_analysis/data_model_specific/omit_early_layer_figures/artist_songs/mistralai/Mistral-7B-Instruct-v0.2/subject_logit.png",
                "caption": "Figure 26:Token Lens logit values (left) and MLP logit differences (right) of subject and answer tokens of Mistral-7B-Instruct on Artist-Songs dataset when attending to or knocking out the subject tokens.",
                "position": 1215
            },
            {
                "img": "https://arxiv.org/html/2502.20475/extracted/6254738/figures/critical_token_analysis/data_model_specific/omit_early_layer_figures/actor_movies/meta-llama/Meta-Llama-3-8B-Instruct/subject_logit.png",
                "caption": "Figure 27:Token Lens logit values (left) and MLP logit differences (right) of subject and answer tokens of Llama-3-8B-Instruct on Actor-Movies dataset when attending to or knocking out the subject tokens.",
                "position": 1218
            },
            {
                "img": "https://arxiv.org/html/2502.20475/extracted/6254738/figures/critical_token_analysis/data_model_specific/omit_early_layer_figures/actor_movies/mistralai/Mistral-7B-Instruct-v0.2/subject_logit.png",
                "caption": "Figure 28:Token Lens logit values (left) and MLP logit differences (right) of subject and answer tokens of Mistral-7B-Instruct on Actor-Movies dataset when attending to or knocking out the subject tokens.",
                "position": 1221
            },
            {
                "img": "https://arxiv.org/html/2502.20475/extracted/6254738/figures/head_promotion_vs_suppression_rate.png",
                "caption": "Figure 29:Promotion rate versus suppression rate of all attention heads across three answer steps macro-averaged across all models and datasets. The promotion rate and suppression rate positively correlate with each other, suggesting that answer promotion and suppression may not be independent of each other.",
                "position": 1288
            }
        ]
    },
    {
        "header": "Appendix FAre Knowledge Recall and Suppression Independent?",
        "images": []
    }
]