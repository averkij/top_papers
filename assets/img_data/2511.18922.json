[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.18922/x1.png",
                "caption": "",
                "position": 57
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.18922/x2.png",
                "caption": "Figure 2:Architecture comparison for joint RGB and geometry modeling. (a) Channel-wise and (b) spatial-wise concatenation feed RGB and XYZ into a single diffusion model with a shared LoRA branch. (c) Our Decoupled LoRA Control (DLC) employs two modality-specific LoRA branches with zero-initialized control links, achieving decoupled yet controlled RGB–XYZ joint generation.©\\copyrightdenotes concatenation and⊕\\oplusdenotes pixel-wise addition.",
                "position": 117
            },
            {
                "img": "https://arxiv.org/html/2511.18922/x3.png",
                "caption": "Figure 3:Comparison of architectures for joint RGB–geometry generation. Our Decoupled LoRA Control produces cleaner RGB and sharper, more consistent XYZ and depth than channel-wise and spatial-wise concatenation, while channel-wise concatenation severely degrades both appearance and geometry.",
                "position": 128
            },
            {
                "img": "https://arxiv.org/html/2511.18922/x4.png",
                "caption": "Figure 4:Overview of the One4D framework. Unified Masked Conditioning (UMC) packs single-image, sparse-frame, and full-video inputs into a masked conditioning video. RGB and XYZ videos are encoded into latent spaces via video VAEs, and the conditioning latents are concatenated only with noisy RGB latents. These RGB and XYZ latents are then processed by a DiT backbone with Decoupled LoRA Control (DLC). DLC employs modality-specific LoRA branches to decouple computation, and zero-initialized cross-modal control links to learn pixel-wise consistency. The denoised RGB and XYZ latents are finally decoded into RGB frames and pointmaps.",
                "position": 131
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.18922/x5.png",
                "caption": "Figure 5:Single-image-to-4D generation comparison between 4DNeX[5]and our One4D. Compared to 4DNeX, One4D produces more dynamic and realistic videos, sharper and cleaner depth, and more complete, coherent 4D point clouds with cameras.",
                "position": 321
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.18922/x6.png",
                "caption": "Figure 6:Additional single-image-to-4D generation results from One4D. It can generate coherent 4D geometry for various types of scenes.",
                "position": 343
            },
            {
                "img": "https://arxiv.org/html/2511.18922/x7.png",
                "caption": "Figure 7:Qualitative full-video 4D reconstruction comparison with CUT3R[42], MonST3R[56], and Geo4D[14]. The proposed One4D recovers sharper object boundaries and more accurate depth, especially on thin structures and challenging geometry.",
                "position": 757
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]