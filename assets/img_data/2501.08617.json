[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08617/x1.png",
                "caption": "Figure 1:RLHFcan incentivize AI systems to provide inaccurate or deceptive information to their users, prioritizing positive on-the-spot feedback and neglecting long-term consequences. For example, a customer may prefer to hear good news while shopping but will ultimately be disappointed (and objectively worse off) if stuck with an ill-informed purchase.\nThe proposedRLHSintroduces hindsight for human feedback, focusing on evaluations after simulating the outcome.\nThis enables more informed feedback that improves alignment between the AI and the human’s true utility.",
                "position": 112
            }
        ]
    },
    {
        "header": "2Background and Preliminaries",
        "images": []
    },
    {
        "header": "3Alignment Algorithm: RL from Hindsight Simulation",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08617/x2.png",
                "caption": "Figure 2:Illustration of hindsight’s advantage: Delaying human feedback until the human has experienced the outcome corresponding to the bulk of reward significantly mitigates the misalignment in the AI’s learned reward model.",
                "position": 325
            },
            {
                "img": "https://arxiv.org/html/2501.08617/x3.png",
                "caption": "Figure 3:Qualitative results for Llama-2-7b trained with either immediate feedback (RLHF) or partial hindsight (RLHS). The RLHF model (trained with immediate feedback) deceives the user by falsely claiming Options A and C meet the customer’s 8K resolution requirement, though neither does. In contrast, the RLHS model truthfully states that none of the options include 8K resolution.",
                "position": 403
            }
        ]
    },
    {
        "header": "4Experimental Design",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08617/x4.png",
                "caption": "Figure 4:Results on Llama-2-7b trained with PPO.Left:Demonstrates the Misalignment of real utility and satisfaction ratings using immediate feedback.Middle:Shows how partial hindsight mitigate the misalignment.Right:Shows the alignment achieved with oracle hindsight.",
                "position": 471
            },
            {
                "img": "https://arxiv.org/html/2501.08617/x4.png",
                "caption": "",
                "position": 474
            },
            {
                "img": "https://arxiv.org/html/2501.08617/x5.png",
                "caption": "",
                "position": 478
            },
            {
                "img": "https://arxiv.org/html/2501.08617/x6.png",
                "caption": "",
                "position": 483
            },
            {
                "img": "https://arxiv.org/html/2501.08617/x7.png",
                "caption": "Figure 5:Results on Llama-2-7b trained with DPO.Left:Demonstrates the Misalignment of real utility and satisfaction ratings using immediate feedback.Middle:Shows how partial hindsight mitigate the misalignment.Right:Shows the alignment achieved with oracle hindsight.",
                "position": 489
            },
            {
                "img": "https://arxiv.org/html/2501.08617/x7.png",
                "caption": "",
                "position": 492
            },
            {
                "img": "https://arxiv.org/html/2501.08617/x8.png",
                "caption": "",
                "position": 496
            },
            {
                "img": "https://arxiv.org/html/2501.08617/x9.png",
                "caption": "",
                "position": 501
            }
        ]
    },
    {
        "header": "5Simulation Results",
        "images": []
    },
    {
        "header": "6Human Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08617/x10.png",
                "caption": "Figure 6:The policy trained using the proposedRLHSoutperforms that ofRLHFin both true utility (left) and hindsight rating (right). In both plots, each point represents the mean ratio for a scenario, with lines indicating the standard deviation. The identity line is plotted in dashed grey.",
                "position": 577
            },
            {
                "img": "https://arxiv.org/html/2501.08617/x10.png",
                "caption": "",
                "position": 580
            },
            {
                "img": "https://arxiv.org/html/2501.08617/x11.png",
                "caption": "",
                "position": 584
            }
        ]
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Quantitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08617/x12.png",
                "caption": "Figure 7:Results on Llama-3-8b trained with PPO.Left:Misalignment of real utility and satisfaction ratings using immediate feedback.Right:Partial hindsight mitigate the misalignment.",
                "position": 1492
            },
            {
                "img": "https://arxiv.org/html/2501.08617/x12.png",
                "caption": "",
                "position": 1495
            },
            {
                "img": "https://arxiv.org/html/2501.08617/x13.png",
                "caption": "",
                "position": 1499
            },
            {
                "img": "https://arxiv.org/html/2501.08617/x14.png",
                "caption": "Figure 8:Results on Llama-3-8b trained with DPO.Left:Misalignment of real utility and satisfaction ratings using immediate feedback.Right:Partial hindsight mitigate the misalignment.",
                "position": 1505
            },
            {
                "img": "https://arxiv.org/html/2501.08617/x14.png",
                "caption": "",
                "position": 1508
            },
            {
                "img": "https://arxiv.org/html/2501.08617/x15.png",
                "caption": "",
                "position": 1512
            },
            {
                "img": "https://arxiv.org/html/2501.08617/x16.png",
                "caption": "(a)PPO training result",
                "position": 1518
            },
            {
                "img": "https://arxiv.org/html/2501.08617/x16.png",
                "caption": "(a)PPO training result",
                "position": 1521
            },
            {
                "img": "https://arxiv.org/html/2501.08617/x17.png",
                "caption": "(b)DPO training result",
                "position": 1526
            },
            {
                "img": "https://arxiv.org/html/2501.08617/x18.png",
                "caption": "(a)PPO training result",
                "position": 1533
            },
            {
                "img": "https://arxiv.org/html/2501.08617/x18.png",
                "caption": "(a)PPO training result",
                "position": 1536
            },
            {
                "img": "https://arxiv.org/html/2501.08617/x19.png",
                "caption": "(b)DPO training result",
                "position": 1541
            },
            {
                "img": "https://arxiv.org/html/2501.08617/x20.png",
                "caption": "(a)Immediate feedback",
                "position": 1548
            },
            {
                "img": "https://arxiv.org/html/2501.08617/x20.png",
                "caption": "(a)Immediate feedback",
                "position": 1551
            },
            {
                "img": "https://arxiv.org/html/2501.08617/x21.png",
                "caption": "(b)Partial hindsight",
                "position": 1556
            }
        ]
    },
    {
        "header": "Appendix BTraining algorithms.",
        "images": []
    },
    {
        "header": "Appendix CPrompts",
        "images": []
    },
    {
        "header": "Appendix DHuman Study Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08617/extracted/6132868/figure/human_study/Screenshot1.png",
                "caption": "Figure 12:Example of user interaction interface for our main human experiments studying the misalignment of RLHF and the effecitveness of RLHS.",
                "position": 2024
            },
            {
                "img": "https://arxiv.org/html/2501.08617/extracted/6132868/figure/human_study/Screenshot1.png",
                "caption": "",
                "position": 2027
            },
            {
                "img": "https://arxiv.org/html/2501.08617/extracted/6132868/figure/human_study/Screenshot2.png",
                "caption": "",
                "position": 2032
            },
            {
                "img": "https://arxiv.org/html/2501.08617/extracted/6132868/figure/human_study/interface_act.png",
                "caption": "Figure 13:Example of user interaction interface for additional human experiments assessing the alignment of LLM actions and feedback with those of humans.",
                "position": 2039
            },
            {
                "img": "https://arxiv.org/html/2501.08617/extracted/6132868/figure/human_study/interface_act.png",
                "caption": "",
                "position": 2042
            },
            {
                "img": "https://arxiv.org/html/2501.08617/extracted/6132868/figure/human_study/interface_feed.png",
                "caption": "",
                "position": 2047
            }
        ]
    },
    {
        "header": "Appendix EDiscussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08617/x22.png",
                "caption": "Figure 14:Qualitative results for Llama-2-7b trained with DPO using immediate feedback versus partial hindsight. The model trained with immediate feedback falsely claims that Option B is most affordable with 8K resolution, which is incorrect. In contrast, the model trained with partial hindsight truthfully states that option C is the most affordable option that includes 8K resolution.",
                "position": 2399
            },
            {
                "img": "https://arxiv.org/html/2501.08617/x23.png",
                "caption": "Figure 15:Qualitative results for Llama-3-8b trained with DPO using immediate feedback versus partial hindsight. The model trained with immediate feedback falsely claims that Option C can play 3D movies, which is incorrect. In contrast, the model trained with partial hindsight accurately states that Option C’s 3D capability is not specified, and recommends Option B, the cheapest option that includes 3D capability.",
                "position": 2404
            },
            {
                "img": "https://arxiv.org/html/2501.08617/x24.png",
                "caption": "Figure 16:Failure case for Llama-2-7b trained with DPO using partial hindsight. The model trained with immediate feedback deceives about specific features, while the model trained with partial hindsight withholds some information. This reveals shortcomings of partial hindsight, as it does not have observations for all other items. Consequently, it might still encourage the agent to deceive about the price or conceal price information.",
                "position": 2408
            }
        ]
    },
    {
        "header": "Appendix FAdditional Qualitative Results",
        "images": []
    }
]