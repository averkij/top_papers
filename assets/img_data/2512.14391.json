[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14391/figs/repo_overall.png",
                "caption": "Figure 1:Overall performance on four evaluation dimensions.",
                "position": 110
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Methods",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14391/figs/repo_long_context.png",
                "caption": "Figure 2:Long-context Evaluation on RULER. YaRN(peng2024yarn)is used for all RoPE layers to extend the context. We observe consistent results on the more realistic benchmark LongBench in Table3.",
                "position": 381
            }
        ]
    },
    {
        "header": "5Analyses",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14391/figs/stats_pos_dist.png",
                "caption": "Figure 3:Statistics for the distances between maximum and minimum positions in each attention head of the LLM. The averaged and maximum number of tokens in the MMLUPro-Math benchmark are 1971 and 2512, while those for RULER-QA are 2995 and 3555, respectively.",
                "position": 655
            },
            {
                "img": "https://arxiv.org/html/2512.14391/figs/stats_pos_pi.png",
                "caption": "Figure 4:Statistics for the patterns of assigned positions. We split the context into non-overlapping chunks of 16 tokens. \"Constant\" means assigned positions are all close to a constant position, \"Mono\" means the positions are monotonically increasing or decreasing in the chunk, and \"Hybrid\" means all other patterns.",
                "position": 761
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Impact Statement",
        "images": []
    },
    {
        "header": "9Acknowledgment",
        "images": []
    },
    {
        "header": "Appendix AWhatâ€™s the Real Difference between Conventional PEs, NoPE, and RePo?",
        "images": []
    },
    {
        "header": "Appendix BDetails of Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14391/figs/ablation.png",
                "caption": "Figure 5:Sensitivity to the starting layer ofRePo(i.e.l=3,5,7l=3,5,7). We validate on the NIAH subtask of RULER benchmark and MMLUPro of general benchmarks.",
                "position": 932
            }
        ]
    },
    {
        "header": "Appendix CPreliminary Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14391/figs/layer_visual.png",
                "caption": "Figure 6:Visualization of predicted positions from a 4-layer GPT-2 model in the reversal task. The area with blue background color indicates input context, while the orange region is the generated sequence. We use A-K to replace the real tokens to save space for illustration. The x and y-axis represent the input order and predicted positionzzof a token, respectively.",
                "position": 947
            },
            {
                "img": "https://arxiv.org/html/2512.14391/figs/synthetic_task.png",
                "caption": "Figure 7:Performance on the text reversal task. We report the accuracy on all lengths of input sequences.",
                "position": 966
            },
            {
                "img": "https://arxiv.org/html/2512.14391/figs/l5h1.png",
                "caption": "(a)Layer 5 & Attention Head 1",
                "position": 987
            },
            {
                "img": "https://arxiv.org/html/2512.14391/figs/l5h1.png",
                "caption": "(a)Layer 5 & Attention Head 1",
                "position": 990
            },
            {
                "img": "https://arxiv.org/html/2512.14391/figs/l8h0.png",
                "caption": "(b)Layer 8 & Attention Head 0",
                "position": 996
            },
            {
                "img": "https://arxiv.org/html/2512.14391/figs/l13h3.png",
                "caption": "(c)Layer 13 & Attention Head 3",
                "position": 1002
            }
        ]
    },
    {
        "header": "Appendix DCase Study",
        "images": []
    }
]