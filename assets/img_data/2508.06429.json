[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.06429/framework.png",
                "caption": "Figure 1:Three-phase framework for semi-supervised learning with limited labeled data. Our approach integrates three specialized networks: a generator (𝒢\\mathscr{G}) for class-conditioned image synthesis, a discriminator (𝒟\\mathscr{D}) for authenticity assessment and classification signalling, and a dedicated classifier (𝒞\\mathscr{C}) for the primary classification task. The generator comprises an encoder (ℰ𝒢\\mathscr{E_{G}}) and decoder (𝒟𝒢\\mathscr{D_{G}}) for image-to-image translation.(a) Initial Training Phase: Joint training of these three networks using limited labeled data. (b) Self-Supervised Pre-training: Two-part approach combining (1) ensemble-based pseudo-labeling using confidence-weighted voting and temporal ensembling, and (2) class-conditioned image translation with Wasserstein GAN training and cycle consistency. (c) Synthetic Data Enhancement Phase: Training classifier on generated samples using target class conditions as supervision signals, where (1) input images paired with target class conditions are processed by the generator to create synthetic training samples, and (2) generated samples are used to train the classifier with target class conditions as supervision signals. Fire symbols indicate trainable networks while ice symbols represent frozen weights during respective training phases.",
                "position": 206
            }
        ]
    },
    {
        "header": "4Experimental Configuration",
        "images": []
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.06429/mu_shot_accuracy_plot.png",
                "caption": "Figure 2:Average classification accuracy as a function of the number of labeled samples per class (shots) for different unsupervised training frequencies (μ\\mu). Results are averaged across all eleven MedMNIST datasets and computed on the validation set. The parameterμ\\mucontrols how frequently the unsupervised training phase is executed, withμ=0\\mu=0representing supervised learning only (no unsupervised phase),μ=1\\mu=1indicating unsupervised training at every epoch, and higher values (μ∈10,25,50,100\\mu\\in{10,25,50,100}) representing unsupervised training everyμ\\muepochs.",
                "position": 1740
            }
        ]
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    }
]