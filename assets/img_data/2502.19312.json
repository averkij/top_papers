[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.19312/x1.png",
                "caption": "Figure 1:Overview of FSPO.NùëÅNitalic_Npreviously collected preferences are fed into the LLM along with the current query, allowing the LLM to personalize its response to the query using the past preferences.",
                "position": 136
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries and Notation",
        "images": []
    },
    {
        "header": "4The Few-Shot Preference Optimization (FSPO) Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.19312/x2.png",
                "caption": "Figure 2:User Description Chain-of-Thought (COT).Prediction is a two-stage process: first predicting a (synthetic) user description from the few-shot preferences and next predicting the response.",
                "position": 237
            }
        ]
    },
    {
        "header": "5Domains to Study Personalization",
        "images": []
    },
    {
        "header": "6Sim2Real: Synthetic Preference Data Transfers to Real Users",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.19312/x3.png",
                "caption": "Figure 3:Overview of Domain Randomization Techniques.View-Conditioning (left) decomposes a given question into multiple viewpoints, allowing for diverse response generation. Iterative Persona Generation (right) allows for better structure by removing underspecification of the persona by iteratively refining a persona if it is insufficient to make a preference prediction.",
                "position": 386
            },
            {
                "img": "https://arxiv.org/html/2502.19312/x4.jpg",
                "caption": "Figure 4:Flowchart of Roleplay dataset generation:Starting from a set of traits, a seed persona is constructed and a set of specific questions about that trait. Then responses are constructed with View-Conditioning. The seed personas are then iteratively refined to not be underspecified. Finally, the refined persona is used to score consistent preferences.",
                "position": 409
            },
            {
                "img": "https://arxiv.org/html/2502.19312/extracted/6234251/figures/disagreement.png",
                "caption": "Figure 5:Disagreement Matrix across 5 users in Roleplay.Here we plot the disagreement of preferences for 5 users. There is a mix of users with high and low disagreement.",
                "position": 412
            }
        ]
    },
    {
        "header": "7Experimental Evaluation",
        "images": []
    },
    {
        "header": "8Discussion and Conclusion",
        "images": []
    },
    {
        "header": "9Limitations and Potential Risks",
        "images": []
    },
    {
        "header": "10Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.19312/x5.png",
                "caption": "Figure 6:Sample Personalized Response for ELIX (top) and Reviews (bottom).",
                "position": 1491
            },
            {
                "img": "https://arxiv.org/html/2502.19312/x6.png",
                "caption": "",
                "position": 1495
            },
            {
                "img": "https://arxiv.org/html/2502.19312/extracted/6234251/figures/human_study.png",
                "caption": "Figure 7:An overview of the Human Study Interface.First, users label a set of preferences. Then, a set of personalized answers are provided, conditioned on label preferences.",
                "position": 1504
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]