[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.19400/x1.png",
                "caption": "",
                "position": 105
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.19400/x2.png",
                "caption": "Figure 2:An overview of the multimodal theorem explanation framework.",
                "position": 115
            },
            {
                "img": "https://arxiv.org/html/2502.19400/x3.png",
                "caption": "Figure 3:TheoremExplainAgent consists of two LLM agents. Taking a theorem as input, the planner agent create plans for execution. The coding agent then generates Python scripts to produce visuals and audio.",
                "position": 131
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.19400/x4.png",
                "caption": "Figure 4:Subfields of TheoremExplainBench under Computer Science, Chemistry, Mathematics, and Physics.",
                "position": 156
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.19400/x5.png",
                "caption": "Figure 5:Visualizations expose reasoning errors more clearly than text, making it easier to diagnose model mistakes.",
                "position": 517
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "7Potential Risks",
        "images": []
    },
    {
        "header": "8Artifacts",
        "images": []
    },
    {
        "header": "9Computational Experiments",
        "images": []
    },
    {
        "header": "10Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AGallery",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.19400/x6.png",
                "caption": "Figure 6:We show the high-quality videos generated by TheoremExplainAgent,across the four STEM domains.",
                "position": 1217
            },
            {
                "img": "https://arxiv.org/html/2502.19400/x7.png",
                "caption": "Figure 7:We show the poorly generated videos from TheoremExplainAgent, zooming in the artifacts.",
                "position": 1220
            },
            {
                "img": "https://arxiv.org/html/2502.19400/extracted/6233886/figures/casestudy_1.png",
                "caption": "Figure 8:Comparison on a scene of a high quality animation and a low quality animation.",
                "position": 1223
            }
        ]
    },
    {
        "header": "Appendix BPrompt Templates",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.19400/x8.png",
                "caption": "Figure 9:The user interface of our annoatation website.",
                "position": 1518
            }
        ]
    },
    {
        "header": "Appendix CSupplementary Information",
        "images": []
    }
]