[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05125/x1.png",
                "caption": "Figure 1:Paradigm shift proposed in this work. Traditionally, the quality of synthetic images in a dataset is assessed from an anthropocentric perspective, answering the question of whether such images appear photorealistic. In contrast, this work proposes evaluating the images from the model’s perspective, which entails analyzing their visual embeddings to determine whether a synthetic image lies within the target distribution, as perceived by the model itself.",
                "position": 106
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05125/x2.png",
                "caption": "Figure 2:VERSE methodology components. The validation datasetMERIT secret(A) is processed by models’ visual encoders (B) to obtainvisual embeddings. F1 scores are obtained after inference on the validation dataset with fine-tuned models on the differentMERIT Datasetversions. High-dimensional visual embeddings are reduced to a lower-dimensional space (D), known as theReduced Embedding Space. This space provides better modelinterpretability, while overlaying the samples’ visual features and the F1 scores enhances modelexplainability(E). Sections3.1.Ato3.1.Eexplain in further detail the components (A-E) involved in the methodology.",
                "position": 184
            },
            {
                "img": "https://arxiv.org/html/2601.05125/x3.png",
                "caption": "Figure 3:Validation dataset: MERIT Secret, a dataset with real and anonymized samples, with 10 categories subdivided by school. Table8shows the relevant features extracted in MERIT Secret.",
                "position": 205
            },
            {
                "img": "https://arxiv.org/html/2601.05125/x4.png",
                "caption": "Figure 4:Training samples used. We employ the Spanish-language subsets of the MERIT Dataset, across its different versions (A). These versions are detailed in Table2). Each version comprises data from seven different schools (B). New versions complement the vanilla MERIT Dataset, composed of digital document samples (C) and their renderized versions (D).",
                "position": 403
            },
            {
                "img": "https://arxiv.org/html/2601.05125/x5.png",
                "caption": "Figure 5:Qualitative illustration of the pipeline used to obtain a single visual embedding per image. The procedure is encoder-specific: Donut, PaliGemma, and LLaVA employ a lightweight MLP projection to align visual and textual embedding dimensions, whereas Idefics2 incorporates a more advanced projection mechanism within the vision encoder. For consistency across models, we extract visual embeddings from the output of the vision block in all cases.",
                "position": 481
            },
            {
                "img": "https://arxiv.org/html/2601.05125/x6.png",
                "caption": "Figure 6:Graphical representation (PC1 vs. PC2) of the Reduced Embedding Space (RES) obtained for Idefics2[19](A) and LLaVA[21](B). The spaces were computed by applying PCA to the 152 embeddings from the validation dataset. VERSE enables a visual assessment of the model’s visual-world representation quality: well-defined and separable clusters (A) indicate stronger representational capacity and higher task potential, whereas a disorganized or highly entangled RES (B) reflects weaker visual discrimination and poorer downstream performance.",
                "position": 487
            },
            {
                "img": "https://arxiv.org/html/2601.05125/x7.png",
                "caption": "Figure 7:Representation of visual features projected onto Donut’s[15]RES. Each subplot shows a specific document feature distribution across PC1 vs. PC2. Number ofcolumns(A),layouttype (B), number ofvertical information blocks(C),table complexity(D),row height / image heightratio (zoom) (E),table vertical(F) andhorizontal(G)position,vertical density(H),table gridpresence (I), androw width / image widthratio (zoom)(J).",
                "position": 739
            },
            {
                "img": "https://arxiv.org/html/2601.05125/x8.png",
                "caption": "Figure 8:Representation of visual features projected onto Idefics2’s[19]RES. Each subplot shows a specific document feature distribution across PC1 vs. PC2. Features exposed are the same as the ones in Figure7.",
                "position": 745
            },
            {
                "img": "https://arxiv.org/html/2601.05125/x9.png",
                "caption": "Figure 9:Inference results for Donut[15](A) and Idefics2[19](B). For each pre-trained model, the horizontal axis shows the different versions of the training dataset ordered by increasing visual richness (new versions include a green mark), while the vertical axis represents the F1 score obtained for each fine-tuned version. Both models exhibit clear sensitivity to the increasing level of visual richness.",
                "position": 758
            },
            {
                "img": "https://arxiv.org/html/2601.05125/x10.png",
                "caption": "Figure 10:Reduced Embedding Space (RES) for Donut[15](PC1 vs. PC2). Higher F1 values are shown in green, while lower ones appear in red. Each subplot includes, in the lower-left corner, a representative training sample used for fine-tuning and a subset of training examples shown in purple. The visual richness of the training data increases progressively from A to F, causing the corresponding embeddings to transition from concentrated regions to a more dispersed distribution across the RES.",
                "position": 764
            },
            {
                "img": "https://arxiv.org/html/2601.05125/x11.png",
                "caption": "Figure 11:Analysis of Donut’s[15]Reduced Embedding Space (RES). Two low-performing regions are identified:region A, characterized by a small number of information blocks and high table complexity, primarily affected by an external factor (the low zoom level); andregion B, driven by intrinsic document properties such as table structure and alphanumeric grading, but also affected by low zoom level.",
                "position": 779
            },
            {
                "img": "https://arxiv.org/html/2601.05125/x12.png",
                "caption": "Figure 12:Effect of training zoom levels on Donut’s[15]performance. Fixed zoom-level experiments reveal that excessively low zoom values prevent the model from extracting textual information, while excessively high zoom values make the training task insufficiently challenging compared to the validation data. An intermediate zoom level (0.625) yields optimal results in the challenging areascluster Aandcluster B(A). Training with a uniform distribution of zoom values between 0.3 and 1.0 introduces visual diversity that enhances generalization (B). Comparative results across different regions (validation),cluster A, andcluster B(confirm that mixed zoom levels during fine-tuning consistently improve performance over fixed-zoom configurations (C)).",
                "position": 794
            },
            {
                "img": "https://arxiv.org/html/2601.05125/x13.png",
                "caption": "Figure 13:Effect of training data composition on Donut’s[15]performance. Fine-tuning with thebooster set(a subset containing intrinsic features) improves performance in the low-performingregion B(A). However, combining the booster with the digital dataset induces partial forgetting in that region while maintaining comparable generalization on the validation set. When the booster is combined with the rendered dataset (B), bothregion Band the overall validation performance improve significantly. This complementary effect arises from jointly leveraging the intrinsic features of the booster and the zoom variability introduced by rendering.",
                "position": 813
            }
        ]
    },
    {
        "header": "4Results",
        "images": []
    },
    {
        "header": "5Discusion",
        "images": []
    },
    {
        "header": "6Conclusions and contributions",
        "images": []
    },
    {
        "header": "Data availability",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Appendix A",
        "images": []
    },
    {
        "header": "Appendix B",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05125/x14.png",
                "caption": "Figure 14:Reduced Embedding Space (RES) for Idefics2[19](PC1 vs. PC2). Higher F1 values are shown in green, while lower ones appear in red. Each subplot includes, in the lower-left corner, a representative training sample used for fine-tuning and a subset of training examples shown in purple. The visual richness of the training data increases progressively from A to F.",
                "position": 1574
            },
            {
                "img": "https://arxiv.org/html/2601.05125/x15.png",
                "caption": "Figure 15:Effect of training data composition on Idefics2’s[19]performance.Regions A,B, andCare detected in the RES as conflictive clusters. SinceRegions AandCalready achieve solid performance when the model is fine-tuned on thezoomversion (C), we defineregion Bas the target cluster for thebooster set. VERSE suggests including samples with low zoom level and two tables as driving features of thebooster set(B). Fine-tuning the modle with the suggested combination improves performance in the problematicregion Bwhile preserving consistent generalization across thevalidationset and the remaining conflictive regions (C).",
                "position": 1580
            },
            {
                "img": "https://arxiv.org/html/2601.05125/x16.png",
                "caption": "Figure 16:Fine-tuning with thebooster setimproves Idefics2’s[19]performance in the problematicregion Bwhile preserving consistent generalization across thevalidationset.",
                "position": 1586
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]