[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05044/x1.png",
                "caption": "",
                "position": 113
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05044/x2.png",
                "caption": "Figure 2:TrajScene-60K curation pipeline.We curate videos from WebVid-10M, filtered via VLMs for structured motion and countable entities. Dense 4D point tracks are extracted and refined via depth filtering and Gaussian Splatting, producing 60K high-quality 4D scenes.",
                "position": 135
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Dataset Curation – TrajScene-60K",
        "images": []
    },
    {
        "header": "4Proposed Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05044/x3.png",
                "caption": "Figure 3:Pipeline of MoRe4D.Top: The 4D Scene Trajectory Generator (Sec.4.2), a Diffusion Transformer, jointly generates geometry and motion. Bottom-Left: The Motion Perception Module (MPM) identifies potential motion regions and semantic structure from the input image. Bottom-Right: The 4D View Synthesis Module (Sec.4.3) renders the output into novel-view videos.",
                "position": 173
            },
            {
                "img": "https://arxiv.org/html/2512.05044/x4.png",
                "caption": "Figure 4:Qualitative results of our model.The first row shows the 4D point cloud generated by our 4D-STraG. The second and third rows show the videos rendered by our 4D-ViSM under two distinct, user-defined camera trajectories.",
                "position": 375
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05044/x5.png",
                "caption": "Figure 5:Qualitative comparison with baseline methods.For each sample, the first row shows the baseline results while the second row presents our MoRe4D results. The first column displays the input image and text prompt.",
                "position": 401
            },
            {
                "img": "https://arxiv.org/html/2512.05044/x6.png",
                "caption": "Figure 6:Ablation studies on normalization methods and module components.(Rows 1-2) Depth-guided motion normalization stabilizes 4D point cloud generation. (Rows 3-6) Removing the MPM module reduces motion magnitude while excluding depth guidance breaks structural motion consistency, validating our design choices.",
                "position": 404
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "AMore Experiment Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05044/x7.png",
                "caption": "Figure A:Two examples comparing our joint 4D-STraG framework (top row) against sequential pipelines.For each sample, rows 2-4 show results from a cascaded approach: Wan2.1-I2V video generation, followed by DELTA tracking or VGGT reconstruction. Our method yields superior spatio-temporal coherence, while sequential approaches exhibit fragmentation from error accumulation. All samples are consistently rendered from the fixed camera viewpoint for fair comparison.",
                "position": 689
            },
            {
                "img": "https://arxiv.org/html/2512.05044/x8.png",
                "caption": "Figure B:Qualitative results of our model.We visualize the generated results for two more samples.\nThe first row shows the 4D point cloud generated by our 4D-STraG. The second, third and fourth rows show the videos rendered by our 4D-ViSM under three distinct, user-defined camera trajectories.",
                "position": 781
            }
        ]
    },
    {
        "header": "BMore Ablation Analysis",
        "images": []
    },
    {
        "header": "CMore Visualization Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05044/x9.png",
                "caption": "Figure C:Qualitative results of Motion-Sensitive VAE.The figure presents two examples. For each, the top row shows pseudo-GT from the original camera view, while the bottom is the corresponding VAE rendering at the same pose. The close visual alignment demonstrates our VAE’s high-fidelity reconstruction capability.",
                "position": 861
            },
            {
                "img": "https://arxiv.org/html/2512.05044/x10.png",
                "caption": "Figure D:Our input promptused to query the model for identifying and counting self-initiated and articulated motion.",
                "position": 865
            }
        ]
    },
    {
        "header": "DDataset Curation",
        "images": []
    },
    {
        "header": "EMore Experimental Settings",
        "images": []
    },
    {
        "header": "FImplementation Details",
        "images": []
    },
    {
        "header": "GApplications and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]