[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20240/x1.png",
                "caption": "",
                "position": 131
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Background",
        "images": []
    },
    {
        "header": "4Unconditional Priors Matter",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20240/extracted/6307886/figures/unconditional-samples/unconditional_horizontal_stack.jpg",
                "caption": "Figure 2:Unconditional samples from different diffusion models.Stable Diffusion[4], which often serves as the base model for fine-tuning conditional diffusion models, generates plausible images, whereas other fine-tuned diffusion models fail to sample realistic images.",
                "position": 379
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20240/extracted/6307886/figures/zero123/zero123_main.jpg",
                "caption": "Figure 3:Novel View Synthesis with Zero-1-to-3[46]. Outputs from Zero-1-to-3 often show inaccuracies in lighting or shape distortions during novel view synthesis. By incorporating unconditional noise predictions from Stable Diffusion[52]or PixArt-Œ±ùõº\\alphaitalic_Œ±[10], our method achieves clear improvements in output quality.",
                "position": 741
            },
            {
                "img": "https://arxiv.org/html/2503.20240/extracted/6307886/figures/versatile-diffusion/selected-synthetic/CFG/540.jpg",
                "caption": "Figure 4:Image Variations with Versatile Diffusion[64].Versatile Diffusion often suffers from style and detail degradation‚Äîexcessive saturation (rows 1 and 3) or loss of key content (row 2). In contrast, our method, leveragingSD1.4,SD2.1, orPixArt-Œ±ùõº\\alphaitalic_Œ±as unconditional priors, achieves noticeable improvements in performance.",
                "position": 821
            },
            {
                "img": "https://arxiv.org/html/2503.20240/extracted/6307886/figures/versatile-diffusion/selected-synthetic/CFG/381.jpg",
                "caption": "",
                "position": 877
            },
            {
                "img": "https://arxiv.org/html/2503.20240/extracted/6307886/figures/versatile-diffusion/selected-synthetic/CFG/464.jpg",
                "caption": "",
                "position": 880
            },
            {
                "img": "https://arxiv.org/html/2503.20240/extracted/6307886/figures/dynamicrafter/biker.jpg",
                "caption": "Figure 5:Image-to-Video Generation with DynamiCrafter[62].Our method is more temporally consistent (lighting on the biker) and less distorted (the hand and face in the second video).",
                "position": 1215
            },
            {
                "img": "https://arxiv.org/html/2503.20240/extracted/6307886/figures/dynamicrafter/sombrero.jpg",
                "caption": "",
                "position": 1229
            },
            {
                "img": "https://arxiv.org/html/2503.20240/extracted/6307886/figures/ip2p/piebench/10.jpg",
                "caption": "Figure 6:Image Editing with InstructPix2Pix (IP2P)[7].Applying our method improves alignment with the editing prompt while preserving the identity of the source image.",
                "position": 1419
            },
            {
                "img": "https://arxiv.org/html/2503.20240/extracted/6307886/figures/ip2p/piebench/08.jpg",
                "caption": "",
                "position": 1485
            },
            {
                "img": "https://arxiv.org/html/2503.20240/extracted/6307886/figures/ip2p/piebench/01.jpg",
                "caption": "",
                "position": 1492
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AQuantitative Evaluation of Unconditional Samples",
        "images": []
    },
    {
        "header": "Appendix BExperiment Details",
        "images": []
    },
    {
        "header": "Appendix CComparison with Autoguidance[38]",
        "images": []
    },
    {
        "header": "Appendix DAdditional Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20240/extracted/6307886/figures/zero123/zero123_supp.jpg",
                "caption": "Figure 7:Novel View Synthesis with Zero-1-to-3[46]. Zero-1-to-3 tends to produce views that have inaccurate lighting, coloring, or shape. Combining Zero-1-to-3 with the unconditional noise fromSD1.4,SD2.1, orPixArt-Œ±ùõº\\alphaitalic_Œ±corrects these inaccuracies.",
                "position": 2622
            },
            {
                "img": "https://arxiv.org/html/2503.20240/extracted/6307886/figures/versatile-diffusion/selected-mscoco/cat.jpg",
                "caption": "Figure 8:Image Variations with Versatile Diffusion[64].Images generated from Versatile Diffusion tend to be oversaturated and distorted. Combining Versatile Diffusion with the unconditional noise predictions fromSD1.4,SD2.1, orPixArt-Œ±ùõº\\alphaitalic_Œ±corrects these artifacts.",
                "position": 2692
            },
            {
                "img": "https://arxiv.org/html/2503.20240/extracted/6307886/figures/versatile-diffusion/selected-mscoco/church.jpg",
                "caption": "",
                "position": 2748
            },
            {
                "img": "https://arxiv.org/html/2503.20240/extracted/6307886/figures/versatile-diffusion/selected-synthetic/CFG/428.jpg",
                "caption": "",
                "position": 2751
            },
            {
                "img": "https://arxiv.org/html/2503.20240/extracted/6307886/figures/versatile-diffusion/selected-synthetic/CFG/445.jpg",
                "caption": "",
                "position": 2754
            },
            {
                "img": "https://arxiv.org/html/2503.20240/extracted/6307886/figures/versatile-diffusion/selected-synthetic/CFG/405.jpg",
                "caption": "",
                "position": 2757
            },
            {
                "img": "https://arxiv.org/html/2503.20240/extracted/6307886/figures/versatile-diffusion/selected-synthetic/CFG/559.jpg",
                "caption": "",
                "position": 2760
            },
            {
                "img": "https://arxiv.org/html/2503.20240/extracted/6307886/figures/DiT/DiT_qualitatives.jpg",
                "caption": "Figure 9:Class-conditional generation with DiT[50]. Class-conditional generation using DiT fine-tuned on SUN397[61], Food101[6], and Caltech101[24].",
                "position": 2765
            },
            {
                "img": "https://arxiv.org/html/2503.20240/extracted/6307886/figures/dynamicrafter/horse.jpg",
                "caption": "Figure 10:Image-to-Video Generation with DynamiCrafter[62].Our method is more temporally consistent (number of horses in the first video, shading of the guitar in the second video) and less distorted (hand and face in the last video).",
                "position": 2768
            },
            {
                "img": "https://arxiv.org/html/2503.20240/extracted/6307886/figures/dynamicrafter/guitar.jpg",
                "caption": "",
                "position": 2782
            },
            {
                "img": "https://arxiv.org/html/2503.20240/extracted/6307886/figures/dynamicrafter/pizza.jpg",
                "caption": "",
                "position": 2785
            },
            {
                "img": "https://arxiv.org/html/2503.20240/extracted/6307886/figures/ip2p/piebench/03.jpg",
                "caption": "Figure 11:Image Editing with InstructPix2Pix (IP2P)[7].InstructPix2Pix tends to produce distorted edits. Replacing the IP2P fully unconditional noise with the unconditional noise fromSD1.5,SD2.1, orPixArt-Œ±ùõº\\alphaitalic_Œ±corrects these distortions and improves image quality.",
                "position": 2790
            },
            {
                "img": "https://arxiv.org/html/2503.20240/extracted/6307886/figures/ip2p/piebench/07.jpg",
                "caption": "",
                "position": 2856
            },
            {
                "img": "https://arxiv.org/html/2503.20240/extracted/6307886/figures/ip2p/piebench/04.jpg",
                "caption": "",
                "position": 2863
            },
            {
                "img": "https://arxiv.org/html/2503.20240/extracted/6307886/figures/ip2p/piebench/06.jpg",
                "caption": "",
                "position": 2870
            },
            {
                "img": "https://arxiv.org/html/2503.20240/extracted/6307886/figures/ip2p/piebench/12.jpg",
                "caption": "",
                "position": 2877
            }
        ]
    },
    {
        "header": "Appendix EChoice of CFG Scale",
        "images": []
    },
    {
        "header": "Appendix FMemory and Inference Speed",
        "images": []
    }
]