[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.24119/extracted/6584094/assets/logos/github.png",
                "caption": "",
                "position": 203
            },
            {
                "img": "https://arxiv.org/html/2506.24119/extracted/6584094/assets/logos/huggingface.png",
                "caption": "",
                "position": 203
            },
            {
                "img": "https://arxiv.org/html/2506.24119/x1.png",
                "caption": "Figure 1:SPIRAL achieves consistent improvements over base models across game performance and reasoning benchmarks. It also surpasses SFT on expert game trajectories and RL baselines trained against fixed opponents (Mistral and Gemini).",
                "position": 205
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.24119/x2.png",
                "caption": "Figure 2:From human-designed rewards to self-discovered reasoning through SPIRAL.Left: Traditional RL requires human experts to design complex reward functions.Middle: Fixed opponent training leads to exploitation of static strategies.Right: SPIRAL enables continuous reasoning improvement through self-play, where both players develop increasingly sophisticated strategies without human supervision.",
                "position": 229
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4The SPIRAL Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.24119/x3.png",
                "caption": "Figure 3:The SPIRAL Framework.SPIRAL employs an actor-learner architecture for scalable self-play training. Parallel actors sample trajectories from a diverse set of games using vectorized environments. A single policyœÄisubscriptùúãùëñ\\pi_{i}italic_œÄ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTplays both roles, generating zero-sum, sparse reward game trajectories. The centralized learner processes these trajectories using Role-conditioned Advantage Estimation (RAE) to compute separate advantages,A0‚Å¢(s,a)subscriptùê¥0ùë†ùëéA_{0}(s,a)italic_A start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( italic_s , italic_a )andA1‚Å¢(s,a)subscriptùê¥1ùë†ùëéA_{1}(s,a)italic_A start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_s , italic_a ), for each role. These are then used for on-policy reinforcement learning updates.",
                "position": 315
            }
        ]
    },
    {
        "header": "5Experiment Setup",
        "images": []
    },
    {
        "header": "6Experimental Results and Findings",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.24119/x4.png",
                "caption": "Figure 4:Evolution of reasoning patterns during SPIRAL training and their transfer to mathematical reasoning.We track three core reasoning patterns (Pattern Recognition, Expected Value Calculation, and Case-by-Case Analysis) across 290 game trajectories and 46,792 math solutions.Left: In the game domain, all patterns show substantial growth, with Expected Value Calculation reaching 78% by late training.Middle: These patterns transfer to mathematical reasoning with varying effectiveness: Case-by-Case Analysis maintains high transfer (72% to 71%), Pattern Recognition shows amplification (35% to 45%), while Expected Value Calculation transfers more selectively (78% to 28%).Right: Math benchmark scores improve from 31.2 to 39.6 as these reasoning patterns develop, demonstrating that game-learned strategies enhance mathematical problem-solving capabilities.",
                "position": 828
            },
            {
                "img": "https://arxiv.org/html/2506.24119/x5.png",
                "caption": "Figure 5:Performance comparison of self-play training and fixed-opponent baselines. All evaluations are averaged over multiple games/benchmarks (seeSec.5.3). Mistral Opponent refers to against Mistral-Small-3; Gemini Opponent refers to against Gemini-2.0-Flash-Lite.",
                "position": 882
            },
            {
                "img": "https://arxiv.org/html/2506.24119/x6.png",
                "caption": "Figure 6:Training dynamics comparing REINFORCE with RAE (orange) versus vanilla REINFORCE (gray). RAE maintains stable performance across all metrics while vanilla REINFORCE suffers catastrophic thinking collapse.Top row: Performance on game playing, math reasoning, and general reasoning benchmarks.Bottom row: Policy gradient norm shows training instability without RAE; response length reveals thinking collapse where models stop generating reasoning traces.",
                "position": 1217
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ATraining Paradigms in Turn-Level MDPs",
        "images": []
    },
    {
        "header": "Appendix BExperimental Setup Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.24119/x7.png",
                "caption": "Figure 7:Example observations of three training game environments.",
                "position": 2138
            },
            {
                "img": "https://arxiv.org/html/2506.24119/x8.png",
                "caption": "Figure 8:Example observations of five evaluation game environments.",
                "position": 2147
            }
        ]
    },
    {
        "header": "Appendix CAdditional Results and Analysis",
        "images": []
    },
    {
        "header": "Appendix DCase Study Methodology",
        "images": []
    },
    {
        "header": "Appendix EGame Environment Specifications",
        "images": []
    }
]