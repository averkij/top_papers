[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04728/x1.png",
                "caption": "Figure 1:An overview of the proposed method. Our test-time compute scaling approach consists of two main steps:\n(1) Best-of-N Sampling for PDDL Initialization (see Section3.2): We start by running a parallel sampling process to generate multiple chain-of-thought responses that are composed of the formalized PDDL-based world model representationùêÉisubscriptùêÉùëñ\\mathbf{D}_{i}bold_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTand the natural language thoughtùêìisubscriptùêìùëñ\\mathbf{T}_{i}bold_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT.\n(2) Closed-loop Iteration with iVML (see Section3.3): We use Instance Verbalized Machine Learning (iVML) to iteratively improve the solutions.\nThe iVML incorporates: (1) An optimizer LLMfoptsubscriptùëìoptf_{\\mathrm{opt}}italic_f start_POSTSUBSCRIPT roman_opt end_POSTSUBSCRIPTthat evaluates the solutions from the previous iteration, and (2) A learner LLMflearnersubscriptùëìlearnerf_{\\mathrm{learner}}italic_f start_POSTSUBSCRIPT roman_learner end_POSTSUBSCRIPTthat learns from the feedback and updates the PDDL-based world modelùêÉisubscriptùêÉùëñ\\mathbf{D}_{i}bold_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT.\nThe most optimal PDDL-based world model would be sent to the systematic search engine for planning.",
                "position": 588
            },
            {
                "img": "https://arxiv.org/html/2502.04728/x2.png",
                "caption": "Figure 2:OpenAI-o1 plans for Termes: o1 frequently exhibits hallucination during the planning process. Specifically, in steps three and four, the LLM violates predefined rules when selecting and leveraging actions. Additionally, step four hallucinates the achievement of the goal, leading to incorrect or unrealistic outcomes. Even when using o1 itself to evaluate the hallucinated plan, it incorrectly identifies the plan as valid.",
                "position": 641
            },
            {
                "img": "https://arxiv.org/html/2502.04728/x3.png",
                "caption": "Figure 3:Left: The performance trend of iVML with increasing training epochs. Right: The performance trend of BoN with increasing sampling numbers.",
                "position": 1407
            },
            {
                "img": "https://arxiv.org/html/2502.04728/x4.png",
                "caption": "Figure 4:The performance of iVML on NL2Domain tasks across different initialization settings.",
                "position": 1590
            },
            {
                "img": "https://arxiv.org/html/2502.04728/x4.png",
                "caption": "Figure 4:The performance of iVML on NL2Domain tasks across different initialization settings.",
                "position": 1593
            },
            {
                "img": "https://arxiv.org/html/2502.04728/x5.png",
                "caption": "Figure 5:The performance of iVML on Prob2Domain tasks across different initialization settings.",
                "position": 1598
            },
            {
                "img": "https://arxiv.org/html/2502.04728/extracted/6185517/content/imgs/generated.png",
                "caption": "Figure 6:The planning graph for Termes",
                "position": 3395
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]