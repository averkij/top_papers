[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15228/x1.png",
                "caption": "Figure 1:By leveraginga) Collaborative Multi-Modal Codingencoded from photometric (RGB, RGBD) and geometric (RGBD, Point Clouds) information,b) TriMMcan create high-quality textured meshes within 4 seconds from a single image.",
                "position": 112
            },
            {
                "img": "https://arxiv.org/html/2508.15228/x2.png",
                "caption": "Figure 2:Overview of our TriMM.To extract the unique attributes of multi-modal triplanes and avoid their specific weakness, we introduce the loss_2,i.e., reconstruction loss during training. It can guide our generative model to leverage the strength of multi-modalities coding, thereby achieving promising performance in 3D modeling.",
                "position": 154
            }
        ]
    },
    {
        "header": "IIRelated Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15228/x3.png",
                "caption": "Figure 3:Detailed structure of our Collaborative Multi-Modal Coding.The proposed Collaborative Multi-Modal Coding can be tokenized from each of the modalities (i.e.RGB, RGBD, Point Clouds) using different encoders, shown as the three branches above. By adopting a share-weight triplane-flexicube decoder, the coding (i.e.corresponding triplanes) from different modalities collaboratively share a joint latent space.",
                "position": 194
            },
            {
                "img": "https://arxiv.org/html/2508.15228/x4.png",
                "caption": "Figure 4:Training pipeline of our triplane latent diffusion model.It can harness and integrate the distinctive attributes of various modalities via reconstruction loss, thereby producing 3D assets enriched with rich texture and finely detailed structures.",
                "position": 278
            },
            {
                "img": "https://arxiv.org/html/2508.15228/x5.png",
                "caption": "Figure 5:Details of the decoder in our collaborative multi-modal coding.Leveraging the lightweight decoder, our model efficiently and effectively transforms the triplane into a colored mesh.",
                "position": 360
            },
            {
                "img": "https://arxiv.org/html/2508.15228/x6.png",
                "caption": "Figure 6:Qualitative results.We compared our TriMM with other methods on image-to-3D. Thanks to our multi-modal coding the collaboratively marries photometric and geometric information in the unified triplane space, our TriMM achieves impressive generative performance, especially for fine-grained geometric details like wings and hairs.",
                "position": 363
            },
            {
                "img": "https://arxiv.org/html/2508.15228/x7.png",
                "caption": "Figure 7:Qualitative comparison among branches based on different modalities.It shows the strengths of different modality data for 3D modeling. The point cloud branch excels at capturing geometry, whereas the RGB and RGBD branches perform better at modeling texture.",
                "position": 381
            },
            {
                "img": "https://arxiv.org/html/2508.15228/x8.png",
                "caption": "Figure 8:More qualitative results of our proposed method.It shows that our method can generate high-quality 3D assets with photo-realistic textures and detailed geometry",
                "position": 515
            },
            {
                "img": "https://arxiv.org/html/2508.15228/x9.png",
                "caption": "Figure 9:Ablation studies of reconstruction loss.By introducing the reconstruction loss, our model can avoid the weakness of specific modality, thereby enhancing the generative performance further.",
                "position": 825
            },
            {
                "img": "https://arxiv.org/html/2508.15228/x10.png",
                "caption": "Figure 10:Comparison between model using 2D supervision and model using 2D&3D supervision.By leveraging 3D supervision, our method can directly learn geometric structures and effectively avoid artifacts.",
                "position": 851
            },
            {
                "img": "https://arxiv.org/html/2508.15228/x11.png",
                "caption": "",
                "position": 856
            },
            {
                "img": "https://arxiv.org/html/2508.15228/x12.png",
                "caption": "",
                "position": 859
            },
            {
                "img": "https://arxiv.org/html/2508.15228/x13.png",
                "caption": "Figure 13:User study results on the overall quality of 3D generation from different methods.The normalized score proves the promising performance of our method.",
                "position": 1004
            }
        ]
    },
    {
        "header": "IIIConclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15228/ziangcao.jpg",
                "caption": "",
                "position": 1452
            },
            {
                "img": "https://arxiv.org/html/2508.15228/zhaoxi.jpg",
                "caption": "",
                "position": 1465
            },
            {
                "img": "https://arxiv.org/html/2508.15228/PanLiang.png",
                "caption": "",
                "position": 1490
            },
            {
                "img": "https://arxiv.org/html/2508.15228/zw.jpg",
                "caption": "",
                "position": 1503
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]