[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.04598/x1.png",
                "caption": "Figure 1:Training dynamics for 1.2B dense models with Pre-Norm, HybridNorm and HybridNorm∗under 1T training tokens. We present the training loss, validation loss, and downstream performance on HellaSwag and ARC-Easy (0.8 EMA smoothed), demonstrating that HybridNorm∗achieves superior performance.",
                "position": 104
            },
            {
                "img": "https://arxiv.org/html/2503.04598/x2.png",
                "caption": "Figure 2:Illustrations of different transformer layer structures: (a) Post-Norm architecture; (b) Pre-Norm architecture; (c) Pre-Norm with QK-Norm architecture; (d) HybridNorm architecture.",
                "position": 109
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.04598/x3.png",
                "caption": "Figure 3:Training dynamics for MoE-1B-7B models with Pre-Norm and HybridNorm∗under 500B training tokens. We present the training loss, validation loss, and downstream performance on HellaSwag and MMLU Var, demonstrating that HybridNorm∗achieves superior performance.",
                "position": 435
            },
            {
                "img": "https://arxiv.org/html/2503.04598/x4.png",
                "caption": "Figure 4:Training loss and accuracy on HellaSwag of 550M dense models with different normalization methods for the first block.",
                "position": 684
            },
            {
                "img": "https://arxiv.org/html/2503.04598/x5.png",
                "caption": "Figure 5:Gradient norm comparison of Pre-Norm, Post-Norm, and HybridNorm at different training steps.",
                "position": 707
            },
            {
                "img": "https://arxiv.org/html/2503.04598/x6.png",
                "caption": "Figure 6:Scaling law curves of Pre-Norm and HybridNorm∗.",
                "position": 788
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATheoretical Gradient Analysis",
        "images": []
    },
    {
        "header": "Appendix BDetails of Experiments",
        "images": []
    },
    {
        "header": "Appendix CAdditional Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.04598/x7.png",
                "caption": "Figure 7:Overall loss and downstream evaluations for the 1.2B dense models with 1T training tokens.",
                "position": 2543
            },
            {
                "img": "https://arxiv.org/html/2503.04598/x8.png",
                "caption": "Figure 8:Overall loss and downstream evaluations for the MoE models with 500B training\ntokens.",
                "position": 2554
            }
        ]
    },
    {
        "header": "Appendix DPyTorch Style Implementation of HybridNorm",
        "images": []
    },
    {
        "header": "Appendix EFormulas for Different Positions of Normalization Layers",
        "images": []
    }
]