[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15671/extracted/6020576/Figures/GSM.png",
                "caption": "Figure 1:Overview of Graph Sequence Model (GSM). GSM Consists of three stages: (1) Tokenization, (2) Local Encoding, and (3) Global Encoding. We provide a foundation for strengths and weaknesses of different tokenizations and sequence models. Finally, we present three methods to enhance the power of GSMs.",
                "position": 162
            }
        ]
    },
    {
        "header": "2Encoding Graphs to Sequences: A Unified Model",
        "images": []
    },
    {
        "header": "3Choosing a Sequence Model",
        "images": []
    },
    {
        "header": "4Enhancing Graph to Sequence Models",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15671/extracted/6020576/Figures/GSM++.png",
                "caption": "Figure 2:Overview of GSM++.GSM++ is a special instance of GSMs that uses: (1) HAC tokenization, (2) hierarchical PE, and (3) a hybrid sequence model.",
                "position": 2913
            }
        ]
    },
    {
        "header": "Appendix ABackgrounds",
        "images": []
    },
    {
        "header": "Appendix BRelated Work",
        "images": []
    },
    {
        "header": "Appendix CSpecial Instances of GSMs",
        "images": []
    },
    {
        "header": "Appendix DProofs of Theoretical Results",
        "images": []
    },
    {
        "header": "Appendix EComparisons between Transformers and Recurrent Models",
        "images": []
    },
    {
        "header": "Appendix FAdvantages and Disadvantages of Local Encoding",
        "images": []
    },
    {
        "header": "Appendix GOverview of GSM++",
        "images": []
    },
    {
        "header": "Appendix HExperimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15671/extracted/6020576/Figures/All.png",
                "caption": "Figure 3:Normalized score of different combination of tokenization and global encoder (sequence models). Even TTT + HAC is in Top-3 only in 3/7 datasets.",
                "position": 4260
            }
        ]
    },
    {
        "header": "Appendix IAdditional Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15671/extracted/6020576/Figures/PE-plot.png",
                "caption": "Figure 4:The effect of number of nodes on the preprocessing time for the construction of positional encodings.",
                "position": 4711
            }
        ]
    },
    {
        "header": "Appendix JTime Complexity of GSM++",
        "images": []
    }
]