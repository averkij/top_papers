[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19460/x1.png",
                "caption": "Figure 1:Memory usage comparison across sequence lengths for Mamba-2-2.7B with different checkpointing methods, demonstrating the memory-saving capability of Multi-Axis Gradient Checkpointing (MA-GC).",
                "position": 148
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Video-Ma2mba",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19460/x2.png",
                "caption": "Figure 2:Overview of MA-GC grid structure. Checkpoints are stored everylùëôlitalic_llayers andsùë†sitalic_ssteps. Theblue,red, andgreenarrows indicate forward propagation, activation restoration, and gradient propagation, respectively. This grid design optimizes memory by selectively restoring activations as needed. The below table shows comparison of checkpointing usage, maximum sequence length on 80GB VRAM, and peak activation memory in BFloat16 at sequence length 16384.",
                "position": 290
            },
            {
                "img": "https://arxiv.org/html/2411.19460/x3.png",
                "caption": "Figure 3:The overall summarization for the training stages of Video-Ma2mba.",
                "position": 599
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19460/x4.png",
                "caption": "Table 3:Memory overhead (GB) for GC methods in Mamba-2-2.7B across sequence lengths (S=2nùëÜsuperscript2ùëõS=2^{n}italic_S = 2 start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT). ‚ÄúGC off‚Äù indicates no checkpointing; ‚ÄúGC on‚Äù applies checkpointing per layer; ‚ÄúSqrt GC‚Äù groups layers byLùêø\\sqrt{L}square-root start_ARG italic_L end_ARG; and ‚ÄúMA-GC‚Äù optimizes based on sequence length. Each cell show peak memory during activation and backpropagation (BF16 precision), excluding model weights and gradients.",
                "position": 1153
            }
        ]
    },
    {
        "header": "5Discussion and Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AInteraction Schema",
        "images": []
    },
    {
        "header": "Appendix BTraining Details",
        "images": []
    },
    {
        "header": "Appendix CMemory Estimation Logic",
        "images": []
    },
    {
        "header": "Appendix DGradient Checkpointing Time Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19460/x4.png",
                "caption": "Figure 4:Qualitative examples on Video-MME[13]with Video-Ma2mba-3.1B.",
                "position": 2730
            },
            {
                "img": "https://arxiv.org/html/2411.19460/x5.png",
                "caption": "Figure 5:Qualitative examples from the Generative Subset of VideoChatGPT[29]with Video-Ma2mba-3.1B.",
                "position": 2733
            },
            {
                "img": "https://arxiv.org/html/2411.19460/x6.png",
                "caption": "Figure 6:Qualitative examples on LongVideoBench[43]with Video-Ma2mba-3.1B.",
                "position": 2736
            }
        ]
    },
    {
        "header": "Appendix EQualitative Evaluation",
        "images": []
    }
]