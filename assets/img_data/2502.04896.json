[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04896/x1.png",
                "caption": "(a)Text-to-Image Samples",
                "position": 161
            },
            {
                "img": "https://arxiv.org/html/2502.04896/x1.png",
                "caption": "(a)Text-to-Image Samples",
                "position": 164
            },
            {
                "img": "https://arxiv.org/html/2502.04896/x2.png",
                "caption": "(b)Text-to-Video Samples",
                "position": 170
            }
        ]
    },
    {
        "header": "2Goku: Generative Flow Models for Visual Creation",
        "images": []
    },
    {
        "header": "3Infrastructure Optimization",
        "images": []
    },
    {
        "header": "4Data Curation Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04896/x3.png",
                "caption": "Figure 2:The data curation pipeline inGoku.Given a large volume of video/image data collected from Internet, we generate high-quality video/image-text pairs through a series of data filtering, captioning and balancing steps.",
                "position": 484
            },
            {
                "img": "https://arxiv.org/html/2502.04896/extracted/6186370/figures/label_distribution_bar.png",
                "caption": "(a)Semantic distribution of video clips.",
                "position": 681
            },
            {
                "img": "https://arxiv.org/html/2502.04896/extracted/6186370/figures/label_distribution_bar.png",
                "caption": "(a)Semantic distribution of video clips.",
                "position": 684
            },
            {
                "img": "https://arxiv.org/html/2502.04896/x4.png",
                "caption": "(b)The balanced semantic distribution of subcategories.",
                "position": 690
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04896/x5.png",
                "caption": "Figure 4:Samples ofGoku-I2V.Reference images are presented in the leftmost columns. We omitted redundant information from the long prompts, displaying only the key details in each one. Key words are highlighted inRED.",
                "position": 1227
            },
            {
                "img": "https://arxiv.org/html/2502.04896/x6.png",
                "caption": "(a)Model Scaling",
                "position": 1250
            },
            {
                "img": "https://arxiv.org/html/2502.04896/x6.png",
                "caption": "(a)Model Scaling",
                "position": 1253
            },
            {
                "img": "https://arxiv.org/html/2502.04896/x7.png",
                "caption": "(b)Joint Training",
                "position": 1259
            },
            {
                "img": "https://arxiv.org/html/2502.04896/x8.png",
                "caption": "Figure 6:Qualitative comparisons with state-of-the-art (SoTA) video generation models.This figure showcases comparisons with leading models, including(Yang et al., 2024c,), Open-Sora Plan(Lab and etc.,,2024), Pika(pika,,2024), DreamMachine(Luma,,2024), Vidu(Bao et al.,,2024), and Kling v1.5(Kuaishou,,2024).",
                "position": 1272
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix Appendix ABenchmark Configurations",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04896/x9.png",
                "caption": "Figure 7:Qualitative samples ofGoku-T2I.Key words are highlighted inRED.",
                "position": 1317
            },
            {
                "img": "https://arxiv.org/html/2502.04896/x10.png",
                "caption": "Figure 8:Qualitative samples ofGoku-T2I.Key words are highlighted inRED. For clarity, we zoom in on specific regions to enhance visualization.",
                "position": 1325
            }
        ]
    },
    {
        "header": "Appendix Appendix BMore Visualization Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04896/x11.png",
                "caption": "Figure 9:Qualitative samples ofGoku-T2V.Key words are highlighted inRED.",
                "position": 1351
            },
            {
                "img": "https://arxiv.org/html/2502.04896/x12.png",
                "caption": "Figure 10:Qualitative comparisons ofGoku-T2V with SOTA video generation methods.Key words are highlighted inRED.",
                "position": 1363
            },
            {
                "img": "https://arxiv.org/html/2502.04896/x13.png",
                "caption": "Figure 11:Qualitative comparisons ofGoku-T2V with SOTA video generation methods.Key words are highlighted inRED.",
                "position": 1371
            },
            {
                "img": "https://arxiv.org/html/2502.04896/x14.png",
                "caption": "Figure 12:Qualitative samples ofGoku-I2V.Key words are highlighted inRED.",
                "position": 1870
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]