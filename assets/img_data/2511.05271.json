[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.05271/pictures/logo.jpg",
                "caption": "",
                "position": 64
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.05271/x1.png",
                "caption": "Figure 1:Illustration of agentic multimodal models.(a) Existing models show unsatisfactory performance in real-world scenarios, showing clear limitations especially when perception, reasoning, and search must be tightly integrated.\n(b) A multi-step visual reasoning example requiring coordinated perception, search, and reasoning.",
                "position": 83
            },
            {
                "img": "https://arxiv.org/html/2511.05271/x2.png",
                "caption": "Figure 2:Case reasoning trajectory ofDeepEyesV2.DeepEyesV2seamlessly integrates code execution and web search within its iterative reasoning process. Notably, in the right case, the behavior of accessing webpages via code does not exist in cold start data and is spontaneously acquired during reinforcement learning.",
                "position": 100
            },
            {
                "img": "https://arxiv.org/html/2511.05271/x3.png",
                "caption": "Figure 3:Pipeline ofDeepEyesV2.DeepEyesV2invokes tools and incorporates execution results into subsequent reasoning steps, enabling iterative and tool-augmented multimodal inference.",
                "position": 113
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.05271/x4.png",
                "caption": "Figure 4:Pioneer Experimentsreveal that existing multimodal models cannot directly acquire reliable tool use ability through RL, demonstrating the necessity of a cold start phase. Thered dashed linerepresents tool calls number in a single rollout, and theblue solid linerepresents the averge response length.",
                "position": 135
            }
        ]
    },
    {
        "header": "3DeepEyesV2",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.05271/x5.png",
                "caption": "Figure 5:Statistics of RealX-Bench.(a) Domain distribution across five representative categories: Daily Life, Media, Sports, Knowledge, and Games.\n(b) Distribution of subsets classified by required abilities: perception, reasoning, search, and integration. These numbers may overlap because the challenges are not mutually exclusive. Integration denotes questions that are difficult across all three abilities simultaneously.",
                "position": 166
            }
        ]
    },
    {
        "header": "4RealX-Bench",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.05271/x6.png",
                "caption": "Figure 6:Tool distribution comparison.DeepEyesV2demonstrates the task-specific tool-calling distribution across different tasks. Reinforcement learning leads to a distribution shift.",
                "position": 1234
            },
            {
                "img": "https://arxiv.org/html/2511.05271/x6.png",
                "caption": "Figure 6:Tool distribution comparison.DeepEyesV2demonstrates the task-specific tool-calling distribution across different tasks. Reinforcement learning leads to a distribution shift.",
                "position": 1237
            },
            {
                "img": "https://arxiv.org/html/2511.05271/x7.png",
                "caption": "Figure 7:Tool invocation statics.After reinforcement learning,DeepEyesV2’s tool-calling frequency decreases, which enhancesDeepEyesV2’s tool-calling flexibility and allow it to decide dynamically whether to invoke tools.",
                "position": 1243
            },
            {
                "img": "https://arxiv.org/html/2511.05271/pictures/final_dynamic.png",
                "caption": "Figure 8:Training dynamics of RL.On the right, the green parts indicate the mean and standard deviation of the number of tool calls. During training, although the average response length steadily declines, the variance in tool-call counts remains high, indicating that the model can still perform complex tool-usage reasoning. Overall, reinforcement learning improves the efficiency ofDeepEyesV2’s reasoning and tool usage.",
                "position": 1250
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.05271/x8.png",
                "caption": "Figure 9:Distribution of cold start and reinforcement learning data.",
                "position": 1332
            },
            {
                "img": "https://arxiv.org/html/2511.05271/x9.png",
                "caption": "Figure 10:Error analysis.",
                "position": 1516
            },
            {
                "img": "https://arxiv.org/html/2511.05271/x10.png",
                "caption": "Figure 11:Case Study 1.",
                "position": 1671
            },
            {
                "img": "https://arxiv.org/html/2511.05271/x11.png",
                "caption": "Figure 12:Case Study 2.",
                "position": 1674
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]