[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19189/x1.png",
                "caption": "",
                "position": 92
            },
            {
                "img": "https://arxiv.org/html/2411.19189/extracted/6032098/image/spectral_colormap.png",
                "caption": "Figure 1:The RollingDepth modeltakes an unconstrained video and reconstructs a corresponding depth video. Unlike methods that rely on video diffusion models, it extends a single-image monodepth estimator such that it can process short snippets. To account for temporal context, snippets with varying frame rate are sampled from the video, processed, and reassembled through a global alignment algorithm to obtain long, temporally coherent depth videos. Depth is colour-coded from nearfar.",
                "position": 94
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19189/x2.png",
                "caption": "Figure 2:Overview of the RollingDepth Inference Pipeline.Given a video sequenceùê±ùê±\\mathbf{x}bold_x(withisithsuperscriptùëñthi^{\\text{th}}italic_i start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPTframe), we constructNTsubscriptùëÅùëáN_{T}italic_N start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPToverlapping snippets using adilated rolling kernelwith varying dilation rates, and perform 1-step inference to obtain initial depth snippets (). Next, depth co-alignment optimizesNTsubscriptùëÅùëáN_{T}italic_N start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPTpairs of scale and shift values to achieve globally consistent depth throughout the full video. An optional refinement step further enhances details by applying additional, snippet-based denoising steps.",
                "position": 203
            },
            {
                "img": "https://arxiv.org/html/2411.19189/x5.png",
                "caption": "",
                "position": 204
            },
            {
                "img": "https://arxiv.org/html/2411.19189/x6.png",
                "caption": "",
                "position": 204
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19189/x7.png",
                "caption": "Figure 3:Depth Refinementencodes the co-aligned depth video into latent space, contaminates it with a moderate amount of noise, then denoises it with a series of reverse diffusion steps with decreasing snippet dilation rate. After each step, overlapping latents are averaged to propagate information between snippets.",
                "position": 289
            },
            {
                "img": "https://arxiv.org/html/2411.19189/x8.png",
                "caption": "Figure 4:Qualitative comparisonbetween different methods. RollingDepth excels at preserving fine-grained details (cf.the chandelier in the first sample and the tripod in the third sample) and recovering accurate scene layout (cf.the far plane in the second sample).",
                "position": 528
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19189/x9.png",
                "caption": "Figure 5:AbsRel error over time:The line plot (left) shows the depth error at every individual frame, end-of-line numbers are the average error across the video. The images (right) display error maps (lowhigh) for two specific frames. RollingDepth¬†achieves the lowest error overall, competing methods recover scene layout less faithfully and tend to be biased towards the foreground or the background.",
                "position": 573
            },
            {
                "img": "https://arxiv.org/html/2411.19189/extracted/6032098/image/OrRd_colormap.png",
                "caption": "",
                "position": 574
            },
            {
                "img": "https://arxiv.org/html/2411.19189/x10.png",
                "caption": "Figure 6:Qualitative comparisonof depth predictions (nearfar) from in-the-wild videos. To graphically show temporal consistency, we display temporal profiles (red box) for a fixed column (marked with a red line). RollingDepth picks up subtle details like accessories and wrinkled cloth, and mitigates spurious depth discontinuities (cf.background in temporal profile of the first sample) in time.",
                "position": 576
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19189/extracted/6032098/image/PO_simple.jpg",
                "caption": "Figure S1:Examples of PointOdyssey toy scenes (left) and scenes with smoke (right).",
                "position": 1913
            },
            {
                "img": "https://arxiv.org/html/2411.19189/extracted/6032098/image/PO_simple.jpg",
                "caption": "",
                "position": 1916
            },
            {
                "img": "https://arxiv.org/html/2411.19189/extracted/6032098/image/PO_smoke.jpg",
                "caption": "",
                "position": 1921
            },
            {
                "img": "https://arxiv.org/html/2411.19189/x11.png",
                "caption": "Figure S2:Examples of PointOdyssey samples that challenge video models. In the cases above, the (inverse) depth range varies significantly across frames. The arrows highlight situations where video models yield distorted depth maps.\nIn the first two rows, this occurs in regions where the depth deviates significantly from the surrounding scene.\nIn the last row, the depth predictions get drawn towards the near plane to match the object close to the camera, biasing the depth in the far field.",
                "position": 1958
            },
            {
                "img": "https://arxiv.org/html/2411.19189/x11.png",
                "caption": "Figure S2:Examples of PointOdyssey samples that challenge video models. In the cases above, the (inverse) depth range varies significantly across frames. The arrows highlight situations where video models yield distorted depth maps.\nIn the first two rows, this occurs in regions where the depth deviates significantly from the surrounding scene.\nIn the last row, the depth predictions get drawn towards the near plane to match the object close to the camera, biasing the depth in the far field.",
                "position": 1961
            },
            {
                "img": "https://arxiv.org/html/2411.19189/x12.png",
                "caption": "Figure S3:The two samples on the left show incorrect depth predictions in the cloudy sky.\nThe two samples on the right show inconsistencies between different frames of the same video, where the depth at the glass windows fluctuates between the solid and transparent states.",
                "position": 1970
            }
        ]
    },
    {
        "header": "Appendix BAdditional Experiment Results",
        "images": []
    }
]