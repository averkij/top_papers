[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.22304/x1.png",
                "caption": "Figure 1:Illustration of the incremental production flow. The Answer LLM is designated to generate an answer chunk with a limited number of tokens. The Stop LLM determines if the current partial answer has reached a satisfying final answer.",
                "position": 126
            },
            {
                "img": "https://arxiv.org/html/2410.22304/x2.png",
                "caption": "Figure 2:Illustration of the DPO training with rollouts. At each node of the initial generation, we do a random rollout that is different from the original node and continue generation to a final answer. A pair that leads to different answers (correct and incorrect) is considered a DPO training data.",
                "position": 144
            }
        ]
    },
    {
        "header": "3Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.22304/x3.png",
                "caption": "Figure 3:Progressive validation accuracy of Llama-3-Instruct on MetaMath.",
                "position": 188
            },
            {
                "img": "https://arxiv.org/html/2410.22304/x3.png",
                "caption": "Figure 3:Progressive validation accuracy of Llama-3-Instruct on MetaMath.",
                "position": 190
            },
            {
                "img": "https://arxiv.org/html/2410.22304/x4.png",
                "caption": "Figure 4:Progressive validation accuracy of Phi-3-Medium on MetaMath.",
                "position": 194
            }
        ]
    },
    {
        "header": "4Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperiment Details",
        "images": []
    }
]