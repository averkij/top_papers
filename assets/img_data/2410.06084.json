[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.06084/x1.png",
                "caption": "Figure 1:Left.Illustration of the two objectives: CFG distillation (above) and the diversity reward (below), multiplied by the diversity coefficientβ𝛽\\betaitalic_βin the joint finetuning objective.Right.Quality-diversity trade-off for different strategies.\nThe first four lines represent the training trajectories of our approach, distilling CFG (withγ=3𝛾3\\gamma=3italic_γ = 3) with varying diversity coefficientβ𝛽\\betaitalic_βin{0,5,10,15}051015\\{0,5,10,15\\}{ 0 , 5 , 10 , 15 }; every500500500500training steps, we evaluate the quality and diversity of the generations. Larger values ofβ𝛽\\betaitalic_βlead to more diverse models yet slightly less quality.\nFor linear interpolation (LERP), each cross corresponds to a0≤λ≤10𝜆10\\leq\\lambda\\leq 10 ≤ italic_λ ≤ 1when interpolating between the weightsθqsubscript𝜃𝑞\\theta_{q}italic_θ start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPTof a quality-focused model (β=0𝛽0\\beta=0italic_β = 0) and thoseθdsubscript𝜃𝑑\\theta_{d}italic_θ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPTof a diversity-focused model (β=15𝛽15\\beta=15italic_β = 15); the evaluated generations are obtained from the weights(1−λ)⋅θq+λ⋅θd⋅1𝜆subscript𝜃𝑞⋅𝜆subscript𝜃𝑑(1-\\lambda)\\cdot\\theta_{q}+\\lambda\\cdot\\theta_{d}( 1 - italic_λ ) ⋅ italic_θ start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT + italic_λ ⋅ italic_θ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT.\nFor the CFG baseline, each dot corresponds to a different value for the guidance factor1≤γ≤71𝛾71\\leq\\gamma\\leq 71 ≤ italic_γ ≤ 7.\nThis plot shows that our method improves the quality-diversity trade-off; notably, LERP uncovers a strong and steerable front of solutions by just interpolating between the weights of two models, at deployment time.",
                "position": 178
            },
            {
                "img": "https://arxiv.org/html/2410.06084/x2.png",
                "caption": "",
                "position": 189
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Diversity-rewarded CFG distillation",
        "images": []
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.06084/x3.png",
                "caption": "Figure 2:Left.Evolution of the KL divergence between the CFG-distilled student and the CFG-augmented teacher along training. GKD distillation alone (β=0𝛽0\\beta=0italic_β = 0) decreases the KL between the two policies.Middle.Evolution of the quality along training, showing improved quality for all selected values ofβ𝛽\\betaitalic_β.Right.Evolution of the diversity across generations along training, showing that CFG distillation alone reduces diversity, but that using a diversity reward (β≠0𝛽0\\beta\\neq 0italic_β ≠ 0) can actually increase it.\nThe“CFG”line shows the quality/diversity performance of the CFG-augmented base model serving as a teacher.\nThe“upper-bound”line indicates the mean diversity of two generations (from the base model) for two different prompts.",
                "position": 478
            },
            {
                "img": "https://arxiv.org/html/2410.06084/x4.png",
                "caption": "",
                "position": 489
            },
            {
                "img": "https://arxiv.org/html/2410.06084/x5.png",
                "caption": "",
                "position": 494
            },
            {
                "img": "https://arxiv.org/html/2410.06084/x6.png",
                "caption": "Figure 3:Quality-diversity trade-off for multiple strategies. The first four lines linear interpolate (LERP) between the quality-focused model (β=0𝛽0\\beta=0italic_β = 0) and more diverse models (those trained withβ>0𝛽0\\beta>0italic_β > 0, or the base model), slidingλ𝜆\\lambdaitalic_λbetween00and1111with a step of0.050.050.050.05. We also report the performance from the uniform (λ=14𝜆14\\lambda=\\frac{1}{4}italic_λ = divide start_ARG 1 end_ARG start_ARG 4 end_ARG) averaging of the four models finetuned with differentβ𝛽\\betaitalic_β, denoted as“LERP(0,5,10,150510150,5,10,150 , 5 , 10 , 15) uniform”. We include inference-time baseline strategies — CFG (when varyingγ𝛾\\gammaitalic_γ) and temperature sampling (when varying the temperatureT𝑇Titalic_T) — applied either on the base model or on the CFG-distilled model.",
                "position": 523
            },
            {
                "img": "https://arxiv.org/html/2410.06084/x7.png",
                "caption": "Figure 4:Left.Linear interpolation between the weights of a model focused on quality (β=0𝛽0\\beta=0italic_β = 0) and a model focused on diversity (β=15𝛽15\\beta=15italic_β = 15), sliding the coefficientλ𝜆\\lambdaitalic_λbetween00and1111. The dashed diagonal represents the expected values if abilities were traded-off linearly between those two models. While the diversity stays close to the diagonal, the quality is above it, explaining the benefits of model merging.Right.For comparison, we also include the results for CFG when slidingγ𝛾\\gammaitalic_γbetween1111and7777, performing worse than merged models.",
                "position": 535
            },
            {
                "img": "https://arxiv.org/html/2410.06084/x8.png",
                "caption": "",
                "position": 546
            },
            {
                "img": "https://arxiv.org/html/2410.06084/x9.png",
                "caption": "",
                "position": 551
            },
            {
                "img": "https://arxiv.org/html/2410.06084/x10.png",
                "caption": "",
                "position": 556
            },
            {
                "img": "https://arxiv.org/html/2410.06084/x11.png",
                "caption": "Figure 5:Left.Side-by-side human evaluation for quality.Right.Side-by-side human evaluation for diversity. The score corresponds to the win rate of model A over model B, computed asW+T/2W+L+T𝑊𝑇2𝑊𝐿𝑇\\frac{W+T/2}{W+L+T}divide start_ARG italic_W + italic_T / 2 end_ARG start_ARG italic_W + italic_L + italic_T end_ARGwithW𝑊Witalic_Wthe number of wins of A over B,T𝑇Titalic_Tthe number of ties,L𝐿Litalic_Lthe number of losses of A against B.\nThis confirms that our approach improves the quality-diversity trade-off.\nFor instance, the merged model LERP(0, 15) generates music with higher diversity than the CFG-augmented base model (γ=3𝛾3\\gamma=3italic_γ = 3) in 57% of the comparisons, while being rated as more qualitative half of the time (51%).",
                "position": 582
            },
            {
                "img": "https://arxiv.org/html/2410.06084/x12.png",
                "caption": "",
                "position": 593
            }
        ]
    },
    {
        "header": "4Related work",
        "images": []
    },
    {
        "header": "5Discussions and limitations",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APolicy gradient for the diversity reward",
        "images": []
    },
    {
        "header": "Appendix BCFG for text-to-music generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.06084/x13.png",
                "caption": "Figure 6:Effect of CFG on quality and diversity as a function ofγ𝛾\\gammaitalic_γ. The quality is greatly improved at the expense of diversity. Negative prompting outperforms unconditional prompting.",
                "position": 2208
            },
            {
                "img": "https://arxiv.org/html/2410.06084/x14.png",
                "caption": "Figure 7:Comparison between different measures of diversity;\nthe one provided by our embedding model (left), the Elo rankings provided by human evaluation of diversity (middle) and finally the entropy per token (right). CFG at inference time (yellow) reduces all those measures of diversity.\nModels trained with our RL diversity reward (β=15𝛽15\\beta=15italic_β = 15in green) have increased diversity according to our embedding model and human evaluation, but not in terms of entropy.",
                "position": 2245
            },
            {
                "img": "https://arxiv.org/html/2410.06084/x15.png",
                "caption": "",
                "position": 2256
            },
            {
                "img": "https://arxiv.org/html/2410.06084/x16.png",
                "caption": "",
                "position": 2261
            }
        ]
    },
    {
        "header": "Appendix CDiversity in music",
        "images": []
    }
]