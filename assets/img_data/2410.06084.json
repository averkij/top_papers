[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.06084/x1.png",
                "caption": "Figure 1:Left.Illustration of the two objectives: CFG distillation (above) and the diversity reward (below), multiplied by the diversity coefficientÎ²ğ›½\\betaitalic_Î²in the joint finetuning objective.Right.Quality-diversity trade-off for different strategies.\nThe first four lines represent the training trajectories of our approach, distilling CFG (withÎ³=3ğ›¾3\\gamma=3italic_Î³ = 3) with varying diversity coefficientÎ²ğ›½\\betaitalic_Î²in{0,5,10,15}051015\\{0,5,10,15\\}{ 0 , 5 , 10 , 15 }; every500500500500training steps, we evaluate the quality and diversity of the generations. Larger values ofÎ²ğ›½\\betaitalic_Î²lead to more diverse models yet slightly less quality.\nFor linear interpolation (LERP), each cross corresponds to a0â‰¤Î»â‰¤10ğœ†10\\leq\\lambda\\leq 10 â‰¤ italic_Î» â‰¤ 1when interpolating between the weightsÎ¸qsubscriptğœƒğ‘\\theta_{q}italic_Î¸ start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPTof a quality-focused model (Î²=0ğ›½0\\beta=0italic_Î² = 0) and thoseÎ¸dsubscriptğœƒğ‘‘\\theta_{d}italic_Î¸ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPTof a diversity-focused model (Î²=15ğ›½15\\beta=15italic_Î² = 15); the evaluated generations are obtained from the weights(1âˆ’Î»)â‹…Î¸q+Î»â‹…Î¸dâ‹…1ğœ†subscriptğœƒğ‘â‹…ğœ†subscriptğœƒğ‘‘(1-\\lambda)\\cdot\\theta_{q}+\\lambda\\cdot\\theta_{d}( 1 - italic_Î» ) â‹… italic_Î¸ start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT + italic_Î» â‹… italic_Î¸ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT.\nFor the CFG baseline, each dot corresponds to a different value for the guidance factor1â‰¤Î³â‰¤71ğ›¾71\\leq\\gamma\\leq 71 â‰¤ italic_Î³ â‰¤ 7.\nThis plot shows that our method improves the quality-diversity trade-off; notably, LERP uncovers a strong and steerable front of solutions by just interpolating between the weights of two models, at deployment time.",
                "position": 178
            },
            {
                "img": "https://arxiv.org/html/2410.06084/x2.png",
                "caption": "",
                "position": 189
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Diversity-rewarded CFG distillation",
        "images": []
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.06084/x3.png",
                "caption": "Figure 2:Left.Evolution of the KL divergence between the CFG-distilled student and the CFG-augmented teacher along training. GKD distillation alone (Î²=0ğ›½0\\beta=0italic_Î² = 0) decreases the KL between the two policies.Middle.Evolution of the quality along training, showing improved quality for all selected values ofÎ²ğ›½\\betaitalic_Î².Right.Evolution of the diversity across generations along training, showing that CFG distillation alone reduces diversity, but that using a diversity reward (Î²â‰ 0ğ›½0\\beta\\neq 0italic_Î² â‰  0) can actually increase it.\nTheâ€œCFGâ€line shows the quality/diversity performance of the CFG-augmented base model serving as a teacher.\nTheâ€œupper-boundâ€line indicates the mean diversity of two generations (from the base model) for two different prompts.",
                "position": 478
            },
            {
                "img": "https://arxiv.org/html/2410.06084/x4.png",
                "caption": "",
                "position": 489
            },
            {
                "img": "https://arxiv.org/html/2410.06084/x5.png",
                "caption": "",
                "position": 494
            },
            {
                "img": "https://arxiv.org/html/2410.06084/x6.png",
                "caption": "Figure 3:Quality-diversity trade-off for multiple strategies. The first four lines linear interpolate (LERP) between the quality-focused model (Î²=0ğ›½0\\beta=0italic_Î² = 0) and more diverse models (those trained withÎ²>0ğ›½0\\beta>0italic_Î² > 0, or the base model), slidingÎ»ğœ†\\lambdaitalic_Î»between00and1111with a step of0.050.050.050.05. We also report the performance from the uniform (Î»=14ğœ†14\\lambda=\\frac{1}{4}italic_Î» = divide start_ARG 1 end_ARG start_ARG 4 end_ARG) averaging of the four models finetuned with differentÎ²ğ›½\\betaitalic_Î², denoted asâ€œLERP(0,5,10,150510150,5,10,150 , 5 , 10 , 15) uniformâ€. We include inference-time baseline strategies â€” CFG (when varyingÎ³ğ›¾\\gammaitalic_Î³) and temperature sampling (when varying the temperatureTğ‘‡Titalic_T) â€” applied either on the base model or on the CFG-distilled model.",
                "position": 523
            },
            {
                "img": "https://arxiv.org/html/2410.06084/x7.png",
                "caption": "Figure 4:Left.Linear interpolation between the weights of a model focused on quality (Î²=0ğ›½0\\beta=0italic_Î² = 0) and a model focused on diversity (Î²=15ğ›½15\\beta=15italic_Î² = 15), sliding the coefficientÎ»ğœ†\\lambdaitalic_Î»between00and1111. The dashed diagonal represents the expected values if abilities were traded-off linearly between those two models. While the diversity stays close to the diagonal, the quality is above it, explaining the benefits of model merging.Right.For comparison, we also include the results for CFG when slidingÎ³ğ›¾\\gammaitalic_Î³between1111and7777, performing worse than merged models.",
                "position": 535
            },
            {
                "img": "https://arxiv.org/html/2410.06084/x8.png",
                "caption": "",
                "position": 546
            },
            {
                "img": "https://arxiv.org/html/2410.06084/x9.png",
                "caption": "",
                "position": 551
            },
            {
                "img": "https://arxiv.org/html/2410.06084/x10.png",
                "caption": "",
                "position": 556
            },
            {
                "img": "https://arxiv.org/html/2410.06084/x11.png",
                "caption": "Figure 5:Left.Side-by-side human evaluation for quality.Right.Side-by-side human evaluation for diversity. The score corresponds to the win rate of model A over model B, computed asW+T/2W+L+Tğ‘Šğ‘‡2ğ‘Šğ¿ğ‘‡\\frac{W+T/2}{W+L+T}divide start_ARG italic_W + italic_T / 2 end_ARG start_ARG italic_W + italic_L + italic_T end_ARGwithWğ‘ŠWitalic_Wthe number of wins of A over B,Tğ‘‡Titalic_Tthe number of ties,Lğ¿Litalic_Lthe number of losses of A against B.\nThis confirms that our approach improves the quality-diversity trade-off.\nFor instance, the merged model LERP(0, 15) generates music with higher diversity than the CFG-augmented base model (Î³=3ğ›¾3\\gamma=3italic_Î³ = 3) in 57% of the comparisons, while being rated as more qualitative half of the time (51%).",
                "position": 582
            },
            {
                "img": "https://arxiv.org/html/2410.06084/x12.png",
                "caption": "",
                "position": 593
            }
        ]
    },
    {
        "header": "4Related work",
        "images": []
    },
    {
        "header": "5Discussions and limitations",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APolicy gradient for the diversity reward",
        "images": []
    },
    {
        "header": "Appendix BCFG for text-to-music generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.06084/x13.png",
                "caption": "Figure 6:Effect of CFG on quality and diversity as a function ofÎ³ğ›¾\\gammaitalic_Î³. The quality is greatly improved at the expense of diversity. Negative prompting outperforms unconditional prompting.",
                "position": 2208
            },
            {
                "img": "https://arxiv.org/html/2410.06084/x14.png",
                "caption": "Figure 7:Comparison between different measures of diversity;\nthe one provided by our embedding model (left), the Elo rankings provided by human evaluation of diversity (middle) and finally the entropy per token (right). CFG at inference time (yellow) reduces all those measures of diversity.\nModels trained with our RL diversity reward (Î²=15ğ›½15\\beta=15italic_Î² = 15in green) have increased diversity according to our embedding model and human evaluation, but not in terms of entropy.",
                "position": 2245
            },
            {
                "img": "https://arxiv.org/html/2410.06084/x15.png",
                "caption": "",
                "position": 2256
            },
            {
                "img": "https://arxiv.org/html/2410.06084/x16.png",
                "caption": "",
                "position": 2261
            }
        ]
    },
    {
        "header": "Appendix CDiversity in music",
        "images": []
    }
]