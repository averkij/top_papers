[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.10507/x1.png",
                "caption": "Figure 1:Example of AdvancedIF benchmark. The example is from the capability of multi-turn carried context where prompts and rubrics written by human experts.",
                "position": 189
            },
            {
                "img": "https://arxiv.org/html/2511.10507/x2.png",
                "caption": "Figure 2:Framework of RIFL.",
                "position": 228
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3AdvancedIF: Rubric-based Evaluation for Instruction Following",
        "images": []
    },
    {
        "header": "4RIFL: Rubric-Based Instruction-Following Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.10507/assets/rubric_verifier_training_RL_v2.png",
                "caption": "Figure 4:RL ofrubric verifiertraining inSectionËœ4.3. The reward is computed as the ratio of agreement between the verified results and expert labels across each criterion.",
                "position": 736
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Taxonomy of AdvancedIF",
        "images": []
    },
    {
        "header": "8Rubric Verifier Prompt",
        "images": []
    },
    {
        "header": "9Ablations on Rubric Verifier",
        "images": []
    },
    {
        "header": "10Ablations on Reward Hacking Prevention Rubric",
        "images": []
    }
]