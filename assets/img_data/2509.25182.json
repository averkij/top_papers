[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25182/x1.png",
                "caption": "Figure 1:DC-VideoGen can generate high-quality videos on a single NVIDIA H100 GPU with resolutions ranging from 480px, 720px, 1080px, and 2160px. On 2160×\\times3840 resolution, DC-VideoGen delivers 14.8×\\timesacceleration compared to the Wan-2.1-T2V-1.3B model.",
                "position": 141
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25182/x2.png",
                "caption": "Figure 2:DC-VideoGen Overview.DC-VideoGen is a post-training acceleration framework for video diffusion models. It achieves acceleration by transferring models to DC-AE-V’s latent space (Section3.2) and rapidly recovering the base model’s quality and semantics using AE-Adapt-V (Section3.3.2). Compared with training from scratch, DC-VideoGen is far more efficient — for example, DC-VideoGen-14B requires only 10 NVIDIA H100 GPU days, a 230×\\timesreduction in training cost relative to Wan-2.1-14B.",
                "position": 198
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25182/x3.png",
                "caption": "Figure 3:Video Autoencoder Reconstruction Visualization.Under deep compression settings, causal video autoencoders suffer from low reconstruction quality. In contrast, non-causal video autoencoders achieve better reconstruction quality but generalize poorly to longer videos.",
                "position": 231
            },
            {
                "img": "https://arxiv.org/html/2509.25182/x4.png",
                "caption": "Figure 4:Illustration of Chunk-Causal Temporal Modeling in DC-AE-V.Our chunk-causal temporal modeling preserves causal information flow across chunks while enabling bidirectional flow within each chunk. This design improves reconstruction quality over non-causal temporal modeling, while maintaining generalization to longer videos at inference time.",
                "position": 267
            },
            {
                "img": "https://arxiv.org/html/2509.25182/x5.png",
                "caption": "Figure 5:Ablation Study on Chunk Size.",
                "position": 502
            },
            {
                "img": "https://arxiv.org/html/2509.25182/x6.png",
                "caption": "Figure 6:Direct fine-tuning without AE-Adapt-V leads to training instability and suboptimal quality. In contrast, AE-Adapt-V provides a robust initialization that preserves semantics in the new latent space for the video diffusion model, enabling rapid recovery of visual quality and allowing the model to match the base model’s performance with lightweight fine-tuning.",
                "position": 589
            },
            {
                "img": "https://arxiv.org/html/2509.25182/x7.png",
                "caption": "Figure 7:Illustration of Video Embedding Space Alignment.We present detailed ablation studies in Figure11, showing that both alignment steps—patch embedder alignment and output head alignment—are essential for effective video embedding space alignment.",
                "position": 650
            },
            {
                "img": "https://arxiv.org/html/2509.25182/x8.png",
                "caption": "(a)Quantitative Comparison",
                "position": 665
            },
            {
                "img": "https://arxiv.org/html/2509.25182/x8.png",
                "caption": "(a)Visual Comparison",
                "position": 710
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25182/x9.png",
                "caption": "Figure 9:Model Architecture of DC-AE-V.",
                "position": 1976
            },
            {
                "img": "https://arxiv.org/html/2509.25182/x10.png",
                "caption": "Figure 10:Additional Video Autoencoder Reconstruction Visualization.Under deep compression settings, causal video autoencoders suffer from low reconstruction quality. In contrast, non-causal video autoencoders achieve better reconstruction quality but generalize poorly to longer videos.",
                "position": 2352
            },
            {
                "img": "https://arxiv.org/html/2509.25182/x11.png",
                "caption": "Figure 11:Ablation Study on Video Embedding Space Alignment.",
                "position": 2361
            },
            {
                "img": "https://arxiv.org/html/2509.25182/x12.png",
                "caption": "Figure 12:Visual Comparison of DC-VideoGen-Wan2.1-I2V-14B and the Base Model Wan2.1-I2V-14B.",
                "position": 2694
            },
            {
                "img": "https://arxiv.org/html/2509.25182/x13.png",
                "caption": "Figure 13:Visual Comparison of DC-VideoGen-Wan2.1-T2V-14B and the Base Model Wan2.1-T2V-14B.",
                "position": 2699
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]