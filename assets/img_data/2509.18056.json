[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.18056/x1.png",
                "caption": "Table 4:Ablation results comparing GRPO with enhanced variants incorporating mixed-policy rewards and alternative advantage shaping strategies.",
                "position": 746
            },
            {
                "img": "https://arxiv.org/html/2509.18056/x1.png",
                "caption": "Figure 3:Skewness of the advantage distributions during training for different variants.",
                "position": 792
            },
            {
                "img": "https://arxiv.org/html/2509.18056/x2.png",
                "caption": "Figure 5:Qualitative comparisons on temporal grounding using TempSamp-R1.\nThe CoT inference mode (left) demonstrates stronger contextual reasoning for complex queries, such as inferring lighting changes from visual context.\nIn contrast, the non-CoT mode (right) provides sharper temporal boundaries for straightforward actions.",
                "position": 1026
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ALimitations and broader impact",
        "images": []
    },
    {
        "header": "Appendix BTraining details",
        "images": []
    },
    {
        "header": "Appendix CVisualizations",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.18056/x3.png",
                "caption": "Figure 6:Qualitative comparisons on temporal grounding on Charades-STA and ActivityNet Captions.\nTempSamp-R1 accurately localizes the queried events.",
                "position": 2187
            },
            {
                "img": "https://arxiv.org/html/2509.18056/x4.png",
                "caption": "Figure 7:Qualitative analysis of failure cases.\nMost prediction errors arise from ambiguous visual cues or imperfect ground-truth annotations, such as repetitive events and ill-defined temporal boundaries.",
                "position": 2193
            }
        ]
    },
    {
        "header": "Appendix DDatasets of training and evaluation",
        "images": []
    },
    {
        "header": "Appendix EPrompts",
        "images": []
    }
]