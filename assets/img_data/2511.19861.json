[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19861/x1.png",
                "caption": "",
                "position": 218
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3GigaWorld-0 Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19861/x2.png",
                "caption": "Figure 2:The framework ofGigaWorld-0-Video-Dreamer.",
                "position": 311
            },
            {
                "img": "https://arxiv.org/html/2511.19861/x3.png",
                "caption": "Figure 3:Qualitative comparison of action inference on the test set.Predictedjoint trajectories fromGigaWorld-0-IDMclosely align withground-truthactions across all 12 arm joints and 2 gripper degrees of freedom, demonstrating high fidelity in recovering physically plausible manipulation policies from visual input alone.",
                "position": 318
            },
            {
                "img": "https://arxiv.org/html/2511.19861/x4.png",
                "caption": "Figure 4:The control branch ofGigaWorld-Video.",
                "position": 410
            },
            {
                "img": "https://arxiv.org/html/2511.19861/x5.png",
                "caption": "Figure 5:Training data pair ofGigaWorld-0-Video-ViewTransfer.",
                "position": 435
            },
            {
                "img": "https://arxiv.org/html/2511.19861/x6.png",
                "caption": "Figure 6:Training data pair ofGigaWorld-0-Video-MimicTransfer.",
                "position": 479
            },
            {
                "img": "https://arxiv.org/html/2511.19861/Figs/mv.png",
                "caption": "Figure 7:GigaWorld can generate multi-view consistent videos, thereby enabling 3D-aware training and improving spatial reasoning in downstream tasks.",
                "position": 492
            },
            {
                "img": "https://arxiv.org/html/2511.19861/x7.png",
                "caption": "Figure 8:Overall pipeline ofGigaWorld-0-3D-FG.",
                "position": 511
            },
            {
                "img": "https://arxiv.org/html/2511.19861/x8.png",
                "caption": "Figure 9:Visualization of novel view synthesis before and after view restoration.",
                "position": 528
            },
            {
                "img": "https://arxiv.org/html/2511.19861/x9.png",
                "caption": "Figure 10:The learning pipeline of the differentiable physics network inGigaWorld-0-3D-Phys.",
                "position": 545
            },
            {
                "img": "https://arxiv.org/html/2511.19861/x10.png",
                "caption": "Figure 11:The overall pipeline ofGigaWorld-0-3D-Act.",
                "position": 568
            }
        ]
    },
    {
        "header": "4GigaWorld-0 Training",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19861/x11.png",
                "caption": "Figure 12:Visualization results ofGigaWorld-0-Video-Dreamerconditioned on the same initial frame but different text prompts, demonstrating its ability to produce diverse, semantically consistent future trajectories.",
                "position": 760
            },
            {
                "img": "https://arxiv.org/html/2511.19861/x12.png",
                "caption": "Figure 13:Multi-view visualization results ofGigaWorld-0-Video-Dreamerconditioned on the same initial frame but different text prompts.",
                "position": 764
            },
            {
                "img": "https://arxiv.org/html/2511.19861/x13.png",
                "caption": "Figure 14:Visualization results ofGigaWorld-0-Video-AppearanceTransfer, which enables photorealistic editing of texture, material, and lighting in real-world or simulation-acquired videos while preserving scene geometry, object semantics, and temporal coherence.",
                "position": 768
            },
            {
                "img": "https://arxiv.org/html/2511.19861/x14.png",
                "caption": "Figure 15:Visualization results ofGigaWorld-0-Video-ViewTransfer, which synthesizes photorealistic videos from arbitrary camera viewpoints while simultaneously adapting robot arm trajectories to maintain physical plausibility and action consistency, enabling the generation of diverse embodied manipulation data.",
                "position": 975
            },
            {
                "img": "https://arxiv.org/html/2511.19861/x15.png",
                "caption": "Figure 16:Visualization results ofGigaWorld-0-Video-MimicTransfer, which translates first-person human demonstration videos into robot-executable manipulation trajectories, enabling scalable synthesis of cross-embodiment training data for VLA models.",
                "position": 979
            },
            {
                "img": "https://arxiv.org/html/2511.19861/x16.png",
                "caption": "Figure 17:Visualization results ofGigaWorld-0-3D, showcasing geometrically consistent rendering and physically realistic robot actions.",
                "position": 983
            },
            {
                "img": "https://arxiv.org/html/2511.19861/x17.png",
                "caption": "Figure 18:Deployment ofGigaBrain-0on the G1 humanoid robot for real-worldlaundry folding.",
                "position": 1008
            },
            {
                "img": "https://arxiv.org/html/2511.19861/x18.png",
                "caption": "Figure 19:Deployment ofGigaBrain-0on the PiPER arms for real-worldpaper towel preparation.",
                "position": 1011
            },
            {
                "img": "https://arxiv.org/html/2511.19861/x19.png",
                "caption": "Figure 20:Deployment ofGigaBrain-0on PiPER arms for real-worldtable bussing.",
                "position": 1014
            },
            {
                "img": "https://arxiv.org/html/2511.19861/x20.png",
                "caption": "Figure 21:Deployment ofGigaBrain-0on G1 humanoid robot for real-worldjuice preparation.",
                "position": 1017
            },
            {
                "img": "https://arxiv.org/html/2511.19861/x21.png",
                "caption": "Figure 22:Deployment ofGigaBrain-0on the G1 humanoid robot for real-worldpaper towel preparation.",
                "position": 1020
            },
            {
                "img": "https://arxiv.org/html/2511.19861/x22.png",
                "caption": "Figure 23:Deployment ofGigaBrain-0on the PiPER arms for real-worldlaundry baskets moving.",
                "position": 1023
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    }
]