[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3M3FinMeeting: A Financial Meeting Benchmark",
        "images": []
    },
    {
        "header": "4Experimentation",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.02510/x1.png",
                "caption": "Figure 1:Illustration of the summarization evaluation.",
                "position": 685
            },
            {
                "img": "https://arxiv.org/html/2506.02510/x2.png",
                "caption": "Figure 2:Performance based on GPT-4-Judge scores across languages (a), GICS sectors (b), and input lengths (c).",
                "position": 855
            },
            {
                "img": "https://arxiv.org/html/2506.02510/extracted/6507022/rag.png",
                "caption": "Figure 3:Performance (in GPT-4-Judge score) across different input lengths before or after adding RAG module. Baseline 1 refers to prompting the LLM to answer a list of questions, while Baseline 2 refers to answering one question per response.",
                "position": 911
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethics",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExamples ofM3FinMeeting",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.02510/extracted/6507022/example.png",
                "caption": "Figure 4:Example of annotatedM3FinMeeting.",
                "position": 1888
            }
        ]
    },
    {
        "header": "Appendix BDetails of Precision, Recall and F1 Metrics",
        "images": []
    },
    {
        "header": "Appendix CPrompt Examples for Tasks inM3FinMeeting",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.02510/x3.png",
                "caption": "Figure 5:Prompt template used for the summarization task.",
                "position": 1945
            },
            {
                "img": "https://arxiv.org/html/2506.02510/x4.png",
                "caption": "Figure 6:Prompt template used for the QA pair extraction task.",
                "position": 1948
            },
            {
                "img": "https://arxiv.org/html/2506.02510/x5.png",
                "caption": "Figure 7:Prompt template used for the question answering task. All questions from a document are listed, and the LLMs are prompted to answer them in a single response.",
                "position": 1951
            },
            {
                "img": "https://arxiv.org/html/2506.02510/x6.png",
                "caption": "Figure 8:Prompt template utilized for assessing summarization with GPT-4.",
                "position": 1954
            },
            {
                "img": "https://arxiv.org/html/2506.02510/x7.png",
                "caption": "Figure 9:Prompt template utilized for assessing both QA pair extraction and question answering with GPT-4-Judge.",
                "position": 1957
            }
        ]
    },
    {
        "header": "Appendix DPerformance in BLEU and ROUGE",
        "images": []
    },
    {
        "header": "Appendix EDetailed Performance Across Languages, Lengths, and GICS Sectors",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.02510/x8.png",
                "caption": "Figure 10:Annotation guidelines ofM3FinMeeting.",
                "position": 3691
            }
        ]
    },
    {
        "header": "Appendix FAnnotation Guidelines",
        "images": []
    }
]