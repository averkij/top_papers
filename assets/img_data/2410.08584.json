[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.08584/x1.png",
                "caption": "Figure 1:The attention maps exhibit distinct sparse patterns across different layers (subfigures (a) and (b)) and vary significantly between tasks (subfigures (b) and (c)). Data was collected from the LLaVA-Next-7B model using input samples from the VQAv2 and ChartQA datasets.",
                "position": 114
            },
            {
                "img": "https://arxiv.org/html/2410.08584/x2.png",
                "caption": "Figure 2:Overview of the proposed ZipVL framework during the prefill phase. Here,œÑùúè\\tauitalic_œÑrepresents the threshold for retaining attention scores,nùëõnitalic_nandpùëùpitalic_pare the total number of tokens and the number of important tokens, respectively. After determining the ratio of important tokens and identifying them, we optimize the prefill phase by exclusively computing attention for important tokens. Additionally, we apply mixed-precision quantization to the KV cache, where the KV cache of less important tokens is quantized to a lower bit-width.",
                "position": 128
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.08584/x3.png",
                "caption": "Figure 3:The ratio of important tokens distributed across layers. Data was collected from the LLaVA-Next-7B model using input samples from the VQAv2 and ChartQA datasets.",
                "position": 263
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.08584/x4.png",
                "caption": "(a)LLaVA-v1.5-7B",
                "position": 674
            },
            {
                "img": "https://arxiv.org/html/2410.08584/x4.png",
                "caption": "(a)LLaVA-v1.5-7B",
                "position": 677
            },
            {
                "img": "https://arxiv.org/html/2410.08584/x5.png",
                "caption": "(b)LLaVA-Next-7B",
                "position": 682
            },
            {
                "img": "https://arxiv.org/html/2410.08584/x6.png",
                "caption": "(c)LLaVA-Next-13B",
                "position": 687
            },
            {
                "img": "https://arxiv.org/html/2410.08584/x7.png",
                "caption": "Figure 5:The effect of attention scores retention thresholdœÑùúè\\tauitalic_œÑon the ratio of important tokens and the model performance. Data was collected on GQA benchmark over LLaVA-v1.5-7B model.",
                "position": 904
            },
            {
                "img": "https://arxiv.org/html/2410.08584/x8.png",
                "caption": "(a)",
                "position": 920
            },
            {
                "img": "https://arxiv.org/html/2410.08584/x8.png",
                "caption": "(a)",
                "position": 923
            },
            {
                "img": "https://arxiv.org/html/2410.08584/x9.png",
                "caption": "(b)",
                "position": 928
            }
        ]
    },
    {
        "header": "6Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AEfficient Approximation of Full Attention Scores",
        "images": []
    }
]