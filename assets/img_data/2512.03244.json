[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03244/figs/spark_logo.png",
                "caption": "",
                "position": 65
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03244/x1.png",
                "caption": "Figure 1:Spark: A three-stage pipeline for reference-free RL training with generative process reward models. Stage I: Generate synthetic verification data using inference-time scaling methods (self-consistency and meta-critique) without ground truth through a multi-scale generator-verifier framework. Stage II: Train three generative reward model variants (ORM, PRM, PRM-CoT) via supervised fine-tuning on the synthetic data. Stage III: Apply trained PRMs in RL with GRPO using different reward designs.",
                "position": 86
            },
            {
                "img": "https://arxiv.org/html/2512.03244/x2.png",
                "caption": "Figure 2:Multi-scale generator-verifier framework for synthetic verification data generation. The generator produces multiple solutions per problem, and the verifier evaluates them without ground truth using different inference-time scaling methods.Parallel scaling (Self-Consistency):Generates multiple independent verifications and aggregates them through either outcome-level majority voting (voting on final Yes/No verdicts) or step-level majority voting (voting on each step’s correctness).Sequential scaling (Meta-Critique):Generates an initial verification, critiques it to identify errors, and merges both into a refined verification.",
                "position": 109
            }
        ]
    },
    {
        "header": "2Generating Synthetic Verification Data for PRM Training",
        "images": []
    },
    {
        "header": "3Training Generative Process Reward Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03244/x3.png",
                "caption": "Figure 3:Average F1 scores on ProcessBench for PRM variants trained using synthetic data from different inference-time scaling methods. Leftmost bars show Single Verification baseline (no scaling). All PRMs are fine-tuned from Qwen2.5-14B-Instruct.",
                "position": 194
            }
        ]
    },
    {
        "header": "4Reinforcement Learning with Process Rewards",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03244/x4.png",
                "caption": "Figure 4:Comparison of reference-free PRM-CoT, ground-truth RLVR, and random rewards.Left:Training rewards for (1) PRM-CoT with process-aware rewards (Section4.1), (2) RLVR with ground-truth answer verification, and (3) random rewards via coin flip (50% probability) independent of correctness.Right:Average test accuracy on MATH-500, AIME 2024, and AIME 2025. PRM-CoT consistently outperforms RLVR while spurious random rewards fail to improve from baseline.",
                "position": 490
            },
            {
                "img": "https://arxiv.org/html/2512.03244/x5.png",
                "caption": "Figure 5:Comparison of generative reward models during RL training. Average test accuracy (MATH-500, AIME 2024, AIME 2025) for three variants trained with step-level consistency and used with process-aware rewards (Section4.1). PRM-CoT with chain-of-thought verification consistently outperforms direct step judgment (PRM) and outcome-only verification (ORM).",
                "position": 497
            },
            {
                "img": "https://arxiv.org/html/2512.03244/x6.png",
                "caption": "Figure 6:Comparison of different reward formulations using PRM-CoT on average test accuracy across MATH-500, AIME 2024, and AIME 2025. Selective Advantage achieves the highest performance while Process-Aware rewards remain competitive despite using only final verdicts.",
                "position": 513
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Appendix ASynthetic Verification Data Generation Details",
        "images": []
    },
    {
        "header": "Appendix BGenerative Reward Model Specifications",
        "images": []
    },
    {
        "header": "Appendix CDetailed ProcessBench Evaluation Results",
        "images": []
    },
    {
        "header": "Appendix DExtended RL Experimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03244/x7.png",
                "caption": "Figure 7:INTUITOR’s(zhao2025learning)self-certainty based approach exhibits catastrophic reward hacking. Left: Training reward increases steadily throughout training. Right: Average Pass@16 accuracy across MATH-500, AIME 2024, and AIME 2025 collapses after 150 steps as the model learns to maximize reward by generating confidently wrong answers.",
                "position": 1157
            },
            {
                "img": "https://arxiv.org/html/2512.03244/x8.png",
                "caption": "Figure 8:Training reward dynamics for different step-level integration methods with PRM-CoT as the reward model. Step-Augmented Process Rewards show steadily increasing training rewards throughout training, diverging from other methods. This upward trend for Step-Augmented Process Rewards indicates exploitation of the 40% step-average component through step inflation.",
                "position": 1161
            }
        ]
    },
    {
        "header": "Appendix EReward Exploitation Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03244/x9.png",
                "caption": "Figure 9:RL training dynamics with outcome-level rewards showingreward hacking. (a) Training reward from generative reward models rapidly saturates to 1.0 for PRM and PRM-CoT, indicating exploitation of the reward signal. (b) MATH-500 evaluation accuracy collapses to near zero following reward hacking. (c) Average accuracy on AIME 2024 and 2025 similarly degrades. Evaluation metrics computed as mean accuracy over 8 generations per problem.",
                "position": 1171
            },
            {
                "img": "https://arxiv.org/html/2512.03244/x10.png",
                "caption": "Figure 10:Step count dynamics revealing different exploitation patterns during RL training. (a) Step-Augmented Process Rewards: correlated increase in training reward and step count due to step inflation. (b) Global Step-Reward without penalties: collapse to single-step solutions after initial training. (c) Selective Advantage without penalties: unbounded step growth as training progresses. Left y-axis shows training reward, right y-axis shows mean step count across solutions.",
                "position": 1300
            }
        ]
    },
    {
        "header": "Appendix FPrompts",
        "images": []
    }
]