[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08619/x1.png",
                "caption": "",
                "position": 103
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08619/x2.png",
                "caption": "Figure 2:Overview of LightGen efficient pretraining. (a) Training: Images are encoded into tokens via a pre-trained tokenizer, while text embeddings from a T5 encoder are refined by a trainable aligner. A masked autoencoder uses text tokens as queries/values and image tokens as keys for cross-attention, followed by refinement with a Diffusion MLP (D-MLP). (b) Inference: Tokens are predicted and iteratively refined overNùëÅNitalic_Nsteps, then decoded by the image tokenizer to generate final images.",
                "position": 228
            }
        ]
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08619/extracted/6271669/fig/figure3.png",
                "caption": "Figure 3:Illustrate of DPO Post-processing of LightGen.",
                "position": 705
            },
            {
                "img": "https://arxiv.org/html/2503.08619/x3.png",
                "caption": "Figure 4:Visualization Results.Sample outputs generated using LightGen, showcasing high-quality images at multiple resolutions (256√ó256256256256\\times 256256 √ó 256,512√ó512512512512\\times 512512 √ó 512,1024√ó1024102410241024\\times 10241024 √ó 1024) and across diverse styles (realistic, animated, virtual, etc.), which demonstrate the versatility and scalability of our approach.",
                "position": 778
            },
            {
                "img": "https://arxiv.org/html/2503.08619/extracted/6271669/fig/figure5.png",
                "caption": "Figure 5:Image inpainting demonstrations.",
                "position": 781
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08619/x4.png",
                "caption": "Figure 6:Ablation studies. (a) Pre-train in different data scales, we find it achieves the limitation when pre-train inùí™‚Å¢(1‚Å¢M)ùí™1ùëÄ\\mathcal{O}(1M)caligraphic_O ( 1 italic_M )data scale. (b) demonstrate different iteration results.",
                "position": 843
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08619/extracted/6271669/fig/supp_1.png",
                "caption": "",
                "position": 1599
            },
            {
                "img": "https://arxiv.org/html/2503.08619/x5.png",
                "caption": "Figure 8:Additional generation result.",
                "position": 1822
            }
        ]
    },
    {
        "header": "Appendix ARAR is better than Pure AR",
        "images": []
    },
    {
        "header": "Appendix BWhy MAR is better?",
        "images": []
    },
    {
        "header": "Appendix CUnbiased Estimator",
        "images": []
    }
]