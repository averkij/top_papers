[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.00432/extracted/6582594/figures/hf-icon.png",
                "caption": "",
                "position": 383
            },
            {
                "img": "https://arxiv.org/html/2507.00432/x1.png",
                "caption": "Figure 1:Impact of SFT and RL using math-only training queries on the same base model, Qwen3-14B-Base. Performance improvements are measured relative to the base model. While SFT-trained models partially generalize to other reasoning tasks, they show limited transfer to non-reasoning tasks. In contrast, RL-trained models exhibit broader generalization across both reasoning and non-reasoning scenarios.",
                "position": 385
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.00432/x2.png",
                "caption": "Figure 2:Transferability of mathematical reasoning to other reasoning and non-reasoning tasks. The Transferability Index measures a model‚Äôs ability to transfer performance from mathematics to other domains, with positive values indicating successful transfer and negative values indicating performance degradation. Details of this metric can be found in Section2.1. RL models consistently outperform SFT models, regardless of model size, architecture, or training data, demonstrating superior transferability. Values are displayed using a signed logarithmic transformation for better visualization.",
                "position": 394
            }
        ]
    },
    {
        "header": "2Phenomena: Performance Discrepancies of Reasoning Models",
        "images": []
    },
    {
        "header": "3Latent Representation Shifts: Insights from PCA Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.00432/x3.png",
                "caption": "Figure 3:PCA shift of Qwen3-14B-Base across different training methods and tasks.d(‚àó)superscriptùëëd^{(*)}italic_d start_POSTSUPERSCRIPT ( ‚àó ) end_POSTSUPERSCRIPTis the Euclidean distance between representation centroids before and after training. The first two rows show models trained withSFT, and the last row shows models trained withRL. RL training results in the smallest PCA shift for all task types, suggesting more stable latent representations.",
                "position": 716
            }
        ]
    },
    {
        "header": "4Token Distribution Shifts: Insights from KL Divergence and Rank Analyses",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.00432/x4.png",
                "caption": "Figure 4:Word clouds showing significantly shifted tokens between UniReason-Qwen3-14B-RL (left) and UniReason-Qwen3-14B-SFT-think(right) models on mathematical reasoning tasks. Tokens are extracted based on frequency and rank shifts compared with Qwen3-14B-Base model then categorized as logical-structural words (highlighted in red) or content-specific words (highlighted in blue). Under math reasoning task, the RL model promptly shifts logic-related tokens such asButandSowhile the SFT model shifts various types of tokens, including many irrelevant tokens to the task.",
                "position": 810
            },
            {
                "img": "https://arxiv.org/html/2507.00432/x5.png",
                "caption": "Figure 5:KL divergence analysis of RL and SFT models. Higher KL divergence indicates greater distribution shifts from the original backbone model. We observe that RL models consistently exhibit significantly lower KL divergence compared to SFT models across different tasks, suggesting less distribution shift during training.",
                "position": 817
            },
            {
                "img": "https://arxiv.org/html/2507.00432/x6.png",
                "caption": "Figure 6:Visualization of token rank shifts across different position indices for both reasoning and non-reasoning tasks.\nWe observe that RL models exhibit less token rank shifts while SFT models demonstrate substantial rank shifts across numerous positions throughout the sequence.",
                "position": 820
            },
            {
                "img": "https://arxiv.org/html/2507.00432/x7.png",
                "caption": "Table 3:Case study of shifted tokens for RL and SFT models. Completed queries and answers could be found in AppendixA.4. RL models selectively shift task-relevant or logic-token tokens (labeled in red). In contrast, SFT models exhibit extensive token shifting, including numerous query-irrelevant tokens. For example, non-reasoning queries inappropriately introduce reasoning-related tokens, leading to unnecessary overthinking that detracts from performance.",
                "position": 824
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Author Contributions",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.00432/x7.png",
                "caption": "Figure 7:PCA shift of Qwen2.5-7B across different training methods and tasks.d(‚àó)superscriptùëëd^{(*)}italic_d start_POSTSUPERSCRIPT ( ‚àó ) end_POSTSUPERSCRIPTis the Euclidean distance between representation centroids before and after training. The first row shows models trained withSFT, and the last row shows models trained withRL. RL training results in the smallest PCA shift for all task types, suggesting more stable latent representations.",
                "position": 2671
            },
            {
                "img": "https://arxiv.org/html/2507.00432/x8.png",
                "caption": "Figure 8:PCA shift for Qwen-2.5-7B-Instruct and Qwen2.5-7B under different training paradigms and benchmarks. The first row corresponds to models trained withSFT, while the second row corresponds to theRL. The RL-based training paradigm achieves the lowest PCA shift across all task categories, indicating enhanced stability in the model‚Äôs latent representation space for math, other reasoning, and non-reasoning tasks.d(‚àó)superscriptùëëd^{(*)}italic_d start_POSTSUPERSCRIPT ( ‚àó ) end_POSTSUPERSCRIPTdenotes the Euclidean distance between the centroids of representations before and after training.",
                "position": 2684
            },
            {
                "img": "https://arxiv.org/html/2507.00432/x9.png",
                "caption": "Figure 9:PCA shift for Qwen3-14B-Base under different training paradigms and benchmarks. The first row corresponds to models trained withSFT, while the second row corresponds to theRL. The RL-based training paradigm achieves the lowest PCA shift across all task categories, indicating enhanced stability in the model‚Äôs latent representation space for math, other reasoning, and non-reasoning tasks.d(‚àó)superscriptùëëd^{(*)}italic_d start_POSTSUPERSCRIPT ( ‚àó ) end_POSTSUPERSCRIPTdenotes the Euclidean distance between the centroids of representations before and after training.",
                "position": 2689
            },
            {
                "img": "https://arxiv.org/html/2507.00432/x10.png",
                "caption": "Figure 10:PCA shift for Qwen2.5-32B-Instruct and Qwen2.5-32B under different training paradigms and benchmarks. The first row corresponds to models trained withSFT, while the second row corresponds to theRL. The RL-based training paradigm achieves the lowest PCA shift across all task categories, indicating enhanced stability in the model‚Äôs latent representation space for math, other reasoning, and non-reasoning tasks.d(‚àó)superscriptùëëd^{(*)}italic_d start_POSTSUPERSCRIPT ( ‚àó ) end_POSTSUPERSCRIPTdenotes the Euclidean distance between the centroids of representations before and after training.",
                "position": 2694
            },
            {
                "img": "https://arxiv.org/html/2507.00432/x11.png",
                "caption": "Figure 11:PCA shift for under different model sizes (7B vs 32B) and benchmarks. The first row corresponds to the7Bmodel, while the second row corresponds to the32Bmodel.d(‚àó)superscriptùëëd^{(*)}italic_d start_POSTSUPERSCRIPT ( ‚àó ) end_POSTSUPERSCRIPTdenotes the Euclidean distance between the centroids of representations before and after training.",
                "position": 2703
            },
            {
                "img": "https://arxiv.org/html/2507.00432/x12.png",
                "caption": "Figure 12:PCA shift under different model families (Qwen vs Llama) and benchmarks. The first row corresponds to theQwenmodel, while the second row corresponds to theLlamamodel.d(‚àó)superscriptùëëd^{(*)}italic_d start_POSTSUPERSCRIPT ( ‚àó ) end_POSTSUPERSCRIPTdenotes the Euclidean distance between the centroids of representations before and after training.",
                "position": 2708
            },
            {
                "img": "https://arxiv.org/html/2507.00432/x13.png",
                "caption": "Figure 13:Visualization of token rank shifts across different position indices for off-the-sheld SFT models.",
                "position": 2859
            },
            {
                "img": "https://arxiv.org/html/2507.00432/x14.png",
                "caption": "Figure 14:Visualization of token rank shifts across different position indices for off-the-sheld RL models.",
                "position": 2862
            },
            {
                "img": "https://arxiv.org/html/2507.00432/x15.png",
                "caption": "Figure 15:Average token rank shift of SFT and RL models compared to their backbone models. We generate tokens using fine-tuned models and evaluate their rank shifts under the backbone model‚Äôs distribution. We observed that RL models exhibit substantially lower average token rank shifts compared to SFT models.",
                "position": 2865
            },
            {
                "img": "https://arxiv.org/html/2507.00432/x16.png",
                "caption": "Figure 16:KL divergence analysis of RL and SFT models. Higher KL divergence indicates greater distribution shifts from the original backbone model. We observe that RL models consistently exhibit significantly lower KL divergence compared to SFT models across different tasks, suggesting less distribution shift during training.",
                "position": 2869
            },
            {
                "img": "https://arxiv.org/html/2507.00432/x17.png",
                "caption": "Figure 17:Average token rank shift of SFT and RL models compared to their backbone models. We generate tokens using fine-tuned models and evaluate their rank shifts under the backbone model‚Äôs distribution. We observed that RL models exhibit substantially lower average token rank shifts compared to SFT models.",
                "position": 2872
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]