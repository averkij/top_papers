[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18840/x1.png",
                "caption": "Figure 1:Left:Reading proceeds through avisual–linguistic pathway: thevisualstream identifies letter shapes and patterns in the visual cortex and packages them into recognizable word forms via the visual word form area; thelinguisticstream in the left-hemisphere derives meaning.Right:Subword tokenization tends toover-segment low-resource languagesdue to insufficient vocabulary coverage,e.g., a 2-word Kyrgyz phrase (“world knowledge”) is split into 11 text tokens.\nOur vision-centric tokenization instead compresses the phrase into a single visual token by aggregating features from four adjacent image patches through the projector.",
                "position": 107
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18840/x2.png",
                "caption": "Figure 2:Overview ofSeeTok.Text is rendered into an image, processed by the vision-centric tokenization, and fed to the LLM.\nLoRA layers further boost its ability to follow visual-text instructions.",
                "position": 197
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18840/x3.png",
                "caption": "Figure 3:Left:Fertility and token compression ratio across low-resource languages, comparing text and vision-centric tokenization.Right:COMET-22 scores on FLORES for translations from low-resource languages into English, comparing Qwen2.5-VL 3B trained with visual-text input and with pure-text input.",
                "position": 520
            },
            {
                "img": "https://arxiv.org/html/2510.18840/x4.png",
                "caption": "Figure 4:Accuracy dropon MMLU(Hendrycks et al.,2021)under different orthographic perturbations (character-, visual-, and word-level noise). The vision tokenization–based model (blue) showsmarkedly smaller declinesthan the text-tokenization counterpart (orange), demonstrating stronger robustness to surface noise.",
                "position": 527
            },
            {
                "img": "https://arxiv.org/html/2510.18840/x5.png",
                "caption": "Figure 5:Compositional evaluation of token embeddingsfrom text and vision tokenization across three languages.\nCosine similarity and angle are computed between original full-word embedding (e.g.,offline) and its composed embedding (e.g., {off,line}).Sum (“+”)means the composed embedding is obtained by summing subword embeddings.Space (“@”)denotes composition by concatenating subwords with a space.Vision tokenization yields composed embeddings more consistent with the full word across all languages.",
                "position": 553
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18840/x6.png",
                "caption": "Figure 6:Layer-wise residual norms from orthogonal Procrustes alignment between visual-text and pure-text embeddings. Instruction tuning lowers residual norm in deeper layers, reflecting more consistent processing of pure-text and visual-text inputs.",
                "position": 767
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AExperimental Details",
        "images": []
    },
    {
        "header": "Appendix BDownstream Task Evaluation Details",
        "images": []
    },
    {
        "header": "Appendix CPromising Direction: The Vision-Centric Paradigm",
        "images": []
    },
    {
        "header": "Appendix DEffectiveness under Limited Data",
        "images": []
    },
    {
        "header": "Appendix ECompression Ratio across High- and Low Languages",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18840/x7.png",
                "caption": "Figure 7:Compared with the text tokenization, our vision-centric tokenization achieves a compression ratio of5.71×\\timesin high-resource languages and7.85×\\timesin low-resource languages, significantly reducing sequence length.",
                "position": 1892
            }
        ]
    },
    {
        "header": "Appendix FVisualization of Visual-Text and Low-resource Languages",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18840/x8.png",
                "caption": "Figure 8:Examples of Visual-Text. The text is rendered as images at font sizes 7 and 10, and all images are resized to224×224224\\times 224pixels.",
                "position": 1904
            },
            {
                "img": "https://arxiv.org/html/2510.18840/x9.png",
                "caption": "Figure 9:Visualization of text samples from low-resource languages",
                "position": 1911
            }
        ]
    },
    {
        "header": "Appendix GPerturbation Implementation Details",
        "images": []
    },
    {
        "header": "Appendix HTypoglycemia",
        "images": []
    },
    {
        "header": "Appendix ICompositionality Across Languages",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18840/x10.png",
                "caption": "Figure 10:Visualization of subword compositionality across English and Russian. Vision-centric tokenization captures hierarchical relationships and compositional structure more effectively than subword tokenization.",
                "position": 2132
            },
            {
                "img": "https://arxiv.org/html/2510.18840/x11.png",
                "caption": "Figure 11:Visualization of word-level perturbations in Chinese sentences (including synonym substitution, deletion, and reordering).Redindicates letters whose order has been changed,blueindicates letters that have been added or deleted, andgreenindicates letters that have been replaced with another character.",
                "position": 2136
            }
        ]
    },
    {
        "header": "Appendix JAblation on Fine-tuning Scope",
        "images": []
    },
    {
        "header": "Appendix KGeneralization ofSeeTokto Additional Tasks",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18840/x12.png",
                "caption": "Figure 12:Performance ofSeeTokacross Summarization, Character Count, and Recognizing Textual Entailment tasks. The results highlight the ability of our method to generalize effectively to diverse text processing challenges.",
                "position": 2209
            }
        ]
    },
    {
        "header": "Appendix LLLM Usage Statement",
        "images": []
    }
]