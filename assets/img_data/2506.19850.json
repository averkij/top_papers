[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19850/x1.png",
                "caption": "Figure 1:We present UniVLA, a unified vision-language-action model.Unlike prior VLA approaches that typically rely on an extra vision encoder to extract image features and generate only action outputs, UniVLA represents vision, language, and action as discrete tokens within a unified autoregressive framework. This unified modeling paradigm enables multi-modal outputs and supports a wide range of tasks—such as text-supervised perception grounding, vision-supervised world modeling, and action-supervised policy learning—within a single architecture. The unified token-based design further allows UniVLA to effectively leverage large-scale multimodal data, particularly video, for scalable and generalizable learning. UniVLA achieves new state-of-the-art results on CALVIN, LIBERO, and SimplerEnv-Bridge, significantly surpassing existing methods.",
                "position": 150
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19850/x2.png",
                "caption": "Figure 2:Overview of the UniVLA framework.Our model unifies information from different modalities into a discrete interleaved sequence, which is modeled using an autoregressive Transformer. To enable unified modeling, images are discretized using vector-quantized (VQ) encoders, while actions are transformed into the frequency domain and discretized via Discrete Cosine Transform (DCT) encoding. This causal multimodal sequence naturally preserves the temporal dynamics and causality required for real-world tasks. The model builds upon a pretrained vision-language model and follows a two-stage training strategy: (1) a post-training phase that adopts world-model training on large-scale datasets without requiring actions, and (2) a fine-tuning phase that interleaves actions into the sequence, enabling policy learning on downstream tasks.",
                "position": 233
            }
        ]
    },
    {
        "header": "3Unified Vision-Language-Action Model",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19850/x3.png",
                "caption": "Figure 3:Multimodal capabilities of UniVLA. Top: Action outputs for executing long-horizon tasks in the LIBERO benchmark. Bottom: Visual predictions and spatial grounding demonstrating the model’s spatiotemporal understanding. The red box marks the current observation; green boxes indicate predicted object detections.",
                "position": 1059
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BReal-Robot Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19850/x4.png",
                "caption": "Figure 4:Real-world setup of the AgileX Cobot Magic dual-arm robot.The system is equipped with three RGB cameras for visual observation: one mounted on the left wrist, one on the right wrist, and one positioned above for a high-angle view.",
                "position": 2440
            },
            {
                "img": "https://arxiv.org/html/2506.19850/x5.png",
                "caption": "Figure 5:Real-world task examples.These include diverse tasks such as wiping a whiteboard, organizing tableware, making a burger, and plugging in a connector.",
                "position": 2449
            }
        ]
    },
    {
        "header": "Appendix CAutonomous Driving Experiments",
        "images": []
    }
]