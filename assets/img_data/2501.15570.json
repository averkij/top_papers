[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.15570/extracted/6157273/arwkv-2.drawio.png",
                "caption": "Figure 1:replace self-attention by RWKV-7 time mixing module",
                "position": 143
            }
        ]
    },
    {
        "header": "3From Transformer to RNN",
        "images": []
    },
    {
        "header": "4Evaluation",
        "images": []
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "6Future Work",
        "images": []
    },
    {
        "header": "7Acknowledgements",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.15570/extracted/6157273/rwkv7.png",
                "caption": "Figure 2:RWKV-7 architecture.capability of attention is the key for RNN-based LLMs, which in this case is Time mixing module",
                "position": 529
            },
            {
                "img": "https://arxiv.org/html/2501.15570/extracted/6157273/arwkv-decoderlayer.png",
                "caption": "Figure 3:General Decoder Layer in transformer",
                "position": 532
            },
            {
                "img": "https://arxiv.org/html/2501.15570/extracted/6157273/arwkv-workflow.png",
                "caption": "Figure 4:We replace the standard Attention with an AttentionWrapper that contains both the original self-attention mechanism and a TimeMixer. The TimeMixer is trained to minimize the gap between its output and that of the self-attention module. The final output combines the hidden states from the original self-attention with the residual difference between self-attention and TimeMixer outputs. This architecture enables the model to optimize the TimeMixer to progressively reduce the discrepancy between self-attention and TimeMixer outputs.",
                "position": 535
            },
            {
                "img": "https://arxiv.org/html/2501.15570/extracted/6157273/stage1_no_norm_loss.png",
                "caption": "Figure 5:Stage-1 loss, 18 hours with one 8*h800 80G , context length 2048, 4B tokens",
                "position": 538
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]