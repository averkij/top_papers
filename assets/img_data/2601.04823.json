[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04823/x1.png",
                "caption": "Figure 1:The overview of the DR-LoRA framework.\nPre-trained expert weights are frozen, while each expert is equipped with a trainable LoRA module. These modules start with a small initial rank (rinitr_{\\text{init}}) and can dynamically grow (Δ​r\\Delta r) during training.\nExpert Saliency Scoring guides rank growth by integrating two real-time signals: (1)Expert Routing Frequency (fif_{i}), tracked from the router’s decisions to measure task relevance, and (2)LoRA Rank Importance (gig_{i}), derived from the gradient signals of the trainable LoRA matrices (A and B) to measure learning intensity.",
                "position": 248
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04823/x2.png",
                "caption": "Figure 2:Average accuracy on task-aligned benchmarks (GSM8k, HumanEval, IFEval) during training. DR-LoRA establishes early superiority and maintains the advantage throughout training.",
                "position": 765
            }
        ]
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04823/x3.png",
                "caption": "Figure 3:Performance degradation when masking expert subgroups. On math tasks (GSM8k), masking large experts causes 4×\\timesgreater degradation than masking small experts, confirming task-aligned capacity allocation. On general knowledge (MMLU), both groups contribute similarly.",
                "position": 896
            },
            {
                "img": "https://arxiv.org/html/2601.04823/x4.png",
                "caption": "Figure 4:Expert activation heatmaps on different tasks. On GSM8k, DR-LoRA and LoRA exhibit distinctly different activation patterns. On MMLU, both methods activate largely overlapping expert sets.",
                "position": 907
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitation",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04823/x5.png",
                "caption": "Figure 5:Evolution of expert LoRA ranks during DR-LoRA training on OLMoE. Each heatmap shows the average rank per expert (averaged overup_projanddown_proj) at different training stages, with high-rank experts (darker red) concentrated in task-relevant positions.",
                "position": 1348
            },
            {
                "img": "https://arxiv.org/html/2601.04823/x6.png",
                "caption": "Figure 6:Analysis of per-layer vs. global rank allocation strategies. (a) Expert saliency scores show systematic layer-wise heterogeneity, with deeper layers exhibiting substantially higher average saliency. (b) Under global allocation, this heterogeneity causes rank monopolization: deep layers receive up to+10.7%+10.7\\%excess ranks while shallow layers suffer up to−20.9%-20.9\\%under-provisioning, leading to inferior performance. Per-layer allocation prevents this imbalance by ensuring uniform allocation across layers.",
                "position": 1351
            },
            {
                "img": "https://arxiv.org/html/2601.04823/x7.png",
                "caption": "Figure 7:Expert rank allocation comparison between full DR-LoRA and ablated variants.Top:Overlap with the variant without rank importance (using only routing frequencyfℓ,if_{\\ell,i}).Bottom:Overlap with the variant without routing frequency (using only rank importancegℓ,ig_{\\ell,i}).\nEach heatmap shows the top-25% highest-ranked experts (16 out of 64 per layer) across all layers.",
                "position": 1354
            }
        ]
    },
    {
        "header": "Appendix AExperimental Details and Reproducibility",
        "images": []
    },
    {
        "header": "Appendix BComputational Cost Analysis",
        "images": []
    },
    {
        "header": "Appendix CAdditional Experimental Results",
        "images": []
    }
]