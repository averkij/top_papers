[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/teaser_1step.png",
                "caption": "Figure 1:Qualitative comparisons between DD and vanilla LlamaGenSun et¬†al. (2024)on ImageNet 256√ó\\times√ó256.We show that the generated images ofDDhave small quality loss compared to the pre-trained AR model, while achieving‚â•\\geq‚â•200√ó\\times√óspeedup. More examples are inApp.F.",
                "position": 104
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/teaser_2step.png",
                "caption": "",
                "position": 114
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/teaser_42step_samenoise.png",
                "caption": "",
                "position": 120
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/teaser_256step.png",
                "caption": "",
                "position": 126
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/dd_10.png",
                "caption": "Figure 2:Qualitative results ofDD-2step on text-to-image task.The model is distilled from LlamaGen model with prompts from LAION-COCO dataset. The speedup is around93√ó\\times√ócompared to the teacher model. More examples are inApp.F.",
                "position": 133
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/dd_12.png",
                "caption": "",
                "position": 143
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/dd_17.png",
                "caption": "",
                "position": 149
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/dd_18.png",
                "caption": "",
                "position": 155
            },
            {
                "img": "https://arxiv.org/html/2412.17153/x1.png",
                "caption": "Figure 3:Comparison ofDDmodels, pre-trained models, and other acceleration methods for pre-trained models.DDachieves significant speedup compared to pre-trained models with comparable performance. In contrast, other methods‚Äô performance degrades quickly as inference time decreases.",
                "position": 163
            },
            {
                "img": "https://arxiv.org/html/2412.17153/x2.png",
                "caption": "Figure 4:High-level comparison between ourDistilled Decoding(DD) and prior work. To generate a sequence of tokensqisubscriptùëûùëñq_{i}italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT: (a) the vanilla AR model generates token-by-token, thus being slow; (b) parallel decoding generates multiple tokens in parallel (Sec.4.1), which fundamentally cannot match the generated distribution of the original AR model with one-step generation (seeSec.3.1); (c) ourDDmaps noise tokensœµisubscriptitalic-œµùëñ\\epsilon_{i}italic_œµ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTfrom Gaussian distribution to the whole sequence of generated tokens directlyin one stepand it is guaranteed that (in the optimal case) the distribution of generated tokens matches that of the original AR model.",
                "position": 166
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.17153/x3.png",
                "caption": "Figure 5:AR flow matching. Given all previous tokens, the teacher AR model gives a probability vector for the next token, which defines a mixture of Dirac delta distributions over all tokens in the codebook. We then construct a deterministic mapping between the Gaussian distribution and the Dirac delta distribution with flow matching. The next noise tokenœµ4subscriptitalic-œµ4\\epsilon_{4}italic_œµ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPTis sampled from the Gaussian distribution, and its corresponding token in the codebook becomes the next tokenq4subscriptùëû4q_{4}italic_q start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT.",
                "position": 303
            },
            {
                "img": "https://arxiv.org/html/2412.17153/x4.png",
                "caption": "Figure 6:The training and generation workflow ofDD. GivenX1subscriptùëã1X_{1}italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTwith noise tokensœµisubscriptitalic-œµùëñ\\epsilon_{i}italic_œµ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, the whole trajectoryX1,‚ãØ,X5subscriptùëã1‚ãØsubscriptùëã5X_{1},\\cdots,X_{5}italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚ãØ , italic_X start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPTconsists of data tokensqisubscriptùëûùëñq_{i}italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTand noise tokensœµisubscriptitalic-œµùëñ\\epsilon_{i}italic_œµ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTisuniquelydetermined (Sec.3.2). Assuming the timesteps are set to{t1=1,t2=3}formulae-sequencesubscriptùë°11subscriptùë°23\\{t_{1}=1,t_{2}=3\\}{ italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 1 , italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 3 }. During training (Sec.3.3), we trainDDmodel to reconstructX5subscriptùëã5X_{5}italic_X start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPTgivenX1subscriptùëã1X_{1}italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTorX3subscriptùëã3X_{3}italic_X start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPTas input. TheDDwill then have the capability of jumping fromX1subscriptùëã1X_{1}italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTandX3subscriptùëã3X_{3}italic_X start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPTtoany point in the later trajectory(e.g.,X1subscriptùëã1X_{1}italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTto any of{X2,‚ãØ,X5}subscriptùëã2‚ãØsubscriptùëã5\\{X_{2},\\cdots,X_{5}\\}{ italic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚ãØ , italic_X start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT }).\nDuring generation (Sec.3.3), we can either do 1-step (X1‚ÜíX5‚Üísubscriptùëã1subscriptùëã5X_{1}\\rightarrow X_{5}italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ‚Üí italic_X start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT) or 2-step generation (X1‚ÜíX3‚ÜíX5‚Üísubscriptùëã1subscriptùëã3‚Üísubscriptùëã5X_{1}\\rightarrow X_{3}\\rightarrow X_{5}italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ‚Üí italic_X start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ‚Üí italic_X start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT). Additionally, we can do generation with more steps by incorporating the teacher AR model in part of the generation process, such as 3-step generationX1‚ÜíX2‚ÜíX3‚ÜíX5‚Üísubscriptùëã1subscriptùëã2‚Üísubscriptùëã3‚Üísubscriptùëã5X_{1}\\rightarrow X_{2}\\rightarrow X_{3}\\rightarrow X_{5}italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ‚Üí italic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ‚Üí italic_X start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ‚Üí italic_X start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPTwhereX2‚ÜíX3‚Üísubscriptùëã2subscriptùëã3X_{2}\\rightarrow X_{3}italic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ‚Üí italic_X start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPTutilizes the AR model and other steps use theDDmodel.",
                "position": 387
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Limitations and Future Work",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProof ofProp.3.1",
        "images": []
    },
    {
        "header": "Appendix BModel Architecture Design",
        "images": []
    },
    {
        "header": "Appendix CExperimental Details",
        "images": []
    },
    {
        "header": "Appendix DAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.17153/x5.png",
                "caption": "Figure 7:The training curve of FID vs. epoch or iteration for different intermediate timesteps. FIDs are calculated with 5k generated sample.",
                "position": 2167
            },
            {
                "img": "https://arxiv.org/html/2412.17153/x6.png",
                "caption": "Figure 8:The training curve of FID vs. epoch for different dataset sizes. FIDs are calculated with 5k generated sample.",
                "position": 2170
            }
        ]
    },
    {
        "header": "Appendix EMore Related Work Discussions",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/var1_update_samenoise.png",
                "caption": "Figure 9:Generation results with VAR model(Tian et¬†al.,2024). From left to right: one-stepDDmodel, two-stepDDmodel,DD-pre-trained-4-6, and the pre-trained VAR model.",
                "position": 2197
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/var2_update_samenoise.png",
                "caption": "Figure 10:Generation results with VAR model(Tian et¬†al.,2024). From left to right: one-stepDDmodel, two-stepDDmodel,DD-pre-trained-4-6, and the pre-trained VAR model.",
                "position": 2200
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/llamagen1_update_samenoise.png",
                "caption": "Figure 11:Generation results with LlamaGen model(Sun et¬†al.,2024). From left to right: one-stepDDmodel, two-stepDDmodel,DD-pre-trained-41-81, and the pre-trained LlamaGen model.",
                "position": 2203
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/llamagen2_update_samenoise.png",
                "caption": "Figure 12:Generation results with LlamaGen model(Sun et¬†al.,2024). From left to right: one-stepDDmodel, two-stepDDmodel,DD-pre-trained-41-81, and the pre-trained LlamaGen model.",
                "position": 2206
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/teacher_1.png",
                "caption": "Figure 13:Comparisons of text-to-image results between DD and the pre-trained LlamaGen model(Sun et¬†al.,2024). Prompts without*are taken from the LAION-COCO dataset.",
                "position": 2209
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/dd_1.png",
                "caption": "",
                "position": 2223
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/teacher_2.png",
                "caption": "",
                "position": 2230
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/dd_2.png",
                "caption": "",
                "position": 2230
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/teacher_4.png",
                "caption": "",
                "position": 2236
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/dd_4.png",
                "caption": "",
                "position": 2236
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/teacher_5.png",
                "caption": "",
                "position": 2244
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/dd_5.png",
                "caption": "",
                "position": 2244
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/teacher_6.png",
                "caption": "",
                "position": 2250
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/dd_6.png",
                "caption": "",
                "position": 2250
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/teacher_7.png",
                "caption": "",
                "position": 2257
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/dd_7.png",
                "caption": "",
                "position": 2257
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/teacher_3.png",
                "caption": "Figure 14:Comparisons of text-to-image results between DD and the pre-trained LlamaGen modelSun et¬†al. (2024). Prompts without*are taken from the LAION-COCO dataset.",
                "position": 2263
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/dd_3.png",
                "caption": "",
                "position": 2279
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/teacher_8.png",
                "caption": "",
                "position": 2286
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/dd_8.png",
                "caption": "",
                "position": 2286
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/teacher_9.png",
                "caption": "",
                "position": 2294
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/dd_9.png",
                "caption": "",
                "position": 2294
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/teacher_10.png",
                "caption": "",
                "position": 2300
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/teacher_11.png",
                "caption": "",
                "position": 2308
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/dd_11.png",
                "caption": "",
                "position": 2308
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/teacher_17.png",
                "caption": "",
                "position": 2314
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/teacher_12.png",
                "caption": "Figure 15:Comparisons of text-to-image results between DD and the pre-trained LlamaGen modelSun et¬†al. (2024). Prompts without*are taken from the LAION-COCO dataset.",
                "position": 2320
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/teacher_13.png",
                "caption": "",
                "position": 2339
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/dd_13.png",
                "caption": "",
                "position": 2339
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/teacher_14.png",
                "caption": "",
                "position": 2345
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/dd_14.png",
                "caption": "",
                "position": 2345
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/teacher_15.png",
                "caption": "",
                "position": 2353
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/dd_15.png",
                "caption": "",
                "position": 2353
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/teacher_16.png",
                "caption": "",
                "position": 2359
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/dd_16.png",
                "caption": "",
                "position": 2359
            },
            {
                "img": "https://arxiv.org/html/2412.17153/extracted/6089291/fig/t2i/teacher_18.png",
                "caption": "",
                "position": 2365
            }
        ]
    },
    {
        "header": "Appendix FVisualization",
        "images": []
    }
]