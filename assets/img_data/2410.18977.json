[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18977/x1.png",
                "caption": "Figure 1:MotionCLR (\\textipa/\\textprimstressmoUSn klIr/) supports versatile motion generation and editing.The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of ‚Äújump‚Äù. (b) In-place replacing the action of ‚Äúwalks‚Äù with ‚Äújumps‚Äù and ‚Äúdances‚Äù. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion.",
                "position": 110
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work and Contribution",
        "images": []
    },
    {
        "header": "3Base Motion Generation Model and Understanding Attention Mechanisms",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18977/x2.png",
                "caption": "Figure 2:System overview of MotionCLR architecture.(a) The basic CLR block includes four layers. (b) The sampling (\\akaSamp.) block includes two CLR blocks and one down/up-sampling operation. (c) MotionCLR is a U-Net-like architecture, composed of several Sampling blocks.",
                "position": 178
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x3.png",
                "caption": "Figure 3:Empirical study of attention mechanisms.We use ‚Äúa person jumps.‚Äù as an example. (a) Key frames of generated motion. (b) The root trajectory along theYùëåYitalic_Y-axis (vertical). The character jumps on‚àº15‚àí40similar-toabsent1540\\sim 15-40‚àº 15 - 40f,‚àº60‚àí80similar-toabsent6080\\sim 60-80‚àº 60 - 80f, and‚àº125‚àí145similar-toabsent125145\\sim 125-145‚àº 125 - 145f, respectively. (c) Thecross-attentionbetween timesteps and words. The ‚Äújump‚Äù word is highly activated aligning with the ‚Äújump‚Äù action. (d) Theself-attentionmap visualization. It is obvious that the character jumps three times. Different jumps share similar local motion patterns.",
                "position": 229
            }
        ]
    },
    {
        "header": "4Versatile Applications via Attention Manipulations",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18977/x4.png",
                "caption": "(a)Motion (de-)emphasizing via in/de-creasing cross-attention value.",
                "position": 313
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x4.png",
                "caption": "(a)Motion (de-)emphasizing via in/de-creasing cross-attention value.",
                "position": 316
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x5.png",
                "caption": "(b)In-place motion replacement via replacing self-attention map.",
                "position": 321
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x6.png",
                "caption": "(c)Motion sequence shifting via shifting cross-attention map.",
                "position": 326
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18977/x7.png",
                "caption": "(a)The height of the character‚Äôs root. The highlighted area is obvious when comparing different weights.",
                "position": 534
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x7.png",
                "caption": "(a)The height of the character‚Äôs root. The highlighted area is obvious when comparing different weights.",
                "position": 537
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x8.png",
                "caption": "(b)Visualization of the edited motions on different (de-)emphasizing weight settings.",
                "position": 542
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x9.png",
                "caption": "(a)‚Äúa man jumps.‚Äù case. The second jumping action is erased.",
                "position": 624
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x9.png",
                "caption": "(a)‚Äúa man jumps.‚Äù case. The second jumping action is erased.",
                "position": 627
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x10.png",
                "caption": "(b)‚Äúwaving hand‚Äù case. The final 1/3 waving action is removed.",
                "position": 633
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x11.png",
                "caption": "(a)Rootheightand the motion of ‚Äúa man jumps.‚Äù before editing.",
                "position": 640
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x11.png",
                "caption": "(a)Rootheightand the motion of ‚Äúa man jumps.‚Äù before editing.",
                "position": 643
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x12.png",
                "caption": "(b)Rootvelocityand motion visualization of the edited ‚Äúa manjumps.‚Äù‚Üí‚Üí\\rightarrow‚Üí‚Äúa manwalks.‚Äù motion.",
                "position": 648
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x13.png",
                "caption": "(c)Motion in-place replacement results of a motion including multiple actions.",
                "position": 654
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x14.png",
                "caption": "Figure 8:t-SNE visualization of different example-based generated results.Different colors imply different driven examples.",
                "position": 667
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x15.png",
                "caption": "(a)Prompt: ‚Äúa person walks straight and then waves.‚Äù Original (blue)\\vsshifted (red) motion.",
                "position": 673
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x15.png",
                "caption": "(a)Prompt: ‚Äúa person walks straight and then waves.‚Äù Original (blue)\\vsshifted (red) motion.",
                "position": 676
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x16.png",
                "caption": "(b)Prompt: ‚Äúa man gets up from the ground.‚Äù Original (blue)\\vsshifted (red) motion.",
                "position": 681
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x17.png",
                "caption": "(a)Examples (blue) and generated (red) motions.",
                "position": 688
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x17.png",
                "caption": "(a)Examples (blue) and generated (red) motions.",
                "position": 691
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x18.png",
                "caption": "(b)Root trajectory visualization.",
                "position": 696
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x19.png",
                "caption": "(a)Style reference: ‚Äúthe person dances very happily‚Äù, content reference: ‚Äúthe man is walking‚Äù. The transferred result shows a figure walking in a back-and-forth happy pace.",
                "position": 703
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x19.png",
                "caption": "(a)Style reference: ‚Äúthe person dances very happily‚Äù, content reference: ‚Äúthe man is walking‚Äù. The transferred result shows a figure walking in a back-and-forth happy pace.",
                "position": 706
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x20.png",
                "caption": "(b)Style reference:a man is doing hip-hop dance‚Äù, Content reference:a person runs around a circle‚Äù. The stylized result shows a running motion with bent hands, shaking left and right.",
                "position": 712
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x21.png",
                "caption": "Figure 12:Action counting error rate comparison. Root trajectory (Traj.)\\vsattention map (Ours). ‚ÄúœÉùúé\\sigmaitalic_œÉ‚Äù is the smoothing parameter.",
                "position": 774
            }
        ]
    },
    {
        "header": "6Failure Cases Analysis and Correction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18977/x22.png",
                "caption": "Figure 13:Comparison between w/\\vsw/o grounded motion generation settings.The root height and motion visualization of the textual prompt ‚Äúa person jumps four times‚Äù.",
                "position": 788
            }
        ]
    },
    {
        "header": "7Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "Contribution Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ASupplemental Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18977/x23.png",
                "caption": "Figure 14:Additional visualization results for different (de-)emphasizing weights.The self-attention maps show how varying the different weights (\\eg,‚Üì0.05‚Üìabsent0.05\\downarrow 0.05‚Üì 0.05,‚Üì0.10‚Üìabsent0.10\\downarrow 0.10‚Üì 0.10,‚Üë0.33‚Üëabsent0.33\\uparrow 0.33‚Üë 0.33, and‚Üë1.00‚Üëabsent1.00\\uparrow 1.00‚Üë 1.00) affect the emphasis on motion.",
                "position": 2490
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x24.png",
                "caption": "Figure 15:The effect of varyingwùë§witalic_win classifier-free guidance on generated motions.While changingwùë§witalic_winfluences the general alignment between the text ‚Äúa man jumps.‚Äù and the generated motion, it does not provide precise control over finer details like jump height and frequency.",
                "position": 2543
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x25.png",
                "caption": "Figure 16:Human motion generation result of MotionCLR.",
                "position": 2671
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x26.png",
                "caption": "(a)Prompt: ‚Äúthe person is walking forward on uneven terrain.‚Äù Original (blue)\\vsshifted (red) motion.",
                "position": 2685
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x26.png",
                "caption": "(a)Prompt: ‚Äúthe person is walking forward on uneven terrain.‚Äù Original (blue)\\vsshifted (red) motion.",
                "position": 2688
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x27.png",
                "caption": "(b)Prompt: ‚Äúa person walks then jumps.‚Äù Original (blue)\\vsshifted (red) motion.",
                "position": 2693
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x28.png",
                "caption": "(a)The example motion (blue) and the generated diverse motion (red).",
                "position": 2709
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x28.png",
                "caption": "(a)The example motion (blue) and the generated diverse motion (red).",
                "position": 2712
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x29.png",
                "caption": "(b)The trajectory visualizations of the example motion and diverse motions.",
                "position": 2717
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x30.png",
                "caption": "(a)The example motion (blue) and the generated diverse motion (red).",
                "position": 2724
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x30.png",
                "caption": "(a)The example motion (blue) and the generated diverse motion (red).",
                "position": 2727
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x31.png",
                "caption": "(b)The trajectory visualizations of the example motion and diverse motions.",
                "position": 2732
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x32.png",
                "caption": "(a)The root height comparison. The red area denotes the timesteps to execute actions.",
                "position": 2744
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x32.png",
                "caption": "(a)The root height comparison. The red area denotes the timesteps to execute actions.",
                "position": 2747
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x33.png",
                "caption": "(b)The motion visualization. The vanilla generated result (blue)\\vsedited result (red) w/ temporal grounds.",
                "position": 2753
            }
        ]
    },
    {
        "header": "Appendix BUser Interface for Interactive Motion Generation and Editing",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18977/extracted/5952241/images/demo-figs/gen-1.jpg",
                "caption": "(a)Motion generation interface example.",
                "position": 2782
            },
            {
                "img": "https://arxiv.org/html/2410.18977/extracted/5952241/images/demo-figs/gen-1.jpg",
                "caption": "(a)Motion generation interface example.",
                "position": 2785
            },
            {
                "img": "https://arxiv.org/html/2410.18977/extracted/5952241/images/demo-figs/gen-2.jpg",
                "caption": "(b)Generated Motion Example",
                "position": 2790
            },
            {
                "img": "https://arxiv.org/html/2410.18977/extracted/5952241/images/demo-figs/app1.jpg",
                "caption": "(a)Motion (de-)Emphasizing interface.",
                "position": 2842
            },
            {
                "img": "https://arxiv.org/html/2410.18977/extracted/5952241/images/demo-figs/app1.jpg",
                "caption": "(a)Motion (de-)Emphasizing interface.",
                "position": 2845
            },
            {
                "img": "https://arxiv.org/html/2410.18977/extracted/5952241/images/demo-figs/app4.jpg",
                "caption": "(b)In-place replacement example.",
                "position": 2850
            },
            {
                "img": "https://arxiv.org/html/2410.18977/extracted/5952241/images/demo-figs/app2.jpg",
                "caption": "(c)Example-based motion generation progress.",
                "position": 2856
            },
            {
                "img": "https://arxiv.org/html/2410.18977/extracted/5952241/images/demo-figs/app3.jpg",
                "caption": "(d)Example-based motion generation results.",
                "position": 2861
            }
        ]
    },
    {
        "header": "Appendix CDetailed Diagram of Attention Mechanisms",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18977/x34.png",
                "caption": "(a)ùêí=ùêê‚Å¢ùêä‚ä§ùêíùêêsuperscriptùêätop\\mathbf{\\color[rgb]{0.6875,0.6875,0.6875}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{0.6875,0.6875,0.6875}S}=\\mathbf{{\\color[rgb]{%\n0.96484375,0.7421875,0.5390625}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n0.96484375,0.7421875,0.5390625}Q}{\\color[rgb]{0.9765625,0.828125,0.703125}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{0.9765625,0.828125,0.703125}K}}^{\\top}bold_S = bold_Q bold_K start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT.",
                "position": 2885
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x34.png",
                "caption": "(a)ùêí=ùêê‚Å¢ùêä‚ä§ùêíùêêsuperscriptùêätop\\mathbf{\\color[rgb]{0.6875,0.6875,0.6875}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{0.6875,0.6875,0.6875}S}=\\mathbf{{\\color[rgb]{%\n0.96484375,0.7421875,0.5390625}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n0.96484375,0.7421875,0.5390625}Q}{\\color[rgb]{0.9765625,0.828125,0.703125}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{0.9765625,0.828125,0.703125}K}}^{\\top}bold_S = bold_Q bold_K start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT.",
                "position": 2888
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x35.png",
                "caption": "(b)ùêó‚Ä≤=ùöúùöòùöèùöùùöñùöäùö°‚Å¢(ùêí/d)‚Å¢ùêïsuperscriptùêó‚Ä≤ùöúùöòùöèùöùùöñùöäùö°ùêíùëëùêï\\mathbf{\\color[rgb]{0.92578125,0.71484375,0.515625}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0.92578125,0.71484375,0.515625}X^{\\prime}}=\\mathtt{%\nsoftmax}(\\mathbf{\\color[rgb]{0.6875,0.6875,0.6875}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0.6875,0.6875,0.6875}S}/\\sqrt{d})\\mathbf{\\color[rgb]{%\n0.96484375,0.7421875,0.5390625}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n0.96484375,0.7421875,0.5390625}V}bold_X start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT = typewriter_softmax ( bold_S / square-root start_ARG italic_d end_ARG ) bold_V.",
                "position": 2901
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x36.png",
                "caption": "(a)ùêí=ùêê‚Å¢ùêä‚ä§ùêíùêêsuperscriptùêätop\\mathbf{\\color[rgb]{0.6875,0.6875,0.6875}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{0.6875,0.6875,0.6875}S}=\\mathbf{{\\color[rgb]{1,0.6015625,0.24609375}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{1,0.6015625,0.24609375}Q}{\\color[rgb]%\n{0.94921875,0.62109375,0.66015625}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n0.94921875,0.62109375,0.66015625}K}}^{\\top}bold_S = bold_Q bold_K start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT.",
                "position": 2971
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x36.png",
                "caption": "(a)ùêí=ùêê‚Å¢ùêä‚ä§ùêíùêêsuperscriptùêätop\\mathbf{\\color[rgb]{0.6875,0.6875,0.6875}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{0.6875,0.6875,0.6875}S}=\\mathbf{{\\color[rgb]{1,0.6015625,0.24609375}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{1,0.6015625,0.24609375}Q}{\\color[rgb]%\n{0.94921875,0.62109375,0.66015625}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n0.94921875,0.62109375,0.66015625}K}}^{\\top}bold_S = bold_Q bold_K start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT.",
                "position": 2974
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x37.png",
                "caption": "(b)ùêó‚Ä≤=ùöúùöòùöèùöùùöñùöäùö°‚Å¢(ùêí/d)‚Å¢ùêïsuperscriptùêó‚Ä≤ùöúùöòùöèùöùùöñùöäùö°ùêíùëëùêï\\mathbf{\\color[rgb]{0.32421875,0.40625,0.6953125}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0.32421875,0.40625,0.6953125}X^{\\prime}}=\\mathtt{softmax}%\n(\\mathbf{\\color[rgb]{0.6875,0.6875,0.6875}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{0.6875,0.6875,0.6875}S}/\\sqrt{d})\\mathbf{{\\color[rgb]{%\n0.703125,0.8671875,0.58203125}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n0.703125,0.8671875,0.58203125}V}}bold_X start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT = typewriter_softmax ( bold_S / square-root start_ARG italic_d end_ARG ) bold_V.",
                "position": 2987
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x38.png",
                "caption": "Figure 25:Comparison with previous diffusion-based motion generation models.(a) MDM-like fashion:Tevet et¬†al. [2022b]and its follow-up methods treat text embeddings as a whole and mix them with motion representations using a denoising Transformer. (b) Auto-regressive fashion:Zhang et¬†al. [2023a]and its follow-up methods concatenate the text with the motion sequence and feed them into an auto-regressive transformer without explicit correspondence modeling. (c) Our proposed method establishes fine-grained word-level correspondence using cross-attention mechanisms.",
                "position": 3055
            }
        ]
    },
    {
        "header": "Appendix DMore Visualization Results of Empirical Evidence",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18977/x39.png",
                "caption": "Figure 26:Empirical study of attention patterns.We use the example ‚Äúa person walks stop and then jumps.‚Äù (a) Horizontal distance traveled by the person over time, highlighting distinct walking and jumping phases. (b) The vertical height changes of the person, indicating variations during walking and jumping actions. (c) Thecross-attentionmap between timesteps and the described actions. Notice that ‚Äúwalk‚Äù and ‚Äújump‚Äù receive a stronger attention signal corresponding to the walk and jump segments. (d) Theself-attentionmap, which clearly identifies repeated walking and jumping cycles, shows similar patterns in the sub-actions. (e) Visualization of the motion sequences, demonstrating the walking and jumping actions.",
                "position": 3074
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x40.png",
                "caption": "Figure 27:Empirical study of attention patterns in a consistent walking sequence.We use the example: ‚Äúa man walks.‚Äù. (a) The horizontal distance traveled over time reflects a steady walking motion. (b) Vertical height changes indicate minimal variation, characteristic of walking actions. (c) Thecross-attentionmap shows that the ‚Äúwalks‚Äù word maintains consistent activation. (d) Theself-attentionmap highlights the repeated walking cycles, capturing the temporal stability. (e) Visualization of the motion sequence.",
                "position": 3087
            }
        ]
    },
    {
        "header": "Appendix EImplementation and Evaluation Details",
        "images": []
    },
    {
        "header": "Appendix FDetails of Motion Editing",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18977/x41.png",
                "caption": "Figure 28:The illustration of motion style transfer process. (a) Direct generating style reference: The style information is generated directly using the query (ùêêùêê\\mathbf{Q}bold_Q), key (ùêäùêä\\mathbf{K}bold_K), and value (ùêïùêï\\mathbf{V}bold_V) from the style reference motion sequence (blue). (b) Direct generating content reference: The content information is generated directly from the content reference motion sequence (orange). (c) Generating transferred result: The final transferred motion sequence combines the style from the style reference sequence with the content from the content reference sequence, usingùêêùêê\\mathbf{Q}bold_Qfrom the style reference (blue) andùêäùêä\\mathbf{K}bold_K,ùêïùêï\\mathbf{V}bold_Vfrom the content reference (orange).",
                "position": 3675
            }
        ]
    },
    {
        "header": "Appendix GDetails of Action Counting in a Motion",
        "images": []
    }
]