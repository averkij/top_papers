[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18977/x1.png",
                "caption": "Figure 1:MotionCLR (\\textipa/\\textprimstressmoUSn klIr/) supports versatile motion generation and editing.The blue and red figures represent original and edited motions. (a) Motion deemphasizing and emphasizing via adjusting the weight of â€œjumpâ€. (b) In-place replacing the action of â€œwalksâ€ with â€œjumpsâ€ and â€œdancesâ€. (c) Generating diverse motion with the same example motion. (d) Transferring motion style referring to two motions (style and content reference). (e) Editing the sequentiality of a motion.",
                "position": 110
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work and Contribution",
        "images": []
    },
    {
        "header": "3Base Motion Generation Model and Understanding Attention Mechanisms",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18977/x2.png",
                "caption": "Figure 2:System overview of MotionCLR architecture.(a) The basic CLR block includes four layers. (b) The sampling (\\akaSamp.) block includes two CLR blocks and one down/up-sampling operation. (c) MotionCLR is a U-Net-like architecture, composed of several Sampling blocks.",
                "position": 178
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x3.png",
                "caption": "Figure 3:Empirical study of attention mechanisms.We use â€œa person jumps.â€ as an example. (a) Key frames of generated motion. (b) The root trajectory along theYğ‘ŒYitalic_Y-axis (vertical). The character jumps onâˆ¼15âˆ’40similar-toabsent1540\\sim 15-40âˆ¼ 15 - 40f,âˆ¼60âˆ’80similar-toabsent6080\\sim 60-80âˆ¼ 60 - 80f, andâˆ¼125âˆ’145similar-toabsent125145\\sim 125-145âˆ¼ 125 - 145f, respectively. (c) Thecross-attentionbetween timesteps and words. The â€œjumpâ€ word is highly activated aligning with the â€œjumpâ€ action. (d) Theself-attentionmap visualization. It is obvious that the character jumps three times. Different jumps share similar local motion patterns.",
                "position": 229
            }
        ]
    },
    {
        "header": "4Versatile Applications via Attention Manipulations",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18977/x4.png",
                "caption": "(a)Motion (de-)emphasizing via in/de-creasing cross-attention value.",
                "position": 313
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x4.png",
                "caption": "(a)Motion (de-)emphasizing via in/de-creasing cross-attention value.",
                "position": 316
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x5.png",
                "caption": "(b)In-place motion replacement via replacing self-attention map.",
                "position": 321
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x6.png",
                "caption": "(c)Motion sequence shifting via shifting cross-attention map.",
                "position": 326
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18977/x7.png",
                "caption": "(a)The height of the characterâ€™s root. The highlighted area is obvious when comparing different weights.",
                "position": 534
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x7.png",
                "caption": "(a)The height of the characterâ€™s root. The highlighted area is obvious when comparing different weights.",
                "position": 537
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x8.png",
                "caption": "(b)Visualization of the edited motions on different (de-)emphasizing weight settings.",
                "position": 542
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x9.png",
                "caption": "(a)â€œa man jumps.â€ case. The second jumping action is erased.",
                "position": 624
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x9.png",
                "caption": "(a)â€œa man jumps.â€ case. The second jumping action is erased.",
                "position": 627
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x10.png",
                "caption": "(b)â€œwaving handâ€ case. The final 1/3 waving action is removed.",
                "position": 633
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x11.png",
                "caption": "(a)Rootheightand the motion of â€œa man jumps.â€ before editing.",
                "position": 640
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x11.png",
                "caption": "(a)Rootheightand the motion of â€œa man jumps.â€ before editing.",
                "position": 643
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x12.png",
                "caption": "(b)Rootvelocityand motion visualization of the edited â€œa manjumps.â€â†’â†’\\rightarrowâ†’â€œa manwalks.â€ motion.",
                "position": 648
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x13.png",
                "caption": "(c)Motion in-place replacement results of a motion including multiple actions.",
                "position": 654
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x14.png",
                "caption": "Figure 8:t-SNE visualization of different example-based generated results.Different colors imply different driven examples.",
                "position": 667
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x15.png",
                "caption": "(a)Prompt: â€œa person walks straight and then waves.â€ Original (blue)\\vsshifted (red) motion.",
                "position": 673
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x15.png",
                "caption": "(a)Prompt: â€œa person walks straight and then waves.â€ Original (blue)\\vsshifted (red) motion.",
                "position": 676
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x16.png",
                "caption": "(b)Prompt: â€œa man gets up from the ground.â€ Original (blue)\\vsshifted (red) motion.",
                "position": 681
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x17.png",
                "caption": "(a)Examples (blue) and generated (red) motions.",
                "position": 688
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x17.png",
                "caption": "(a)Examples (blue) and generated (red) motions.",
                "position": 691
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x18.png",
                "caption": "(b)Root trajectory visualization.",
                "position": 696
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x19.png",
                "caption": "(a)Style reference: â€œthe person dances very happilyâ€, content reference: â€œthe man is walkingâ€. The transferred result shows a figure walking in a back-and-forth happy pace.",
                "position": 703
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x19.png",
                "caption": "(a)Style reference: â€œthe person dances very happilyâ€, content reference: â€œthe man is walkingâ€. The transferred result shows a figure walking in a back-and-forth happy pace.",
                "position": 706
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x20.png",
                "caption": "(b)Style reference:a man is doing hip-hop danceâ€, Content reference:a person runs around a circleâ€. The stylized result shows a running motion with bent hands, shaking left and right.",
                "position": 712
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x21.png",
                "caption": "Figure 12:Action counting error rate comparison. Root trajectory (Traj.)\\vsattention map (Ours). â€œÏƒğœ\\sigmaitalic_Ïƒâ€ is the smoothing parameter.",
                "position": 774
            }
        ]
    },
    {
        "header": "6Failure Cases Analysis and Correction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18977/x22.png",
                "caption": "Figure 13:Comparison between w/\\vsw/o grounded motion generation settings.The root height and motion visualization of the textual prompt â€œa person jumps four timesâ€.",
                "position": 788
            }
        ]
    },
    {
        "header": "7Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "Contribution Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ASupplemental Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18977/x23.png",
                "caption": "Figure 14:Additional visualization results for different (de-)emphasizing weights.The self-attention maps show how varying the different weights (\\eg,â†“0.05â†“absent0.05\\downarrow 0.05â†“ 0.05,â†“0.10â†“absent0.10\\downarrow 0.10â†“ 0.10,â†‘0.33â†‘absent0.33\\uparrow 0.33â†‘ 0.33, andâ†‘1.00â†‘absent1.00\\uparrow 1.00â†‘ 1.00) affect the emphasis on motion.",
                "position": 2490
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x24.png",
                "caption": "Figure 15:The effect of varyingwğ‘¤witalic_win classifier-free guidance on generated motions.While changingwğ‘¤witalic_winfluences the general alignment between the text â€œa man jumps.â€ and the generated motion, it does not provide precise control over finer details like jump height and frequency.",
                "position": 2543
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x25.png",
                "caption": "Figure 16:Human motion generation result of MotionCLR.",
                "position": 2671
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x26.png",
                "caption": "(a)Prompt: â€œthe person is walking forward on uneven terrain.â€ Original (blue)\\vsshifted (red) motion.",
                "position": 2685
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x26.png",
                "caption": "(a)Prompt: â€œthe person is walking forward on uneven terrain.â€ Original (blue)\\vsshifted (red) motion.",
                "position": 2688
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x27.png",
                "caption": "(b)Prompt: â€œa person walks then jumps.â€ Original (blue)\\vsshifted (red) motion.",
                "position": 2693
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x28.png",
                "caption": "(a)The example motion (blue) and the generated diverse motion (red).",
                "position": 2709
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x28.png",
                "caption": "(a)The example motion (blue) and the generated diverse motion (red).",
                "position": 2712
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x29.png",
                "caption": "(b)The trajectory visualizations of the example motion and diverse motions.",
                "position": 2717
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x30.png",
                "caption": "(a)The example motion (blue) and the generated diverse motion (red).",
                "position": 2724
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x30.png",
                "caption": "(a)The example motion (blue) and the generated diverse motion (red).",
                "position": 2727
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x31.png",
                "caption": "(b)The trajectory visualizations of the example motion and diverse motions.",
                "position": 2732
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x32.png",
                "caption": "(a)The root height comparison. The red area denotes the timesteps to execute actions.",
                "position": 2744
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x32.png",
                "caption": "(a)The root height comparison. The red area denotes the timesteps to execute actions.",
                "position": 2747
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x33.png",
                "caption": "(b)The motion visualization. The vanilla generated result (blue)\\vsedited result (red) w/ temporal grounds.",
                "position": 2753
            }
        ]
    },
    {
        "header": "Appendix BUser Interface for Interactive Motion Generation and Editing",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18977/extracted/5952241/images/demo-figs/gen-1.jpg",
                "caption": "(a)Motion generation interface example.",
                "position": 2782
            },
            {
                "img": "https://arxiv.org/html/2410.18977/extracted/5952241/images/demo-figs/gen-1.jpg",
                "caption": "(a)Motion generation interface example.",
                "position": 2785
            },
            {
                "img": "https://arxiv.org/html/2410.18977/extracted/5952241/images/demo-figs/gen-2.jpg",
                "caption": "(b)Generated Motion Example",
                "position": 2790
            },
            {
                "img": "https://arxiv.org/html/2410.18977/extracted/5952241/images/demo-figs/app1.jpg",
                "caption": "(a)Motion (de-)Emphasizing interface.",
                "position": 2842
            },
            {
                "img": "https://arxiv.org/html/2410.18977/extracted/5952241/images/demo-figs/app1.jpg",
                "caption": "(a)Motion (de-)Emphasizing interface.",
                "position": 2845
            },
            {
                "img": "https://arxiv.org/html/2410.18977/extracted/5952241/images/demo-figs/app4.jpg",
                "caption": "(b)In-place replacement example.",
                "position": 2850
            },
            {
                "img": "https://arxiv.org/html/2410.18977/extracted/5952241/images/demo-figs/app2.jpg",
                "caption": "(c)Example-based motion generation progress.",
                "position": 2856
            },
            {
                "img": "https://arxiv.org/html/2410.18977/extracted/5952241/images/demo-figs/app3.jpg",
                "caption": "(d)Example-based motion generation results.",
                "position": 2861
            }
        ]
    },
    {
        "header": "Appendix CDetailed Diagram of Attention Mechanisms",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18977/x34.png",
                "caption": "(a)ğ’=ğâ¢ğŠâŠ¤ğ’ğsuperscriptğŠtop\\mathbf{\\color[rgb]{0.6875,0.6875,0.6875}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{0.6875,0.6875,0.6875}S}=\\mathbf{{\\color[rgb]{%\n0.96484375,0.7421875,0.5390625}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n0.96484375,0.7421875,0.5390625}Q}{\\color[rgb]{0.9765625,0.828125,0.703125}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{0.9765625,0.828125,0.703125}K}}^{\\top}bold_S = bold_Q bold_K start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT.",
                "position": 2885
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x34.png",
                "caption": "(a)ğ’=ğâ¢ğŠâŠ¤ğ’ğsuperscriptğŠtop\\mathbf{\\color[rgb]{0.6875,0.6875,0.6875}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{0.6875,0.6875,0.6875}S}=\\mathbf{{\\color[rgb]{%\n0.96484375,0.7421875,0.5390625}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n0.96484375,0.7421875,0.5390625}Q}{\\color[rgb]{0.9765625,0.828125,0.703125}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{0.9765625,0.828125,0.703125}K}}^{\\top}bold_S = bold_Q bold_K start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT.",
                "position": 2888
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x35.png",
                "caption": "(b)ğ—â€²=ğšœğš˜ğšğšğš–ğšŠğš¡â¢(ğ’/d)â¢ğ•superscriptğ—â€²ğšœğš˜ğšğšğš–ğšŠğš¡ğ’ğ‘‘ğ•\\mathbf{\\color[rgb]{0.92578125,0.71484375,0.515625}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0.92578125,0.71484375,0.515625}X^{\\prime}}=\\mathtt{%\nsoftmax}(\\mathbf{\\color[rgb]{0.6875,0.6875,0.6875}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0.6875,0.6875,0.6875}S}/\\sqrt{d})\\mathbf{\\color[rgb]{%\n0.96484375,0.7421875,0.5390625}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n0.96484375,0.7421875,0.5390625}V}bold_X start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT = typewriter_softmax ( bold_S / square-root start_ARG italic_d end_ARG ) bold_V.",
                "position": 2901
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x36.png",
                "caption": "(a)ğ’=ğâ¢ğŠâŠ¤ğ’ğsuperscriptğŠtop\\mathbf{\\color[rgb]{0.6875,0.6875,0.6875}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{0.6875,0.6875,0.6875}S}=\\mathbf{{\\color[rgb]{1,0.6015625,0.24609375}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{1,0.6015625,0.24609375}Q}{\\color[rgb]%\n{0.94921875,0.62109375,0.66015625}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n0.94921875,0.62109375,0.66015625}K}}^{\\top}bold_S = bold_Q bold_K start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT.",
                "position": 2971
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x36.png",
                "caption": "(a)ğ’=ğâ¢ğŠâŠ¤ğ’ğsuperscriptğŠtop\\mathbf{\\color[rgb]{0.6875,0.6875,0.6875}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{0.6875,0.6875,0.6875}S}=\\mathbf{{\\color[rgb]{1,0.6015625,0.24609375}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{1,0.6015625,0.24609375}Q}{\\color[rgb]%\n{0.94921875,0.62109375,0.66015625}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n0.94921875,0.62109375,0.66015625}K}}^{\\top}bold_S = bold_Q bold_K start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT.",
                "position": 2974
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x37.png",
                "caption": "(b)ğ—â€²=ğšœğš˜ğšğšğš–ğšŠğš¡â¢(ğ’/d)â¢ğ•superscriptğ—â€²ğšœğš˜ğšğšğš–ğšŠğš¡ğ’ğ‘‘ğ•\\mathbf{\\color[rgb]{0.32421875,0.40625,0.6953125}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0.32421875,0.40625,0.6953125}X^{\\prime}}=\\mathtt{softmax}%\n(\\mathbf{\\color[rgb]{0.6875,0.6875,0.6875}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{0.6875,0.6875,0.6875}S}/\\sqrt{d})\\mathbf{{\\color[rgb]{%\n0.703125,0.8671875,0.58203125}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n0.703125,0.8671875,0.58203125}V}}bold_X start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT = typewriter_softmax ( bold_S / square-root start_ARG italic_d end_ARG ) bold_V.",
                "position": 2987
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x38.png",
                "caption": "Figure 25:Comparison with previous diffusion-based motion generation models.(a) MDM-like fashion:Tevet etÂ al. [2022b]and its follow-up methods treat text embeddings as a whole and mix them with motion representations using a denoising Transformer. (b) Auto-regressive fashion:Zhang etÂ al. [2023a]and its follow-up methods concatenate the text with the motion sequence and feed them into an auto-regressive transformer without explicit correspondence modeling. (c) Our proposed method establishes fine-grained word-level correspondence using cross-attention mechanisms.",
                "position": 3055
            }
        ]
    },
    {
        "header": "Appendix DMore Visualization Results of Empirical Evidence",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18977/x39.png",
                "caption": "Figure 26:Empirical study of attention patterns.We use the example â€œa person walks stop and then jumps.â€ (a) Horizontal distance traveled by the person over time, highlighting distinct walking and jumping phases. (b) The vertical height changes of the person, indicating variations during walking and jumping actions. (c) Thecross-attentionmap between timesteps and the described actions. Notice that â€œwalkâ€ and â€œjumpâ€ receive a stronger attention signal corresponding to the walk and jump segments. (d) Theself-attentionmap, which clearly identifies repeated walking and jumping cycles, shows similar patterns in the sub-actions. (e) Visualization of the motion sequences, demonstrating the walking and jumping actions.",
                "position": 3074
            },
            {
                "img": "https://arxiv.org/html/2410.18977/x40.png",
                "caption": "Figure 27:Empirical study of attention patterns in a consistent walking sequence.We use the example: â€œa man walks.â€. (a) The horizontal distance traveled over time reflects a steady walking motion. (b) Vertical height changes indicate minimal variation, characteristic of walking actions. (c) Thecross-attentionmap shows that the â€œwalksâ€ word maintains consistent activation. (d) Theself-attentionmap highlights the repeated walking cycles, capturing the temporal stability. (e) Visualization of the motion sequence.",
                "position": 3087
            }
        ]
    },
    {
        "header": "Appendix EImplementation and Evaluation Details",
        "images": []
    },
    {
        "header": "Appendix FDetails of Motion Editing",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18977/x41.png",
                "caption": "Figure 28:The illustration of motion style transfer process. (a) Direct generating style reference: The style information is generated directly using the query (ğğ\\mathbf{Q}bold_Q), key (ğŠğŠ\\mathbf{K}bold_K), and value (ğ•ğ•\\mathbf{V}bold_V) from the style reference motion sequence (blue). (b) Direct generating content reference: The content information is generated directly from the content reference motion sequence (orange). (c) Generating transferred result: The final transferred motion sequence combines the style from the style reference sequence with the content from the content reference sequence, usingğğ\\mathbf{Q}bold_Qfrom the style reference (blue) andğŠğŠ\\mathbf{K}bold_K,ğ•ğ•\\mathbf{V}bold_Vfrom the content reference (orange).",
                "position": 3675
            }
        ]
    },
    {
        "header": "Appendix GDetails of Action Counting in a Motion",
        "images": []
    }
]