[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.03698/x1.png",
                "caption": "Figure 1:Workflow of SAMed-2. It integrates a temporal adapter in the image encoder to capture multi-dimensional context and a confidence-driven memory module to store high-certainty features. During inference, the model retrieves these memory features and fuses them with image embeddings via attention.",
                "position": 141
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.03698/x2.png",
                "caption": "",
                "position": 362
            },
            {
                "img": "https://arxiv.org/html/2507.03698/x3.png",
                "caption": "Figure 3:Few-shot scaling result of our method and compared methods on external prostate segmentation task.",
                "position": 844
            },
            {
                "img": "https://arxiv.org/html/2507.03698/x3.png",
                "caption": "Figure 3:Few-shot scaling result of our method and compared methods on external prostate segmentation task.",
                "position": 847
            },
            {
                "img": "https://arxiv.org/html/2507.03698/x4.png",
                "caption": "Figure 4:Annotation time comparison between manual annotation and SAMed-2-assisted annotation.",
                "position": 852
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]