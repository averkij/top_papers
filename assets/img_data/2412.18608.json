[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18608/x1.png",
                "caption": "",
                "position": 160
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18608/x2.png",
                "caption": "Figure 2:Overview of PartGen.Our method begins with text, single images, or existing 3D objects to obtain an initial grid view of the object. This view is then processed by a diffusion-based segmentation network to achieve multi-view consistent part segmentation. Next, the segmented parts, along with contextual information, are input into a multi-view part completion network to generate a fully completed view of each part. Finally, a pre-trained reconstruction model generates the 3D parts.",
                "position": 305
            },
            {
                "img": "https://arxiv.org/html/2412.18608/x3.png",
                "caption": "Figure 3:Training data.We obtain a dataset of 3D objects decomposed into parts from assets created by artists.\nThese come ‘naturally’ decomposed into parts according to the artist’s design.",
                "position": 377
            },
            {
                "img": "https://arxiv.org/html/2412.18608/x4.png",
                "caption": "Figure 4:Examples of automatic multi-view part segmentations.By running our method several times, we obtain different segmentations, covering the space of artist intents.",
                "position": 447
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18608/x5.png",
                "caption": "Figure 5:Qualitative results of part completion.The images with blue borders are the inputs. Our algorithm produces various plausible outputs across different runs. Even if given an empty part, PartGen attempts to generate internal structures inside the object, such as sand or inner wheels.",
                "position": 660
            },
            {
                "img": "https://arxiv.org/html/2412.18608/x6.png",
                "caption": "Figure 6:Examples of applications.PartGen can effectively generate or reconstruct 3D objects with meaningful and realistic parts in different scenarios: a) Part-aware text-to-3D generation; b) Part-aware image-to-3D generation; c) 3D decomposition.",
                "position": 663
            },
            {
                "img": "https://arxiv.org/html/2412.18608/x7.png",
                "caption": "Figure 7:3D part editing.We can edit the appearance and shape of the 3D objects with text prompt.",
                "position": 666
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18608/x8.png",
                "caption": "Figure 8:3D part editing and captioning examples.The top section illustrates training examples for the editing network, where a mask, a masked image, and text instructions are provided as conditioning to the diffusion network, which fills in the part based on the given textual input. The bottom section demonstrates the input for the part captioning pipeline. Here, a red circle and highlights are used to help the large vision-language model (LVLM) identify and annotate the specific part.",
                "position": 2247
            },
            {
                "img": "https://arxiv.org/html/2412.18608/x9.png",
                "caption": "Figure 9:Recall curve of different methods.Our method achieve better performance comparing with SAM2 and its variants.",
                "position": 2250
            },
            {
                "img": "https://arxiv.org/html/2412.18608/x9.png",
                "caption": "",
                "position": 2253
            },
            {
                "img": "https://arxiv.org/html/2412.18608/x10.png",
                "caption": "",
                "position": 2258
            },
            {
                "img": "https://arxiv.org/html/2412.18608/x11.png",
                "caption": "Figure 10:More examples.Additional examples illustrate that PartGen can process various modalities and effectively generate or reconstruct 3D objects with distinct parts.",
                "position": 2264
            },
            {
                "img": "https://arxiv.org/html/2412.18608/x12.png",
                "caption": "Figure 11:Iteratively adding parts.We show that users can iteratively add parts and combine the results of PartGen pipeline.",
                "position": 2267
            },
            {
                "img": "https://arxiv.org/html/2412.18608/x13.png",
                "caption": "Figure 12:Failure Cases.(a) Multi-view grid generation failure, where the generated views lack 3D consistency. (b) Segmentation failure, where semantically distinct parts are incorrectly grouped together. (c) Reconstruction model failure, where the complex geometry of the input leads to inaccuracies in the depth map.",
                "position": 2270
            }
        ]
    },
    {
        "header": "Appendix BAdditional Experiment Details",
        "images": []
    },
    {
        "header": "Appendix CAdditional Examples",
        "images": []
    },
    {
        "header": "Appendix DFailure Cases",
        "images": []
    },
    {
        "header": "Appendix EEthics and Limitation",
        "images": []
    }
]