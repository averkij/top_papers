[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Our Benchmark ‚ÄîMMLongBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.10610/x1.png",
                "caption": "Figure 1:Performance onMMLongBench.\nWe report results for selected frontier models, and the full results of all models are provided inFigure21. Note that Claude-3.7-Sonnet supports at most 100 images, and we mark the results as N/A for cases with more images (More inSectionD.4)",
                "position": 602
            }
        ]
    },
    {
        "header": "4Evaluation and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.10610/x2.png",
                "caption": "Figure 2:Distribution of long-document VQA (DocVQA) with respect to performance on MM-NIAH variants. We find that the models are concentrated in the coral-shaded areas.",
                "position": 650
            },
            {
                "img": "https://arxiv.org/html/2505.10610/x3.png",
                "caption": "Figure 3:Model performance on VH-Multi dataset. Random guess yields 50% accuracy, highlighting its difficulty.",
                "position": 655
            },
            {
                "img": "https://arxiv.org/html/2505.10610/x4.png",
                "caption": "Figure 4:Spearman‚ÄôsœÅùúå\\rhoitalic_œÅacross all 46 models at 128K tokens.",
                "position": 679
            },
            {
                "img": "https://arxiv.org/html/2505.10610/x5.png",
                "caption": "Figure 5:Spearman‚ÄôsœÅùúå\\rhoitalic_œÅbetween all categories withL=ùêøabsentL=italic_L =128K. For each category, theAvgexcludes the correlation with itself.",
                "position": 691
            },
            {
                "img": "https://arxiv.org/html/2505.10610/x6.png",
                "caption": "Figure 6:Error Analysis on MMLongBench-Doc. Instead of PDF-formatted documents, we feed OCR-extracted plain text to LCVLMs (‚ãÑ‚ãÑ\\diamond‚ãÑw/ OCR) and also test corresponding LLMs: Qwen2.5-7B and Qwen2.5-32B (‚ãÑ‚ãÑ\\diamond‚ãÑw/ LLM). We also show scores on examples with different answer sources.",
                "position": 711
            },
            {
                "img": "https://arxiv.org/html/2505.10610/x7.png",
                "caption": "Figure 7:Error analysis on ViQuAE. We replace the image with its original entity name (‚ãÑ‚ãÑ\\diamond‚ãÑw/ name) and also test text-only counterparts: Qwen2.5-7B and Qwen2.5-32B (‚ãÑ‚ãÑ\\diamond‚ãÑw/ LLM).",
                "position": 716
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADataset Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.10610/x8.png",
                "caption": "Figure 8:Comparison between ROUGE-L and the GPT-4o evaluation on summarization datasets.\nGPT-4o evaluation reflects the performance gain on Gemma3-27B with increased input length, and it also clearly sets apart open-source models with different sizes. In comparison, ROUGE-L remains almost the same for all models and input lengths.",
                "position": 2203
            },
            {
                "img": "https://arxiv.org/html/2505.10610/x9.png",
                "caption": "Figure 9:Data distribution of MMLongBench-Doc and LongDocURL after our pre-processing. Both datasets remain well-distributed, and their distributions are similar to the ones in the original paper.",
                "position": 2286
            }
        ]
    },
    {
        "header": "Appendix BFull Model List",
        "images": []
    },
    {
        "header": "Appendix CExperimental Setup",
        "images": []
    },
    {
        "header": "Appendix DAddtional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.10610/x10.png",
                "caption": "Figure 10:The performance of selected models on VH-Single and VH-Multi. The accuracy of a random guess is 50%. We find that these two tasks are very challenging.",
                "position": 3932
            },
            {
                "img": "https://arxiv.org/html/2505.10610/x11.png",
                "caption": "Figure 11:Spearman‚Äôs correlation at 128K input length, calculated across 46 LCVLMs, between all NIAH and other downstream tasks.",
                "position": 3948
            },
            {
                "img": "https://arxiv.org/html/2505.10610/x12.png",
                "caption": "Figure 12:Results of selected models on the subsets of MM-NIAH containing only text-needle or image-needle examples. (T) and (I) represent text-needle and image-needle examples, respectively.",
                "position": 3953
            },
            {
                "img": "https://arxiv.org/html/2505.10610/x13.png",
                "caption": "Figure 13:Spearman‚Äôs correlation at 128K input length, calculated across 46 LCVLMs, between allMMLongBenchdatasets and category averages.",
                "position": 3969
            },
            {
                "img": "https://arxiv.org/html/2505.10610/x14.png",
                "caption": "Figure 14:Results of applying YaRN[37]to Qwen2.5-VL models. We find that YaRN substantially improves the performance of Qwen2.5-VL-3B. However, the SoTA performance from larger models (i.e., 32B and 72B) only fluctuates slightly.",
                "position": 3994
            },
            {
                "img": "https://arxiv.org/html/2505.10610/x15.png",
                "caption": "Figure 15:Results of applying V2PE[52]to InternVL2-2B. We find V2PE is very sensitive to the visual incrementŒ¥ùõø\\deltaitalic_Œ¥and overfitted to NIAH tasks. Note that we use ROUGE-L here. It is already sufficient to distinguish between models, so there is no need to use the costly GPT-4o evaluation. The numbers in parentheses (i.e., 16, 64, and 256) correspond to the visual incrementŒ¥‚àà{116,164,1256}ùõø1161641256\\delta\\in\\{\\frac{1}{16},\\frac{1}{64},\\frac{1}{256}\\}italic_Œ¥ ‚àà { divide start_ARG 1 end_ARG start_ARG 16 end_ARG , divide start_ARG 1 end_ARG start_ARG 64 end_ARG , divide start_ARG 1 end_ARG start_ARG 256 end_ARG }, respectively.",
                "position": 4014
            }
        ]
    },
    {
        "header": "Appendix EPrompts and Data Examples",
        "images": []
    },
    {
        "header": "Appendix FLimitation",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.10610/x16.png",
                "caption": "Figure 16:Performance of models on ViQuAE at different depths.\nDepth is the position of the gold passage, and its values are[0.0,0.2,0.4,0.6,0.8,1.0]0.00.20.40.60.81.0[0.0,0.2,0.4,0.6,0.8,1.0][ 0.0 , 0.2 , 0.4 , 0.6 , 0.8 , 1.0 ], where0.00.00.00.0is the beginning of the context (the top of each heatmap) and1.01.01.01.0is the end (the bottom of each heatmap).",
                "position": 4118
            },
            {
                "img": "https://arxiv.org/html/2505.10610/x17.png",
                "caption": "Figure 17:Performance of models on VH-Single at different depths.\nDepth is the position of the image containing the target object, and its values are[0.0,0.2,0.4,0.6,0.8,1.0]0.00.20.40.60.81.0[0.0,0.2,0.4,0.6,0.8,1.0][ 0.0 , 0.2 , 0.4 , 0.6 , 0.8 , 1.0 ], where0.00.00.00.0is the beginning of the context (the top of each heatmap) and1.01.01.01.0is the end (the bottom of each heatmap).",
                "position": 4125
            },
            {
                "img": "https://arxiv.org/html/2505.10610/x18.png",
                "caption": "Figure 18:Performance of models on MM-NIAH-Ret (T) at different depths.\nDepth is the position of the text needle, and its values are[0.0,0.2,0.4,0.6,0.8,1.0]0.00.20.40.60.81.0[0.0,0.2,0.4,0.6,0.8,1.0][ 0.0 , 0.2 , 0.4 , 0.6 , 0.8 , 1.0 ], where0.00.00.00.0is the beginning of the context (the top of each heatmap) and1.01.01.01.0is the end (the bottom of each heatmap).",
                "position": 4132
            },
            {
                "img": "https://arxiv.org/html/2505.10610/x19.png",
                "caption": "Figure 19:Performance of models on MM-NIAH-Ret (I) at different depths.\nDepth is the position of the image needle, and its values are[0.0,0.2,0.4,0.6,0.8,1.0]0.00.20.40.60.81.0[0.0,0.2,0.4,0.6,0.8,1.0][ 0.0 , 0.2 , 0.4 , 0.6 , 0.8 , 1.0 ], where0.00.00.00.0is the beginning of the context (the top of each heatmap) and1.01.01.01.0is the end (the bottom of each heatmap).",
                "position": 4139
            },
            {
                "img": "https://arxiv.org/html/2505.10610/x20.png",
                "caption": "Figure 20:Performance of models on MM-NIAH-Reason (I) at different depths.\nDepth is the position of the needle image used for reasoning, and its values are[0.0,0.2,0.4,0.6,0.8,1.0]0.00.20.40.60.81.0[0.0,0.2,0.4,0.6,0.8,1.0][ 0.0 , 0.2 , 0.4 , 0.6 , 0.8 , 1.0 ], where0.00.00.00.0is the beginning of the context (the top of each heatmap) and1.01.01.01.0is the end (the bottom of each heatmap).",
                "position": 4146
            },
            {
                "img": "https://arxiv.org/html/2505.10610/x21.png",
                "caption": "Figure 21:Results of all 46 models onMMLongBenchat various lengths.",
                "position": 4153
            },
            {
                "img": "https://arxiv.org/html/2505.10610/x22.png",
                "caption": "Figure 22:Results of 46 models on categories VRAG and Summ at various lengths.",
                "position": 4157
            },
            {
                "img": "https://arxiv.org/html/2505.10610/x23.png",
                "caption": "Figure 23:Results of 46 name on the category NIAH at various lengths.",
                "position": 4161
            },
            {
                "img": "https://arxiv.org/html/2505.10610/x24.png",
                "caption": "Figure 24:Results of 46 models on the category ICL at various lengths.",
                "position": 4165
            },
            {
                "img": "https://arxiv.org/html/2505.10610/x25.png",
                "caption": "Figure 25:Results of 46 models on the category DocVQA at various lengths.",
                "position": 4169
            },
            {
                "img": "https://arxiv.org/html/2505.10610/x26.png",
                "caption": "Figure 26:Example of InfoSeek dataset in the VRAG category.",
                "position": 4173
            },
            {
                "img": "https://arxiv.org/html/2505.10610/x27.png",
                "caption": "Figure 27:Example of Visual Haystak-Single dataset in NIAH category.Note: The input image list is shown in two columns for display clarity; in the actual input, the images are arranged in a single sequence.",
                "position": 4178
            },
            {
                "img": "https://arxiv.org/html/2505.10610/x28.png",
                "caption": "Figure 28:Example of MM-NIAH-Ret dataset in NIAH category.",
                "position": 4183
            },
            {
                "img": "https://arxiv.org/html/2505.10610/x29.png",
                "caption": "Figure 29:Example of the Stanford Cars dataset in the ICL category.Note: The input image list is shown in three columns for display clarity; in the actual input, the images are arranged in a single sequence.",
                "position": 4188
            },
            {
                "img": "https://arxiv.org/html/2505.10610/x30.png",
                "caption": "Figure 30:Example of GovReport in the summarization category. We only show two pages due to limited space.",
                "position": 4193
            },
            {
                "img": "https://arxiv.org/html/2505.10610/x31.png",
                "caption": "Figure 31:Example of LongDocURL dataset in the DocVQA category. We only show three pages due to limited space.",
                "position": 4198
            }
        ]
    },
    {
        "header": "Appendix GBroader Impact",
        "images": []
    }
]