[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23925/x1.png",
                "caption": "Figure 1:Comparison of different training algorithms for visual reasoning. PPO implicitly approximates the rationale distribution but tends to under‑represent its full diversity due to limited exploration constrained by its reference policy (e.g., the SFT model), and it heavily relies on a critic (reward) model. In contrast, AVI explicitly estimates the true target posteriorP​(Z|X,Y)P(Z|X,Y)through latent rationales, which promote diverse trajectories and inherently prevent reward hacking.",
                "position": 133
            }
        ]
    },
    {
        "header": "3Amortizing Variational Inference for Latent Visual CoT",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23925/x2.png",
                "caption": "Figure 2:Within a complete rationale sequence, we compute the actual reward after eachλ\\lambdasteps and adopt a linear interpolation strategy to estimate the intermediate steps.",
                "position": 161
            },
            {
                "img": "https://arxiv.org/html/2510.23925/x3.png",
                "caption": "Figure 3:Allowing the policy model to explore the state space without constraint causes the catastrophic forgetting issue. The proposed reference-guided exploration effectively addresses this problem.",
                "position": 224
            },
            {
                "img": "https://arxiv.org/html/2510.23925/x4.png",
                "caption": "Figure 4:Inference pipeline of BiN.",
                "position": 260
            }
        ]
    },
    {
        "header": "4Empirical results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23925/x5.png",
                "caption": "Figure 5:Input sequence of training a reasoning LVLM. We usetokento represent learnable parts. Specifically, the fine-tuned reasoning LVLM heavily relies on annotated data during optimization, and the object tokens followed byAssistantenforce reasoning for all instructions. We introduce a new role tokenAnalyzer, so the model can selectively provide reasoning steps.",
                "position": 308
            },
            {
                "img": "https://arxiv.org/html/2510.23925/x6.png",
                "caption": "Figure 6:Maximum log-likelihood and diversity of the sampled rationale. LaCoT model (⋆\\star) samples higher log-likelihood rationale while maintaining higher rationale diversity than SFT (∙\\bullet) model.",
                "position": 491
            },
            {
                "img": "https://arxiv.org/html/2510.23925/x7.png",
                "caption": "Figure 7:Test accuracy on reasoning benchmarks using LaCoT-Qwen-3B. We evaluate the impact of #rationale candidates (NN) and random temperature (T).",
                "position": 608
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23925/x8.png",
                "caption": "Figure 8:Qualitative results of visual reasoning. LaCoT can sample a more diverse and comprehensive reasoning chain than the GRPO model.",
                "position": 677
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments and Disclosure of Funding",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProof of Proposition 1",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23925/x9.png",
                "caption": "Figure F9:Qualitative results of visual reasoning. Wehighlightthe important reasoning steps.",
                "position": 1537
            },
            {
                "img": "https://arxiv.org/html/2510.23925/x10.png",
                "caption": "Figure F10:Qualitative results of visual reasoning. Wehighlightthe important reasoning steps.",
                "position": 1540
            },
            {
                "img": "https://arxiv.org/html/2510.23925/x11.png",
                "caption": "Figure F11:Qualitative results of visual reasoning. Wehighlightthe important reasoning steps.",
                "position": 1543
            }
        ]
    },
    {
        "header": "Appendix BExperiments",
        "images": []
    },
    {
        "header": "NeurIPS Paper Checklist",
        "images": []
    }
]