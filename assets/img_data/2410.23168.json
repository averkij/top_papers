[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.23168/x1.png",
                "caption": "Figure 1:Traditionally, large transformer architectures are trained from scratch without reusing previous smaller-scale models (represented by blue dots on the left). In this paper, we propose a novel fully attention-based architecture that allows scaling model incrementally, thus greatly reducing the overall cost of training large transformer architectures (depicted by red dots on the left). The right panel delineates a comparison between conventional Transformer and our Tokenformer.",
                "position": 74
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.23168/x2.png",
                "caption": "Figure 2:\\ourmethodis a fully attention-driven architecture featuring a new token-Parameterattention(Pattention) layer.\nThe Pattention uses a set of learnable tokens to represent model parameters and lets the input tokens attend to them. As the model scales,\\ourmethodadds new learnable tokens to expand the existing key-value parameter sets, while keeping the feature dimension constant and leaving the rest of the computation unaffected.",
                "position": 156
            },
            {
                "img": "https://arxiv.org/html/2410.23168/x3.png",
                "caption": "Figure 3:Evaluating model scaling costs through cumulative computational budgets. The Transformer baseline incurs expenses for each individual scaling step performed independently from scratch, whereas\\ourmethodaggregates costs across all scaling stages, including training a 124M model initially, progressively scaling to 354M, 757M, and 1.4B parameters.",
                "position": 307
            },
            {
                "img": "https://arxiv.org/html/2410.23168/x3.png",
                "caption": "Figure 3:Evaluating model scaling costs through cumulative computational budgets. The Transformer baseline incurs expenses for each individual scaling step performed independently from scratch, whereas\\ourmethodaggregates costs across all scaling stages, including training a 124M model initially, progressively scaling to 354M, 757M, and 1.4B parameters.",
                "position": 310
            },
            {
                "img": "https://arxiv.org/html/2410.23168/x4.png",
                "caption": "Figure 4:Evaluating model scaling costs by measuring the budget required at each scaling stage. The Transformer baselines used are consistent with those depicted in Figure4, trained with 30B and 300B tokens. Similarly, for\\ourmethod, the cost is the budget required for each incremental scaling step from a smaller one. All the experiments were conducted on TPU v4 hardware.",
                "position": 315
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.23168/x5.png",
                "caption": "Figure 5:The relationship between FLOPs and text length for both Transformer and\\ourmethod. As shown in Table3, Transformer exhibits an increase in computational cost for token-token interactions asdmodelsubscriptùëëmodeld_{\\text{model}}italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPTscales upwards. Our\\ourmethodmodel, however, offers a flexible parameter scaling mechanism that maintainsdtokensubscriptùëëtokend_{\\text{token}}italic_d start_POSTSUBSCRIPT token end_POSTSUBSCRIPTat a constant value. This strategy results in controllable computational costs for token-token interactions and markedly enhances the efficiency of long-text modeling.",
                "position": 726
            },
            {
                "img": "https://arxiv.org/html/2410.23168/x6.png",
                "caption": "Figure 6:Loss curves comparing pre-trained Transformer and\\ourmethodas their parameters are scaled during continued training on enwik8.",
                "position": 770
            },
            {
                "img": "https://arxiv.org/html/2410.23168/x6.png",
                "caption": "Figure 6:Loss curves comparing pre-trained Transformer and\\ourmethodas their parameters are scaled during continued training on enwik8.",
                "position": 773
            },
            {
                "img": "https://arxiv.org/html/2410.23168/x7.png",
                "caption": "Figure 7:Performance benchmarking on incremental model scaling between Transformer with Net2Net scheme and our\\ourmethod.",
                "position": 778
            }
        ]
    },
    {
        "header": "5Future Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AGradient of Pattention Layer",
        "images": []
    },
    {
        "header": "Appendix BZero initialization in\\ourmethod",
        "images": []
    },
    {
        "header": "Appendix CTabular Main Results",
        "images": []
    },
    {
        "header": "Appendix DExperimental Details on Progressive model scaling",
        "images": []
    },
    {
        "header": "Appendix EExperimental Details on Scaling without losing the well-learned distribution",
        "images": []
    },
    {
        "header": "Appendix FExperiments on Language modeling Benchmarking",
        "images": []
    }
]