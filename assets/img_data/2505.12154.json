[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.12154/extracted/6449595/figures/teaser_free.png",
                "caption": "Figure 1:We propose a new task that aims to transform poorly mixed audio into a well-balanced mix using visual guidance. One of our key insights is to use well-curated audio-visual content from a movie database as free supervision to learn the appropriate highlighting effect for audio (L2H).",
                "position": 88
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.12154/x1.png",
                "caption": "Figure 2:Overview of VisAH: (a) Our model takes a poorly mixed audio waveform as input and produces the highlighted audio using a dual U-Net architecture. For simplicity, skip connections are omitted in the illustration. (b) The latent highlighting transformer incorporates vision and text encoders to integrate temporal information, guiding the transformer decoder to transform audio features effectively.",
                "position": 157
            }
        ]
    },
    {
        "header": "4the muddy mix dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.12154/extracted/6449595/figures/data_creation_free.png",
                "caption": "Figure 3:We generate poorly mixed audio from the well-mixed movie audio through the following steps: 1)Separation: We separate the ground truth movie audio into individual tracks for speech, music, and sound effects, allowing for some imperfections in the separation process; 2)Adjustment: For each separated track, we apply either suppression or emphasis, with the intensity selected from three levels: [high, moderate, low]; 3)Remixing: Finally, we combine the adjusted tracks through simple addition to create the poorly mixed input audio.",
                "position": 374
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.12154/extracted/6449595/figures/qualitative_free2.png",
                "caption": "Figure 4:We perform a qualitative comparison by visualizing the waveform and magnitude spectrograms of the highlighted audio results from different methods, along with the input and ground truth. Our method produces results that are closest to the movie GT. Theorange boxdenotes suppressed snippets, andgreen boxindicates highlighted snippets.",
                "position": 614
            },
            {
                "img": "https://arxiv.org/html/2505.12154/extracted/6449595/figures/ranking_distribution.png",
                "caption": "Figure 5:Subjective test: We ask users to rank the four methods based on audio-visual balance to evaluate acoustic highlighting.",
                "position": 723
            },
            {
                "img": "https://arxiv.org/html/2505.12154/extracted/6449595/figures/application.jpg",
                "caption": "Figure 6:We demonstrate that our VisAH method enhances the quality of video-to-audio generation results.",
                "position": 741
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Project Page",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.12154/x2.png",
                "caption": "Figure 7:Failure case analysis: the sound effect (waterfall) overwhelms the speech.",
                "position": 1817
            },
            {
                "img": "https://arxiv.org/html/2505.12154/x3.png",
                "caption": "Figure 8:Failure cases analysis: Our method highlights the breathing sound based on the video context but diverges from the movie audio ground truth.",
                "position": 1820
            }
        ]
    },
    {
        "header": "8Failure Case Analysis",
        "images": []
    },
    {
        "header": "9Subjective Test Design",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.12154/extracted/6449595/figures/subjective_platform.png",
                "caption": "Figure 9:Screenshot of subjective test interface.",
                "position": 1846
            }
        ]
    },
    {
        "header": "10Network Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.12154/x4.png",
                "caption": "Figure 10:Design of magnitude and waveform encoders. Each encoder consists of five layers. The features from the waveform and magnitude encoders are combined through element-wise addition after the fifth layer, followed by an additional layer to encode the fused features.",
                "position": 1857
            }
        ]
    },
    {
        "header": "11Loss Function Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.12154/x5.png",
                "caption": "Figure 11:An example of a video frame and its generated caption.",
                "position": 1877
            }
        ]
    },
    {
        "header": "12Motivation for Text Condition.",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.12154/extracted/6449595/figures/metrics_comparison.png",
                "caption": "Figure 12:The improvement trend across the three difficulty levels is evaluated over five metrics.",
                "position": 1888
            }
        ]
    },
    {
        "header": "13Inference Time Comparison",
        "images": []
    },
    {
        "header": "14Analysis of Dataset Difficulty",
        "images": []
    },
    {
        "header": "15Limitations and Future Works",
        "images": []
    }
]