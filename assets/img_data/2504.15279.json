[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15279/x1.png",
                "caption": "Figure 1:Composition of the VisuLogic benchmark and performance of representative MLLMs.The left figure shows the distribution of the 6 categories and their subcategories in VisuLogic. The right figure shows accuracies (%) achieved by MLLMs and by human on each category of VisuLogic.",
                "position": 95
            },
            {
                "img": "https://arxiv.org/html/2504.15279/x2.png",
                "caption": "(a)Pipeline of “MLLM description→→\\rightarrow→LLM” for Question in MMMUyue2023mmmu. It is trivial that SOTA MLLMs extract key visual details, thereby enabling the LLM to answer questions solely based on language reasoning.",
                "position": 101
            },
            {
                "img": "https://arxiv.org/html/2504.15279/x2.png",
                "caption": "(a)Pipeline of “MLLM description→→\\rightarrow→LLM” for Question in MMMUyue2023mmmu. It is trivial that SOTA MLLMs extract key visual details, thereby enabling the LLM to answer questions solely based on language reasoning.",
                "position": 104
            },
            {
                "img": "https://arxiv.org/html/2504.15279/x3.png",
                "caption": "(b)Pipeline of “MLLM description→→\\rightarrow→LLM” for Question in VisuLogic. Even SOTA MLLMs struggle to describe images precisely, leading to ambiguous interpretations.",
                "position": 110
            },
            {
                "img": "https://arxiv.org/html/2504.15279/x4.png",
                "caption": "Figure 3:Comparison of questions from different Benchmarks.Compared to MathVistalu2023mathvista, MathVisionwang2024measuring, and MMMUyue2023mmmu, VisuLogic focuses more explicitly on assessing pure visual reasoning capabilities.",
                "position": 118
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3VisuLogic",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15279/x5.png",
                "caption": "Figure 4:Data curation pipeline of VisuLogic.The pipeline includes Data Collection, Quality Control and Data Taxonomy.",
                "position": 183
            },
            {
                "img": "https://arxiv.org/html/2504.15279/x6.png",
                "caption": "Figure 5:Question examples of different categories in our VisuLogic Benchmark.VisuLogic contains 6 categories of questions, which require models’ abilities in visual logic reasoning.",
                "position": 208
            },
            {
                "img": "https://arxiv.org/html/2504.15279/x7.png",
                "caption": "Figure 6:Solution examples generated by different models.Reference solution and outputs generated by GPT-4ohurst2024gpt, Qwen2.5VL-72B-InstructQwen2.5-VL, InternVL2.5-38Bchen2024internvl, and InternVL2.5-38B with RL. Additionally, the image description and solution from LLMs (o3-mini) are also illustrated.",
                "position": 211
            },
            {
                "img": "https://arxiv.org/html/2504.15279/x8.png",
                "caption": "Figure 7:Hint prompts visualization.Hint prompts examples, which supply solution guidance for MLLMs, are shown in the image, with solution-critical elements highlighted inred.",
                "position": 982
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15279/x9.png",
                "caption": "(a)LLMs’ error distribution.",
                "position": 1077
            },
            {
                "img": "https://arxiv.org/html/2504.15279/x9.png",
                "caption": "(a)LLMs’ error distribution.",
                "position": 1080
            },
            {
                "img": "https://arxiv.org/html/2504.15279/x10.png",
                "caption": "(b)MLLMs’ error distribution.",
                "position": 1085
            },
            {
                "img": "https://arxiv.org/html/2504.15279/x11.png",
                "caption": "(c)Humans’ error distribution.",
                "position": 1090
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AOverview of the Appendix",
        "images": []
    },
    {
        "header": "Appendix BBenchmark Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15279/x12.png",
                "caption": "Figure 9:Image size distribution.The size of images is limited to within the same order of magnitude.",
                "position": 2308
            },
            {
                "img": "https://arxiv.org/html/2504.15279/x13.png",
                "caption": "Figure 10:Distribution of text token length in VisuLogic.",
                "position": 2311
            },
            {
                "img": "https://arxiv.org/html/2504.15279/x14.png",
                "caption": "Figure 11:More examples in VisuLogic of Quantitative Reasoning, Spatial Reasoning, Positional Reasoning.",
                "position": 2321
            },
            {
                "img": "https://arxiv.org/html/2504.15279/x15.png",
                "caption": "Figure 12:More examples in VisuLogic of Attribute Reasoning, Stylistic Reasoning, and Other.",
                "position": 2324
            },
            {
                "img": "https://arxiv.org/html/2504.15279/x16.png",
                "caption": "Figure 13:Distribution of tokens length in LLM evaluation settings, including image description.",
                "position": 2343
            },
            {
                "img": "https://arxiv.org/html/2504.15279/x17.png",
                "caption": "Figure 14:Part of image caption in LLM evaluation.",
                "position": 2346
            },
            {
                "img": "https://arxiv.org/html/2504.15279/x18.png",
                "caption": "Figure 15:Part of image caption in LLM evaluation.",
                "position": 2349
            },
            {
                "img": "https://arxiv.org/html/2504.15279/x19.png",
                "caption": "Figure 16:Solution examples generated by different models.Reference solution and outputs generated by GPT-4o, Qwen2.5VL-72B-Instruct, Gemini-2.0-pro-exp-02-05, Doubao-1.5-Vision-Pro-32K and Claude-3.7-sonnet-thinking. Additionally, the image caption and solution from LLMs (Qwen2.5-72B-Instruct) are also illustrated.",
                "position": 2359
            },
            {
                "img": "https://arxiv.org/html/2504.15279/x20.png",
                "caption": "Figure 17:Solution examples generated by different models.Reference solution and outputs generated by GPT-4o, Kimi-latest, Gemini-2.0-pro-exp-02-05 and Doubao-1.5-Vision-Pro-32K. Additionally, the image caption and solution from LLMs (Qwen2.5-72B-Instruct) are also illustrated.",
                "position": 2362
            },
            {
                "img": "https://arxiv.org/html/2504.15279/x21.png",
                "caption": "Figure 18:Solution examples generated by different models.Reference solution and outputs generated by GPT-4o, Qwen2.5VL-72B, Gemini-2.0-pro-exp-02-05 and Doubao-1.5-Vision-Pro-32k. Additionally, the image caption and solution from LLMs (o3-mini) are also illustrated.",
                "position": 2365
            },
            {
                "img": "https://arxiv.org/html/2504.15279/x22.png",
                "caption": "Figure 19:Examples of hint prompts. Hint prompts are provided to guide reasoning without revealing the final answer directly.",
                "position": 2375
            },
            {
                "img": "https://arxiv.org/html/2504.15279/x23.png",
                "caption": "Figure 20:Comparison of model outputs before and after RL training stage for Qwen2.5-VL-7B.",
                "position": 2557
            },
            {
                "img": "https://arxiv.org/html/2504.15279/x24.png",
                "caption": "Figure 21:Comparison of model outputs before and after RL training stage for Qwen2.5-VL-7B.",
                "position": 2560
            },
            {
                "img": "https://arxiv.org/html/2504.15279/x25.png",
                "caption": "Figure 22:Comparison of model outputs before and after RL training stage for Qwen2.5-VL-7B.",
                "position": 2563
            },
            {
                "img": "https://arxiv.org/html/2504.15279/x26.png",
                "caption": "Figure 23:Comparison of model outputs before and after RL training stage for InternVL-2.5-38B.",
                "position": 2566
            },
            {
                "img": "https://arxiv.org/html/2504.15279/x27.png",
                "caption": "Figure 24:Comparison of model outputs before and after RL training stage for InternVL-2.5-38B.",
                "position": 2569
            },
            {
                "img": "https://arxiv.org/html/2504.15279/x28.png",
                "caption": "Figure 25:Comparison of model outputs before and after RL training stage for InternVL-2.5-38B.",
                "position": 2572
            }
        ]
    },
    {
        "header": "Appendix CEvaluation & Experiment",
        "images": []
    }
]