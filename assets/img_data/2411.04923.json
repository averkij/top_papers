[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04923/extracted/5975936/figures/logo_videoglamm.png",
                "caption": "",
                "position": 48
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04923/extracted/5975936/figures/cvpr25-teaser.png",
                "caption": "Figure 1:Grounded Conversation with VideoGLaMM.Our proposed multimodal video conversational model provides text responses grounded at the pixel level in the input video. The generated masks are spatio-temporally consistent across frames. The fine-grained grounded outputs from VideoGLaMM describe different levels of granularity, e.g., person, objects (bike), stuff (road), and explain object and scene attributes. Existing Video-LMMs do not\noffer pixel-level grounded conversational capability.",
                "position": 74
            }
        ]
    },
    {
        "header": "2Related work",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04923/x1.png",
                "caption": "Figure 2:Working of VideoGLaMM. VideoGLaMM consists of a dual spatio-temporal encoder for encoding image and video level features. The spatial features represent the local information and the temporal features represent global information. The spatial and temporal tokens are passed through V-L adapters and concatenated with the text tokens, before feeding to LLM. A L-V projector is employed to align LLMâ€™s response with the visual space of pixel decoder. Finally, the aligned LLM features along with the frame features from a frame encoder are passed to a grounded pixel decoder, to obtain the fine-grained object masks corresponding to the LLM response.",
                "position": 141
            }
        ]
    },
    {
        "header": "3VideoGLaMM",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04923/extracted/5975936/figures/videoglamm_annotation_pipeline.png",
                "caption": "Figure 3:Proposed Semi-automatic Annotation Pipeline. Our dataset for grounded conversation generation (GCG) is built from three video dataset types: i)Videos having masks only:Object patches are extracted from video frames using masks and processed by the Gemini model for initial object descriptions, which are then refined to produce detailed object captions. These refined captions and masks are used again with the Gemini model to create dense, grounded captions. ii)Videos having bbox annotations and captions:Frames are first processed with a Video-LMM to generate a comprehensive caption which is combined with the original caption and fed to GPT-4o to obtain dense grounded captions. Masks are generated using frames and ground-truth bounding boxes with the SAM model. iii)Videos having object bboxes and referring expressions:Frames, bounding boxes, and referring expressions are input to GPT-4o for dense grounded captions, while masks are generated by feeding frames and bounding boxes to the SAM model.",
                "position": 255
            }
        ]
    },
    {
        "header": "4Our Benchmark & Annotation Pipeline",
        "images": []
    },
    {
        "header": "5Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04923/extracted/5975936/figures/cvpr25_qualitative_new.jpg",
                "caption": "Figure 4:Qualitative results of VideoGLaMM on grounded conversation generation (GCG). Given user queries, the VideoGLaMM generates textual\nresponses and grounds objects and phrases using pixel-level masks, showing its detailed understanding of the video.",
                "position": 306
            }
        ]
    },
    {
        "header": "6Limitations and Future Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]