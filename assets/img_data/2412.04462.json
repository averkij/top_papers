[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04462/x1.png",
                "caption": "",
                "position": 92
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04462/x2.png",
                "caption": "Figure 2:Overview of 4Real-Video.Left: we initialize the grid of frames with a (generated or real) fixed-viewpoint video in the first row and a freeze-time video in the first column. Middle: our architecture consists of two parallel token streams. The top part processesğ±lvsuperscriptsubscriptğ±ğ‘™v\\mathbf{x}_{l}^{\\text{v}}bold_x start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT v end_POSTSUPERSCRIPTwith viewpoint updates and the bottom part processesğ±ltsuperscriptsubscriptğ±ğ‘™t\\mathbf{x}_{l}^{\\text{t}}bold_x start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT t end_POSTSUPERSCRIPTwith temporal updates. Subsequently, a synchronization layer computes the new tokensğ±l+1vsuperscriptsubscriptğ±ğ‘™1v\\mathbf{x}_{l+1}^{\\text{v}}bold_x start_POSTSUBSCRIPT italic_l + 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT v end_POSTSUPERSCRIPTğ±l+1tsuperscriptsubscriptğ±ğ‘™1t\\mathbf{x}_{l+1}^{\\text{t}}bold_x start_POSTSUBSCRIPT italic_l + 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT t end_POSTSUPERSCRIPTfor the next layer in the diffusion transformer architecture. Right: we propose two implementations of the synchronization layer: hard and soft synchronization.",
                "position": 166
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04462/x3.png",
                "caption": "(a)Relative magnitude ofÎ”â¢ğ²lvÎ”superscriptsubscriptğ²ğ‘™v\\Delta\\mathbf{y}_{l}^{\\text{v}}roman_Î” bold_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT v end_POSTSUPERSCRIPT,Î”â¢ğ²ltÎ”superscriptsubscriptğ²ğ‘™t\\Delta\\mathbf{y}_{l}^{\\text{t}}roman_Î” bold_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT t end_POSTSUPERSCRIPTin Eq.Â (10) at each layer.",
                "position": 405
            },
            {
                "img": "https://arxiv.org/html/2412.04462/x3.png",
                "caption": "(a)Relative magnitude ofÎ”â¢ğ²lvÎ”superscriptsubscriptğ²ğ‘™v\\Delta\\mathbf{y}_{l}^{\\text{v}}roman_Î” bold_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT v end_POSTSUPERSCRIPT,Î”â¢ğ²ltÎ”superscriptsubscriptğ²ğ‘™t\\Delta\\mathbf{y}_{l}^{\\text{t}}roman_Î” bold_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT t end_POSTSUPERSCRIPTin Eq.Â (10) at each layer.",
                "position": 408
            },
            {
                "img": "https://arxiv.org/html/2412.04462/x4.png",
                "caption": "(b)Similarity betweenğ±lvsuperscriptsubscriptğ±ğ‘™v\\mathbf{x}_{l}^{\\text{v}}bold_x start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT v end_POSTSUPERSCRIPTandğ±ltsuperscriptsubscriptğ±ğ‘™t\\mathbf{x}_{l}^{\\text{t}}bold_x start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT t end_POSTSUPERSCRIPTat each layer.",
                "position": 414
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04462/x5.png",
                "caption": "Figure 4:Visual Comparisons.We show two viewpoints for a fixed time for each method. Our method produces high-quality images, even under significant camera motion. In contrast, frames generated by 4Real and SV4D tend to appear more blurred, with objects notably distorted in SV4D. MotionCtrl struggles to generate frames under substantial camera motion. We useredbounding boxes to highlight regions with distortions and flickering, which become particularly noticeable when viewed as a video.",
                "position": 437
            },
            {
                "img": "https://arxiv.org/html/2412.04462/x6.png",
                "caption": "Figure 5:Results from 4Real-Video. We can generate diverse and high-quality dynamic content.",
                "position": 441
            },
            {
                "img": "https://arxiv.org/html/2412.04462/x7.png",
                "caption": "Figure 6:Deformable 3D Gaussian Splatting Reconstructionfrom the generated 4D videos demonstrate the spatial and temporal consistency of the proposed method.",
                "position": 445
            },
            {
                "img": "https://arxiv.org/html/2412.04462/x8.png",
                "caption": "Figure 7:Ablation comparisons.We visually compare the video quality and consistency among different design choices.",
                "position": 683
            },
            {
                "img": "https://arxiv.org/html/2412.04462/x9.png",
                "caption": "Figure 8:User studyagainst optimization-based 4D generation methods across different rating criteria.",
                "position": 687
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Deformable 3D GS\nreconstruction details",
        "images": []
    },
    {
        "header": "7Implementation details of ablation study",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04462/extracted/6046198/figure/user-study-screenshot.png",
                "caption": "Figure 9:A screenshot of the interface for user study.",
                "position": 1592
            }
        ]
    },
    {
        "header": "8User study details",
        "images": []
    }
]