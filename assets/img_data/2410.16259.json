[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.16259/x1.png",
                "caption": "",
                "position": 75
            },
            {
                "img": "https://arxiv.org/html/2410.16259/x2.png",
                "caption": "Figure 1:Learning agent behavior from longitudinal casual video recordings.We answer the following question: can we simulate the behavior of an agent, by learning from casually-captured videos of thesameagent recorded across a long period of time (e.g., a month)? A) We first reconstruct videos in 4D (3D & time), which includes the scene, the trajectory of the agent, and the trajectory of the observer (i.e., camera held by the observer). Such individual 4D reconstructions are registered across time, resulting in acompleteandpersistent4D representation. B) Then we learn a model of the agent for interactive behavior generation. The behavior model explicitly reasons about goals, paths, and full body movements conditioned on the agent‚Äôs ego-perception and past trajectory. Such an agent representation allows generation of novel scenarios through conditioning. For example, conditioned on different observer trajectories, the cat agent chooses to walk to the carpet, stays still while quivering his tail, or hide under the tray stand.Please see videos results in the supplement.",
                "position": 81
            },
            {
                "img": "https://arxiv.org/html/2410.16259/x3.png",
                "caption": "",
                "position": 94
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.16259/x4.png",
                "caption": "Figure 2:Pipeline for behavior generation. We encode egocentric information into a perception codeœâùúî\\omegaitalic_œâ, conditioned on which we generate fully body motion in a hierarchical fashion. We start by generating goalsùêôùêô{\\bf Z}bold_Z, then pathsùêèùêè{\\bf P}bold_Pand finally body posesùêÜùêÜ{\\bf G}bold_G. Each node is represented by the gradient of its log distribution, trained with denoising objectives (Eq.8). GivenùêÜùêÜ{\\bf G}bold_G, the full body motion of an agent can be computed via blend skinning (Eq.3).",
                "position": 291
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.16259/x5.png",
                "caption": "Figure 3:Comparison on multi-video scene reconstruction. We show birds-eye-view rendering of the reconstructed scene using the bunny dataset.\nCompared to TotalRecon that does not register multiple videos, ATS produces higher-quality scene reconstruction. Neural localizer (NL) and featuremetric losses (FBA) are shown important for camera registration. Scene annealing is important for reconstructing a complete scene from partial video captures.",
                "position": 377
            },
            {
                "img": "https://arxiv.org/html/2410.16259/x6.png",
                "caption": "Figure 4:Analysis of conditioning signals. We show results of removing one conditioning signal at a time. Removing observer conditioning and past trajectory conditioning makes the sampled goals more spread out (e.g., regions both in front of the agent and behind the agent); removing the environment conditioning introduces infeasible goals that penetrate the ground and the walls.",
                "position": 703
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.16259/x7.png",
                "caption": "Figure 5:Results of 4D reconstruction. Top: reference images and renderings. Background color represents correspondence. Colored blobs on the cat representB=25ùêµ25B=25italic_B = 25bones (e.g., head is represented by the yellow blob).\nThe magenta colored lines represents reconstructed trajectories of each blob in the world space.\nBottom: Bird‚Äôs eye view of the reconstructed scene and agent trajectories, registered to the same scene coordinate. Each colored line represents a unique video sequence where boxes and spheres indicate the starting and the end location.",
                "position": 730
            }
        ]
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.16259/x8.png",
                "caption": "Figure 6:Qualitative comparison with TotalRecon(Song et¬†al.,2023)on 4D reconstruction. Top: reconstruction of the agent at at specific frame. Total-recon produces shapes with missing limbs and bone transformations that are misaligned with the shape, while our method produces complete shapes and good alignment. Bottom: reconstruction of the environment. TotalRecon produces distorted and incomplete geometry (due to lack of observations from a single video), while our method produces an accurate and complete environment reconstruction.",
                "position": 2020
            },
            {
                "img": "https://arxiv.org/html/2410.16259/x9.png",
                "caption": "Figure 7:Qualitative comparison on 4D reconstruction (Tab.3).We compare with TotalRecon on 4D reconstruction quality. We show novel views rendered with a held-out camera that looks from the opposite side. ATS is able to leverage multiple videos captured at different times to reconstruct the wall (blue box) and the tripod stand (red box) even they are not visible in the input views. Multi-video TotalRecon produces blurry RGB and depth due to bad camera registration. The original TotalRecon takes a single video as input and therefore fails to reconstruct the regions (the tripod and the wall) that are not visible in the input video.",
                "position": 2023
            },
            {
                "img": "https://arxiv.org/html/2410.16259/x10.png",
                "caption": "Figure 8:Visual ablation on scene awareness.We demonstrate the effect of the scene codeœâssubscriptùúîùë†\\omega_{s}italic_œâ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPTthrough goal-conditioned path generation (bird‚Äôs-eye-view, blue sphere‚Üí‚Üí\\rightarrow‚Üígoal; gradient color‚Üí‚Üí\\rightarrow‚Üígenerated path; gray blocks‚Üí‚Üí\\rightarrow‚Üílocations that have been visited in the training data). Conditioned on scene, the generated path abide by the scene geometry, while removing the scene code, the generated paths go through the wall in between two empty spaces.",
                "position": 2030
            },
            {
                "img": "https://arxiv.org/html/2410.16259/x11.png",
                "caption": "Figure 9:Given the 3D trajectories of the agent and the user accumulated over time (top), one could compute their preference represented by 3D heatmaps (bottom). Note the high agent preference over table and sofa.",
                "position": 2037
            },
            {
                "img": "https://arxiv.org/html/2410.16259/x12.png",
                "caption": "Figure 10:Interactivity of the agent. By changing the classifier-free guidance scalesùë†sitalic_s, we can find a trade-off between interactive behavior and unconditional behavior. We demonstrate the control over interactivity by goal-conditioned path generation (bird‚Äôs-eye-view, blue sphere‚Üí‚Üí\\rightarrow‚Üígoal; gradient color‚Üí‚Üí\\rightarrow‚Üígenerated path). With a higher classifier-free guidance scalesùë†sitalic_s, the model is controlled more by the conditional generator, and therefore exhibits higher interactivity.s=0ùë†0s=0italic_s = 0corresponds to fully unconditional generation.",
                "position": 2040
            },
            {
                "img": "https://arxiv.org/html/2410.16259/x13.png",
                "caption": "Figure 11:Generalization ability of the behavior model.Thanks to the ego-centric encoding design (Eq.¬†12), a specific behavior can be learned and generalized to novel situations even it was seen once. Although there‚Äôs only one data point where the cat jumps off the dining table, our method can generate diverse motion of cat jumping off the table while landing at different locations (to the left, middle, and right of the table) as shown in the visual.",
                "position": 2043
            },
            {
                "img": "https://arxiv.org/html/2410.16259/x14.png",
                "caption": "Figure 12:GT correspondence and 3D alignment.Left: Annotated 2D correspondence between the canonical scene (top) and the input image (bottom). Right: we visualize the GT camera registration by transforming the input frame 3D points (blue, back-projected from depth) to the canonical frame (red). The points align visually.",
                "position": 2068
            },
            {
                "img": "https://arxiv.org/html/2410.16259/x14.png",
                "caption": "Figure 12:GT correspondence and 3D alignment.Left: Annotated 2D correspondence between the canonical scene (top) and the input image (bottom). Right: we visualize the GT camera registration by transforming the input frame 3D points (blue, back-projected from depth) to the canonical frame (red). The points align visually.",
                "position": 2071
            },
            {
                "img": "https://arxiv.org/html/2410.16259/x15.png",
                "caption": "Figure 13:Robustness to layout changes.We find our camera localization to be robust to layout changes, e.g., the cushion and the large boxes (left) and the box (right). However, it fails toreconstructlayout changes, especially when they are only observed in a few views.",
                "position": 2076
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]