[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18234/x1.png",
                "caption": "(a)Figure (a) shows the compression ratio (number of text tokens in ground truth/number of vision tokens model used) testing on Fox[21]benchmark; Figure (b) shows performance comparisons on OmniDocBench[27]. DeepSeek-OCR can achieve state-of-the-art performance among end-to-end models enjoying the fewest vision tokens.",
                "position": 136
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18234/x1.png",
                "caption": "Figure 2:Typical vision encoders in popular VLMs. Here are three types of encoders commonly used in current open-source VLMs, all of which suffer from their respective deficiencies.",
                "position": 261
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18234/x2.png",
                "caption": "Figure 3:The architecture of DeepSeek-OCR. DeepSeek-OCR consists of a DeepEncoder and a DeepSeek-3B-MoE decoder. DeepEncoder is the core of DeepSeek-OCR, comprising three components: a SAM[17]for perception dominated by window attention, a CLIP[29]for knowledge with dense global attention, and a 16×\\timestoken compressor that bridges between them.",
                "position": 280
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18234/x3.png",
                "caption": "Figure 4:To test model performance under different compression ratios (requiring different numbers of vision tokens) and enhance the practicality of DeepSeek-OCR, we configure it with multiple resolution modes.",
                "position": 299
            },
            {
                "img": "https://arxiv.org/html/2510.18234/x4.png",
                "caption": "(a)OCR 1.0 fine annotations display. We format the ground truth into an interleaved layout and text format, where each paragraph of text is preceded by the coordinates and label of it in the original image. All coordinates are normalized into 1000 bins.",
                "position": 408
            },
            {
                "img": "https://arxiv.org/html/2510.18234/x4.png",
                "caption": "(b)For charts, we do not use OneChart’s[7]dictionary format, but instead use HTML table format as labels, which can save a certain amount of tokens. For plane geometry, we convert the ground truth to dictionary format, where the dictionary contains keys such as line segments, endpoint coordinates, line segment types, etc., for better readability. Each line segment is encoded using the Slow Perception[39]manner.",
                "position": 428
            }
        ]
    },
    {
        "header": "4Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18234/x4.png",
                "caption": "Figure 6:In the field of financial research reports, the deep parsing mode of DeepSeek-OCR can be used to obtain structured results of charts within documents. Charts are a crucial form of data representation in finance and scientific fields, and the chart structured extraction is an indispensable capability for future OCR models.",
                "position": 1152
            },
            {
                "img": "https://arxiv.org/html/2510.18234/x5.png",
                "caption": "Figure 7:For books and articles, the deep parsing mode can output dense captions for natural images in the documents. With just a prompt, the model can automatically identify what type of image it is and output the required results.",
                "position": 1155
            },
            {
                "img": "https://arxiv.org/html/2510.18234/x6.png",
                "caption": "Figure 8:DeepSeek-OCR in deep parsing mode can also recognize chemical formulas within chemical documents and convert them to SMILES format. In the future, OCR 1.0+2.0 technology may play a significant role in the development of VLM/LLM in STEM fields.",
                "position": 1158
            },
            {
                "img": "https://arxiv.org/html/2510.18234/x7.png",
                "caption": "Figure 9:DeepSeek-OCR also possesses the capability to copy (structure) simple planar geometric figures. Due to the intricate interdependencies among line segments in geometric shapes, parsing geometry task is extremely challenging and has a long way to go.",
                "position": 1162
            },
            {
                "img": "https://arxiv.org/html/2510.18234/x8.png",
                "caption": "Figure 10:To endow the capability of processing widely crawled PDFs (multilingual data), we train our model with OCR capabilities for nearly 100 languages. Minority language documents can also support both layout and non-layout outputs through different prompts.",
                "position": 1172
            },
            {
                "img": "https://arxiv.org/html/2510.18234/x9.png",
                "caption": "Figure 11:We retain DeepSeek-OCR’s capabilities in general visual understanding, mainly including image description, object detection, grounding, etc. Meanwhile, due to the inclusion of text-only data, DeepSeek-OCR’s language capabilities are also retained. Note that since we do not include SFT (Supervised Fine-Tuning) stage, the model is not a chatbot, and some capabilities need completion prompts to be activated.",
                "position": 1182
            },
            {
                "img": "https://arxiv.org/html/2510.18234/x10.png",
                "caption": "Figure 12:Forgetting mechanisms constitute one of the most fundamental characteristics of human memory. The contexts optical compression approach can simulate this mechanism by rendering previous rounds of historical text onto images for initial compression, then progressively resizing older images to achieve multi-level compression, where token counts gradually decrease and text becomes increasingly blurred, thereby accomplishing textual forgetting.",
                "position": 1185
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]