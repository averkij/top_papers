[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13438/x1.png",
                "caption": "Figure 1:Left: We optimizeanytime reasoningby sampling thinking budgets from a prior distributionp‚Ñ¨subscriptùëù‚Ñ¨p_{\\mathcal{B}}italic_p start_POSTSUBSCRIPT caligraphic_B end_POSTSUBSCRIPTand maximizing the rewards at sampled budgets to push up the area under the curve.\nThis objective naturally introducesverifiable dense rewardsinto the thinking process.Right: Budget Relative Policy Optimization (BRPO) leverages these dense rewards to improve advantage estimation via the Monte Carlo return (RùëÖRitalic_R) and an interpolated baseline that combines current progress (V1subscriptùëâ1V_{1}italic_V start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT) and the average return within the rollout group (V2subscriptùëâ2V_{2}italic_V start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT).",
                "position": 91
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13438/x2.png",
                "caption": "Figure 2:By introducing dense rewards, we achieve better credit assignment during RL training. We assume a uniform distribution over thinking budgets and omit the probability for simplicity.",
                "position": 205
            },
            {
                "img": "https://arxiv.org/html/2505.13438/x3.png",
                "caption": "Figure 3:Left: The correlation coefficient ofV1subscriptùëâ1V_{1}italic_V start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTandV2subscriptùëâ2V_{2}italic_V start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTwithR‚Å¢(x,z,jt)ùëÖùë•ùëßsubscriptùëóùë°R(x,z,j_{t})italic_R ( italic_x , italic_z , italic_j start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ).Right: The normalized variance of our BRPO. We evaluate the R1-Distill-1.5B model under the scenario whereŒª=0.5ùúÜ0.5\\lambda=0.5italic_Œª = 0.5, andp‚Ñ¨subscriptùëù‚Ñ¨p_{\\mathcal{B}}italic_p start_POSTSUBSCRIPT caligraphic_B end_POSTSUBSCRIPTis a uniform distribution over{1000,2000,‚Ä¶,8000}10002000‚Ä¶8000\\{1000,2000,...,8000\\}{ 1000 , 2000 , ‚Ä¶ , 8000 }.",
                "position": 274
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13438/x4.png",
                "caption": "Figure 4:The comparison of anytime reasoning performance between GRPO and ourAnytimeReasonerwith various prior budget distributions. Notably, the accuracies at the maximum token budget (8000) reflect the performance in the standard reasoning task.",
                "position": 377
            },
            {
                "img": "https://arxiv.org/html/2505.13438/x5.png",
                "caption": "Figure 5:Ablation on verifiable dense rewards.",
                "position": 421
            },
            {
                "img": "https://arxiv.org/html/2505.13438/x6.png",
                "caption": "Figure 6:Ablation on decoupled optimization for summary policy.",
                "position": 435
            },
            {
                "img": "https://arxiv.org/html/2505.13438/x7.png",
                "caption": "Figure 7:Ablation on variance reduction.",
                "position": 448
            }
        ]
    },
    {
        "header": "4Related Works",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Table of Contents",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13438/x8.png",
                "caption": "(a)Thinkingis stopped by</think>.",
                "position": 985
            },
            {
                "img": "https://arxiv.org/html/2505.13438/x8.png",
                "caption": "(a)Thinkingis stopped by</think>.",
                "position": 988
            },
            {
                "img": "https://arxiv.org/html/2505.13438/x9.png",
                "caption": "(b)Thinkingis stopped due to out of budget.",
                "position": 993
            }
        ]
    },
    {
        "header": "Appendix BTree-like Generation and Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13438/x10.png",
                "caption": "Figure 9:Our methods utilize a tree-like structure for generation and training.",
                "position": 1004
            }
        ]
    },
    {
        "header": "Appendix CRelation Between Standard and Anytime Reasoning",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13438/x11.png",
                "caption": "Figure 10:The training curves for main results.",
                "position": 1116
            }
        ]
    },
    {
        "header": "Appendix DExperimental Results",
        "images": []
    }
]