[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22603/assets/a32.png",
                "caption": "Fig. 1:(a,b,c)Attention sinks present in BOS and intermediate tokens for different tasks and compression rates (e.g., AVSR task at audio-video compression rates of (16, 5)).(d)Activation magnitudes (z-axis) of the hidden state in Llama-AVSR (16, 5) at layer 5 reveal some features with massive activation in sink tokens.",
                "position": 114
            },
            {
                "img": "https://arxiv.org/html/2510.22603/assets/v5.png",
                "caption": "",
                "position": 124
            },
            {
                "img": "https://arxiv.org/html/2510.22603/assets/a16v5.png",
                "caption": "",
                "position": 131
            },
            {
                "img": "https://arxiv.org/html/2510.22603/x1.png",
                "caption": "",
                "position": 137
            }
        ]
    },
    {
        "header": "3Analysis of Audio-Visual LLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22603/assets/attention_layer.png",
                "caption": "Fig. 2:(a)Intermediate attention sinks originate after layer 2 in Llama-AVSR (16,5).(b)Massive activations in Llama-AVSR (16,5) originate from the MLP of Layer 2.",
                "position": 175
            },
            {
                "img": "https://arxiv.org/html/2510.22603/x2.png",
                "caption": "",
                "position": 185
            },
            {
                "img": "https://arxiv.org/html/2510.22603/x3.png",
                "caption": "Fig. 3:(a)Cosine similarity of intermediate sink tokens and non-sink tokens with BOS token across layers of Llama-AVSR (16, 5).(b)Pairwise cosine similarity heatmap of hidden-state embeddings of tokens of Llama-AVSR (16, 5) in layer 5.",
                "position": 233
            },
            {
                "img": "https://arxiv.org/html/2510.22603/x4.png",
                "caption": "",
                "position": 243
            },
            {
                "img": "https://arxiv.org/html/2510.22603/assets/after_rotate.png",
                "caption": "Fig. 4:(a,b)Sinks and massive activations vanish when disaligned from BOS.(c,d)They emerge when aligned with BOS.",
                "position": 261
            },
            {
                "img": "https://arxiv.org/html/2510.22603/assets/after_rotate_massive.png",
                "caption": "",
                "position": 271
            },
            {
                "img": "https://arxiv.org/html/2510.22603/assets/sink_intro.png",
                "caption": "",
                "position": 278
            },
            {
                "img": "https://arxiv.org/html/2510.22603/assets/sink_intro_massive.png",
                "caption": "",
                "position": 284
            }
        ]
    },
    {
        "header": "4Proposed Method",
        "images": []
    },
    {
        "header": "5Experimental Results",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]