[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14077/x1.png",
                "caption": "Figure 1:Illustrative comparison of a standard multi-turn conversational AI and the ERGO system",
                "position": 170
            }
        ]
    },
    {
        "header": "2Background and Related Works",
        "images": []
    },
    {
        "header": "3Entropy-Guided Context Resetting",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14077/x2.png",
                "caption": "Figure 2:Example Llama3.1-8B run on a GSM8K question withFULL,SHARDED andERGO settings. Each row represents a separate prompt given to the model while each table represents a context window.",
                "position": 329
            },
            {
                "img": "https://arxiv.org/html/2510.14077/icons/list.png",
                "caption": "",
                "position": 330
            },
            {
                "img": "https://arxiv.org/html/2510.14077/icons/shard.png",
                "caption": "",
                "position": 330
            },
            {
                "img": "https://arxiv.org/html/2510.14077/icons/reset.png",
                "caption": "",
                "position": 330
            }
        ]
    },
    {
        "header": "4Experimentation Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14077/icons/document.png",
                "caption": "",
                "position": 353
            },
            {
                "img": "https://arxiv.org/html/2510.14077/icons/database.png",
                "caption": "",
                "position": 382
            },
            {
                "img": "https://arxiv.org/html/2510.14077/icons/api.png",
                "caption": "",
                "position": 386
            },
            {
                "img": "https://arxiv.org/html/2510.14077/icons/data-transformation.png",
                "caption": "",
                "position": 390
            },
            {
                "img": "https://arxiv.org/html/2510.14077/icons/math.png",
                "caption": "",
                "position": 394
            }
        ]
    },
    {
        "header": "5Results & Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14077/x3.png",
                "caption": "Figure 3:Effect of SHARDED and ERGO on Aptitude and Unreliability. Icons represent modelsFULL performance. Green dots represent performance withERGO while red dots representSHARDED performance",
                "position": 467
            },
            {
                "img": "https://arxiv.org/html/2510.14077/icons/python.png",
                "caption": "Table 1:Average PerformanceP¯\\bar{P}comparison across three settings:FULL(single‑turn),SHARDED(multi‑turn baseline), andERGO(multi‑turn with entropy‑guided resetting). Arrow represents change in performance forrelative to, with arrow size representing magnitude of change.",
                "position": 470
            },
            {
                "img": "https://arxiv.org/html/2510.14077/icons/meta.png",
                "caption": "",
                "position": 513
            },
            {
                "img": "https://arxiv.org/html/2510.14077/icons/OpenAI.png",
                "caption": "",
                "position": 549
            },
            {
                "img": "https://arxiv.org/html/2510.14077/x4.png",
                "caption": "Figure 4:Comparison of performance point gains (percentage-point increase in accuracy relative toSHARDED) and number of resets across entropy-guided, random, and quintet reset methods onDatabase,Actions, andMath tasks. Icons represent their respective task with their color determining method used.",
                "position": 722
            },
            {
                "img": "https://arxiv.org/html/2510.14077/icons/python.png",
                "caption": "Table 2:Comparison of average performance acrossCode,Database,Actions,Data-to-Text andMath tasks.",
                "position": 738
            },
            {
                "img": "https://arxiv.org/html/2510.14077/icons/snowball.png",
                "caption": "",
                "position": 775
            },
            {
                "img": "https://arxiv.org/html/2510.14077/icons/recap.png",
                "caption": "",
                "position": 779
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AThreshold Selection Procedure",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14077/icons/microsoft.png",
                "caption": "Table 4:Model versions, thresholds, and calibration percentiles used in our experiments. (Versions included where applicable.)",
                "position": 1208
            }
        ]
    },
    {
        "header": "Appendix BSensitivity to Entropy Threshold (τ\\tau)",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14077/x5.png",
                "caption": "Figure 5:Comparison of maximum performance points gains (highest increase in accuracy when compared to)\nand number of resets between different thresholds across Database, Actions, and\nMath tasks.",
                "position": 1286
            }
        ]
    },
    {
        "header": "Appendix CComputational Cost and Reset Overhead Analysis",
        "images": []
    },
    {
        "header": "Appendix DMetrics",
        "images": []
    }
]