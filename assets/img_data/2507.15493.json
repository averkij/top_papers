[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15493/extracted/6638863/images/teaser.jpeg",
                "caption": "Figure 1:Overview.GR-3 is able to learn from three types of data: vision-language data, robot trajectory data, and human trajectory data.\nIt is able to perform dexterous and long-horizon tasks with exceptional robustness and generalize well to novel objects, environments, and instructions.",
                "position": 148
            },
            {
                "img": "https://arxiv.org/html/2507.15493/extracted/6638863/images/capability.jpg",
                "caption": "Figure 2:Capabilities.GR-3 strictly follows instructions and is capable of understanding unseen instructions involving abstract concepts.\nIt performs robustly and reliably on long-horizon table bussing and dexterous cloth manipulation.",
                "position": 173
            },
            {
                "img": "https://arxiv.org/html/2507.15493/extracted/6638863/images/model_arch.jpg",
                "caption": "Figure 3:The GR-3 Model.GR-3 is co-trained on both robot trajectories and vision-language data with a flow-matching objective (left) and a next-token-prediction objective (right), respectively.",
                "position": 180
            }
        ]
    },
    {
        "header": "2The GR-3 Model",
        "images": []
    },
    {
        "header": "3Training Recipe",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15493/extracted/6638863/images/data.jpg",
                "caption": "Figure 4:The GR-3 Data.We leverage three types of data during training: robot trajectory data (top), human trajectory data (middle), and vision-language data (bottom).",
                "position": 273
            }
        ]
    },
    {
        "header": "4Hardware & System",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15493/extracted/6638863/images/bytemini_robot.jpg",
                "caption": "Figure 5:The ByteMini Robot.We show the robot specifications, multi-camera views, and motion range of the unique wrist sphere joint.",
                "position": 355
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15493/extracted/6638863/images/ppa_exp_setting.jpg",
                "caption": "Figure 6:Experiment Settings of Generalizable Pick-and-Place.(a) Test objects that are seen during training.\n(b) Test objects that are unseen during training.\n(c) The Basic environment is seen during training. The others are out-of-distribution environments that are unseen during training.",
                "position": 427
            },
            {
                "img": "https://arxiv.org/html/2507.15493/extracted/6638863/images/ppa_exp_result.jpg",
                "caption": "Figure 7:Experiment Results of Generalizable Pick-and-Place.(a) Results on generalizable pick-and-place under four different settings.\n(b) Results on few-shot generalization with human trajectories.",
                "position": 462
            },
            {
                "img": "https://arxiv.org/html/2507.15493/extracted/6638863/images/table_exp.jpg",
                "caption": "Figure 8:Experiment Settings & Results of Table Bussing.(a) Flat:\nthe robot is required to perform long-horizon table bussing in a single run.\n(b) Instruction following (IF):\nthe robot is prompted with multiple sub-task descriptions in a roll.\n(c) Test objects.\n(d) Results on the flat and instruction following (IF) settings.",
                "position": 521
            },
            {
                "img": "https://arxiv.org/html/2507.15493/extracted/6638863/images/cloth_exp_setting.jpg",
                "caption": "Figure 9:Experiment Settings of Dexterous Cloth Manipulation.(a) Seen and unseen clothes in the test set.\n(b) The Basic and Position settings.",
                "position": 596
            },
            {
                "img": "https://arxiv.org/html/2507.15493/extracted/6638863/images/cloth_exp_result.jpg",
                "caption": "Figure 10:Experiment Results on Dexterous Cloth Manipulation.(a) Sankey diagram of success (solid) and failure (hatch) across entire rollouts of the Basic setting.\n(b) Average task progresses ofœÄ0subscriptùúã0\\pi_{0}italic_œÄ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTand GR-3 in the three evaluation settings.",
                "position": 664
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Limitations & Conclusions",
        "images": []
    },
    {
        "header": "8Contributions and Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]