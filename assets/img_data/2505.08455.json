[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.08455/extracted/6434868/figures/web.png",
                "caption": "",
                "position": 77
            },
            {
                "img": "https://arxiv.org/html/2505.08455/extracted/6434868/figures/git.png",
                "caption": "",
                "position": 78
            },
            {
                "img": "https://arxiv.org/html/2505.08455/extracted/6434868/figures/hf.png",
                "caption": "",
                "position": 79
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.08455/x1.png",
                "caption": "Figure 1:Example question and video.We present an example of video-based long-form causal reasoning task fromVCRBench.The correct order is: Clip 1: Cut lemon into slices, Clip 5: Squeeze lemon into the pitcher, Clip 4: Pour lemon juice and water into the pitcher, Clip 3: Stir the lemonade mixture, Clip 2: Pour lemonade into a glass.",
                "position": 103
            },
            {
                "img": "https://arxiv.org/html/2505.08455/x2.png",
                "caption": "Figure 2:Impact of RRD.Qwen2.5-VL-Instruct72B72B{}_{\\text{72B}}start_FLOATSUBSCRIPT 72B end_FLOATSUBSCRIPTwith RRD outperforms Gemini-1.5-Pro and achieve comparable performance to Gemini-2-Flash-Thinking.",
                "position": 142
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Video-based long-form Causal Reasoning Benchmark (VCRBench)",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.08455/x3.png",
                "caption": "Figure 3:Overview of video construction.Step 1:Given a complete video, key procedural steps are identified based on human-annotated timestamps.Step 2:We keep the key events and discard those that do not depict visual events directly associated with the goal, such as talking or narrating in this example of grilling steak.Step 3:Each key event is shuffled across time and assigned a clip number. These clips are then merged together to form the final test sample.",
                "position": 226
            },
            {
                "img": "https://arxiv.org/html/2505.08455/x4.png",
                "caption": "Figure 5:Keystatisticsof ourVCRBench.",
                "position": 255
            },
            {
                "img": "https://arxiv.org/html/2505.08455/x5.png",
                "caption": "",
                "position": 261
            },
            {
                "img": "https://arxiv.org/html/2505.08455/x6.png",
                "caption": "",
                "position": 262
            },
            {
                "img": "https://arxiv.org/html/2505.08455/x7.png",
                "caption": "",
                "position": 273
            },
            {
                "img": "https://arxiv.org/html/2505.08455/x8.png",
                "caption": "",
                "position": 274
            }
        ]
    },
    {
        "header": "4Benchmarking Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.08455/extracted/6434868/figures/bronze.png",
                "caption": "Table 1:Results onVCRBench.Most open-source LVLMs perform at or below random guess, and even the best LVLM falls significantly short of human performance.\nWefadenumbers that fall below the random guess baseline.",
                "position": 345
            },
            {
                "img": "https://arxiv.org/html/2505.08455/extracted/6434868/figures/bronze.png",
                "caption": "Table 1:Results onVCRBench.Most open-source LVLMs perform at or below random guess, and even the best LVLM falls significantly short of human performance.\nWefadenumbers that fall below the random guess baseline.",
                "position": 348
            },
            {
                "img": "https://arxiv.org/html/2505.08455/extracted/6434868/figures/silver.png",
                "caption": "",
                "position": 570
            },
            {
                "img": "https://arxiv.org/html/2505.08455/extracted/6434868/figures/gold.png",
                "caption": "",
                "position": 579
            },
            {
                "img": "https://arxiv.org/html/2505.08455/x9.png",
                "caption": "",
                "position": 618
            },
            {
                "img": "https://arxiv.org/html/2505.08455/x10.png",
                "caption": "Figure 8:Failure examples.We observe that Gemini-2.0-Flash-Thinking generates a response with a detailed rationale explaining how it arrives at the final answer, unlike Gemini-1.5-Pro and Qwen2.5-VL-Instruct, which directly provide the final answer. Based on its detailed response, Gemini-2.0-Flash-Thinking correctly interprets most actions except for Clip 2 (highlighted inred). However, it entirely fails to arrange the identified events according to their causal dependencies. Additionally, both Gemini-1.5-Pro and Qwen2.5-VL-Instruct72B72B{}_{\\text{72B}}start_FLOATSUBSCRIPT 72B end_FLOATSUBSCRIPTmake the same mistake: they fail to recognize the causal link between steps4444and5555, i.e.,the shelves must be sanded before being painted.\nThe correct order is2,3,5,4,1235412,3,5,4,12 , 3 , 5 , 4 , 1.",
                "position": 636
            }
        ]
    },
    {
        "header": "5A Simple Step Towards Improving Video-based Causal Reasoning",
        "images": []
    },
    {
        "header": "6Discussions",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Results onVCRBench",
        "images": []
    },
    {
        "header": "Appendix BAdditional Details ofVCRBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.08455/extracted/6434868/figures/human_eval_setup.png",
                "caption": "Figure S1:User interface for human evaluation. Questions are randomly shuffled to avoid any potential bias.",
                "position": 2559
            }
        ]
    },
    {
        "header": "Appendix CAdditional Details of RRD",
        "images": []
    },
    {
        "header": "Appendix DBroader Impact",
        "images": []
    }
]