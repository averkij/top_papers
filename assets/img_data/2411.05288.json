[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.05288/x1.png",
                "caption": "Figure 1:Repeating pattern in an imbalanced pipeline. Bubbles are incurred due to an extra output layer in the last pipeline stage.",
                "position": 117
            },
            {
                "img": "https://arxiv.org/html/2411.05288/x2.png",
                "caption": "Figure 2:Ratio of compute and memory of vocabulary layers compared to transformer layers in Gemma2-9B.",
                "position": 126
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.05288/x3.png",
                "caption": "Figure 3:Transformer Layer Redistribution for a 7B GPT-like model with vocabulary size 128k. In this case, each stage has 2 transformer layers, while output layer is equivalent to 2.4x of transformer layer on compute and 2.6x on parameter memory.",
                "position": 144
            },
            {
                "img": "https://arxiv.org/html/2411.05288/x4.png",
                "caption": "",
                "position": 148
            }
        ]
    },
    {
        "header": "3Vocabulary Parallelism in Pipeline Parallelism",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.05288/x5.png",
                "caption": "Figure 4:Computation graph of the output layer after partitioning across the vocabulary dimension. There are three all-reduce communications across all devices.",
                "position": 198
            }
        ]
    },
    {
        "header": "4Vocabulary Passes Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.05288/x6.png",
                "caption": "Figure 5:Overlapping all-reduce communication with transformer layer computation.",
                "position": 220
            },
            {
                "img": "https://arxiv.org/html/2411.05288/x7.png",
                "caption": "Figure 6:Scheduling dependencies in the na√Øve output layer implementation.",
                "position": 226
            },
            {
                "img": "https://arxiv.org/html/2411.05288/x8.png",
                "caption": "Figure 7:Computation order in the output layer for a single microbatch, corresponding to the na√Øve implementation, Algorithm1and Algorithm2respectively.",
                "position": 428
            },
            {
                "img": "https://arxiv.org/html/2411.05288/x9.png",
                "caption": "",
                "position": 435
            },
            {
                "img": "https://arxiv.org/html/2411.05288/x10.png",
                "caption": "",
                "position": 438
            }
        ]
    },
    {
        "header": "5Pipeline Scheduling",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.05288/x11.png",
                "caption": "Figure 8:Scheduling Dependencies in Algorithms1and2.",
                "position": 482
            },
            {
                "img": "https://arxiv.org/html/2411.05288/x12.png",
                "caption": "",
                "position": 486
            },
            {
                "img": "https://arxiv.org/html/2411.05288/x13.png",
                "caption": "Figure 9:Modified building blocks for the 1F1B schedule corresponding to Algorithm1and Algorithm2. The output layer passes are inserted.",
                "position": 506
            },
            {
                "img": "https://arxiv.org/html/2411.05288/x14.png",
                "caption": "",
                "position": 510
            },
            {
                "img": "https://arxiv.org/html/2411.05288/x15.png",
                "caption": "Figure 10:Full 1F1B schedules with Vocabulary Parallelism, corresponding to (a) Algorithm1and (b) Algorithm2respectively. Algorithm1requires activation memory forp+2ùëù2p+2italic_p + 2microbatches while Algorithm2only requiresp+1ùëù1p+1italic_p + 1, wherepùëùpitalic_pis the number of devices.",
                "position": 514
            },
            {
                "img": "https://arxiv.org/html/2411.05288/x16.png",
                "caption": "",
                "position": 518
            }
        ]
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.05288/x17.png",
                "caption": "Figure 11:Throughput of different methods on 1F1B. Interlaced OOMs when training with 32 GPUs and sequence length 4096.",
                "position": 662
            },
            {
                "img": "https://arxiv.org/html/2411.05288/x18.png",
                "caption": "",
                "position": 666
            },
            {
                "img": "https://arxiv.org/html/2411.05288/x19.png",
                "caption": "",
                "position": 668
            },
            {
                "img": "https://arxiv.org/html/2411.05288/x20.png",
                "caption": "Figure 12:Peak memory of different methods on 1F1B",
                "position": 672
            },
            {
                "img": "https://arxiv.org/html/2411.05288/x21.png",
                "caption": "",
                "position": 676
            },
            {
                "img": "https://arxiv.org/html/2411.05288/x22.png",
                "caption": "",
                "position": 678
            },
            {
                "img": "https://arxiv.org/html/2411.05288/x23.png",
                "caption": "Figure 13:Throughput of different methods onV-Half. Baseline OOMs when training with 32 GPUs and vocabulary size 256k.",
                "position": 682
            },
            {
                "img": "https://arxiv.org/html/2411.05288/x24.png",
                "caption": "",
                "position": 686
            },
            {
                "img": "https://arxiv.org/html/2411.05288/x25.png",
                "caption": "",
                "position": 688
            },
            {
                "img": "https://arxiv.org/html/2411.05288/x26.png",
                "caption": "Figure 14:Peak memory of different methods onV-Half. The shaded area denotes the range of maximum allocated memory for all devices.",
                "position": 692
            },
            {
                "img": "https://arxiv.org/html/2411.05288/x27.png",
                "caption": "",
                "position": 696
            },
            {
                "img": "https://arxiv.org/html/2411.05288/x28.png",
                "caption": "",
                "position": 698
            }
        ]
    },
    {
        "header": "7Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AQuantitative Analysis of Vocabulary Layers",
        "images": []
    },
    {
        "header": "Appendix BMore Analysis of Interlaced Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.05288/x29.png",
                "caption": "(a)Building block of 1F1B",
                "position": 1244
            },
            {
                "img": "https://arxiv.org/html/2411.05288/x29.png",
                "caption": "(a)Building block of 1F1B",
                "position": 1247
            },
            {
                "img": "https://arxiv.org/html/2411.05288/x30.png",
                "caption": "(b)Building block of interlaced pipeline parallel. The red vertical lines indicate synchronization introduced by TP of vocabulary layers.",
                "position": 1253
            }
        ]
    },
    {
        "header": "Appendix CVocabulary Parallelism for The Input Layer",
        "images": []
    },
    {
        "header": "Appendix DV-HalfPipeline Scheduling",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.05288/x31.png",
                "caption": "Figure 16:Modified building block for theV-Halfschedule.",
                "position": 1311
            }
        ]
    },
    {
        "header": "Appendix EDetailed Experiment Data",
        "images": []
    }
]