[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.09568/x1.png",
                "caption": "",
                "position": 141
            },
            {
                "img": "https://arxiv.org/html/2505.09568/x2.png",
                "caption": "",
                "position": 155
            },
            {
                "img": "https://arxiv.org/html/2505.09568/x3.png",
                "caption": "",
                "position": 160
            },
            {
                "img": "https://arxiv.org/html/2505.09568/x4.png",
                "caption": "Figure 1:The architecture of BLIP3-o. For image understanding part, we use CLIP to encode the image and compute the cross entropy loss between the target text token and predicted text token. For image generation part, autoregressive model first generates a sequence of intermediate visual\nfeatures, which are then used as conditioning inputs to a diffusion transformer that\ngenerates CLIP image features to approximate the ground-truth CLIP features. By using CLIP encoder, image understanding and image generation share the same semantic space, effectively unifying these two tasks.",
                "position": 206
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.09568/x5.png",
                "caption": "Figure 2:Visualization results of BLIP3-o 8B at 1024×1024 resolution.",
                "position": 229
            }
        ]
    },
    {
        "header": "2Unified Multimodal for Image Generation and Understanding",
        "images": []
    },
    {
        "header": "3Image Generation in Unified Multimodal",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.09568/x6.png",
                "caption": "Figure 3:Three design choices for image generation in unified multimodal model. All designs use aAutoregressive + Diffusionframework but vary in their image generation components. For the flow matching loss, we keep the autoregressive model frozen and only fine-tune the image generation module to preserve the model’s language capabilities.",
                "position": 403
            },
            {
                "img": "https://arxiv.org/html/2505.09568/x7.png",
                "caption": "Figure 4:Comparison of different design choices.",
                "position": 450
            }
        ]
    },
    {
        "header": "4Training Strategies for Unified Multimodal",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.09568/x8.png",
                "caption": "Figure 5:Joint Training vs. Sequential Training: Joint training performs multitask learning by mixing image-understanding and image-generation data, updating both the autoregressive backbone and the generation module simultaneously. Sequential training separates the process: first, the model is trained only on image-understanding tasks; then the autoregressive backbone is frozen and only the image-generation module is trained in a second stage.",
                "position": 484
            }
        ]
    },
    {
        "header": "5BLIP3-o: Our State-of-the-Art Unified Multimodal",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.09568/x9.png",
                "caption": "Table 2:Image generation benchmark results.",
                "position": 801
            },
            {
                "img": "https://arxiv.org/html/2505.09568/x9.png",
                "caption": "Figure 6:Human study results for DPG-Bench between Janus Pro and our model.",
                "position": 885
            }
        ]
    },
    {
        "header": "6Future Work",
        "images": []
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Appendix APrompt used in Figure2",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]