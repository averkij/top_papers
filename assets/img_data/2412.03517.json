[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03517/x1.png",
                "caption": "",
                "position": 97
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03517/x2.png",
                "caption": "Figure 2:Framework illustration of NVComposer. It contains animage-pose dual-stream diffusion modelthat generates novel views while implicitly estimating camera poses for conditional images, and ageometry-aware feature alignment adapterthat uses geometric priors distilled from pretrained dense stereo models[34].",
                "position": 107
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03517/x3.png",
                "caption": "Figure 3:Structure of the geometry-aware feature alignment adapter in NVComposer, which aligns the internal features of the dual-stream diffusion models with the 3D point maps produces by DUSt3R[34]during training. Block with notation “×2absent2\\times 2× 2”, “×4absent4\\times 4× 4”, and “×8absent8\\times 8× 8” refer to bilinear upsampling on spatial dimensions. The four red bars refer to the channel-wise MLPs.",
                "position": 234
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03517/x4.png",
                "caption": "Figure 4:Visual comparison of NVS results on the RealEstate10K[47]and DL3DV[18]test sets. MotionCtrl[36]and CameraCtrl[9]uses the first view as input while other methods use two views as input. MotionCtrl and CameraCtrl produce incorrect camera trajectories. DUSt3R and ViewCrafter exhibit better camera control but introduce artifacts due to occlusions or misaligned multi-view inputs. Our model generates views that are visually closer to the reference. We provide zoomed-in details of the first three scenes in white boxes for a closer look. Additional visual comparisons can be found in the supplementary material.",
                "position": 703
            },
            {
                "img": "https://arxiv.org/html/2412.03517/x5.png",
                "caption": "Figure 5:Visual comparison of novel view generation results on the Objaverse[4]test set. All input views are unposed and randomly rendered from the same 3D object.",
                "position": 846
            },
            {
                "img": "https://arxiv.org/html/2412.03517/x6.png",
                "caption": "Figure 6:A visual sample in the ablation results of the geometry-aware feature alignment with two input views given. Some patches are zoomed in for a better view. The feature alignment helps NVComposer to properly utilize contents from other views.",
                "position": 937
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]