[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16259/sections/hf_logo.png",
                "caption": "",
                "position": 128
            },
            {
                "img": "https://arxiv.org/html/2510.16259/diagram.png",
                "caption": "Figure 1:An overview of the reasoning distraction setting, where we examine whether LRMs can be diverted from their primary objective by irrelevant or maliciously embedded tasks in the prompt.",
                "position": 131
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16259/x1.png",
                "caption": "Figure 2:An illustration of a reasoning distraction attack in which a malicious distractor task manipulates the reasoning of an LRM-as-a-judge, compelling it to select an attacker-specified response.",
                "position": 140
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Evaluating LRM Susceptibility to Reasoning Distraction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16259/Claude2.png",
                "caption": "Table 2:Model performance on downstream tasks under various reasoning distraction attacks. We report the accuracy (ACC) and the number of reasoning and answer tokens.✗indicates the ACC is less than 1%.",
                "position": 525
            },
            {
                "img": "https://arxiv.org/html/2510.16259/qwen.png",
                "caption": "",
                "position": 824
            },
            {
                "img": "https://arxiv.org/html/2510.16259/microsoft.png",
                "caption": "",
                "position": 1006
            }
        ]
    },
    {
        "header": "4Mitigating Reasoning Distraction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16259/x2.png",
                "caption": "Figure 3:A case study demonstrating how an LRM trained on our dataset learns to ignore malicious instructions while remaining focused on its primary task.",
                "position": 1194
            },
            {
                "img": "https://arxiv.org/html/2510.16259/deepseek0.png",
                "caption": "Table 3:Efficacy of our training-based method in mitigating reasoning distraction. We show the ACC of three fine-tuning methods. Values in parentheses show the change from the original base model’s ACC.",
                "position": 1206
            }
        ]
    },
    {
        "header": "5Analysis",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALLM Usage",
        "images": []
    },
    {
        "header": "Appendix BDistractor Prompts",
        "images": []
    },
    {
        "header": "Appendix CDistractor Experimental Setup Details",
        "images": []
    },
    {
        "header": "Appendix DDistraction Rate",
        "images": []
    },
    {
        "header": "Appendix EData Generation Process",
        "images": []
    },
    {
        "header": "Appendix FDistractor Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16259/distractor_position_impact_horizontal.png",
                "caption": "Figure 4:Accuracy drop by distractor position.",
                "position": 3099
            }
        ]
    },
    {
        "header": "Appendix GComplete Experiment Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16259/deepseek0.png",
                "caption": "Table 7:Detailed efficacy of fine-tuning strategies in mitigating reasoning distraction, broken down by distractor type. For each model, we show the performance of three fine-tuning methods against AIME, Code, and Logic distractors. Values in parentheses would show the change from the original base model’s accuracy on that specific distractor task. The distraction rate is formatted as Reasoning Distraction % / Answer Distraction %.",
                "position": 3120
            },
            {
                "img": "https://arxiv.org/html/2510.16259/Claude2.png",
                "caption": "Table 8:Model performance on downstream tasks under various reasoning distraction attacks. For each task, we report the Distraction Rate, formatted as Reasoning Distraction % / Answer Distraction %. A higher distraction rate indicates lower robustness.",
                "position": 3836
            },
            {
                "img": "https://arxiv.org/html/2510.16259/deepseek0.png",
                "caption": "Table 9:Detailed efficacy of fine-tuning strategies in mitigating reasoning distraction, broken down by distractor type. For each model, we show the performance of three fine-tuning methods against AIME, Code, and Logic distractors. The distraction rate is reported as Reasoning Distraction % (Dist. R) and Answer Distraction % (Dist. A).",
                "position": 4410
            }
        ]
    },
    {
        "header": "Appendix HHuman and LLM annotation",
        "images": []
    }
]