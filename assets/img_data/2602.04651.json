[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Method:",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04651/Asymmetric.png",
                "caption": "Figure 1:Training dynamics comparing asymmetric KL control ( blue) versus PPO (red) over 2000 steps. Asymmetric control achieves stronger KL regulation but exhibits higher value loss volatility, indicating the need for additional stabilization (Section3.4).",
                "position": 513
            }
        ]
    },
    {
        "header": "4Components",
        "images": []
    },
    {
        "header": "5Training Algorithm",
        "images": []
    },
    {
        "header": "6Objective Function",
        "images": []
    },
    {
        "header": "7Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04651/training_plots_mainD.png",
                "caption": "Figure 2:Training dynamics of SAFE.\nTop row: reward trajectory, KL divergence with adaptive threshold, and value loss.\nBottom row: policy entropy with entropy floor, completion length, and smoothed reward.\nThe controller maintains entropy above the configured floor while dynamically regulating KL magnitude.",
                "position": 1701
            },
            {
                "img": "https://arxiv.org/html/2602.04651/tts.png",
                "caption": "Figure 3:Training dynamics and stability comparison between SAFE and PPO.Top row:Reward trajectory with confidence intervals, KL divergence with adaptive thresholds, and value function loss.Middle row:Reward distribution, KL distribution, and completion length evolution.Bottom row:Reward–KL trade-off scatter, cumulative reward accumulation, rolling reward stability, reward box plots, KL box plots, and batch-level reward variance.\nSAFE exhibits tighter reward confidence bands, improved reward stability, controlled KL excursions, and smoother cumulative reward growth compared to PPO, indicating improved robustness under long-horizon RLHF optimization.",
                "position": 1814
            },
            {
                "img": "https://arxiv.org/html/2602.04651/hardware_benchmark.jpeg",
                "caption": "Figure 4:GPU memory and runtime comparison between SAFE and PPO.Left:GPU peak memory usage per training step over 2,000 iterations. Both\nmethods exhibit similar memory profiles with comparable peak allocations (∼\\sim54 GB).Center:Wall-clock time per training step. SAFE maintains slightly faster\nstep times with reduced variance, indicating that the additional control logic does not\nintroduce computational bottlenecks.Right:Summary comparison of initialization\nmemory, peak memory, and average step time. SAFE achieves near-identical resource usage\nwith−0.9%-0.9\\%memory overhead and−1.4%-1.4\\%time overhead, demonstrating that the multi-layer\nstabilization framework is computationally efficient.",
                "position": 2051
            }
        ]
    },
    {
        "header": "8Experimental Analysis",
        "images": []
    },
    {
        "header": "9Conclusion",
        "images": []
    },
    {
        "header": "10Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATheoretical Analysis: Hypothesized Reward Hacking Resistance",
        "images": []
    }
]