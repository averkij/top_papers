[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09007/x1.png",
                "caption": "Figure 1:Comparison of evaluation paradigms across different benchmark types.Existing image generation benchmarks prioritize general-domain visual fidelity and video generation benchmarks evaluate continuous state transitions. GEBench uniquely evaluates discrete state transitions induced by user actions, capturing the essence of GUI interactions.",
                "position": 189
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09007/x2.png",
                "caption": "Figure 2:Examples of the five task types in GEBench, which are designed to comprehensively evaluate the capabilities of image generation models as GUI environments. GEBench provides image generation models with user instructions and reference GUI state (no reference provided for the Fiction App task) and evaluates the generated GUIs.",
                "position": 250
            }
        ]
    },
    {
        "header": "3GEBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09007/x3.png",
                "caption": "Figure 3:GEBench data construction pipeline. The process involves raw data capture through recording user interactions, task annotation of actions, quality control via preprocessing and verification, and data construction across five task categories: Single-Step, Multi-Step, Grounding, Real App, and Fictional App, totaling 700 samples.",
                "position": 307
            }
        ]
    },
    {
        "header": "4Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09007/figures/radar.png",
                "caption": "Figure 4:Performance of models across GEBench task suites.The radar chart illustrates the performance of 12 prominent image generation models, including commercial models (solid line) and open-sourced models (dashed line). The reported results represent the average scores on Chinese and English subsets.",
                "position": 638
            },
            {
                "img": "https://arxiv.org/html/2602.09007/figures/type5_score.png",
                "caption": "Figure 5:Comparison of GOAL score on grounding task.The universally low scores across all models highlight a critical deficiency in current generative modelsâ€™ ability to perceive and align with precise spatial grounding points.",
                "position": 705
            },
            {
                "img": "https://arxiv.org/html/2602.09007/figures/VLM-as-a-judge.png",
                "caption": "Figure 6:Pearson correlation analysis between human expert scores and VLM-based evaluation. Results for Nano Banana Proteam2023geminiand GPT-Image-1openai2025gptimagedemonstrate a strong alignment between the VLM-as-a-Judge framework and human judgment across different models.",
                "position": 708
            },
            {
                "img": "https://arxiv.org/html/2602.09007/x4.png",
                "caption": "Figure 7:Qualitative results of the three primary weaknesses identified in image generation models acting as GUI environments. The comparison highlights significant deficiencies in text rendering accuracy, icon interpretation for state transitions, and localization precision regarding coordinate-based grounding.",
                "position": 726
            }
        ]
    },
    {
        "header": "5Discussion and Analysis",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AEvaluation Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09007/x5.png",
                "caption": "Figure A1:GEBench Evaluation Framework Overview. This diagram outlines the comprehensive evaluation process of the GEBench framework, which assesses the performance of image generation models in generating GUI sequences.",
                "position": 785
            }
        ]
    },
    {
        "header": "Appendix BDetailed Performance On GEBench Using Different Judges",
        "images": []
    },
    {
        "header": "Appendix CDetailed Rubric on five tasks",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09007/x6.png",
                "caption": "Figure A2:Evaluation Rubrics for Single-Step Transition Generation",
                "position": 2650
            },
            {
                "img": "https://arxiv.org/html/2602.09007/x7.png",
                "caption": "Figure A3:Evaluation Rubrics for Multi-Step Planning Generation",
                "position": 2654
            },
            {
                "img": "https://arxiv.org/html/2602.09007/x8.png",
                "caption": "Figure A4:Evaluation Rubrics for Zero-shot Virtual GUI Generation and Rare Trajectory Synthesis",
                "position": 2658
            },
            {
                "img": "https://arxiv.org/html/2602.09007/x9.png",
                "caption": "Figure A5:Evaluation Rubrics for Grounding-based Generation",
                "position": 2662
            }
        ]
    },
    {
        "header": "Appendix DDetailed Rubric on five tasks",
        "images": []
    }
]