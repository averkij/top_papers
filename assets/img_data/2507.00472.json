[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.00472/extracted/6585428/figures/intro9.png",
                "caption": "Figure 1:Development of interactive head motion generation. a) Only speaker generation based on last speaking behaviors. b) Different explicit generators for listener/speaker. c) Implicit clip-by-clip generator based on a unified model. d) Implicit frame-by-frame generator based on a unified non-quantized AR model.",
                "position": 168
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.00472/extracted/6585428/figures/pip7.png",
                "caption": "Figure 2:Overall framework of ARIG. Given the previous frame‚Äôs audio and motionAT‚àí1usuperscriptsubscriptùê¥ùëá1ùë¢A_{T\\!-\\!1}^{u}italic_A start_POSTSUBSCRIPT italic_T - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT,MT‚àí1asuperscriptsubscriptùëÄùëá1ùëéM_{T\\!-\\!1}^{a}italic_M start_POSTSUBSCRIPT italic_T - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT,MT‚àí1usuperscriptsubscriptùëÄùëá1ùë¢M_{T\\!-\\!1}^{u}italic_M start_POSTSUBSCRIPT italic_T - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPTalong with the current audioATasuperscriptsubscriptùê¥ùëáùëéA_{T}^{a}italic_A start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT, IBU first effectively uses dual-track dual-modal signals to perform bidirectional-integrated learning over short ranges and contextual understanding over long ranges, obtaining a contextual interaction summary (cis-token). Then, CSU combines voice activity and cis-token to predict state features. Finally, PMP progressively predicts motions and uses the DiffusionMLP to sample the final motion.",
                "position": 264
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.00472/extracted/6585428/figures/state6.png",
                "caption": "Figure 3:Illustration of agent‚Äôs conversational state. In addition to regular speaking/listening, it also includes many complex states, which are classified according to the voice activity detection.",
                "position": 412
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.00472/x1.png",
                "caption": "Figure 4:Visual results of our method on RealTalk[7]dataset. The two shown scenarios involves regular speaking/listening, and complex states such as interruptions, feedback, and pauses for thinking.\nOur method can generate highly realistic and natural agent videos in complex conversational scenarios.",
                "position": 448
            },
            {
                "img": "https://arxiv.org/html/2507.00472/x2.png",
                "caption": "Figure 5:Qualitative comparisons with DIM[27]and INFP[39]. The two sample videos are from the DyConv dataset proposed by INFP[39], which is not open-sourced, thus lacking ground truth.",
                "position": 613
            },
            {
                "img": "https://arxiv.org/html/2507.00472/x3.png",
                "caption": "Figure 6:Ablation study. (a) Ablation of long-range context. (b) Ablation of conversational state. (c) Ablation of visual modality in the IBU module.",
                "position": 807
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.00472/extracted/6585428/supp_sec/app_dual.png",
                "caption": "Figure 7:The structure of the Bidirectional-learning.",
                "position": 1394
            },
            {
                "img": "https://arxiv.org/html/2507.00472/extracted/6585428/supp_sec/single.png",
                "caption": "Figure 8:The structure of the Integrated-learning.",
                "position": 1397
            },
            {
                "img": "https://arxiv.org/html/2507.00472/extracted/6585428/supp_sec/diffusionmlp.png",
                "caption": "Figure 9:The structure of the DiffusionMLP.",
                "position": 1400
            },
            {
                "img": "https://arxiv.org/html/2507.00472/x4.png",
                "caption": "Figure 10:Ablation Study.",
                "position": 1403
            },
            {
                "img": "https://arxiv.org/html/2507.00472/x5.png",
                "caption": "Figure 11:Qualitative comparisons with DIM[27]on RealTalk[7]dataset.",
                "position": 1520
            }
        ]
    },
    {
        "header": "BMore Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.00472/x6.png",
                "caption": "Figure 12:Qualitative comparisons with state-of-the-art talking head generation methods on HDTF[34]dataset.",
                "position": 1550
            },
            {
                "img": "https://arxiv.org/html/2507.00472/x7.png",
                "caption": "Figure 13:Qualitative comparisons with state-of-the-art listenning head generation methods on ViCo[36]dataset.",
                "position": 1553
            }
        ]
    },
    {
        "header": "CSupplementary Visual Results",
        "images": []
    },
    {
        "header": "DLimitations and Social Impact",
        "images": []
    }
]