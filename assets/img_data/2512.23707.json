[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23707/x1.png",
                "caption": "Figure 1:Summary of methodology.(Bottom)We train models to generate research plans for a given research goal. We obtain rewards for RL by using the initial model to grade generated plans with the help of rubrics.(Left)To collect training data, we use asample creatormodel (Llama-4-Maverick) to extract up to three samples per research paper, each including a research goal, goal-specific grading rubric, and reference solution. For each of these components, we provide guidelines to asample selectormodel (Claude-4-Sonnet) that picks one best sample per paper for our use.(Right)During grading, the goal-specific rubrics are used alongside a list of seven general guidelines that are checked for the part of the plan relevant to each rubric item. Rubric items that meet all guidelines are marked satisfied, and the fraction of satisfied rubric items is used as part of the training reward and evaluation scores.",
                "position": 310
            }
        ]
    },
    {
        "header": "3Experiment Setup",
        "images": []
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23707/figures/preferences/trained_preferred_percentage.png",
                "caption": "Table 1:Results from human expert annotations. Preference plots (right) show win-rates of research plans generated by the ML finetuned model over the initial Qwen-3-30B model across criteria, for a subset of 100 test set research goals inResearchPlanGenML. The shaded grey region shows the 95% confidence interval based on bootstrap sampling.",
                "position": 569
            },
            {
                "img": "https://arxiv.org/html/2512.23707/figures/preferences/addresses_requirements.png",
                "caption": "",
                "position": 612
            },
            {
                "img": "https://arxiv.org/html/2512.23707/figures/preferences/soundness.png",
                "caption": "",
                "position": 625
            },
            {
                "img": "https://arxiv.org/html/2512.23707/figures/preferences/clear_execution.png",
                "caption": "",
                "position": 639
            },
            {
                "img": "https://arxiv.org/html/2512.23707/figures/preferences/feasibility.png",
                "caption": "",
                "position": 652
            },
            {
                "img": "https://arxiv.org/html/2512.23707/figures/preferences/predicted_outcomes.png",
                "caption": "",
                "position": 666
            },
            {
                "img": "https://arxiv.org/html/2512.23707/figures/final/main-benchmarks.png",
                "caption": "Figure 3:Research plan generation scores of models across research goals extracted from ML, medical, and arXiv papers.We report average scores on the full test sets across rubric grading by three frontier models, and use bootstrap sampling for error bars. In (a)-(c), we observe that our domain finetuned models always improve over the initial policy (Qwen-3-30B-A3B-Instruct). We also find thatGPT modelsconsistently outperform the rest, while within model families, more recent and larger models perform better, as expected. In (d), we observe that our finetuning also leads to significant cross-domain generalization. For example, themedical finetuneimproves significantly on ML and arXiv research goals.",
                "position": 691
            },
            {
                "img": "https://arxiv.org/html/2512.23707/figures/draft1/rewardmodeloveroptimization.png",
                "caption": "Table 2:Training ablations on a smaller Qwen-3-4B policy, leading up to our final methodology.Results are based on Claude-4-Sonnet rubric judgements on the validation set. We see that SFT worsens performance. For rubric RL, we see large improvements from disabling the KL penalty, and providing both the goal-specific rubricsandgeneral grading guidelines to the reward model (RM). We also see improvements from providing the grader a reference solution, and shifting from a 4B RM to a stronger 30B RM. In contrast, we see only a minor improvement from our filtering of training data.",
                "position": 703
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Limitations and Future Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Additional details",
        "images": []
    },
    {
        "header": "9Additional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23707/figures/draft1/main_ft_bar.png",
                "caption": "Figure 5:Finetuning improvements based on rubric grading across a jury of three judges (GPT-5-Thinking, Claude-4-Sonnet, and Gemini-2.5-Pro).",
                "position": 1266
            },
            {
                "img": "https://arxiv.org/html/2512.23707/figures/draft1/Training_Instruct_vs_Thinking.png",
                "caption": "Figure 6:Training both Qwen-3-4B instruct and thinking model with Qwen-3-30B MoE Reward modelÎ¸r\\theta_{r}. Results on validation set. The grader used for scoring is Claude-4-Sonnet. We see very similar performance between instruct and thinking, even though thinking requires more than 2x compute for the same number of training steps.",
                "position": 1355
            },
            {
                "img": "https://arxiv.org/html/2512.23707/figures/draft1/Effect_of_Verifier_Capability.png",
                "caption": "Figure 7:Effect of reward model capability. We show the benefit of a stronger 30B MoE RM (thick lines) vs a smaller 4B RM (dashed lines).",
                "position": 1366
            },
            {
                "img": "https://arxiv.org/html/2512.23707/figures/draft1/Effect_of_Verifier_Capability.png",
                "caption": "Figure 7:Effect of reward model capability. We show the benefit of a stronger 30B MoE RM (thick lines) vs a smaller 4B RM (dashed lines).",
                "position": 1368
            },
            {
                "img": "https://arxiv.org/html/2512.23707/figures/draft1/Effect_of_KL_Penalty.png",
                "caption": "Figure 8:Effect of Disabling KL. Disabling the KL penalty in GRPO leads to improved results after some training.",
                "position": 1372
            },
            {
                "img": "https://arxiv.org/html/2512.23707/figures/draft1/Effect_of_Specific_Rubrics.png",
                "caption": "Figure 9:Effect of Specific Rubrics. Providing specific rubrics leads to better scores across guidelines.",
                "position": 1376
            },
            {
                "img": "https://arxiv.org/html/2512.23707/figures/draft1/Effect_of_Generic_Rubrics.png",
                "caption": "Figure 10:Effect of Generic Rubrics. Providing generic rubrics leads to better scores across guidelines.",
                "position": 1380
            },
            {
                "img": "https://arxiv.org/html/2512.23707/figures/draft1/Effect_of_Best_of_3_Filtering.png",
                "caption": "Figure 11:Effect of Best of 3 Filtering. No significant improvement seen from training on the filtered set.",
                "position": 1384
            },
            {
                "img": "https://arxiv.org/html/2512.23707/figures/draft1/Effect_of_Reference_Solution.png",
                "caption": "Figure 12:Effect of Reference Solution. Providing the reference solution leads to better scores across guidelines.",
                "position": 1388
            },
            {
                "img": "https://arxiv.org/html/2512.23707/figures/draft1/ml_by_Desiderata_Scores.png",
                "caption": "Figure 13:ML by guidelines scores",
                "position": 2100
            },
            {
                "img": "https://arxiv.org/html/2512.23707/figures/draft1/ml_by_Desiderata_Scores.png",
                "caption": "Figure 13:ML by guidelines scores",
                "position": 2102
            },
            {
                "img": "https://arxiv.org/html/2512.23707/figures/draft1/arxiv_by_Desiderata_Scores.png",
                "caption": "Figure 14:ArXiv by guidelines scores",
                "position": 2106
            },
            {
                "img": "https://arxiv.org/html/2512.23707/figures/draft1/pubmed_by_Desiderata_Scores.png",
                "caption": "Figure 15:PubMed by guidelines scores",
                "position": 2110
            }
        ]
    },
    {
        "header": "10Annotation Guidelines: Evaluating AI Generated Research Plans",
        "images": []
    },
    {
        "header": "11Prompts",
        "images": []
    },
    {
        "header": "12Qualitative Examples",
        "images": []
    }
]