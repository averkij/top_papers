[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21263/extracted/6573568/dilocox-framework-part1.drawio.png",
                "caption": "Figure 1:Pipeline Parallelism with Dual Optimizer. The example infrastructure comprises 32 workers distributed among 2 decentralized clusters, with 16 workers in each cluster. Each worker maintains distributed inner and outer optimizers‚Äô state and a fraction of model parameters. Two clusters are trained independently forHùêªHitalic_Hsteps respectively. The parallel strategy is PP = 8, DP = 2 for each cluster, and the parameters are updated by using their respective inner optimizers. Finally, the outer distributed optimizer performs a step operation to update local parameters.",
                "position": 210
            },
            {
                "img": "https://arxiv.org/html/2506.21263/extracted/6573568/one-step-delay-1.png",
                "caption": "Figure 2:One-Step-Delay Overlap of Communication and Local Training.",
                "position": 249
            }
        ]
    },
    {
        "header": "3Theoretical Analysis",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21263/extracted/6573568/1.3b.png",
                "caption": "(a)Loss of AllReduce, DiLoCoX, OpenDiLoCo and CocktailSGD on OPT-1.3B model training",
                "position": 679
            },
            {
                "img": "https://arxiv.org/html/2506.21263/extracted/6573568/1.3b.png",
                "caption": "(a)Loss of AllReduce, DiLoCoX, OpenDiLoCo and CocktailSGD on OPT-1.3B model training",
                "position": 682
            },
            {
                "img": "https://arxiv.org/html/2506.21263/extracted/6573568/loss-107b.png",
                "caption": "(b)Loss of AllReduce, DiLoCoX and CocktailSGD on Qwen1.5-107B model training",
                "position": 688
            },
            {
                "img": "https://arxiv.org/html/2506.21263/extracted/6573568/throughput.png",
                "caption": "Figure 4:Throughput Comparison between AllReduce, OpenDiLoCo, CocktailSGD and DiLoCoX.",
                "position": 712
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATheoretical Analysis of Convergence and Extensive",
        "images": []
    }
]