[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12303/extracted/6288884/sec/figures/run_ex_cap_new.drawio.png",
                "caption": "Figure 1:(a) TheSIcogframework enhances an MLLM’s systematic cognition during multimodal pre-training using self-generated data, enabling the construction of a next-generation foundation MLLM. Specifically,SIcogfirst improves perception and reasoning throughChain-of-Descriptionand structured chain-of-thought with minimal external annotations. The enhanced model then generates captions and VQA responses, refined via self-consistency for multimodal pre-training to construct a foundation MLLM from scratch. (b) With up to 213K self-generated pre-training samples,SIcogproduces foundation MLLMs with superior cognitive capabilities, demonstrating benchmark-leading performance compared to prevalent pre-training approaches.",
                "position": 116
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3SIcog",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12303/extracted/6288884/sec/figures/sicog_no_iter_short_version.drawio.png",
                "caption": "Figure 2:TheSIcogframework comprises four steps:(i)i(\\textup{\\it i})( i )Developing multimodal cognitive capabilities by finetuning an MLLM with minimal annotated image-captioning data (withChain-of-Description) and visual instruction-tuning data (with structured Chain-of-Thought), enhancing systematic perception and reasoning (upper left).(ii)ii(\\textup{\\it ii})( ii )Generating candidate captions and responses for pre-training by sampling from the improved models (upper right).(iii)iii(\\textup{\\it iii})( iii )Curating self-generated pre-training data through self-consistency-guided quality evaluation, selecting the most consistent candidates for refinement (lower right).(iv)iv(\\textup{\\it iv})( iv )Constructing a next-generation foundation MLLM by performing self-refinement via multimodal pre-training with the curated data (lower left).\nFor brevity, language ability preservation is omitted; see Figure11for the complete version.",
                "position": 260
            },
            {
                "img": "https://arxiv.org/html/2503.12303/extracted/6288884/sec/figures/method_cod.drawio.png",
                "caption": "Figure 3:Illustration ofChain-of-Descriptionfor enhancing systematic perception.",
                "position": 573
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12303/x1.png",
                "caption": "Figure 4:Fine-grained evaluation of six core capabilities on LLaVA-Qwen2-7B-UHD using the MMStar benchmark (direct answer).",
                "position": 1369
            },
            {
                "img": "https://arxiv.org/html/2503.12303/extracted/6288884/sec/figures/reasoning_mllm_case_study.drawio2.png",
                "caption": "Figure 5:Qualitative comparison of responses generated by LLaVA-Qwen2-7B-UHD andSIcog-LLaVA-Qwen2-7B-UHD for the visual instruction-following task.",
                "position": 1509
            },
            {
                "img": "https://arxiv.org/html/2503.12303/x2.png",
                "caption": "Figure 6:Impact of scaling self-generated captions onSIcog-LLaVA-Qwen2-7B-UHD during multimodal pre-training, evaluated across four dimensions on ten benchmarks.",
                "position": 1615
            },
            {
                "img": "https://arxiv.org/html/2503.12303/extracted/6288884/sec/figures/iter_improve.png",
                "caption": "Figure 7:Evaluation results for next-generation foundation MLLM construction through continuous self-improvement usingSIcog. Accuracy gains are reported as absolute improvements over the base model LLaVA-Qwen2-7B-UHD, with the base model’s performance shown in parentheses.",
                "position": 1734
            }
        ]
    },
    {
        "header": "5Detailed Analysis ofChain-of-Descriptionand Chain-of-Thought",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12303/x3.png",
                "caption": "Figure 8:Distribution of caption token lengths: perception-enhanced LLaVA-Qwen2-7B-UHD (fine-tuned on curated captions using the standard detailed description format and the mixture of detailed description andChain-of-Descriptionformats)vs.LLaVA-NeXT-34B.",
                "position": 2034
            },
            {
                "img": "https://arxiv.org/html/2503.12303/extracted/6288884/sec/figures/caption_qualitative_analysis_v1.drawio.png",
                "caption": "Figure 9:Qualitative comparison of captions generated by LLaVA-Qwen2-7B-UHD and perception-enhanced LLaVA-Qwen2-7B-UHD across six key dimensions in the image captioning task.",
                "position": 2050
            },
            {
                "img": "https://arxiv.org/html/2503.12303/extracted/6288884/sec/figures/cot_v2.drawio.png",
                "caption": "Figure 10:Qualitative comparison of responses generated by LLaVA-Qwen2-7B-UHD and reasoning-enhanced LLaVA-Qwen2-7B-UHD in the visual question-answering task.",
                "position": 2120
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AComprehensive Overview ofSIcog",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12303/extracted/6288884/sec/figures/sicog_no_iter_0312_complete.png",
                "caption": "Figure 11:TheSIcogframework comprises four steps:(i)i(\\textup{\\it i})( i )Developing multimodal cognitive capabilities by finetuning an MLLM with minimal annotated image-captioning data (withChain-of-Description) and visual instruction-tuning data (with structured Chain-of-Thought), enhancing systematic perception and reasoning (upper left).(ii)ii(\\textup{\\it ii})( ii )Generating candidate captions and responses for pre-training by sampling from the improved models (upper right).(iii)iii(\\textup{\\it iii})( iii )Curating self-generated pre-training data through self-consistency-guided quality evaluation, selecting the most consistent candidates for refinement (lower right).(iv)iv(\\textup{\\it iv})( iv )Constructing a next-generation foundation MLLM by performing self-refinement via multimodal pre-training with the curated data (lower left).",
                "position": 3010
            },
            {
                "img": "https://arxiv.org/html/2503.12303/extracted/6288884/sec/figures/method_cod_cot_v2.png",
                "caption": "Figure 12:Illustration ofChain-of-Description(left) for enhancing systematic perception and structured Chain-of-Thought (right) for strengthening reasoning capabilities.",
                "position": 3751
            },
            {
                "img": "https://arxiv.org/html/2503.12303/extracted/6288884/sec/figures/corruption_types.drawio.png",
                "caption": "Figure 13:Illustration of (a) the original image and (b) the four types of image corruption.",
                "position": 4420
            },
            {
                "img": "https://arxiv.org/html/2503.12303/extracted/6288884/sec/figures/cod.png",
                "caption": "Table 15:An example from the image captioning training dataset curated to elicit systematic multimodal perception.",
                "position": 4841
            },
            {
                "img": "https://arxiv.org/html/2503.12303/extracted/6288884/sec/figures/llava_cot_math.png",
                "caption": "Table 16:An example from the visual instruction-following training dataset designed to enhance systematic multimodal reasoning.",
                "position": 4995
            },
            {
                "img": "https://arxiv.org/html/2503.12303/extracted/6288884/sec/figures/llava_cot_sport.jpg",
                "caption": "Table 17:An example from the visual instruction-following training dataset designed to enhance systematic multimodal reasoning.",
                "position": 5217
            }
        ]
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    }
]