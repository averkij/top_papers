[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07374/x1.png",
                "caption": "(a)Responses of the base model, with Long CoT SFT, and with Long CoT LoRA.",
                "position": 134
            },
            {
                "img": "https://arxiv.org/html/2502.07374/x1.png",
                "caption": "(a)Responses of the base model, with Long CoT SFT, and with Long CoT LoRA.",
                "position": 137
            },
            {
                "img": "https://arxiv.org/html/2502.07374/x2.png",
                "caption": "(b)Performance of different models on five difference reasoning benchmarks.",
                "position": 143
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Simple distillation is effective",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07374/x3.png",
                "caption": "Figure 2:Model accuracy with different data sizes, and comparison to DeepSeek R1.The teacher model is DeepSeek R1, and the student model is Qwen-32B-Instruct trained with full parameter fine-tuning. While the student model continues to benefits from more SFT data from DeepSeek R1, a small amount of data, e.g., 16k is sufficient to significantly boost the average performance by 15.2%.",
                "position": 248
            }
        ]
    },
    {
        "header": "4Long CoT: Structure Is The Key",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07374/x4.png",
                "caption": "Figure 3:Reasoning step modifications.To evaluate perturbations to global structure across reasoning steps, we perform three modifications: deletion, insertion, and shuffling. These modifications break logical consistency across steps and degrade model accuracy far more than changes to local content within reasoning steps.",
                "position": 623
            }
        ]
    },
    {
        "header": "5Ablation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07374/x5.png",
                "caption": "Figure 4:Generalization to other models.Accuracy for models of different sizes and architectures without SFT (green) and with SFT (blue). Most models show significant improvements when fine-tuned with 17k samples from R1-Preview, showing that the Long CoT fine-tuning is beneficial across models.",
                "position": 808
            },
            {
                "img": "https://arxiv.org/html/2502.07374/x6.png",
                "caption": "Figure 5:SFT with Long CoT vs Best-of-N.Accuracy of Qwen2.5-32B-Instruct before SFT (Zero-Shot), after SFT on 17k R1 samples (Fine-tuned), and Best-of-N samples on OlympiadBench. We find that fine-tuning on Long CoT achieves performance similar to Best of 2 to 16 samples.",
                "position": 824
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AShort and Long CoT Response Examples",
        "images": []
    },
    {
        "header": "Appendix BReasoning keywords",
        "images": []
    },
    {
        "header": "Appendix CLong CoT System Prompt",
        "images": []
    },
    {
        "header": "Appendix DAverage response lengths and keyword counts",
        "images": []
    }
]