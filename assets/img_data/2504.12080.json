[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.12080/x1.png",
                "caption": "Figure 1:Overview of the proposed DC-SAM method and IC-VOS benchmark. a) Comparison of the previous few-shot segmentation methods in 1), existing methods based on SAM/SAM2 in 2), and DC-SAM in 3). DC-SAM leverages multi-source features and generates positive and negative prompts by ensuring prompt consistency, integrating with SAM/SAM2 to achieve in-context segmentation for both images and videos; b) Visualization of image and video settings by DC-SAM; c) Quantitative comparison of DC-SAM with state-of-the-art approaches in terms of mIoU on COCO-20iand PASCAL-5i,ùí•&‚Ñ±ùí•‚Ñ±\\mathcal{J\\&F}caligraphic_J & caligraphic_Fon the IC-VOS benchmark.",
                "position": 101
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.12080/x2.png",
                "caption": "Figure 2:Overview of our constructed IC-VOS benchmark. a) Distribution of video sources and their proportions. b) Word cloud of expressions. c) Categories in the dataset with the number of clips and frames for each category. d) Example cases illustrating the support image, support mask, and query video.",
                "position": 235
            }
        ]
    },
    {
        "header": "3In-Context VOS Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.12080/x3.png",
                "caption": "Figure 3:Overview of the proposed DC-SAM framework. We use positive and negative branches to generate respective prompts, thereby refining the scope of the final generated mask. Additionally, we incorporate SAM features during the prompt generation process to better capture the characteristics of SAM, resulting in more accurate prompt boundaries. During the prompt generation process, we introduce cyclic consistent cross-attention to filter out non-cycle-consistent feature points, enhancing the precision of the prompts.",
                "position": 401
            }
        ]
    },
    {
        "header": "4Proposed Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.12080/x4.png",
                "caption": "Figure 4:Illustration of our proposed cyclic consistent cross-attention mechanism. This figure shows the version applied to query features with one head. The ‚ÄúCyc‚Äù operation represents the process described in Equation19, which ultimately generates a bias to filter out features that are not cycle-consistent.",
                "position": 737
            },
            {
                "img": "https://arxiv.org/html/2504.12080/x5.png",
                "caption": "Figure 5:Comparison of SAM segmentation results with and without negative prompts. (a) Segmentation of the cage using only positive prompts. (b) Segmentation of the cage using both positive and negative prompts. Although not achieving optimal segmentation results, adding negative prompts allowed for better differentiation between the background, the dinosaur, and the cage, resulting in a significantly improved result.",
                "position": 740
            },
            {
                "img": "https://arxiv.org/html/2504.12080/x6.png",
                "caption": "Figure 6:Illustration of our proposed DC-SAM framework with SAM2. Unlike the image-level framework, we train the entire model for the video to acquire the image-to-video prompt ability. We apply different data augmentation techniques to the query image, and the augmented images compose a mask tube for training.",
                "position": 743
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.12080/x7.png",
                "caption": "Figure 7:Comparison of segmentation results from different methods on the PASCAL-5isuperscript5ùëñ5^{i}5 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT. Each row displays an RGB image along with its corresponding ground truth segmentation and the results of the four methods. Notable errors are marked with a yellow ‚Äú√ó\\times√ó‚Äù.",
                "position": 1419
            },
            {
                "img": "https://arxiv.org/html/2504.12080/x8.png",
                "caption": "Figure 8:Visual comparisons of our proposed model with PFENet and VRP-SAM on our proposed benchmark. The support mask in the video indicates the semantic category of motorcycle, and all three models share the same support image and mask.",
                "position": 1422
            },
            {
                "img": "https://arxiv.org/html/2504.12080/x9.png",
                "caption": "Figure 9:Ablation study on the number of queries. Thexùë•xitalic_x-axis represents the number of queries in one branch. Note that since our DC-SAM consists of both positive and negative branches, the total number of queries is twice the number shown on thexùë•xitalic_x-axis. Theyùë¶yitalic_y-axis represents the model‚Äôs performance. These experiments are conducted on the PASCAL-5isuperscript5ùëñ5^{i}5 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPTdataset.",
                "position": 1497
            },
            {
                "img": "https://arxiv.org/html/2504.12080/x10.png",
                "caption": "Figure 10:Visualization of two failure cases of our proposed DC-SAM on IC-VOS. We still find missing matching objects due to the occlusion (in the top) and multiple instance inputs with the fast motion (in the bottom).",
                "position": 1748
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.12080/x11.png",
                "caption": "Figure 11:Comparison of one-shot segmentation results on the PASCAL-5isuperscript5ùëñ5^{i}5 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPTdataset.",
                "position": 2594
            },
            {
                "img": "https://arxiv.org/html/2504.12080/x12.png",
                "caption": "Figure 12:Comparison of semantic segmentation results for the ‚ÄúSheep‚Äù category on the IC-VOS dataset.",
                "position": 2600
            },
            {
                "img": "https://arxiv.org/html/2504.12080/x13.png",
                "caption": "Figure 13:Comparison of semantic segmentation results for the ‚ÄúDog‚Äù category on the IC-VOS dataset.",
                "position": 2603
            },
            {
                "img": "https://arxiv.org/html/2504.12080/x14.png",
                "caption": "Figure 14:Comparison of semantic segmentation results for the ‚ÄúCup‚Äù category on the IC-VOS dataset.",
                "position": 2606
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]