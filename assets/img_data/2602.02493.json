[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02493/x1.png",
                "caption": "Figure 1:This work shows that pixel diffusion with perceptual loss outperforms latent diffusion. (a) A traditional two-stage latent diffusion denoises in the latent space, which is influenced by the artifacts of the VAE. (b) PixelGen introduces perceptual loss to encourage the diffusion model to focus on the perceptual manifold, enabling the pixel diffusion to learn a meaningful manifold rather than the complex full image manifold. (c) PixelGen outperforms the latent diffusion models using only 80 training epochs on ImageNet without CFG.",
                "position": 81
            },
            {
                "img": "https://arxiv.org/html/2602.02493/x2.png",
                "caption": "Figure 2:Illustration of different manifolds within the pixel space. The image manifold is a large manifold containing both perceptually significant information and imperceptible signals. The perceptual manifold contains perceptually important signals, providing a better target for pixel space diffusion. P-DINO and LPIPS are the two complementary perceptual supervision utilized in PixelGen.",
                "position": 105
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02493/x3.png",
                "caption": "Figure 3:Overview of PixelGen. The diffusion model directly predicts the imagexxinstead of velocityvvor noiseϵ\\epsilonto simplify the prediction target. A flow-matching diffusion loss is retained to keep the advantages of flow matching via velocity conversion. Two complementary perceptual losses are introduced to encourage the diffusion model to focus on the perceptual manifold.",
                "position": 236
            },
            {
                "img": "https://arxiv.org/html/2602.02493/x4.png",
                "caption": "Figure 4:Effectiveness of perceptual supervision in PixelGen. LPIPS and P-DINO losses are progressively added to a baseline pixel diffusion model. The LPIPS loss improves local texture fidelity, while P-DINO further enhances global semantics.",
                "position": 287
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02493/x5.png",
                "caption": "Figure 5:Qualitative results of text-to-image generation of PixelGen. All images are 512×\\times512 resolution.",
                "position": 790
            },
            {
                "img": "https://arxiv.org/html/2602.02493/x6.png",
                "caption": "Figure 6:Qualitative results of class-to-image generation of PixelGen with CFG. All images are 256×\\times256 resolution.",
                "position": 987
            }
        ]
    },
    {
        "header": "5Conclusions and Future Works",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Implementary Details",
        "images": []
    },
    {
        "header": "Appendix BText-to-Image Prompts",
        "images": []
    },
    {
        "header": "Appendix CPseudocodes for PixelGen",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02493/x7.png",
                "caption": "Figure 7:More Qualitative results of text-to-image generation at a 512×\\times512 resolution. Our PixelGen supports multiple languages with the Qwen3 text encoder, such as Chinese and English.",
                "position": 2272
            },
            {
                "img": "https://arxiv.org/html/2602.02493/x8.png",
                "caption": "Figure 8:More qualitative results of class-to-image generation at a 256×\\times256 resolution with CFG. The CFG scale is set to 3.0.",
                "position": 2275
            }
        ]
    },
    {
        "header": "Appendix DMore Visualizations",
        "images": []
    }
]