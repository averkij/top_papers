[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23019/x1.png",
                "caption": "Figure 1:Illustration of BIRA. A watermarked LLM typically increases the likelihood of sampling green tokens by adding a positive biasγ>0\\gamma>0to their logits at each generation step. In contrast, BIRA applies a negative biasβ<0\\beta<0to a proxy set of green tokens (since the true set is unknown), thereby suppressing their sampling probability. This inversion lowers the probability of generating green tokens and weakens the watermark signal, enabling the paraphrased text to evade detection.",
                "position": 200
            }
        ]
    },
    {
        "header": "4Method",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23019/x2.png",
                "caption": "Figure 2:Comparison of detection performance with the adjusted threshold across watermarking algorithms, mitigating the effect of default threshold. We show the best F1 score (↓\\downarrow) and TPR (↓\\downarrow) at FPR of 1% and 10%. BIRA consistently achieves lower F1 and TPR than all baselines, indicating greater difficulty for detectors in distinguishing attacked text from human-written text. Exact values are provided in AppendixG.1.",
                "position": 638
            },
            {
                "img": "https://arxiv.org/html/2509.23019/x3.png",
                "caption": "Figure 3:Comparison of text quality across different attacks for various watermarking methods, evaluated by LLM judgment score (↑\\uparrow), Self-BLEU score (↓\\downarrow), and Perplexity (↓\\downarrow). Our method preserves semantic fidelity to the original text compared to other attack baselines (DIPPER and SIRA) while providing stronger paraphrasing, as reflected in lower Self-BLEU scores. Additional results for NLI score (↑\\uparrow) and S-BERT score (↑\\uparrow) are provided in Figure6and exact values are detailed in AppendixG.2.",
                "position": 641
            },
            {
                "img": "https://arxiv.org/html/2509.23019/x4.png",
                "caption": "Table 3:zz-score comparison of attacks on SIR and Unigram watermarking scheme.",
                "position": 859
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProof of Theorem",
        "images": []
    },
    {
        "header": "Appendix BDetails of the Text Degeneration Detection Function",
        "images": []
    },
    {
        "header": "Appendix CPrompt for Semantic Judgment (GPT)",
        "images": []
    },
    {
        "header": "Appendix DParaphrasing Prompt",
        "images": []
    },
    {
        "header": "Appendix Ezz-score comparison of attacks on different watermarking schemes",
        "images": []
    },
    {
        "header": "Appendix FQualitative examples",
        "images": []
    },
    {
        "header": "Appendix GDetailed Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23019/x5.png",
                "caption": "Figure 6:Comparison of text quality across different attacks for various watermarking methods, evaluated by NLI score (↑\\uparrow) and S-BERT score (↑\\uparrow). Our method is comparable to or outperforms other baselines on both metrics. Following(Cheng et al.,2025), we evaluate attacks on S-BERT score. However, we observe that the S-BERT score often fails to capture factual accuracy and fine-grained meaning, sometimes assigning high scores despite factual errors and low scores even when the original meaning is preserved, likely because heavily paraphrased text is less familiar to the model.",
                "position": 2444
            }
        ]
    },
    {
        "header": "Appendix HLLM Usage",
        "images": []
    }
]