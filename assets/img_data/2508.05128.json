[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "Introduction",
        "images": []
    },
    {
        "header": "Related Works",
        "images": []
    },
    {
        "header": "Observation and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05128/x1.png",
                "caption": "(a)Attention Basin Phenomenon",
                "position": 187
            },
            {
                "img": "https://arxiv.org/html/2508.05128/x1.png",
                "caption": "(a)Attention Basin Phenomenon",
                "position": 190
            },
            {
                "img": "https://arxiv.org/html/2508.05128/x2.png",
                "caption": "(b)Effect of Disrupted Delimiters",
                "position": 195
            },
            {
                "img": "https://arxiv.org/html/2508.05128/x3.png",
                "caption": "Figure 2:Model QA accuracy on HotpotQA across all permutations of two relevant documents and one noise document. Blue bars: permutations where relevant documents receive the highest attention. Red bars: permutations where the noise document receives the highest attention. Orange bar: noise-free baseline. Aligning relevant documents with high-attention positions consistently yields the best performance.",
                "position": 255
            },
            {
                "img": "https://arxiv.org/html/2508.05128/x4.png",
                "caption": "Figure 3:Accuracy of a reranking strategy based on attention from different Transformer layers. Reranking using shallow-layer attention consistently outperforms using deeper layers, indicating that LLM’s core positional bias is established early.",
                "position": 296
            }
        ]
    },
    {
        "header": "Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05128/x5.png",
                "caption": "Figure 4:Overview of the AttnRank framework. Step 1: Profiling Positional Attention—We perform a one-time, low-cost analysis to capture the model’s intrinsic attention pattern (the attention basin) across document positions. Step 2: Attention-ariven Reranking—For any new query, we reorder the retrieved documents, mapping the most relevant document (highest similarity) to the position with the highest profiled attention score, thus aligning relevance with the model’s natural focus.",
                "position": 306
            }
        ]
    },
    {
        "header": "Experiments",
        "images": []
    },
    {
        "header": "Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATechnical Appendix",
        "images": []
    },
    {
        "header": "Appendix BImplementation of experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05128/figs/prompt.png",
                "caption": "Figure 5:Prompt for multi-hop QA experiment.",
                "position": 1431
            },
            {
                "img": "https://arxiv.org/html/2508.05128/figs/fewshot_prompt.png",
                "caption": "Figure 6:Prompt for dew-shot experiment.",
                "position": 1434
            },
            {
                "img": "https://arxiv.org/html/2508.05128/figs/nostruct_prompt.png",
                "caption": "Figure 7:Prompt after disrupting the structural blocks",
                "position": 1440
            }
        ]
    },
    {
        "header": "Appendix CMore details of how LLMs distribute their attention.",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05128/figs/appedix_attention_1.png",
                "caption": "(a)Two documents",
                "position": 1451
            },
            {
                "img": "https://arxiv.org/html/2508.05128/figs/appedix_attention_1.png",
                "caption": "(a)Two documents",
                "position": 1454
            },
            {
                "img": "https://arxiv.org/html/2508.05128/figs/appedix_attention_2.png",
                "caption": "(b)Three documents",
                "position": 1460
            },
            {
                "img": "https://arxiv.org/html/2508.05128/figs/appedix_attention_3.png",
                "caption": "(a)Four documents",
                "position": 1467
            },
            {
                "img": "https://arxiv.org/html/2508.05128/figs/appedix_attention_3.png",
                "caption": "(a)Four documents",
                "position": 1470
            },
            {
                "img": "https://arxiv.org/html/2508.05128/figs/appedix_attention_4.png",
                "caption": "(b)Five documents",
                "position": 1476
            },
            {
                "img": "https://arxiv.org/html/2508.05128/figs/appedix_attention_5.png",
                "caption": "(a)Six documents",
                "position": 1483
            },
            {
                "img": "https://arxiv.org/html/2508.05128/figs/appedix_attention_5.png",
                "caption": "(a)Six documents",
                "position": 1486
            },
            {
                "img": "https://arxiv.org/html/2508.05128/figs/appedix_attention_6.png",
                "caption": "(b)Seven documents",
                "position": 1492
            }
        ]
    },
    {
        "header": "Appendix DTheoretical analysis of attention-guided reranking in long-context tasks",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05128/figs/mistral_num.png",
                "caption": "(a)Mistral-7B-Instruct",
                "position": 1840
            },
            {
                "img": "https://arxiv.org/html/2508.05128/figs/mistral_num.png",
                "caption": "(a)Mistral-7B-Instruct",
                "position": 1843
            },
            {
                "img": "https://arxiv.org/html/2508.05128/figs/internlm_num.png",
                "caption": "(b)Internlm3-8b-instruct",
                "position": 1848
            }
        ]
    },
    {
        "header": "Appendix EHow many documents are required?",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05128/figs/Qwen2.5-7B_num.png",
                "caption": "(a)Qwen2.5-7B",
                "position": 1863
            },
            {
                "img": "https://arxiv.org/html/2508.05128/figs/Qwen2.5-7B_num.png",
                "caption": "(a)Qwen2.5-7B",
                "position": 1866
            },
            {
                "img": "https://arxiv.org/html/2508.05128/figs/deepseek_llm_num.png",
                "caption": "(b)Deepseek-llm",
                "position": 1871
            }
        ]
    },
    {
        "header": "Appendix FAttention distribution experiments and case studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05128/figs/case2.png",
                "caption": "Figure 13:Case 1’s input prompt and ranking outcomes under different reranking strategies.",
                "position": 2044
            },
            {
                "img": "https://arxiv.org/html/2508.05128/figs/case1.png",
                "caption": "Figure 14:Average attention scores for relevant and noise documents under various reranking strategies in case 1.AttnRankattains the highest attention on relevant documents and the lowest on irrelevant documents, validating its effectiveness.",
                "position": 2051
            },
            {
                "img": "https://arxiv.org/html/2508.05128/figs/case2_1.png",
                "caption": "Figure 15:Case 2’s input prompt and ranking outcomes under different reranking strategies.",
                "position": 2055
            },
            {
                "img": "https://arxiv.org/html/2508.05128/figs/case2_2.png",
                "caption": "Figure 16:Average attention scores for relevant and noise documents in case 2.",
                "position": 2058
            },
            {
                "img": "https://arxiv.org/html/2508.05128/figs/case3_1.png",
                "caption": "Figure 17:Case 3’s input prompt and ranking outcomes under different reranking strategies.",
                "position": 2061
            },
            {
                "img": "https://arxiv.org/html/2508.05128/figs/case3_2.png",
                "caption": "Figure 18:Average attention scores for relevant and noise documents in case 3.",
                "position": 2064
            }
        ]
    },
    {
        "header": "Appendix GLimitation and future works",
        "images": []
    }
]