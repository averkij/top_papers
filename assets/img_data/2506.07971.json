[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07971/x1.png",
                "caption": "Figure 1:Performance comparison on the VideoMMMU benchmark.Left:CyberV boosts small open-source models with only 7B parameters to outperform GPT-4o; with a larger model, CyberV surpasses the prior state-of-the-art and approaches the human level.Right:CoT reasoning improves results when using Qwen2.5-VL-7B, but multi-round reflection via “Wait” degrades performance.",
                "position": 92
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07971/x2.png",
                "caption": "Figure 2:Overall framework of CyberV.CyberV models test-time video understanding as a closed-loop cybernetic process comprising three modules:MLLM Inference System,Sensor, andController. TheMLLM Inference Systemexecutes inference scaling strategies with a frozen MLLM on multi-modal input, generating N outputs. TheSensormonitors the forward process of the MLLM, extracting intermediate signals such as parsed prediction and attention drift. TheControlleraggregates multiple signals to evaluate response reliability withScore Forestand triggers an self-correction actions when confidence falls below threshold throughInference Feedbackmodule. The updated input is then used to re-invoke the MLLM. This feedback loop enables adaptive and robust test-time reasoning.",
                "position": 201
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07971/x3.png",
                "caption": "Figure 3:Attention map visualizations on VideoMMMU. Red boxes highlight segments where attention significantly drops after applying “CoT”. They may correspond to content that contains critical information. Under our control system, adding key frames after “CoT” helps rectify previously incorrect responses.",
                "position": 937
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Overview",
        "images": []
    },
    {
        "header": "7More Implementation Details",
        "images": []
    },
    {
        "header": "8More Results and Ablation studies",
        "images": []
    },
    {
        "header": "9Pseudo-code of CyberV",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07971/x4.png",
                "caption": "Figure 5:Without confidence-based filtering in the Controller, high-confidence correct answers in the first round also need to trigger unnecessary key frames extraction, leading to errors in the second round due to noisy frames. In this case, although the question refers to 3:28 of a 4:09 video, the selected key frames focus on the beginning and end, resulting in an incorrect revision.",
                "position": 2161
            }
        ]
    },
    {
        "header": "10More Visualization Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07971/x5.png",
                "caption": "Figure 6:CoT fails to answer but our method performs well.",
                "position": 2188
            },
            {
                "img": "https://arxiv.org/html/2506.07971/x6.png",
                "caption": "Figure 7:Both the base model and CoT fail to answer but our method performs well.",
                "position": 2191
            },
            {
                "img": "https://arxiv.org/html/2506.07971/x7.png",
                "caption": "Figure 8:Noisy frames may degrade model performance.",
                "position": 2194
            }
        ]
    },
    {
        "header": "11Limitations and Future Work Discussion",
        "images": []
    }
]