[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.13976/x1.png",
                "caption": "Figure 1:Overview ofFantasyVLN.FantasyVLNis a VLN framework that integrates the strengths of textual and visual CoT reasoning modes, thereby jointly modeling semantic planning and spatial understanding.",
                "position": 139
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.13976/x2.png",
                "caption": "Figure 2:Overview of our unified multimodal Chain-of-Thought reasoning framework. The model supports four reasoning modes under a shared architecture: (a) non-CoT reasoning for real-time inference, (b) textual CoT, (c) visual CoT enabled by VAR-compressed imagined observations, and (d) multimodal CoT combining textual and visual reasoning. A gating mechanism switches the model across reasoning modes, while the action predictions from CoT modes are consistently aligned with the non-CoT mode.",
                "position": 165
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methods",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.13976/x3.png",
                "caption": "Figure 3:ISR variation with respect to different VAR scales.",
                "position": 749
            },
            {
                "img": "https://arxiv.org/html/2601.13976/x4.png",
                "caption": "Figure 4:Qualitative comparison of image reconstruction results produced by the VAR model using latent inputs across different scales. For each image, the VAR model receives the ground truth latents up to a specified scale and predicts all remaining scales; the final reconstruction is obtained by decoding the combined ground truth and predicted latents.",
                "position": 752
            },
            {
                "img": "https://arxiv.org/html/2601.13976/x5.png",
                "caption": "Figure 5:Comparison of training efficiency betweenFantasyVLNand WorldVLA.",
                "position": 856
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AData Preparation",
        "images": []
    },
    {
        "header": "Appendix BDetail Formulations.",
        "images": []
    },
    {
        "header": "Appendix CImplementation Details",
        "images": []
    }
]