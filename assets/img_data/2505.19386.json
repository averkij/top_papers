[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19386/",
                "caption": "Figure 1:Force promptingallows users to apply either global or local forces to objects in an image and then generate the resultant video.\nDespite being trained on a limited set of synthetic videos (15k for global force and 23k for local force), we observe significant generalization to different settings, materials, objects, geometries, affordances, and some initial hints at mass understanding.\nTrajectory visualization or alpha overlay are incorporated to better illustrate movement for some examples.",
                "position": 105
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method: Force Prompting",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19386/x2.png",
                "caption": "Figure 2:Visualizing the point force control signal.The magnitude of applied force is proportional to the gaussian blob’s velocity in the control signal, producing proportionally stronger impulses.\nStronger forces (bottom) generate faster-moving blobs and correspondingly larger physical responses than gentler forces (top). Note, red line added at the same location in each image for visualization.\nIn our method, we enable the force prompt to dictate the object’s trajectory, deliberately excluding such specifics from the text prompt.",
                "position": 182
            },
            {
                "img": "https://arxiv.org/html/2505.19386/",
                "caption": "Figure 3:Qualitative results for the Local Force (Poke) model.Top section:For local forces, the control signal can specify both the location, magnitude, and direction of the force.Bottom section:despite the limited training data, the model generalizes to different types of motion.",
                "position": 240
            }
        ]
    },
    {
        "header": "4Quantitative and Qualitative Results",
        "images": []
    },
    {
        "header": "5Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19386/",
                "caption": "Figure 4:Qualitative results for the Global Force (Wind) model.Top:from the same starting image, different directions for the force result in different videos.Bottom:while the model was only trained on flags, it can generalize to many different settings producing different types of motion.",
                "position": 483
            },
            {
                "img": "https://arxiv.org/html/2505.19386/extracted/6477538/figures_active/ablations_shortened.png",
                "caption": "Figure 5:Results from our ablation studies on synthetic dataset design choices.Left:when the global wind force model is trained on a dataset with only one flag, it overfits, causing the woman’s arm to wave unnaturally like fabric.Middle: when trained with a single background, the global force model has significantly degraded overall visual quality.Right:when trained without distractor objects, the point force model cannot properly localize motion, applying forces indiscriminately rather than to the intended target.",
                "position": 491
            },
            {
                "img": "https://arxiv.org/html/2505.19386/x5.png",
                "caption": "Figure 6:Mass understanding:We find that the model has some degree of understanding of mass, in that the same force applied to two objects with different masses will result in different amounts of motion. We demonstrate this qualitatively in (a) and (b) and quantitatively in (c), showing that this result is consistent across a range of force magnitudes. See additional examples in the project webpage.",
                "position": 587
            }
        ]
    },
    {
        "header": "6Mass Understanding",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation details",
        "images": []
    },
    {
        "header": "Appendix BAdditional qualitative results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19386/extracted/6477538/figures_active/ablation_synthetic_dataset_diversity_v1.png",
                "caption": "Figure 7:Results from our ablation studies on synthetic dataset design choices.Top:when the global wind force model is trained on a dataset with only one flag, it overfits, causing the woman’s arm to wave unnaturally like fabric and failing to generalize to fluid dynamics scenarios such as smoke.Middle:when trained with a single background, the global force model fails to differentiate between foreground and background elements, significantly degrading overall visual quality.Bottom:when trained without distractor objects, the point force model cannot properly localize motion, applying forces indiscriminately rather than to the intended target.",
                "position": 1314
            },
            {
                "img": "https://arxiv.org/html/2505.19386/extracted/6477538/figures_active/ablation_text_prompt_specificity.png",
                "caption": "Figure 8:Results from our ablation studies on text prompt specificity.In this ablation study, we investigate how material descriptions in text prompts affect model generalization through a2×2222\\times 22 × 2grid search ablation study.\nWe train and test our wind model with and without wind-related keywords (wind/breeze/blow).\nOur results demonstrate that omitting these keywords during training significantly increases failure cases in our benchmark dataset.\nFor example, steam is conjured out of thin air instead of being blown correctly.\nIn contrast, models trained with wind-specific terminology demonstrated superior generalization to diverse wind scenarios.",
                "position": 1328
            },
            {
                "img": "https://arxiv.org/html/2505.19386/extracted/6477538/figures_active/failures_shortened.png",
                "caption": "Figure 9:Analysis of failure cases.We illustrate and categorize failure cases of Force Prompting.\nThetop rowshows scenarios where the generated physical motion is out-of-domain for the base CogVideoX model, leading to partial adherence to the force prompt.\nThebottom rowdepicts failures in visual fidelity or physical realism when the video prior conflicts with the force prompt’s intent.\nMore examples are available on our project webpage.",
                "position": 1343
            }
        ]
    },
    {
        "header": "Appendix CAdditional emergent phenomena",
        "images": []
    },
    {
        "header": "Appendix DImpact Statement",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19386/extracted/6477538/figures_active/survey_screenshot_example1.png",
                "caption": "Figure 10:A demonstration question from one of our surveys.Participants are shown an example question with a response along with the reasoning for that response.",
                "position": 1437
            },
            {
                "img": "https://arxiv.org/html/2505.19386/extracted/6477538/figures_active/survey_screenshot_question4.png",
                "caption": "Figure 11:A question from one of our surveys.Participants are shown two videos side to side, with radio buttons beneath that they may use to make a selection of which better adheres to the question. The videos play automatically and simultaneously.",
                "position": 1441
            },
            {
                "img": "https://arxiv.org/html/2505.19386/extracted/6477538/figures_active/filmstrip_ball.png",
                "caption": "Figure 12:Samples from our synthetic training datasets.Top (ball) and middle (flower) are timelapses from our point force training dataset; bottom (flag) are timelapses from the global force training dataset.\nOur key finding is that video generation models can generalize well when adapted to follow physical force conditioning from videos synthesized by Blender, even with limited demonstrations on few ojects.",
                "position": 1445
            },
            {
                "img": "https://arxiv.org/html/2505.19386/extracted/6477538/figures_active/filmstrip_background_aerial_grass_rock_4k_angle_110.1105_force_57.9815_coordx_407_coordy_172_bowling.png",
                "caption": "",
                "position": 1449
            },
            {
                "img": "https://arxiv.org/html/2505.19386/extracted/6477538/figures_active/filmstrip_background_coast_sand_04_4k_angle_12.7063_force_37.5982_coordx_417_coordy_315.png",
                "caption": "",
                "position": 1451
            },
            {
                "img": "https://arxiv.org/html/2505.19386/extracted/6477538/figures_active/filmstrip_background_forest_floor_4k_angle_111.5938_force_51.4091_coordx_183_coordy_299.png",
                "caption": "",
                "position": 1453
            },
            {
                "img": "https://arxiv.org/html/2505.19386/extracted/6477538/figures_active/filmstrip_flower.png",
                "caption": "",
                "position": 1455
            },
            {
                "img": "https://arxiv.org/html/2505.19386/extracted/6477538/figures_active/filmstrip_carnation_camera_0_point_4_force_4.88_angle_153.88_coordx_818_coordy_433.png",
                "caption": "",
                "position": 1457
            },
            {
                "img": "https://arxiv.org/html/2505.19386/extracted/6477538/figures_active/filmstrip_carnation_camera_3_point_0_force_4.46_angle_345.23_coordx_655_coordy_763.png",
                "caption": "",
                "position": 1459
            },
            {
                "img": "https://arxiv.org/html/2505.19386/extracted/6477538/figures_active/filmstrip_carnation_camera_2_point_1_force_4.50_angle_269.57_coordx_607_coordy_713.png",
                "caption": "",
                "position": 1461
            },
            {
                "img": "https://arxiv.org/html/2505.19386/extracted/6477538/figures_active/filmstrip_flag_sample_4001.6_0.0_162.4_0.0_background_urban_street_04_4k.png",
                "caption": "",
                "position": 1463
            },
            {
                "img": "https://arxiv.org/html/2505.19386/extracted/6477538/figures_active/filmstrip_flag.png",
                "caption": "",
                "position": 1465
            },
            {
                "img": "https://arxiv.org/html/2505.19386/extracted/6477538/figures_active/filmstrip_flag_sample_3981.9_0.0_180.2_0.0_background_belfast_sunset_4k.png",
                "caption": "",
                "position": 1467
            },
            {
                "img": "https://arxiv.org/html/2505.19386/extracted/6477538/figures_active/filmstrip_flag_sample_3978.1_0.0_30.8_0.0_background_stuttgart_suburbs_4k.png",
                "caption": "",
                "position": 1469
            }
        ]
    },
    {
        "header": "Appendix ESurvey Details and Instructions",
        "images": []
    }
]