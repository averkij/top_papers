[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.05369/x1.png",
                "caption": "Figure 2:Single Motion Captioning performance divided bysimpleandcomplexmotion sequences.We report the single motion captioning performance of state-of-the-art motion-language models on the simple and complex subsets of HumanML3D[11]’s test set, as defined in Sec.3.1.",
                "position": 145
            }
        ]
    },
    {
        "header": "3From Simple to Complex Motions",
        "images": []
    },
    {
        "header": "4DEMO: Dense Motion Captioning Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.05369/x2.png",
                "caption": "Figure 3:DEMO overview :Given a motion sequencemm, our method encodes it with the motion adapterΦW,γ\\Phi_{\\textbf{W},\\gamma}, which maps it into the language embedding space of the LLMfϕf_{\\phi}. Using the resulting motion embeddings and a textual instructionxi​n​s​tx_{inst}, the model generates dense captions with temporal boundaries. Training is conducted in two stages. Here,denotes the subset of parameters being trained.",
                "position": 327
            },
            {
                "img": "https://arxiv.org/html/2511.05369/x4.png",
                "caption": "",
                "position": 329
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.05369/x5.png",
                "caption": "Figure 4:Qualitative Results.We show two motion sequence examples from the CompMo dataset, along with the ground truth annotations (GT) and the dense captions predicted by our DEMO and UniMotion. For each sequence, the top rows show the temporal intervals of the input motion divided according to the GT and the two model predictions, with the corresponding captions listed below.\nPredicted captions that align with the GT are highlighted in the same color and connected with arrows to indicate the alignment.",
                "position": 733
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.05369/x6.png",
                "caption": "Figure A.1:Overview of CompMo generation pipelineWe illustrate the three steps of the data generation pipeline, as detailed inSec.˜3.2.",
                "position": 1494
            }
        ]
    },
    {
        "header": "Appendix BAdditional Experimental Results",
        "images": []
    }
]