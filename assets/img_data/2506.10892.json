[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10892/extracted/6537091/figures/duo_schematic.png",
                "caption": "Figure 1:An illustration ofUniform-state discrete diffusion(top) and the underlyingGaussian diffusion(bottom). While both are separate Markov processes, applyingargâ¢maxargmax{\\color[rgb]{0.60546875,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n0.60546875,0,0}\\operatorname*{arg\\,max}}roman_arg roman_maxmaps Gaussian latentsð°tâˆˆâ„nsubscriptð°ð‘¡superscriptâ„ð‘›{\\mathbf{w}}_{t}\\in\\mathbb{R}^{n}bold_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPTto discrete latentsð³tâˆˆð’±subscriptð³ð‘¡ð’±{\\mathbf{z}}_{t}\\in\\mathcal{V}bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT âˆˆ caligraphic_V, transforming their marginals fromq~t(.|ð±;Î±~t){\\color[rgb]{0.90234375,0.5703125,0.21875}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{0.90234375,0.5703125,0.21875}\\tilde{q}_{t}}(.|{\\mathbf{x}};{\\color[rgb]{%\n0.90234375,0.5703125,0.21875}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n0.90234375,0.5703125,0.21875}\\tilde{\\alpha}_{t}})over~ start_ARG italic_q end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( . | bold_x ; over~ start_ARG italic_Î± end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )(6) toqt(.|ð±;ð’¯(Î±~t)){\\color[rgb]{0.04296875,0.32421875,0.58984375}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0.04296875,0.32421875,0.58984375}q_{t}}(.|{\\mathbf{x}};{%\n\\color[rgb]{0.60546875,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n0.60546875,0,0}\\mathcal{T}}({\\color[rgb]{0.90234375,0.5703125,0.21875}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{0.90234375,0.5703125,0.21875}\\tilde{%\n\\alpha}_{t}}))italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( . | bold_x ; caligraphic_T ( over~ start_ARG italic_Î± end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) )(1) and adjusting diffusion parameters fromÎ±~tsubscript~ð›¼ð‘¡{\\color[rgb]{0.90234375,0.5703125,0.21875}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{0.90234375,0.5703125,0.21875}\\tilde{\\alpha}_{t}}over~ start_ARG italic_Î± end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTtoÎ±t=ð’¯â¢(Î±~t)subscriptð›¼ð‘¡ð’¯subscript~ð›¼ð‘¡{\\color[rgb]{0.04296875,0.32421875,0.58984375}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0.04296875,0.32421875,0.58984375}\\alpha_{t}}={\\color[rgb]%\n{0.60546875,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0.60546875,0,0}%\n\\mathcal{T}}({\\color[rgb]{0.90234375,0.5703125,0.21875}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0.90234375,0.5703125,0.21875}\\tilde{\\alpha}_{t}})italic_Î± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = caligraphic_T ( over~ start_ARG italic_Î± end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )(10).\nNotably, the ELBO for Uniform-state diffusion induces a tighter bound on the likelihood than Gaussian diffusion, as established in Theorem3.1.",
                "position": 214
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3The Diffusion Duality",
        "images": []
    },
    {
        "header": "4Applications",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10892/x1.png",
                "caption": "Figure 2:Curriculum learning drastically lowers the gradient variance in Duo trained with a fixedÏ„=0.001ðœ0.001\\tau=0.001italic_Ï„ = 0.001. The figure shows the summed gradient variance of the 100 weights with the highest variance, comparing Duo with CL (blue) and without CL (grey).",
                "position": 1213
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10892/x2.png",
                "caption": "Figure 3:Sample quality comparison of Duo vs. MDLM. Duo outperforms MDLM in Gen PPL (â†“â†“\\downarrowâ†“) for base models and in low-NFE regime after 5 distillation rounds.",
                "position": 1531
            },
            {
                "img": "https://arxiv.org/html/2506.10892/x3.png",
                "caption": "Figure 4:Sample quality comparison between the base Duo model and Duo distilled for 5 rounds using our DCD algorithm. The distilled model matches the base modelâ€™s sample quality in just 16 steps (vs. 1024) with ancestral sampling. With our Greedy-Tail sampler, sampling steps can be further reduced to 8, achieving slightly better Gen PPL and lower entropy.",
                "position": 1803
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AThe Diffusion Duality",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10892/x4.png",
                "caption": "(a)Autoregressive Model",
                "position": 4051
            },
            {
                "img": "https://arxiv.org/html/2506.10892/x4.png",
                "caption": "(a)Autoregressive Model",
                "position": 4054
            },
            {
                "img": "https://arxiv.org/html/2506.10892/x5.png",
                "caption": "(b)Masked Diffusion",
                "position": 4059
            },
            {
                "img": "https://arxiv.org/html/2506.10892/x6.png",
                "caption": "(c)Uniform-state Diffusion",
                "position": 4065
            },
            {
                "img": "https://arxiv.org/html/2506.10892/x7.png",
                "caption": "(d)ð’«DDTsubscriptð’«DDT\\mathcal{P}_{\\text{DDT}}caligraphic_P start_POSTSUBSCRIPT DDT end_POSTSUBSCRIPT",
                "position": 4070
            }
        ]
    },
    {
        "header": "Appendix BAdditional Proofs",
        "images": []
    },
    {
        "header": "Appendix CExperimental details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10892/extracted/6537091/figures/diffusion_constant.png",
                "caption": "Figure 6:Diffusion transformation operatorð’¯â¢(Î±~t)ð’¯subscript~ð›¼ð‘¡{\\color[rgb]{0.60546875,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n0.60546875,0,0}\\mathcal{T}}({\\color[rgb]{0.90234375,0.5703125,0.21875}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{0.90234375,0.5703125,0.21875}\\tilde{%\n\\alpha}_{t}})caligraphic_T ( over~ start_ARG italic_Î± end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )(10) for thebert-base-uncasedtokenizer.",
                "position": 4740
            },
            {
                "img": "https://arxiv.org/html/2506.10892/x8.png",
                "caption": "Figure 7:Training loss curves for Duo (ours) with curriculum learning, UDLM, and MDLM. We see observe that curriculum learning leads to low-variance training. Duoâ€™s curve is lower because its a biased estimate of the ELBO.",
                "position": 4749
            },
            {
                "img": "https://arxiv.org/html/2506.10892/x9.png",
                "caption": "Figure 8:We study the training bias and variance introduced byÏ„>0ðœ0\\tau>0italic_Ï„ > 0. Models were trained on the LM1B dataset.",
                "position": 4852
            },
            {
                "img": "https://arxiv.org/html/2506.10892/x10.png",
                "caption": "Figure 9:Sample quality comparison using Gen PPL (â†“â†“\\downarrowâ†“) between Duo (ours), MDLM, SEDD (Absorb / Uniform), and AR. Values in brackets indicate sample entropy (â†‘â†‘\\uparrowâ†‘). Among USDMs, Duo achieves lower Gen PPL than SEDD-Uniform, indicating higher sample quality. Compared to MDMs, Duo yields lower Gen PPL with a slight trade-off in entropy. Exact quantitative numbers for Gen PPL can be found inÂ Table4.",
                "position": 4862
            },
            {
                "img": "https://arxiv.org/html/2506.10892/x11.png",
                "caption": "Figure 10:We compare DCD using denoising weights (Alg.1) vs. EMA weights (Alg.2) as the teacher. Using the denoising model yields a more effective distilled model. Quantitative numbers for Gen PPL can be found inÂ Table6.",
                "position": 5027
            },
            {
                "img": "https://arxiv.org/html/2506.10892/x12.png",
                "caption": "Figure 11:Entropy of MDLM distilled using SDTT, and of Duo distilled using CDC. The entropy of the SDTT-distilled MDLM decreases with distillation, while the entropy of the CDC-distilled Duo model increases. The curves corresponding to a higher number of sampling steps are displayed with lighter colors to emphasize the low sampling step regimes.",
                "position": 5176
            },
            {
                "img": "https://arxiv.org/html/2506.10892/x13.png",
                "caption": "Figure 12:Sample quality comparision using Gen PPL (â†“â†“\\downarrowâ†“) of Duo (Ours) distilled with our proposed DCD algorithm and MDLM distilled with SDTT after successive distillation round. Duo always dominates in the low sampling steps regime. ReferÂ Table7for the exact quantitative numbers.",
                "position": 5179
            }
        ]
    },
    {
        "header": "Appendix DAdditional Experiments",
        "images": []
    }
]