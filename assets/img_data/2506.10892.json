[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10892/extracted/6537091/figures/duo_schematic.png",
                "caption": "Figure 1:An illustration ofUniform-state discrete diffusion(top) and the underlyingGaussian diffusion(bottom). While both are separate Markov processes, applyingarg⁢maxargmax{\\color[rgb]{0.60546875,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n0.60546875,0,0}\\operatorname*{arg\\,max}}roman_arg roman_maxmaps Gaussian latents𝐰t∈ℝnsubscript𝐰𝑡superscriptℝ𝑛{\\mathbf{w}}_{t}\\in\\mathbb{R}^{n}bold_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPTto discrete latents𝐳t∈𝒱subscript𝐳𝑡𝒱{\\mathbf{z}}_{t}\\in\\mathcal{V}bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∈ caligraphic_V, transforming their marginals fromq~t(.|𝐱;α~t){\\color[rgb]{0.90234375,0.5703125,0.21875}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{0.90234375,0.5703125,0.21875}\\tilde{q}_{t}}(.|{\\mathbf{x}};{\\color[rgb]{%\n0.90234375,0.5703125,0.21875}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n0.90234375,0.5703125,0.21875}\\tilde{\\alpha}_{t}})over~ start_ARG italic_q end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( . | bold_x ; over~ start_ARG italic_α end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )(6) toqt(.|𝐱;𝒯(α~t)){\\color[rgb]{0.04296875,0.32421875,0.58984375}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0.04296875,0.32421875,0.58984375}q_{t}}(.|{\\mathbf{x}};{%\n\\color[rgb]{0.60546875,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n0.60546875,0,0}\\mathcal{T}}({\\color[rgb]{0.90234375,0.5703125,0.21875}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{0.90234375,0.5703125,0.21875}\\tilde{%\n\\alpha}_{t}}))italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( . | bold_x ; caligraphic_T ( over~ start_ARG italic_α end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) )(1) and adjusting diffusion parameters fromα~tsubscript~𝛼𝑡{\\color[rgb]{0.90234375,0.5703125,0.21875}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{0.90234375,0.5703125,0.21875}\\tilde{\\alpha}_{t}}over~ start_ARG italic_α end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTtoαt=𝒯⁢(α~t)subscript𝛼𝑡𝒯subscript~𝛼𝑡{\\color[rgb]{0.04296875,0.32421875,0.58984375}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0.04296875,0.32421875,0.58984375}\\alpha_{t}}={\\color[rgb]%\n{0.60546875,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0.60546875,0,0}%\n\\mathcal{T}}({\\color[rgb]{0.90234375,0.5703125,0.21875}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0.90234375,0.5703125,0.21875}\\tilde{\\alpha}_{t}})italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = caligraphic_T ( over~ start_ARG italic_α end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )(10).\nNotably, the ELBO for Uniform-state diffusion induces a tighter bound on the likelihood than Gaussian diffusion, as established in Theorem3.1.",
                "position": 214
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3The Diffusion Duality",
        "images": []
    },
    {
        "header": "4Applications",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10892/x1.png",
                "caption": "Figure 2:Curriculum learning drastically lowers the gradient variance in Duo trained with a fixedτ=0.001𝜏0.001\\tau=0.001italic_τ = 0.001. The figure shows the summed gradient variance of the 100 weights with the highest variance, comparing Duo with CL (blue) and without CL (grey).",
                "position": 1213
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10892/x2.png",
                "caption": "Figure 3:Sample quality comparison of Duo vs. MDLM. Duo outperforms MDLM in Gen PPL (↓↓\\downarrow↓) for base models and in low-NFE regime after 5 distillation rounds.",
                "position": 1531
            },
            {
                "img": "https://arxiv.org/html/2506.10892/x3.png",
                "caption": "Figure 4:Sample quality comparison between the base Duo model and Duo distilled for 5 rounds using our DCD algorithm. The distilled model matches the base model’s sample quality in just 16 steps (vs. 1024) with ancestral sampling. With our Greedy-Tail sampler, sampling steps can be further reduced to 8, achieving slightly better Gen PPL and lower entropy.",
                "position": 1803
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AThe Diffusion Duality",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10892/x4.png",
                "caption": "(a)Autoregressive Model",
                "position": 4051
            },
            {
                "img": "https://arxiv.org/html/2506.10892/x4.png",
                "caption": "(a)Autoregressive Model",
                "position": 4054
            },
            {
                "img": "https://arxiv.org/html/2506.10892/x5.png",
                "caption": "(b)Masked Diffusion",
                "position": 4059
            },
            {
                "img": "https://arxiv.org/html/2506.10892/x6.png",
                "caption": "(c)Uniform-state Diffusion",
                "position": 4065
            },
            {
                "img": "https://arxiv.org/html/2506.10892/x7.png",
                "caption": "(d)𝒫DDTsubscript𝒫DDT\\mathcal{P}_{\\text{DDT}}caligraphic_P start_POSTSUBSCRIPT DDT end_POSTSUBSCRIPT",
                "position": 4070
            }
        ]
    },
    {
        "header": "Appendix BAdditional Proofs",
        "images": []
    },
    {
        "header": "Appendix CExperimental details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10892/extracted/6537091/figures/diffusion_constant.png",
                "caption": "Figure 6:Diffusion transformation operator𝒯⁢(α~t)𝒯subscript~𝛼𝑡{\\color[rgb]{0.60546875,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n0.60546875,0,0}\\mathcal{T}}({\\color[rgb]{0.90234375,0.5703125,0.21875}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{0.90234375,0.5703125,0.21875}\\tilde{%\n\\alpha}_{t}})caligraphic_T ( over~ start_ARG italic_α end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )(10) for thebert-base-uncasedtokenizer.",
                "position": 4740
            },
            {
                "img": "https://arxiv.org/html/2506.10892/x8.png",
                "caption": "Figure 7:Training loss curves for Duo (ours) with curriculum learning, UDLM, and MDLM. We see observe that curriculum learning leads to low-variance training. Duo’s curve is lower because its a biased estimate of the ELBO.",
                "position": 4749
            },
            {
                "img": "https://arxiv.org/html/2506.10892/x9.png",
                "caption": "Figure 8:We study the training bias and variance introduced byτ>0𝜏0\\tau>0italic_τ > 0. Models were trained on the LM1B dataset.",
                "position": 4852
            },
            {
                "img": "https://arxiv.org/html/2506.10892/x10.png",
                "caption": "Figure 9:Sample quality comparison using Gen PPL (↓↓\\downarrow↓) between Duo (ours), MDLM, SEDD (Absorb / Uniform), and AR. Values in brackets indicate sample entropy (↑↑\\uparrow↑). Among USDMs, Duo achieves lower Gen PPL than SEDD-Uniform, indicating higher sample quality. Compared to MDMs, Duo yields lower Gen PPL with a slight trade-off in entropy. Exact quantitative numbers for Gen PPL can be found in Table4.",
                "position": 4862
            },
            {
                "img": "https://arxiv.org/html/2506.10892/x11.png",
                "caption": "Figure 10:We compare DCD using denoising weights (Alg.1) vs. EMA weights (Alg.2) as the teacher. Using the denoising model yields a more effective distilled model. Quantitative numbers for Gen PPL can be found in Table6.",
                "position": 5027
            },
            {
                "img": "https://arxiv.org/html/2506.10892/x12.png",
                "caption": "Figure 11:Entropy of MDLM distilled using SDTT, and of Duo distilled using CDC. The entropy of the SDTT-distilled MDLM decreases with distillation, while the entropy of the CDC-distilled Duo model increases. The curves corresponding to a higher number of sampling steps are displayed with lighter colors to emphasize the low sampling step regimes.",
                "position": 5176
            },
            {
                "img": "https://arxiv.org/html/2506.10892/x13.png",
                "caption": "Figure 12:Sample quality comparision using Gen PPL (↓↓\\downarrow↓) of Duo (Ours) distilled with our proposed DCD algorithm and MDLM distilled with SDTT after successive distillation round. Duo always dominates in the low sampling steps regime. Refer Table7for the exact quantitative numbers.",
                "position": 5179
            }
        ]
    },
    {
        "header": "Appendix DAdditional Experiments",
        "images": []
    }
]