[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.12814/extracted/6021228/fig1.png",
                "caption": "Figure 1:We collected 110 medical image datasets from various sources and generated the IMed-361M dataset, which contains over 361 million masks, through a rigorous and standardized data processing pipeline. Using this dataset, we developed the IMIS baseline network.",
                "position": 84
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.12814/extracted/6021228/fig2.png",
                "caption": "Figure 2:Overview of the IMed-361M dataset. (a) Number of images and masks for each modality. (b) Information on six anatomical structures. (c) Distribution of image resolutions. (d) Analysis of mask proportions. (e) Comparison with other existing public datasets.",
                "position": 122
            }
        ]
    },
    {
        "header": "3IMIS Benchmark Dataset: IMed-361M",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.12814/extracted/6021228/fig3.png",
                "caption": "Figure 3:Evaluation of the quality of interactive masks.",
                "position": 174
            }
        ]
    },
    {
        "header": "4IMIS Baseline Network",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.12814/extracted/6021228/fig4.png",
                "caption": "Figure 4:The training process of IMIS-Net simulates K consecutive steps of interactive segmentation.",
                "position": 201
            },
            {
                "img": "https://arxiv.org/html/2411.12814/extracted/6021228/fig5.png",
                "caption": "Figure 5:Comparison of IMIS-Net with existing foundation models, with performance statistics at both image and mask levels.",
                "position": 220
            },
            {
                "img": "https://arxiv.org/html/2411.12814/extracted/6021228/fig6.png",
                "caption": "Figure 6:Comparison of segmentation performance across different anatomical structures under single-click (a) and bounding box (b) interactions. (c) Changes in segmentation performance with increasing interaction numbers. (d) and (e) Impact of click position and bounding box offset on performance.",
                "position": 233
            },
            {
                "img": "https://arxiv.org/html/2411.12814/extracted/6021228/fig7.png",
                "caption": "Figure 7:(a) and (b) segmentation performance increases with the increasing mask density or data scales up. (c) effectiveness of text and its combination with other prompts.",
                "position": 435
            }
        ]
    },
    {
        "header": "5Results",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix A: Demo and Code",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.12814/extracted/6021228/sup_fig1.png",
                "caption": "Figure 8:Example predictions of IMIS-Net across different modalities and segmentation tasks. \"*\" Indicates that the corresponding image modality or segmentation task was not included in our training plan. Our model demonstrates its versatility by effectively handling multiple medical image modalities and performing various segmentation tasks, even on those that it has not previously encountered.",
                "position": 1231
            }
        ]
    },
    {
        "header": "Appendix B: IMed-361M Information and Availability",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.12814/extracted/6021228/sup_fig2.png",
                "caption": "Figure 9:IMed-361M:A comprehensive dataset of multimodal medical images encompassing nearly all human organs and lesions, with interactive masks offering detailed, dense annotations.",
                "position": 1240
            },
            {
                "img": "https://arxiv.org/html/2411.12814/extracted/6021228/sup_fig3.png",
                "caption": "Figure 10:Comparison of segmentation performance of different methods in 14 medical image modalities, where the red points represent the means.",
                "position": 4290
            }
        ]
    },
    {
        "header": "Appendix C: Additional Experimental Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.12814/extracted/6021228/sup_fig4.png",
                "caption": "Figure 11:Simulate interactive segmentation results with identical bounding box coordinates for all model inputs.",
                "position": 4299
            },
            {
                "img": "https://arxiv.org/html/2411.12814/extracted/6021228/sup_fig5.png",
                "caption": "Figure 12:An interactive segmentation example of the stomach in CT images. SAM and SAM-2 typically require more prompts to achieve better results, while IMIS-Net achieves comparable performance with fewer interactions.",
                "position": 4302
            },
            {
                "img": "https://arxiv.org/html/2411.12814/extracted/6021228/sup_fig7.png",
                "caption": "Figure 13:An interactive segmentation example of the cardiac myocardium in MR images. SAM performs poorly when dealing with annular myocardium, while SAM-2 and IMIS-Net are able to obtain predictions of the target area through multiple interactions. Our network consistently outperforms other methods.",
                "position": 4305
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]