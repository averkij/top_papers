[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09864/x1.png",
                "caption": "Figure 1:Illustration of UniUGP, a unified model with three hybrid\nexperts. The understanding expert performs the next-token prediction for causal reasoning. The planning expert forms a MoT architecture with the understanding expert, and performs the velocity prediction in flow matching for production future actions. The generation expert is cascaded as a world model to produce future videos.",
                "position": 719
            },
            {
                "img": "https://arxiv.org/html/2512.09864/x2.png",
                "caption": "Figure 2:Dataset Construction Pipeline.This figure depicts the pipeline of data collection (integrating multiple challenging driving datasets) and data processing (featuring four task categories: understanding, chain-of-thought, planning, and instruction following) to train and assess the cognitive abilities of end-to-end autonomous driving models within a unified QA framework.",
                "position": 723
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09864/x3.png",
                "caption": "Figure 3:The ablation experiment on the absence or presence of world model knowledge.The world model enables the VLA to pay more attention to future causal relationships, thereby focusing on the semantics of distant objects.",
                "position": 1249
            },
            {
                "img": "https://arxiv.org/html/2512.09864/x4.png",
                "caption": "Figure 4:Trajectory controllable generation visualization.We control the generation of future frames of the video by modifying the trajectories fed into the generation model, which demonstrates the controllability of our generation experts.",
                "position": 1505
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Contributions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09864/x5.png",
                "caption": "Figure 5:Long-tail perception and understanding of questions and answers.",
                "position": 3051
            },
            {
                "img": "https://arxiv.org/html/2512.09864/x5.png",
                "caption": "",
                "position": 3054
            },
            {
                "img": "https://arxiv.org/html/2512.09864/x6.png",
                "caption": "",
                "position": 3059
            },
            {
                "img": "https://arxiv.org/html/2512.09864/x7.png",
                "caption": "",
                "position": 3064
            },
            {
                "img": "https://arxiv.org/html/2512.09864/x8.png",
                "caption": "Figure 6:Future Trajectories-Based CoT Reasoning.",
                "position": 3254
            }
        ]
    },
    {
        "header": "7More Cases",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09864/x9.png",
                "caption": "Figure 7:Comparison of CoT in our method versus GPT4o.Our approach provides more specific planning results, while the general large model does not offer sufficiently detailed planning outcomes.",
                "position": 3522
            },
            {
                "img": "https://arxiv.org/html/2512.09864/x9.png",
                "caption": "",
                "position": 3525
            },
            {
                "img": "https://arxiv.org/html/2512.09864/x10.png",
                "caption": "",
                "position": 3530
            },
            {
                "img": "https://arxiv.org/html/2512.09864/x11.png",
                "caption": "Figure 8:Video generation visualization with controllable weather conditions.Our model can generate videos of different weather conditions, which proves the efficiency of our generation model. Please zoom in to the best view.",
                "position": 3536
            },
            {
                "img": "https://arxiv.org/html/2512.09864/x11.png",
                "caption": "",
                "position": 3539
            },
            {
                "img": "https://arxiv.org/html/2512.09864/x12.png",
                "caption": "",
                "position": 3544
            },
            {
                "img": "https://arxiv.org/html/2512.09864/x13.png",
                "caption": "Figure 9:Video generation visualization with controllable trajectory conditions.Our model can generate videos of different trajectory conditions, which proves the efficiency of our generation model. Please zoom in to the best view.",
                "position": 3550
            },
            {
                "img": "https://arxiv.org/html/2512.09864/x13.png",
                "caption": "",
                "position": 3553
            },
            {
                "img": "https://arxiv.org/html/2512.09864/x14.png",
                "caption": "",
                "position": 3558
            },
            {
                "img": "https://arxiv.org/html/2512.09864/x15.png",
                "caption": "",
                "position": 3563
            }
        ]
    },
    {
        "header": "8Limitation and Future Directions",
        "images": []
    }
]