[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.14979/figures/logos/home.png",
                "caption": "",
                "position": 228
            },
            {
                "img": "https://arxiv.org/html/2602.14979/x1.png",
                "caption": "",
                "position": 232
            },
            {
                "img": "https://arxiv.org/html/2602.14979/x2.png",
                "caption": "",
                "position": 236
            },
            {
                "img": "https://arxiv.org/html/2602.14979/figures/logos/ms_logo.jpg",
                "caption": "",
                "position": 240
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.14979/x3.png",
                "caption": "Figure 1:Overview of the RynnBrain embodied foundation model. RynnBrain integrates four core capabilities: egocentric cognition, spatio-temporal localization, physically grounded reasoning, and physics-aware planning.\nOn the input side, RynnBrain processes multimodal signals including images, videos, and spatio-temporal coordinates. On the output side, it jointly produces natural language and explicit spatial grounding primitives such as points, bounding boxes, and trajectories, enabling coherent perception, reasoning, and planning in physical environments.",
                "position": 254
            }
        ]
    },
    {
        "header": "2Overview",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.14979/x4.png",
                "caption": "Figure 2:Overview of the RynnBrain architecture. RynnBrain processes omni vision inputs, including single view images, multi view images, and videos, together with language instructions. A shared dense or mixture of experts decoder generates aligned multimodal outputs, including text, regions, trajectories, and pointing signals. This unified output space supports egocentric understanding, spatiotemporal grounding, physically grounded reasoning, and fine grained action planning in real world environments.",
                "position": 335
            }
        ]
    },
    {
        "header": "3Physics-Aware Spatio-temporal Pretraining",
        "images": []
    },
    {
        "header": "4Physically Grounded Chain-of-Point Reasoning",
        "images": []
    },
    {
        "header": "5Post-training for Embodied Tasks",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.14979/x5.png",
                "caption": "Figure 3:RynnBrain-VLA architecture.",
                "position": 1110
            },
            {
                "img": "https://arxiv.org/html/2602.14979/x6.png",
                "caption": "Figure 4:Overview of evaluation dimensions in RynnBrain-Bench. RynnBrain-Bench includes two subsets: cognition and location, evaluating a total of 21 spatio-temporal fine-grained embodied abilities.",
                "position": 1241
            }
        ]
    },
    {
        "header": "6Evaluation",
        "images": []
    }
]