[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24910/x1.png",
                "caption": "(a)IL on Human Demonstrations.",
                "position": 132
            },
            {
                "img": "https://arxiv.org/html/2509.24910/x1.png",
                "caption": "(a)IL on Human Demonstrations.",
                "position": 135
            },
            {
                "img": "https://arxiv.org/html/2509.24910/x2.png",
                "caption": "(b)Shortest-Path Augmentation.",
                "position": 140
            },
            {
                "img": "https://arxiv.org/html/2509.24910/x3.png",
                "caption": "(c)Self-Improving Demonstrations.",
                "position": 145
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Self-Improving Demonstrations for Goal-oriented VLN",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24910/x4.png",
                "caption": "Figure 2:Our proposed Self-Improving Demonstrations paradigm for goal-oriented VLN.We learn an initial navigation agent using trajectories sampled from MP3D, generate new paths using this agent, and reserve the successful exploration ones. These trajectories give demonstrations on the exploration strategies, resulting in a more capable agent. This iterative semi-supervised learning can gradually improve navigation agent’s performance ceiling and produce effective exploration trajectories at scale, which can be transferred with caption augmentation for goal-oriented VLN.",
                "position": 217
            },
            {
                "img": "https://arxiv.org/html/2509.24910/x5.png",
                "caption": "Figure 3:Filtering the trajectories generated by the navigation agent.The agent may fail in various scenarios, such as terminating at similar but incorrect targets or exceeding the path length limitation. Only the trajectories that successfully reach the correct target with efficient exploration will be retained for subsequent training iterations.",
                "position": 259
            },
            {
                "img": "https://arxiv.org/html/2509.24910/x5.png",
                "caption": "",
                "position": 262
            },
            {
                "img": "https://arxiv.org/html/2509.24910/x6.png",
                "caption": "",
                "position": 266
            },
            {
                "img": "https://arxiv.org/html/2509.24910/x7.png",
                "caption": "",
                "position": 270
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BTransferring to language-guided tasks",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24910/x8.png",
                "caption": "Figure 4:Prompt and Model Output of the Detail-style Captions.",
                "position": 2612
            },
            {
                "img": "https://arxiv.org/html/2509.24910/x9.png",
                "caption": "Figure 5:Prompt and Model Output of the REVERIE-style Captions.",
                "position": 2618
            },
            {
                "img": "https://arxiv.org/html/2509.24910/x10.png",
                "caption": "Figure 6:Prompt and Model Output of the SOON-style Captions.",
                "position": 2623
            }
        ]
    },
    {
        "header": "Appendix CExperimental Details of the downstream navigation tasks.",
        "images": []
    },
    {
        "header": "Appendix DVisualization of the trajectories",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24910/x11.png",
                "caption": "Figure 7:Visualization of the navigation agent’s exploration trajectory and the predicted actions at each step. Right-Bottom: The initial shortest-path trajectory towards the navigation target. Left: The navigation agent’s exploration and error-correction trajectory across different rooms. Right-Up: The navigation agent’s final trajectory and the corresponding image and caption of the target.",
                "position": 2805
            },
            {
                "img": "https://arxiv.org/html/2509.24910/x12.png",
                "caption": "",
                "position": 2814
            },
            {
                "img": "https://arxiv.org/html/2509.24910/x13.png",
                "caption": "",
                "position": 2814
            }
        ]
    },
    {
        "header": "Appendix ELimitation",
        "images": []
    }
]