[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10038/x1.png",
                "caption": "Figure 1:Effect of using Ambient-o for (a) training a text-to-image model (Micro-Diffusion[52]) and (b) a class-conditional model for ImageNet (EDM-2[35]). All generations are initialized with the same noise. The baseline models are trained using all the data equally. Ambient-o changes the way the data is used during the diffusion process based on its quality. This leads to significant visual improvements without sacrificing diversity, as would happen with a filtering approach (see Fig.8).",
                "position": 155
            }
        ]
    },
    {
        "header": "2Background and Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10038/x2.png",
                "caption": "Figure 2:A time-dependent classifier trained to distinguish noisy clean and blurry images(blur kernel standard deviationœÉB=0.6subscriptùúéùêµ0.6\\sigma_{B}=0.6italic_œÉ start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT = 0.6). At low noise the classifier is able to perfectly identify the blurry images, and outputs a probability close to 0. As the noise increases and the information in the image is destroyed, the clean and blurry distributions converge and the classifier outputs a prediction close to0.50.50.50.5. The red line plots the threshold (selected atœÑ=0.45ùúè0.45\\tau=0.45italic_œÑ = 0.45), which is crossed atœÉt=1.64subscriptùúéùë°1.64\\sigma_{t}=1.64italic_œÉ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 1.64.",
                "position": 260
            },
            {
                "img": "https://arxiv.org/html/2506.10038/x3.png",
                "caption": "Figure 3:Visual summary of our method for using low-quality data at high-noise.We see how the various corrupted images become indistinguishable from the High Quality (HQ) after a minimum noise level. These noisy versions of Low Quality (LQ) images are actually high-quality data, which filtering approaches discard, but Ambient Omni uses.",
                "position": 340
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/ambient_logo.png",
                "caption": "Table 1:ImageNet results with and without classifier-free guidance.",
                "position": 386
            },
            {
                "img": "https://arxiv.org/html/2506.10038/x4.png",
                "caption": "High quality images",
                "position": 494
            },
            {
                "img": "https://arxiv.org/html/2506.10038/x4.png",
                "caption": "High quality images",
                "position": 497
            },
            {
                "img": "https://arxiv.org/html/2506.10038/x5.png",
                "caption": "Low quality images",
                "position": 502
            },
            {
                "img": "https://arxiv.org/html/2506.10038/x6.png",
                "caption": "(a) High quality crops",
                "position": 509
            },
            {
                "img": "https://arxiv.org/html/2506.10038/x6.png",
                "caption": "(a) High quality crops",
                "position": 512
            },
            {
                "img": "https://arxiv.org/html/2506.10038/x7.png",
                "caption": "(b) Low quality crops",
                "position": 517
            }
        ]
    },
    {
        "header": "4Theory",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/ambient_logo.png",
                "caption": "Table 2:In a controlled experiment with restricted access only to 10% of the clean dataset, our method of Ambient-o uses corrupted and out-of-distribution data to improve performance.",
                "position": 707
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/ambient_logo.png",
                "caption": "(a)Gaussian blurred data at different levels.",
                "position": 710
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/ood/good_cat.jpg",
                "caption": "Figure 6:Patch level probabilities for dogness in a cat image.",
                "position": 866
            },
            {
                "img": "https://arxiv.org/html/2506.10038/x8.png",
                "caption": "(a)DrawBench",
                "position": 900
            },
            {
                "img": "https://arxiv.org/html/2506.10038/x8.png",
                "caption": "(a)DrawBench",
                "position": 903
            },
            {
                "img": "https://arxiv.org/html/2506.10038/x9.png",
                "caption": "(b)PartiPrompts",
                "position": 908
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/text2image/diversity/battle_nodiffdb-baseline.jpg",
                "caption": "(a)\"the great battle of middle earth, unreal engine, trending on artstation, masterpiece\"",
                "position": 918
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/text2image/diversity/battle_nodiffdb-baseline.jpg",
                "caption": "(a)\"the great battle of middle earth, unreal engine, trending on artstation, masterpiece\"",
                "position": 921
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/text2image/diversity/battle_ours.jpg",
                "caption": "",
                "position": 924
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/text2image/diversity/snowman_nodiffdb-baseline.jpg",
                "caption": "(b)\"an abominable snowman trapped in ice by greg rutkowski\"",
                "position": 931
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/text2image/diversity/snowman_ours.jpg",
                "caption": "",
                "position": 934
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/ambient_logo.png",
                "caption": "(a)Measuring fidelity and prompt alignment of generated images on COCO dataset.",
                "position": 953
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/ambient_logo.png",
                "caption": "(a)Measuring fidelity and prompt alignment of generated images on COCO dataset.",
                "position": 956
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/ambient_logo.png",
                "caption": "(b)Measuring performance on the GenEval benchmark.",
                "position": 989
            }
        ]
    },
    {
        "header": "6Limitations and Future Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATheoretical Results",
        "images": []
    },
    {
        "header": "Appendix BAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/corruptions/cifar10/blur/severity1/img00000000.jpg",
                "caption": "(a)CIFAR-10 images corrupted with blur at increasing levels (œÉB=0.4,0.6,1.0subscriptùúéùêµ0.40.61.0\\sigma_{B}=0.4,0.6,1.0italic_œÉ start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT = 0.4 , 0.6 , 1.0).",
                "position": 2512
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/corruptions/cifar10/blur/severity1/img00000000.jpg",
                "caption": "(a)CIFAR-10 images corrupted with blur at increasing levels (œÉB=0.4,0.6,1.0subscriptùúéùêµ0.40.61.0\\sigma_{B}=0.4,0.6,1.0italic_œÉ start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT = 0.4 , 0.6 , 1.0).",
                "position": 2515
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/corruptions/cifar10/blur/severity1/img00000000.jpg",
                "caption": "",
                "position": 2518
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/corruptions/cifar10/blur/severity1/img00000001.jpg",
                "caption": "",
                "position": 2522
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/corruptions/cifar10/blur/severity1/img00000002.jpg",
                "caption": "",
                "position": 2526
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/corruptions/cifar10/blur/severity2/img00000000.jpg",
                "caption": "",
                "position": 2531
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/corruptions/cifar10/blur/severity2/img00000001.jpg",
                "caption": "",
                "position": 2535
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/corruptions/cifar10/blur/severity2/img00000002.jpg",
                "caption": "",
                "position": 2539
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/corruptions/cifar10/blur/severity3/img00000000.jpg",
                "caption": "",
                "position": 2544
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/corruptions/cifar10/blur/severity3/img00000001.jpg",
                "caption": "",
                "position": 2548
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/corruptions/cifar10/blur/severity3/img00000002.jpg",
                "caption": "",
                "position": 2552
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/corruptions/cifar10/jpeg/severity1/img00000000.jpg",
                "caption": "(b)CIFAR-10 images corrupted with JPEG at compression rates:25%,18%,15%percent25percent18percent1525\\%,18\\%,15\\%25 % , 18 % , 15 %respectively.",
                "position": 2560
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/corruptions/cifar10/jpeg/severity1/img00000000.jpg",
                "caption": "",
                "position": 2563
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/corruptions/cifar10/jpeg/severity1/img00000001.jpg",
                "caption": "",
                "position": 2567
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/corruptions/cifar10/jpeg/severity1/img00000002.jpg",
                "caption": "",
                "position": 2571
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/corruptions/cifar10/jpeg/severity2/img00000000.jpg",
                "caption": "",
                "position": 2576
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/corruptions/cifar10/jpeg/severity2/img00000001.jpg",
                "caption": "",
                "position": 2580
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/corruptions/cifar10/jpeg/severity2/img00000002.jpg",
                "caption": "",
                "position": 2584
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/corruptions/cifar10/jpeg/severity3/img00000000.jpg",
                "caption": "",
                "position": 2589
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/corruptions/cifar10/jpeg/severity3/img00000001.jpg",
                "caption": "",
                "position": 2593
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/corruptions/cifar10/jpeg/severity3/img00000002.jpg",
                "caption": "",
                "position": 2597
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/corruptions/cifar10/motion_blur/severity1/img00000000.jpg",
                "caption": "Figure 12:CIFAR-10 images corrupted with motion blur at increasing levels of corruption.",
                "position": 2606
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/corruptions/cifar10/motion_blur/severity1/img00000000.jpg",
                "caption": "",
                "position": 2609
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/corruptions/cifar10/motion_blur/severity1/img00000001.jpg",
                "caption": "",
                "position": 2613
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/corruptions/cifar10/motion_blur/severity1/img00000002.jpg",
                "caption": "",
                "position": 2617
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/corruptions/cifar10/motion_blur/severity2/img00000000.jpg",
                "caption": "",
                "position": 2622
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/corruptions/cifar10/motion_blur/severity2/img00000001.jpg",
                "caption": "",
                "position": 2626
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/corruptions/cifar10/motion_blur/severity2/img00000002.jpg",
                "caption": "",
                "position": 2630
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/corruptions/cifar10/motion_blur/severity3/img00000000.jpg",
                "caption": "",
                "position": 2635
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/corruptions/cifar10/motion_blur/severity3/img00000001.jpg",
                "caption": "",
                "position": 2639
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/corruptions/cifar10/motion_blur/severity3/img00000002.jpg",
                "caption": "",
                "position": 2643
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/ambient_logo.png",
                "caption": "Table 5:Additional comparison between EDM-2 XXL and our Ambient-o model using the CLIP IQA metric for image quality assesment. Ambient-o leads to improved scores despite using the exact same architecture, data and hyperparameters. For this experiment, we use the models with guidance optimized for DINO FD since they are the ones producing the higher quality images.",
                "position": 2778
            }
        ]
    },
    {
        "header": "Appendix CAmbient diffusion implementation details and loss ablations",
        "images": []
    },
    {
        "header": "Appendix DClassifier annotation ablations",
        "images": []
    },
    {
        "header": "Appendix ETraining Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/imagenet-gen/ours_only.jpg",
                "caption": "Figure 13:Uncurated generations from our Ambient-oXXL model trained on ImageNet.",
                "position": 3104
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/imagenet-gen/ours_only_crops.jpg",
                "caption": "Figure 14:Uncurated generations from our Ambient-o+cropsXXL model trained on ImageNet.",
                "position": 3109
            },
            {
                "img": "https://arxiv.org/html/2506.10038/x10.png",
                "caption": "Figure 15:Amount of samples available at each noise level when training a generative model for dogs in the following setting: (1) we have 10% of the dogs dataset uncorrupted, (2) we have the other 90% of the dogs dataset corrupted with gaussian blur withœÉB=0.6subscriptùúéùêµ0.6\\sigma_{B}=0.6italic_œÉ start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT = 0.6, and (3) we have 100% of the clean dataset of cats. At low noise levels, we can train on both the high quality dogs and a lot of the cats, resulting in>100%absentpercent100>100\\%> 100 %of samples available relative to the original dogs dataset size. As the noise level starts to increase, we stop being able to use to the out-of-distribution cat samples, but start gaining some blurry dog samples. As the noise level approaches the maximum all the blurry dogs become available for training, such that the amount of data available approaches 100%.",
                "position": 3114
            },
            {
                "img": "https://arxiv.org/html/2506.10038/x11.png",
                "caption": "Figure 16:ImageNet-512x512: denoising loss of an optimally trained model, measured at2√ó2222\\times 22 √ó 2center patch, as we increase the context size given to the model (horizontal axis) and the noise level (different curves). As expected, for higher noise, more context is needed for optimal denoising. The large dot on each curve marks the point where the loss nearly plateaus.",
                "position": 3117
            },
            {
                "img": "https://arxiv.org/html/2506.10038/x12.png",
                "caption": "Figure 17:ImageNet-512x512: context size needed to be withinœµ=1‚Å¢e‚àí3italic-œµ1ùëí3\\epsilon=1e-3italic_œµ = 1 italic_e - 3of the optimal loss for different noise levels. As expected, for higher noise, more context is needed for optimal denoising.",
                "position": 3120
            },
            {
                "img": "https://arxiv.org/html/2506.10038/x13.png",
                "caption": "Figure 18:FFHQ: denoising loss of an optimally trained model, measured at2√ó2222\\times 22 √ó 2center patch, as we increase the context size given to the model (horizontal axis) and the noise level (different curves). As expected, for higher noise, more context is needed for optimal denoising. The large dot on each curve marks the point where the loss nearly plateaus.",
                "position": 3123
            },
            {
                "img": "https://arxiv.org/html/2506.10038/x14.png",
                "caption": "Figure 19:FFHQ: context size needed to be withinœµ=1‚Å¢e‚àí3italic-œµ1ùëí3\\epsilon=1e-3italic_œµ = 1 italic_e - 3of the optimal loss for different noise levels. As expected, for higher noise, more context is needed for optimal denoising.",
                "position": 3126
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/ood/bad_cat.jpg",
                "caption": "(a)Cat image and classification probabilities over patches.",
                "position": 3129
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/ood/bad_cat.jpg",
                "caption": "(a)Cat image and classification probabilities over patches.",
                "position": 3132
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/ood/good_cat.jpg",
                "caption": "(b)Cat image and classification probabilities over patches.",
                "position": 3137
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/ood/cat_annotated_8.jpg",
                "caption": "(a)Cat annotated by a cats vs. dogs classifier that operates with crops of size8888.",
                "position": 3144
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/ood/cat_annotated_8.jpg",
                "caption": "(a)Cat annotated by a cats vs. dogs classifier that operates with crops of size8888.",
                "position": 3147
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/ood/cat_annotated_16.jpg",
                "caption": "(b)Cat annotated by a cats vs. dogs classifier that operates with crops of size16161616.",
                "position": 3152
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/ood/cat_annotated_24.jpg",
                "caption": "(c)Cat annotated by a cats vs. dogs classifier that operates with crops of size24242424.",
                "position": 3157
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/ood/synthetic_dog.jpg",
                "caption": "Figure 22:Patch level probabilities for dogness in a synthetic image (procedural program). The cat has more useful patches than this non-realistic procedural program.",
                "position": 3164
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/ood/failed_dog.jpg",
                "caption": "(a)Synthetic image and classification probabilities over patches.",
                "position": 3167
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/ood/failed_dog.jpg",
                "caption": "(a)Synthetic image and classification probabilities over patches.",
                "position": 3170
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/ood/synthetic_dog.jpg",
                "caption": "(b)Synthetic image and classification probabilities over patches.",
                "position": 3175
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/ood/bad_wildlife.jpg",
                "caption": "(a)Cat image and classification probabilities over patches.",
                "position": 3182
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/ood/bad_wildlife.jpg",
                "caption": "(a)Cat image and classification probabilities over patches.",
                "position": 3185
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/ood/good_wildlife.jpg",
                "caption": "(b)Cat image and classification probabilities over patches.",
                "position": 3190
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/ood/batch.jpg",
                "caption": "(a)Example batch.",
                "position": 3197
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/ood/batch.jpg",
                "caption": "(a)Example batch.",
                "position": 3200
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/ood/noisy_batch.jpg",
                "caption": "(b)Noisy batch.",
                "position": 3205
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/clip_annotations/cc12m_top_quality_images.jpg",
                "caption": "(a)Highest quality images from CC12M according to CLIP.",
                "position": 3212
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/clip_annotations/cc12m_top_quality_images.jpg",
                "caption": "(a)Highest quality images from CC12M according to CLIP.",
                "position": 3215
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/clip_annotations/cc12m_bottom_quality_images.jpg",
                "caption": "(b)Lowest quality images from CC12M according to CLIP.",
                "position": 3220
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/clip_annotations/sa1b_top_quality_images.jpg",
                "caption": "(a)Highest quality images from SA1B according to CLIP.",
                "position": 3227
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/clip_annotations/sa1b_top_quality_images.jpg",
                "caption": "(a)Highest quality images from SA1B according to CLIP.",
                "position": 3230
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/clip_annotations/sa1b_bottom_quality_images.jpg",
                "caption": "(b)Lowest quality images from SA1B according to CLIP.",
                "position": 3235
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/clip_annotations/diff_db_top_quality_images.jpg",
                "caption": "(a)Highest quality images from DiffDB according to CLIP.",
                "position": 3242
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/clip_annotations/diff_db_top_quality_images.jpg",
                "caption": "(a)Highest quality images from DiffDB according to CLIP.",
                "position": 3245
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/clip_annotations/diff_db_bottom_quality_images.jpg",
                "caption": "(b)Lowest quality images from DiffDB according to CLIP.",
                "position": 3250
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/clip_annotations/jdb_top_quality_images.jpg",
                "caption": "(a)Highest quality images from JDB according to CLIP.",
                "position": 3257
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/clip_annotations/jdb_top_quality_images.jpg",
                "caption": "(a)Highest quality images from JDB according to CLIP.",
                "position": 3260
            },
            {
                "img": "https://arxiv.org/html/2506.10038/extracted/6526332/figures/clip_annotations/jdb_bottom_quality_images.jpg",
                "caption": "(b)Lowest quality images from JDB according to CLIP.",
                "position": 3265
            },
            {
                "img": "https://arxiv.org/html/2506.10038/x15.png",
                "caption": "Figure 30:Distribution of image qualities according to CLIP for ImageNet-512.",
                "position": 3272
            }
        ]
    },
    {
        "header": "Appendix FAdditional Figures",
        "images": []
    }
]