[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Observations and Motivations",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.00531/x1.png",
                "caption": "(a)",
                "position": 145
            },
            {
                "img": "https://arxiv.org/html/2410.00531/x1.png",
                "caption": "(a)",
                "position": 148
            },
            {
                "img": "https://arxiv.org/html/2410.00531/x2.png",
                "caption": "(b)",
                "position": 153
            },
            {
                "img": "https://arxiv.org/html/2410.00531/x3.png",
                "caption": "(c)",
                "position": 158
            }
        ]
    },
    {
        "header": "3TPI-LLM Framework with Sliding Window Memory Scheduling",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.00531/x4.png",
                "caption": "Figure 2:Overview of the TPI-LLM parallel framework.",
                "position": 195
            },
            {
                "img": "https://arxiv.org/html/2410.00531/x5.png",
                "caption": "Figure 3:Impact of link latencyœÑùúè\\tauitalic_œÑ.",
                "position": 442
            },
            {
                "img": "https://arxiv.org/html/2410.00531/x6.png",
                "caption": "Figure 4:An illustration of the sliding window memory scheduling. Blue blocks indicate the blocks currently executed, with numbered blocks for attention or FFN computing and unnumbered blocks for allreduce communication. Green blocks indicate loaded model weights. The dashed box represents the sliding window, with size 4 in this case.",
                "position": 492
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.00531/x7.png",
                "caption": "Figure 5:Token latency over varying number of devices, CPU cores, and network bandwidth on Llama 2-70B.",
                "position": 839
            },
            {
                "img": "https://arxiv.org/html/2410.00531/x8.png",
                "caption": "Figure 6:Comparison of TPI-LLM with three benchmarks.",
                "position": 846
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Reproducibility",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.00531/x9.png",
                "caption": "Figure 7:Comparison of traffic models for star, tree, and ring-based allreduce algorithms.",
                "position": 1354
            },
            {
                "img": "https://arxiv.org/html/2410.00531/x10.png",
                "caption": "Figure 8:Illustration of the memory window at the peak memory footprint.",
                "position": 1660
            },
            {
                "img": "https://arxiv.org/html/2410.00531/extracted/5892352/figures/testbed.jpg",
                "caption": "Figure 9:Testbed built upon Klonet.",
                "position": 1825
            },
            {
                "img": "https://arxiv.org/html/2410.00531/extracted/5892352/figures/real-testbed.jpg",
                "caption": "Figure 10:A real testbed composed of 4 laptops connected via local Wi-Fi.",
                "position": 2021
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]