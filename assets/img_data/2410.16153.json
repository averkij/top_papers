[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.16153/x1.png",
                "caption": "",
                "position": 124
            },
            {
                "img": "https://arxiv.org/html/2410.16153/x2.png",
                "caption": "Figure 1:Overview of the aggregate performance of various multimodal LLMs onPangeaBench. OurPangea-7B demonstrates comparable performance to SoTA open-source models in English settings, while significantly outperforming them in multilingual scenarios.",
                "position": 145
            },
            {
                "img": "https://arxiv.org/html/2410.16153/x3.png",
                "caption": "Figure 2:Statistics of ourPangeaIns, which comprises 6M multimodal instructions in 39 languages.PangeaInsincludes general instructions, document and chart question answering, captioning, domain-specific, culturally relevant, and text-only instructions. To address the data scarcity of multilingual multimodal instructions, we employ three strategies: 1) translating high-quality English instructions into multiple languages, filtering culturally relevant images; 2) generating culturally aware instructions, and 3) collecting and re-purposing existing open-source multimodal datasets.",
                "position": 148
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2PangeaIns: Multilingual Multimodal Instruction Tuning",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.16153/x4.png",
                "caption": "Figure 3:Overview of multicultural understanding instructions data generation pipeline.",
                "position": 206
            },
            {
                "img": "https://arxiv.org/html/2410.16153/x5.png",
                "caption": "Figure 4:Overview ofPangeaBench, which contains 5 multimodal and 3 text tasks covering 14 datasets (including two newly curated xChatBench and xMMMU datasets). The table provides details about the datasets, while the figure shows evaluation examples from five different multimodal eval tasks in ourPangeaBench.",
                "position": 270
            }
        ]
    },
    {
        "header": "3PangeaBench: Evaluation of Multilingual Multimodal Models",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.16153/x6.png",
                "caption": "Figure 5:Scaling effect of training samples on English and multilingual scores across datasets.",
                "position": 1138
            },
            {
                "img": "https://arxiv.org/html/2410.16153/x7.png",
                "caption": "Figure 6:Impact of English training data proportion on English vs. multilingual performance.",
                "position": 1141
            },
            {
                "img": "https://arxiv.org/html/2410.16153/x8.png",
                "caption": "Figure 7:The relationship between training sample size (relative to English) and performance (relative to English) of different languages across four datasets.",
                "position": 1148
            },
            {
                "img": "https://arxiv.org/html/2410.16153/x9.png",
                "caption": "Figure 8:A preliminary exploration of multilingual OCR.",
                "position": 1158
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelated Work",
        "images": []
    },
    {
        "header": "Appendix BPrompts used in the data construction",
        "images": []
    },
    {
        "header": "Appendix CRecaptioning Example from LAION-Cultural",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.16153/x10.png",
                "caption": "Figure 9:An example from LAION-Cultural illustrating why the filtered informative alt text helps generate a more informative caption. With the high-quality alt text, the model incorporates important details like“President and CEO of The Walt Disney Company standing in front of a model of Shanghai Disneyland”into the generated caption.",
                "position": 2637
            }
        ]
    },
    {
        "header": "Appendix DDatasets used inPangeaBench",
        "images": []
    },
    {
        "header": "Appendix EExplanation of xChatBench",
        "images": []
    },
    {
        "header": "Appendix FQualitative Examples from xChatBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.16153/x11.png",
                "caption": "Figure 10:An xChat example from Japanese subset and our modelPangea’s response. The English part of the instruction, response, and reference answer is additionally added only on the figure to help to understand and was not given nor generated byPangea.",
                "position": 2765
            },
            {
                "img": "https://arxiv.org/html/2410.16153/x12.png",
                "caption": "Figure 11:An xChat example from Hindi subset and our modelPangea’s response. The English part of the instruction, response, and reference answer is additionally added only on the figure to help to understand and was not given nor generated byPangea.",
                "position": 2769
            },
            {
                "img": "https://arxiv.org/html/2410.16153/x13.png",
                "caption": "Figure 12:An xChat example from the Korean subset and our modelPangea’s response. The English part of the instruction, response, and reference answer is additionally added only on the figure to help to understand and was not given nor generated byPangea.",
                "position": 2773
            },
            {
                "img": "https://arxiv.org/html/2410.16153/x14.png",
                "caption": "Figure 13:An xChat example from the Indonesian subset and our modelPangea’s response. The English part of the instruction, response, and reference answer is additionally added only on the figure to help to understand and was not given nor generated byPangea.",
                "position": 2777
            },
            {
                "img": "https://arxiv.org/html/2410.16153/x15.png",
                "caption": "Figure 14:An xChat example from Spanish subset and our modelPangea’s response. The English part of the instruction, response, and reference answer is additionally added only on the figure to help to understand and was not given nor generated byPangea.",
                "position": 2781
            },
            {
                "img": "https://arxiv.org/html/2410.16153/x16.png",
                "caption": "Figure 15:An xChat example from the Chinese subset and our modelPangea’s response. The English part of the instruction, response, and reference answer is additionally added only on the figure to help to understand and was not given nor generated byPangea.",
                "position": 2785
            },
            {
                "img": "https://arxiv.org/html/2410.16153/x17.png",
                "caption": "Figure 16:An xChat example from the Chinese subset and our modelPangea’s response. The English part of the instruction, response, and reference answer is additionally added only on the figure to help to understand and was not given nor generated byPangea.",
                "position": 2789
            },
            {
                "img": "https://arxiv.org/html/2410.16153/x18.png",
                "caption": "Figure 17:An xChat example from the Korean subset and our modelPangea’s response. The English part of the instruction, response, and reference answer is additionally added only on the figure to help to understand and was not given nor generated byPangea.",
                "position": 2793
            }
        ]
    },
    {
        "header": "Appendix GTraining Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.16153/x19.png",
                "caption": "Figure 18:Translated Task: An example from the Cambrian dataset where it discusses the concept of hourly wages based on a given prompt in Russian.",
                "position": 2818
            },
            {
                "img": "https://arxiv.org/html/2410.16153/x20.png",
                "caption": "Figure 19:Multimodal Translated Task: An example from the ALLaVa-LAION dataset where the GPT model answers a prompt in Turkish regarding the maximum time displayed on a digital timer.",
                "position": 2823
            },
            {
                "img": "https://arxiv.org/html/2410.16153/x21.png",
                "caption": "Figure 20:Multimodal Translated Task: An example from the ShareGPT-4v dataset where the model describes an image of a wine rack in Thai, detailing its structure and the arrangement of wine bottles.",
                "position": 2829
            },
            {
                "img": "https://arxiv.org/html/2410.16153/x22.png",
                "caption": "Figure 21:Text-only Translated Task: An example from the OpenHermes2.5 dataset translated into Japanese.",
                "position": 2834
            },
            {
                "img": "https://arxiv.org/html/2410.16153/x23.png",
                "caption": "Figure 22:Text-only Translated Task: An example from the NumininaMath dataset translated into Spanish.",
                "position": 2840
            },
            {
                "img": "https://arxiv.org/html/2410.16153/x24.png",
                "caption": "Figure 23:Text-only Translated Task: An example from the Code-Feedback dataset translated into German.",
                "position": 2845
            },
            {
                "img": "https://arxiv.org/html/2410.16153/x25.png",
                "caption": "Figure 24:Multicultural Understanding: An example from the LAION-Culture dataset where the GPT model describes and analyzes the Petrobras logo in Portuguese.",
                "position": 2851
            },
            {
                "img": "https://arxiv.org/html/2410.16153/x26.png",
                "caption": "Figure 25:Multicultural Understanding: Example from the LAION-Culture dataset where the model interprets the meaning of a Japanese character and its representation in calligraphy, described in Bulgarian.",
                "position": 2854
            },
            {
                "img": "https://arxiv.org/html/2410.16153/x27.png",
                "caption": "Figure 26:Multicultural Understanding: LAION-Culture sample in Hebrew.",
                "position": 2860
            },
            {
                "img": "https://arxiv.org/html/2410.16153/x28.png",
                "caption": "Figure 27:Multicultural Understanding: LAION-Culture sample in Tamil.",
                "position": 2865
            }
        ]
    },
    {
        "header": "Appendix HBreakdown Results of Different Languages onPangeaBench",
        "images": []
    },
    {
        "header": "Appendix IA Preliminary Exploration of Constructing Multilingual OCR Instructions",
        "images": []
    }
]