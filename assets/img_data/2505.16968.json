[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.16968/extracted/6476954/assets/cass_logo.png",
                "caption": "",
                "position": 97
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.16968/x1.png",
                "caption": "Figure 1:CASSPipeline: We collect CUDA code from public repositories and synthesize additional samples via prompt-based LLM generation. After filtering and deduplication, all CUDA files are translated to HIP using HIPIFY, then compiled to extract host and device assembly. Matched outputs form theCASSdataset with aligned source and assembly pairs across Nvidia and AMD stacks.",
                "position": 318
            }
        ]
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.16968/x2.png",
                "caption": "Figure 2:The Nvidia (left) and AMD (right) stacks illustrate the compilation process for CUDA and HIP. Blue denotes device-side steps; green denotes host-side steps. Nvidia’s stack is opaque; accessing device assembly (SASS) requires first compiling to binary, then usingcuobjdump. In contrast, AMD’s process is transparent, allowing direct inspection and modification of device assembly (RDNA3) before host integration.",
                "position": 369
            }
        ]
    },
    {
        "header": "4CASS-Instruct and CASS-Bench Datasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.16968/x3.png",
                "caption": "Figure 3:CASScoverage across dataset and benchmark (left) domain distribution of training samples (right) category distribution inCASS-Bench.",
                "position": 390
            },
            {
                "img": "https://arxiv.org/html/2505.16968/x3.png",
                "caption": "",
                "position": 393
            },
            {
                "img": "https://arxiv.org/html/2505.16968/x4.png",
                "caption": "",
                "position": 397
            },
            {
                "img": "https://arxiv.org/html/2505.16968/x5.png",
                "caption": "(a)",
                "position": 409
            },
            {
                "img": "https://arxiv.org/html/2505.16968/x5.png",
                "caption": "(a)",
                "position": 412
            },
            {
                "img": "https://arxiv.org/html/2505.16968/x6.png",
                "caption": "(b)",
                "position": 417
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.16968/x7.png",
                "caption": "Figure 5:Source and assembly-level accuracy across categories.",
                "position": 596
            },
            {
                "img": "https://arxiv.org/html/2505.16968/x7.png",
                "caption": "Figure 5:Source and assembly-level accuracy across categories.",
                "position": 599
            },
            {
                "img": "https://arxiv.org/html/2505.16968/x8.png",
                "caption": "Figure 6:t-SNE projection of CUDA and HIP assembly embeddings.",
                "position": 604
            }
        ]
    },
    {
        "header": "7Limitations and Future Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.16968/extracted/6476954/assets/Asm_loc_vs_source_loc.png",
                "caption": "Figure 7:Relationship between source and assembly-level LoC in the CASS dataset. Scatter plot comparing source code lines of code (LoC) to the corresponding assembly LoC for both CUDA and HIP backends across the Stackv2 and Synthetic subsets. Trend lines and density contours illustrate that CUDA typically produces more verbose assembly output than HIP for equivalent source sizes.",
                "position": 1218
            },
            {
                "img": "https://arxiv.org/html/2505.16968/extracted/6476954/assets/graph5.png",
                "caption": "Figure 8:Opcode Category Distribution by Dataset and Code Type. Stacked bar chart showing the distribution of assembly instructions across 10 opcode categories for device and host code in the Synthetic, Stackv2, and OpenCL subsets. Each bar represents a (dataset, code type) pair, illustrating the functional composition of the code across memory, tensor, control flow, synchronization, and other operations.",
                "position": 1221
            },
            {
                "img": "https://arxiv.org/html/2505.16968/extracted/6476954/assets/hip_word_cloud.png",
                "caption": "(a)HIP assembly opcodes.",
                "position": 1224
            },
            {
                "img": "https://arxiv.org/html/2505.16968/extracted/6476954/assets/hip_word_cloud.png",
                "caption": "(a)HIP assembly opcodes.",
                "position": 1227
            },
            {
                "img": "https://arxiv.org/html/2505.16968/extracted/6476954/assets/cuda_word_cloud.png",
                "caption": "(b)CUDA assembly opcodes.",
                "position": 1232
            },
            {
                "img": "https://arxiv.org/html/2505.16968/x9.png",
                "caption": "Figure 10:Accuracy vs. training steps for source/assembly across CASS model scales (1.5B, 3B, 7B).",
                "position": 1239
            },
            {
                "img": "https://arxiv.org/html/2505.16968/x10.png",
                "caption": "Figure 11:Comparison of memory usage (left) and execution time (right) between predicted and ground truth HIP programs, measured via compilation and runtime profiling.",
                "position": 1242
            },
            {
                "img": "https://arxiv.org/html/2505.16968/x10.png",
                "caption": "",
                "position": 1245
            },
            {
                "img": "https://arxiv.org/html/2505.16968/x11.png",
                "caption": "",
                "position": 1249
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]