[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13444/extracted/6287740/assets/icon.png",
                "caption": "",
                "position": 71
            },
            {
                "img": "https://arxiv.org/html/2503.13444/x1.png",
                "caption": "Figure 1:An illustration of VideoMind’s Chain-of-LoRA reasoning strategyapplied to a complex question for a 50-min long video. The problem is decomposed by Planner and distributed to Grounder, Verifier, and Answerer to systematically localize, verify, and interpret the relevant video moments. Such a role-based pipeline enables more human-like video reasoning compared with the pure textual CoT process.",
                "position": 87
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13444/x2.png",
                "caption": "Figure 2:The overall workflow of VideoMind. Given a video and a query, VideoMind adaptively activates different roles (Planner→→\\to→Grounder→→\\to→Verifier→→\\to→Answerer in this case) and perform step-by-step reasoning by calling individual modules.",
                "position": 138
            }
        ]
    },
    {
        "header": "3VideoMind",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13444/x3.png",
                "caption": "Figure 3:The Planner coordinates other roles based on the query, providing three modes and rephrasing tailored for different needs.",
                "position": 175
            },
            {
                "img": "https://arxiv.org/html/2503.13444/x4.png",
                "caption": "Figure 4:Detailed architecture of the timestamp decoder.This module accepts hidden states of both the frame tokens and the<REG>token, decoding them into the start and end timestamps.",
                "position": 241
            },
            {
                "img": "https://arxiv.org/html/2503.13444/x5.png",
                "caption": "Figure 5:The grounder generates multiple candidate moments, which are then refined by applying azoom-instrategy and evaluated byVerifierto select the best moment.",
                "position": 337
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13444/x6.png",
                "caption": "Table 4:Grounded VideoQA on NExT-GQA[88].VideoMind beats both agent-based solutions[99]and end-to-end methods[92]trained with large-scale data. Notably, our 2B model is comparable with 7B counterparts.",
                "position": 1179
            },
            {
                "img": "https://arxiv.org/html/2503.13444/x6.png",
                "caption": "Figure 6:Visualization of the VideoMind workflow.The Planner first determines the need for function calls and generates several candidate moments using the Grounder. It then applies the Verifier to select the most relevant video segment (highlighted in yellow). After zooming in, the segment is passed to the Answerer. By chaining the Grounder, Verifier, and Answerer roles, VideoMind accurately localizes the critical moment and selects the correct answer, thereby avoiding confusion from an incorrect segment (red box).",
                "position": 2127
            }
        ]
    },
    {
        "header": "5Conclusion and Limitations",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AModel Details",
        "images": []
    },
    {
        "header": "Appendix BExperimental Details",
        "images": []
    },
    {
        "header": "Appendix CPrompt Templates",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]