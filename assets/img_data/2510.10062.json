[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10062/x1.png",
                "caption": "Figure 1:Overall ranking of human performance versus 13 embedding models across 16 tasks. Human annotators achieve 4th place with a score of 77.6, demonstrating competitive but not dominant performance. The ranking reveals significant variation in model performance across different parameter scales and architectures. Darker shades of blue means a larger model.",
                "position": 128
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10062/x2.png",
                "caption": "Figure 2:Comprehensive view of human performance relative to all model performance ranges across 16 tasks by language.",
                "position": 812
            },
            {
                "img": "https://arxiv.org/html/2510.10062/x3.png",
                "caption": "Figure 3:Human win rates across task categories and languages. Top left: By task category shows humans perform moderately in classification but struggle in clustering, reranking, and STS against best models. Top right: English-only vs multilingual tasks reveals humans perform better on multilingual tasks (29% vs 0% against best models). Bottom left: Performance varies dramatically by baseline comparison (15% vs best, 62% vs mean models). Bottom right: Language-specific breakdown shows varying performance across different language codes.",
                "position": 821
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetailed Results by Task Category",
        "images": []
    },
    {
        "header": "Appendix BTask Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10062/x4.png",
                "caption": "Figure 4:Emotion Classification annotation interface showing the 6-category emotion labeling task. This task achieved fair inter-annotator agreement (κ=0.39\\kappa=0.39) due to ambiguous emotional states and mixed emotions in social media text. Human performance: 45.8%, Best model: 87.1%.",
                "position": 2349
            },
            {
                "img": "https://arxiv.org/html/2510.10062/x5.png",
                "caption": "Figure 5:Tweet Sentiment Classification annotation interface demonstrating sentiment polarity annotation. This task achieved moderate inter-annotator agreement (κ=0.48\\kappa=0.48) with reasonable consensus on positive/negative sentiment. Human performance: 84.4%, Best model: 90.9%.",
                "position": 2352
            },
            {
                "img": "https://arxiv.org/html/2510.10062/x6.png",
                "caption": "Figure 6:ArXiv Clustering annotation interface showing academic papers that caused complete annotator disagreement (ARI=−0.001\\text{ARI}=-0.001) due to interdisciplinary research overlap. Papers could be categorized by methodology, application domain, or research community, leading to fundamental disagreement. Human performance: 49.2%, Best model: 84.6%.",
                "position": 2355
            },
            {
                "img": "https://arxiv.org/html/2510.10062/x7.png",
                "caption": "Figure 7:Reddit Clustering annotation interface demonstrating thematic grouping of discussion topics. This task achieved fair agreement (ARI=0.34\\text{ARI}=0.34) due to overlapping themes across different discussion topics. Human performance: 68.8%, Best model: 100%.",
                "position": 2358
            },
            {
                "img": "https://arxiv.org/html/2510.10062/x8.png",
                "caption": "Figure 8:SIB200 Clustering annotation interface showing multilingual sentence clustering task. This task achieved moderate inter-annotator agreement (ARI=0.42\\text{ARI}=0.42) with variation across languages depending on cultural context and sentence complexity.",
                "position": 2361
            },
            {
                "img": "https://arxiv.org/html/2510.10062/x9.png",
                "caption": "Figure 9:Robust04 Reranking annotation interface showing document relevance assessment for information retrieval queries. This task achieved strong inter-annotator agreement (ρ=0.72\\rho=0.72). Human performance: 88.5%, Best model: 98.8%.",
                "position": 2364
            },
            {
                "img": "https://arxiv.org/html/2510.10062/x10.png",
                "caption": "Figure 10:Wikipedia Multilingual Reranking annotation interface demonstrating cross-lingual relevance judgment. This task achieved moderate agreement (ρ=0.64\\rho=0.64) due to cross-lingual complexity.",
                "position": 2367
            },
            {
                "img": "https://arxiv.org/html/2510.10062/x11.png",
                "caption": "Figure 11:STS12 annotation interface showing semantic similarity assessment using a 0-5 scale. This well-curated dataset achieved strong inter-annotator agreement (ρ=0.77\\rho=0.77). Human performance: 91.2%, Best model: 92.0%.",
                "position": 2370
            },
            {
                "img": "https://arxiv.org/html/2510.10062/x12.png",
                "caption": "Figure 12:SICK-R annotation interface showing semantic relatedness and entailment task. This task achieved moderate agreement (ρ=0.68\\rho=0.68) due to task complexity. Human performance: 82.6%, Best model: 94.1%.",
                "position": 2373
            }
        ]
    },
    {
        "header": "Appendix CTask Quality Analysis",
        "images": []
    },
    {
        "header": "Appendix DInter-Annotator Agreement Analysis",
        "images": []
    },
    {
        "header": "Appendix EAdditional Human vs Model Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10062/x13.png",
                "caption": "Figure 13:Human performance gaps versus best-performing models across 26 task-language pairs. Humans outperform the best models on only 4 tasks (15.4%), with largest advantages in Arabic semantic similarity and sentiment analysis. The analysis reveals systematic model advantages in technical domains (clustering, reranking) versus human advantages in culturally-informed tasks.",
                "position": 3219
            },
            {
                "img": "https://arxiv.org/html/2510.10062/x14.png",
                "caption": "Figure 14:Human performance gaps versus median-performing models across 26 tasks by language. Humans achieve 61.5% win rate against median models, demonstrating competitive performance when compared to typical rather than best-performing models. This analysis reveals that human performance is much more competitive when compared against representative model performance rather than cherry-picked best results.",
                "position": 3222
            },
            {
                "img": "https://arxiv.org/html/2510.10062/x15.png",
                "caption": "Figure 15:Task difficulty categorization based on human performance levels. The majority of tasks (69%) fall into the “easy” category (human performance≥0.7\\geq 0.7), shown ingreen. Only two tasks fall below0.50.5(shown inred), both with notably low inter-annotator agreement,\nsuggesting fundamental task ambiguity rather than limitations of human ability.",
                "position": 3225
            },
            {
                "img": "https://arxiv.org/html/2510.10062/x16.png",
                "caption": "Figure 16:Model consistency analysis showing performance ranges across tasks.Higher value indicates greater variability across models(lower consistency).\nTasks with small ranges (high consistency) often align with high human agreement,\nwhereas tasks with large ranges (low consistency) typically correspond to tasks where humans also struggle.\nThis pattern suggests that both human and model performance reflect underlying task quality and clarity of task specification.",
                "position": 3229
            }
        ]
    },
    {
        "header": "Appendix FModels Evaluated",
        "images": []
    },
    {
        "header": "Appendix GLLM Usage Statement",
        "images": []
    }
]