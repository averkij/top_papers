[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.06752/x1.png",
                "caption": "",
                "position": 105
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.06752/x2.png",
                "caption": "Figure 2:Synthetic data generation.We curate 10 diverse 3D assets, and render them in diverse layouts and orientations in Blender[15]. The rendered scenes are augmented with realistic generations from Canny[8]ControlNet[68]. The final dataset consists of one and two object scenes.",
                "position": 229
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.06752/x3.png",
                "caption": "Figure 3:Compass Control.Given an orientation angleŒ∏jsubscriptùúÉùëó\\theta_{j}italic_Œ∏ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT, we project it to acompasstoken with a lightweight encoder model. Thecompasstokens are interleaved with the text tokens (as shown in the figure) and passed through the text encoder. The outputs of the text encoder are used to condition the denoising process in the U-Net. We trainùí´ùí´\\mathcal{P}caligraphic_Pand also fine-tune the U-Net using LoRA[26].",
                "position": 251
            },
            {
                "img": "https://arxiv.org/html/2504.06752/x4.png",
                "caption": "Figure 4:Binding thecompasstokens:We visualize the averaged cross attention of thecompasstoken(s) when training with CALL (shown on the left) and without it (shown on the right). CALL localizes the influence of thecompasstoken at thecorrectregions, which (a) improves orientation control (b) disentangles orientations in multi-object scenes. In (b),ùêúùüèsubscriptùêú1\\mathbf{c_{1}}bold_c start_POSTSUBSCRIPT bold_1 end_POSTSUBSCRIPTandùêúùüêsubscriptùêú2\\mathbf{c_{2}}bold_c start_POSTSUBSCRIPT bold_2 end_POSTSUBSCRIPTare compass tokens for car and motorbike, respectively.",
                "position": 297
            },
            {
                "img": "https://arxiv.org/html/2504.06752/x5.png",
                "caption": "Figure 5:Staged trainingresults in improved adherence of objects to the bounding boxes, leading to orientation learning.",
                "position": 310
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.06752/x6.png",
                "caption": "Figure 6:Main Results.Compass Controlgenerates complex scenes aligned with the text prompts and the orientations (shown asfrustum, the colored face is forward direction). It generalizes well to unseen object categories -pirate ship, boat, yacht, bicycle, pram, dog, and even human. Further, it generates high-quality compositions of several objects, despite beingtrained only on one and two object scenes.",
                "position": 318
            },
            {
                "img": "https://arxiv.org/html/2504.06752/x7.png",
                "caption": "Figure 7:Qualitative Comparison.We compare our method against three baselines. Cont-3D-words[14]does not generate the intended object whereas View-NeTI[7]generates objects in plain backgrounds. LooseControl[4]generates realistic compositions but does not follow the input orientation well. In contrast, our method aligns with the input text prompt and follows the input orientation, while generating realistic scenes.",
                "position": 348
            },
            {
                "img": "https://arxiv.org/html/2504.06752/x8.png",
                "caption": "Figure 8:User study.We compare all methods on the three image metrics. Each bar indicates the fraction of people that preferred our result (gray) vs the baseline (other color).",
                "position": 408
            },
            {
                "img": "https://arxiv.org/html/2504.06752/x9.png",
                "caption": "Figure 9:Personalization.Given a few (‚âà10absent10\\approx 10‚âà 10) unposed images of an object, our method can personalize the diffusion models and allow for orientation control of the new object. Notably, our method can also generate scenes with two personalized objects with precise orientation control. Additionally, we compare our method with CustomDiffusion-360[33]that uses‚âà100absent100\\approx 100‚âà 100posed images.",
                "position": 421
            },
            {
                "img": "https://arxiv.org/html/2504.06752/x10.png",
                "caption": "Figure 10:Ablation studies.We show the impact of several design\nchoices of our approach. Refer Sec.4.5for details.",
                "position": 434
            }
        ]
    },
    {
        "header": "5Conclusion and Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.06752/x11.png",
                "caption": "Figure 11:Failure Cases",
                "position": 453
            }
        ]
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProject page",
        "images": []
    },
    {
        "header": "Appendix BControlling 3D orientation",
        "images": []
    },
    {
        "header": "Appendix CAdditional Control",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.06752/x12.png",
                "caption": "Figure 12:Additional Controls: (Top) Conditioning with camera elevation angle. (Bottom) Conditioning on object scale.",
                "position": 1463
            },
            {
                "img": "https://arxiv.org/html/2504.06752/x13.png",
                "caption": "Figure 13:Conditioning on all three orientation angles for a single object.",
                "position": 1466
            },
            {
                "img": "https://arxiv.org/html/2504.06752/x14.png",
                "caption": "Figure 14:Conditioning on all three orientation angles",
                "position": 1469
            }
        ]
    },
    {
        "header": "Appendix DGeneralization to StableDiffusion-XL",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.06752/x15.png",
                "caption": "Figure 15:Compass Control on StableDiffusion-XL.",
                "position": 1479
            }
        ]
    },
    {
        "header": "Appendix EDiverse poses for non-rigid objects",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.06752/x16.png",
                "caption": "Figure 16:Pose variations for non-rigid objects",
                "position": 1489
            }
        ]
    },
    {
        "header": "Appendix FRobustness to the 2D bounding boxes",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.06752/x17.png",
                "caption": "Figure 17:Robustness of 2D bounding boxes.Our method generates realistic scene compositions with different 2D bounding box sizes. Allowing for a loose bounding box during training provides this flexibility to the model to generate realistic scenes while coarsely following the input 2D box. Further, random non-overlapping boxes can also be spawned during inference without any degradation in quality. This robustness to the actual bounding box shape, reduces the burden on the user and is enabled by theloosebounding box used during training.",
                "position": 1505
            },
            {
                "img": "https://arxiv.org/html/2504.06752/x18.png",
                "caption": "Figure 18:Overlapping bounding boxes.Our method can handle overlap between the two input bounding boxes up to a good extent. However, with a large overlap, the model struggles to generate accurate orientations (jeep in the fourth example), due to the mixing of pose tokens.",
                "position": 1508
            }
        ]
    },
    {
        "header": "Appendix GDiscussion with SoTA object-centric works.",
        "images": []
    },
    {
        "header": "Appendix HSynthetic data generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.06752/x19.png",
                "caption": "Figure 19:Samples from data generation process",
                "position": 1591
            }
        ]
    },
    {
        "header": "Appendix IOrientation Regressor",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.06752/x20.png",
                "caption": "Figure 20:Predictions of the trained orientation regressor on unseen samples generated from Stable Diffusion[51]. The model can predict the orientations accurately for the diverse unseen data and acts as a good critic to evaluate orientation consistency in generated images.",
                "position": 1695
            }
        ]
    },
    {
        "header": "Appendix JBaseline details",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.06752/x21.png",
                "caption": "Figure 21:Comparsison with modified Continuous 3D Words[14]trained on multiple assets. Compass control generates more realistic outputs and follows the text prompt better than the Cont-3D-Words trained on multiple object datasets.",
                "position": 1720
            }
        ]
    },
    {
        "header": "Appendix KAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.06752/x22.png",
                "caption": "Figure 22:Additional comparison results with the baselines for single object and multi-object scenes.",
                "position": 1745
            },
            {
                "img": "https://arxiv.org/html/2504.06752/x23.png",
                "caption": "Figure 23:More qualitative results from our method,Compass Control.",
                "position": 1748
            }
        ]
    },
    {
        "header": "Appendix LImplementation Details",
        "images": []
    }
]