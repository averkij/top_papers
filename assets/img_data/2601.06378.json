[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.06378/x1.png",
                "caption": "Figure 1:Overview of theRigMo-VAEframework.\nGiven temporal vertex trajectories from deforming mesh sequences, RigMo employs a dual-path encoder to disentangle static geometry (rigging branch) and dynamic motion (motion branch), learning a compact latent representation that captures both spatial structure and temporal dynamics.\nThe decoder maps these latent features to physically interpretable rig components: Gaussian bone descriptors defining geodesic-aware skinning weights and variational motion parameters for local and root transformations.\nDifferent colors indicate the influence regions of learned Gaussian bones, demonstrating semantically meaningful decomposition of mesh deformation without manual rigging supervision.",
                "position": 116
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.06378/x2.png",
                "caption": "Figure 2:Overview of theMotion DiT. Given static rigging features, a condition encoder produces anchor and global tokens that guide a diffusion transformer operating in RigMoâ€™s motion-latent space. The model uses spatial, temporal, and frame-conditioned cross-attention to predict denoised motion latents, which are decoded into bone transformations and vertex sequences via Gaussian skinning.",
                "position": 295
            },
            {
                "img": "https://arxiv.org/html/2601.06378/x3.png",
                "caption": "Figure 3:Results produced by the full RigMo. Given a sparse input sequence, where a subset of frames is observed according to a frame mask, RigMo reconstructs a complete animatable model by jointly predicting the rigging structure (Gaussian bones and skinning weights) and synthesizing the missing motion frames through diffusion in the RigMo latent space. The resulting rigged model produces coherent, articulated motion across humans, animals, and diverse non-human shapes, demonstrating that sparse observations are sufficient to recover a full animation without category-specific priors.",
                "position": 420
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.06378/x4.png",
                "caption": "Figure 4:Comparison between UniRig+Optimization and our RigMo Rigging Module.\nAlthough UniRig may produce visually plausible skinning weights in some cases (e.g., the fox),\nits rigging does not generalize and collapses under actual animation, leading to severe deformation artifacts.\nIn contrast, RigMo learns robust and transferable rig structuresdirectly from motion, without any ground-truth rig supervision, and achieves stable, high-fidelity deformations across diverse poses and animal species.",
                "position": 471
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Overview of Supplementary Materials",
        "images": []
    },
    {
        "header": "Comparison with prior work.",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.06378/x5.png",
                "caption": "Figure 5:Side-by-side skinning weights comparisons of the 48-token and 128-token configurations.",
                "position": 1697
            }
        ]
    },
    {
        "header": "MotionDiT Evaluation",
        "images": []
    }
]