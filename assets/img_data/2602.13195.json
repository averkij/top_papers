[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13195/figures/teaser.png",
                "caption": "",
                "position": 128
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13195/figures/qualitative-benchmark.png",
                "caption": "Figure 2:ConverSegQualitative Examples.Representative samples across five concept categories. Prompts require reasoning about attributes, spatial relations, interactions, functional properties, and physical constraints beyond standard object reference.",
                "position": 193
            },
            {
                "img": "https://arxiv.org/html/2602.13195/figures/concept-coverage.png",
                "caption": "Figure 3:Concept Coverage in Benchmarks.Distribution of concepts across five existing benchmarks versusConverSeg. Prior datasets primarily focus on entities/spatial relations, whereasConverSegoffers near-uniform coverage across all five concepts.",
                "position": 218
            },
            {
                "img": "https://arxiv.org/html/2602.13195/figures/dataengine.png",
                "caption": "Figure 4:Conversational Data Engine.Five-stage pipeline for automated concept-mask synthesis: (1) VLM generates region descriptions; (2) Object detector and SAM2 produce masks, VLM filters and refines them. (3) Concept-driven meta-prompts generate conversational queries assigned to region-mask pairs; (4) Alignment verification ensures prompt-mask correctness.",
                "position": 222
            }
        ]
    },
    {
        "header": "3Conversational Image Segmentation",
        "images": []
    },
    {
        "header": "4TheConverSegBenchmark",
        "images": []
    },
    {
        "header": "5The Conversational Data Engine",
        "images": []
    },
    {
        "header": "6Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13195/figures/model.png",
                "caption": "Figure 5:Model Architecture.An image encoder processes the image, while a VLM jointly encodes image and text. Lightweight adapters map the text-token embeddings from the VLM to a mask decoder that predicts the target segment.",
                "position": 411
            }
        ]
    },
    {
        "header": "7Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13195/figures/qualitative-predictions.png",
                "caption": "Figure 6:Qualitative results onConverSeg.ConverSeg-Netproduces more accurate masks for spatial, affordance, and physics concepts than LISA, a popular leading reasoning segmentation model.",
                "position": 737
            },
            {
                "img": "https://arxiv.org/html/2602.13195/figures/attention-maps.png",
                "caption": "Figure 7:Cross-attention maps showing that each text prompt focuses on its corresponding image region.",
                "position": 1417
            }
        ]
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AQualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13195/supp-figures/ood.png",
                "caption": "Figure 8:Out-of-distribution qualitative examples.Predictions ofConverSeg-Neton images from the DROID dataset[14]and the Warehouse dataset[25].\nFor each example, we show the input image, conversational prompt, and the predicted mask overlaid.ConverSeg-Netoften localizes the regions implied by the prompts despite the distribution shift, hinting at prospective applications in domestic and warehouse robotics.",
                "position": 2061
            },
            {
                "img": "https://arxiv.org/html/2602.13195/supp-figures/refcoco-noisy.png",
                "caption": "Figure 9:Noisy annotations in RefCOCO/+/g.ConverSeg-Netpredictions (blue) are semantically reasonable but receive low gIoU due to problematic ground truth masks (pink): incomplete coverage, irrelevant regions, or poor boundaries.",
                "position": 2081
            }
        ]
    },
    {
        "header": "Appendix BConversational Data Engine Details",
        "images": []
    },
    {
        "header": "Appendix CBenchmark Construction and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13195/supp-figures/annotation-ui.png",
                "caption": "Figure 10:Annotation interface for constructingConverSeg.Annotators are shown the input image with the AI-generated mask overlaid, along with the corresponding conversational prompt and the suggested decision from the AI verifier.\nThey then decide whether the prompt and mask are semantically aligned and select eitherAccept/SelectorReject/Discard; rejected examples are discarded without further editing.",
                "position": 2155
            },
            {
                "img": "https://arxiv.org/html/2602.13195/supp-figures/human_annotated_bar.png",
                "caption": "(a)Human-annotated split",
                "position": 2201
            },
            {
                "img": "https://arxiv.org/html/2602.13195/supp-figures/human_annotated_bar.png",
                "caption": "(a)Human-annotated split",
                "position": 2204
            },
            {
                "img": "https://arxiv.org/html/2602.13195/supp-figures/sam_seeded_bar.png",
                "caption": "(b)SAM-seeded split",
                "position": 2207
            },
            {
                "img": "https://arxiv.org/html/2602.13195/figures/rebuttal-non-instance.png",
                "caption": "Figure 12:Examples of non-instance regions inConverSeg. The masks capture object parts, surfaces, and functional areas, demonstrating coverage of diverse region types beyond complete object instances.",
                "position": 2214
            },
            {
                "img": "https://arxiv.org/html/2602.13195/supp-figures/negative-examples.png",
                "caption": "Figure 13:Examples of negative prompts generated by our data engine.Each row shows an image with its corresponding negative prompt. The correct model response for all these prompts is an empty mask.",
                "position": 2236
            },
            {
                "img": "https://arxiv.org/html/2602.13195/supp-figures/before-after-negatives-compressed.png",
                "caption": "Figure 14:Impact of negative training data.Model predictions before and after negative training on adversarial prompts. After training, the model correctly produces empty masks for invalid prompts. Robustness also transfers to simpler literal negatives (e.g.\"Segment the tiger\"), despite not being explicitly trained on them.",
                "position": 2239
            },
            {
                "img": "https://arxiv.org/html/2602.13195/supp-figures/verifier-robustness.png",
                "caption": "Figure 15:Qualitative examples of VLM verifier behavior,illustrating agreement and disagreement with human annotators on candidate prompt–mask pairs inConverSeg.",
                "position": 2242
            },
            {
                "img": "https://arxiv.org/html/2602.13195/supp-figures/supp-benchmark-qualitative.png",
                "caption": "Figure 16:Additional qualitative examples fromConverSeg.Each panel shows an input image, its conversational prompt, and the corresponding ground-truth mask overlaid.\nExamples span all concept families (entities, spatial & layout, relations & events, affordances & functions, physics & safety) and belong to the human-annotated split.",
                "position": 2245
            }
        ]
    },
    {
        "header": "Appendix DImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13195/supp-figures/region-description-meta-prompt.png",
                "caption": "Figure 17:Meta-prompt for Stage 1 (Scene Understanding).Prompt template used to query Gemini-2.5-Flash to produce detailed region-level descriptions of the scene, which later serve as the semantic basis for mask generation and concept-driven prompt construction.",
                "position": 3661
            },
            {
                "img": "https://arxiv.org/html/2602.13195/supp-figures/meta-prompt-mask-verify.png",
                "caption": "Figure 18:Meta-prompt for Stage 3 mask–text consistency checking.Prompt template used to ask the VLM whether a candidate mask is consistent with its associated region description, enabling automatic filtering of low-quality or mismatched masks.",
                "position": 3665
            },
            {
                "img": "https://arxiv.org/html/2602.13195/supp-figures/meta-prompt-mask-compare.png",
                "caption": "Figure 19:Meta-prompt for Stage 3 mask refinement and selection.Prompt template used to compare two candidate masks for the same region description and select the most appropriate one, based on coverage, tightness, and semantic alignment.",
                "position": 3669
            },
            {
                "img": "https://arxiv.org/html/2602.13195/supp-figures/meta-prompt-affordances.png",
                "caption": "Figure 20:Meta-prompt for Stage 4 (Affordances & Functions).Concept-specific prompt template used to turn region descriptions into conversational queries about object affordances and functional roles; analogous templates are used for the other concept families.",
                "position": 3673
            },
            {
                "img": "https://arxiv.org/html/2602.13195/supp-figures/meta-prompt-prompt-verify.png",
                "caption": "Figure 21:Meta-prompt for Stage 5 prompt–mask alignment verification.Prompt template used to ask the VLM whether a generated conversational prompt correctly and unambiguously describes the masked region, providing a final quality gate for training examples.",
                "position": 3677
            },
            {
                "img": "https://arxiv.org/html/2602.13195/supp-figures/negative-data-prompt.png",
                "caption": "Figure 22:Meta-prompt for Negative Data Generation (Affordances & Functions).Concept-specific template used to generate adversarial negative prompts that describe plausible but absent affordances or incorrect functional states. Similar templates are used for other concept families.",
                "position": 3681
            },
            {
                "img": "https://arxiv.org/html/2602.13195/supp-figures/negative-verification-prompt.png",
                "caption": "Figure 23:Meta-prompt for Negative Prompt Verification.Template used to verify that generated negative prompts have no valid corresponding masks in the image. The VLM checks whether any regions satisfy the prompt requirements before the negative example is included in training.",
                "position": 3684
            },
            {
                "img": "https://arxiv.org/html/2602.13195/supp-figures/supp-predictions-human-annotated-1.png",
                "caption": "Figure 24:Qualitative comparisons on the human-annotated split ofConverSeg(1/3).Each row shows an image with its conversational prompt (between rows), the ground-truth mask (left), and predictions fromConverSeg-Net(Qwen2.5-VL-3B), LISA (LLaVA-7B), and LISA (Llama2-13B) from left to right.ConverSeg-Netmore reliably segments the regions implied by the conversational intent despite using a smaller 3B backbone.",
                "position": 3687
            },
            {
                "img": "https://arxiv.org/html/2602.13195/supp-figures/supp-predictions-human-annotated-2.png",
                "caption": "Figure 25:Qualitative comparisons on the human-annotated split ofConverSeg(2/3).Each row shows an image with its conversational prompt (between rows), the ground-truth mask (left), and predictions fromConverSeg-Net(Qwen2.5-VL-3B), LISA (LLaVA-7B), and LISA (Llama2-13B) from left to right.ConverSeg-Netmore reliably segments the regions implied by the conversational intent despite using a smaller 3B backbone.",
                "position": 3691
            },
            {
                "img": "https://arxiv.org/html/2602.13195/supp-figures/supp-predictions-human-annotated-3.png",
                "caption": "Figure 26:Qualitative comparisons on the human-annotated split ofConverSeg(3/3).Each row shows an image with its conversational prompt (between rows), the ground-truth mask (left), and predictions fromConverSeg-Net(Qwen2.5-VL-3B), LISA (LLaVA-7B), and LISA (Llama2-13B) from left to right.ConverSeg-Netmore reliably segments the regions implied by the conversational intent despite using a smaller 3B backbone.",
                "position": 3695
            },
            {
                "img": "https://arxiv.org/html/2602.13195/supp-figures/supp-predictions-sam-seeded-1.png",
                "caption": "Figure 27:Qualitative comparisons on the SAM-seeded split ofConverSeg(1/3).Each row shows an image with its conversational prompt (between rows), the ground-truth mask (left), and predictions fromConverSeg-Net(Qwen2.5-VL-3B), LISA (LLaVA-7B), and LISA (Llama2-13B) from left to right.ConverSeg-Netmore reliably segments the regions implied by the conversational intent despite using a smaller 3B backbone.",
                "position": 3699
            },
            {
                "img": "https://arxiv.org/html/2602.13195/supp-figures/supp-predictions-sam-seeded-2.png",
                "caption": "Figure 28:Qualitative comparisons on the SAM-seeded split ofConverSeg(2/3).Each row shows an image with its conversational prompt (between rows), the ground-truth mask (left), and predictions fromConverSeg-Net(Qwen2.5-VL-3B), LISA (LLaVA-7B), and LISA (Llama2-13B) from left to right.ConverSeg-Netmore reliably segments the regions implied by the conversational intent despite using a smaller 3B backbone.",
                "position": 3703
            },
            {
                "img": "https://arxiv.org/html/2602.13195/supp-figures/supp-predictions-sam-seeded-3.png",
                "caption": "Figure 29:Qualitative comparisons on the SAM-seeded split ofConverSeg(3/3).Each row shows an image with its conversational prompt (between rows), the ground-truth mask (left), and predictions fromConverSeg-Net(Qwen2.5-VL-3B), LISA (LLaVA-7B), and LISA (Llama2-13B) from left to right.ConverSeg-Netmore reliably segments the regions implied by the conversational intent despite using a smaller 3B backbone.",
                "position": 3707
            },
            {
                "img": "https://arxiv.org/html/2602.13195/supp-figures/supp-predictions-human-annotated-failure-cases.png",
                "caption": "Figure 30:Representative failure cases ofConverSeg-Neton the human-annotated split ofConverSeg.Each row shows an image with its conversational prompt (between rows), the ground-truth mask (left), and predictions fromConverSeg-Net(Qwen2.5-VL-3B), LISA (LLaVA-7B), and LISA (Llama2-13B) from left to right.",
                "position": 3711
            },
            {
                "img": "https://arxiv.org/html/2602.13195/supp-figures/supp-predictions-sam-seeded-failure-cases.png",
                "caption": "Figure 31:Representative failure cases ofConverSeg-Neton the SAM-seeded split ofConverSeg.Each row shows an image with its conversational prompt (between rows), the ground-truth mask (left), and predictions fromConverSeg-Net(Qwen2.5-VL-3B), LISA (LLaVA-7B), and LISA (Llama2-13B) from left to right.",
                "position": 3715
            }
        ]
    },
    {
        "header": "Appendix EAdditional Quantitative Results",
        "images": []
    }
]