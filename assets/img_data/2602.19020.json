[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.19020/x1.png",
                "caption": "Figure 1:Active Data Reconstruction Attack.Language model generates reconstructions from a candidate prefix and is rewarded via a contrastive objective. Members become easier to reconstruct than non-members over RL training, improving MIA performance.",
                "position": 203
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Active Data Reconstruction Attack",
        "images": []
    },
    {
        "header": "4Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.19020/x2.png",
                "caption": "Table 3:Pre-training MIA resultson BookMIA, WikiMIA2024Hard, and Dolma3 arXiv.Bolddenotes the top-2 performance for each dataset. Our active MIA methods (ADRA and ADRA+) consistently outperform passive MIAs.",
                "position": 588
            },
            {
                "img": "https://arxiv.org/html/2602.19020/x2.png",
                "caption": "Table 4:Pre-training member data reconstructionfor original verbatim setting.Bolddenotes the best average performance for each dataset. ADRA+ and ADRA consistently outperform the N-Sampling across all metrics. SeeAppendixEfor paraphrased setting results.",
                "position": 733
            },
            {
                "img": "https://arxiv.org/html/2602.19020/x2.png",
                "caption": "Table 5:Post-training MIA resultson AIME, Olympia Math, and Tulu3 Mix. Our active MIAs (ADRA and ADRA+) outperform calibration-free passive methods and match the calibration-based reference method (R-Loss).",
                "position": 931
            },
            {
                "img": "https://arxiv.org/html/2602.19020/x2.png",
                "caption": "Table 6:Post-training member data reconstructionfor original verbatim setting. ADRA+ and ADRA consistently outperform the N-Sampling across both lexical and semantic metrics. See AppendixAppendixFfor paraphrased setting results.",
                "position": 1073
            }
        ]
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.19020/x2.png",
                "caption": "Table 7:Distillation MIA results.ADRAoutperforms all passive MIAs on both distillation datasets (S1.1 and S1).",
                "position": 1378
            },
            {
                "img": "https://arxiv.org/html/2602.19020/x2.png",
                "caption": "Table 8:Distillation member data reconstruction.ADRA achieves significantly better reconstruction across all metrics.",
                "position": 1437
            }
        ]
    },
    {
        "header": "6Ablations & Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.19020/x2.png",
                "caption": "Figure 2:Performance comparison between RL and SFT.As RL training continues, AUROC improves, whereas SFT decreases.ADRAandADRA+meaningfully improves over naive RL.",
                "position": 1562
            }
        ]
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperimental Details",
        "images": []
    },
    {
        "header": "Appendix BMIA Dataset Construction",
        "images": []
    },
    {
        "header": "Appendix CActive Data Reconstruction Attack Details",
        "images": []
    },
    {
        "header": "Appendix DEvaluation Details",
        "images": []
    },
    {
        "header": "Appendix EAdditional Results: Pre-training Reconstruction",
        "images": []
    },
    {
        "header": "Appendix FAdditional Results: Post-training Reconstruction",
        "images": []
    },
    {
        "header": "Appendix GReconstructions Examples",
        "images": []
    },
    {
        "header": "Appendix HAblation Details",
        "images": []
    },
    {
        "header": "Appendix ILimitations & Discussions",
        "images": []
    }
]