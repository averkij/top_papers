[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14284/x1.png",
                "caption": "Figure 1.SS4D generates high-quality 4D content in 2 minutes. The monocular input videos, corresponding voxelized structure, and the final 4D content from an alternative viewpoint are presented. For additional dynamic results, please refer to our supplementary demo video.https://lizb6626.github.io/SS4D/",
                "position": 144
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14284/x2.png",
                "caption": "Figure 2.SS4D Overview.Left: SS4D pipeline. Our method takes a monocular video as input. It extracts a coarse voxel-based structure using a 4D Flow Transformer, then generates spacetime latents through a 4D Sparse Flow Transformer, both incorporating a Temporal Layer to capture temporal consistency. These latents are subsequently decoded into a sequence of 3D Gaussians, forming the final 4D content.Right: Temporal Layer. It combines temporal self-attention with shifted windows and hybrid 1D Rotary Position Embeddings to efficiently model dynamic 4D content and ensure consistency across frames.",
                "position": 166
            }
        ]
    },
    {
        "header": "2.Related Work",
        "images": []
    },
    {
        "header": "3.Background: Structured 3D Latents",
        "images": []
    },
    {
        "header": "4.Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14284/x3.png",
                "caption": "Figure 3.Implementation of our 4D Compression Strategy.The strategy improve efficiency by compressing 4D representations for long-term sequence generation in our 4D Sparse Flow Transformer.",
                "position": 282
            }
        ]
    },
    {
        "header": "5.Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14284/x4.png",
                "caption": "Figure 4.Qualitative Comparisons on Synthetic Data.We show two frames from the conditioning video alongside two corresponding novel view renderings. Compared to the baselines, our method generates more accurate geometry and detailed textures that remain consistent over time.",
                "position": 325
            },
            {
                "img": "https://arxiv.org/html/2512.14284/x5.png",
                "caption": "Figure 5.Qualitative Comparisonson real-world videos fromDAVIS. For each method, we show two frames from the conditioning video under two novel viewpoints in a2×22\\times 2grid. Compared to the baseline methods, our model remains robust to real-world cases with more complex and high-speed motion.",
                "position": 328
            },
            {
                "img": "https://arxiv.org/html/2512.14284/x6.png",
                "caption": "Figure 6.Temporal Alignment in VAE Reconstruction.Temporal alignment reduces flickering and improves consistency across frames.",
                "position": 596
            },
            {
                "img": "https://arxiv.org/html/2512.14284/x7.png",
                "caption": "Figure 7.Masking Augmentationenhances the model’s robustness to occlusions and improves overall geometry quality.",
                "position": 644
            },
            {
                "img": "https://arxiv.org/html/2512.14284/x8.png",
                "caption": "Figure 8.Failure Cases.Our method exhibits limitations in handling transparent layers, high-frequency details, and rapid motion.",
                "position": 693
            },
            {
                "img": "https://arxiv.org/html/2512.14284/x9.png",
                "caption": "Figure 9.Qualitative Results of SS4D on our Test Set.Three frames from the input video are shown alongside three corresponding novel view renderings. Our method produces accurate geometry and detailed textures, maintaining consistency over time under significant motion.",
                "position": 722
            },
            {
                "img": "https://arxiv.org/html/2512.14284/x10.png",
                "caption": "Figure 10.Qualitative Resultsof SS4D onsynthetic data from the Internet.",
                "position": 725
            },
            {
                "img": "https://arxiv.org/html/2512.14284/x11.png",
                "caption": "Figure 11.Qualitative Resultsof SS4D onreal-world data.",
                "position": 728
            }
        ]
    },
    {
        "header": "6.Conclusions",
        "images": []
    }
]