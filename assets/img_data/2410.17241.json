[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17241/x1.png",
                "caption": "Figure 1:Introductory diagram.We depict (a) the anatomy of the large intestine (colon) within the digestive tract, the polypectomy procedure during colonoscopy examination, and the components of a colonoscope. The bottom figure (b) summarises three highlights of this study.",
                "position": 107
            },
            {
                "img": "https://arxiv.org/html/2410.17241/x2.png",
                "caption": "Figure 2:Colonscopic scene perception from visual to multimodal perspectives.In clinical practice, purely visual tasks, including (a) classification, (b) detection, and (c) segmentation, are applied to identify\ntargets of interest such as polyps and instruments.\n(d) Multimodal applications\nimprove colonoscopy procedures by performing interactive, user-driven tasks aligned with clinical needs. The chatbot provides personalised advice, automated reporting, and streamline procedural workflows.",
                "position": 118
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Revisiting Colonoscopy Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17241/x3.png",
                "caption": "Figure 3:Gallery of deep-based architectures.The single-stream framework (SF) features a single input and output with sequential data flow. Multi-stream frameworks predict a single output but involve parallel processing streams, either at the decoding stage (MF#1) or the encoding stage (MF#2). Branched frameworks extend multi-stream framework to produce multiple outputs from either a single input (BF#1) or multiple inputs (BF#2). These side outputs typically receive supervision from additional supervisory signals, such as boundary cues.",
                "position": 1817
            }
        ]
    },
    {
        "header": "4Revisiting Colonoscopy Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17241/x4.png",
                "caption": "TABLE V:Comparison of image polyp segmentation models.They are evaluated using the mean scores (%) of structure measure (ùíÆùíÆ\\mathcal{S}caligraphic_S[258]) and Dice coefficient (ùíüùíü\\mathcal{D}caligraphic_D) on two test sets, with boxplots illustrating the distribution of their consistency and variability across test cases.",
                "position": 3534
            },
            {
                "img": "https://arxiv.org/html/2410.17241/x5.png",
                "caption": "",
                "position": 3585
            },
            {
                "img": "https://arxiv.org/html/2410.17241/x6.png",
                "caption": "",
                "position": 3589
            },
            {
                "img": "https://arxiv.org/html/2410.17241/x7.png",
                "caption": "",
                "position": 3593
            }
        ]
    },
    {
        "header": "5Stepping into the Multimodal Land",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17241/x8.png",
                "caption": "TABLE VI:Details of instruction tuning dataset ColonINST.For each task, we provide five templates for human instructions, the data sources used to organise human-machine dialogues, and an example of a human-machine conversation.",
                "position": 3902
            },
            {
                "img": "https://arxiv.org/html/2410.17241/x9.png",
                "caption": "",
                "position": 3977
            },
            {
                "img": "https://arxiv.org/html/2410.17241/x10.png",
                "caption": "",
                "position": 4009
            },
            {
                "img": "https://arxiv.org/html/2410.17241/x11.png",
                "caption": "",
                "position": 4046
            },
            {
                "img": "https://arxiv.org/html/2410.17241/x12.png",
                "caption": "Figure 5:Response comparison for colonoscopy image classification.We evaluate the zero-shot language responses from three AI chatbots against the response from our multimodal model, ColonGPT.",
                "position": 4073
            },
            {
                "img": "https://arxiv.org/html/2410.17241/x13.png",
                "caption": "Figure 6:Details of our multimodal language model, ColonGPT.",
                "position": 4103
            },
            {
                "img": "https://arxiv.org/html/2410.17241/x14.png",
                "caption": "Figure 7:Illustration of ColonGPT‚Äôs multimodal capabilities.Our model can execute various multimodal colonoscopy tasks through conversational interactions, including comprehension (CLS, REG), localisation (REC), and captioning (CAP) based.",
                "position": 4884
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]