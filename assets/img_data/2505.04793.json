[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.04793/extracted/6420128/first_figure.jpg",
                "caption": "Figure 1:Comparison between the most important features of the publicly available datasets (ground-ground, aerial-aerial, and aerial-ground) and theDetReIDXdataset. Unlike its counterparts,DetReIDXincludes clothing variationswithin subjects, with detection and tracking annotations, action labels, at wide altitude ranges (5.8m–120m).",
                "position": 94
            },
            {
                "img": "https://arxiv.org/html/2505.04793/extracted/6420128/annotation_examples.jpg",
                "caption": "Figure 2:Examples of soft biometric annotations for two individuals in theDetReIDXdataset. Each subject is labeled with 16 visual and demographic attributes, facilitating fine-grained person analysis across multiple scenes.",
                "position": 847
            }
        ]
    },
    {
        "header": "IIRelated Work",
        "images": []
    },
    {
        "header": "IIITheDetReIDXDataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.04793/extracted/6420128/location_maps_resized.jpg",
                "caption": "Figure 3:Satellite view of the data collection sites across the university campuses in Turkey, Angola, and India. The star markers indicate indoor dataset collection, and the green cones represent drone flight zones.",
                "position": 926
            },
            {
                "img": "https://arxiv.org/html/2505.04793/extracted/6420128/indoor_settigs.jpg",
                "caption": "Figure 4:Overview of the indoor data collection setup: (left) mugshots taken from three angles (left, front, right); (right) gait video.",
                "position": 945
            },
            {
                "img": "https://arxiv.org/html/2505.04793/extracted/6420128/outdoor_setting.jpg",
                "caption": "Figure 5:UAV-based outdoor capture protocol. Each subject is recorded from 18 drone viewpoints (P1–P18), spanning a wide range of altitudes, distances, and pitch angles. Recordings are repeated across two sessions (S1, S2) with varied clothing for appearance diversity.",
                "position": 948
            },
            {
                "img": "https://arxiv.org/html/2505.04793/extracted/6420128/actual_drone_footage.jpg",
                "caption": "Figure 6:Actual drone-captured frames from all 18 UAV viewpoints (P1–P18), grouped by pitch angle: 30°, 60°, and 90°. Each image illustrates real-world scale variation, subject visibility, and background context. Yellow insets highlight degradation in resolution at extreme long-range positions (e.g., P6, P12, P18).",
                "position": 1583
            },
            {
                "img": "https://arxiv.org/html/2505.04793/extracted/6420128/soft_bio.png",
                "caption": "Figure 7:Distributions of the soft biometric labels inDetReIDX. The top row corresponds to the demographic distributions: the dataset is moderately male-dominated (58% male), predominantly composed of individuals aged 18–24 (89%), and has a high proportion of subjects in the [160, 170cm] height interval and ¡60kg weight ranges. Ethnic composition is skewed towards Indian (68%) and Black (25%) categories. The remaining rows provide different visual attributes annotated per person, including hair color, style, presence of facial hair, glasses, clothing, and accessories. Most individuals have black hair (98%), short hairstyles (59%), and wear normal glasses (91%). Clothing is casual with jeans (66%) and shirts/t-shirts being common, while accessories like bags are rare (3%).",
                "position": 1598
            },
            {
                "img": "https://arxiv.org/html/2505.04793/extracted/6420128/Clothing_Variation_Under_Multiview_UAV_Perspectives.jpg",
                "caption": "Figure 8:Example of one subject captured in 18 viewpoints (P1–P18), with clothing changes between sessions. Top row: Session 1. Bottom row: Session 2, with different attire.",
                "position": 1602
            },
            {
                "img": "https://arxiv.org/html/2505.04793/extracted/6420128/Fig_9.jpg",
                "caption": "Figure 9:Scatter plots of ROIs height/width in three different distance bins. The bottom-right plot provides the distribution of the ROI heights (in pixels) of the indoor and outdoor data.",
                "position": 1644
            },
            {
                "img": "https://arxiv.org/html/2505.04793/extracted/6420128/impact_of_aerial_distance_on_detection.jpg",
                "caption": "Figure 10:Effect of distance on pedestrian detection accuracy. The black curve provides the mean Intersection-over-Union (IoU) of correctly matched detections, with shaded areas representing ±1 standard deviation. The orange curve shows the proportion of missed ground truth (GT) annotations. A critical distance (70 meters) is highlighted where performance began to significantly deteriorate. The top inset visualizations illustrate example detections atclose(green box: predictions; red box: ground truth) andlongdistances, corresponding to low and high GT miss rates, respectively. The bar plot above the graph indicates the number of annotations per distance bin, confirming data balance across ranges. These results provide evidence of a substantial degradation in both detection precision and recall at long distances.",
                "position": 1649
            },
            {
                "img": "https://arxiv.org/html/2505.04793/extracted/6420128/impact_of_angles_on_detection.jpg",
                "caption": "Figure 11:Qualitative analysis of pedestrian detection under varying viewpoints and distances. Rows represent different UAV pitch angles (30°, 60°, and 90°), while columns compare detections at close (left) and long ranges (right). Predicted bounding boxes from the detection model are shown in green, and ground-truth annotations are in red. As both the angle and distance increase, detection becomes more challenging due to reduced resolution, occlusion, and distortion.",
                "position": 1653
            },
            {
                "img": "https://arxiv.org/html/2505.04793/extracted/6420128/low_resolution_challenge.jpg",
                "caption": "Figure 12:Challenging conditions in person identification from UAV footage: (a) low resolution, (b) clothing variation, (c) long-range observations, (d) occlusion, (e) top-down viewpoints, (f) pose variation, and (g) motion blur.",
                "position": 1657
            },
            {
                "img": "https://arxiv.org/html/2505.04793/extracted/6420128/cloths_challenge.jpg",
                "caption": "",
                "position": 1667
            },
            {
                "img": "https://arxiv.org/html/2505.04793/extracted/6420128/longaltitudeandlrange_challenge.jpg",
                "caption": "",
                "position": 1674
            },
            {
                "img": "https://arxiv.org/html/2505.04793/extracted/6420128/occluded_challeng.jpg",
                "caption": "",
                "position": 1680
            },
            {
                "img": "https://arxiv.org/html/2505.04793/extracted/6420128/elevatedview_challenge.jpg",
                "caption": "",
                "position": 1687
            },
            {
                "img": "https://arxiv.org/html/2505.04793/extracted/6420128/pose_challenge.jpg",
                "caption": "",
                "position": 1693
            },
            {
                "img": "https://arxiv.org/html/2505.04793/extracted/6420128/blur_challenge.jpg",
                "caption": "",
                "position": 1700
            },
            {
                "img": "https://arxiv.org/html/2505.04793/extracted/6420128/view_per.jpg",
                "caption": "",
                "position": 1706
            }
        ]
    },
    {
        "header": "IVExperiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.04793/extracted/6420128/reid_rank.jpg",
                "caption": "Figure 13:Qualitative evaluation of Person-ViT ReID model onDetReIDXdataset. The left panel (green) illustrates successful retrieval cases where UAV-based query images (”Q”) yield correct matches among top-5 retrieved identities (Rank-1 to Rank-5). The right panel (red) shows failure cases highlighting typical conditions challenging ReID performance, including severe aerial-to-ground (A→G), aerial-to-aerial (A→A), and ground-to-aerial (G→A) viewpoint changes, extreme long-range resolution loss, significant appearance variations due to clothing changes across recording sessions, and environmental factors such as motion blur and occlusion. These results underline the limitations of current state-of-the-art models in real-world UAV surveillance scenarios, as explicitly addressed by theDetReIDXdataset.",
                "position": 2159
            },
            {
                "img": "https://arxiv.org/html/2505.04793/extracted/6420128/cmc_subplots_comparison.png",
                "caption": "Figure 14:Cumulative Match Characteristic (CMC) curves showing the impact of aerial distances on ReID performance using the Person-ViT model, evaluated across three domain transfer scenarios provided by theDetReIDXdataset: A2A, A2G, and G2A. Each scenario compares retrieval performance at different aerial distance intervals: close-range (D1:<<<20m), mid-range (D2: 20–50m), and long-range (D3:>>>50m) against an all-distance baseline. Results highlight significant degradation in ReID accuracy with increasing aerial distance due to factors such as severe resolution loss, viewpoint distortion, and reduced discriminative appearance features. Mean Average Precision (mAP) scores provided in the legends quantify performance drops, emphasizing long-range recognition challenges specifically targeted byDetReIDX.",
                "position": 2264
            }
        ]
    },
    {
        "header": "VConclusions",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]