[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IINTRODUCTION",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16283/extracted/5876733/figures/teaser_new_gen2act.png",
                "caption": "Figure 1:Gen2Actlearns to generate a human video followed by robot policy execution conditioned on the generated video. This enables diverse real-world manipulation in unseen scenarios.",
                "position": 74
            },
            {
                "img": "https://arxiv.org/html/2409.16283/extracted/5876733/figures/archfigure_new.png",
                "caption": "Figure 2:Architecture of the translation model ofGen2Act(closed-loop policyπθsubscript𝜋𝜃\\pi_{\\theta}italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT). Given an image of a scene𝐈0subscript𝐈0\\mathbf{I}_{0}bold_I start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTand a language-goal description of the task𝒢𝒢\\mathcal{G}caligraphic_G, we generate a human video𝐕gsubscript𝐕𝑔\\mathbf{V}_{g}bold_V start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPTwith a pre-trained video generation model𝒱⁢(𝐈0,𝒢)𝒱subscript𝐈0𝒢\\mathcal{V}(\\mathbf{I}_{0},\\mathcal{G})caligraphic_V ( bold_I start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , caligraphic_G ). During training of the policy, we incorporate track prediction from the policy latents as an auxiliary loss in addition to a behavior cloning loss. Dotted pathways show training-specific computations. During inference, we do not require track prediction and only use the video model𝒱𝒱\\mathcal{V}caligraphic_Vin conjunction with the policyπθ⁢(𝐈t−k:t,𝐕g)subscript𝜋𝜃subscript𝐈:𝑡𝑘𝑡subscript𝐕𝑔\\pi_{\\theta}(\\mathbf{I}_{t-k:t},\\mathbf{V}_{g})italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_I start_POSTSUBSCRIPT italic_t - italic_k : italic_t end_POSTSUBSCRIPT , bold_V start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ).",
                "position": 77
            }
        ]
    },
    {
        "header": "IIRelated Works",
        "images": []
    },
    {
        "header": "IIIAPPROACH",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16283/extracted/5876733/figures/qualgens_new.png",
                "caption": "Figure 3:Visualization of zero-shot video generation for different tasks. The blue frame and the language description are input to the video generation model ofGen2Actand the black frames show sub-sampled frames of the generated video. These results demonstrate the applicability of off-the-shelf video generation models for image+text conditioned video generation that preserves the scene and performs the desired manipulation task.",
                "position": 132
            },
            {
                "img": "https://arxiv.org/html/2409.16283/extracted/5876733/figures/qualevals_new.png",
                "caption": "Figure 4:Visualization of the closed-loop policy rollouts (bottom row) conditioned on the generated human videos (top row) for four tasks. The red frame and the language description are input to the video generation model ofGen2Act. The black frames show sub-sampled frames of the generated video, and the blue frames show robot executions conditioned on the generated video.",
                "position": 135
            }
        ]
    },
    {
        "header": "IVEXPERIMENTS",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16283/extracted/5876733/figures/longhorizon_new.png",
                "caption": "Figure 5:Robot executions for a sequence of tasks. The last frame of the previous execution serves as the conditioning frame for next stage video generation.",
                "position": 257
            }
        ]
    },
    {
        "header": "VDiscussion and Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16283/extracted/5876733/figures/failures_gen2act.png",
                "caption": "Figure 6:Analysis of failures ofGen2Act. The tasks here correspond to object type generalization. We can see that most of the failures of robot execution (top 3 rows) are correlated with incorrect video generations. In the last row the video generation is plausible but the execution is incorrect in following the trajectory of the generated video afetr grasping the object.",
                "position": 1165
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]