[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IINTRODUCTION",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16283/extracted/5876733/figures/teaser_new_gen2act.png",
                "caption": "Figure 1:Gen2Actlearns to generate a human video followed by robot policy execution conditioned on the generated video. This enables diverse real-world manipulation in unseen scenarios.",
                "position": 74
            },
            {
                "img": "https://arxiv.org/html/2409.16283/extracted/5876733/figures/archfigure_new.png",
                "caption": "Figure 2:Architecture of the translation model ofGen2Act(closed-loop policyÏ€Î¸subscriptğœ‹ğœƒ\\pi_{\\theta}italic_Ï€ start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT). Given an image of a sceneğˆ0subscriptğˆ0\\mathbf{I}_{0}bold_I start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTand a language-goal description of the taskğ’¢ğ’¢\\mathcal{G}caligraphic_G, we generate a human videoğ•gsubscriptğ•ğ‘”\\mathbf{V}_{g}bold_V start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPTwith a pre-trained video generation modelğ’±â¢(ğˆ0,ğ’¢)ğ’±subscriptğˆ0ğ’¢\\mathcal{V}(\\mathbf{I}_{0},\\mathcal{G})caligraphic_V ( bold_I start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , caligraphic_G ). During training of the policy, we incorporate track prediction from the policy latents as an auxiliary loss in addition to a behavior cloning loss. Dotted pathways show training-specific computations. During inference, we do not require track prediction and only use the video modelğ’±ğ’±\\mathcal{V}caligraphic_Vin conjunction with the policyÏ€Î¸â¢(ğˆtâˆ’k:t,ğ•g)subscriptğœ‹ğœƒsubscriptğˆ:ğ‘¡ğ‘˜ğ‘¡subscriptğ•ğ‘”\\pi_{\\theta}(\\mathbf{I}_{t-k:t},\\mathbf{V}_{g})italic_Ï€ start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( bold_I start_POSTSUBSCRIPT italic_t - italic_k : italic_t end_POSTSUBSCRIPT , bold_V start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ).",
                "position": 77
            }
        ]
    },
    {
        "header": "IIRelated Works",
        "images": []
    },
    {
        "header": "IIIAPPROACH",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16283/extracted/5876733/figures/qualgens_new.png",
                "caption": "Figure 3:Visualization of zero-shot video generation for different tasks. The blue frame and the language description are input to the video generation model ofGen2Actand the black frames show sub-sampled frames of the generated video. These results demonstrate the applicability of off-the-shelf video generation models for image+text conditioned video generation that preserves the scene and performs the desired manipulation task.",
                "position": 132
            },
            {
                "img": "https://arxiv.org/html/2409.16283/extracted/5876733/figures/qualevals_new.png",
                "caption": "Figure 4:Visualization of the closed-loop policy rollouts (bottom row) conditioned on the generated human videos (top row) for four tasks. The red frame and the language description are input to the video generation model ofGen2Act. The black frames show sub-sampled frames of the generated video, and the blue frames show robot executions conditioned on the generated video.",
                "position": 135
            }
        ]
    },
    {
        "header": "IVEXPERIMENTS",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16283/extracted/5876733/figures/longhorizon_new.png",
                "caption": "Figure 5:Robot executions for a sequence of tasks. The last frame of the previous execution serves as the conditioning frame for next stage video generation.",
                "position": 257
            }
        ]
    },
    {
        "header": "VDiscussion and Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16283/extracted/5876733/figures/failures_gen2act.png",
                "caption": "Figure 6:Analysis of failures ofGen2Act. The tasks here correspond to object type generalization. We can see that most of the failures of robot execution (top 3 rows) are correlated with incorrect video generations. In the last row the video generation is plausible but the execution is incorrect in following the trajectory of the generated video afetr grasping the object.",
                "position": 1165
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]