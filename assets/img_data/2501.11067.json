[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "",
        "images": []
    },
    {
        "header": "IntellAgent: A Multi-Agent Framework for Evaluating Conversational AI Systems",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.11067/extracted/6137699/figures/overview.png",
                "caption": "Figure 1:System diagram. (1) Given a chatbot prompt and a Schema DB, the system generates an event that targets a subset of policies, which includes a user request and a system DB state. (2) For each event the system simulates a conversation between the user and the chatbot. (3) A fine-grained report on the chatbot performances is generated.",
                "position": 123
            },
            {
                "img": "https://arxiv.org/html/2501.11067/extracted/6137699/figures/experiments/lines_Airline.png",
                "caption": "Table 1:Comparison of random walk sampling strategies. (Left) Uniform sampling of the next node. (Middle) Selection of the next node based on maximal edge weight. (Right) IntellAgent weighted probability sampling, which balances diversity and alignment with real-world distributions.",
                "position": 315
            },
            {
                "img": "https://arxiv.org/html/2501.11067/extracted/6137699/figures/experiments/lines_Airline.png",
                "caption": "Figure 2:Model success rates across different challenge levels. While all models show reduced performance as the challenge level increases, they exhibit distinct patterns of decline, differing in both the onset level and the magnitude of the decrease.",
                "position": 375
            },
            {
                "img": "https://arxiv.org/html/2501.11067/extracted/6137699/figures/experiments/lines_Retail.png",
                "caption": "",
                "position": 378
            },
            {
                "img": "https://arxiv.org/html/2501.11067/extracted/6137699/figures/experiments/bars_Airline.png",
                "caption": "Figure 3:Comparison of the success rates of the top four models across various policy categories, highlighting that some categories are more challenging than others. Additionally, the relative performance order of different models varies across categories.",
                "position": 462
            },
            {
                "img": "https://arxiv.org/html/2501.11067/extracted/6137699/figures/experiments/bars_Retail.png",
                "caption": "",
                "position": 465
            },
            {
                "img": "https://arxiv.org/html/2501.11067/extracted/6137699/figures/event_generator_architecture.png",
                "caption": "Figure 4:Event generator architecture overview.",
                "position": 928
            },
            {
                "img": "https://arxiv.org/html/2501.11067/extracted/6137699/figures/dialog_architecture.png",
                "caption": "Figure 5:Simulator architecture overview.",
                "position": 931
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]