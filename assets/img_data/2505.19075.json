[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19075/extracted/6484925/figs/main_figure4.png",
                "caption": "Figure 1:UniR Framework Overview. Our approach trains a lightweight, transferable reasoning module (πrsubscript𝜋𝑟{\\pi_{r}}italic_π start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT) using predefined rewards to guide a frozen backbone model (πbsubscript𝜋𝑏{\\pi_{b}}italic_π start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT), offering (1) transferability across different backbone models or tasks; and (2) composability by combining multiple specialized reasoning modules through reward optimization.",
                "position": 163
            }
        ]
    },
    {
        "header": "3Problem Definition",
        "images": []
    },
    {
        "header": "4UniR: Universal Reasoner",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19075/extracted/6484925/figs/transfer_results_math2.png",
                "caption": "Figure 2:Effectiveness of Reasoning Policy Transfer. Results demonstrate that a trained reasoning module can improve performance when integrated with larger backbone models across diverse mathematical reasoning tasks.",
                "position": 702
            },
            {
                "img": "https://arxiv.org/html/2505.19075/extracted/6484925/figs/math+trans.png",
                "caption": "Figure 4:Performance on a German-to-English Math problem-solving task.The numbers in the figure indicate the value ofα𝛼\\alphaitalic_α.",
                "position": 752
            },
            {
                "img": "https://arxiv.org/html/2505.19075/extracted/6484925/figs/vram_usage_by_method.png",
                "caption": "Figure 5:VRAM usage versus batch size under an 80GB constraint.Our method scales to batch size 128, while full fine-tuning and LoRA are limited, demonstrating memory efficiency for large batch.",
                "position": 771
            },
            {
                "img": "https://arxiv.org/html/2505.19075/extracted/6484925/figs/vram_usage_by_method.png",
                "caption": "Figure 5:VRAM usage versus batch size under an 80GB constraint.Our method scales to batch size 128, while full fine-tuning and LoRA are limited, demonstrating memory efficiency for large batch.",
                "position": 773
            },
            {
                "img": "https://arxiv.org/html/2505.19075/extracted/6484925/figs/reward_stable2.png",
                "caption": "Figure 6:Training Dynamics.We visualize the mean (Left) and the standard deviation (Right) of the reward on the GSM8k (Top) and Math-12k (Bottom) during training.",
                "position": 777
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperimental Details",
        "images": []
    },
    {
        "header": "Appendix BQualitative Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19075/x1.png",
                "caption": "Figure 16:Illustrative examples of responses from the baseline VLM and our UniR-extended framework on tasks from the (Left) Geometry3k and (Right) MathVerse benchmarks.",
                "position": 2037
            },
            {
                "img": "https://arxiv.org/html/2505.19075/extracted/6484925/figs/transfer_results_math3_1.5b.png",
                "caption": "Figure 17:Transferability of the 0.5Bπrsubscript𝜋𝑟{\\pi_{r}}italic_π start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPTand 1.5Bπrsubscript𝜋𝑟{\\pi_{r}}italic_π start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPTreasoning modules when combined with a 14B frozen backbone model. The 1.5Bπrsubscript𝜋𝑟{\\pi_{r}}italic_π start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPTmodule demonstrated superior performance.",
                "position": 2044
            },
            {
                "img": "https://arxiv.org/html/2505.19075/extracted/6484925/figs/gsm8k_combined_plot.png",
                "caption": "Figure 18:Performance comparison on the GSM8K dataset between standalone reasoning modules (πrsubscript𝜋𝑟{\\pi_{r}}italic_π start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT,green) and our UniR framework (red), evaluated against their respective backbone models (orange). (Left)πrsubscript𝜋𝑟{\\pi_{r}}italic_π start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPTmodule using LLaMA-3.2-1B with a LLaMA-3.2-3B backbone. (Right)πrsubscript𝜋𝑟{\\pi_{r}}italic_π start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPTmodule using Qwen2.5-0.5B with a Qwen2.5-3B backbone.",
                "position": 2159
            }
        ]
    },
    {
        "header": "Appendix CAdditional Experiments",
        "images": []
    }
]