[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15870/x1.png",
                "caption": "Figure 1:OmniVinci demonstrates strong performance across widely used omni-modal (+19.05 on Dailyomni), audio (+1.7 on MMAR), and vision (+3.9 on Video-MME) understanding benchmarks.",
                "position": 172
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Model Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15870/x2.png",
                "caption": "Figure 2:We introduce a foundation model for omni-modal understanding. Our model blends information from vision, audio, and text modalities into a unified omni-modal token sequence via the proposed omni-modal alignment mechanism.",
                "position": 203
            },
            {
                "img": "https://arxiv.org/html/2510.15870/x3.png",
                "caption": "Figure 3:Illustration of the proposed OmniAlignNet module.",
                "position": 227
            }
        ]
    },
    {
        "header": "3Training Strategy",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15870/x4.png",
                "caption": "Figure 4:Omni-modal captions generation pipeline.\nVideo is segmented into 20-second clips. Visual and audio captions are generated independently for each segment, but lack cross-modal context and contain wrong understanding (modality-specific hallucination). A separate LLM performs cross-modal correction and summarization to create accurate omni-modal captions.",
                "position": 369
            },
            {
                "img": "https://arxiv.org/html/2510.15870/x5.png",
                "caption": "Figure 5:Pie chart of overall distribution of training data across modalities, showing proportions for image (36%), non-speech sound (21%), speech (17%), omni (15%), and video (11%).",
                "position": 393
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15870/x6.png",
                "caption": "Figure 6:Left:Accuracy reward and format reward curves of OmniVinci and Qwen2.5-Omni in RL training.Right:Accuracy reward curve of OmniVinci with and without audio.",
                "position": 1404
            },
            {
                "img": "https://arxiv.org/html/2510.15870/x7.png",
                "caption": "Figure 7:OmniVinci demonstrates strong vision and audio perception capabilities to handle single or joint modality scenarios. The model also supports audio prompts and outputs.",
                "position": 1419
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix Table of Contents",
        "images": []
    },
    {
        "header": "Appendix ARelated Works",
        "images": []
    },
    {
        "header": "Appendix BDownstream Agents",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15870/x8.png",
                "caption": "Figure 8:An illustration of our speech-driven navigation agent based on OmniVinci.Left:Agent’s current visual observation.Middle:Top-down map indicating the goal position and the agent’s past trajectory.Right:the input speech instruction and the agent’s predicted action given the current observation.",
                "position": 3032
            },
            {
                "img": "https://arxiv.org/html/2510.15870/figs/tennis_highlight_with_title.png",
                "caption": "Figure 9:Example of tennis broadcast commentary generation.\nFor better visualization, we added red circle highlights to the tennis ball.",
                "position": 3129
            },
            {
                "img": "https://arxiv.org/html/2510.15870/x9.png",
                "caption": "Figure 10:Sample frames and transcript trunks from one of the curated radiologist-narrated CT interpretation video. For annotation, the radiologist maintains a 2D axial view while progressively adjusting visualization (e.g., window/level, zoom) and annotating across slices.",
                "position": 3401
            },
            {
                "img": "https://arxiv.org/html/2510.15870/x10.png",
                "caption": "Figure 11:Qualitative comparison between OmniVinci and Qwen2.5-Omni on an omni-modal medical QA task based on radiologist-narrated CT interpretation videos. We organize the evaluation into four categories of questions: long-horizon temporal reasoning and localization, audio-visual synchronization and understanding, anti-shortcutting, and temporal reasoning.",
                "position": 3457
            },
            {
                "img": "https://arxiv.org/html/2510.15870/x11.png",
                "caption": "Figure 12:Illustration of wafer robust defect analysis task for smart factory agent.",
                "position": 3474
            },
            {
                "img": "https://arxiv.org/html/2510.15870/x12.png",
                "caption": "Figure 13:Illustration of SPC chart recognition for industrial fault detection.",
                "position": 3526
            }
        ]
    },
    {
        "header": "Appendix CMethod Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15870/x13.png",
                "caption": "Figure 14:Data distribution of our synthetic speech-prompted multimodal conversation.",
                "position": 3692
            },
            {
                "img": "https://arxiv.org/html/2510.15870/x14.png",
                "caption": "Figure 15:Latency comparison between Qwen2.5-Omni and our OmniVinci model on a GeForce RTX 4090 GPU. Our model achieves 1.7×\\timesfaster time-to-first-token latency and 2.72×\\timesfaster decoding latency.",
                "position": 3873
            },
            {
                "img": "https://arxiv.org/html/2510.15870/x15.png",
                "caption": "Figure 16:We illustrate two test-time scaling methods using an extra ASR model: (a) OmniVinci-Cascaded, using ASR history as an additional input to the Omni model with the audio inputs, and (b) OmniVinci-RAG, using the retrieval token for prediction. The related results are reported in Table19.",
                "position": 4002
            }
        ]
    },
    {
        "header": "Appendix DMore Experiments",
        "images": []
    }
]