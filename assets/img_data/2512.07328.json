[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07328/figures/teaser.png",
                "caption": "",
                "position": 62
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07328/figures/method.png",
                "caption": "Figure 2:Framework of ContextAnyone.(a) Input:The model takes a text prompt and a reference imageIrI_{r}. The prompt is augmented by a VLM and encoded by an LLM encoder, while the image is processed by two encoders: one for cross-attention guidance and one for latent concatenation.(b) Backbone:The DiT backbone contains stacked blocks with three attention modules. The input latent merges reference latents (green) and noisy video latents (blue), which are decoded separately by the VAE after the DiT blocks.(c) Emphasize Attention:Latents are split into reference and video parts; video latents serve as queries and reference latents as keys and values to reinforce identity.(d) Self-Attention:The attention map is masked so that reference latents do not query video latents, enforcing a one-way information flow from reference to video tokens.",
                "position": 131
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07328/figures/gap_rope.png",
                "caption": "Figure 3:Left:Architecture of self-attention with Gap-RoPE applied to Q, K, and V.Upper Right:Standard RoPE assigns continuous temporal indices across all tokens.Lower Right:Gap-RoPE introduces a shiftβ\\betafrom the generated video frame onward, creating a positional gap between reference and video tokens.",
                "position": 284
            },
            {
                "img": "https://arxiv.org/html/2512.07328/figures/data_pipeline.png",
                "caption": "Figure 4:Dataset pipeline. Given a ground-truth video, we extract its first frame (green box) and randomly sample action and environment prompt from pools. These prompts and first frame are fed into an image editing model to modify the person’s action and the scene’s illumination. A vision-language model then assesses and filters out invalid edits, after which a segmentation model isolates the person foreground to obtain the final reference image.",
                "position": 317
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07328/figures/comparison.png",
                "caption": "Figure 5:Qualitative evaluation. Each group shows, from top to bottom, the reference image (Ref.), the results of our method (Ours), Phantom, and VACE. As illustrated, our method produces the most realistic and consistent results in terms of facial identity and overall appearance. In contrast, Phantom and VACE exhibit noticeable artifacts and inconsistencies in facial regions or outfit alignment (highlighted by red boxes). Our approach achieves superior appearance consistency and motion fidelity compared to existing methods.",
                "position": 416
            },
            {
                "img": "https://arxiv.org/html/2512.07328/figures/comparison2.png",
                "caption": "Figure 6:More results. Each group presents a reference image alongside the augmented prompts in the first row, followed by the cross-video generation results in the second row. The yellow-highlighted and blue-highlighted prompts correspond to the inputs of the left and right videos, respectively. Our method preserves strong consistency in the subject’s identity and appearance across the generated videos.",
                "position": 420
            },
            {
                "img": "https://arxiv.org/html/2512.07328/figures/ablation.png",
                "caption": "Figure 7:Visualization of the ablation study. The upper-left panel shows the reference image and the text prompt. Among all variants, our full model yields the most faithful identity preservation and contextual consistency.",
                "position": 443
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    }
]