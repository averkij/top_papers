[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13800/x1.png",
                "caption": "",
                "position": 76
            },
            {
                "img": "https://arxiv.org/html/2510.13800/x2.png",
                "caption": "",
                "position": 76
            },
            {
                "img": "https://arxiv.org/html/2510.13800/x3.png",
                "caption": "",
                "position": 76
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13800/x4.png",
                "caption": "Figure 1:We proposeGS-Reasoner, which integrates visual grounding as an intermediate chain-of-thought for spatial reasoning. All the bounding boxes shown above are autoregressively derived by GS-Reasoner in the reasoning process. Notably, the showcased video is captured in the wild without sensory 3D inputs, highlighting the strong generalization capability of our model.",
                "position": 98
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3GS-Reasoner Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13800/x5.png",
                "caption": "Figure 2:Overview of GS-Reasoner framework. Our method builds a semantic-geometric hybrid 3D scene representation, enabling 3D LLM to perform 3D visual grounding autoregressively, which allows grounding to be integrated as a chain-of-thought within the spatial reasoning process.",
                "position": 158
            }
        ]
    },
    {
        "header": "4GCoT: Grounded Chain-of-Thought Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13800/x6.png",
                "caption": "Figure 3:Overview of Grounded Chain-of-Thought (GCoT) Dataset.We first construct spatial QA pairs without CoT, and then prompt GPT-4o to generate CoT paths based on the birdâ€™s-eye view, object information, and QA pairs.",
                "position": 226
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Dataset Details",
        "images": []
    },
    {
        "header": "Appendix BAdditional Implementation Details",
        "images": []
    },
    {
        "header": "Appendix CAdditional Related Work",
        "images": []
    },
    {
        "header": "Appendix DAdditional Experimental Analysis and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13800/x7.png",
                "caption": "Figure 4:Qualitative results on VSI-Bench.",
                "position": 2291
            },
            {
                "img": "https://arxiv.org/html/2510.13800/x8.png",
                "caption": "Figure 5:Qualitative results on VSI-Bench.",
                "position": 2296
            },
            {
                "img": "https://arxiv.org/html/2510.13800/x9.png",
                "caption": "Figure 6:Qualitative results on VSI-Bench.",
                "position": 2301
            },
            {
                "img": "https://arxiv.org/html/2510.13800/x10.png",
                "caption": "Figure 7:Qualitative results on VSI-Bench.",
                "position": 2306
            }
        ]
    },
    {
        "header": "Appendix EFuture Work",
        "images": []
    },
    {
        "header": "Appendix FThe Use of Large Language Models (LLMs)",
        "images": []
    }
]