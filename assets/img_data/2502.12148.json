[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12148/x1.png",
                "caption": "Figure 1:Architecture comparison between (a) DPO training improve multimodal understanding(Zhou et al.,2024c; He et al.,2024), (b) DPO training improve multimodal generation(Wang et al.,2024)and (c) our HermesFlow.",
                "position": 132
            },
            {
                "img": "https://arxiv.org/html/2502.12148/x2.png",
                "caption": "Figure 2:Motivation of HermesFlow.(a) A general pipeline to quantitatively assess the MLLM’s performance of multimodal understanding and generation. (b) The imbalance between understanding and generation capabilities is a common phenomenon in MLLMs, and our method ssignificantly narrows this disparity. For detailed descriptions, please refer toSection5.2.",
                "position": 140
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12148/x3.png",
                "caption": "Figure 3:Pipeline of HermesFlow.We begin by curating paired data that captures both understanding and generation preferences from homologous input data. Leveraging this homologous preference data, we design Pair-DPO and employ self-play iterative optimization to seamlessly bridge the gap between multimodal understanding and generation.",
                "position": 259
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12148/x4.png",
                "caption": "Figure 4:Qualitative comparison between our HermesFlow and three outstanding Multimodal LLMs VILA-U(Wu et al.,2024c), Janus(Wu et al.,2024a), and Show-o(Xie et al.,2024a). Colored text denotes the advantages of HermesFlow in generated images.",
                "position": 569
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12148/x5.png",
                "caption": "Figure 5:Results of user study.",
                "position": 916
            },
            {
                "img": "https://arxiv.org/html/2502.12148/x6.png",
                "caption": "Figure 6:The influence of the richness of each preference sample.",
                "position": 1098
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADerivation of the Pair-DPO Optimization Objective",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12148/x7.png",
                "caption": "Figure 7:An example of the curation of paired preference data.",
                "position": 1999
            }
        ]
    },
    {
        "header": "Appendix BExample of Paired Preference Data",
        "images": []
    }
]