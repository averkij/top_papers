[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19904/x1.png",
                "caption": "",
                "position": 157
            },
            {
                "img": "https://arxiv.org/html/2503.19904/x2.png",
                "caption": "",
                "position": 160
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19904/x3.png",
                "caption": "Figure 2:Overview of Tracktention.\nWe begin by using an off-the-shelf point tracker to extract a number of video tracks.\nGiven these, we firstsampleimage tokens at the track locations, obtaining corresponding track tokens (Sec.3.1).\nNext, we use a Track Transformer toupdatethese tokens, propagating information temporally at consistent spatial locations (Sec.3.2).\nFinally, wesplatthe information back to the image tokens (Sec.3.3).\nBy explicitly incorporating motion information through point tracks, Tracktention improves temporal alignment, effectively captures complex object movements, and ensures stable feature representations over time.",
                "position": 175
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19904/x4.png",
                "caption": "Figure 3:Left: theTracktention architecturecomprises Attentional Sampling, pooling information from images to track, Track Transformer, processing this information temporally, and Attentional Splatting, moving the processed information back to the images.\nRight: Tracktention is easilyintegratedin ViTs and ConvNets to make video networks out image ones.",
                "position": 288
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19904/x5.png",
                "caption": "Figure 4:Video depth prediction, comparing Tracktention (+DepthAnything), DepthCrafter[20], and DUSt3R[56].\nWe visualize a column of pixels (highlighted in red) over time to illustrate temporal variation.\nOur model shows stable, coherent depth estimation over time, while DepthCrafter exhibits significant errors in certain regions (blue box).\nDUSt3R struggles with dynamic content (green box).",
                "position": 970
            },
            {
                "img": "https://arxiv.org/html/2503.19904/x6.png",
                "caption": "Figure 5:Qualitative comparison of video colorization methods.Tracktention (+DDColor) yields vibrant, realistic, and consistent colors. In contrast, TCVC appears less vibrant (21.7 vs. 29.5 Colorfulness), while DDColor lacks temporal consistency.",
                "position": 977
            },
            {
                "img": "https://arxiv.org/html/2503.19904/x7.png",
                "caption": "Table 3:Quantitative comparison of video colorization methods on the DAVIS and Videvo datasets.Our method, when augmented onto four different baseline models, consistently improves the Color Distribution Consistency (CDC) metric across both datasets.",
                "position": 981
            },
            {
                "img": "https://arxiv.org/html/2503.19904/x7.png",
                "caption": "Figure 6:Attentional Sampling module.The module attends to image features in correspondence with each track, as shown by the attention maps next to each frame.",
                "position": 1181
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19904/x8.png",
                "caption": "",
                "position": 2864
            },
            {
                "img": "https://arxiv.org/html/2503.19904/x9.png",
                "caption": "Figure 8:Comparison of query initialization strategies for point tracking.The top row shows the point tracks obtained with grid-initialized queries, which suffer from significant coverage loss in later frames. In contrast, the bottom row illustrates tracks obtained with our random initialization method, which maintains comprehensive coverage across the scene over time. Larger dots with white edges represent queries, which are seed coordinates used to initiate tracking in the video. Numbers at the bottom left of each frame indicate the frame index, highlighting the improved completeness of tracks produced by our approach, particularly in later frames.",
                "position": 2874
            },
            {
                "img": "https://arxiv.org/html/2503.19904/x10.png",
                "caption": "Figure 9:Comparison of different attention mechanisms for video processing.The red-bordered block represents the query token, while the blue-highlighted blocks indicate the range of tokens the mechanism attends to. See text for details.",
                "position": 2877
            }
        ]
    },
    {
        "header": "Appendix ADesign and Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19904/x11.png",
                "caption": "Figure 10:The impact of selective tracks used on video colorization consistency.In the top example, using tracks corresponding to the left bird (Row 1) results in consistent colorization of the left bird (Row 2, green box), while the right bird exhibits inconsistent colorization (red box). Conversely, using tracks for the right bird (Row 3) ensures stable colorization for the right bird (Row 4, green box), while the left bird becomes inconsistent (red box). Similar patterns are observed for the train and dog examples. Green boxes highlight regions with stable and consistent colors, while red boxes indicate inconsistent colorization.",
                "position": 3091
            },
            {
                "img": "https://arxiv.org/html/2503.19904/x12.png",
                "caption": "Figure 11:Comparison of depth prediction results across different models:DepthCrafter, DUSt3R, Depth-Anything (marked with * as the base model), and Ours (+Depth-Anything). Each row shows depth predictions, corresponding error maps, and the ground truth for the input video frames. Highlighted rectangles emphasize key issues in baseline methods: DepthCrafter exhibits significant errors in certain areas, DUSt3R tends to fail in dynamic regions, and Depth-Anything produces flickering results, evident from inconsistent error patterns.",
                "position": 3094
            },
            {
                "img": "https://arxiv.org/html/2503.19904/x13.png",
                "caption": "Figure 12:Additional comparison of depth prediction resultsacross DepthCrafter, DUSt3R, Depth-Anything (*denotes the base model), and Ours (+Depth-Anything). The rows present depth predictions, their error maps, and ground truth for a different set of input video frames, illustrating consistency across varying scenes.",
                "position": 3097
            },
            {
                "img": "https://arxiv.org/html/2503.19904/x14.png",
                "caption": "Figure 13:Video colorization resultscomparing TCVC, DDColor (*denoting the base model), and Ours (+DDColor). The rows show the input grayscale video frames, followed by the colorized outputs from each method. Highlighted areas indicate inconsistencies in the base model (DDColor), which are resolved by our model, demonstrating its ability to produce consistent and accurate colorization.",
                "position": 3100
            },
            {
                "img": "https://arxiv.org/html/2503.19904/x15.png",
                "caption": "Figure 14:Additional video colorization resultscomparing TCVC, DDColor (*denoting the base model), and Ours (+DDColor). The rows display the input grayscale video frames alongside the colorized outputs from each method. Highlighted areas pinpoint inconsistencies in the base model (DDColor), which are effectively resolved by our model, showcasing its improved consistency and color accuracy.",
                "position": 3103
            }
        ]
    },
    {
        "header": "Appendix BAdditional Experimental Results",
        "images": []
    }
]