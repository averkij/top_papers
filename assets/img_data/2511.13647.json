[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.13647/x1.png",
                "caption": "Figure 1:Part-X-MLLM is a natively 3D, part-aware multimodal large language model that provides comprehensive understanding of 3D shapes and supports a wide range of 3D understanding tasks. It also seamlessly integrates with diffusion-based pipelines, enabling semantically precise part-aware 3D shape generation and editing.",
                "position": 153
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.13647/x2.png",
                "caption": "Figure 2:The Part-X-MLLM Framework.Our pipeline begins by encoding geometry and appearance features separately using a dual-encoder architecture, which are then fused together with text prompts. These combined features are passed to an autoregressive decoder that generates a program-like token sequence representing a plan (e.g., bounding boxes, edit commands). Finally, specialized geometry heads execute this plan to enable part-aware generation and editing.",
                "position": 236
            },
            {
                "img": "https://arxiv.org/html/2511.13647/x3.png",
                "caption": "Figure 3:Task realization with a planning language.A decoder outputs program tokens that unify diverse interactions: (Top) part-aware generation guided by bounding boxes; (Middle) grounded Q&A whose answers embed BBox tokens; (Bottom) auto-located 3D editing executed via cuboid masks and commands. The numbered circles (e.g.,x) denote the corresponding task types.",
                "position": 289
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.13647/x4.png",
                "caption": "Figure 4:Qualitative shape decomposition results.",
                "position": 385
            },
            {
                "img": "https://arxiv.org/html/2511.13647/x5.png",
                "caption": "Figure 5:Qualitative results for part-aware editing.Our model successfully interprets natural language instructions to perform localized edits, while preserving the integrity of the original object.",
                "position": 388
            },
            {
                "img": "https://arxiv.org/html/2511.13647/x6.png",
                "caption": "Figure 6:Semantic granularity control via part clustering.By clustering parts based on the semantic similarity of their descriptions, we can progressively merge fine-grained components into coarser structures. The number of components is automatically reduced from 22 to 2.",
                "position": 397
            },
            {
                "img": "https://arxiv.org/html/2511.13647/x7.png",
                "caption": "Figure 7:Confidence-aware face segmentation.By leveraging the generated bounding boxes and their associated confidence scores, we can achieve high-quality, fine-grained face-level segmentation of 3D objects without any additional training.",
                "position": 406
            },
            {
                "img": "https://arxiv.org/html/2511.13647/x8.png",
                "caption": "Figure 8:Qualitative results for overall object captioning.",
                "position": 696
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.13647/x9.png",
                "caption": "Figure 9:Qualitative results for part-aware Q&A.Our model provides more accurate and descriptive answers, with precise part grounding indicated by bounding box tokens.",
                "position": 809
            },
            {
                "img": "https://arxiv.org/html/2511.13647/x10.png",
                "caption": "Figure 10:t-SNE visualization of special token embeddings.The tokens form distinct, well-structured clusters based on their function, indicating a meaningful learned representation.",
                "position": 884
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]