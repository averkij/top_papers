[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2404.07097/x1.png",
                "caption": "Figure 1:We presentTracksTo4D, a method for mapping a set of 2D point tracks extracted from casual dynamic videos into their corresponding 3D locations and camera motion. At inference time, our network predicts the dynamic structure and camera motion in a single feed-forward pass. Our network takes as input a set of 2D point tracks (left) and uses several multi-head attention layers while alternating between the time dimension and the track dimension (middle). The network predicts cameras, per-frame 3D points, and per-world point movement value (right). The 3D point internal colors illustrate the predicted 3D movement level values, such that points with high/low 3D motion are presented in red/purple colors respectively. These outputs are used to reproject the predicted points into the frames for calculating the reprojection error losses. See details in the text. The reader is encouraged to watch the supplementary video visualizations.",
                "position": 137
            }
        ]
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2404.07097/extracted/5694121/figures/symmetries.png",
                "caption": "Figure 2:The symmetry structure of our problem. Frames (vertical) have time translation symmetry while points (horizontal) have set permutation symmetry.",
                "position": 203
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2404.07097/x2.png",
                "caption": "Figure 3:Qualitative Results.Top. Frames from 2 different test video sequences with point tracks marked with corresponding colors.Bottom. A 3D visualization of our method‚Äôs outputs, from two time stamps. The camera trajectory is present as gray frustums, whereas the current camera is marked in red. The reconstructed 3D scene points are presented in corresponding colors to the input\ntracks on the top. The scene is observed from the same viewpoint, enabling the visualization of the dynamic reconstructed structure.",
                "position": 1136
            },
            {
                "img": "https://arxiv.org/html/2404.07097/x3.png",
                "caption": "Figure 4:Œ≥ùõæ\\gammaitalic_Œ≥Visualization.We show a visualization of theŒ≥ùõæ\\gammaitalic_Œ≥outputs of our network that are described in Sec.2.2. In each video sequence, we show the input tracks, where each color visualizes its movement level value,Œ≥ùõæ\\gammaitalic_Œ≥. Purple marks static points with lowŒ≥ùõæ\\gammaitalic_Œ≥whereas red marks dynamic points with highŒ≥ùõæ\\gammaitalic_Œ≥. Note, that our network did not get any direct supervision for these values, but only the raw point tracks predictions from[18]. TheŒ≥ùõæ\\gammaitalic_Œ≥visualizations for cats were produced by the model that was only trained on dogs and vice versa. We note that our model generalizes well to out-of-domain (non-pet) cases as well.",
                "position": 1140
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusions and limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASupplementary results",
        "images": []
    },
    {
        "header": "Appendix BImplementation details",
        "images": []
    }
]