[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07319/x1.png",
                "caption": "Figure 1:Given the expression, we first generate temporal prompts as the reference proposal and candidate tracks. Our proposedTenetframework then selects the one that best aligns with the expression to prompt SAM, achieving referring video object segmentation.",
                "position": 103
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Proposed Framework: Tenet",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07319/x2.png",
                "caption": "Figure 2:Qualitative results when taking visual prompts derived from different methods to prompt SAM on the Refer-DAVIS17dataset.Note that the score in the left is the box mIoU.",
                "position": 225
            },
            {
                "img": "https://arxiv.org/html/2510.07319/x3.png",
                "caption": "Figure 3:Overview of the proposed Tenet framework.We first produce the reference proposal and candidate tracks as described in Section3.2, and then perform Prompt Preference Learning as detailed in Section3.3.",
                "position": 321
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07319/x4.png",
                "caption": "Figure 4:Qualitative results on Ref-YouTube-VOS.",
                "position": 657
            },
            {
                "img": "https://arxiv.org/html/2510.07319/x5.png",
                "caption": "Figure 5:Qualitative results on Ref-DAVIS17.",
                "position": 660
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]