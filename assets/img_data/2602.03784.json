[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03784/x1.png",
                "caption": "Figure 1:Visualization oftwo structural weaknessesof existing LLM-as-a-compressor methods.(i) Representation overwriting: The information carried by the compression token that captures Year2012is gradually overwritten into highly abstract features during the layer-by-layer encoding, leading to information loss for the decoder.(ii) Lack of global coordination: The key tokenParisis not attended by the compression tokens due to the lack of global coordination of compression capacity allocation.",
                "position": 125
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03784/x2.png",
                "caption": "Figure 2:A comparison between existing LLM-as-a-compressor methods (left) and ComprExIT (right).Existing methodsintroduce gist tokens that are iteratively encoded by the self-attention layers in the LLMs, which are trained to aggregate information from context tokens and align the representations to the decoder’s input space.ComprExITinstead leverages the hidden states of the context tokens encoded in a forward pass. The hidden states across layers are selectively aggregated into token anchors, which are then transmitted to the compression tokens through a coordinated transmission plan.",
                "position": 175
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03784/x3.png",
                "caption": "Figure 3:Pearson correlation between compression slots/tokens’ aggregation distributions. For each compression slot, we treat its normalized aggregation weights over input tokens (i.e., the attention weights in ICAE (left) or the transmission plan in ComprExIT (right)) as a vector, and compute pairwise Pearson correlation between these vectors across slots. Large off-diagonal values indicate that different slots aggregate highly overlapping subsets of tokens, reflecting duplicated allocation behavior. As shown, ICAE exhibits substantial off-diagonal correlations with large high correlation areas (marked out in the figure), whereas ComprExIT maintains low inter-slot correlation, indicating a more coordinated information allocation.",
                "position": 927
            },
            {
                "img": "https://arxiv.org/html/2602.03784/x4.png",
                "caption": "Figure 4:Singular value spectrum (normalized) and effective rank (erank in the figure) of the aggregation matrix of compression tokens, where compression slot has a vector of aggregation weights over input tokens. The spectrum shows much lower effective rank of ICAE’s aggregation matrix compared with ComprExIT, indicating much higher allocation redundancies in compression.",
                "position": 931
            },
            {
                "img": "https://arxiv.org/html/2602.03784/x5.png",
                "caption": "Figure 5:Depth-wise gating weights across layers in ComprExIT. The weights represent ComprExIT’s preference for layers at the position of each input token.",
                "position": 945
            },
            {
                "img": "https://arxiv.org/html/2602.03784/x6.png",
                "caption": "(a)",
                "position": 956
            },
            {
                "img": "https://arxiv.org/html/2602.03784/x6.png",
                "caption": "(a)",
                "position": 959
            },
            {
                "img": "https://arxiv.org/html/2602.03784/x7.png",
                "caption": "(b)",
                "position": 964
            }
        ]
    },
    {
        "header": "5Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03784/x8.png",
                "caption": "(a)",
                "position": 1962
            },
            {
                "img": "https://arxiv.org/html/2602.03784/x8.png",
                "caption": "(a)",
                "position": 1965
            },
            {
                "img": "https://arxiv.org/html/2602.03784/x9.png",
                "caption": "(b)",
                "position": 1971
            },
            {
                "img": "https://arxiv.org/html/2602.03784/x10.png",
                "caption": "Figure 8:F1 scores on single-hop and multi-hop QA datasets (SQuAD and HotpotQA) using different layers from Llama-3.2-1B (16 layers) for compression. We use a simple mean-pooling to obtain compression representations.",
                "position": 1985
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]