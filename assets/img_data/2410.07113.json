[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07113/extracted/5914271/figures/logo.png",
                "caption": "",
                "position": 119
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07113/extracted/5914271/figures/pipeline.png",
                "caption": "Figure 1:ThePersonalized Visual Instruction Tuning (PVIT)framework consists of three phases. In thevisual concept curationphase, we extract individuals and their faces from images, then augment them with different poses and angles. During thedual-level textual information extraction and fusionphase, MLLMs first generate both holistic information and personal information, then integrate them to get more detailed and contextually accurate information. In thePVIT dataset generationphase, LLMs create QA pair templates based on the extracted textual information, which are filled with selected names to construct training data.",
                "position": 186
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Problem Formulation",
        "images": []
    },
    {
        "header": "4Data Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07113/extracted/5914271/figures/demo.png",
                "caption": "Figure 2:Qualitative examples of P-LLaVA results: Each example includes the userâ€™s query,input individual photos, and thescene image. Thecurrent MLLMsfail to recognize the person of interest and conduct personalized conversations, whereasour model, after training with PVIT, enables coherent and accurate personalized dialogues. Examples illustrate both answerable and unanswerable scenarios. For answerable cases, inputs involve single or multiple individuals, and our model incorporates names from the prefix for personalized responses. In unanswerable cases, current MLLMs provide incorrect answers, while the our model appropriately refuses and explains the reason.",
                "position": 389
            }
        ]
    },
    {
        "header": "5Evaluation Using P-Bench",
        "images": []
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07113/extracted/5914271/figures/ablation.jpg",
                "caption": "",
                "position": 833
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails of PVIT-3M",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07113/extracted/5914271/figures/pvit-3m.png",
                "caption": "Figure 3:Statistics of PVIT-3M, a large scale personalized instruct tuning dataset. Left: Data Distribution within Each Category. The outer circle shows the distribution of all data categories and the inner circle shows the distribution of data subsets. Right: The detailed quantities of datasets.",
                "position": 1529
            }
        ]
    },
    {
        "header": "Appendix BDetails of P-Bench",
        "images": []
    },
    {
        "header": "Appendix CDetailed Prompts for Data Generation",
        "images": []
    },
    {
        "header": "Appendix DTraining Details",
        "images": []
    }
]