[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.18738/x1.png",
                "caption": "Figure 1.An illustration of VLM-based 3D detection on an apple, showing key advantages including semantic-awareness, zero-shot learning, multimodal fusion, human-aligned queries, and scalable data utilization.",
                "position": 109
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.18738/x2.png",
                "caption": "Figure 2.Temporal trend analysis highlighting the surge in global attention toward LLMs and VLMs. Following the public launch of ChatGPT on November 30, 2022, there has been a marked increase in interest and adoption of VLMs, particularly in domains such as 3D object detection, reflecting a shift toward multimodal and prompt-driven AI systems.",
                "position": 142
            }
        ]
    },
    {
        "header": "2.Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.18738/x3.png",
                "caption": "Figure 3.Conceptual overview of the methodological and analytical framework used in this review. The mindmap illustrates the study’s structure: starting with a comparison of 3D detection methods, followed by VLM-specific architectures, strengths and trade-offs, and ending with a discussion on challenges and future directions.",
                "position": 171
            },
            {
                "img": "https://arxiv.org/html/2504.18738/x4.png",
                "caption": "Figure 4.Methodology diagram of this review paper illustrating the hybrid academic and AI-based search strategy, filtering process, and final paper selection, reducing 459 initial papers to 105.",
                "position": 183
            }
        ]
    },
    {
        "header": "3.Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.18738/x5.png",
                "caption": "Figure 5.Illustration of a Vision-Language Model (VLM) performing multimodal reasoning—detecting, segmenting, and describing objects from images using textual prompts, demonstrating open-vocabulary 3D detection and contextual understanding.",
                "position": 1580
            },
            {
                "img": "https://arxiv.org/html/2504.18738/x6.png",
                "caption": "Figure 6.Overview of VLM architecture showing pretraining and fine-tuning stages. Visual embeddings from an image encoder are projected into a shared space and concatenated with text tokens. During pretraining, only the projector is trained to align visual and textual representations. In fine-tuning, the decoder is adapted to downstream tasks like object detection. This modular design supports flexible multimodal learning, enabling VLMs to generalize across vision-language tasks such as grounding, captioning, and 3D object detection.",
                "position": 1603
            },
            {
                "img": "https://arxiv.org/html/2504.18738/x7.png",
                "caption": "Figure 7.Illustration of 2D and 3D object detection using VLMs in a commercial orchard setting. The robot uses VLMs to perform 2D detection by aligning visual features with textual prompts, enabling zero-shot object identification. These 2D proposals are projected into 3D space using depth data to generate volumetric bounding boxes. Cross-modal fusion with LiDAR or depth inputs refines object localization, allowing context-aware reasoning about apple position and ripeness. This pipeline highlights how VLMs bridge semantic understanding and geometric reasoning for real-world agricultural tasks, enabling robots to detect and interact with fruit in complex, dynamic environments.",
                "position": 1615
            },
            {
                "img": "https://arxiv.org/html/2504.18738/x8.png",
                "caption": "Figure 8.Examples of 3D Object detection with VLMs:(a) Cube R-CNN with VLM-based 3D object detection on OMNI3D, showcasing predictions in a room, kitchen, and traffic scenes with 3D bounding boxes.(Brazil et al.,2023); b) Illustrates CoDA’s 3D detection process(Cao et al.,2023), showcasing its ability to localize and classify novel objects with high accuracy in diverse scenes.",
                "position": 1710
            },
            {
                "img": "https://arxiv.org/html/2504.18738/x9.png",
                "caption": "Figure 9.Examples of 3D Object Detection with VLMs: a) shows 3D detection with VLMs for ”this is a chair by the wall, the fifth chair from the right wall” by(Wang et al.,2024b); b) illustrates ”the left-most table in the center” bby(Wang et al.,2024b); c) depicts ”a black office chair.” by(Wang et al.,2024b); de)(Zhang et al.,2024a)",
                "position": 1728
            }
        ]
    },
    {
        "header": "4.Discussion",
        "images": []
    },
    {
        "header": "5.Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "Declarations",
        "images": []
    },
    {
        "header": "Statement on AI Writing Assistance",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]