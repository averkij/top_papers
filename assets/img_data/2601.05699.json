[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05699/images/final/main_image.png",
                "caption": "Figure 1:Examples of Afri-MCQA  datapoints, containing parallel text and speech QA pairs grounded in culturally relevant images across English and native African languages.",
                "position": 203
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05699/images/final/updated/category_distribution.png",
                "caption": "Figure 2:Image categories in our dataset and their distributions.",
                "position": 517
            }
        ]
    },
    {
        "header": "4Experimental setup",
        "images": []
    },
    {
        "header": "5Evaluation",
        "images": []
    },
    {
        "header": "6Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05699/images/final/heatmap_2.png",
                "caption": "(a)Performance comparison of models on text-based question answering tasks: (a) Text MC-VQA (Multiple Choice) and (b) Text Open-ended QA in English and Native languages.",
                "position": 655
            },
            {
                "img": "https://arxiv.org/html/2601.05699/images/final/heatmap_2.png",
                "caption": "(b)Performance comparison of models on audio-based question answering tasks: (a) Audio MC-VQA (Multiple Choice) and (b) Audio Open-ended VQA in English and Native languages.",
                "position": 660
            },
            {
                "img": "https://arxiv.org/html/2601.05699/images/final/heatmap_2.png",
                "caption": "Figure 4:Human Evaluation for Text Open-ended VQA.Accuracy across best-performing models for English and Native. We observed that, while most models perform best in the English setting, Gemini-2.5 Pro seems to perform better in the Native language.",
                "position": 681
            },
            {
                "img": "https://arxiv.org/html/2601.05699/x1.png",
                "caption": "Figure 5:Audio probing results on native African languages: (a) LID (↑\\uparrowhigher is better), (b) ASR (↓\\downarrowlower is better), and (c) Open-ended VQA (↑\\uparrowhigher is better). (++) means WER is more than 100%",
                "position": 802
            }
        ]
    },
    {
        "header": "7Discussion",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "9Limitations",
        "images": []
    },
    {
        "header": "10Ethical Considerations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImage Categories",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05699/images/final/categories.png",
                "caption": "Figure 6:Language-wise distribution of the categories.",
                "position": 1316
            },
            {
                "img": "https://arxiv.org/html/2601.05699/images/final/category_faceted.png",
                "caption": "Figure 7:Top 6 Image category distribution",
                "position": 1322
            }
        ]
    },
    {
        "header": "Appendix BPrompt",
        "images": []
    },
    {
        "header": "Appendix CAdditional Experiments results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05699/images/final/heatmap-MCQA.png",
                "caption": "Figure 10:Language-wise accuracy of seven multimodal LLMs on the text-based VQA task, evaluated separately on English and native African language questions.",
                "position": 4752
            }
        ]
    },
    {
        "header": "Appendix DCorrelation statistics",
        "images": []
    },
    {
        "header": "Appendix FAnnotator Demography",
        "images": []
    }
]