[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20776/x1.png",
                "caption": "",
                "position": 114
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20776/x2.png",
                "caption": "Figure 2:Method overview.Given an input monocular video, we infer 2D priors to segment static background (represented by static 3D Gaussians augmented with latent features) and dynamic foreground (represented by dynamic 3D Gaussians guided by Motion Scaffolds[35], a set of nodes{ùêØi}subscriptùêØùëñ\\{\\mathbf{v}_{i}\\}{ bold_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }encoding 3D motion trajectories and latent featureshisubscript‚Ñéùëñh_{i}italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT). Dynamic Gaussian features and motions are computed via interpolation from theirKùêæKitalic_K-nearest scaffold nodes. At each timestep, dynamic Gaussians are warped and fused with static Gaussians. A parallel rasterization[102]generates RGB images and a unified latent feature map, decoded into task-specific features‚Äîillustrated here by SAM2[68], CLIP-LSeg[36], and InternVideo2[84]for representative 2D (novel view segmentation), 3D (scene editing), and 4D (spatiotemporal VQA) tasks. Our framework generalizes to any 2D vision foundation model and is trained end-to-end using input RGB frames and customized features from pretrained 2D models. At inference, rendered feature maps from arbitrary views and timesteps are directly fed into task-specific decoders, seamlessly supporting user prompts and LLM interactions to form a unified 4D agentic AI system.",
                "position": 160
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20776/x3.png",
                "caption": "Figure 3:Segment Anything in Dynamic 4D Scenes with SAM2 Feature Field.For any rendered novel view video, we support: (a) Promptless segmentation (segment everything): when no user prompt is provided, segmentation masks are automatically assigned at the first frame (t=0ùë°0t=0italic_t = 0) and then propagated across all frames. (b) Promptable segmentation (segment anything): the user can segment any object‚Äîstatic or dynamic‚Äîat any timestep using a point or box prompt, and the corresponding mask is robustly tracked and propagated through subsequent frames.",
                "position": 203
            },
            {
                "img": "https://arxiv.org/html/2503.20776/x4.png",
                "caption": "Figure 4:Baseline Comparison on SAM2 Inference.We compare segmentation quality and inference speed between (a) the naive RGB-based approach and (b) our feature-based method. Ours achieves comparable segmentation, accurately tracking the object over time, and avoids RGB artifacts (red box region att=70ùë°70t=70italic_t = 70), while reducing inference time to about 4√ó\\times√óspeed-up.",
                "position": 274
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20776/x5.png",
                "caption": "Figure 5:Semantic 4D Scene Understanding with CLIP Feature Field.By lifting CLIP-LSeg[36]features into a 4D feature field, we enable pixel-level semantic segmentation from any view at any timestep. This allows robust 4D scene understanding, even as object appearances change over time‚Äîfor example, accurately identifying a blooming flower from bud to full bloom across views.",
                "position": 315
            },
            {
                "img": "https://arxiv.org/html/2503.20776/x6.png",
                "caption": "Figure 6:Scene Editing with AI Agent.Given user prompts, our GPT-powered agent interprets editing intent and autonomously performs scene edits via our 4D CLIP feature field. Examples include both geometric (e.g., ‚Äúextract‚Äù and ‚Äúdelete‚Äù) and appearance (e.g., ‚Äúchange color‚Äù) editing in 3D space. While results may not be perfect due to imperfect fine-grained feature alignment and non-optimal editing parameter tuning, the agent adaptively refines parameters and applies edits consistently across views and time‚Äîgreatly reducing the need for manual tuning‚Äîand demonstrates robust, interactive 4D scene manipulation.",
                "position": 371
            },
            {
                "img": "https://arxiv.org/html/2503.20776/x7.png",
                "caption": "Figure 7:VQA with Chatbot Agent.(Left) Our model supports free-form VQA across diverse question types‚Äîgeneral, spatial, and temporal‚Äîby distilling InternVideo2[84]features. (Right) At each timestep, we reconstruct both a 4D radiance field and a 4D feature field, providing more inference sources beyond the input video frame‚Äîincluding local (moving camera) and global (zoomed-out) novel views and their corresponding feature maps‚Äîthereby supporting VQA in 4D and enhancing the model‚Äôs spatiotemporal reasoning capabilities.",
                "position": 415
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "ADetails of 4D Reconstruction",
        "images": []
    },
    {
        "header": "BImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20776/x8.png",
                "caption": "Figure A:Feature Field Visualizations. We visualize our versatile Gaussian feature field along with its decoded SAM2, CLIP, and InternVideo feature fields using PCA.",
                "position": 2067
            }
        ]
    },
    {
        "header": "CFeature Extraction and Inference with Foundation Models",
        "images": []
    },
    {
        "header": "DDetails of LLM-powered 4D Editing",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20776/x9.png",
                "caption": "Figure B:Overview of the editing framework.GPT-4o generates different editing configurations based on user prompts, selects target regions via hybrid filtering, evaluates their outputs, and selects the best configuration.",
                "position": 2112
            }
        ]
    },
    {
        "header": "EBaseline Comparisons",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20776/x10.png",
                "caption": "Figure C:CLIP semantic segmentation quality comparison. We compare the CLIP semantic segmentation quality between ground-truth (inference from RGB) and our implementation (inference from feature map) for both training and novel views.",
                "position": 2682
            },
            {
                "img": "https://arxiv.org/html/2503.20776/x11.png",
                "caption": "Figure D:SAM2 segmentation quality comparison for different dimensions of unified latent feature mapsBest performing SAM2 segmentation is derived from the 32-dimensional unified latent feature map.",
                "position": 2904
            },
            {
                "img": "https://arxiv.org/html/2503.20776/x12.png",
                "caption": "Figure E:Training Time vs Unified Latent Feature DimensionsWe show the training time required with different dimensions of unified latent feature map.",
                "position": 2907
            },
            {
                "img": "https://arxiv.org/html/2503.20776/x12.png",
                "caption": "Figure E:Training Time vs Unified Latent Feature DimensionsWe show the training time required with different dimensions of unified latent feature map.",
                "position": 2910
            },
            {
                "img": "https://arxiv.org/html/2503.20776/x13.png",
                "caption": "Figure F:Rendering Time vs Unified Latent Feature DimensionsWe show the rendering time required for different dimensions of unified latent feature map.",
                "position": 2915
            },
            {
                "img": "https://arxiv.org/html/2503.20776/x14.png",
                "caption": "Figure G:Training Time vs CLIP Feature DimensionsWe show the training time required with different dimensions of rendered CLIP features.",
                "position": 2921
            },
            {
                "img": "https://arxiv.org/html/2503.20776/x15.png",
                "caption": "Figure H:Rendering Time vs CLIP Feature DimensionsWe show the rendering time required for different dimensions of rendered CLIP features.",
                "position": 2926
            },
            {
                "img": "https://arxiv.org/html/2503.20776/x16.png",
                "caption": "Figure I:mIoU vs CLIP Feature DimensionsWe show mIoU with respect to different rendered CLIP feature dimensions.",
                "position": 2932
            },
            {
                "img": "https://arxiv.org/html/2503.20776/x17.png",
                "caption": "Figure J:Accuracy vs CLIP Feature DimensionsWe show accuracy with respect to different rendered CLIP feature dimensions.",
                "position": 2937
            }
        ]
    },
    {
        "header": "FAblation Studies",
        "images": []
    }
]