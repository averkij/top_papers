[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.01634/x1.png",
                "caption": "Figure 1:Zero-shot predictions.DepthAnything-AC achieves better details compared to other approaches, including Depth Anything V2[90], DepthPro[7], and RobustDepth[61], under challenging lighting and climatic conditions.",
                "position": 123
            },
            {
                "img": "https://arxiv.org/html/2507.01634/x2.png",
                "caption": "Figure 2:Visualization of perturbation types.We apply a combination of darkness, weather, blur, and contrast perturbations during training. Note that different weather or blur perturbations do not appear simultaneously. For example, motion blur and zoom blur are mutually exclusive.",
                "position": 176
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.01634/x3.png",
                "caption": "Figure 3:Training pipeline for DepthAnyting-AC.The perturbation consistency framework encourages DepthAnything-AC to generate consistent predictions under augmentations while retaining generality via the frozen original model. To enhance semantic boundaries and details, the spatial distance constraint is used to strengthen the understanding of inter-patch relationships.",
                "position": 214
            }
        ]
    },
    {
        "header": "3Depth Anything at Any Condition",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.01634/x4.png",
                "caption": "Figure 4:Visualization of spatial distance relationships corresponding to different query tokens.The pentagram denotes the current query token, illustrating the spatial distance between the patch containing the pentagram and all other patches in the frame.",
                "position": 285
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.01634/x5.png",
                "caption": "Figure 5:Visualization of feature representations.‘Feature’ refers to the representation extracted from the original image, while the two columns on the right show the features generated from the perturbed images.\nCompared to Depth Anything V2[90], our DepthAnything-AC demonstrates the ability to recover feature representations from perturbed images.",
                "position": 1241
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ADetailed Evaluation Benchmarks",
        "images": []
    },
    {
        "header": "Appendix BMore Experimental Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.01634/x6.png",
                "caption": "Figure 6:Comparison between DepthAnything-V2[90]and our DepthAnything-AC",
                "position": 3355
            },
            {
                "img": "https://arxiv.org/html/2507.01634/x7.png",
                "caption": "Figure 7:Comparison between DepthAnything-V1[89]and our DepthAnything-AC",
                "position": 3358
            },
            {
                "img": "https://arxiv.org/html/2507.01634/x8.png",
                "caption": "Figure 8:Comparison between DepthPro[7]and our DepthAnything-AC",
                "position": 3361
            }
        ]
    },
    {
        "header": "Appendix CMore Visualization",
        "images": []
    }
]