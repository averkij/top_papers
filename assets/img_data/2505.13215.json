[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13215/extracted/6454415/images/fast.jpg",
                "caption": "",
                "position": 127
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary",
        "images": []
    },
    {
        "header": "4Hybrid 3D-4D Gaussian Splatting",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13215/extracted/6454415/images/scale.jpg",
                "caption": "Figure 2:Distribution of the t-axis scale for Gaussians in thecoffee_martiniscene. Most Gaussians cluster at smaller scales, indicating dynamic content, while a minority have larger scales that suggest static regions.",
                "position": 321
            },
            {
                "img": "https://arxiv.org/html/2505.13215/extracted/6454415/images/main3.png",
                "caption": "Figure 3:Overview of our hybrid 3D‚Äì4D Gaussian Splatting framework. (a) 4D Gaussians are optimized over time, and those exceeding a temporal scale threshold (œÑùúè\\tauitalic_œÑ) are converted into 3D Gaussians. (b) Both 3D and 4D Gaussians are projected into screen space, assigned tile and depth keys, and sorted for rasterization. The rendered image is generated by blending static (3D) and dynamic (4D) Gaussians.",
                "position": 324
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13215/extracted/6454415/images/qual1.jpg",
                "caption": "Figure 4:Qualitative comparison on the N3V dataset. While most methods yield comparable results,\nour approach can preserve subtle motion cues and slightly more consistent colors in some challenging regions.\nZoom in for best viewing.",
                "position": 875
            },
            {
                "img": "https://arxiv.org/html/2505.13215/extracted/6454415/images/tau.jpg",
                "caption": "Figure 5:Visual comparison of different scale thresholdsœÑùúè\\tauitalic_œÑ.",
                "position": 973
            },
            {
                "img": "https://arxiv.org/html/2505.13215/extracted/6454415/images/opa.jpg",
                "caption": "Figure 6:Influence of opacity resets on a dynamic scene.",
                "position": 984
            },
            {
                "img": "https://arxiv.org/html/2505.13215/extracted/6454415/images/numpoints.jpg",
                "caption": "Figure 7:Visualization of spatially distributed Gaussians.",
                "position": 1003
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ACUDA Rasterization Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13215/extracted/6454415/images/supple.png",
                "caption": "Figure 8:Per-scene PSNR curves on the N3V datasetfor different temporal scale thresholds\n(œÑ=2.5,3.0,3.5ùúè2.53.03.5\\tau=2.5,3.0,3.5italic_œÑ = 2.5 , 3.0 , 3.5). Each plot corresponds to one scene, showing how PSNR evolves over\n6000 iterations of training. The mid-range setting (œÑ=3.0ùúè3.0\\tau=3.0italic_œÑ = 3.0) often strikes a balance,\nmaintaining competitive final quality across a range of motion complexities.",
                "position": 2101
            },
            {
                "img": "https://arxiv.org/html/2505.13215/extracted/6454415/images/long.jpg",
                "caption": "Figure 9:Comparison with Ground Truth on the 40-second sequence.We sample frames at different timestamps (top: GT, bottom: ours) to illustrate that our approach\npreserves both global structure and subtle motion details over extended temporal ranges.",
                "position": 2107
            },
            {
                "img": "https://arxiv.org/html/2505.13215/extracted/6454415/images/qual_supple.jpg",
                "caption": "Figure 10:Additional results on N3V and Technicolor scenes.Despite challenging lighting conditions and fast motion, our hybrid 3D-4D approach maintains crisp object boundaries\nand more consistent textures across frames.",
                "position": 2112
            },
            {
                "img": "https://arxiv.org/html/2505.13215/extracted/6454415/images/static.png",
                "caption": "Figure 11:Dynamic vs.¬†Static Visualization.Each row shows (left) the dynamic portion on a white background, (middle) the static region,\nand (right) the fully rendered result. By converting most static elements into 3D Gaussians,\nour approach effectively handles dynamic scenes while reducing redundant computations\nand preserving high-fidelity details.",
                "position": 2117
            }
        ]
    },
    {
        "header": "Appendix BAdditional Results",
        "images": []
    }
]