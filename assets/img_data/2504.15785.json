[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15785/x1.png",
                "caption": "",
                "position": 155
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15785/x2.png",
                "caption": "Figure 1:Overview ofWALL-E 2.0. The agent determines actions to take via MPC, which optimizes future steps’ actions by interacting with a neurosymbolic world model. The world model adopts an LLM whose predictions are aligned with environment dynamics through code rules converted from symbolic knowledge (action rules, knowledge/scene graph) learned via inductive reasoning from real trajectories and predicted trajectories.",
                "position": 179
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3World Alignment by NeuroSymbolic Learning\n(WALL-E 2.0)",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15785/x3.png",
                "caption": "Figure 2:NeuroSymbolic Learning of Code Rules.WALL-E 2.0iteratively refines the symbolic knowledge with the agent’s actual trajectories in the environment and the world model’s predicted trajectories.\nThe NeuroSymbolic learning takes 4 stages: (1) comparing predicted and actual trajectories; (2) learning new symbolic knowledge from real trajectories; (4) translating symbolic knowledge to code; and (4) Code rule set pruning via solving a maximum coverage problem.",
                "position": 278
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15785/x4.png",
                "caption": "Figure 3:WALL-E 2.0vs. baselines on 134 testing tasks from the ALFWorld.WALL-E 2.0exhibiting superior planning ability and achieves the highest success rate after only 4 iterations, significantly surpassing other baselines.",
                "position": 1166
            },
            {
                "img": "https://arxiv.org/html/2504.15785/x5.png",
                "caption": "Figure 4:Comparison betweenWALL-E 2.0and IfR (the best baseline in Table1) over learning iterations in Mars.WALL-E 2.0achieves a clear advantage over IfR in both learning efficiency and overall performance, due to the world alignment with diverse symbolic knowledge and code rules.",
                "position": 1198
            },
            {
                "img": "https://arxiv.org/html/2504.15785/x6.png",
                "caption": "Figure 5:Left:WALL-E 2.0code rules’ cover rate (higher the better) over neurosymsbolic learning iterations in Mars. The cover rate measures the percentage of LLM failed predictions that can be corrected by our world model.Right:\nComparison betweenWALL-E 2.0’s world model and skill library when applied to LLM agents with reasoning and reflection (base agent).WALL-E 2.0—base agent + world model;\nSkill Library—base agent + skill library.WALL-E 2.0’s neurosymbolic learning significantly improves world alignment and brings greater gains to LLM agents than skill library.",
                "position": 1210
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetailed Related Work",
        "images": []
    },
    {
        "header": "Appendix BDetailed Prompt",
        "images": []
    },
    {
        "header": "Appendix CEnvironments’ Observation Space and Action Space",
        "images": []
    },
    {
        "header": "Appendix DLearned Rules",
        "images": []
    },
    {
        "header": "Appendix EExperiment Details",
        "images": []
    },
    {
        "header": "Appendix FLimitation and Future Work",
        "images": []
    }
]