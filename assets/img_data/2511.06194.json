[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.06194/x1.png",
                "caption": "Figure 1:Text-to-CAD generation results from NURBGen, showcasing reconstructed CAD models from text prompts.",
                "position": 64
            }
        ]
    },
    {
        "header": "Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.06194/x2.png",
                "caption": "Figure 2:Overview of our partABC dataset, data preparation and fine-tuning pipeline.Left:We extract part-level CAD models from the ABC dataset by decomposing CAD assemblies into individual components.Middle:Each part is represented using a hybrid formatâ€”faces are encoded as untrimmed NURBS surfaces, with analytic primitives used where NURBS fitting fails. We also generate high-quality captions using InternVL3-13B with a metadata-guided annotation pipeline.Right:We fine-tune Qwen3-4B to map text captions to structured hybrid CAD representations, which can be directly converted to BRep models.",
                "position": 108
            }
        ]
    },
    {
        "header": "Background",
        "images": []
    },
    {
        "header": "Data Preparation",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.06194/x3.png",
                "caption": "Figure 3:Our proposed hybrid representation.Left:Untrimmed NURBS surfaces introduce artifacts in hole-like or thin regions.RightWe resolve this by substituting their NURB representation with analytic curves (e.g., lines, circles) for improved geometric fidelity.",
                "position": 220
            },
            {
                "img": "https://arxiv.org/html/2511.06194/x4.png",
                "caption": "Figure 4:Qualitative comparison of reconstructed CAD models from text prompts. From top to bottom, we show generations from GPT-4o, DeepCAD, Text2CAD, and our proposed NURBGen. NURBGen consistently produces more detailed and structurally coherent results, with higher fidelity to the input prompt and fewer geometric artifacts compared to baselines.",
                "position": 223
            },
            {
                "img": "https://arxiv.org/html/2511.06194/x5.png",
                "caption": "Figure 5:Captions from partABC dataset generated using our captioning pipeline.",
                "position": 265
            }
        ]
    },
    {
        "header": "Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.06194/x6.png",
                "caption": "Figure 6:Example CAD parts from the partABC dataset across complexity tiers- simple (top-left), moderate (top-right), and complex (bottom).",
                "position": 297
            },
            {
                "img": "https://arxiv.org/html/2511.06194/x7.png",
                "caption": "Figure 7:Failure cases of NURBGen illustrating limitations in handling complex prompts, geometric artifacts like self-intersections, and challenges in text engraving.",
                "position": 310
            }
        ]
    },
    {
        "header": "Ablation Study",
        "images": []
    },
    {
        "header": "Limitation",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.06194/x8.png",
                "caption": "Figure 8:Comparison of NURBS-only (left) and hybrid (right) models, showing improved handling of thin and hole-adjacent regions.",
                "position": 402
            }
        ]
    },
    {
        "header": "Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "CAD Representation",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.06194/x9.png",
                "caption": "Figure 9:Our proposed hybrid CAD representation.",
                "position": 626
            },
            {
                "img": "https://arxiv.org/html/2511.06194/x10.png",
                "caption": "Figure 10:Impact of metadata-guided annotation in our annotation pipeline. From left to right: CAD model, caption generated by GPT-4o without metadata, and caption from our pipeline using metadata such as dimensions, and hole count.",
                "position": 629
            },
            {
                "img": "https://arxiv.org/html/2511.06194/x11.png",
                "caption": "Figure 11:Impact of metadata-guided annotation in our annotation pipeline. From left to right: CAD model, caption generated by GPT-4o without metadata, and caption from our pipeline using metadata such as dimensions, and hole count.",
                "position": 632
            }
        ]
    },
    {
        "header": "Impact of Metadata on Caption Quality",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.06194/x12.png",
                "caption": "Figure 12:Text-to-CAD generation using NURBGen.",
                "position": 647
            },
            {
                "img": "https://arxiv.org/html/2511.06194/x13.png",
                "caption": "Figure 13:Text-to-CAD generation using NURBGen.",
                "position": 650
            }
        ]
    },
    {
        "header": "More Qualitative Results",
        "images": []
    }
]