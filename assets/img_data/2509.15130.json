[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15130/x1.png",
                "caption": "Figure 1:We present WorldForge, a fully training-free framework leveraging a pre-trained video diffusion model for various 3D/4D tasks, such as monocular 3D scene generation (up) and dynamic 4D scene re-rendering (down), enabling precise camera trajectory control and high-quality outputs.",
                "position": 84
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15130/x2.png",
                "caption": "Figure 2:Overview of our porposed method. Given a single image or video frames, a vision foundation model reconstructs a scene point cloud, which is warped and rendered along a user-specified trajectory to produce a guidance video. The input image (or first frame) is also converted into a textual prompt and latent representation for an image-to-video diffusion model. Trajectory control is injected through a training-free strategy comprising IRR, FLF, and DSG (detailed in Sec. 3.2–3.4), enabling precise control and high-quality synthesis without additional training.",
                "position": 145
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Proposed Methods",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15130/x3.png",
                "caption": "Figure 3:Qualitative comparison of novel view synthesis from a single input image. The first row shows the input frame and its depth-based warped views, where disoccluded regions appear as holes. Compared to existing SOTA methods, our approach produces more consistent scene content under novel viewpoints, with improved image detail, trajectory accuracy, and structural plausibility.",
                "position": 434
            },
            {
                "img": "https://arxiv.org/html/2509.15130/x4.png",
                "caption": "Figure 4:Static 3D generation on human-centric scenes. Existing methods struggle, particularly with motion-prone shots (left) and portrait close-ups (right). On the left, baselines introduce artifacts and unintended motion. On the right, most fail to produce plausible results; TrajectoryCrafter(Yu et al.,2025)recovers coarse structure but lacks detail and visual appeal. In contrast, our method maintains scene stationarity under trajectory guidance and produces natural, faithful renderings, achieving both precise control and high perceptual quality.",
                "position": 855
            },
            {
                "img": "https://arxiv.org/html/2509.15130/x5.png",
                "caption": "Figure 5:360° orbit views from a single real-world outdoor image. With precise trajectory control and realistic rendering, our method overcomes the viewpoint limitation of single-image generation and produces ultra-wide views of complex real scenes. Unlike panorama-based approaches, it directly supports object-centric trajectories and achieves higher visual quality.",
                "position": 861
            },
            {
                "img": "https://arxiv.org/html/2509.15130/x6.png",
                "caption": "Figure 6:Comparison of 4D trajectory-controlled re-rendering. Baselines often produce implausible artifacts (e.g., flattened faces, floating heads), reflecting limited use of pretrained priors. Our inference-time guidance leverages these latent world priors to re-render realistic, high-quality content along the target trajectory. We compare against state-of-the-art baselines under identical inputs; for ReCamMaster (text-controlled), parameters are adjusted to match the target path.",
                "position": 874
            },
            {
                "img": "https://arxiv.org/html/2509.15130/x7.png",
                "caption": "Figure 7:Other video effects enabled by our method. Beyond video re-cam, our flexible depth-based warping also supports various video editing operations, such as freezing the camera, stabilizing video, and editing video content. These extensions further broaden the practical scope of our approach.",
                "position": 881
            },
            {
                "img": "https://arxiv.org/html/2509.15130/x8.png",
                "caption": "Figure 8:Ablation of the proposed components. IRR enables trajectory injection; without it, the model defaults to prompt-only free generation, and FLF/DSG cannot be applied. FLF decouples trajectory cues from noisy content; removing it introduces noise from warped frames. DSG guides sampling toward high-quality, trajectory-consistent results; without it, detail and plausibility drop. The full model achieves the best fidelity and control, demonstrating their complementary effects.",
                "position": 892
            },
            {
                "img": "https://arxiv.org/html/2509.15130/x9.png",
                "caption": "Figure 9:Ablation across different VDMs. To rule out the influence of the intrinsic performance advantage of the VDM (Wan2.1(Wan et al.,2025)) and to verify the method’s transferability, we port the proposed guidance to a compact U-Net–based SVD model(Blattmann et al.,2023)and compare against SVD-based SOTA baselines. Experiments show that the guidance transfers seamlessly, makes the native SVD controllable, and achieves SOTA performance in content quality, structural plausibility, and trajectory consistency.",
                "position": 904
            },
            {
                "img": "https://arxiv.org/html/2509.15130/x10.png",
                "caption": "Figure 10:Depth estimation models ablation. Our method leverages the inherent world knowledge of VDMs to correct errors and fill missing regions even under challenging inputs (left). This strong self-correction ability ensures broad compatibility with different depth estimators (right). Despite variations or noise in depth-based warping, it reliably compensates through learned priors and produces realistic, high-quality results.",
                "position": 907
            }
        ]
    },
    {
        "header": "5Conclusion and Limitation",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProof of the equivalence between diffusion and flow models",
        "images": []
    },
    {
        "header": "Appendix BEvaluation Metrics",
        "images": []
    }
]