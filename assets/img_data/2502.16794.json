[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16794/extracted/6227110/figures/ear.png",
                "caption": "",
                "position": 104
            },
            {
                "img": "https://arxiv.org/html/2502.16794/extracted/6227110/figures/brain.png",
                "caption": "",
                "position": 104
            },
            {
                "img": "https://arxiv.org/html/2502.16794/x1.png",
                "caption": "Figure 1:AAD-LLM is a brain-computer interface (BCI) for auditory scene understanding. It decodes neural signals to identify the attended speaker and integrates this information into a language model, generating responses that align with the listener’s perceptual focus.",
                "position": 148
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Intention-Informed Auditory Scene Understanding",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16794/x2.png",
                "caption": "Figure 2:AAD-LLM is a multimodal, attention-aware LLM designed for auditory scene understanding. The model takes three inputs: a textual question, an auditory scene (containing mixed speech sources), and neural signals representing listener attention. Each input is processed by dedicated modules before being integrated into the LLM. AAD-LLM decodes auditory attention to determine the attended speaker and prioritizes information from the target speaker while generating responses. The model is trained to differentiate between attended and ignored speech, ensuring that its output aligns with the listener’s perceptual focus.",
                "position": 211
            }
        ]
    },
    {
        "header": "3AAD-LLM",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16794/x3.png",
                "caption": "Figure 3:(a) Objective evaluation across four tasks. The dotted lines represent the lower bound (Qwen2-Audio given the mixture sound) and upper bound (Qwen2-Audio given the attended talker as oracle). “Qwen2-Audio Random” represents the model receiving a randomly selected talker. “AAD-LLM Decoded Attention” represents our method, where attention is decoded from brain signals, while \"AAD-LLM Oracle Attention\" represents our model given the actual attended talker as oracle. (b) Subjective evaluation measuring the alignment between model outputs and human listeners, who assessed whether the model’s response matched what should have been the answer for the attended talker. * denotes p < 0.05, ** denotes p < 0.01, *** denotes p < 0.001.",
                "position": 240
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16794/extracted/6227110/figures/snow.png",
                "caption": "Table 2:Intention-informed auditory scene understanding performance when listeners were attending to one of the two speakers with background noise. A higher number indicates a better performance for all metrics except word error rate (WER). Full results with more metrics are shown in the Table5.",
                "position": 412
            },
            {
                "img": "https://arxiv.org/html/2502.16794/extracted/6227110/figures/snow.png",
                "caption": "Table 4:Mean Opinion Scores (MOS) for Foreground Summarization and Free Q&A tasks. “Oracle speaker” serves the performance upper bound.",
                "position": 673
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethical Statement",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADataset Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16794/x4.png",
                "caption": "Figure 4:Electrode coverage across subjects in the clinical dataset, with each subject represented by a different color.",
                "position": 1568
            },
            {
                "img": "https://arxiv.org/html/2502.16794/extracted/6227110/figures/snow.png",
                "caption": "Table 5:Intention-informed auditory scene understanding performance in the clinical testing set when listeners were attending to one of the two speakers with background noise. A higher number indicates a better performance for all metrics except word error rate (WER). This is the full version of Table2.",
                "position": 1580
            }
        ]
    },
    {
        "header": "Appendix BModel and Training Details",
        "images": []
    },
    {
        "header": "Appendix CTasks, Prompts, and Metrics",
        "images": []
    },
    {
        "header": "Appendix DAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16794/extracted/6227110/figures/snow.png",
                "caption": "Table 6:AAD-LLM’s attentional state can be transferred to other tasks unseen in intention-informed training, such as speech translation.",
                "position": 1993
            },
            {
                "img": "https://arxiv.org/html/2502.16794/extracted/6227110/figures/snow.png",
                "caption": "Table 7:Free Q&A on same-topic speech mixtures. Both foreground and background speech can be used to answer the question, so auditory attention to the correct speaker is necessary to answer the question correctly.",
                "position": 2075
            },
            {
                "img": "https://arxiv.org/html/2502.16794/extracted/6227110/figures/fire.png",
                "caption": "Table 8:Offline evaluation on speech-only data. One of the two speaker is randomly designated as the foreground and the other as the background.",
                "position": 2109
            },
            {
                "img": "https://arxiv.org/html/2502.16794/x5.png",
                "caption": "Figure 5:t-SNE visualization of x-vectors after K-Means clustering (K=8). The left plot (clinical dataset, 280 sentences) shows distinct clusters for eight speakers. The right plot (speech-only dataset, 10,000 sentences) reveals two main clusters for male and female speakers. Colors indicate K-means clustering with eight groups.",
                "position": 2414
            }
        ]
    },
    {
        "header": "Appendix EAnalysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16794/x6.png",
                "caption": "Figure 6:In the left figure, the two speakers have distinct voice characteristics, so even with an incorrectly predicted speaker label, AAD-LLM can still attend to the correct speaker. In contrast, in the right figure, the two speakers have very similar voice characteristics, making it difficult for AAD-LLM to distinguish between the foreground and background speakers—even with a correctly predicted speaker label.",
                "position": 2438
            },
            {
                "img": "https://arxiv.org/html/2502.16794/extracted/6227110/figures/PsychoExpScreenshot.jpeg",
                "caption": "Figure 7:A screenshot from the psychcophysics task where human raters were asked to rate answers from different models (blind and in random order).",
                "position": 2547
            }
        ]
    },
    {
        "header": "Appendix FSubjective Evaluation Details",
        "images": []
    }
]