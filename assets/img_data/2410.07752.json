[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/fig1.png",
                "caption": "Figure 1:TVBench a temporal video-language benchmark.In TVBench, state-of-the-art text-only, image-based, and most video-language models perform close to random chance, with only the latest strong temporal models, such as Tarsier, outperforming the random baseline. In contrast to MVBench, the performance of these temporal models significantly drops when videos are reversed.",
                "position": 124
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Problems in Video MCQA Benchmarks",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/static_bias.png",
                "caption": "Figure 2:Spatial bias of MVBench video-language benchmark.We show different tasks of the MVBench benchmark and observe that the question can be answered without requiring any temporal understanding. For quantitative results see Tab.1.",
                "position": 152
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/tab_Static_bias.png",
                "caption": "Table 1:Spatial bias of the MVBench video-language benchmark.Our evaluation reveals that temporal understanding is not required for solving temporal tasks of MVBench such as fine-grained action. We find that tasks can be solved with a random frame, and shuffling videos does not impact the performance of current video-language models.",
                "position": 178
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/text_bias.png",
                "caption": "Figure 3:Textual bias of MVBench video-language benchmark.We show different tasks of MVBench and find that questions can be answered without taking the visual part into account.",
                "position": 199
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/tab_text_bias.png",
                "caption": "Table 2:Textual bias of the MVBench video-language benchmark. Our analysis reveals that vision is not required for solving tasks from MVBench as text-only LLMs score high above the random baseline and nearly on par with video models.",
                "position": 213
            }
        ]
    },
    {
        "header": "4Open-ended QA to the rescue?",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/oeqa_eval.png",
                "caption": "Figure 4:Unreliability of open-ended video-language benchmarks.GPT 3.5 is commonly used as an evaluator of open-ended responses, here we use Llama 3 in a text-only setting to generate answers. GPT gives confusing accuracies and scores. Smiley emoji shows truthful or unreliable evaluation from GPT 3.5.",
                "position": 241
            }
        ]
    },
    {
        "header": "5TVBench: A temporal video question answering Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/tv_bench_principle.png",
                "caption": "Figure 5:Defining temporally hard answer candidates in TVBench.To address the spatial bias in MVBench, we redesign questions and answers to be temporally challenging.",
                "position": 445
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/barplot.png",
                "caption": "Figure 6:TVBench addresses the limitations of MVBench.In TVBench, text-only, image-based, and basic video models perform on par with random chance. Only strong temporal models, such as Tarsier, surpass the random baseline. Notably, shuffling the video leads to a significant performance drop on TVBench, whereas it has little impact on MVBench.",
                "position": 448
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix_tvbench/action_antonym_S001C001P003R001A008_rgb.jpg",
                "caption": "Figure 7:TVBench: Samples from our benchmark (1).Row 1-2: Action Antonym; Row 3-4: Action Count; Row 5-6: Action Localization; Row 7-8: Action Sequence.",
                "position": 1624
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix_tvbench/action_antonym_S001C001P003R001A008_rgb.jpg",
                "caption": "",
                "position": 1627
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix_tvbench/action_antonym_S005C001P021R001A021_rgb.jpg",
                "caption": "",
                "position": 1632
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix_tvbench/action_count_video_145.jpg",
                "caption": "",
                "position": 1637
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix_tvbench/action_count_video_308.jpg",
                "caption": "",
                "position": 1642
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix_tvbench/action_localization_6IL0C.jpg",
                "caption": "",
                "position": 1647
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix_tvbench/action_localization_8CCEV.jpg",
                "caption": "",
                "position": 1652
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix_tvbench/action_sequence_1HGEX.jpg",
                "caption": "",
                "position": 1657
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix_tvbench/action_sequence_B7LO8.jpg",
                "caption": "",
                "position": 1662
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix_tvbench/count_video_10023.jpg",
                "caption": "Figure 8:TVBench: Samples from our benchmark (2).Row 1-2: Object Count; Row 3-4: Moving Direction; Row 5-6: Object Shuffle.",
                "position": 1668
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix_tvbench/count_video_10023.jpg",
                "caption": "",
                "position": 1671
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix_tvbench/count_video_10081.jpg",
                "caption": "",
                "position": 1676
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix_tvbench/moving_direction_video_10040.jpg",
                "caption": "",
                "position": 1681
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix_tvbench/moving_direction_video_10068.jpg",
                "caption": "",
                "position": 1686
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix_tvbench/object_shuffle_video_441.jpg",
                "caption": "",
                "position": 1691
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix_tvbench/object_shuffle_video_482.jpg",
                "caption": "",
                "position": 1696
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix_tvbench/scene_transition_Top023_05940.jpg",
                "caption": "Figure 9:TVBench: Samples from our benchmark (3).Row 1-2: Scene Transition; Row 3-4: Unexpected Action.",
                "position": 1702
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix_tvbench/scene_transition_Top023_05940.jpg",
                "caption": "",
                "position": 1705
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix_tvbench/scene_transition_Top014_03245.jpg",
                "caption": "",
                "position": 1710
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix_tvbench/unexpected_action_H_H_121_3397_3512.jpg",
                "caption": "",
                "position": 1715
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix_tvbench/unexpected_action_M_C_654_0217_0421.jpg",
                "caption": "",
                "position": 1720
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Object_Interaction_GLGQJ.jpg",
                "caption": "Figure 10:Spatial bias in MVBench (1).Multiple-choice questions from various MVBench tasks can be solved using only a single frame from the video.",
                "position": 1735
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Object_Interaction_GLGQJ.jpg",
                "caption": "",
                "position": 1738
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/State_Change_video_10997.jpg",
                "caption": "",
                "position": 1743
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Unexpected_Action_C_KT_12_5112_5200.jpg",
                "caption": "",
                "position": 1748
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Unexpected_Action_C_KT_16_1059_1115.jpg",
                "caption": "",
                "position": 1753
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Unexpected_Action_C_KT_18_7649_7708.jpg",
                "caption": "",
                "position": 1758
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Unexpected_Action_H_H_103_0920_1010.jpg",
                "caption": "",
                "position": 1763
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Fine-grained_Action_getty-female-biathlon-athlete-crossing-the-finish-line-first-video-id481224542_14.jpg",
                "caption": "Figure 11:Spatial bias in MVBench (2).Multiple-choice questions from various MVBench tasks can be solved using only a single frame from the video.",
                "position": 1769
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Fine-grained_Action_getty-female-biathlon-athlete-crossing-the-finish-line-first-video-id481224542_14.jpg",
                "caption": "",
                "position": 1772
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Fine-grained_Action_yt-6sboi4evh4E_350.jpg",
                "caption": "",
                "position": 1777
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Object_Interaction_3B81O.jpg",
                "caption": "",
                "position": 1782
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Object_Interaction_5W1Z7.jpg",
                "caption": "",
                "position": 1787
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Object_Interaction_7ZCXJ.jpg",
                "caption": "",
                "position": 1792
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Object_Interaction_FKJ9L.jpg",
                "caption": "",
                "position": 1797
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Object_Interaction_XFRYR.jpg",
                "caption": "",
                "position": 1802
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Character_Order_video_2125.jpg",
                "caption": "Figure 12:Spatial bias in MVBench (3).A single frame containing all characters inquired is sufficient to answer MVBench.",
                "position": 1808
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Character_Order_video_2125.jpg",
                "caption": "",
                "position": 1811
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Character_Order_video_3707.jpg",
                "caption": "",
                "position": 1816
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Character_Order_video_5492.jpg",
                "caption": "",
                "position": 1821
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Character_Order_video_5828.jpg",
                "caption": "",
                "position": 1826
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Character_Order_video_6696.jpg",
                "caption": "",
                "position": 1831
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Character_Order_video_6814.jpg",
                "caption": "",
                "position": 1836
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Moving_Count_video_12512.jpg",
                "caption": "Figure 13:Spatial bias in MVBench (4).Temporal understanding is not needed as a single frame suffices to solve these questions on MVBench.",
                "position": 1842
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Moving_Count_video_12512.jpg",
                "caption": "",
                "position": 1845
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Moving_Count_video_14343.jpg",
                "caption": "",
                "position": 1850
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Moving_Count_video_14956.jpg",
                "caption": "",
                "position": 1855
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Moving_Count_video_14965.jpg",
                "caption": "",
                "position": 1860
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Object_Existence_video_10635.jpg",
                "caption": "",
                "position": 1865
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Object_Existence_video_11301.jpg",
                "caption": "",
                "position": 1870
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Object_Existence_video_12262.jpg",
                "caption": "Figure 14:Spatial bias in MVBench (5).Temporal understanding is not needed as a single frame suffices to solve these questions on MVBench.",
                "position": 1876
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Object_Existence_video_12262.jpg",
                "caption": "",
                "position": 1879
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Object_Existence_video_12359.jpg",
                "caption": "",
                "position": 1884
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Object_Existence_video_13866.jpg",
                "caption": "",
                "position": 1889
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Object_Existence_video_14587.jpg",
                "caption": "",
                "position": 1894
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Object_Existence_video_14727.jpg",
                "caption": "",
                "position": 1899
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Moving_Attribute_video_10439.jpg",
                "caption": "Figure 15:Spatial bias in MVBench (6).Temporal understanding is not needed as a single frame suffices to solve these questions on MVBench.",
                "position": 1905
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Moving_Attribute_video_10439.jpg",
                "caption": "",
                "position": 1908
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Moving_Count_video_10030.jpg",
                "caption": "",
                "position": 1913
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Moving_Count_video_12152.jpg",
                "caption": "",
                "position": 1918
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Moving_Count_video_12406.jpg",
                "caption": "",
                "position": 1923
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Scene_Transition_Top009_02005.jpg",
                "caption": "Figure 16:Spatial bias in MVBench (6).A single frame is sufficient to discard incorrect candidates, as the scenes described in these candidates never occurred in the video.",
                "position": 1929
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Scene_Transition_Top009_02005.jpg",
                "caption": "",
                "position": 1932
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Scene_Transition_Top046_00175.jpg",
                "caption": "",
                "position": 1937
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Scene_Transition_Top077_02770.jpg",
                "caption": "",
                "position": 1942
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Scene_Transition_Top079_00580.jpg",
                "caption": "",
                "position": 1947
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Scene_Transition_Top136_04890.jpg",
                "caption": "",
                "position": 1952
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Action_Antonym_14136.jpg",
                "caption": "Figure 17:Textual bias in MVBench (1).Answer candidates are generated with an LLM. The “Not sure” candidate is never correct, while the other incorrect candidate makes no textual sense e.g. “catching something against something”.",
                "position": 1958
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Action_Antonym_14136.jpg",
                "caption": "",
                "position": 1961
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Action_Antonym_40597.jpg",
                "caption": "",
                "position": 1966
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Action_Antonym_43350.jpg",
                "caption": "",
                "position": 1971
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Action_Antonym_52020.jpg",
                "caption": "",
                "position": 1976
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Action_Antonym_67650.jpg",
                "caption": "",
                "position": 1981
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Action_Antonym_107628.jpg",
                "caption": "",
                "position": 1986
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Action_Antonym_108982.jpg",
                "caption": "",
                "position": 1991
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Episodic_Reasoning_friends_s02e18_seg02_clip_04.jpg",
                "caption": "Figure 18:Textual bias in MVBench (2): Overreliance on world knowledge.Answers can be inferred using world knowledge of TV shows. Questions cannot be answered from the video only as answers contain e.g. character names of the TV show.",
                "position": 1997
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Episodic_Reasoning_friends_s02e18_seg02_clip_04.jpg",
                "caption": "",
                "position": 2000
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Episodic_Reasoning_friends_s03e25_seg02_clip_19.jpg",
                "caption": "",
                "position": 2005
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Episodic_Reasoning_house_s04e16_seg02_clip_15.jpg",
                "caption": "",
                "position": 2010
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Episodic_Reasoning_met_s03e05_seg02_clip_14.jpg",
                "caption": "",
                "position": 2015
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Episodic_Reasoning_met_s06e24_seg02_clip_07.jpg",
                "caption": "",
                "position": 2020
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Episodic_Reasoning_s02e23_seg02_clip_11.jpg",
                "caption": "",
                "position": 2025
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Scene_Transition_Top046_04815.jpg",
                "caption": "Figure 19:Textual bias in MVBench (3).Some candidates in MVBench contain artifacts due to the automatic LLM-based generation process. In these cases, the correct answers end with “1” or contain the keyword “true”.",
                "position": 2031
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Scene_Transition_Top046_04815.jpg",
                "caption": "",
                "position": 2034
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Scene_Transition_Top051_05975.jpg",
                "caption": "",
                "position": 2039
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Scene_Transition_Top055_05350.jpg",
                "caption": "",
                "position": 2044
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Scene_Transition_Top064_06575.jpg",
                "caption": "",
                "position": 2049
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Scene_Transition_Top113_00550.jpg",
                "caption": "",
                "position": 2054
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Object_Interaction_6JGXL.jpg",
                "caption": "Figure 20:Textual bias in MVBench (4).Since two of the candidate objects cannot be eaten, the choice is reduced to two remaining candidates.",
                "position": 2060
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Counterfactual_Inference_video_10462.jpg",
                "caption": "Figure 21:Textual bias in MVBench (5).Just one candidate answer is provided for some QA pairs on MVBench.",
                "position": 2063
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Counterfactual_Inference_video_10462.jpg",
                "caption": "",
                "position": 2066
            },
            {
                "img": "https://arxiv.org/html/2410.07752/extracted/5916043/figs/figs_apendix/Counterfactual_Inference_video_10978.jpg",
                "caption": "",
                "position": 2071
            }
        ]
    },
    {
        "header": "Appendix BSpatial/Textual bias in MVBench",
        "images": []
    }
]