[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Method",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.21800/x1.png",
                "caption": "Figure 1:The training and validation loss of XL-size models (1.5B) trained with different optimizers on the FineWeb-Edu 100B dataset.",
                "position": 499
            },
            {
                "img": "https://arxiv.org/html/2510.21800/x2.png",
                "caption": "",
                "position": 502
            },
            {
                "img": "https://arxiv.org/html/2510.21800/x3.png",
                "caption": "Figure 2:The training and validation loss of XL-size models (1.5B) trained with different optimizers on the FineWeb-Edu 100B dataset.",
                "position": 506
            },
            {
                "img": "https://arxiv.org/html/2510.21800/x4.png",
                "caption": "",
                "position": 509
            },
            {
                "img": "https://arxiv.org/html/2510.21800/x5.png",
                "caption": "Figure 3:The training and validation loss of large-size models (770M) trained with different optimizers on the OpenWebText dataset.",
                "position": 513
            },
            {
                "img": "https://arxiv.org/html/2510.21800/x6.png",
                "caption": "",
                "position": 516
            },
            {
                "img": "https://arxiv.org/html/2510.21800/x7.png",
                "caption": "Figure 4:The test loss and test accuracy for different optimizers on CIFAR-10 dataset.",
                "position": 585
            },
            {
                "img": "https://arxiv.org/html/2510.21800/x8.png",
                "caption": "",
                "position": 588
            },
            {
                "img": "https://arxiv.org/html/2510.21800/x9.png",
                "caption": "Figure 5:The training and validation loss of small-size models (125M) trained with MARS-M with differentÎ³\\gammas on the OpenWebText 100B dataset.",
                "position": 596
            },
            {
                "img": "https://arxiv.org/html/2510.21800/x10.png",
                "caption": "",
                "position": 599
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AHyper-parameter Settings",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.21800/x11.png",
                "caption": "Figure 6:The training and validation loss of small-size models (125M) trained with different optimizers on the FineWeb-Edu 100B dataset.",
                "position": 1852
            },
            {
                "img": "https://arxiv.org/html/2510.21800/x12.png",
                "caption": "",
                "position": 1855
            },
            {
                "img": "https://arxiv.org/html/2510.21800/x13.png",
                "caption": "Figure 7:The training and validation loss of medium-size models (355M) trained with different optimizers on the FineWeb-Edu 100B dataset.",
                "position": 1859
            },
            {
                "img": "https://arxiv.org/html/2510.21800/x14.png",
                "caption": "",
                "position": 1862
            },
            {
                "img": "https://arxiv.org/html/2510.21800/x15.png",
                "caption": "Figure 8:The training and validation loss of large-size models (770M) trained with different optimizers on the FineWeb-Edu 100B dataset.",
                "position": 1866
            },
            {
                "img": "https://arxiv.org/html/2510.21800/x16.png",
                "caption": "",
                "position": 1869
            },
            {
                "img": "https://arxiv.org/html/2510.21800/x17.png",
                "caption": "Figure 9:The training and validation loss of small-size models (125M) trained with different optimizers on the OpenWebText dataset.",
                "position": 1873
            },
            {
                "img": "https://arxiv.org/html/2510.21800/x18.png",
                "caption": "",
                "position": 1876
            },
            {
                "img": "https://arxiv.org/html/2510.21800/x19.png",
                "caption": "Figure 10:The training and validation loss of medium-size models (355M) trained with different optimizers on the OpenWebText dataset.",
                "position": 1880
            },
            {
                "img": "https://arxiv.org/html/2510.21800/x20.png",
                "caption": "",
                "position": 1883
            },
            {
                "img": "https://arxiv.org/html/2510.21800/x21.png",
                "caption": "Figure 11:The training and validation loss of large-size models (770M) trained with different optimizers on the OpenWebText dataset.",
                "position": 1887
            },
            {
                "img": "https://arxiv.org/html/2510.21800/x22.png",
                "caption": "",
                "position": 1890
            }
        ]
    },
    {
        "header": "Appendix CProof of Theorem4.3",
        "images": []
    }
]