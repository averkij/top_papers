[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.16813/x1.png",
                "caption": "Figure 1:Our flow map language model (FMLM) outperforms discrete diffusion models (gray) and matches the 8-step generation performance of few-step distilled discrete diffusion models (blue) in onlyone step, achieving an≈8.3×\\approx 8.3\\timesspeedup on LM1B.",
                "position": 203
            },
            {
                "img": "https://arxiv.org/html/2602.16813/x2.png",
                "caption": "Figure 2:Overview.Left:We leverage continuous interpolation between Gaussian noise and one-hot language encoding.Middle:Our flow-based language model (FLM) learns a denoiser that predicts clean data, which we convert into a flow for sampling.Right:Our distilled flow map language model (FMLM) directly transports states between distant timepoints, enabling few-step generation.",
                "position": 213
            }
        ]
    },
    {
        "header": "2Background & Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.16813/x3.png",
                "caption": "Figure 3:Factorization error in discrete diffusion.A toy dataset with two correlated modesnew-yorkandsan-diego.Left:In many-step sampling, both continuous flow and discrete diffusion generate valid data.Right:In few-step sampling, the factorized transition of discrete diffusion yields a spurious mixture of all possible combinations (including invalid pairingsnew-diegoandsan-york).",
                "position": 266
            }
        ]
    },
    {
        "header": "3Theoretical Framework",
        "images": []
    },
    {
        "header": "4Algorithmic Aspects",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.16813/x4.png",
                "caption": "Figure 4:Decoding error rate over time across vocabulary sizes. Our time reparameterizationτ​(t)\\tau(t)redistributes time so each step contributes uniformly to the denoising signal.",
                "position": 656
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.16813/x5.png",
                "caption": "Figure 5:Generation performance ofFLMon LM1B (top) and OWT (bottom) compared to diffusion baselines.",
                "position": 796
            },
            {
                "img": "https://arxiv.org/html/2602.16813/x6.png",
                "caption": "Figure 6:Few-step generation performance ofFMLMon LM1B (left) and OWT (right) compared to distilled discrete diffusion. Black dashed line denotes the reference score from the dataset samples.",
                "position": 922
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExtended related work",
        "images": []
    },
    {
        "header": "Appendix BBackground on flow maps",
        "images": []
    },
    {
        "header": "Appendix CDenoiser flow maps",
        "images": []
    },
    {
        "header": "Appendix DAuxiliary results",
        "images": []
    },
    {
        "header": "Appendix EImplementation details",
        "images": []
    },
    {
        "header": "Appendix FSupplementary evaluation results",
        "images": []
    },
    {
        "header": "Appendix GSupplementary qualitative results",
        "images": []
    }
]