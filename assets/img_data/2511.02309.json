[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02309/lossfunk-logo.jpg",
                "caption": "",
                "position": 116
            },
            {
                "img": "https://arxiv.org/html/2511.02309/x1.png",
                "caption": "Figure 1:Key Results: Qwen3-235B Chain Length Analysis.Sequential reasoning (green) consistently outperforms parallel approaches (red) across AIME-2025 and GPQA-Diamond benchmarks with all chain configurations. Performance numbers are clearly visible, demonstrating advantages up to 46.7% on AIME-2025 and consistent 11-14.6% improvements on GPQA-Diamond, proving the power of iterative refinement.",
                "position": 130
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background and Motivation",
        "images": []
    },
    {
        "header": "3The Sequential Reasoning Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02309/x2.png",
                "caption": "Figure 2:Sequential Reasoning Framework Overview.Iterative refinement process where each attempt builds upon previous reasoning, enabling self-correction and verification through progressive steps. The framework demonstrates how sequential chains leverage context accumulation and error correction, culminating in inverse-entropy weighted voting for optimal answer aggregation based on model confidence.",
                "position": 179
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Results",
        "images": []
    },
    {
        "header": "6Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02309/x3.png",
                "caption": "Figure 3:Creative Task Performance: Sequential vs Parallel Reasoning (using GPT-OSS-120B).Parallel approaches demonstrate superior semantic diversity while sequential approaches show greater lexical diversity across all chain configurations.",
                "position": 979
            },
            {
                "img": "https://arxiv.org/html/2511.02309/x4.png",
                "caption": "Figure 4:Token Budget Scaling: Sequential vs Parallel Reasoning Laws.Sequential self-refinement (green) consistently outperforms parallel self-consistency (red) across all computational budgets from 2K to 16K tokens, with advantages ranging from 6.6 to 8.9 percentage points. These align with recent wider-vs-deeper inference scaling laws(Inoue etÂ al.,,2025)and suggests sequential methods are more compute-efficient at lower budgets, potentially enabling deployment on edge devices.",
                "position": 1008
            }
        ]
    },
    {
        "header": "7Future Work",
        "images": []
    },
    {
        "header": "8Limitations",
        "images": []
    },
    {
        "header": "9Conclusion",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetailed Voting Methods Analysis",
        "images": []
    },
    {
        "header": "Appendix BComplete Reproducibility Details",
        "images": []
    },
    {
        "header": "Appendix COptimal Chain Length Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02309/x5.png",
                "caption": "Figure 5:Kimi-K2 Instruct Chain Length Analysis Across All Benchmarks.Sequential reasoning (green) consistently outperforms parallel approaches (red) across AIME-2024, AIME-2025, and GPQA-Diamond with all chain configurations. Notable advantages include 13.3% on AIME-2024 (6 chains), 20.0% on AIME-2025 (6 chains), and consistent 1-3% improvements on GPQA-Diamond, demonstrating robustness across instruction-tuned architectures and diverse reasoning domains.",
                "position": 1870
            }
        ]
    },
    {
        "header": "Appendix DTop-K Tokens Robustness Analysis",
        "images": []
    },
    {
        "header": "Appendix EImplementation Details and Robustness Analysis",
        "images": []
    },
    {
        "header": "Appendix FEntropy Aggregation Variants Analysis",
        "images": []
    },
    {
        "header": "Appendix GTest-Time Scaling Comparison with S1 Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02309/x6.png",
                "caption": "Figure 6:Comparison of Our Sequential Refinement with S1 Framework.Our continuous self-refinement (green) demonstrates superior scaling properties compared to the S1 framework (blue) across increasing token budgets on GPQA-Diamond, achieving higher accuracy without requiring specialized fine-tuning. This highlights the natural emergence of sequential advantages in post-trained models.",
                "position": 2218
            }
        ]
    },
    {
        "header": "Appendix HCreative Task Evaluation Metrics",
        "images": []
    }
]