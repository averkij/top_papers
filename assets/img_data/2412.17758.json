[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.17758/x1.png",
                "caption": "Figure 1:Difference betweenARC ChallengeandARC Easyaccuracies when considering each answer separately compared to seeing all options. The gap is vastly reduced, up to six times in this comparison.",
                "position": 65
            },
            {
                "img": "https://arxiv.org/html/2412.17758/x2.png",
                "caption": "Figure 2:Model considers particular choices inAyseparationwithout knowing the alternative (prompt includes only the question). Because options may vary in length, it is a good practice to normalize themGao (2021).",
                "position": 75
            },
            {
                "img": "https://arxiv.org/html/2412.17758/x3.png",
                "caption": "Figure 3:Model sees the context of all possibleAyoptionsin the prompt. Because all of the options are single letters (likely single tokens), scores require no normalization.",
                "position": 79
            }
        ]
    },
    {
        "header": "2Impact on evaluation results",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.17758/x4.png",
                "caption": "Figure 4:ARC Challengeevaluation results depending on whether the model sees other options or considers each answer separately. Differences reach up to 35%, and assumed setup impacts model rankings.",
                "position": 103
            }
        ]
    },
    {
        "header": "3Are other benchmarks affected?",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.17758/x5.png",
                "caption": "Figure 5:OpenBookQAevaluation results depending on whether the model sees other options or considers each answer separately. In a setup with options, current models outperform human test takers.",
                "position": 131
            },
            {
                "img": "https://arxiv.org/html/2412.17758/x6.png",
                "caption": "Figure 6:SIQAevaluation results depending on whether the model sees other options or considers each answer separately. Reformulation leads to up to 24% improvement.",
                "position": 134
            }
        ]
    },
    {
        "header": "4Why does it matter?",
        "images": []
    },
    {
        "header": "5Suggestions for multi-choice eval",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "6Summary",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AClaiming setup used by other authors",
        "images": []
    },
    {
        "header": "Appendix BEstimating number of questions hardly answerable in separation",
        "images": []
    },
    {
        "header": "Appendix CEvaluation details",
        "images": []
    }
]