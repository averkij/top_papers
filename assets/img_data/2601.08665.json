[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08665/x1.png",
                "caption": "Figure 1:Overview of VLingNav.VLingNav is a VLA model enhanced with adaptive CoT reasoning and visual-assisted linguistic memory. This architecture allows the model to leverage historical visual and linguistic memory, achieving SOTA results on several embodied navigation benchmarks. Furthermore, VLingNav can be deployed zero-shot on real-world robots to perform diverse and complex navigation tasks.",
                "position": 205
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08665/x2.png",
                "caption": "Figure 2:The overall framework of VLingNav.The framework takes video streams and multimodal instruction as input to produce robot action for navigation with tailored linguistic designs. AdaCoT can adaptively generate linguistic thinking according to its observation, while VLingMem summarizes CoT cues with key visual features for globally informed decision-making.",
                "position": 312
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Data Collection",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08665/x3.png",
                "caption": "Figure 3:Data distribution and instruction word cloud for the VLingNav training dataset.",
                "position": 870
            },
            {
                "img": "https://arxiv.org/html/2601.08665/x4.png",
                "caption": "Figure 4:The autonomous adaptive CoT labeling pipeline of VLingNav.",
                "position": 918
            }
        ]
    },
    {
        "header": "5Training Recipe",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08665/x5.png",
                "caption": "Figure 5:Online post-training with a hybrid rollout procedure.",
                "position": 955
            }
        ]
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08665/x6.png",
                "caption": "Figure 6:Performance visualization of VLingNav across various navigation benchmarks.",
                "position": 1693
            },
            {
                "img": "https://arxiv.org/html/2601.08665/x7.png",
                "caption": "Figure 7:Real-world robot platform setup.",
                "position": 1794
            },
            {
                "img": "https://arxiv.org/html/2601.08665/x7.png",
                "caption": "Figure 7:Real-world robot platform setup.",
                "position": 1797
            },
            {
                "img": "https://arxiv.org/html/2601.08665/x8.png",
                "caption": "Figure 8:Real-world experiment results.",
                "position": 1802
            },
            {
                "img": "https://arxiv.org/html/2601.08665/x9.png",
                "caption": "Figure 9:Qualitative performance of VLingNav in real-world deployments.",
                "position": 1840
            },
            {
                "img": "https://arxiv.org/html/2601.08665/x10.png",
                "caption": "Figure 10:Ablation study on training steps.",
                "position": 2153
            },
            {
                "img": "https://arxiv.org/html/2601.08665/x10.png",
                "caption": "Figure 10:Ablation study on training steps.",
                "position": 2156
            },
            {
                "img": "https://arxiv.org/html/2601.08665/x11.png",
                "caption": "Figure 11:Ablation study on online post-training iteration steps.",
                "position": 2161
            },
            {
                "img": "https://arxiv.org/html/2601.08665/x12.png",
                "caption": "Figure 12:Ablation study on multi-task learning. We present the multi-task synergy of VLingNav and illustrate the performance comparison between models trained on a single task and those trained on multiple tasks.",
                "position": 2167
            }
        ]
    },
    {
        "header": "7Discussion",
        "images": []
    },
    {
        "header": "8Conclusion and Limitation",
        "images": []
    },
    {
        "header": "9Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]