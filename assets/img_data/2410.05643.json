[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05643/x1.png",
                "caption": "(a)Video Structure",
                "position": 204
            },
            {
                "img": "https://arxiv.org/html/2410.05643/x1.png",
                "caption": "(a)Video Structure",
                "position": 207
            },
            {
                "img": "https://arxiv.org/html/2410.05643/x2.png",
                "caption": "(b)Performance Gap between Models",
                "position": 212
            },
            {
                "img": "https://arxiv.org/html/2410.05643/x3.png",
                "caption": "Figure 2:Overview of the training process of TRACE model.We employ a variety of encoders and heads to handletime,score, andtextinputs and outputs. The timestamps of the sampled frames are converted into time tokens and subsequently integrated into the visual tokens of each frame. In the answer section, time tokens, score tokens, and text tokens are inserted in a sequential manner. The generation process of TRACE is summarized in Figure4.",
                "position": 219
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3TRACE",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05643/x4.png",
                "caption": "Figure 3:Illustration of token sequence of TRACE.Following Eq2, the sequence begins with visual frame tokens (ùêÖùêÖ\\mathbf{F}bold_F) followed by instruction tokens (ùêàùêà\\mathbf{I}bold_I). Event (eùëíeitalic_e) tokens are structured in the sequence of time tokens (tùë°titalic_t), score tokenssùë†sitalic_s, and text tokenscùëêcitalic_c, with events ordered chronologically based on their occurrence time.",
                "position": 361
            },
            {
                "img": "https://arxiv.org/html/2410.05643/x5.png",
                "caption": "Figure 4:Generation process of TRACE.The TRACE generate tokens following the order of time tokens, score tokens, and text tokens. The decoding heads are switched when‚ü®s‚Å¢y‚Å¢n‚Å¢c‚ü©delimited-‚ü®‚ü©ùë†ùë¶ùëõùëê\\langle sync\\rangle‚ü® italic_s italic_y italic_n italic_c ‚ü©tokens are generated.",
                "position": 453
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05643/x6.png",
                "caption": "(a)Performance on Youcook2",
                "position": 833
            },
            {
                "img": "https://arxiv.org/html/2410.05643/x6.png",
                "caption": "(a)Performance on Youcook2",
                "position": 836
            },
            {
                "img": "https://arxiv.org/html/2410.05643/x7.png",
                "caption": "(b)Performance on Charades-STA",
                "position": 841
            },
            {
                "img": "https://arxiv.org/html/2410.05643/x8.png",
                "caption": "(c)Performance on QVHighlights",
                "position": 847
            },
            {
                "img": "https://arxiv.org/html/2410.05643/x9.png",
                "caption": "(d)Performance on ActivityNet Captions",
                "position": 852
            }
        ]
    },
    {
        "header": "5Conclusion and Future Works",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADataset Preparation",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05643/x10.png",
                "caption": "Figure 6:Annotation example of video caption task.",
                "position": 2231
            },
            {
                "img": "https://arxiv.org/html/2410.05643/x11.png",
                "caption": "Figure 7:Annotation example of dense video caption task.",
                "position": 2234
            },
            {
                "img": "https://arxiv.org/html/2410.05643/x12.png",
                "caption": "Figure 8:Annotation example of moment retrieval task.",
                "position": 2237
            },
            {
                "img": "https://arxiv.org/html/2410.05643/x13.png",
                "caption": "Figure 9:Annotation example of video highlight detection task.",
                "position": 2240
            },
            {
                "img": "https://arxiv.org/html/2410.05643/x14.png",
                "caption": "Figure 10:Annotation example of video summarization task.",
                "position": 2243
            },
            {
                "img": "https://arxiv.org/html/2410.05643/x15.png",
                "caption": "Figure 11:Case study of TRACE.",
                "position": 2521
            }
        ]
    },
    {
        "header": "Appendix BExperiments",
        "images": []
    }
]