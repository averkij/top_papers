[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminary",
        "images": []
    },
    {
        "header": "3Lizard: Subquadratic Attention for Infinite Context",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.09025/x1.png",
                "caption": "Figure 1:Overview of the Lizardtraining pipeline. The model is trained in two stages.\nStage 1: The pretrained model is frozen, and the added modules are trained to approximate both the positional encoding patterns and the softmax attention outputs.\nStage 2: The softmax attention is replaced with the trained hybrid modules, then fine-tuned on the next-word prediction objective.",
                "position": 211
            },
            {
                "img": "https://arxiv.org/html/2507.09025/x2.png",
                "caption": "Figure 2:Visualize attention map of Lizard, as a combination of Gated Linear Attention and Sliding Window Attention with Meta Memory (withw=3ùë§3w=3italic_w = 3andm=2ùëö2m=2italic_m = 2)",
                "position": 317
            }
        ]
    },
    {
        "header": "4Experimental Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.09025/x3.png",
                "caption": "Figure 3:Example from the synthetic passkey retrieval dataset.",
                "position": 755
            },
            {
                "img": "https://arxiv.org/html/2507.09025/x4.png",
                "caption": "Figure 4:Needle-in-a-Haystack evaluation.Each cell shows retrieval accuracy by sequence length (X-axis) and target distance (Y-axis). Green indicates success; red indicates failure. The white dashed line marks the max training length.",
                "position": 758
            },
            {
                "img": "https://arxiv.org/html/2507.09025/x5.png",
                "caption": "Figure 5:Throughput and memory comparison.",
                "position": 769
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitations",
        "images": []
    },
    {
        "header": "Appendix BInference Efficiency",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.09025/extracted/6616322/Figures/inference_time.png",
                "caption": "Figure 6:Inference speed comparison between GLA and Lizard kernel..",
                "position": 1452
            }
        ]
    },
    {
        "header": "Appendix CAblation Study",
        "images": []
    },
    {
        "header": "Appendix DExperimental Details",
        "images": []
    },
    {
        "header": "Appendix EEvaluation on small-size LLMs",
        "images": []
    },
    {
        "header": "Appendix FSample Generations",
        "images": []
    }
]