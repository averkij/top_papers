[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09462/x1.png",
                "caption": "Figure 1:AI Control and Protocols Subversion.Left:The AI control setup is an auditing game between a red team and a blue team.(1)The red team sequentially submits solutions to agentic tasks, some of which are malicious (e.g., backdoored).(2)These solutions are audited by the blue team with the assistance of a trusted AI monitor.(3)Finally, the blue team appliescontrol protocolsto decide which solutions should be escalated for costly human auditing. We show that modern AI control protocols can be effectively subverted if the red team’s attack policy includes insertingprompt injectionsinto its actions to sabotage the blue team.Bottom right:Our attack policy significantly increases the proportion of malicious actions by the red team that are classified as benign by three different monitor models.Top right:As a result, the safety-usefulness Pareto frontiers provided by modern AI control protocolscollapseto upfront auditing as monitors become ineffective.",
                "position": 134
            },
            {
                "img": "https://arxiv.org/html/2510.09462/x2.png",
                "caption": "",
                "position": 143
            },
            {
                "img": "https://arxiv.org/html/2510.09462/x3.png",
                "caption": "Figure 2:Prompt Injections Subvert Monitors.Left:Suspiciousness scores assigned by the monitor (GPT-4.1) to APPS solutions from the untrusted model (GPT-5), comparing honest (no-attack) and backdoored submissions with and without prompt injection.\nGPT-4.1 is particularly susceptible to injections, assigning a score of0toall375375backdoored code samples.Center & Right:ROC curves for monitors on APPS and BashBench, w/ and w/o prompt injections.\nPerformance universally drops under attack, with monitors rating prompt-injected code as less suspicious than honest code.\nCurves below the diagonal indicate that, under the injection, the monitor classifies backdoored examples as more safe than honest, non-backdoored examples.",
                "position": 256
            },
            {
                "img": "https://arxiv.org/html/2510.09462/x4.png",
                "caption": "Figure 3:Prompt Injections Subvert Control on APPS.Left:ROC curves for the audit scores used by each protocol; under prompt injection the ROC curve often goes below the diagonal, indicating that monitors rate backdoored attempts as less suspicious than honest ones.Center & Right:Prompt injections collapse the safety–usefulness Pareto frontiers across all protocols (center) and across all monitoring models (right). We highlight the Always-Defer instantiations that do not rely on the monitor. Safety estimates are reported with bootstrapped95%95\\%confidence intervals.",
                "position": 290
            },
            {
                "img": "https://arxiv.org/html/2510.09462/x5.png",
                "caption": "Figure 4:Injections Subvert Control on BashBench.Safety is shown with bootstrapped95%95\\%confidence intervals; usefulness is shown with95%95\\%Wilson intervals. Safety of UA is computed analytically.",
                "position": 293
            },
            {
                "img": "https://arxiv.org/html/2510.09462/x6.png",
                "caption": "Figure 5:A Zero-Shot Prompt Injection Attack Policy.Left:ROC of the monitor (GPT-4o-mini) when classifying backdoors by untrusted model (GPT-5), w/ and w/o prompt injectionsgenerated by GPT-5.Right:Safety of Trusted Monitoring; error bars show bootstrapped95%95\\%confidence intervals.",
                "position": 302
            },
            {
                "img": "https://arxiv.org/html/2510.09462/x7.png",
                "caption": "Figure 7:Comparison of min (left) and max (right) DTR, deferral threshold set to4.54.5out of100100(this threshold corresponds to a point with the usefulness of approx.0.850.85inFigure˜3). The best-of-nneffect of the DTR protocol under the prompt injection, which makes it worse than DTT/TE in this setting, disappears when the max-heuristic is used. Without prompt injections, max-heuristic makes the extra samples useful, and the overall discrimination ability of the monitor better.",
                "position": 1424
            },
            {
                "img": "https://arxiv.org/html/2510.09462/x8.png",
                "caption": "Figure 8:Suspiciousness score statistics when running DTT with two deferral thresholds,d=11d=11andd=41d=41. At each steptt, the blue team can select an FPRftf_{t}of audited actions, subject to the constraint of the total auditing budget∑tft=c\\sum_{t}f_{t}=c. Corresponding to the FPRftf_{t}is the suspiciousness scorests_{t}(top right). Conditional on the red team attempting a backdoor, the red(top left)and blue(top center)team win probabilities depend on this scorests_{t}. In safety computations, at a given FPR, higher success rates on the side task ford=41d=41(bottom left)benefit the red team, but this is counteracted by a higher detection rate by the blue team(bottom center).",
                "position": 1449
            },
            {
                "img": "https://arxiv.org/html/2510.09462/x9.png",
                "caption": "Figure 9:Safety-usefulness trade-offs for two definitions of safety.With the definition of safety from the paper, the blue team has until the end of the task to catch the red team. In that case, safety degrades less at high deferral thresholds (blue Pareto frontier). With an alternative definition, the red team wins immediately after the action with the backdoor is deployed. Then, safety degrades significantly with fewer deferrals (orange Pareto frontier).",
                "position": 1469
            },
            {
                "img": "https://arxiv.org/html/2510.09462/x10.png",
                "caption": "Figure 10:Sensitivity of the Pareto frontiers on BashBench to auditing budget.From top to bottom:c=0.01c=0.01,0.020.02, and0.040.04.",
                "position": 1475
            },
            {
                "img": "https://arxiv.org/html/2510.09462/x11.png",
                "caption": "",
                "position": 1479
            },
            {
                "img": "https://arxiv.org/html/2510.09462/x12.png",
                "caption": "",
                "position": 1481
            },
            {
                "img": "https://arxiv.org/html/2510.09462/x13.png",
                "caption": "Figure 11:Sensitivity of the Pareto frontiers on APPS to auditing budget.From top to bottom:c=0.01c=0.01,0.020.02, and0.040.04.",
                "position": 1485
            },
            {
                "img": "https://arxiv.org/html/2510.09462/x14.png",
                "caption": "",
                "position": 1489
            },
            {
                "img": "https://arxiv.org/html/2510.09462/x15.png",
                "caption": "",
                "position": 1491
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]