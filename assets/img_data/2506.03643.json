[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03643/x1.png",
                "caption": "Figure 1:Dynamic Visual Representations.As the number of tokens used by DOVE increases, the reconstructed images shows finer and high frequency details.",
                "position": 98
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Dynamic Vision Tokenizer",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03643/x2.png",
                "caption": "Figure 2:Dynamic Tokenizer.",
                "position": 150
            },
            {
                "img": "https://arxiv.org/html/2506.03643/x3.png",
                "caption": "Figure 3:Query Conditioning.DOVE is trained with a bounding-box based loss, learning to focus its dynamic token resources on representing query-relevant image regions.",
                "position": 386
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03643/x4.png",
                "caption": "Figure 4:Reconstructed images on ImageNet-1K using different methods. As the token length increases, our method produces progressively clearer reconstructions with more visual details.",
                "position": 413
            },
            {
                "img": "https://arxiv.org/html/2506.03643/x5.png",
                "caption": "(a)CIFAR100",
                "position": 557
            },
            {
                "img": "https://arxiv.org/html/2506.03643/x5.png",
                "caption": "(a)CIFAR100",
                "position": 560
            },
            {
                "img": "https://arxiv.org/html/2506.03643/x6.png",
                "caption": "(b)ImageNet100",
                "position": 565
            },
            {
                "img": "https://arxiv.org/html/2506.03643/x7.png",
                "caption": "(c)STL-10",
                "position": 570
            },
            {
                "img": "https://arxiv.org/html/2506.03643/x8.png",
                "caption": "(a)Distribution of token sequence lengths (i.e.,EOS positions) generated by DOVE.",
                "position": 582
            },
            {
                "img": "https://arxiv.org/html/2506.03643/x8.png",
                "caption": "(a)Distribution of token sequence lengths (i.e.,EOS positions) generated by DOVE.",
                "position": 585
            },
            {
                "img": "https://arxiv.org/html/2506.03643/x9.png",
                "caption": "(b)The relation between token length and reconstruction loss across different input samples.",
                "position": 590
            },
            {
                "img": "https://arxiv.org/html/2506.03643/x10.png",
                "caption": "(c)The relation between token sequence lengths (i.e.,EOS positions) and image complexity.",
                "position": 595
            },
            {
                "img": "https://arxiv.org/html/2506.03643/x11.png",
                "caption": "Figure 7:Reconstructed images from the Q-DOVE. When the text query is set to “null”, the model reconstructs the entire image. When a query is provided, the model focuses on query-relevant regions.",
                "position": 609
            },
            {
                "img": "https://arxiv.org/html/2506.03643/x12.png",
                "caption": "Figure 8:Semantics Visualization with PCA on latent features.",
                "position": 946
            }
        ]
    },
    {
        "header": "4Related Works",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03643/x13.png",
                "caption": "Figure 9:Effect of varying GAN loss weight on image reconstruction quality. A small weight (e.g.,5×10−105superscript10105\\times 10^{-10}5 × 10 start_POSTSUPERSCRIPT - 10 end_POSTSUPERSCRIPT) improves perceptual detail without sacrificing fidelity, while larger weights introduce artifacts and increase L1 loss.",
                "position": 1893
            }
        ]
    },
    {
        "header": "Appendix BMultimodal Understanding",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03643/x14.png",
                "caption": "Figure 10:Model predictions under varying token counts. As the number of tokens increases, both image reconstruction quality and answer accuracy improve.",
                "position": 2055
            }
        ]
    },
    {
        "header": "Appendix CLinear Probing Datasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03643/x15.png",
                "caption": "Figure 11:Reconstruction results of DOVE and DOVE (Gaussian) under varying token budgets. Overall, DOVE (Gaussian) achieves similar visual quality to DOVE.",
                "position": 2266
            }
        ]
    },
    {
        "header": "Appendix DExperiments with Gaussian Latent Space",
        "images": []
    }
]