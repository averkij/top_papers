[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.16713/extracted/6644291/figures/exptease.png",
                "caption": "Figure 1:Robot completes a new task guided by previously self-generated memory.We propose grounding VLMs through memory generated by the robot itself from direct hardware experiments, establishing awareness of the robot‚Äôs own capability.\nWhen executing a novel task, the robot keeps a short-term memory that helps the robot reflect and complete the task (illustrated in the grey clip).\nThe experience is then stored as long-term memory and retrieved to guide the VLM‚Äôs task planning whenever a similar scenario is encountered (illustrated in the main figure).",
                "position": 89
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Problem Formulation",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.16713/extracted/6644291/figures/new_pipeline_figure.jpeg",
                "caption": "Figure 2:ExpTeachpipeline overview.At the start of each task, the system takes the user instructionùêàùêà\\mathbf{I}bold_Iand egocentric observationùê®0subscriptùê®0\\mathbf{o}_{0}bold_o start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, which the VLM summarizes into a scenario. RAG retrieves relevant experiences from long-term memoryùêåùêå\\mathbf{M}bold_M(section4.3) and, together with the instruction and observation, feeds them into the VLM task plannerùíØùíØ\\mathcal{T}caligraphic_T(section4.1) to generate the next actionùêöùêö\\mathbf{a}bold_a. After execution, success is checked by the VLM. If the task is not completed, the actionùêöùêö\\mathbf{a}bold_aand its feedbackùê´ùê´\\mathbf{r}bold_rare accumulated into short-term memoryùê¶ùê¶\\mathbf{m}bold_m(section4.2) and fed back into planning. Once the task is completed, the short-term memoryùê¶ùê¶\\mathbf{m}bold_mis summarized and stored in long-term memoryùêåùêå\\mathbf{M}bold_Mfor future use.",
                "position": 330
            },
            {
                "img": "https://arxiv.org/html/2507.16713/x1.png",
                "caption": "Figure 3:Illustration of image annotation tools.",
                "position": 426
            }
        ]
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.16713/extracted/6644291/figures/stm_to_ltm_example.jpg",
                "caption": "Figure 4:Examples ofExpTeachon different scenarios. In the first STM example (top row), after failing to push the candy with its gripper, the robot reflects and uses a sponge as a tool. In the second STM example (middle row), after dropping the apple during bowl collection, the robot reflects with user feedback and learns to move the apple first. These experiences are then summarized and stored in the LTM, allowing the robot to generalize its learning to similar future scenarios (bottom row). For example, when asked to ‚ÄúMove the screw to the toolbox,‚Äù the robot immediately decides to use a towel to push the screw successfully. Likewise, when tasked with ‚ÄúPick up the milk carton,‚Äù it remembers to reposition the apple first before retrieving the target item.",
                "position": 645
            },
            {
                "img": "https://arxiv.org/html/2507.16713/x2.png",
                "caption": "Figure 5:Ablation study of the image annotation module.Left:success rates for picking objects (higher is better).Right:distance errors in pushing one object to another (lower is better). The results demonstrate that incorporating image annotation consistently enhances performance.",
                "position": 752
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix APrompts for the VLM Modules",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.16713/extracted/6644291/figures/stm_bowl/scene_init.png",
                "caption": "Table 5:Illustration of self-reflection for the instruction: ‚ÄúPick up the bowl.‚Äù",
                "position": 2787
            },
            {
                "img": "https://arxiv.org/html/2507.16713/extracted/6644291/figures/stm_bowl/2025-04-24_14-23_1.png",
                "caption": "",
                "position": 2818
            },
            {
                "img": "https://arxiv.org/html/2507.16713/extracted/6644291/figures/stm_bowl/scene_rebuilt.png",
                "caption": "",
                "position": 2829
            },
            {
                "img": "https://arxiv.org/html/2507.16713/extracted/6644291/figures/stm_bowl/2025-04-24_14-24.png",
                "caption": "",
                "position": 2840
            },
            {
                "img": "https://arxiv.org/html/2507.16713/extracted/6644291/figures/stm_bowl/2025-04-24_14-25.png",
                "caption": "",
                "position": 2859
            },
            {
                "img": "https://arxiv.org/html/2507.16713/extracted/6644291/figures/stm_bowl/2025-04-24_14-25_1.png",
                "caption": "",
                "position": 2878
            },
            {
                "img": "https://arxiv.org/html/2507.16713/extracted/6644291/figures/ltm_milk_carton/2025-04-24_15-04.png",
                "caption": "Table 7:Past experiences retrieved via RAG.",
                "position": 2947
            },
            {
                "img": "https://arxiv.org/html/2507.16713/extracted/6644291/figures/ltm_milk_carton/2025-04-24_15-04.png",
                "caption": "Table 8:Illustration of the grounding with retrieved LTM based on the instruction: ‚ÄúPick up the milk carton.‚Äù",
                "position": 3011
            },
            {
                "img": "https://arxiv.org/html/2507.16713/extracted/6644291/figures/ltm_milk_carton/2025-04-24_15-06.png",
                "caption": "",
                "position": 3042
            },
            {
                "img": "https://arxiv.org/html/2507.16713/extracted/6644291/figures/ltm_milk_carton/2025-04-24_15-06_1.png",
                "caption": "",
                "position": 3061
            },
            {
                "img": "https://arxiv.org/html/2507.16713/extracted/6644291/figures/ltm_milk_carton/2025-04-24_15-07.png",
                "caption": "",
                "position": 3080
            },
            {
                "img": "https://arxiv.org/html/2507.16713/extracted/6644291/figures/image_annotation/drumstick.png",
                "caption": "Figure 6:Left: The robot picked up the meat section without VLM guidance through image annotation.Right: With image annotation, the robot successfully picked up the bone section of the drumstick.",
                "position": 3102
            },
            {
                "img": "https://arxiv.org/html/2507.16713/extracted/6644291/figures/image_annotation/skewer.png",
                "caption": "Figure 7:Left: The robot picked up the meat of the skewer without VLM guidance through image annotation.Right: With image annotation, the robot successfully picked up the stick of the skewer.",
                "position": 3105
            },
            {
                "img": "https://arxiv.org/html/2507.16713/extracted/6644291/figures/image_annotation/brush.png",
                "caption": "Figure 8:Left: The robot picked up the body of the brush without VLM guidance through image annotation.Right: With image annotation, the robot successfully picked up the handle of the brush.",
                "position": 3108
            }
        ]
    },
    {
        "header": "Appendix BAdditional Results",
        "images": []
    }
]