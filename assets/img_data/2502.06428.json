[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06428/extracted/6195800/figures/motivation_new.png",
                "caption": "Figure 1:The effects of changing shot-sampling rates on video understanding\ntask performance on videos of different lengths in the\nVideoMME(Fu etÂ al.,2024a)dataset. Two models are evaluated\nincluding LongVA(Zhang etÂ al.,2024a)and\nVideo-XL(Shu etÂ al.,2024). As the number of sampled shots increased, performance did not consistently improve across various video lengths. That is because while sparse sampling may miss crucial details, exhaustive sampling often overwhelms the model with excessive irrelevant content. This illustrates the key challenge of optimal shot selection especially in long video understanding. That is, how to sample variable details in order to maximise semantic task information extraction whilst minimising distractions from irrelevant details (noise) in video understanding.",
                "position": 96
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06428/extracted/6195800/figures/motivation_sce2_v3.png",
                "caption": "Figure 2:The critical problem of how to select shots in video understanding.\nIn a video that depicts how a boy gradually gains a dragonâ€™s trust,\ndifferent sampling methods create two distinct narratives: split\nvideo A shows the boy being attacked by the dragon, while split\nvideo B shows him happily sharing food with the dragon.\nThis shows that minor differences in video sampling leads to significant variations in semantic understanding (interpretation).",
                "position": 119
            },
            {
                "img": "https://arxiv.org/html/2502.06428/extracted/6195800/figures/framework_v5.png",
                "caption": "Figure 3:The overall framework of CoS. It first utilises LLaVA to\nperform a mosaicing binary coding to bootstrap video\nsummarisation for temporal grounding on a long video. Specifically, every\nfour shots are aggregated into a mosaicing composition image. LLaVA\ndetermines whether task-related elements exist within each\ncomposition image by encoding a binary value of 1 or 0 (â€˜yesâ€™\nor â€˜noâ€™),\nthereby identifying sparsely distributed task-related shots to\nachieve pseudo temporal grounding. Given this binary video\nsummary, task-related positive shotsSpsuperscriptğ‘†ğ‘S^{p}italic_S start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPTand irrelevant negative shotsSnsuperscriptğ‘†ğ‘›S^{n}italic_S start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPTare generated and represented by binary codes.Spsuperscriptğ‘†ğ‘S^{p}italic_S start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT,Snsuperscriptğ‘†ğ‘›S^{n}italic_S start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPTand the original frame sequenceXğ‘‹Xitalic_Xsampled from\noriginal videoVğ‘‰Vitalic_Vare then fed into the MLLM for co-reasoning, minimising\ninterference of irrelevant video content.",
                "position": 177
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06428/extracted/6195800/figures/sample_v3.png",
                "caption": "Figure 4:An qualitative evaluation example from MLVU(Zhou etÂ al.,2024)dataset.",
                "position": 1358
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]