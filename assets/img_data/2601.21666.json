[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21666/x1.png",
                "caption": "",
                "position": 230
            },
            {
                "img": "https://arxiv.org/html/2601.21666/x2.png",
                "caption": "",
                "position": 230
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21666/x3.png",
                "caption": "Figure 1:Performance comparison across 13 conversational domains. We compare closed-source and open-source MLLMs across 13 conversational domains using LLM-judge scores (0–10) for video summarization task. Gemini 3.0 Pro consistently outperforms open-source models, and high-stakes domains (e.g., Emergency Response, Mental Health) remain more challenging.",
                "position": 242
            },
            {
                "img": "https://arxiv.org/html/2601.21666/x4.png",
                "caption": "Figure 2:Overview of SONIC-O1 tasks and evaluation format:(Top)video summarization,(Middle)evidence-grounded MCQ,(Bottom)temporal localization (event timing). Each example shows the input clip (frames) and the expected output format; demographic attributes shown beneath each clip represent associated metadata, enabling group-wise evaluation across 13 domains.",
                "position": 245
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3SONIC-O1: Dataset and Benchmark Design",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21666/x5.png",
                "caption": "Figure 3:Video categories. Our benchmark covers 5 key domains and 13 video topics.",
                "position": 1117
            },
            {
                "img": "https://arxiv.org/html/2601.21666/AVQA_Figures/plot_duration_category_by_topic_videos.png",
                "caption": "(a)Video duration distribution over topics",
                "position": 1120
            },
            {
                "img": "https://arxiv.org/html/2601.21666/AVQA_Figures/plot_duration_category_by_topic_videos.png",
                "caption": "(a)Video duration distribution over topics",
                "position": 1123
            },
            {
                "img": "https://arxiv.org/html/2601.21666/AVQA_Figures/plot_category_task_distribution_topics.png",
                "caption": "(b)Question type distribution over topics",
                "position": 1129
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21666/AVQA_Figures/plot1_summarization.png",
                "caption": "(a)Summarization Score",
                "position": 2197
            },
            {
                "img": "https://arxiv.org/html/2601.21666/AVQA_Figures/plot1_summarization.png",
                "caption": "(a)Summarization Score",
                "position": 2200
            },
            {
                "img": "https://arxiv.org/html/2601.21666/AVQA_Figures/plot2_mcq.png",
                "caption": "(b)MCQ Accuracy",
                "position": 2205
            },
            {
                "img": "https://arxiv.org/html/2601.21666/AVQA_Figures/plot3_temporal.png",
                "caption": "(c)Temporal R@0.5",
                "position": 2210
            }
        ]
    },
    {
        "header": "5Discussion and Limitations",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AData Collection Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21666/x6.png",
                "caption": "Figure A1:SONIC-O1 data curation and annotation pipeline.We first retrieve candidate real-world scenario videos via topic- and demographic-augmented search queries and filter by licensing constraints (CC BY 4.0), then download video/audio and any available captions. Human reviewers screen candidates for quality and coverage, after which we parse metadata and perform balanced sampling (e.g., by topic and duration). For benchmark construction, videos are segmented into 10-minute chunks for summarization (T1) and into overlapping 3-minute segments for MCQ and temporal localization (T2–T3). If captions are unavailable, we obtain speech transcripts via ASR; otherwise we use downloaded captions. Draft demographic metadata and task annotations can be bootstrapped with model-assisted tools (e.g., Gemini/ASR) and are subsequently corrected and verified by domain experts, with ambiguous or low-evidence items removed. Arrows indicate the flow from raw retrieval to finalized, human-verified instances.",
                "position": 2858
            }
        ]
    },
    {
        "header": "Appendix BTeam and Annotation / Review Guidelines",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21666/AVQA_Figures/QualityLabeling_Light.png",
                "caption": "Figure A2:Quality filtering interface for initial video selection. Reviewers watch candidate videos and label them as “Good” or “Bad” based on inclusion criteria (audio-visual quality, topic relevance, demographic coverage). The interface displays video metadata (duration, topic category, licensing) and allows batch processing with session management.",
                "position": 4119
            },
            {
                "img": "https://arxiv.org/html/2601.21666/AVQA_Figures/Final_Summary_Review_Light.png",
                "caption": "Figure A3:Review interface for Task 1 (Video Summarization). Annotators edit AI-generated summaries (both detailed and bullet-point formats) while watching the full video with synchronized captions. The demographics panel allows verification and correction of demographic labels in JSON format. Changes auto-save when navigating between items.",
                "position": 4122
            },
            {
                "img": "https://arxiv.org/html/2601.21666/AVQA_Figures/Final_MCQ_Review_Light.png",
                "caption": "Figure A4:Review interface for Task 2 (Multiple-Choice Questions). Annotators review and edit MCQ questions, answer options (A–E), correct answer labels, and rationales that reference visual and auditory evidence. The segment indicator shows the current 3-minute video segment being annotated.",
                "position": 4125
            },
            {
                "img": "https://arxiv.org/html/2601.21666/AVQA_Figures/Final_Temporal_Review_Light.png",
                "caption": "Figure A5:Review interface for Task 3 (Temporal Localization). Annotators edit anchor-target event pairs, temporal relations (before/after/during/immediately), answer start/end timestamps, rationales explaining the temporal reasoning, and audio requirements. The segment timeline helps annotators identify precise event boundaries within 3-minute video segments.",
                "position": 4128
            },
            {
                "img": "https://arxiv.org/html/2601.21666/AVQA_Figures/plot_duration_category_by_topic_videos.png",
                "caption": "(a)Video count by duration category per topic",
                "position": 4131
            },
            {
                "img": "https://arxiv.org/html/2601.21666/AVQA_Figures/plot_duration_category_by_topic_videos.png",
                "caption": "(a)Video count by duration category per topic",
                "position": 4134
            },
            {
                "img": "https://arxiv.org/html/2601.21666/AVQA_Figures/plot_average_duration_by_topic_videos.png",
                "caption": "(b)Average video duration per topic (minutes)",
                "position": 4139
            }
        ]
    },
    {
        "header": "Appendix CExtended Dataset Statistics",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21666/AVQA_Figures/plot_race_distribution_topics.png",
                "caption": "(a)Distribution by race",
                "position": 4164
            },
            {
                "img": "https://arxiv.org/html/2601.21666/AVQA_Figures/plot_race_distribution_topics.png",
                "caption": "(a)Distribution by race",
                "position": 4167
            },
            {
                "img": "https://arxiv.org/html/2601.21666/AVQA_Figures/plot_age_distribution_topics.png",
                "caption": "(b)Distribution by age group",
                "position": 4172
            },
            {
                "img": "https://arxiv.org/html/2601.21666/AVQA_Figures/plot_gender_distribution_topics.png",
                "caption": "(c)Distribution by gender",
                "position": 4177
            }
        ]
    },
    {
        "header": "Appendix DPreprocessing and Inference Configuration",
        "images": []
    },
    {
        "header": "Appendix EEvaluation Metrics",
        "images": []
    },
    {
        "header": "Appendix FDetailed Results",
        "images": []
    },
    {
        "header": "Appendix GDetailed Results Per-Topic",
        "images": []
    },
    {
        "header": "Appendix HError Taxonomy Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21666/x7.png",
                "caption": "Figure A9:Qualitative examples of temporal localization errors. Three common error types are illustrated:Too Early(top)—predictions preceding the ground truth event;Relative-to-Absolute mismatch(middle)—model outputs segment-relative rather than absolute timestamps;Too Late(bottom)—predictions following the ground truth event. Each example shows the question, ground truth timestamps, and predictions from UniMoE-2.0 and Gemini-3.0-Pro.",
                "position": 6787
            }
        ]
    },
    {
        "header": "Appendix IDetailed Results Across Demographic Groups",
        "images": []
    },
    {
        "header": "Appendix JEmotional Language Analysis",
        "images": []
    },
    {
        "header": "Data Release and License",
        "images": []
    }
]