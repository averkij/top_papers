[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22699/all-twemojis.pdf",
                "caption": "",
                "position": 164
            },
            {
                "img": "https://arxiv.org/html/2511.22699/x1.png",
                "caption": "",
                "position": 171
            },
            {
                "img": "https://arxiv.org/html/2511.22699/x2.png",
                "caption": "",
                "position": 176
            },
            {
                "img": "https://arxiv.org/html/2511.22699/x3.png",
                "caption": "",
                "position": 181
            },
            {
                "img": "https://arxiv.org/html/2511.22699/x4.png",
                "caption": "",
                "position": 191
            },
            {
                "img": "https://arxiv.org/html/2511.22699/figures/showcase_realistic.jpg",
                "caption": "Figure 1:Showcases of Z-Image-Turbo in photo-realistic image generation. All related prompts can be found in AppendixA.1.",
                "position": 205
            },
            {
                "img": "https://arxiv.org/html/2511.22699/figures/showcase_text.jpg",
                "caption": "Figure 2:Showcases of Z-Image-Turbo in bilingual text-rendering. All related prompts can be found in AppendixA.2.",
                "position": 210
            },
            {
                "img": "https://arxiv.org/html/2511.22699/figures/showcase_editing.jpg",
                "caption": "Figure 3:Showcases of Z-Image-Edit in various image-to-image tasks.\nEach arrow represents an edit from the input to output images. All related prompts can be found in AppendixA.3.",
                "position": 215
            },
            {
                "img": "https://arxiv.org/html/2511.22699/x5.png",
                "caption": "Figure 4:Showcases of comparison between Z-Image-Turbo and currently state-of-the-art modelsqin2025lumina;qwenimage;cao2025hunyuanimage;nanopro;flux-2-2025;seedream2025seedream;gao2025seedream;google2025imagen4. Z-Image-Turbo shows conspicuous photo-realistic generation capacity.",
                "position": 221
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Data Infrastructure",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22699/x6.png",
                "caption": "Figure 5:Overview of the Active Curation Engine. The pipeline refines uncurated data through cross-modal embedding, deduplication, and rule-based filtering to construct a high-quality augmented dataset. A feedback mechanism leverages the Z-Image model to diagnose long-tail distribution deficiencies, dynamically guiding cross-modal retrieval to reinforce the data collection process. The “Squirrel Fish” (松鼠鳜鱼) case illustrates a classic long-tail challenge: it is actually the name of a Chinese cuisine but the model lacks the specific concept for this dish and may rely on compositional reasoning (combining “Squirrel” (松鼠) and “Fish” (鳜鱼)), leading to erroneous generations absent of domain-specific training data.",
                "position": 444
            },
            {
                "img": "https://arxiv.org/html/2511.22699/x7.png",
                "caption": "Figure 6:Illustration of the Human-in-the-Loop Active Learning Cycle. Data sampled from the media pool undergoes concept and quality balancing before being assigned pseudo-labels . A dual-verifier system (Human and AI) filters these proposals: approved samples pass directly, while rejected cases trigger a manual correction phase . This feedback loop iteratively refines the annotations and updates the topology graph to ensure high-precision alignment.",
                "position": 451
            }
        ]
    },
    {
        "header": "3Image Captioner",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22699/figures/dabenzhong.png",
                "caption": "Figure 8:Pipeline for generating text-to-image captions and image editing instructions. OCR results (obtained through CoT) and world knowledge (from meta information) are explicitly included into the captions.",
                "position": 531
            },
            {
                "img": "https://arxiv.org/html/2511.22699/figures/image11.png",
                "caption": "",
                "position": 532
            },
            {
                "img": "https://arxiv.org/html/2511.22699/figures/image618.png",
                "caption": "",
                "position": 532
            },
            {
                "img": "https://arxiv.org/html/2511.22699/figures/world_knowledge.png",
                "caption": "",
                "position": 538
            },
            {
                "img": "https://arxiv.org/html/2511.22699/figures/ocr_show.png",
                "caption": "",
                "position": 538
            },
            {
                "img": "https://arxiv.org/html/2511.22699/x8.png",
                "caption": "Figure 9:Single image caption and difference caption examples. Left: for single image, we have captions of different types and lengths, and notably, OCR results (all the texts transcribed in their original languages) and world knowledge (explicitly and correctly recognizing the famous beauty spot, West Lake, Hangzhou, China, in this example) is included. Right: difference captions are composed step-by-step.",
                "position": 548
            }
        ]
    },
    {
        "header": "4Model Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22699/x9.png",
                "caption": "Figure 10:Architecture overview of the Z-Image series. The S3-DiT consists of single-stream FFN blocks and single-stream attention blocks. It processes inputs from different modalities through lightweight modality-specific processors, then concatenates them into a unified input sequence. This modality-agnostic architecture maximizes cross-modal parameter reuse to ensure parameter efficiency, while providing flexible compatibility for varying input configurations in both Z-Image and Z-Image-Edit.",
                "position": 614
            },
            {
                "img": "https://arxiv.org/html/2511.22699/x10.png",
                "caption": "Figure 11:The training pipeline of Z-Image and Z-Image-Edit. The low-resolution pre-training and omni-pre-training stages provide a suitable initialization for image generation and editing tasks, after which separate post-training processes yield the Z-Image and Z-Image-Edit models respectively.",
                "position": 673
            },
            {
                "img": "https://arxiv.org/html/2511.22699/figures/train_stage.jpg",
                "caption": "Figure 12:Intermediate generation results throughout Z-Image-Turbo’s training process, echoing our analysis of each stage’s contribution.",
                "position": 740
            },
            {
                "img": "https://arxiv.org/html/2511.22699/x11.png",
                "caption": "Figure 13:Few-Step Distillation visualization results across different distillation strategies: (a) the original SFT model; (b) Standard DMD; (c)Decoupled DMD (D-DMD); and (d)D-DMD+DMDR(Z-Image-Turbo). The proposed approach achieves real-time 8-step inference while attaining superior perceived quality and aesthetic appeal.",
                "position": 779
            },
            {
                "img": "https://arxiv.org/html/2511.22699/x12.png",
                "caption": "Figure 14:Visual comparison between Few-Step Distillation (FSD, top row) and RLHF (bottom row).Building upon the strong foundation of the FSD model, RLHF further enhancesphotorealism,aesthetic quality, andinstruction following.",
                "position": 847
            },
            {
                "img": "https://arxiv.org/html/2511.22699/x13.png",
                "caption": "Figure 15:PE visualization. We compare generation results between PE without reasoning (middle column) and PE with reasoning (right column).\nAs shown in the top row, the reasoning chain enables the model to decipher raw coordinates into a specific scenic context (e.g., West Lake) rather than simply rendering the coordinate text.\nIn the second row, the reasoning module plans detailed steps for \"brewing Pu-erh tea,\" allowing the model to generate specific illustrations for each step instead of a generic list.\nThis demonstrates that the reasoning chain effectively injects world knowledge and provides fine-grained content planning for complex user prompts.",
                "position": 888
            }
        ]
    },
    {
        "header": "5Performance Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22699/figures/image_arena_all.jpg",
                "caption": "Figure 16:Z-Image-Turbo secured the 8th position among all evaluated models in the Text-to-Image Elo rankings provided by the Artificial Analysis AI Arena.",
                "position": 911
            },
            {
                "img": "https://arxiv.org/html/2511.22699/figures/image_arena_os.jpg",
                "caption": "Figure 17:Z-Image-Turbo ranks first among open-source models in the Text-to-Image Elo rankings from the Artificial Analysis AI Arena.",
                "position": 914
            },
            {
                "img": "https://arxiv.org/html/2511.22699/x14.png",
                "caption": "Figure 18:Comparison of close-up portrait generation, which indicates that Z-Image exhibits strong capabilities in character emotion and skin texture rendering. Better to zoom in to check the subtle expressions and the texture of the skin.",
                "position": 3980
            },
            {
                "img": "https://arxiv.org/html/2511.22699/x15.png",
                "caption": "Figure 19:Comparison of close-up portrait generation, which indicates that Z-Image exhibits strong capabilities in character emotion and skin texture rendering. Better to zoom in to check the subtle expressions and the texture of the skin.",
                "position": 3985
            },
            {
                "img": "https://arxiv.org/html/2511.22699/x16.png",
                "caption": "Figure 20:Comparison of complex close-up portrait generation, which indicates that Z-Image-Turbo has a strong ability in rendering character expressions and skin textures, as well as generating aesthetic images. Better to zoom in to check the subtle expressions.",
                "position": 3990
            },
            {
                "img": "https://arxiv.org/html/2511.22699/x17.png",
                "caption": "Figure 21:Comparison of scene shooting, which indicates that Z-Image-Turbo shows strong performance in the authenticity of both the person and the background, as well as the aesthetic appeal of layout and posture. Better to zoom in to check the texture of the clothes and hair.",
                "position": 3995
            },
            {
                "img": "https://arxiv.org/html/2511.22699/x18.png",
                "caption": "Figure 22:Comparison of scene shooting, which indicates that Z-Image-Turbo shows strong performance in the authenticity of both the person and the background, as well as the aesthetic appeal of layout and posture. Better to zoom in to check the details.",
                "position": 4000
            },
            {
                "img": "https://arxiv.org/html/2511.22699/x19.png",
                "caption": "Figure 23:Comparison of complex Chinese text rendering. It shows that only Z-Image-Turbo and Nano Banana Pro can accurately\ngenerates the expected Chinese couplet. Better to zoom in to check the correctness of the rendered text and the authenticity of the person.",
                "position": 4005
            },
            {
                "img": "https://arxiv.org/html/2511.22699/x20.png",
                "caption": "Figure 24:Comparison of complex English text rendering. It shows that only Z-Image-Turbo and Nano Banana Pro can accurately\ngenerates the expected English couplet. Better to zoom in to check the correctness of the rendered text and the layout of the scene.",
                "position": 4011
            },
            {
                "img": "https://arxiv.org/html/2511.22699/x21.png",
                "caption": "Figure 25:Comparison of Chinese text rendering in poster design. Z-Image-Turbo not only presents correct text rendering, but also designs a more aesthetically pleasing and realistic poster. Better to zoom in to check the correctness of the rendered text and the fidelity of the food.",
                "position": 4017
            },
            {
                "img": "https://arxiv.org/html/2511.22699/x22.png",
                "caption": "Figure 26:Comparison of English text rendering in poster design. Only Z-Image-Turbo presents correct text rendering with a pleasing and realistic poster. Better to zoom in to check the correctness of the rendered text and the details of the poster.",
                "position": 4022
            },
            {
                "img": "https://arxiv.org/html/2511.22699/x23.png",
                "caption": "Figure 27:The first two columns: Mixed-instruction editing across various tasks in Z-Image-Edit.The last two columns: Text editing (with bounding box) and identity-preservation editing in Z-Image-Edit.",
                "position": 4027
            },
            {
                "img": "https://arxiv.org/html/2511.22699/x24.png",
                "caption": "Figure 28:Showcases of prompt enhancer for logical reasoning.",
                "position": 4032
            },
            {
                "img": "https://arxiv.org/html/2511.22699/x25.png",
                "caption": "Figure 29:Showcases of prompt enhancer for world knowledge injection. Given the poem title \"After Passing the Imperial Examination\" (《登科后》), the baseline (Left) lacks cultural context. Our method (Right) leverages LLM priors to retrieve specific historical details (e.g., the galloping horse, red official robe) and the famous couplet: \"春风得意马蹄疾，一日看尽长安花。\", the reasoning module (center) translates these literary semantics into visual cues, ensuring a culturally faithful rendering with precise text transcription.",
                "position": 4037
            },
            {
                "img": "https://arxiv.org/html/2511.22699/x26.png",
                "caption": "Figure 30:Showcases of prompt enhancer in image editing for handling ambiguous and unclear instructions.",
                "position": 4042
            },
            {
                "img": "https://arxiv.org/html/2511.22699/x27.png",
                "caption": "Figure 31:Showcases of prompt enhancer in image editing for world knowledge injection and reasoning.",
                "position": 4047
            },
            {
                "img": "https://arxiv.org/html/2511.22699/x28.png",
                "caption": "Figure 32:Emerging Multi-lingual and Multi-cultural Understanding Capacity of Z-Image-Turbo. It shows that Z-Image-Turbo can not only understand prompts in multiple languages but also leverage its world knowledge to generate images that align with local cultures and landmarks.",
                "position": 4052
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Authors",
        "images": []
    },
    {
        "header": "Appendix APrompts Used in the Report",
        "images": []
    }
]