[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15807/x1.png",
                "caption": "Figure 1:Examples of using MICL to solve image captioning from MSCOCO(Chen etÂ al.,2015)(top) and Clock Math from our proposed dataset, TrueMICL (bottom). Generating captions relies more on the ability of task recognition, and a correct caption can be answered based on the text in the demos (e.g., mimicking the caption style in the demos), without a deep understanding of the demo images. However, our task requires task learning where the model needs to learn the relationship between the text and images in the demos (i.e., multiplying the two numbers pointed by the clock arrows) to correctly respond to the query.",
                "position": 115
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15807/extracted/6641155/img/heatmap.png",
                "caption": "(a)Attention heatmaps over input images without (top) and with (bottom) applying DARA.",
                "position": 619
            },
            {
                "img": "https://arxiv.org/html/2507.15807/extracted/6641155/img/heatmap.png",
                "caption": "(a)Attention heatmaps over input images without (top) and with (bottom) applying DARA.",
                "position": 622
            },
            {
                "img": "https://arxiv.org/html/2507.15807/extracted/6641155/img/chart_clean.png",
                "caption": "(b)Normalized attention ratios over image and text tokens without and with applying DARA.",
                "position": 627
            },
            {
                "img": "https://arxiv.org/html/2507.15807/extracted/6641155/img/heathead.png",
                "caption": "Figure 3:Learned attention amplification factors across 8 heads and 5 images in the 1st layer of Qwen2-VL. DARA introduces structured visual emphasis on demonstration images.",
                "position": 642
            },
            {
                "img": "https://arxiv.org/html/2507.15807/x2.png",
                "caption": "Figure 4:Performance comparison on Operator and Clock tasks across three models with different numbers of shots, ranging from 2 to 32. Compared to the baseline Random method, DARA consistently improves the MICL performance across different numbers of shots.",
                "position": 779
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADARA as a constrained version of LoRA",
        "images": []
    },
    {
        "header": "Appendix BTrueMICL",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15807/extracted/6641155/img/dataset/OI-d1.png",
                "caption": "Table 4:An overview of task examples in TrueMICL. The label to the query requires the model to learn the relationship between images and text in the demos.",
                "position": 1612
            },
            {
                "img": "https://arxiv.org/html/2507.15807/extracted/6641155/img/dataset/OI-d2.png",
                "caption": "",
                "position": 1633
            },
            {
                "img": "https://arxiv.org/html/2507.15807/extracted/6641155/img/dataset/OI-q.png",
                "caption": "",
                "position": 1635
            },
            {
                "img": "https://arxiv.org/html/2507.15807/extracted/6641155/img/dataset/CM-d1.png",
                "caption": "",
                "position": 1650
            },
            {
                "img": "https://arxiv.org/html/2507.15807/extracted/6641155/img/dataset/CM-d2.png",
                "caption": "",
                "position": 1652
            },
            {
                "img": "https://arxiv.org/html/2507.15807/extracted/6641155/img/dataset/CM-q.png",
                "caption": "",
                "position": 1654
            },
            {
                "img": "https://arxiv.org/html/2507.15807/extracted/6641155/img/dataset/SC-d1.png",
                "caption": "",
                "position": 1668
            },
            {
                "img": "https://arxiv.org/html/2507.15807/extracted/6641155/img/dataset/SC-d2.png",
                "caption": "",
                "position": 1670
            },
            {
                "img": "https://arxiv.org/html/2507.15807/extracted/6641155/img/dataset/SC-q.png",
                "caption": "",
                "position": 1672
            },
            {
                "img": "https://arxiv.org/html/2507.15807/extracted/6641155/img/dataset/CL-d1.png",
                "caption": "",
                "position": 1685
            },
            {
                "img": "https://arxiv.org/html/2507.15807/extracted/6641155/img/dataset/CL-d2.png",
                "caption": "",
                "position": 1687
            },
            {
                "img": "https://arxiv.org/html/2507.15807/extracted/6641155/img/dataset/CL-q.png",
                "caption": "",
                "position": 1689
            },
            {
                "img": "https://arxiv.org/html/2507.15807/extracted/6641155/img/dataset/SD-d1.png",
                "caption": "",
                "position": 1696
            },
            {
                "img": "https://arxiv.org/html/2507.15807/extracted/6641155/img/dataset/SD-d2.png",
                "caption": "",
                "position": 1698
            },
            {
                "img": "https://arxiv.org/html/2507.15807/extracted/6641155/img/dataset/SD-q.png",
                "caption": "",
                "position": 1700
            },
            {
                "img": "https://arxiv.org/html/2507.15807/extracted/6641155/img/dataset/PN-d1.png",
                "caption": "",
                "position": 1713
            },
            {
                "img": "https://arxiv.org/html/2507.15807/extracted/6641155/img/dataset/PN-d2.png",
                "caption": "",
                "position": 1715
            },
            {
                "img": "https://arxiv.org/html/2507.15807/extracted/6641155/img/dataset/PN-q.png",
                "caption": "",
                "position": 1717
            },
            {
                "img": "https://arxiv.org/html/2507.15807/extracted/6641155/img/dataset/CC-d1.jpg",
                "caption": "",
                "position": 1731
            },
            {
                "img": "https://arxiv.org/html/2507.15807/extracted/6641155/img/dataset/CC-d2.jpg",
                "caption": "",
                "position": 1733
            },
            {
                "img": "https://arxiv.org/html/2507.15807/extracted/6641155/img/dataset/CC-q.jpg",
                "caption": "",
                "position": 1735
            }
        ]
    },
    {
        "header": "Appendix CMore Details of Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15807/x3.png",
                "caption": "Table 8:Results on four classical vision-language tasks. Accuracy is used for VQAv2, GQA, and A-OKVQA and CIDEr used for COCO. The performance is similar across all methods, suggesting that classical tasks benefit little from in-context learning, and DARA shows no signs of overfitting.",
                "position": 1979
            },
            {
                "img": "https://arxiv.org/html/2507.15807/x4.png",
                "caption": "Figure 6:Comparison of performances when applying different settings of changed attention layers and heads.",
                "position": 2271
            }
        ]
    },
    {
        "header": "Appendix DMore Experimental Analysis",
        "images": []
    }
]