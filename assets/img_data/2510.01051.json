[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01051/x1.png",
                "caption": "Figure 1:Learning curves of Qwen3-based agents across diverse environments of 5 categories:game(language games);rg(ReasoningGym);code(coding tasks);math(python-integrated math questions);qa(search-integrated general questions). All agents are learned via a simple yet general multi-turn algorithm based on REINFORCE (Algorithm˜1). The comparison between two curves in each subplot illustrate the effectiveness of Return Batch Normalization (ReBN).",
                "position": 91
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2GEM environments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01051/x2.png",
                "caption": "Figure 2:Illustration of autoreset in vectorized environments. Autoresetting resets the environment automatically after termination, allowing users to collect batches of episodes by simply running.step()without needing more complicated logic such as keeping track of whether individual episodes have terminated.",
                "position": 227
            }
        ]
    },
    {
        "header": "3Reinforcement learning with GEM",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01051/x3.png",
                "caption": "Figure 3:The illustration of different view of agentic RL. Green nodes denote tokens responsible for loss.",
                "position": 246
            }
        ]
    },
    {
        "header": "4Empirical studies with GEM",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01051/x4.png",
                "caption": "Figure 4:Algorithm benchmarking using eight representative environments from GEM. All agents are trained fromQwen3-{scale}-Basemodels, withscalespecified in each plot.rgrefers to single-turn reasoning tasks from ReasoningGym;gameconsists of long-horizon language games;qaandmathare tool-integrated multi-turn environments.",
                "position": 336
            },
            {
                "img": "https://arxiv.org/html/2510.01051/x5.png",
                "caption": "Figure 5:(a)Average number of turns and episode return when trained with different discount factors.(b)Comparative experiment results on tool availability.",
                "position": 346
            },
            {
                "img": "https://arxiv.org/html/2510.01051/x6.png",
                "caption": "Figure 6:Training on thegame:sudoku-v0-easyenvironment generalizes to ReasoningGym.",
                "position": 568
            },
            {
                "img": "https://arxiv.org/html/2510.01051/x7.png",
                "caption": "Figure 7:(a)Training curves on two environments showing successful integration of GEM into five existing frameworks.(b)Asynchronous rollout improves wall-clock efficiency of training Sudoku-solving agents based on Qwen3-4B-Base.",
                "position": 586
            }
        ]
    },
    {
        "header": "5Agent evaluation with GEM",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01051/x8.png",
                "caption": "Figure 8:Benchmark results on MCPMark (Postgres subset) and Terminal-Bench (subset) using GEM as a unified evaluation toolkit.",
                "position": 600
            }
        ]
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AEnvironment registration",
        "images": []
    },
    {
        "header": "Appendix BCase studies of language games",
        "images": []
    },
    {
        "header": "Appendix CAlgorithm",
        "images": []
    },
    {
        "header": "Appendix DExtended empirical studies with GEM",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01051/x9.png",
                "caption": "Figure 9:Learning curves of different reward shaping strategies. (a-b) The average success rate of two environments. (c-d) The corresponding average number of turns taken to solve the tasks, equal to the number of tool calls minus one.",
                "position": 1609
            },
            {
                "img": "https://arxiv.org/html/2510.01051/x10.png",
                "caption": "Figure 10:Learning curves of vision-language agents. We RL-tune Qwen2.5-VL-3B/7B-Instruct using Dr. GRPO on themath:Geometry3Kenvironment and track their training rewards (left) and validation scores (right).",
                "position": 1633
            },
            {
                "img": "https://arxiv.org/html/2510.01051/x11.png",
                "caption": "Figure 11:An example problem and the response of a trained agent based on Qwen2.5-VL-7B-Instruct.",
                "position": 1637
            },
            {
                "img": "https://arxiv.org/html/2510.01051/x12.png",
                "caption": "Figure 12:Multi-agent evaluation on TAU-bench retail. Stronger user simulators (rows) consistently improve agent performance (columns) across model strengths.",
                "position": 1645
            }
        ]
    },
    {
        "header": "Appendix ERelated works",
        "images": []
    },
    {
        "header": "Appendix FExperimental settings",
        "images": []
    }
]