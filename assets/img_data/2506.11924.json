[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.11924/x1.png",
                "caption": "Figure 1:Our diffusion-based framework generates novel view image and its geometry at arbitrary target poses, from single or multipleunposedinput reference images. With an off-the-shelf geometry module, we predict an partial point cloud whose RGB values and geometry are projected to target viewpoint. Our diffusion models inpaint from these partial observations to generate plausible novel view image and geometry, with cross-ModalAttentionInstillation (MoAI) ensuring alignment between modalities, combined with partial geometry resulting a complete 3D scene.",
                "position": 126
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.11924/x2.png",
                "caption": "Figure 2:Training methodology.Our method conducts cross-modal attention instillation, replacing the spatial attention maps of geometry denoising networks with those of image denoising networks, so that the image generation U-Net learns a more robust representation aligned with the geometry completion task. On the other hand, the geometry prediction networks leverage the rich semantics from image features to enhance geometry completion capability.",
                "position": 194
            },
            {
                "img": "https://arxiv.org/html/2506.11924/x3.png",
                "caption": "Figure 3:Per-modality attention visualization.",
                "position": 288
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.11924/x4.png",
                "caption": "Figure 4:Qualitative results.We demonstrate our qualitative results on the Co3D[22]dataset, conducting feed-forward novel view synthesis while generating aligned geometry in a robust and consistent manner.",
                "position": 369
            },
            {
                "img": "https://arxiv.org/html/2506.11924/x5.png",
                "caption": "Figure 5:Qualitative comparison with inpainting method on DTU[34]dataset.Our qualitative comparison with the naive warping-and-inpainting method demonstrates our model’s zero-shot generalization capabilities to unseen data, as well as its ability to robustly handle erroneous warped geometries for geometrically consistent generation.",
                "position": 488
            },
            {
                "img": "https://arxiv.org/html/2506.11924/x6.png",
                "caption": "Figure 6:Qualitative comparison on extrapolative setting.Our qualitative comparison of previous approaches demonstrates our model’s extrapolative capabilities to plausibly generate locations not seen in reference images while reconstructing faithfully the known regions.",
                "position": 569
            },
            {
                "img": "https://arxiv.org/html/2506.11924/x7.png",
                "caption": "Figure 7:Comparison with LVSM.",
                "position": 577
            },
            {
                "img": "https://arxiv.org/html/2506.11924/x8.png",
                "caption": "Figure 8:Analysis on number of reference viewpoints.Our model’s multi-view aggregated attention enables our model to generalize towarding receiving arbitrary number of reference images.",
                "position": 637
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AAdditional details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.11924/x9.png",
                "caption": "Figure 9:Ablation on pointmap normalization.Ablation study comparing synthesis results with and without camera-space pointmap normalization. Normalization significantly improves geometric consistency, boundary sharpness, and geometric alignment with projected geometry.",
                "position": 760
            }
        ]
    },
    {
        "header": "Appendix BAdditional results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.11924/x10.png",
                "caption": "Figure 10:Qualitative results on single-view extrapolative setting.Our qualitative comparison of previous approaches demonstrates our model’s extrapolative capabilities to plausibly generate locations not seen in reference images while reconstructing faithfully the known regions.",
                "position": 777
            },
            {
                "img": "https://arxiv.org/html/2506.11924/x11.png",
                "caption": "Figure 11:Qualitative comparison on two-view extrapolative setting.Our qualitative comparison of previous approaches demonstrates our model’s extrapolative capabilities to plausibly generate locations not seen in reference images while reconstructing faithfully the known regions.",
                "position": 781
            },
            {
                "img": "https://arxiv.org/html/2506.11924/x12.png",
                "caption": "Figure 12:Qualitative comparison with other warping-and-inpainting models.Our qualitative comparison of previous approaches demonstrates our model’s extrapolative capabilities to plausibly generate locations not seen in reference images while reconstructing faithfully the known regions.",
                "position": 795
            },
            {
                "img": "https://arxiv.org/html/2506.11924/x13.png",
                "caption": "Figure 13:Ablation results.Our qualitative comparison of previous approaches demonstrates our model’s extrapolative capabilities to plausibly generate locations not seen in reference images while reconstructing faithfully the known regions.",
                "position": 809
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]