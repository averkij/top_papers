[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.04769/x1.png",
                "caption": "Figure 1:Evolution from Isolated Modalities to Unified Vision-Language-Action Models.This figure illustrates the transition from separate vision, language, and action systems-each limited to its own domain-to integrated VLA models. VLA models enable robots to jointly perceive, understand language, and act, overcoming the fragmentation of earlier approaches and marking a major step toward adaptive, generalizable, and intelligent embodied agents.",
                "position": 172
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Concepts of Vision-Language-Action Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.04769/x2.png",
                "caption": "Figure 5:Foundational Concept of VLA Models (in an Apple-Picking Scenario)This illustration depicts a robotic arm autonomously picking a ripe apple in an orchard, guided by a VLA model. On the right, a flowchart outlines the four key stages of VLA models: Multimodal Integration, Tokenization and Representation, Learning Paradigms, and Adaptive Control and Real-Time Execution.",
                "position": 313
            },
            {
                "img": "https://arxiv.org/html/2505.04769/x3.png",
                "caption": "Figure 7:The diagram illustrates the end-to-end tokenization and representation process in VLA models. Visual input (e.g., cluttered tabletop) is encoded by a vision encoder (e.g., ViT), while natural language instructions (e.g., “stack the green blocks”) are processed by a language encoder (e.g., T5). The system fuses prefix, state, and action tokens through a transformer and autoregressively predicts motor actions.",
                "position": 611
            },
            {
                "img": "https://arxiv.org/html/2505.04769/x4.png",
                "caption": "Figure 8:Illustrating how VLA models utilize prefix, state, and action tokens in real-world scenarios. In robotic manipulation, state tokens detect arm extension near fragile objects, enabling path adjustment. In navigation, they represent LiDAR and odometry data. The apple-picking task shows how prefix tokens guide goal understanding, while action tokens generate motion sequences for targeted grasping and execution.",
                "position": 614
            },
            {
                "img": "https://arxiv.org/html/2505.04769/x5.png",
                "caption": "Figure 9:Illustrating the process of how VLAs Encode the World.VLAs encode the world by converting vision, language, and sensor inputs into tokens, fusing them through cross-attention, predicting action sequences via transformers, and executing tasks with real-time feedback-enabling robots to interpret scenes, follow instructions, and adapt actions dynamically.",
                "position": 639
            },
            {
                "img": "https://arxiv.org/html/2505.04769/x6.png",
                "caption": "Figure 10:Learning Paradigms: Data Sources and Training Strategies for VLAs.",
                "position": 709
            }
        ]
    },
    {
        "header": "3Progress in Vision-Language-Action Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.04769/x7.png",
                "caption": "Figure 12:This figure illustrates “Helix,” a next-generation humanoid robot executing a household task using a VLA framework. Upon receiving a verbal command, Helix integrates a vision-language model (e.g., SigLIP) and a language model (e.g., LLaMA-2) to jointly perceive and interpret the environment. A hierarchical VLA controller plans and executes sub-tasks—opening the fridge, grasping a bottle—while an agentic AI module adapts actions in real time. This demonstrates VLA-based generalist robotics with dynamic task adaptation and safe, semantically grounded manipulation.",
                "position": 4626
            },
            {
                "img": "https://arxiv.org/html/2505.04769/x8.png",
                "caption": "Figure 13:This illustration depicts an autonomous delivery vehicle powered by a VLA system, integrating VLMs for visual grounding, LLMs for instruction parsing, and a VLA decoder for path planning. Agentic AI enables adaptive trajectory refinement in dynamic environments, exemplifying how multimodal integration drives safe, interpretable, and autonomous decision-making in real-world navigation tasks.",
                "position": 4654
            },
            {
                "img": "https://arxiv.org/html/2505.04769/x9.png",
                "caption": "Figure 14:a) This figure illustrates a VLA surgical system executing the task “apply a suture to the left coronary artery.” The vision module identifies anatomical targets, the language model interprets the instruction, and the action decoder generates precise motor commands, enabling adaptive tool control, real-time feedback, and safe autonomous operation; b) A VLA-powered assistive robot perceives patient behavior, processes verbal requests (e.g., “bring my walker”), and autonomously executes context-aware motion plans, enabling real-time assistance in eldercare, rehabilitation, and hospital logistics without relying on predefined scripts or manual oversight.",
                "position": 4689
            },
            {
                "img": "https://arxiv.org/html/2505.04769/x10.png",
                "caption": "Figure 15:This diagram illustrates the application of VLA models in precision and automated agriculture. A ground robot uses vision encoders to detect ripe fruits and interprets instructions such as “pick only Grade A fruits” through language encoders. Action tokens then guide robotic manipulators for efficient, damage-free picking. Drones leverage VLA models to analyze aerial imagery and verbal commands for targeted irrigation. Synthetic training environments and LoRA-based adaptation enable models to generalize across crop types, environmental conditions, and geographies. This VLA-driven pipeline promotes sustainable agriculture by improving productivity, reducing manual labor, and enhancing decision-making through multimodal perception and control.",
                "position": 4717
            },
            {
                "img": "https://arxiv.org/html/2505.04769/x11.png",
                "caption": "Figure 16:Showing how VLA models enable interactive AR navigation by fusing real-time visual perception, language understanding, and action planning. In dynamic environments such as airports, VLAs interpret user queries like “avoid stairs to Gate 22,” analyze visual scenes (e.g., detecting escalators), and adjust navigational paths accordingly, supporting personalized, accessible, and context-aware mobility guidance.",
                "position": 4745
            }
        ]
    },
    {
        "header": "4Challenges and Limitations of Vision-Language-Action Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.04769/x12.png",
                "caption": "Figure 17:Figure maps six core VLA challenges—real-time inference, multimodal fusion safety, dataset bias, integration complexity, compute demands, and robustness/ethics—against six targeted solutions: adaptive pruning, hybrid policy architectures, meta/transfer learning, LoRA/quantization, domain randomization, and ethical oversight. This systematic alignment clarifies pathways to robust, efficient, and safe VLA deployment across broader real-world robotic domains.",
                "position": 5122
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.04769/x13.png",
                "caption": "Figure 18:This conceptual illustration presents “Eva,” a future humanoid assistant powered by Vision-Language Models (VLMs), VLA frameworks, and agentic AI systems. VLMs enable semantic scene understanding and object affordance prediction, while VLAs translate language-grounded instructions into hierarchical motor plans. Agentic AI modules ensure adaptive learning, self-refinement, and interactive decision-making in open-ended environments. Together, these components represent a foundational blueprint for Artificial General Intelligence (AGI) in robotics, where perception, language understanding, planning, and safe autonomous behavior converge in real-world, socially aware tasks.",
                "position": 5212
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "Declarations",
        "images": []
    },
    {
        "header": "Statement on AI Writing Assistance",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]