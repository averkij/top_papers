[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07019/x1.png",
                "caption": "Figure 1:Token Dependencies.Illustration for token dependencies of sparse attention, linear attention, and NHA.",
                "position": 178
            },
            {
                "img": "https://arxiv.org/html/2510.07019/x2.png",
                "caption": "Figure 2:Intra-Layer Hybrid in NHA. (a) NHA compresses historical tokens into fixed-size long-term memory slots (brown) through an RNN update, then concatenates them with recent local context (gray) before applying unifiedsoftmax attention. (b) Previous intra-layer hybrid approaches generally compute long-term and short-term outputs separately and combine them through weighted summation. (c) In contrast, NHA employs a non-parametric, context-dependentsoftmax attentionoperation to dynamically determine their respective contributions.",
                "position": 235
            }
        ]
    },
    {
        "header": "2Preliminary",
        "images": []
    },
    {
        "header": "3Naive Hybrid Attention",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07019/x3.png",
                "caption": "Figure 3:Inter-Layer Hybrid in NHA. All layers share the same NHA architecture. By varying the sliding window sizeww, each layer can behave as full attention (w=Nw=N), intra-layer hybrid (w>0w>0), or pure linear RNN (w=0w=0), enabling flexible inter-layer hybridization without changing the model structure.",
                "position": 375
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07019/figs/ops_speed.png",
                "caption": "",
                "position": 916
            },
            {
                "img": "https://arxiv.org/html/2510.07019/x4.png",
                "caption": "Figure 5:Efficiency.Inference latency and memory usage of NHA-Llama3-8B compared with Llama3-8B.",
                "position": 1447
            }
        ]
    },
    {
        "header": "5Future Work",
        "images": []
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAnswers to Research Questions",
        "images": []
    },
    {
        "header": "Appendix BRelated Work and Positioning",
        "images": []
    },
    {
        "header": "Appendix CExperiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07019/figs/hybrid.png",
                "caption": "Figure 6:Average Recall Accuracy vs. Full Attention Layers.NHA-Llama3-8B attains the best performance with only 4 full attention layers.",
                "position": 2596
            }
        ]
    },
    {
        "header": "Appendix DGradient Coupling Analysis",
        "images": []
    },
    {
        "header": "Appendix EMemory Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07019/figs/nha_long_ratio.png",
                "caption": "Figure 7:Visualization of Long-Term Memory Usage.From left to right: (1) layer-wise mean long-ratio averaged over all heads and positions; (2) average long-ratio per layer–head pair aggregated over sequence positions; (3) average long-ratio per layer–position pair aggregated over attention heads. Gray-shaded layers indicate hybrid Transformer layers in the inter-layer configuration.",
                "position": 2639
            },
            {
                "img": "https://arxiv.org/html/2510.07019/figs/add_long_ratio.png",
                "caption": "Figure 8:Visualization of Long-Term Memory Usage.Across layers of the input-projection fusion variant.",
                "position": 2642
            }
        ]
    },
    {
        "header": "Appendix FDatasets",
        "images": []
    },
    {
        "header": "Appendix GUse of AI Assistants",
        "images": []
    }
]