[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10129/x1.png",
                "caption": "Figure 1:Conceptual Illustration of Our Proposed Method LaViT.",
                "position": 181
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Empirical Analysis of Perception Gap",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10129/figures/accuracy_vs_energy_curve_fine.png",
                "caption": "Figure 2:Impact of Visual Attention on Reasoning Accuracy.The monotonic increase in accuracy with higher Visual Focusing Score (Sf​o​c​u​sS_{focus}) thresholds validates that effective visual grounding is a prerequisite for correct reasoning.",
                "position": 273
            },
            {
                "img": "https://arxiv.org/html/2601.10129/figures/attention_divergence.png",
                "caption": "Figure 3:The Perception-Reasoning Gap.While the student aligns closely with the teacher in textual representations (stable Cosine Distance), their visual attention trajectories diverge significantly on attribute-heavy tokens (rising KL Divergence). This reveals thattextual mimicry does not imply visual grounding.",
                "position": 321
            }
        ]
    },
    {
        "header": "4Method",
        "images": []
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10129/figures/entropy_distribution.png",
                "caption": "Table 1:Performance comparison on multimodal benchmarks across three categories:Fine-grained Perception,Visual Reasoning, andMultimodal Robustness.\nThe best and second-best results are marked inboldandunderlined, respectively.\nGreen values indicate the absolute gains ofLaViTover the Qwen2.5-VL-3B baseline.\nAsterisks (*) denote results reported by papers.Fuet al.(2024)Liet al.(2025a)Liuet al.(2025)",
                "position": 507
            },
            {
                "img": "https://arxiv.org/html/2601.10129/figures/entropy_distribution.png",
                "caption": "Figure 4:Attention entropy distribution.",
                "position": 721
            },
            {
                "img": "https://arxiv.org/html/2601.10129/x2.png",
                "caption": "Figure 5:Visualization of attention distributions across Qwen2.5-VL-3B, 32B (Teacher), and LaViT on two representative samples from the BLINK.▲\\blacktriangleindicates the task-relevant critical regions required for correct reasoning.",
                "position": 829
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Implementation Details",
        "images": []
    },
    {
        "header": "Appendix BAnalysis of numbers ofKK",
        "images": []
    },
    {
        "header": "Appendix CTraining Data Construction",
        "images": []
    }
]