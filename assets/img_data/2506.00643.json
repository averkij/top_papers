[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00643/extracted/6500241/0_Abstract/github_logo.png",
                "caption": "",
                "position": 159
            },
            {
                "img": "https://arxiv.org/html/2506.00643/extracted/6500241/0_Abstract/hf_logo.png",
                "caption": "",
                "position": 161
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00643/extracted/6500241/2_Benchmark/fig1_6.jpg",
                "caption": "Figure 1:This is a representative example to show that LLMs struggle with SATA (Select All That Apply) questions. The models often provide wrong answers when multiple correct options are present.",
                "position": 172
            }
        ]
    },
    {
        "header": "2SATA-BenchData Curation",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00643/extracted/6500241/2_Benchmark/fig2_5.jpg",
                "caption": "Figure 2:SATA-BenchData Curation Process. The source data is converted to SATA format and then filtered forreadability,diversity(via question similarity),difficulty(via confusion scoring), andclarity(via human validation). Additional dataset-specific transformation steps are described in AppendixB.",
                "position": 210
            },
            {
                "img": "https://arxiv.org/html/2506.00643/extracted/6500241/2_Benchmark/fig3.jpg",
                "caption": "Figure 3:SATA-BenchDataset Overview.SATA-Benchcovers a diverse set of topics and achieves a balance between readability and difficulty (measured by confusion score). d1: Reading Comprehension, d2: Toxicity, d3: News, d4: Biomedicine, d5: Laws, and d6: Events.",
                "position": 433
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": []
    },
    {
        "header": "4Improving Performance on SATA Questions",
        "images": []
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADataset Description",
        "images": []
    },
    {
        "header": "Appendix BDataset Filtering",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00643/extracted/6500241/A_Appendix/eg1.jpg",
                "caption": "Figure 4:Representative examples of questions from various data sources used to constructSATA-Bench.",
                "position": 2509
            },
            {
                "img": "https://arxiv.org/html/2506.00643/extracted/6500241/A_Appendix/fig_diff_score1.png",
                "caption": "Figure 5:Confusion score distribution across all questions before filtering. d1: Reading Comprehension, d2: Toxicity, d3: News, d4: Biomedicine, d5: Laws, and d6: Events.",
                "position": 2512
            },
            {
                "img": "https://arxiv.org/html/2506.00643/extracted/6500241/A_Appendix/fig_diff_score2.png",
                "caption": "Figure 6:Confusion Score distribution of the filtered questions. d1: Reading Comprehension, d2: Toxicity, d3: News, d4: Biomedicine, d5: Laws, and d6: Events.",
                "position": 2515
            },
            {
                "img": "https://arxiv.org/html/2506.00643/extracted/6500241/A_Appendix/fig_diff_score2.png",
                "caption": "Figure 6:Confusion Score distribution of the filtered questions. d1: Reading Comprehension, d2: Toxicity, d3: News, d4: Biomedicine, d5: Laws, and d6: Events.",
                "position": 2518
            },
            {
                "img": "https://arxiv.org/html/2506.00643/extracted/6500241/A_Appendix/score_fig.jpg",
                "caption": "Figure 7:Confusion Score distribution separately visualized for each source dataset. (Left to right) Top row: Reading Comprehension, Toxicity, News; Bottom row: Biomedicine, Laws, Events.",
                "position": 2524
            }
        ]
    },
    {
        "header": "Appendix CHyperparameters",
        "images": []
    },
    {
        "header": "Appendix DCompute Resources",
        "images": []
    },
    {
        "header": "Appendix ENon-expert Human Benchmark",
        "images": []
    },
    {
        "header": "Appendix FMetrics Definition",
        "images": []
    },
    {
        "header": "Appendix GUnselection bias metric",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00643/extracted/6500241/A_Appendix/fig8.jpg",
                "caption": "Figure 8:Relationship between Selection Probability Divergence (SPD) and prediction probability (ùööùöö\\mathtt{q}typewriter_q) across different ground truth probabilities (ùöôùöô\\mathtt{p}typewriter_p). The curves are averaged over 100 replicates, and the shaded area represents the standard deviation. In each plot, the minimal value of SPD is00atùöö=ùöôùööùöô\\mathtt{q}=\\mathtt{p}typewriter_q = typewriter_p, when the prediction aligns with the ground truth.",
                "position": 3320
            }
        ]
    },
    {
        "header": "Appendix HPrompts used in experimentation",
        "images": []
    },
    {
        "header": "Appendix IInference Error Handling",
        "images": []
    },
    {
        "header": "Appendix JMore Details on Key Observations",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00643/extracted/6500241/A_Appendix/fpfn.png",
                "caption": "Figure 9:Ratio of false positive rate to false negative rate per label for each evaluated LLM.",
                "position": 3974
            },
            {
                "img": "https://arxiv.org/html/2506.00643/extracted/6500241/A_Appendix/recall.png",
                "caption": "Figure 10:Recall score per label (Y-axis), normalized by subtracting the model‚Äôs average recall. Most models exhibit at least one label with significantly lower recall than the rest.",
                "position": 3977
            },
            {
                "img": "https://arxiv.org/html/2506.00643/extracted/6500241/A_Appendix/count.png",
                "caption": "Figure 11:Relationship between predicted and actual correct choice counts across models. Models generally under-select the correct number of answer choices. Y-axis represents the average number of choices selected by the model. X-axis represents the actual number of correct choices. A perfect model would align along the diagonal where X equals Y.",
                "position": 3980
            },
            {
                "img": "https://arxiv.org/html/2506.00643/extracted/6500241/A_Appendix/ea.png",
                "caption": "Figure 12:Relationship between Exact Match Rate and the number of correct choices. As the number of correct choices increases, the exact match rate decreases. None of the models achieve an exact match rate above 20% when the number of correct choices exceeds 7.",
                "position": 3983
            },
            {
                "img": "https://arxiv.org/html/2506.00643/extracted/6500241/2_Benchmark/spider.png",
                "caption": "Figure 13:Performance breakdown of evaluated models across different source datasets.",
                "position": 3997
            }
        ]
    },
    {
        "header": "Appendix KPriDe Debiasing Algorithm Adaptation for SATA",
        "images": []
    },
    {
        "header": "Appendix LExperiment Setup for Choice Funnel",
        "images": []
    },
    {
        "header": "Appendix MAblation Study for Choice Funnel",
        "images": []
    },
    {
        "header": "Appendix NPositional Bias Under Randomized Answer Orderings",
        "images": []
    },
    {
        "header": "Appendix OPer-Dataset Performance Breakdown",
        "images": []
    }
]