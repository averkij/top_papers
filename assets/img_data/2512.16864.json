[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16864/x1.png",
                "caption": "",
                "position": 67
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16864/x2.png",
                "caption": "Figure 2:Overview of our RePlan framework.The bottom part of the figure shows the overall architecture. Given an input image and text instruction, the VLM analyzes them via chain-of-thought reasoning and produces region-aligned guidance, where each guidance includes a region bbox and its editing hint. Each hint is futher encoded by a text encoder into a feature token, while image patch tokens are obtained by VAE encoding and grouped according to the region bounding boxes. A group-specific attention mechanism, detailed in Figure4, is proposed to allow MMDiT to generate the final edited image. The top part of the figure presents an editing examples.",
                "position": 143
            },
            {
                "img": "https://arxiv.org/html/2512.16864/x3.png",
                "caption": "Figure 3:VLM output format Example",
                "position": 180
            },
            {
                "img": "https://arxiv.org/html/2512.16864/x4.png",
                "caption": "Figure 4:Attention rule visualization. We use different highlight colors to indicate different rules, which correspond toHint isolation,Region constraint,Background constraintandImage–latent full interaction.",
                "position": 249
            }
        ]
    },
    {
        "header": "4Data Construction and Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16864/x5.png",
                "caption": "(a)",
                "position": 412
            },
            {
                "img": "https://arxiv.org/html/2512.16864/x5.png",
                "caption": "(a)",
                "position": 415
            },
            {
                "img": "https://arxiv.org/html/2512.16864/x5.png",
                "caption": "(a)",
                "position": 418
            },
            {
                "img": "https://arxiv.org/html/2512.16864/x6.png",
                "caption": "(b)",
                "position": 424
            },
            {
                "img": "https://arxiv.org/html/2512.16864/x7.png",
                "caption": "(c)",
                "position": 432
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16864/x8.png",
                "caption": "Figure 7:Editing results comparison.We use Flux.1 Kontext dev as the backbone of RePlan. Notably, GPT-4o enforces fixed aspect ratios, leading to unavoidable cropping for non-standard images.",
                "position": 624
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix AAPPEAL OF USING LLMS",
        "images": []
    },
    {
        "header": "Appendix BMore IV-Edit Statistics",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16864/x9.png",
                "caption": "Figure 8:Instruction length distribution",
                "position": 733
            },
            {
                "img": "https://arxiv.org/html/2512.16864/x9.png",
                "caption": "Figure 8:Instruction length distribution",
                "position": 736
            },
            {
                "img": "https://arxiv.org/html/2512.16864/x10.png",
                "caption": "Figure 9:Distribution of expected editing region counts per instruction",
                "position": 741
            }
        ]
    },
    {
        "header": "Appendix CData Construction Details",
        "images": []
    },
    {
        "header": "Appendix DComparison with Global Rephrase",
        "images": []
    },
    {
        "header": "Appendix EAttention Rule Discovery",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16864/x11.png",
                "caption": "Figure 10:The right image is the original, and the left image shows the editing result where the attention between image patches of the editing region and the background is cut off. Clear regional boundaries can be observed.",
                "position": 1059
            },
            {
                "img": "https://arxiv.org/html/2512.16864/figure/latent.png",
                "caption": "Figure 11:We decouple the noise patches and image patches, preserving their respective self-attention, but all image patches are only allowed to attend to the noise patches within the editing region. The resulting edited image, as shown in the figure, is able to retain the content and style of the original region.",
                "position": 1062
            },
            {
                "img": "https://arxiv.org/html/2512.16864/figure/origin.png",
                "caption": "Figure 12:The left image shows the editing result, and the right image visualizes the editing region on top of the result. We mask out the background patches’ attention to the global prompt token, and find that the image patches unable to attend to any text tokens exhibit severe distortion.",
                "position": 1066
            },
            {
                "img": "https://arxiv.org/html/2512.16864/figure/edited.png",
                "caption": "",
                "position": 1075
            }
        ]
    },
    {
        "header": "Appendix FB-boxes Overlapping Case",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16864/figure/overlaping/overlap_445.jpg",
                "caption": "Figure 13:Overlapping Case 1",
                "position": 1088
            },
            {
                "img": "https://arxiv.org/html/2512.16864/figure/overlaping/overlap_796.jpg",
                "caption": "Figure 14:Overlapping Case 2",
                "position": 1091
            }
        ]
    },
    {
        "header": "Appendix GRobustness Against B-box Perturbation",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16864/x12.png",
                "caption": "Figure 15:More Comparative Results. We use Flux.1 Kontext dev as the backbone of RePlan. For the second column, the result of Flux.1 Kontext dev has a slight perspective change.",
                "position": 1144
            },
            {
                "img": "https://arxiv.org/html/2512.16864/x13.png",
                "caption": "Figure 16:More Comparative Results. We use Flux.1 Kontext dev as the backbone of RePlan.",
                "position": 1147
            },
            {
                "img": "https://arxiv.org/html/2512.16864/x14.png",
                "caption": "Figure 17:More Comparative Results under text editing scenario. We use Flux.1 Kontext dev as the backbone of RePlan.",
                "position": 1150
            }
        ]
    },
    {
        "header": "Appendix HMore Comparative Results",
        "images": []
    }
]