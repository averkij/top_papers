[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02185/x1.png",
                "caption": "Figure 1:Motivation. Existing Vision-DeepResearch benchmarks often fail to measure realistic multimodal search: many questions can be solved via text-only cues or model priors without genuine visual verification, and whole-image search frequently retrieves near-duplicate images with identifying metadata (“perfect retrieval”). VDR-Bench is designed to be visual-search–centric and to reflect real-world settings that require iterative, entity-level localization (e.g., multi-round cropping), cross-modal evidence collection, and multi-hop reasoning.",
                "position": 102
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3A Quantitative Analysis of Existing Vision-DeepResearch Benchmarks",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02185/x2.png",
                "caption": "Figure 2:Dataset composition and example instances from VDR-Bench.The figure presents the distribution across ten visual domains, along with representative question–answer examples for each category.",
                "position": 181
            }
        ]
    },
    {
        "header": "4VDR-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02185/x3.png",
                "caption": "Figure 3:VDR-Bench is constructed via a multi-stage, vision-centric workflow:(1) annotators manually crop salient regions and perform web-scale visual search; (2) candidate entities are extracted from retrieved results and verified through an MLLM-assisted and human checking process; (3) verified visual entities are used to generate seed VQA pairs; (4) question difficulty is expanded via knowledge-graph–based multi-hop reasoning; and (5) automatic solvability checks and human quality filtering ensure that each instance requires visual evidence, remains unambiguous, and avoids trivial or near-duplicate retrieval.",
                "position": 461
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02185/x4.png",
                "caption": "Figure 4:Relationship between overall answer accuracy and entity-level recall on VDR-Bench. Points correspond to different models evaluated under two retrieval strategies (CIS+TS and CIS+TS+MVF), with colors indicating model families and marker shapes indicating the search mode. The plot shows a strong positive association between correctly answering questions and successfully retrieving task-relevant entities, and further indicates that MVF tends to improve both metrics across models.",
                "position": 888
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Results",
        "images": []
    }
]