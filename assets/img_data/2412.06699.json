[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06699/extracted/6055991/images/logo.png",
                "caption": "",
                "position": 92
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06699/x1.png",
                "caption": "Figure 1:Overview of See3D.(a) We propose a four-step data curation pipeline to select multi-view images from Internet videos, forming the WebVi3D dataset, which includes‚àºsimilar-to\\sim‚àº16M video clips across diverse categories and concepts.\n(b) Given multiple views, we corrupt the original data into corrupted imagesctisuperscriptsubscriptùëêùë°ùëñc_{t}^{i}italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPTat timesteptùë°titalic_tby applying random masks and time-dependent noise. We then reweight the guidance ofctisuperscriptsubscriptùëêùë°ùëñc_{t}^{i}italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPTand the noisy latentxtisuperscriptsubscriptùë•ùë°ùëñx_{t}^{i}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPTfor the diffusion model to formvisual-conditionvtisuperscriptsubscriptùë£ùë°ùëñv_{t}^{i}italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPTthrough a time-dependent mixture.\n(c) MVD model is capable of training at scale to generate multi-view images conditioned onvtisuperscriptsubscriptùë£ùë°ùëñv_{t}^{i}italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT, without requiring pose annotations. Sincevtisuperscriptsubscriptùë£ùë°ùëñv_{t}^{i}italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPTis a task-agnostic visual signal formed through time-dependent noise and mixture, it enables the trained model to robustly adapt to various downstream tasks.",
                "position": 177
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06699/x2.png",
                "caption": "Figure 2:(a-Row1): Dynamic content modifies scene geometry across views; (a-Row2): Limited camera movement provides insufficient multi-view observations; (b) Our WebVi3D comprises static scenes with diverse camera trajectories.",
                "position": 312
            },
            {
                "img": "https://arxiv.org/html/2412.06699/x3.png",
                "caption": "Figure 3:See3D for Multi-View Generation: From iteratively generated views (brown camera), we randomly select a few anchor views (yellow stars) to guide the generation of target views along thegray camera trajectory.\nKeypoint matching is first performed to establish correspondences between the anchor views.\nNext, monocular depth estimation is applied to the latest anchor view, followed by ourIterative Sparse Pixel-Wise Depth Alignmentto refine the depth and recover a dense map. This dense depth is then used to warp images along the gray camera viewpoints. Subsequently, the warped images and anchor images are combined and processed according to Eq.2and Eq.3, without random masking, forming thevisual-condition, which guides MVD model to produce 3D-consistent target views. Finally, thegray cameraturns tobrown, guiding multi-view generation in the next iteration.",
                "position": 425
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06699/x4.png",
                "caption": "Figure 4:Qualitative Comparison of Single/Sparse View Generation.The top three rows are results with a single view input. The bottom two rows are novel view renderings from 3DGS, where Ours is trained on dense multi-view generation given 3 views as input. Our method outperformed other baselines in capturing high-frequency details, such as text and stairs.",
                "position": 776
            },
            {
                "img": "https://arxiv.org/html/2412.06699/x5.png",
                "caption": "Figure 5:Top: Qualitative ablation ofvisual-condition; Bottom: As timestep decreases, visualize the trend ofvisual-condition.",
                "position": 831
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ABroader Impact and Limitations",
        "images": []
    },
    {
        "header": "Appendix BVideo Data Curation",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06699/x6.png",
                "caption": "Figure 6:Single-view to 3D. Compared with LucidDreamer[11]and ViewCrafter[112], which are also conditioned on warped images, our model can consistently generate high-fidelity views with detailed texture and structural information.",
                "position": 2608
            },
            {
                "img": "https://arxiv.org/html/2412.06699/x7.png",
                "caption": "Figure 7:Sparse-views to 3D. Given 3 input views, our model generates clear, high-fidelity novel views that closely match the ground truth (GT), without artifacts or blurring. Note that the results from DepthSplat[105]are cropped and resized following the same data processing as the official source code.",
                "position": 2611
            },
            {
                "img": "https://arxiv.org/html/2412.06699/x8.png",
                "caption": "Figure 8:Examples of Open-world 3D Editing. (a) Occlusion-free Editing: An Asian-style attic is added, and novel views are generated realistically. (b) Full Replacement Editing: A vase is replaced with a toy fox, seamlessly integrated into the scene from various viewpoints. (c) Occluded Editing: Hidden regions in the masked areas are inferred and completed to produce novel views.",
                "position": 2614
            }
        ]
    },
    {
        "header": "Appendix CTechnical Implementations",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06699/extracted/6055991/images/w_t.png",
                "caption": "Figure 9:Piecewise FunctionWtsubscriptùëäùë°W_{t}italic_W start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, showing linear decay for timestepstùë°titalic_tbetween 300 and 1000, and a monotonically decreasing concave behavior fort<300ùë°300t<300italic_t < 300.",
                "position": 2669
            }
        ]
    },
    {
        "header": "Appendix DMore Experimental Results",
        "images": []
    },
    {
        "header": "Appendix EAdditional Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06699/x9.png",
                "caption": "Figure 10:Ablation on Pixel-level Depth Alignment.",
                "position": 3404
            },
            {
                "img": "https://arxiv.org/html/2412.06699/x9.png",
                "caption": "Figure 10:Ablation on Pixel-level Depth Alignment.",
                "position": 3407
            },
            {
                "img": "https://arxiv.org/html/2412.06699/x10.png",
                "caption": "Figure 11:Examples of Long-sequence Generation.High-quality novel views generated along complex camera trajectories, maintaining spatial consistency and visual realism across extended sequences.",
                "position": 3481
            },
            {
                "img": "https://arxiv.org/html/2412.06699/x11.png",
                "caption": "Figure 12:More Examples of Long-sequence Generation.",
                "position": 3486
            }
        ]
    },
    {
        "header": "Appendix FAdditional Visualizations",
        "images": []
    }
]