[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2UniSandBox: Controlled Evaluation",
        "images": []
    },
    {
        "header": "3Reasoning Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20561/x1.png",
                "caption": "Figure 1:Data examples for reasoning generation. All images are generated by BAGEL. Normal and CoT represent generation without/with think mode (Chain-of-Thought mode), respectively. We also shows the relative prompts.",
                "position": 150
            },
            {
                "img": "https://arxiv.org/html/2511.20561/x2.png",
                "caption": "Figure 2:Overview of the STARS framework.It illustrates the three sequential stages: (I) Data Generation, where CoT is leveraged to create reasoning-generation pairs; (II) Sample Filtering, which uses the understanding module of UMMs to curate high-quality data; and (III) Fine-tuning, where the unified model is trained with the filtered data to distill CoT reasoning into its standard generation process.",
                "position": 177
            },
            {
                "img": "https://arxiv.org/html/2511.20561/x3.png",
                "caption": "Figure 3:Framework for Knowledge Transfer evaluation.The framework first injects novel knowledge (Virtual Character Profiles, left) into the Unified Model’s understanding module via fine-tuning. We then evaluate the model’s ability to utilize this new knowledge through two distinct generative tasks: Forward Retrieval (Key→\\rightarrowValue), which requires generating an attribute from a name, and Inverse Search (Value→\\rightarrowKey), which requires identifying and generating a character based on their attributes (right).",
                "position": 642
            }
        ]
    },
    {
        "header": "4Knowledge Transfer",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20561/x4.png",
                "caption": "Figure 4:We visualize the total probability of relevant words corresponding to different queries. The “Last Text Token” entry, serving as a baseline, presents the probability of the last text token from the MLLM before the query. For clarity, only queries with probabilities exceeding 0.01 are displayed.",
                "position": 743
            }
        ]
    },
    {
        "header": "5Related Works",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Acknowledgements",
        "images": []
    },
    {
        "header": "Appendix ASynthetic Data Using GPT4o",
        "images": []
    },
    {
        "header": "Appendix BAblation Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20561/x5.png",
                "caption": "Figure 5:The average results of Bagel(normal) on Math for the ablation of Reject Sampling.",
                "position": 1073
            }
        ]
    },
    {
        "header": "Appendix CEvaluation Using MLLM",
        "images": []
    },
    {
        "header": "Appendix DImplementation Details of Knowledge Injection",
        "images": []
    },
    {
        "header": "Appendix ERelated Works",
        "images": []
    },
    {
        "header": "Appendix FDetails of Reject Sampling",
        "images": []
    },
    {
        "header": "Appendix GLimitation",
        "images": []
    }
]