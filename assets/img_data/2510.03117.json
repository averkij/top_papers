[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.03117/x1.png",
                "caption": "Figure 1:Examples of sounding videos generated by our BridgeDiT model, showcasing high quality, temporal synchronization, and text alignment.\nOur method generates high-fidelity video frames and detailed audio spectrograms that remain faithful to the given text prompts.\nCritically, as highlighted in the dashed boxes, the generated audio and video are precisely synchronized, demonstrating strong temporal coherence between visual events and their corresponding sounds. More cases are shown in the anonymous demo pagehttps://bridgedit-t2sv.github.io.",
                "position": 165
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.03117/x2.png",
                "caption": "Figure 2:Our three-stage Hierarchical Visual-Grounded Captioning (HVGC) framework generates disentangled modality-pure text captions. First, a Vision-Language Large Model (VLLM) produces a detailed video caption (TVT_{V}). Subsequently, a Large Language Model (LLM) extracts relevant audio tags from this video caption. Finally, the framework leverages both the visual context inTVT_{V}and the extracted audio tags to generate a pure audio caption (TAT_{A}).",
                "position": 307
            },
            {
                "img": "https://arxiv.org/html/2510.03117/x3.png",
                "caption": "Figure 3:The BridgeDiT Architecture. (a): The overall dual-tower architecture. Parallel video and audio DiT streams are connected by our proposed BridgeDiT Block at specific layers.Right: Details\nof fusion strategies within the block, showcasing our proposed Dual Cross-Attention (b) alongside the Full-Attention (c) and Additive Fusion (d) baselines.",
                "position": 322
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.03117/figures/av-align-fusion-ablation.png",
                "caption": "Table 2:Performance on VGGSound-SS and Landscape. AV denotes\nAV-Align metric here.Bestandsecond-bestare highlighted.",
                "position": 590
            },
            {
                "img": "https://arxiv.org/html/2510.03117/figures/av-align-fusion-ablation.png",
                "caption": "Table 3:Ablation study on disentangled text condition. We compare shared caption strategies (using the video caption or an Omini model caption) against disentangled caption strategies (using an Audio-LLM or our method) in both full-training and zero-shot settings.",
                "position": 745
            },
            {
                "img": "https://arxiv.org/html/2510.03117/figures/av-align-fusion-ablation.png",
                "caption": "Figure 4:Comparing different fusion mechanisms. Our DCA fusion mechanism outperforms all other baselines in both AV-Align and VA-IB Score.",
                "position": 914
            },
            {
                "img": "https://arxiv.org/html/2510.03117/figures/va-ib-fusion-ablation.png",
                "caption": "",
                "position": 923
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Ethics statement",
        "images": []
    },
    {
        "header": "Reproducibility statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AThe Use of Large Language Models",
        "images": []
    },
    {
        "header": "Appendix BDiffusion and Flow Matching Generation Models",
        "images": []
    },
    {
        "header": "Appendix CExperiments Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.03117/x4.png",
                "caption": "Figure 5:Prompts for Stage1: Detailed Visual Scene Description",
                "position": 2139
            },
            {
                "img": "https://arxiv.org/html/2510.03117/x5.png",
                "caption": "Figure 6:Prompts for Stage2: Auditory Concept Abstraction",
                "position": 2142
            },
            {
                "img": "https://arxiv.org/html/2510.03117/x6.png",
                "caption": "Figure 7:Prompts for Stage3: Visually-Grounded Audio Caption Generation",
                "position": 2146
            },
            {
                "img": "https://arxiv.org/html/2510.03117/x7.png",
                "caption": "Figure 8:Detailed Command for Human Annotation",
                "position": 2153
            }
        ]
    },
    {
        "header": "Appendix DAblation Study on BridgeDiT Block Placement",
        "images": []
    },
    {
        "header": "Appendix ELimitation and Future Work",
        "images": []
    }
]