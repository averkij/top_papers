[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16173/x1.png",
                "caption": "Figure 1:The overview of the SceneWalk dataset includes (a) dataset comparison, (b) detailed statistics, and (c) the annotation pipeline for description and score collection. Note that the scale of circles inFig.1(a) indicates the data size, and the color distribution inFig.1(b) denotes the video duration in each video categoryâ€”brighter colors correspond to shorter video durations. Further details about the dataset are provided in Appendix A.",
                "position": 237
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3SceneWalk Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16173/x2.png",
                "caption": "Figure 2:The network overview of SALOVA. Our framework consists of four structural components: vision encoder, ST-connector, SR-router, and LLMs. Using the FocusFast strategy, our model can concentrate on more detailed local information while maintaining context awareness.",
                "position": 310
            }
        ]
    },
    {
        "header": "4Segment-Augmented LOng Video Assistant",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16173/x3.png",
                "caption": "Figure 3:Comparison results of V-NIAH. The x/y-axis indicates the total video frames and the location of needle image within the video, respectively.",
                "position": 965
            },
            {
                "img": "https://arxiv.org/html/2411.16173/x4.png",
                "caption": "",
                "position": 973
            }
        ]
    },
    {
        "header": "6Discussion and Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails of SceneWalk Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16173/x5.png",
                "caption": "Figure 4:Detailed video duration range statistics for each video category in the SceneWalk dataset.",
                "position": 1960
            },
            {
                "img": "https://arxiv.org/html/2411.16173/x6.png",
                "caption": "Figure 5:WordCloud analysis of the SceneWalk dataset.",
                "position": 2041
            }
        ]
    },
    {
        "header": "Appendix BTraining Details of SALOVA",
        "images": []
    },
    {
        "header": "Appendix CArchitecture Details of SALOVA",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16173/x7.png",
                "caption": "Figure 6:Examples of the SceneWalk dataset (i).",
                "position": 2612
            },
            {
                "img": "https://arxiv.org/html/2411.16173/x8.png",
                "caption": "Figure 7:Examples of the SceneWalk dataset (ii).",
                "position": 2615
            },
            {
                "img": "https://arxiv.org/html/2411.16173/x9.png",
                "caption": "Figure 8:Qualitative examples on Video-MME[20]with SALOVA-Qwen. Note that thered dashed linesindicates the top-1 relevant video segment estimation for the question.",
                "position": 2618
            }
        ]
    },
    {
        "header": "Appendix DAdditional Experiments",
        "images": []
    }
]