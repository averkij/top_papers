[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20515/x1.png",
                "caption": "Figure 2:AlignBenchspans diverse Image2Text (i.e., Captioner) and Text2Image models, diverse image domains, and provides high-quality annotations enriched with hallucination-type labels for deep analysis. The rightmost figure presents the example of annotations. We first conduct sentence-level correctness annotation and further annotate the segment of hallucination and its type label.",
                "position": 94
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20515/x2.png",
                "caption": "Figure 3:Examples of hallucinated sentences in AlignBench.The hallucinated portions are often subtle, requiring fine-grained image-text alignment ability to detect them.",
                "position": 130
            }
        ]
    },
    {
        "header": "3Datasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20515/x3.png",
                "caption": "Figure 4:Left:Ratio of incorrect sentences by position; all captioners make fewer errors at the first position. Different colors indicate different positions.Right:Number of unaligned sentences per category; most mistakes occur in attributes and text.",
                "position": 332
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20515/x4.png",
                "caption": "Figure 5:Examples ofincorrectsentences with detectors’ correctness scores.Higher scores indicate greater confidence in correctness. Detectors are prone to being overconfident in these examples. We highlight detectors’ errors in red within the text and mark the groundedincorrectregions in the image with orange boxes.",
                "position": 798
            },
            {
                "img": "https://arxiv.org/html/2511.20515/x5.png",
                "caption": "(a)",
                "position": 806
            },
            {
                "img": "https://arxiv.org/html/2511.20515/x5.png",
                "caption": "(a)",
                "position": 809
            },
            {
                "img": "https://arxiv.org/html/2511.20515/x6.png",
                "caption": "(b)",
                "position": 814
            },
            {
                "img": "https://arxiv.org/html/2511.20515/x7.png",
                "caption": "Figure 7:Detectors show positional bias in scoring.We average the detectors’ correctness scores (Y-axis) by sentence position (X-axis) and visualize the results using GPT-4o (Left) and Llama-4 (Right) as detectors. Both detectors assign higher scores to sentences appearing near the beginning of the output. The detector isnotprovided with any positional information during inference.",
                "position": 821
            },
            {
                "img": "https://arxiv.org/html/2511.20515/x8.png",
                "caption": "Figure 8:Detectors struggle to detect their own hallucination.Left:Self- and cross-evaluation results. AUROC scores for each Captioner (columns), normalized by the average AUROC of each Detector (rows). Diagonal entries show self-evaluation.Right:We pick GPT-4o as a detector, with their output correctness scores averaged by sentence position.Blueandredlines show scores forcorrectandincorrectGPT-4o’s outputs;greenshows scores forincorrectLlama-4 outputs.",
                "position": 972
            },
            {
                "img": "https://arxiv.org/html/2511.20515/x9.png",
                "caption": "Figure 9:Detectors’ score averaged within each hallucination type.Detectors show weakness inDirectionandNumber.",
                "position": 1205
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix ALimitation",
        "images": []
    },
    {
        "header": "Appendix BThe Use of Large Language Models (LLMs)",
        "images": []
    },
    {
        "header": "Appendix CAttribution to an icon",
        "images": []
    },
    {
        "header": "Appendix DDataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20515/x10.png",
                "caption": "Figure A:Example of an interface used for the hallucination detection.",
                "position": 1437
            },
            {
                "img": "https://arxiv.org/html/2511.20515/x11.png",
                "caption": "Figure B:Example of an interface used for the hallucination type annotation.",
                "position": 1440
            },
            {
                "img": "https://arxiv.org/html/2511.20515/x12.png",
                "caption": "Figure C:Example annotations of error type. Hallucinations are highlighted in red.",
                "position": 1500
            },
            {
                "img": "https://arxiv.org/html/2511.20515/x13.png",
                "caption": "Figure D:Ratio of incorrect sentences for each image domain.All models tend to produce more errors in domains such asillustrationandText.",
                "position": 1757
            },
            {
                "img": "https://arxiv.org/html/2511.20515/x14.png",
                "caption": "Figure E:Ratio of incorrect sentences within each sentence position per model.Different colors indicate different positions. All models produce fewer errors at the 1st position.",
                "position": 1764
            },
            {
                "img": "https://arxiv.org/html/2511.20515/x15.png",
                "caption": "Figure F:Number of hallucinations for each category.Most models make many mistakes in attributes and text.",
                "position": 1774
            }
        ]
    },
    {
        "header": "Appendix EDetails of Experimental Setups",
        "images": []
    },
    {
        "header": "Appendix FAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20515/x16.png",
                "caption": "Figure G:Detector’s output score for their own output captions.",
                "position": 2213
            },
            {
                "img": "https://arxiv.org/html/2511.20515/x17.png",
                "caption": "Figure H:Distributions of evaluators’ output scores.We visualize the evaluators’ scores for GPT-4o captions.",
                "position": 2220
            },
            {
                "img": "https://arxiv.org/html/2511.20515/x18.png",
                "caption": "Figure I:Examples of input image and sentences with detectors’ correctness scores. Higher scores indicate greater confidence in correctness. We highlight detectors’ errors in red within the text.",
                "position": 2227
            },
            {
                "img": "https://arxiv.org/html/2511.20515/x19.png",
                "caption": "Figure J:Example annotations of Share-GPT.",
                "position": 2240
            },
            {
                "img": "https://arxiv.org/html/2511.20515/x20.png",
                "caption": "Figure K:Example annotations of LLaVA.",
                "position": 2243
            },
            {
                "img": "https://arxiv.org/html/2511.20515/x21.png",
                "caption": "Figure L:Example annotations of Qwen-2.",
                "position": 2246
            },
            {
                "img": "https://arxiv.org/html/2511.20515/x22.png",
                "caption": "Figure M:Example annotations of GPT4o.",
                "position": 2249
            },
            {
                "img": "https://arxiv.org/html/2511.20515/x23.png",
                "caption": "Figure N:Example annotations of CogVLM.",
                "position": 2252
            },
            {
                "img": "https://arxiv.org/html/2511.20515/x24.png",
                "caption": "Figure O:Example annotations of Llama-4.",
                "position": 2255
            },
            {
                "img": "https://arxiv.org/html/2511.20515/x25.png",
                "caption": "Figure P:Example annotations of Stable Diffusion.",
                "position": 2258
            },
            {
                "img": "https://arxiv.org/html/2511.20515/x26.png",
                "caption": "Figure Q:Example annotations of GPT-Gen.",
                "position": 2261
            }
        ]
    },
    {
        "header": "Appendix GAdditional Examples of Annotations",
        "images": []
    }
]