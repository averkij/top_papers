[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09575/x1.png",
                "caption": "Figure 1:Comparison of lifting language into 3D representations between ReferSplat[He et al.,2025b]and OpenVoxel.Note that ReferSplat additionally requires manual text annotations for training 3DGS to equipped with Bert embeddings, while ours is totally training-free and do not need any additional annotations.",
                "position": 168
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Preliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09575/x2.png",
                "caption": "Figure 2:Overview of OpenVoxel.Taking a sparse voxel modelV1:NV_{1:N}pre-trained from multi-view imagesI1:KI_{1:K}and their corresponding camera poseξ1:K\\xi_{1:K}, we aim to build a voxel group fieldℱ1:N\\mathcal{F}_{1:N}from segmentation masksM1:KM_{1:K}obtained from SAM2. Withℱ1:N\\mathcal{F}_{1:N}we render images and masks for all groups to obtain canonical captions and construct a Scene MapSSrecoding their position and captions. In the Referring Query Inference stage we take a query description and image to find the ideal target group that matches the description by query refinement and text-to-text retrieving fromSS, enabling complex segmentation tasks such as referring expression segmentation (RES).",
                "position": 217
            }
        ]
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09575/x3.png",
                "caption": "Figure 3:Detail of the grouping process.Taking the pre-trained voxel modelV1:KV_{1:K}, we initialize the Group Fieldℱ1:N0\\mathcal{F}^{0}_{1:N}, Feature weightW1:N0W_{1:N}^{0}as empty tensors, and Group DictionaryG0G^{0}as empty dictionary. Then start fromξ1\\xi_{1}, we project the SAM masksM1M_{1}to 3D voxel and updateℱ1:N\\mathcal{F}_{1:N},W1:NW_{1:N}, andGG. By match and merge masks from the other views repeating this process, the finalℱ1:N\\mathcal{F}_{1:N},W1:NW_{1:N}, andGGis able to represent the group information ofV1:NV_{1:N}.",
                "position": 245
            },
            {
                "img": "https://arxiv.org/html/2601.09575/x4.png",
                "caption": "Figure 4:Detail of the Canonical Captioning.Given the group masks rendered of a specific group (taking the green apple as example) from our group field and their corresponding images, we leverage the Describe Anything Model (DAM) to first obtain a detailed caption. Then a Qwen3-VL model is conducted to canonicalize the caption into a fixed form, benefiting further usage.",
                "position": 337
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09575/x5.png",
                "caption": "Figure 5:Qualitative results of RES task on theFigurinesscene.",
                "position": 586
            },
            {
                "img": "https://arxiv.org/html/2601.09575/x6.png",
                "caption": "Figure 6:Qualitative results of RES task on theRamenscene.",
                "position": 732
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ADetails",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09575/x7.png",
                "caption": "Figure 7:Detail of the mask merging process.",
                "position": 880
            }
        ]
    },
    {
        "header": "Appendix BSemantic Segmentation",
        "images": []
    },
    {
        "header": "Appendix CAblation study.",
        "images": []
    },
    {
        "header": "Appendix DQualitative results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09575/x8.png",
                "caption": "Figure 11:Qualitative results of RES task on theTeatimescene.",
                "position": 1646
            },
            {
                "img": "https://arxiv.org/html/2601.09575/x9.png",
                "caption": "Figure 12:Qualitative results of RES task on theKitchenscene.",
                "position": 1649
            },
            {
                "img": "https://arxiv.org/html/2601.09575/x10.png",
                "caption": "Figure 13:Qualitative results of RES task on theTeatimescene with other queries.Note that these queries are created additionally and all of them are not appeared in the original annotations from Ref-LeRF subset (so there are no ground truth mask for them). We can see that ReferSplat[He et al.,2025b]struggles to recognize unseen target objects even in the same scene it is optimized, showing that it tends to overfit on annotated objects from the dataset.",
                "position": 1657
            }
        ]
    },
    {
        "header": "Appendix EDiscussions and Limitations.",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]