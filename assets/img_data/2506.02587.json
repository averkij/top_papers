[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.02587/extracted/6507361/figures/bevcalib_overview.png",
                "caption": "Figure 1:Overall architecture ofBEVCalib.The overall pipeline of our model consists of BEV feature extraction, FPN BEV Encoder, and geometry-guided BEV decoder (GGBD). For BEV feature extraction (¬ß3.2), the inputs of the camera and LiDAR are extracted into BEV features through different backbones separately, then fused into a shared BEV feature space. The FPN BEV encoder is used to improve the multi-scale geometric information of the BEV representations. For geometry-guided BEV decoder (¬ß3.3) utilizes a novel feature selector that efficiently decodes calibration parameters from BEV features.‚ÑíRsubscript‚ÑíùëÖ\\mathcal{L}_{R}caligraphic_L start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT,‚ÑíTsubscript‚Ñíùëá\\mathcal{L}_{T}caligraphic_L start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT, and‚ÑíP‚Å¢Csubscript‚ÑíùëÉùê∂\\mathcal{L}_{PC}caligraphic_L start_POSTSUBSCRIPT italic_P italic_C end_POSTSUBSCRIPTare loss functions introduced at ¬ß3.4.",
                "position": 111
            },
            {
                "img": "https://arxiv.org/html/2506.02587/extracted/6507361/figures/bevcalib_ggbd.png",
                "caption": "Figure 2:Overall Architecture of Geometry-Guided BEV Decoder (GGBD).The GGBD component contains a feature selector (left) and a refinement module (right). The feature selector calculates the positions of BEV features using Equation1. The corresponding positional embeddings (PE) are added to keep the geometry information of the selected feature. After the decoder, the refinement module adds an average-pooling operation to aggregate high-level information, following two separate heads to predict translation and rotation parameters.",
                "position": 202
            }
        ]
    },
    {
        "header": "4Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.02587/extracted/6507361/figures/calibdb.png",
                "caption": "(a)CalibDB",
                "position": 818
            },
            {
                "img": "https://arxiv.org/html/2506.02587/extracted/6507361/figures/calibdb.png",
                "caption": "(a)CalibDB",
                "position": 821
            },
            {
                "img": "https://arxiv.org/html/2506.02587/extracted/6507361/figures/kitti.png",
                "caption": "(b)KITTI",
                "position": 826
            },
            {
                "img": "https://arxiv.org/html/2506.02587/extracted/6507361/figures/overlay-comparison_high_resolution.png",
                "caption": "Figure 4:Qualitative results. A comparison of LiDAR-camera overlays from KITTI sequences. From top to bottom: ground-truth,BEVCalib, Koide3[10], CalibAnything[11], Regnet[8].",
                "position": 833
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]