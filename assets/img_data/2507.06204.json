[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.06204/extracted/6606677/figures/github.png",
                "caption": "",
                "position": 114
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.06204/x1.png",
                "caption": "Figure 1:Comparative illustration of our variants Diff-Mamba and Diff-S6 versus the original Mamba architecture, where‚äótensor-product\\otimes‚äóis elementwise multiplication,œÉùúé\\sigmaitalic_œÉis the SILU activation, Linear and Conv1D are standard linear projection and 1-dimensional convolution layers, and N stands for normalizations.",
                "position": 347
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.06204/x2.png",
                "caption": "Figure 3:Comparison of test curves through the training for Mamba and Diff-Mamba. The top row shows results for 6-layer models, and the bottom row for 12-layer models. Columns correspond to datasets: Enwik8 (left), Text-8 (center), and WikiText-103 (right).",
                "position": 663
            },
            {
                "img": "https://arxiv.org/html/2507.06204/x3.png",
                "caption": "",
                "position": 668
            },
            {
                "img": "https://arxiv.org/html/2507.06204/x4.png",
                "caption": "",
                "position": 669
            },
            {
                "img": "https://arxiv.org/html/2507.06204/x5.png",
                "caption": "",
                "position": 672
            },
            {
                "img": "https://arxiv.org/html/2507.06204/x6.png",
                "caption": "",
                "position": 673
            },
            {
                "img": "https://arxiv.org/html/2507.06204/x7.png",
                "caption": "",
                "position": 674
            },
            {
                "img": "https://arxiv.org/html/2507.06204/x8.png",
                "caption": "(a)BABILong Finetuned",
                "position": 745
            },
            {
                "img": "https://arxiv.org/html/2507.06204/x8.png",
                "caption": "(a)BABILong Finetuned",
                "position": 748
            },
            {
                "img": "https://arxiv.org/html/2507.06204/x9.png",
                "caption": "(b)Zero-Shot",
                "position": 753
            },
            {
                "img": "https://arxiv.org/html/2507.06204/x10.png",
                "caption": "Figure 5:Measuring Signal-to-Noise Ratio:Noise in intermediate representations is assessed using a controlled setup integrating a well-defined retrieval task with tools from the domain of model understanding and mechanistic interpretability. The y-axis represents the probability of predicting the desired needle token, where lower values indicate higher noise. The x-axis denotes various layers within the model where intermediate noise is measured. Models are fine-tuned BABILong from Subsec.4.3. Results show the average needle probabilities in each layer on 2k examples in test BABILong questions of up to 1k tokens.",
                "position": 767
            },
            {
                "img": "https://arxiv.org/html/2507.06204/extracted/6606677/figures/scaling/per_token_loss.png",
                "caption": "(a) The x-axis is the token index, and the y-axis is the correspondingper-tokenloss.",
                "position": 777
            },
            {
                "img": "https://arxiv.org/html/2507.06204/extracted/6606677/figures/scaling/per_token_loss.png",
                "caption": "(a) The x-axis is the token index, and the y-axis is the correspondingper-tokenloss.",
                "position": 780
            }
        ]
    },
    {
        "header": "5Discussion: The Importance of Differential Design",
        "images": []
    },
    {
        "header": "6Conclusions and Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.06204/x11.png",
                "caption": "Figure 7:Training curve of Mamba and Diff-Mamba during the fine-tuning of PG19.",
                "position": 1675
            },
            {
                "img": "https://arxiv.org/html/2507.06204/x11.png",
                "caption": "Figure 7:Training curve of Mamba and Diff-Mamba during the fine-tuning of PG19.",
                "position": 1678
            },
            {
                "img": "https://arxiv.org/html/2507.06204/x12.png",
                "caption": "(a)Zero-Shot Diff-Mamba",
                "position": 1742
            },
            {
                "img": "https://arxiv.org/html/2507.06204/x12.png",
                "caption": "(a)Zero-Shot Diff-Mamba",
                "position": 1745
            },
            {
                "img": "https://arxiv.org/html/2507.06204/x13.png",
                "caption": "(b)Zero-Shot Mamba",
                "position": 1750
            },
            {
                "img": "https://arxiv.org/html/2507.06204/x14.png",
                "caption": "(a)Fine-tuned Diff-Mamba",
                "position": 1810
            },
            {
                "img": "https://arxiv.org/html/2507.06204/x14.png",
                "caption": "(a)Fine-tuned Diff-Mamba",
                "position": 1813
            },
            {
                "img": "https://arxiv.org/html/2507.06204/x15.png",
                "caption": "(b)Fine-tuned Mamba",
                "position": 1818
            },
            {
                "img": "https://arxiv.org/html/2507.06204/x16.png",
                "caption": "Figure 10:Loss training curve of Diff-Mamba variants compared to Mamba through pre-training on The-Pile. Diff-Mamba-Alternate indicates alternating layers of Mamba and Diff-Mamba, while Diff-Mamba-Full indicates only Diff-Mamba layers model. The trends were smoothed for display adjustment.",
                "position": 1841
            }
        ]
    },
    {
        "header": "Appendix AExperimental Setup",
        "images": []
    }
]