[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "Introduction",
        "images": []
    },
    {
        "header": "Related Work",
        "images": []
    },
    {
        "header": "Recipe for Building MRM",
        "images": []
    },
    {
        "header": "Experimental Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.16127/x1.png",
                "caption": "Figure 1:The Effect of Different Regularization Strategies on Reward Model Performance.The solid red line shows the performance variation withλ\\lambda. The dashed line represents a baseline model trained with only length normalization and no zero-coefficient regularization (λ=0\\lambda=0). The results show that performance generally declines asλ\\lambdaincreases from zero.",
                "position": 662
            },
            {
                "img": "https://arxiv.org/html/2509.16127/x2.png",
                "caption": "Figure 2:Performance Comparison on Pure-text RM Benchmarks.The MLLM trained with all data (Qwen 2.5 VL-7B) shows no performance gain over the same MLLM trained with text-only data, despite the larger dataset. Both are outperformed by LLMs (Qwen 2.5 8B and Qwen 3 8B) trained on the identical text dataset, highlighting that LLM architectures are more suitable for text-centric reward modeling.",
                "position": 1121
            }
        ]
    },
    {
        "header": "BaseReward",
        "images": []
    },
    {
        "header": "Conclusion and Limitation",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]