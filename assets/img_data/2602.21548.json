[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21548/x1.png",
                "caption": "Figure 1.Existing bottleneck (left) and DualPath (right).",
                "position": 111
            }
        ]
    },
    {
        "header": "2.Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21548/x2.png",
                "caption": "Figure 2.Agent trajectory example.",
                "position": 193
            }
        ]
    },
    {
        "header": "3.Bottleneck & Motivation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21548/x3.png",
                "caption": "Figure 3.Left: Hardware trends of NVIDIA GPUs. Right: Relative token throughput with varying request batch size (each request has 30K context with 300 tokens appended).",
                "position": 280
            }
        ]
    },
    {
        "header": "4.DualPath System Overview",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21548/x4.png",
                "caption": "(a)PE Read Path",
                "position": 320
            },
            {
                "img": "https://arxiv.org/html/2602.21548/x4.png",
                "caption": "(a)PE Read Path",
                "position": 323
            },
            {
                "img": "https://arxiv.org/html/2602.21548/x5.png",
                "caption": "(b)DE Read Path",
                "position": 328
            }
        ]
    },
    {
        "header": "5.CNIC-Centric Traffic Manager",
        "images": []
    },
    {
        "header": "6.Adaptive Request Scheduler",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21548/x6.png",
                "caption": "Figure 5.An illustration of Inter-Engine PE Scheduling. All eight GPUs are in the same PE engine group and the scheduler will choose the best.",
                "position": 571
            },
            {
                "img": "https://arxiv.org/html/2602.21548/x7.png",
                "caption": "Figure 6.Intra-Engine Schedule. Left: compute-quota-based batch selection. Right: GPU timeline before and after applying compute quota.",
                "position": 708
            }
        ]
    },
    {
        "header": "7.Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21548/x8.png",
                "caption": "Figure 7.Offline inference performance under varying numbers of agents and maximum agent context lengths. Top: DS 27B. Middle: DS 660B. Bottom: Qwen 32B. N/A for running into an error before finishing.",
                "position": 726
            },
            {
                "img": "https://arxiv.org/html/2602.21548/x9.png",
                "caption": "Figure 8.Impact of prefill-decode ratio on offline inference performance (DS 27B).",
                "position": 856
            },
            {
                "img": "https://arxiv.org/html/2602.21548/x10.png",
                "caption": "Figure 9.Left: varying append lengths (DS 660B, 64K context, 1024 agents). Right: varying generation lengths (DS 660B, 64K, 1024 agents)",
                "position": 860
            },
            {
                "img": "https://arxiv.org/html/2602.21548/x11.png",
                "caption": "Figure 10.TTFT, TTST, and TPOT as functions of agent arrival rate (APS). Shadow means the fluctuation in the last 150s before experiments finish. Top: DS 27B, Bottom: DS 660B.",
                "position": 863
            },
            {
                "img": "https://arxiv.org/html/2602.21548/x12.png",
                "caption": "Figure 11.Average completion time of all trajectories versus arrival rate for online serving.",
                "position": 905
            },
            {
                "img": "https://arxiv.org/html/2602.21548/x13.png",
                "caption": "Figure 12.Left (§ 7.4): TTFT breakdown for online serving (DS 660B) across APSs, Sch. for scheduling, A. for allocating, R. for reading KV-cache, PF. for prefill. In each pair of pillars, the first is for DualPath and the second is for Basic. Right (§ 7.5): Offline inference ablation results (DS 660B, 64K context length). Layer, DPL, Sched stands for Layerwise prefill, Dual-Path Loading, and scheduling, respectively.",
                "position": 917
            },
            {
                "img": "https://arxiv.org/html/2602.21548/x14.png",
                "caption": "Figure 13.Load balance of storage NICs traffic",
                "position": 929
            },
            {
                "img": "https://arxiv.org/html/2602.21548/x15.png",
                "caption": "Figure 14.Load balance of attention execution time.",
                "position": 932
            },
            {
                "img": "https://arxiv.org/html/2602.21548/x16.png",
                "caption": "Figure 15.48P96D offline inference metrics. 1e7 is the scaling factor of Prompt TPS.",
                "position": 996
            }
        ]
    },
    {
        "header": "8.Discussion",
        "images": []
    },
    {
        "header": "9.Related Work",
        "images": []
    },
    {
        "header": "10.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]