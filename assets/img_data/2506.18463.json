[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18463/extracted/6558414/visu/main_figure.png",
                "caption": "Figure 1:Our unsupervised Dense In-context Post-training (DIP) method.\nDuring post-training, the model is given a pseudo in-context task, created automatically without human input. Each task includes a query image and a pseudo-labeled support set with a positive example (sharing objects with the query, as shown in the zebra example) and ‚Äúdistractor‚Äù examples that contain different object categories. The model predicts the pseudo-labeled semantic segmentation of the query image using the support set as reference. To do this, it (1) extracts patch-wise features from both the query and support images using the vision encoderf‚Å¢(‚ãÖ)ùëì‚ãÖf(\\cdot)italic_f ( ‚ãÖ )and projection headh‚Å¢(‚ãÖ)‚Ñé‚ãÖh(\\cdot)italic_h ( ‚ãÖ ); (2) computes segmentation predictions for the query image through cross-attention (query patch features as queries, support patch features as keys, and pseudo-labeled support patches as values). A pixel-wise cross-entropy loss is applied to the predictions. By training on these pseudo tasks,DIPenables the encoder to learn transferable dense representations, which are later used to efficiently solve new real in-context tasks.",
                "position": 180
            },
            {
                "img": "https://arxiv.org/html/2506.18463/extracted/6558414/visu/pseudo_labels/dog_pseudo_vs_label_0_orig.png",
                "caption": "Figure 2:Examples of automatically constructed in-context scene understanding tasks.Each row shows a query image and its corresponding positive support example. (a) and (b) display the query image and its pseudo segmentation labels, while (c) and (d) show the positive support image and its pseudo segmentation labels. Despite being generated in a fully unsupervised manner, the segmentation masks for salient objects are highly accurate, closely matching the actual objects.\nIn addition, the query and positive image pairs share a common object with the same pseudo-label. During post-training with ourDIPmethod, the model predicts the query image‚Äôs segmentation using the positive example as a reference, along with distractor support examples (randomly sampled from other images in the mini-batch, not shown here for brevity).",
                "position": 235
            },
            {
                "img": "https://arxiv.org/html/2506.18463/extracted/6558414/visu/pseudo_labels/dog_pseudo_vs_label_0_pseudo.png",
                "caption": "",
                "position": 240
            },
            {
                "img": "https://arxiv.org/html/2506.18463/extracted/6558414/visu/pseudo_labels/dog_pseudo_vs_label_1_orig.png",
                "caption": "",
                "position": 241
            },
            {
                "img": "https://arxiv.org/html/2506.18463/extracted/6558414/visu/pseudo_labels/dog_pseudo_vs_label_1_pseudo.png",
                "caption": "",
                "position": 242
            },
            {
                "img": "https://arxiv.org/html/2506.18463/extracted/6558414/visu/pseudo_labels/firething_output_0_orig.png",
                "caption": "",
                "position": 245
            },
            {
                "img": "https://arxiv.org/html/2506.18463/extracted/6558414/visu/pseudo_labels/firething_output_0_pseudo.png",
                "caption": "",
                "position": 246
            },
            {
                "img": "https://arxiv.org/html/2506.18463/extracted/6558414/visu/pseudo_labels/firething_output_1_orig.png",
                "caption": "",
                "position": 247
            },
            {
                "img": "https://arxiv.org/html/2506.18463/extracted/6558414/visu/pseudo_labels/firething_output_1_pseudo.png",
                "caption": "",
                "position": 248
            },
            {
                "img": "https://arxiv.org/html/2506.18463/extracted/6558414/visu/pseudo_labels/zebra_output_0_orig.png",
                "caption": "",
                "position": 251
            },
            {
                "img": "https://arxiv.org/html/2506.18463/extracted/6558414/visu/pseudo_labels/zebra_output_0_pseudo.png",
                "caption": "",
                "position": 252
            },
            {
                "img": "https://arxiv.org/html/2506.18463/extracted/6558414/visu/pseudo_labels/zebra_output_1_orig.png",
                "caption": "",
                "position": 253
            },
            {
                "img": "https://arxiv.org/html/2506.18463/extracted/6558414/visu/pseudo_labels/zebra_output_1_pseudo.png",
                "caption": "",
                "position": 254
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18463/extracted/6558414/visu/heatmaps/fire1.png",
                "caption": "Figure 4:Correlation maps between a query image patch (highlighted with a red cross) and a reference image, comparing DINOv2R andDIP.DIPgenerates coherent, object-level correlations, while DINOv2R produces localized, part-level responses.",
                "position": 1149
            },
            {
                "img": "https://arxiv.org/html/2506.18463/extracted/6558414/visu/heatmaps/fire2.png",
                "caption": "",
                "position": 1154
            },
            {
                "img": "https://arxiv.org/html/2506.18463/extracted/6558414/visu/heatmaps/firemapdino.png",
                "caption": "",
                "position": 1155
            },
            {
                "img": "https://arxiv.org/html/2506.18463/extracted/6558414/visu/heatmaps/usfire.png",
                "caption": "",
                "position": 1156
            },
            {
                "img": "https://arxiv.org/html/2506.18463/extracted/6558414/visu/heatmaps/zebracross.png",
                "caption": "",
                "position": 1159
            },
            {
                "img": "https://arxiv.org/html/2506.18463/extracted/6558414/visu/heatmaps/zebra2_resized.png",
                "caption": "",
                "position": 1160
            },
            {
                "img": "https://arxiv.org/html/2506.18463/extracted/6558414/visu/heatmaps/zebramapdino_resized.png",
                "caption": "",
                "position": 1161
            },
            {
                "img": "https://arxiv.org/html/2506.18463/extracted/6558414/visu/heatmaps/zebrausmap_resized.png",
                "caption": "",
                "position": 1162
            },
            {
                "img": "https://arxiv.org/html/2506.18463/extracted/6558414/visu/heatmaps/sheepdinoimage_resized.png",
                "caption": "",
                "position": 1165
            },
            {
                "img": "https://arxiv.org/html/2506.18463/extracted/6558414/visu/heatmaps/sheep2_resized.png",
                "caption": "",
                "position": 1166
            },
            {
                "img": "https://arxiv.org/html/2506.18463/extracted/6558414/visu/heatmaps/sheepdinomap_resized.png",
                "caption": "",
                "position": 1167
            },
            {
                "img": "https://arxiv.org/html/2506.18463/extracted/6558414/visu/heatmaps/sheepusmap_resized.png",
                "caption": "",
                "position": 1168
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation details",
        "images": []
    },
    {
        "header": "Appendix BAdditional quantitative results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18463/extracted/6558414/visu/girrafes_pseudo_vs_label.png",
                "caption": "(a)Query image",
                "position": 2514
            },
            {
                "img": "https://arxiv.org/html/2506.18463/extracted/6558414/visu/sheep_pseudo_vs_label.png",
                "caption": "",
                "position": 2518
            }
        ]
    },
    {
        "header": "Appendix CAdditional qualitative results",
        "images": []
    }
]