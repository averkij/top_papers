[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15021/x1.png",
                "caption": "Figure 1:ECHOdistills collective discussion about a new generative model into a structured benchmark. As a case study, we apply ECHO to GPT-4o Image Gen(OpenAI,2025a)on Twitter/X. Left: ECHOautomaticallysurfaces highly diverse and novel tasks not covered in prior benchmarks.\nRight: Consequently, our image-to-image split shows a 3.2x larger relative performance gap compared to a prior image editing benchmark, GEdit(Liu et al.,2025).",
                "position": 130
            }
        ]
    },
    {
        "header": "2Background & Related Work",
        "images": []
    },
    {
        "header": "3Crowdsourcing a Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15021/x2.png",
                "caption": "Figure 2:ECHO Framework.ECHO is motivated by several challenges inherent to social media. (1)\nWe start with broad queries followed by relevance filtering, since basic querying presents a volume-relevance tradeoff. (2) We then extract prompts from these posts, making sure to utilize the full post tree, as context can be spread across posts. (3) We then apply multimodal processing, since useful data also exists in non-standard formats. (4) Finally, we reserve the highest quality data for benchmarking, while the rest is used for analysis.",
                "position": 161
            },
            {
                "img": "https://arxiv.org/html/2510.15021/x3.png",
                "caption": "Figure 3:Dataset Comparison.The prompts in ECHO are significantly different from prior benchmarks.\nTop: the image-to-image split is more diverse and complex, with more unique first bigrams. Bottom: the text-to-image split is more fluent, as measured by perplexity under Pythia 12B(Biderman et al.,2023).",
                "position": 280
            }
        ]
    },
    {
        "header": "4ECHO: A Social-Media Post-Release Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15021/x4.png",
                "caption": "Figure 4:Common Failures Observed by Users.A word cloud of failure cases, derived from community feedback, showing practical capabilities that users care about in real use cases and curiosity-driven tests that reveal deeper model limitations. Common failures include identity shift, color drift, text rendering errors, and style mismatches; more exploratory failures include originality and reasoning about volume.",
                "position": 318
            },
            {
                "img": "https://arxiv.org/html/2510.15021/x5.png",
                "caption": "Figure 5:How Users Interact with Models.We depict qualitative examples of (a) user solutions and (b) exploratory behaviors, discovered via community feedback. We also visualize (c) activity trends using the timestamps of collected posts.",
                "position": 331
            }
        ]
    },
    {
        "header": "5ECHO differentiates models",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15021/x6.png",
                "caption": "Figure 6:Overall Evaluation.We compare a range of unified models, as well as an image editing (Flux Kontext) and shallow fusion (LLM+Diffusion) baseline. We report the win rate, or percentage of pairwise comparisons won. The win rate is calculated automatically with an ensemble of three VLMs-as-a-judge.",
                "position": 385
            },
            {
                "img": "https://arxiv.org/html/2510.15021/x7.png",
                "caption": "Figure 7:Qualitative Model Comparison. Challenging tasks from our benchmark, ranging from translation to multi-concept combination to mathematical reasoning, elicit diverse model responses. We mark samples where the model fails to generate an output as “Error.”",
                "position": 390
            },
            {
                "img": "https://arxiv.org/html/2510.15021/x8.png",
                "caption": "Figure 8:Specialized Metrics from Community Feedback.Based on qualitative community observations, we validate that 4o Image Gen exhibits large shifts in color (a) and face identity (b), moderate shifts in structure distance (c), but superior text rendering accuracy (d).",
                "position": 424
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ALimitations",
        "images": []
    },
    {
        "header": "Appendix BAdditional Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15021/x9.png",
                "caption": "Figure B.1:Per-Model Average Color Histogram.For each model, we compute the average color histogram of its outputs on the image-to-image split (top), then overlay it on top of a real image as a visual aid (bottom). Evidently, 4o Image Gen exhibits a substantial yellow tint.",
                "position": 1093
            },
            {
                "img": "https://arxiv.org/html/2510.15021/x10.png",
                "caption": "Figure B.2:Qualitative Comparison of Identity and Spatial Shift.Given the prompt“Billy the Kid cleaned up and colorized from the famous photo of him”each model retains the input identity to varying degrees. For the prompt“giving it a fresh twist with a more detailed, realistic touch”each model retains the input image’s spatial layout by a different amount.",
                "position": 1096
            },
            {
                "img": "https://arxiv.org/html/2510.15021/x11.png",
                "caption": "Figure B.3:Image-to-image examples from ECHO.",
                "position": 1100
            },
            {
                "img": "https://arxiv.org/html/2510.15021/x12.png",
                "caption": "Figure B.4:Image-to-image examples from ECHO, continued.",
                "position": 1104
            },
            {
                "img": "https://arxiv.org/html/2510.15021/x13.png",
                "caption": "Figure B.5:Text-to-image examples from ECHO.",
                "position": 1108
            },
            {
                "img": "https://arxiv.org/html/2510.15021/x14.png",
                "caption": "Figure B.6:Text-to-image examples from ECHO.",
                "position": 1112
            }
        ]
    },
    {
        "header": "Appendix CHuman Ranking & Correlation with LLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15021/x15.png",
                "caption": "Figure C.1:The consensus ranking, and mean ranking by annotator for each of the models. As we can see, because of the limited size of the annotation sets, the standard deviations of the bars is quite high, meaning that we can draw very few conclusions about model performance overall from the human data.",
                "position": 1124
            },
            {
                "img": "https://arxiv.org/html/2510.15021/x16.png",
                "caption": "Figure C.2:Kendall’sτb\\tau_{b}for pairs of individual human raters and LLM judges (without consensus ranking). We can see that human annotators have fairly high inter-rater correlation, while LLM judges have slight positive correlations, with most correlations signficant among them. In the figure,∗∗∗→p<0.001***\\to p<0.001,∗∗→p<0.01**\\to p<0.01,∗→p<0.05*\\to p<0.05.",
                "position": 1298
            }
        ]
    },
    {
        "header": "Appendix DData Collection Pipeline",
        "images": []
    },
    {
        "header": "Appendix EAutomatic Evaluation Metrics",
        "images": []
    },
    {
        "header": "Appendix FLLM Disclosure",
        "images": []
    }
]