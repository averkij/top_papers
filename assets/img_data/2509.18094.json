[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.18094/x1.png",
                "caption": "Figure 1:UniPixelflexibly supports a large variety of fine-grained image and video understanding tasks, including referring/reasoning/interactive segmentation, motion-grounded video reasoning, and referred video description & question answering. It can also handle a novelPixelQAtask that jointly requires object-centric referring, segmentation, and question answering in videos.",
                "position": 85
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.18094/x2.png",
                "caption": "Figure 2:Schematic comparison between UniPixel and its counterparts.To the best of our knowledge, UniPixel is the first unified method supporting simultaneous object referring and segmentation.",
                "position": 100
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.18094/x3.png",
                "caption": "Figure 3:The architecture of UniPixel.Given a video, a question, and visual prompts, the model encodes them into tokens via the visual encoder, prompt encoder, and tokenizer, respectively, then predicts a spatial-temporal mask for each visual prompt via the mask decoder. The masks are updated into the object memory bank, and subsequently injected into the prompt for pixel-level reasoning.",
                "position": 138
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.18094/x4.png",
                "caption": "Figure 4:Joint positional & temporal encodingfor point (X1Y1T) and box (X1Y1X2Y2T) prompts.",
                "position": 154
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.18094/x5.png",
                "caption": "Figure 5:Visualization of the outputs from UniPixel on PixelQA task. Star marks and boxes refer to point and box prompts, respectively. The boxed frames denote where the visual prompts are applied. Given different types of visual prompts on a single frame, our method can flexibly infer the relevant object, track it across the entire video, and involve its features in reasoning.",
                "position": 2106
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AModel",
        "images": []
    },
    {
        "header": "Appendix BExperiments",
        "images": []
    },
    {
        "header": "Appendix CDiscussion",
        "images": []
    },
    {
        "header": "Appendix DLicenses",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.18094/x6.png",
                "caption": "Figure 6:Visualization of the predictions from UniPixel on PixelQA.",
                "position": 3811
            },
            {
                "img": "https://arxiv.org/html/2509.18094/x7.png",
                "caption": "Figure 7:Visualization of the predictions from UniPixel on ReVOSvisa.",
                "position": 3814
            },
            {
                "img": "https://arxiv.org/html/2509.18094/x8.png",
                "caption": "Figure 8:Visualization of the predictions from UniPixel on Ref-DAVIS17davis2017.",
                "position": 3817
            },
            {
                "img": "https://arxiv.org/html/2509.18094/x9.png",
                "caption": "Figure 9:Visualization of the predictions from UniPixel on GroundMoRegroundmore.",
                "position": 3820
            },
            {
                "img": "https://arxiv.org/html/2509.18094/x10.png",
                "caption": "Figure 10:Visualization of the predictions from UniPixel on Ref-SAVsa2va.",
                "position": 3823
            },
            {
                "img": "https://arxiv.org/html/2509.18094/x11.png",
                "caption": "Figure 11:Visualization of the predictions from UniPixel on ReasonSeglisa.",
                "position": 3826
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]