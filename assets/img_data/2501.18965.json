[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18965/x1.png",
                "caption": "(a)Real Loss Curves",
                "position": 165
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x1.png",
                "caption": "(a)Real Loss Curves",
                "position": 168
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x2.png",
                "caption": "(b)Theoretical Bound",
                "position": 173
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Convergence Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18965/x3.png",
                "caption": "Figure 2:Learning-rate schedule(left)and theoretical bound(right)forcosineandwsd, and variousTùëáTitalic_T, with base learning-rateŒ≥‚ãÜsuperscriptùõæ‚ãÜ\\gamma^{\\star}italic_Œ≥ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT.",
                "position": 474
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x4.png",
                "caption": "(a)Learning-rate sweep",
                "position": 477
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x4.png",
                "caption": "(a)Learning-rate sweep",
                "position": 480
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x5.png",
                "caption": "(b)Optimal base learning-rate vs.TùëáTitalic_T",
                "position": 485
            }
        ]
    },
    {
        "header": "4Theoretical Simulations",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18965/x6.png",
                "caption": "(a)Learning-rate sweep",
                "position": 617
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x6.png",
                "caption": "(a)Learning-rate sweep",
                "position": 620
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x7.png",
                "caption": "(b)Final bound vs.¬†cooldown fraction",
                "position": 625
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x8.png",
                "caption": "Figure 5:Schedule(left)and theoretical convergence(right)for varying cooldown fraction. With optimal base learning-rateŒ≥‚ãÜsuperscriptùõæ‚ãÜ\\gamma^{\\star}italic_Œ≥ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT, starting the cooldown atT0=1subscriptùëá01T_{0}=1italic_T start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 1is optimal.Fig.21shows the analogous plot for real experiments with the same behavior.",
                "position": 632
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x9.png",
                "caption": "Figure 6:Assumed gradient shape(left)and theoretical converegnce(right). Only withŒ±=0ùõº0\\alpha=0italic_Œ± = 0(constantGtsubscriptùê∫ùë°G_{t}italic_G start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT), the sudden drop forwsdis clearly visible.",
                "position": 657
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x10.png",
                "caption": "(a)wsdwith cooldown fraction0.20.20.20.2",
                "position": 671
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x10.png",
                "caption": "(a)wsdwith cooldown fraction0.20.20.20.2",
                "position": 674
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x11.png",
                "caption": "(b)cosine",
                "position": 679
            }
        ]
    },
    {
        "header": "5Applications",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18965/x12.png",
                "caption": "Figure 8:Transfering the learning-rate schedule from horizonT1=4000subscriptùëá14000T_{1}=4000italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 4000toT2‚àà[1.5‚Å¢T1,4‚Å¢T1]subscriptùëá21.5subscriptùëá14subscriptùëá1T_{2}\\in[1.5T_{1},4T_{1}]italic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ‚àà [ 1.5 italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , 4 italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ](see alsoFig.22, left).\nDecreasing the learning rate(green)after the short run (at iteration3200320032003200) leads to significant better boundŒ©tsubscriptŒ©ùë°\\Omega_{t}roman_Œ© start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTas keeping it constant(grey).\nDashed horizontal lines(blue)mark bounds for linear-decay schedule with tunedŒ≥‚ãÜsuperscriptùõæ‚ãÜ\\gamma^{\\star}italic_Œ≥ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT.",
                "position": 733
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x13.png",
                "caption": "Figure 9:(Left)Transferring thewsdschedule from horizonT1=4000subscriptùëá14000T_{1}=4000italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 4000toT2‚àà[1.5‚Å¢T1,4‚Å¢T1]subscriptùëá21.5subscriptùëá14subscriptùëá1T_{2}\\in[1.5T_{1},4T_{1}]italic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ‚àà [ 1.5 italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , 4 italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ].(Right)Not adapting the cooldown length leads to significant suboptimality.\nDashed horizontal lines mark bound for the linear-decay schedule with tunedŒ≥‚ãÜsuperscriptùõæ‚ãÜ\\gamma^{\\star}italic_Œ≥ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT.",
                "position": 759
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x14.png",
                "caption": "(a)124124124124M model",
                "position": 764
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x14.png",
                "caption": "(a)124124124124M model",
                "position": 767
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x15.png",
                "caption": "(b)210210210210M model",
                "position": 772
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x16.png",
                "caption": "Figure 11:Transferring the optimal base learning-rate from cooldown fractioncùëêcitalic_cto linear-decay (c=1ùëê1c=1italic_c = 1): for linear cooldown(left)and1-sqrtcooldown(right). Dashed lines are fitted polynomial of degree6666.",
                "position": 808
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x16.png",
                "caption": "",
                "position": 811
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x17.png",
                "caption": "",
                "position": 815
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x18.png",
                "caption": "(a)Learning rate transfer",
                "position": 821
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x18.png",
                "caption": "(a)Learning rate transfer",
                "position": 824
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x19.png",
                "caption": "(b)Learning rate sweep",
                "position": 829
            }
        ]
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAblation: Min-Suboptimality Bounds",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18965/x20.png",
                "caption": "Figure 13:Same asFig.2, but withŒ©tsubscriptŒ©ùë°\\Omega_{t}roman_Œ© start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTfrom (11)",
                "position": 1539
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x21.png",
                "caption": "(a)Learning-rate sweep",
                "position": 1542
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x21.png",
                "caption": "(a)Learning-rate sweep",
                "position": 1545
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x22.png",
                "caption": "(b)Optimal base learning-rate vs.TùëáTitalic_T",
                "position": 1550
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x23.png",
                "caption": "(a)Learning-rate sweep",
                "position": 1565
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x23.png",
                "caption": "(a)Learning-rate sweep",
                "position": 1568
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x24.png",
                "caption": "(b)Final bound vs.¬†cooldown fraction",
                "position": 1573
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x25.png",
                "caption": "Figure 16:Same asFig.5, but withŒ©tsubscriptŒ©ùë°\\Omega_{t}roman_Œ© start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTfrom (11)",
                "position": 1580
            }
        ]
    },
    {
        "header": "Appendix BExperiments: Supplementary Material",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18965/x26.png",
                "caption": "Figure 17:(Left)Sudden drop of the loss forwsdschedule for a convex, non-smooth problem.(Right)Iterate path for the three schedules. Forwsd, the cooldown period is indicated with the dashed line. Star marks solution.",
                "position": 1598
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x26.png",
                "caption": "",
                "position": 1601
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x27.png",
                "caption": "",
                "position": 1605
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x28.png",
                "caption": "(a)Convergence",
                "position": 1657
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x28.png",
                "caption": "(a)Convergence",
                "position": 1660
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x29.png",
                "caption": "(b)Learning-rate sweep",
                "position": 1665
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x30.png",
                "caption": "Figure 19:Comparison of cycle lengths for thecosineschedule. Compare to Figure A1 inHoffmann et¬†al. (2022).",
                "position": 1689
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x31.png",
                "caption": "Figure 20:(Left)The benefit of cooldown is reflected in the absence of logarithmic terms. Dark grey marks the bound of the constant schedule.(Right)Plotting the individual terms of the boundŒ©t=ùíØ1/Œ≥+Œ≥‚Å¢ùíØ2subscriptŒ©ùë°subscriptùíØ1ùõæùõæsubscriptùíØ2\\Omega_{t}=\\mathcal{T}_{1}/\\gamma+\\gamma\\mathcal{T}_{2}roman_Œ© start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = caligraphic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT / italic_Œ≥ + italic_Œ≥ caligraphic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTwithŒ≥=Œ≥‚ãÜùõæsuperscriptùõæ‚ãÜ\\gamma=\\gamma^{\\star}italic_Œ≥ = italic_Œ≥ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPTfor thewsdschedule. The sudden drop of the bound comes from the termŒ≥‚Å¢ùíØ2ùõæsubscriptùíØ2\\gamma\\mathcal{T}_{2}italic_Œ≥ caligraphic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT.",
                "position": 1821
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x31.png",
                "caption": "",
                "position": 1824
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x32.png",
                "caption": "",
                "position": 1828
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x33.png",
                "caption": "Figure 21:(Left)Analogous toFig.5(right) with real training curves. We remove cooldown fraction0.60.60.60.6as its loss curve shows a spike and recovers only late.(Right)Analogous ofFig.4(b)with real training data that shows a parabola shape for fixed learning-rates.",
                "position": 1834
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x33.png",
                "caption": "",
                "position": 1837
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x34.png",
                "caption": "",
                "position": 1841
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x35.png",
                "caption": "Figure 22:(Left)Decreasing factor computed withCorollary2for extended schedule up toTùëáTitalic_T(whereT1subscriptùëá1T_{1}italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTis length of the short run). SeeSection5.1,(B2)for details.(Right): Extended schedule for the training runs inFig.10; note that this schedule is multiplied byŒ≥=0.001ùõæ0.001\\gamma=0.001italic_Œ≥ = 0.001.",
                "position": 1847
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x35.png",
                "caption": "",
                "position": 1850
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x36.png",
                "caption": "",
                "position": 1854
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x37.png",
                "caption": "Figure 23:(Left)Transferring the1/sqrtschedule with linear cooldown from horizonT1=4000subscriptùëá14000T_{1}=4000italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 4000toT2‚àà[1.5‚Å¢T1,4‚Å¢T1]subscriptùëá21.5subscriptùëá14subscriptùëá1T_{2}\\in[1.5T_{1},4T_{1}]italic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ‚àà [ 1.5 italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , 4 italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ].(Right)Adapting the cooldown length has only small benefits.\nDashed horizontal lines mark bound for linear-decay schedule with tunedŒ≥‚ãÜsuperscriptùõæ‚ãÜ\\gamma^{\\star}italic_Œ≥ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT. SeeSection5.1,(B1)for details.",
                "position": 1860
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x38.png",
                "caption": "Figure 24:Experiment with adapted cooldown length for124124124124M model(left)and210210210210M(right). SeeSection5.1,(B1)for details.",
                "position": 1865
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x38.png",
                "caption": "",
                "position": 1868
            },
            {
                "img": "https://arxiv.org/html/2501.18965/x39.png",
                "caption": "",
                "position": 1872
            }
        ]
    },
    {
        "header": "Appendix CAuxiliary Lemmas",
        "images": []
    },
    {
        "header": "Appendix DMissing Proofs",
        "images": []
    },
    {
        "header": "Appendix EMirror Descent Analysis",
        "images": []
    },
    {
        "header": "Appendix FAnalysis of Schedules",
        "images": []
    }
]