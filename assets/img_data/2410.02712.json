[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Data Collection",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02712/extracted/5898036/figures/example_data_train.jpg",
                "caption": "Table 1:An example of LLaVA-Critic training data. The top block showspointwise scoring, where LLaVA-Critic predicts a score to evaluate a single response’s quality; the bottom block illustratespairwise ranking, where it rank response pairs. In both settings, LLaVA-Critic learns to provide reasons for its judgments.",
                "position": 216
            },
            {
                "img": "https://arxiv.org/html/2410.02712/x1.png",
                "caption": "Figure 1:Data statistic of LLaVA-Critic-113k training dataset. In the pointwise setting, we categorize datasets by instruction sources and select data based on the task type corresponding to each evaluation prompt. Note that all our training data is sourced from public instruction-following training sets and does not overlap with with any evaluation benchmarks.",
                "position": 408
            }
        ]
    },
    {
        "header": "4LLaVA-Critic",
        "images": []
    },
    {
        "header": "5Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02712/x2.png",
                "caption": "Figure 2:(Top):Overall distribution of evaluation scores across 4 benchmarks.(Bottom):Calculated average evaluation score for each response model on each benchmark. Each color represents a different LMM evaluator. Leveraging high-quality critic training data, LLaVA-Critic closely aligns with GPT-4o in delivering balanced evaluation scores and accurately ranking response LMMs.",
                "position": 813
            }
        ]
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAuthor Contributions",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    },
    {
        "header": "Appendix CAdditional Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02712/x3.png",
                "caption": "Figure 3:Visualization on the performance gain of LLaVA-OneVision obtained through LLaVA-Critic across 6 open-ended multimodal benchmarks.",
                "position": 3175
            },
            {
                "img": "https://arxiv.org/html/2410.02712/extracted/5898036/figures/examples/example_critic_seven.png",
                "caption": "Table 10:Example of LLaVA-Critic in ranking LMM response pairs. While LLaVA-OV misinterprets the handwritten number and givesinaccurate judgment, LLaVA-Criticaligns with human assessmentsand offers clear, visually grounded justifications.",
                "position": 3211
            },
            {
                "img": "https://arxiv.org/html/2410.02712/extracted/5898036/figures/examples/example_critic_seven.png",
                "caption": "Table 10:Example of LLaVA-Critic in ranking LMM response pairs. While LLaVA-OV misinterprets the handwritten number and givesinaccurate judgment, LLaVA-Criticaligns with human assessmentsand offers clear, visually grounded justifications.",
                "position": 3212
            },
            {
                "img": "https://arxiv.org/html/2410.02712/extracted/5898036/figures/examples/critic_example_weather.png",
                "caption": "Table 11:Example of LLaVA-Critic conducting pointwise scoring on an LMM response for LLaVA-Wilder. Unlike LLaVA-OV, which offersvague and uninformative explanations, LLaVA-Critic delivers a thorough and fair assessment with clear justifications on both thestrengthsandweaknessesof the evaluated response, closely resembling the depth and fairness of GPT-4o’s assessments.",
                "position": 3307
            },
            {
                "img": "https://arxiv.org/html/2410.02712/extracted/5898036/figures/examples/critic_example_weather.png",
                "caption": "Table 11:Example of LLaVA-Critic conducting pointwise scoring on an LMM response for LLaVA-Wilder. Unlike LLaVA-OV, which offersvague and uninformative explanations, LLaVA-Critic delivers a thorough and fair assessment with clear justifications on both thestrengthsandweaknessesof the evaluated response, closely resembling the depth and fairness of GPT-4o’s assessments.",
                "position": 3308
            },
            {
                "img": "https://arxiv.org/html/2410.02712/extracted/5898036/figures/examples/example_chat_dog.jpg",
                "caption": "Table 12:Example of LLaVA-Critic in preference learning. Leveraging reward signals from LLaVA-Critic, LLaVA-OneVision improves its visual chat ability through iterative DPO, delivering more detailed, valuable, and structured point-by-point responses.",
                "position": 3416
            },
            {
                "img": "https://arxiv.org/html/2410.02712/extracted/5898036/figures/examples/example_chat_dog.jpg",
                "caption": "Table 12:Example of LLaVA-Critic in preference learning. Leveraging reward signals from LLaVA-Critic, LLaVA-OneVision improves its visual chat ability through iterative DPO, delivering more detailed, valuable, and structured point-by-point responses.",
                "position": 3417
            }
        ]
    },
    {
        "header": "Appendix DMore Qualitative Comparisons",
        "images": []
    }
]