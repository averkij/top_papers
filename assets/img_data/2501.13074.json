[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.13074/x1.png",
                "caption": "Figure 1:Comparison between traditional MoE and AoE.\nArrows indicate data flow, while shadowed modules represent unused parameters or variables.\n(a) Traditional MoE models use a router to assign tokens to specific experts.\nThis separation between the router‘s decision-making and the experts’ execution leads to suboptimal expert selection and ineffective learning.\n(b) In an AoE model, experts operate autonomously.\nThey are ranked based on their internal activation norms, and only the top-activated experts continue processing, while the others are terminated.\nThe AoE expert architecture is modified to maintain efficiency.",
                "position": 104
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background: Mixture-of-Experts (MoE)",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.13074/x2.png",
                "caption": "Figure 2:Pre-training NLL losses.\nAll configurations shown are trained withℒauxsubscriptℒaux\\mathcal{L}_{\\text{aux}}caligraphic_L start_POSTSUBSCRIPT aux end_POSTSUBSCRIPT, though its value is not included in the figure.",
                "position": 828
            },
            {
                "img": "https://arxiv.org/html/2501.13074/x3.png",
                "caption": "Figure 3:Statistical analysis of expert load.\nThe figure reveals several key insights: (1)ℒauxsubscriptℒaux\\mathcal{L}_{\\text{aux}}caligraphic_L start_POSTSUBSCRIPT aux end_POSTSUBSCRIPTenhances load balancing in both traditional MoE and AoE.\n(2) AoEs generally exhibit more balanced load distributions compared to their traditional MoE counterparts, as indicated by higherEntloadsubscriptEntload\\text{Ent}_{\\text{load}}Ent start_POSTSUBSCRIPT load end_POSTSUBSCRIPTvalues.\n(3) AoEs also demonstrate greater confidence in expert selection, reflected by lowerEntconfsubscriptEntconf\\text{Ent}_{\\text{conf}}Ent start_POSTSUBSCRIPT conf end_POSTSUBSCRIPTvalues.",
                "position": 833
            },
            {
                "img": "https://arxiv.org/html/2501.13074/x4.png",
                "caption": "Figure 4:Average activation norm dynamics during training.\nEach plot represents an expert, distinguished by color according to its layer.\nExperts within the same layer achieve similar activation scales, indicating that their self-evaluation criteria for determining whether they are capable of processing inputs are aligned.",
                "position": 986
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARe-running Experiments in Section3.1Using Alternative Expert-Selection Metrics",
        "images": []
    },
    {
        "header": "Appendix BPre-training Language Models Using Alternative Expert-Selection Strategy",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.13074/x5.png",
                "caption": "Figure 5:The overview of our toy experiments training tiny AoE and traditional MoE classifiers.",
                "position": 2063
            }
        ]
    },
    {
        "header": "Appendix CAdditional Interpretation of AoE’s Advantage",
        "images": []
    }
]