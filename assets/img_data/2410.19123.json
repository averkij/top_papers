[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.19123/x1.png",
                "caption": "Figure 1:Overview ofRead-ME. This figure shows the refactoring of a pre-trained dense model (inyellow) into two experts (inredandgreen). After refactoring, the model is deployed, and the serving timeline is depicted. At timet=0ğ‘¡0t=0italic_t = 0, multiple inference requests (each a sequence of tokens) are queued, with expert assignment for each token undecided (\"????\") until processed by the router. Our router pre-gates tokens before inference, enabling expert-aware batching. Tokens are routed to their respective experts and batched accordingly: att=0ğ‘¡0t=0italic_t = 0for Expert 1 (red) and att=1ğ‘¡1t=1italic_t = 1for Expert 2 (green). New tokens enter the queue at each time step, with routing computed only for incoming tokens marked \"????\".",
                "position": 116
            }
        ]
    },
    {
        "header": "2Pre-gating Sparse Mixture of Experts",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.19123/x2.png",
                "caption": "Figure 2:(a) Visualization of transition matrix between the (lğ‘™litalic_l-1111)-th layer and thelğ‘™litalic_l-th layer, where each coordinate[{s,t},{i,j}]ğ‘ ğ‘¡ğ‘–ğ‘—[\\{s,t\\},\\{i,j\\}][ { italic_s , italic_t } , { italic_i , italic_j } ]representsPâ¢(ğ’®(l)={i,j}|ğ’®(lâˆ’1)={s,t})ğ‘ƒsuperscriptğ’®ğ‘™conditionalğ‘–ğ‘—superscriptğ’®ğ‘™1ğ‘ ğ‘¡P(\\mathcal{S}^{(l)}=\\{i,j\\}|\\mathcal{S}^{(l-1)}=\\{s,t\\})italic_P ( caligraphic_S start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT = { italic_i , italic_j } | caligraphic_S start_POSTSUPERSCRIPT ( italic_l - 1 ) end_POSTSUPERSCRIPT = { italic_s , italic_t } ). The row-wise sparse pattern suggests that the router decision becomes almost deterministic given the previous layerâ€™s decision. (b) Mutual informationIâ¢(ğ’®(l);ğ’®(lâˆ’1))ğ¼superscriptğ’®ğ‘™superscriptğ’®ğ‘™1I(\\mathcal{S}^{(l)};\\mathcal{S}^{(l-1)})italic_I ( caligraphic_S start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT ; caligraphic_S start_POSTSUPERSCRIPT ( italic_l - 1 ) end_POSTSUPERSCRIPT ), which indicates the learned knowledge shared by two neighboring layers is high. (c) Overview figure of router tuning and router distillation loss.",
                "position": 194
            }
        ]
    },
    {
        "header": "3Re-factoring Language Model with Pre-Gating MoE",
        "images": []
    },
    {
        "header": "4Expert-aware Inference System",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.19123/x3.png",
                "caption": "Figure 3:Challenges of MoE serving in current serving systems andRead-MEâ€™s batching pipeline.",
                "position": 344
            },
            {
                "img": "https://arxiv.org/html/2410.19123/extracted/5950495/Styles/figures/batching_motivation.png",
                "caption": "",
                "position": 347
            },
            {
                "img": "https://arxiv.org/html/2410.19123/x4.png",
                "caption": "",
                "position": 348
            }
        ]
    },
    {
        "header": "5Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.19123/x5.png",
                "caption": "Figure 4:Evaluation ofRead-MEon MMLU[16]benchmark, compared to other open-source models and compression techniques ( performance numbers are collected from their respective papers.",
                "position": 732
            },
            {
                "img": "https://arxiv.org/html/2410.19123/x6.png",
                "caption": "Figure 5:Latency evaluation and Temporal locality analysis. (Left) Single inference latency measured on a 124 token generation task. (Center) Latency distribution measured on synthetic workload replaying Chatbot Arena Dataset[29](Â§5.1). (Right) Temporal distance measured on Arxiv dataset[20], and a subset of Redpajama[35].",
                "position": 743
            },
            {
                "img": "https://arxiv.org/html/2410.19123/x7.png",
                "caption": "",
                "position": 746
            },
            {
                "img": "https://arxiv.org/html/2410.19123/x8.png",
                "caption": "",
                "position": 747
            },
            {
                "img": "https://arxiv.org/html/2410.19123/x9.png",
                "caption": "Figure 6:Latency impact of prefetching: We measured end-to-end latency on a synthetic workload generated by replaying Chatbot Arena Dataset[29]. (Appendix5.1)",
                "position": 781
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusions and Limitations",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.19123/x10.png",
                "caption": "Figure 7:Visualization on training dynamics.",
                "position": 1716
            }
        ]
    },
    {
        "header": "Appendix AMore Experimental Results",
        "images": []
    }
]