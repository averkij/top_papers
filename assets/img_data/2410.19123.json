[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.19123/x1.png",
                "caption": "Figure 1:Overview ofRead-ME. This figure shows the refactoring of a pre-trained dense model (inyellow) into two experts (inredandgreen). After refactoring, the model is deployed, and the serving timeline is depicted. At timet=0𝑡0t=0italic_t = 0, multiple inference requests (each a sequence of tokens) are queued, with expert assignment for each token undecided (\"????\") until processed by the router. Our router pre-gates tokens before inference, enabling expert-aware batching. Tokens are routed to their respective experts and batched accordingly: att=0𝑡0t=0italic_t = 0for Expert 1 (red) and att=1𝑡1t=1italic_t = 1for Expert 2 (green). New tokens enter the queue at each time step, with routing computed only for incoming tokens marked \"????\".",
                "position": 116
            }
        ]
    },
    {
        "header": "2Pre-gating Sparse Mixture of Experts",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.19123/x2.png",
                "caption": "Figure 2:(a) Visualization of transition matrix between the (l𝑙litalic_l-1111)-th layer and thel𝑙litalic_l-th layer, where each coordinate[{s,t},{i,j}]𝑠𝑡𝑖𝑗[\\{s,t\\},\\{i,j\\}][ { italic_s , italic_t } , { italic_i , italic_j } ]representsP⁢(𝒮(l)={i,j}|𝒮(l−1)={s,t})𝑃superscript𝒮𝑙conditional𝑖𝑗superscript𝒮𝑙1𝑠𝑡P(\\mathcal{S}^{(l)}=\\{i,j\\}|\\mathcal{S}^{(l-1)}=\\{s,t\\})italic_P ( caligraphic_S start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT = { italic_i , italic_j } | caligraphic_S start_POSTSUPERSCRIPT ( italic_l - 1 ) end_POSTSUPERSCRIPT = { italic_s , italic_t } ). The row-wise sparse pattern suggests that the router decision becomes almost deterministic given the previous layer’s decision. (b) Mutual informationI⁢(𝒮(l);𝒮(l−1))𝐼superscript𝒮𝑙superscript𝒮𝑙1I(\\mathcal{S}^{(l)};\\mathcal{S}^{(l-1)})italic_I ( caligraphic_S start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT ; caligraphic_S start_POSTSUPERSCRIPT ( italic_l - 1 ) end_POSTSUPERSCRIPT ), which indicates the learned knowledge shared by two neighboring layers is high. (c) Overview figure of router tuning and router distillation loss.",
                "position": 194
            }
        ]
    },
    {
        "header": "3Re-factoring Language Model with Pre-Gating MoE",
        "images": []
    },
    {
        "header": "4Expert-aware Inference System",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.19123/x3.png",
                "caption": "Figure 3:Challenges of MoE serving in current serving systems andRead-ME’s batching pipeline.",
                "position": 344
            },
            {
                "img": "https://arxiv.org/html/2410.19123/extracted/5950495/Styles/figures/batching_motivation.png",
                "caption": "",
                "position": 347
            },
            {
                "img": "https://arxiv.org/html/2410.19123/x4.png",
                "caption": "",
                "position": 348
            }
        ]
    },
    {
        "header": "5Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.19123/x5.png",
                "caption": "Figure 4:Evaluation ofRead-MEon MMLU[16]benchmark, compared to other open-source models and compression techniques ( performance numbers are collected from their respective papers.",
                "position": 732
            },
            {
                "img": "https://arxiv.org/html/2410.19123/x6.png",
                "caption": "Figure 5:Latency evaluation and Temporal locality analysis. (Left) Single inference latency measured on a 124 token generation task. (Center) Latency distribution measured on synthetic workload replaying Chatbot Arena Dataset[29](§5.1). (Right) Temporal distance measured on Arxiv dataset[20], and a subset of Redpajama[35].",
                "position": 743
            },
            {
                "img": "https://arxiv.org/html/2410.19123/x7.png",
                "caption": "",
                "position": 746
            },
            {
                "img": "https://arxiv.org/html/2410.19123/x8.png",
                "caption": "",
                "position": 747
            },
            {
                "img": "https://arxiv.org/html/2410.19123/x9.png",
                "caption": "Figure 6:Latency impact of prefetching: We measured end-to-end latency on a synthetic workload generated by replaying Chatbot Arena Dataset[29]. (Appendix5.1)",
                "position": 781
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusions and Limitations",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.19123/x10.png",
                "caption": "Figure 7:Visualization on training dynamics.",
                "position": 1716
            }
        ]
    },
    {
        "header": "Appendix AMore Experimental Results",
        "images": []
    }
]