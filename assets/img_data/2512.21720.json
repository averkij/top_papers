[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21720/figures/figure_1.png",
                "caption": "Figure 1:Why compressors matter.Many agentic LM systems rely on compressors, and personal devices are growing powerful enough to host them.(Left)A compressor condenses a long inputXXinto a shorter summaryZZ, which a predictor ingests to extract the final answerYY.(Right)Consumer hardware can now run increasingly large open-weight LMs, shown for Google Pixel phones and Apple MacBook laptops under FP16 precision with memory estimates from Modal(modal2024vram). LM-Arena ranks indicate relative performance.",
                "position": 172
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21720/x1.png",
                "caption": "Figure 2:Downstream accuracy, compression length, and compute cost scale with compressor size (Top:LongHealth; Bottom:FinanceBench).We scale compressor model size and reports a different metric on theyy-axis of each column:(Left)accuracy, with the black dotted line showing the GPT-4o model baseline.(Middle)compression length,(Right)GFLOPs-per-compression. Vertical bars denote standard errors. Larger compressors produce shorter outputs with higher downstream accuracy. Similar trends hold onQASPER(AppendixE.1.1),Wildchat(AppendixE.1.2) andFineWeb(AppendixE.1.3).",
                "position": 404
            },
            {
                "img": "https://arxiv.org/html/2512.21720/x2.png",
                "caption": "Figure 3:Scaling compressors is more effective than scaling predictors onLongHealth.Theyy-axis reports accuracy andxx-axis shows total compute cost in FLOPs-per-generation (log-scale). We compare compressor LMs from two families:(Left)Qwen-2.5,(Right)Llama-3. We scale predictor (marker color) and compressor (marker label) sizes and measure the total FLOPs-per-generation and downstream accuracy on QA tasks. AppendixE.1.6shows consistent trends onFinanceBench.",
                "position": 420
            },
            {
                "img": "https://arxiv.org/html/2512.21720/x3.png",
                "caption": "Figure 4:Larger compressors generate outputs that carry more information about their inputs (conditioned on the query) onLongHealth.We scale compressor model size and estimate the(Left)mutual information, and(Right)bit efficiency (bits of mutual information per token; higher is better) carried by their outputs.\nLarger compressor model sizes compress documents with higher mutual information and bit efficiency. The black dotted line represents the theoretical maximum of the mutual information estimator at the natural logarithmlog⁡(N)\\log(N), whereNNis the number of documents mutual information is computed across. We find consistent trends onFinanceBench(AppendixE.1.6) andQASPER(AppendixE.1.1).",
                "position": 465
            },
            {
                "img": "https://arxiv.org/html/2512.21720/x4.png",
                "caption": "Figure 5:Scaling behavior of compressor model size hold across instructed conciseness (Compressor = Qwen-2.5).We ablate over different levels of compression conciseness by varying the compression prompt instructions. We measure(Left)accuracy,(Middle)GFLOPs-per-generation, and estimate(Right)mutual information.\nWe find that accuracy and mutual information are largely unaffected by conciseness instructions. Compressors instructed to be more concise are more token-efficient, and thus compute-efficient. Trends in accuracy, compute cost, and mutual information as we scale compressor hold across conciseness constraints. AppendixE.1.7shows analogous results onFinanceBench.",
                "position": 475
            },
            {
                "img": "https://arxiv.org/html/2512.21720/x5.png",
                "caption": "Figure 6:Mutual information and bit efficiency correlate strongly with downstream performance.(Left)We vary both predictor and compressor model in the compression-prediction workflow and measure the distortion on theyy-axis and estimate the rate on thexx-axis.\nWe plot the resulting rate-distortion curves across predictor sizes 1B, 8B, 70B, and 405B forLlamacompressors onLongHealth. The colored lines show fitted exponential-decay functions.(Right)We measure perplexity and mutual information on compressions generated byLlamacompressors onFineWeb. The black line shows a fitted linear function (r=−0.84r=-0.84,R2=0.71R^{2}=0.71). AppendixE.2provides further rate-distortion analyses.",
                "position": 493
            },
            {
                "img": "https://arxiv.org/html/2512.21720/x6.png",
                "caption": "Figure 7:Deep Research Scaling Results. (Left)RACE score versus average task cost when usingGPT-4oas a predictor withQwen-2.5compressors of varying sizes. Costs are based onGPT-4oAPI rates (August 2025: $2.50/1M input tokens, $10.00/1M output tokens). Larger compressors improve performance with minimal cost increases. For reference, we includeGPT-4oresults without compression and also for theGPT-4o-search-previewmodel.(Right)RACE scores for differentQwen-2.5compressor sizes (0.5B–14B) under threeLlamapredictors (8B, 70B, 405B).",
                "position": 529
            }
        ]
    },
    {
        "header": "4Discussion",
        "images": []
    },
    {
        "header": "Appendix AExtended Related Work",
        "images": []
    },
    {
        "header": "Appendix BExtended Description of Methods",
        "images": []
    },
    {
        "header": "Appendix CPrompts",
        "images": []
    },
    {
        "header": "Appendix DExtended Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21720/figures/figure_8.png",
                "caption": "Figure 8:Deep Research workflow.A predictor LM decomposes a Deep Research task into(Query, Subtask)pairs, where each pair specifies a targeted web search and an associated analysis instruction. Compressor LMs work in parallel to retrieve evidence, process it according to the subtask, and compress the findings into concise summaries, which the predictor then aggregates into a final report.",
                "position": 1358
            },
            {
                "img": "https://arxiv.org/html/2512.21720/x7.png",
                "caption": "Figure 9:Scaling behavior holds for reasoning and mixture-of-experts compressor models onQASPER.We scale compressor model size and reports a different metric of the compression step on theyy-axis of each column:(Left)accuracy,(Middle)compression length,(Right)mutual information. Mutual information is estimated using the log probabilities of a proxy model forQwen-2.5andLlamacompressors, and using internal log probabilities forQwen-3compressors.\nLarger compressors produce shorter outputs with higher downstream accuracy and higher mutual information.",
                "position": 1536
            },
            {
                "img": "https://arxiv.org/html/2512.21720/x8.png",
                "caption": "Figure 10:Perplexity, compression length, and compute cost scale with compressor size (WildChat).We scale compressor model size and report a different metric on theyy-axis of each column:(Left)Perplexity, with the black dashed line showing baseline perplexity given all 10 full chat conversations,(Middle)compression length of each set of chat conversations,(Right)GFLOPs-per-compression.\nLarger compressors produce shorter outputs with lower perplexity.",
                "position": 1561
            },
            {
                "img": "https://arxiv.org/html/2512.21720/x9.png",
                "caption": "Figure 11:Perplexity, compression length, and compute cost scale with compressor size (FineWeb; Top: Extractive Tasks; Bottom: Creative Tasks).We scale compressor model size and report a different metric on theyy-axis of each column:(Left)perplexity, with the black dashed line showing baseline perplexity given the full context,(Middle)compression length,(Right)GFLOPs-per-compression.\nLarger compressors produce shorter outputs with lower perplexity.",
                "position": 1600
            },
            {
                "img": "https://arxiv.org/html/2512.21720/x10.png",
                "caption": "Figure 12:Perplexity, compression length, and compute cost scale with compressor size for query-agnostic compressions (FineWeb).We scale compressor model size on different types of tasks (extractive and creative) and report a different metric on theyy-axis of each subplot:(Top Left)perplexity evaluated on extractive tasks,(Top Right)perplexity evaluated on creative tasks,(Bottom)compression length.\nLarger compressors produce query-agnostic compressions with lower perplexity across both types of tasks.",
                "position": 1604
            },
            {
                "img": "https://arxiv.org/html/2512.21720/x11.png",
                "caption": "Figure 13:Monte Carlo mutual information estimator is robust towards choice of proxy model.Theyy-axis shows the mutual information estimate onLongHealthcompressions produced by(Left)Llamaand(Right)Qwen-2.5compressor models.\nThe choice of proxy model introduces a fixed vertical offset in MI estimate, but does not affect compressor scaling behavior.",
                "position": 1619
            },
            {
                "img": "https://arxiv.org/html/2512.21720/x12.png",
                "caption": "Figure 14:Compressor scaling behavior holds for reasoning and mixture-of-experts models.We scale compressor model size and reports a different metric of the compression step on theyy-axis of each column:(Left)accuracy,(Middle)compression length,(Right)mutual information. Mutual information is estimated using the log probabilities of a proxy model for non-reasoningQwen-2.5compressors and using internal log probabilities for dense and MoE reasoningQwen-3compressors.\nScaling trends are consistent with our observations for non-reasoning dense models (blue), where larger compressors yield higher accuracy with shorter compressions and higher mutual information.\nAt the same scale (3B), the mixture-of-experts model (green) outperforms the dense models in downstream accuracy, compression conciseness, and mutual information.",
                "position": 1630
            },
            {
                "img": "https://arxiv.org/html/2512.21720/x13.png",
                "caption": "Figure 15:Larger compressors generate outputs that carry more information about their inputs (conditioned on the query) onFinanceBench.We scale compressor model size and estimate the(Left)mutual information, and(Right)bit efficiency (bits of mutual information per token; higher is better) carried by their outputs.\nLarger compressor model sizes compress documents with higher mutual information and bit efficiency.",
                "position": 1644
            },
            {
                "img": "https://arxiv.org/html/2512.21720/x14.png",
                "caption": "Figure 16:Scaling behavior of compressor model size hold across instructed conciseness (Compressor = Llama).We ablate over different levels of compression conciseness by varying the compression prompt instructions. We measure(Left)accuracy,(Middle)GFLOPs-per-generation, and estimate(Right)mutual information.\nWe find that accuracy and mutual information are largely unaffected by conciseness instructions. Compressors instructed to be more concise are more token-efficient, and thus compute-efficient. Trends in accuracy, compute cost, and mutual information as we scale compressor hold across conciseness constraints.",
                "position": 1658
            },
            {
                "img": "https://arxiv.org/html/2512.21720/x15.png",
                "caption": "Figure 17:Generalized Linear Model (GLM) Coefficients.We conduct regression analysis on a GLM predicting QA correctness (0/1) on(Blue)LongHealth,(Red)FinanceBench. The y-axis shows coefficient estimates for each variable, horizontal bars are 95% confidence intervals, asterisks mark variables that are significant atp<0.05p<0.05on both datasets. For more details on our GLM setup, refer to AppendixD.4.",
                "position": 1663
            },
            {
                "img": "https://arxiv.org/html/2512.21720/x16.png",
                "caption": "Figure 18:Mutual information in multi-turn workflows onLongHealth.We allow aLlama-3.1-405Bpredictor to repeatedly query aLlama-3.2-3Bcompressor for additional information. Increasing the compression-prediction workflow to two turns yields an increase in mutual information, but we observe no additional improvement when extending to a third turn.",
                "position": 1680
            },
            {
                "img": "https://arxiv.org/html/2512.21720/x17.png",
                "caption": "Figure 19:Exploring the trade-off between compression and fidelity loss: rate-distortion curve.We vary predictor and compressor model in the compression-prediction workflow and measure the distortion on theyy-axis and estimate the rate on thexx-axis.(Left)We examine a single compressor-predictor LM pairing,compressor=Llama-3andpredictor=Llama-3.3-70B.(Right)We compare different compressor-predictor LM pairings, where the predictor model isQwen-2.5-72B(”Qwen-2.5”) orLlama-3.3-70B(”Llama”).\nMarkers indicate compressor sizes (1B, 3B, 8B) in theLlama-3compressor model family.",
                "position": 1694
            },
            {
                "img": "https://arxiv.org/html/2512.21720/x18.png",
                "caption": "Figure 20:Exploring the trade-off between compression and fidelity loss: rate-distortion curve.We vary both predictor and compressor model in the compression-prediction workflow and measure the distortion on theyy-axis and estimate the rate on thexx-axis.\nWe plot the resulting rate-distortion curves across predictor sizes 1B, 8B, 70B, and 405B for(Left)Qwen-2.5and(Right)Llamacompressors. We evaluate the rate-distortion curves for two datasets:(Top)LongHealth,(Bottom)FinanceBench.",
                "position": 1708
            },
            {
                "img": "https://arxiv.org/html/2512.21720/x19.png",
                "caption": "Figure 21:Exploring the relationship between compression and accuracy.The y-axis depicts accuracy and the x-axis shows bit efficiency.Bit efficiencyis defined as the bits of mutual information encoded in each compression token. Markers indicate compressor sizes in theQwen-2.5(1.5B, 3B, 7B) andLlama(1B, 3B, 8B) compressor model family; vertical and horizontal bars denote standard errors.",
                "position": 1714
            },
            {
                "img": "https://arxiv.org/html/2512.21720/x20.png",
                "caption": "Figure 22:QA Accuracy versus total compute cost onFinanceBench.In each panel, the y-axis shows the accuracy and the x-axis plots total compute cost in FLOPs-per-generation on a log-scale for(Left)Qwen-2.5,(Right)Llama-3compressor LMs. Markers indicate compressor sizes in theQwen-2.5(1.5B, 3B, 7B) andLlama-3(1B, 3B, 8B) compressor model family; vertical and horizontal bars denote standard errors.",
                "position": 1717
            },
            {
                "img": "https://arxiv.org/html/2512.21720/x21.png",
                "caption": "Figure 23:Exploring the trade-off between compression and fidelity loss: alternative definition of distortion (LongHealth).We vary both predictor and compressor model in the compression-prediction workflow and measure the distortion on theyy-axis as 1–cosine difference between the semantic embedding of the prediction and target answer. We estimate the rate on thexx-axis.\nWe plot the resulting rate-distortion curves across predictor sizes 8B, 70B, and 405B for(Left)Qwen-2.5and(Right)Llamacompressors.",
                "position": 1731
            },
            {
                "img": "https://arxiv.org/html/2512.21720/x22.png",
                "caption": "Figure 24:Compressor model size versus compute and token usage.In each panel, the x-axis shows the Qwen compressor size (in billions of parameters).(Left)Total GFLOPs per task grows with compressor model size, with larger predictors (Llama 405B, 70B, 8B) amplifying compute cost.(Right)Compressor output tokens per task, which remain relatively stable across predictors (GPT-4o, Llama 405B, 70B, 8B), increase moderately with larger compressors.",
                "position": 1748
            }
        ]
    },
    {
        "header": "Appendix EExtended Results",
        "images": []
    }
]