[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15270/extracted/6462024/figs/visualization.png",
                "caption": "(a)",
                "position": 216
            },
            {
                "img": "https://arxiv.org/html/2505.15270/extracted/6462024/figs/visualization.png",
                "caption": "(a)",
                "position": 219
            },
            {
                "img": "https://arxiv.org/html/2505.15270/x1.png",
                "caption": "(b)",
                "position": 225
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Scaling Diffusion Transformers byŒºùúá\\muitalic_ŒºP",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15270/x2.png",
                "caption": "(a)",
                "position": 333
            },
            {
                "img": "https://arxiv.org/html/2505.15270/x2.png",
                "caption": "(a)",
                "position": 336
            },
            {
                "img": "https://arxiv.org/html/2505.15270/x3.png",
                "caption": "(b)",
                "position": 342
            }
        ]
    },
    {
        "header": "4Systematic Investigation for DiT-Œºùúá\\muitalic_ŒºP on ImageNet",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15270/extracted/6462024/figs/fid_width_lr_search_dit.png",
                "caption": "(a)",
                "position": 421
            },
            {
                "img": "https://arxiv.org/html/2505.15270/extracted/6462024/figs/fid_width_lr_search_dit.png",
                "caption": "(a)",
                "position": 424
            },
            {
                "img": "https://arxiv.org/html/2505.15270/extracted/6462024/figs/fid_bs_lr_search_dit.png",
                "caption": "(b)",
                "position": 430
            },
            {
                "img": "https://arxiv.org/html/2505.15270/extracted/6462024/figs/fid_epoch_lr_search_dit.png",
                "caption": "(c)",
                "position": 436
            },
            {
                "img": "https://arxiv.org/html/2505.15270/extracted/6462024/figs/fid_pretraining_dit.png",
                "caption": "Figure 4:Œºùúá\\muitalic_ŒºP accerlates the training of diffusion Transformers.Considering FID-50K, DiT-XL-2-Œºùúá\\muitalic_ŒºP with transferred learning rate achieves 2.9√ó\\times√ófaster convergence than the original DiT-XL-2 and a slightly better result.",
                "position": 450
            }
        ]
    },
    {
        "header": "5Large-Scale Text-to-Image Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15270/extracted/6462024/figs/loss_lr_search_mmdit.png",
                "caption": "(a)",
                "position": 685
            },
            {
                "img": "https://arxiv.org/html/2505.15270/extracted/6462024/figs/loss_lr_search_mmdit.png",
                "caption": "(a)",
                "position": 688
            },
            {
                "img": "https://arxiv.org/html/2505.15270/extracted/6462024/figs/loss_maxnorm_search_mmdit.png",
                "caption": "(b)",
                "position": 694
            },
            {
                "img": "https://arxiv.org/html/2505.15270/extracted/6462024/figs/loss_repa_search_mmdit.png",
                "caption": "(c)",
                "position": 700
            },
            {
                "img": "https://arxiv.org/html/2505.15270/extracted/6462024/figs/loss_warmup_search_mmdit.png",
                "caption": "(d)",
                "position": 706
            },
            {
                "img": "https://arxiv.org/html/2505.15270/extracted/6462024/figs/mmdit_loss.png",
                "caption": "Figure 6:MMDiT-Œºùúá\\muitalic_ŒºP-18B achieves consistently lower training lossthan baseline after 50K steps.",
                "position": 723
            },
            {
                "img": "https://arxiv.org/html/2505.15270/extracted/6462024/figs/mmdit_loss.png",
                "caption": "Figure 6:MMDiT-Œºùúá\\muitalic_ŒºP-18B achieves consistently lower training lossthan baseline after 50K steps.",
                "position": 726
            }
        ]
    },
    {
        "header": "6Conclusion and Discussion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Related Work",
        "images": []
    },
    {
        "header": "Appendix BTheoretical Background ofŒºùúá\\muitalic_ŒºP",
        "images": []
    },
    {
        "header": "Appendix CProof of Theorem3.1",
        "images": []
    },
    {
        "header": "Appendix DAdditional Details for Section3",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15270/extracted/6462024/figs/fid_width_out_search_dit.png",
                "caption": "(a)",
                "position": 3550
            },
            {
                "img": "https://arxiv.org/html/2505.15270/extracted/6462024/figs/fid_width_out_search_dit.png",
                "caption": "(a)",
                "position": 3553
            },
            {
                "img": "https://arxiv.org/html/2505.15270/extracted/6462024/figs/fid_epoch_out_search_dit.png",
                "caption": "(b)",
                "position": 3559
            },
            {
                "img": "https://arxiv.org/html/2505.15270/extracted/6462024/figs/mmdit_loss_full.png",
                "caption": "Figure 8:Comparision between training loss.We present the training loss of MMDiT-Œºùúá\\muitalic_ŒºP-18B minus that of MMDiT-18B. The training loss gap less than 0 means that MMDiT-Œºùúá\\muitalic_ŒºP-18B is better. MMDiT-Œºùúá\\muitalic_ŒºP-18B achieves consistently lower training loss than the baseline after 15K steps, and the advantage is gradually expanding.",
                "position": 4474
            },
            {
                "img": "https://arxiv.org/html/2505.15270/x4.png",
                "caption": "Figure 9:Examples of text-image alignment test evaluated by human.Ten human experts conducted the evaluation, with each prompt containing an average of five test points and generating three images, resulting in 22,500 alignment test points. The final score is computed as the average correctness across all test points.",
                "position": 4484
            },
            {
                "img": "https://arxiv.org/html/2505.15270/x5.png",
                "caption": "Figure 10:Visual comparison between MMDiT-Œºùúá\\muitalic_ŒºP-18B and MMDiT-18B baseline.The configurations for generating images (e.g., random seed, cfg value) of two models are the same. MMDiT-Œºùúá\\muitalic_ŒºP-18B shows better text-image alignment than the baseline model.",
                "position": 4494
            }
        ]
    },
    {
        "header": "Appendix EAdditional Experimental Details and Results",
        "images": []
    }
]