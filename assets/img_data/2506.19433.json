[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19433/x1.png",
                "caption": "Figure 1:Long-instruction navigation in urban environments demands that agents retain both fine-grained spatial detail and high-level landmark semantics over many steps—a core challenge that leads to information loss or retrieval overload. Mem4Nav meets this by building a hierarchical spatial-cognition long-short memory system.",
                "position": 112
            },
            {
                "img": "https://arxiv.org/html/2506.19433/x2.png",
                "caption": "Figure 2:Contributions of Mem4Nav: Prior VLN systems treat spatial maps and memory as separate, using flat, monolithic maps that are either too detailed (noisy, slow to query) or too coarse (lossy), and simple text-based memory that merely appends raw history to instructions, leading to clutter and forgetting. Mem4Nav jointly implements a hierarchical spatial representation and a dual long–short memory mechanism, and can be seamlessly integrated into existing vision-and-language navigation pipelines to boost performance.",
                "position": 121
            }
        ]
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19433/x3.png",
                "caption": "Figure 3:Detailed Mem4Nav pipeline:In Reversible Token Processing, each token is mapped to a spatial element—either a voxel in the Sparse Octree or a node in the Semantic Topological Graph—and its read/write tokens are updated by a reversible Transformer block. Concurrently, recent observations are inserted into the STM Cache attached to the current graph node. During Planning and Execution, the agent queries STM for local context and performs nearest-neighbor search over all LTM tokens in the octree and graph to retrieve deep history, then fuses the retrieved vectors with current perception in its policy head to drive route and motion planning.",
                "position": 246
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": []
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19433/extracted/6566468/fig/VELMA.png",
                "caption": "Figure 4:Overview of VELMA",
                "position": 2215
            },
            {
                "img": "https://arxiv.org/html/2506.19433/extracted/6566468/fig/FLAME.png",
                "caption": "Figure 5:Overview of FLAME",
                "position": 2338
            },
            {
                "img": "https://arxiv.org/html/2506.19433/x4.png",
                "caption": "Figure 6:Representative Failure Cases of Mem4Nav.Despite substantial overall gains, we identify four dominant error modes: (a)Depth-induced mapping errorsarise when monocular depth estimates misplace voxels on low-texture façades, corrupting both octree writes and lookups; (b)Memory retrieval misses for far-away referencesoccur because distant landmarks lie outside instantiated spatial bins and yield no sufficiently similar tokens; (c)Semantic graph sparsity or ambiguityresults when subtle or partially occluded landmarks (e.g. crosswalk markings) fail node creation, breaking planned routes; and (d)Memory retrieval misses under severe occlusionhappen when landmarks hidden by overhead structures cannot be matched by either STM filtering or LTM HNSW search.",
                "position": 2629
            },
            {
                "img": "https://arxiv.org/html/2506.19433/x5.png",
                "caption": "Figure 7:Route of a campus real world trial of MemNav.",
                "position": 2780
            },
            {
                "img": "https://arxiv.org/html/2506.19433/extracted/6566468/fig/real_failure.png",
                "caption": "Figure 8:Failure Cases. (a) The agent came to a halt at a busy uncontrolled intersection, where the substantial volume of vehicular and pedestrian traffic rendered it incapable of determining an opportune moment to proceed. (b) The lights from high-velocity oncoming vehicles compromised the agent’s semantic information processing capabilities, requiring experimenter assistance for safe roadside repositioning.",
                "position": 2789
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]