[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05152/x1.png",
                "caption": "Figure 1:One can manipulate rankings to put any model in the lead by varying the single delimiter character.On the left, we show the delimiter used to separate examples in common evals with few-shot examples such as\\mmlu. On the right, we show model rankings based on\\mmluperformance as the example delimiter varies with each column corresponding to a different ranking.",
                "position": 167
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Method: A Common evaluation protocol reflecting real world usage of LLMs",
        "images": []
    },
    {
        "header": "4Experiments: Changing a single delimiter character can dramatically change performance on leading benchmarks",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05152/x2.png",
                "caption": "Figure 2:Changing a single delimiter character can dramatically change performance across model families.We show model performance across Llama, Qwen, and Gemma families on\\mmlu,arc-challenge, and commonsense-qaas we vary only the example delimiter (shown above each bar inblue).",
                "position": 283
            },
            {
                "img": "https://arxiv.org/html/2510.05152/x3.png",
                "caption": "",
                "position": 286
            },
            {
                "img": "https://arxiv.org/html/2510.05152/x4.png",
                "caption": "",
                "position": 287
            },
            {
                "img": "https://arxiv.org/html/2510.05152/x5.png",
                "caption": "Figure 3:The choice of delimiter affects performance across a range of topics.We show the accuracy by topic for\\mmluacross three model families. The choice of delimiter (shown above each bar inblue) affects performance across a range of topics across the three model families.",
                "position": 297
            },
            {
                "img": "https://arxiv.org/html/2510.05152/x5.png",
                "caption": "",
                "position": 300
            },
            {
                "img": "https://arxiv.org/html/2510.05152/x6.png",
                "caption": "",
                "position": 304
            },
            {
                "img": "https://arxiv.org/html/2510.05152/x7.png",
                "caption": "",
                "position": 308
            },
            {
                "img": "https://arxiv.org/html/2510.05152/x8.png",
                "caption": "Figure 4:Larger models are just as brittle to the change in delimiter.We compare the performance of Llama-3.1-instruct across two sizes 8B and 70B as the delimiter varies (shown above each bar inblue). We find model scale despite improving overall performance across all three benchmarks, the larger Llama model is just as susceptible to the choice of delimiter, with a fluctuation on commonsense-qaof 40% (an even larger change compared to the smaller model).",
                "position": 327
            },
            {
                "img": "https://arxiv.org/html/2510.05152/x9.png",
                "caption": "",
                "position": 330
            },
            {
                "img": "https://arxiv.org/html/2510.05152/x10.png",
                "caption": "",
                "position": 331
            },
            {
                "img": "https://arxiv.org/html/2510.05152/x11.png",
                "caption": "Figure 5:The effect of delimiter (“[space]” or “\\n”) on Llama-3.1-8B-instruct and Qwen2.5-7B-instruct in-context learning performance.Delimiter dramatically changes the in-context learning performance regardless of model or the number of demonstrations.",
                "position": 373
            }
        ]
    },
    {
        "header": "5Improving LLMs’ robustness to the choice of delimiter",
        "images": []
    },
    {
        "header": "6Understanding how delimiters steer attention to key tokens",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05152/x12.png",
                "caption": "Figure 6:Attention scores are steered towards the correct lookup key with the “\\n”delimiter. We measure a25%25\\%statistically significant increase in the attention scores for the dictionary lookup task as we vary the delimiter. On the right panel, we show Llama-3.1-8B-instruct performance on the dictionary lookup task as we vary the choice of delimiter.",
                "position": 523
            }
        ]
    },
    {
        "header": "7Conclusions",
        "images": []
    },
    {
        "header": "Reproducibility",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix A\\mmluBenchmark SoTA Evolution across Years",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05152/figures/paperswithcode_archive.png",
                "caption": "Figure 7:Evolution of\\mmlustate-of-the-art performance.",
                "position": 1292
            }
        ]
    },
    {
        "header": "Appendix BThe set of delimiters",
        "images": []
    },
    {
        "header": "Appendix CAdditional Evaluations",
        "images": []
    },
    {
        "header": "Appendix DSFT use varying delimiters failed to boost performance",
        "images": []
    },
    {
        "header": "Appendix EDelimiters are not consistently best per model across benchmarks",
        "images": []
    },
    {
        "header": "Appendix FEvaluation on closed-source model",
        "images": []
    },
    {
        "header": "Appendix GMeasuring attention scores for the dictionary lookup task",
        "images": []
    },
    {
        "header": "Appendix HReproducibility on lm-eval-harness(Gao et al.,2024)",
        "images": []
    }
]