[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13144/x1.png",
                "caption": "Figure 1:Different training paradigms of end-to-end autonomous driving (AD).",
                "position": 85
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13144/x2.png",
                "caption": "Figure 2:Overall framework of RAD.RAD takes a three-stage training paradigm.\nIn the perception pre-training, ground-truths of map and agent are used to guide instance-level tokens to encode corresponding information. In the planning pre-training stage, large-scale driving demonstrations are used to initialize the action distribution. In the reinforced post-training stage, RL and IL synergistically fine-tune the AD policy.",
                "position": 169
            }
        ]
    },
    {
        "header": "3RAD",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13144/x3.png",
                "caption": "Figure 3:Post-training.NùëÅNitalic_Nworkers parallelly run. The generated rollout data(st,at,rt+1,st+1,‚Ä¶)subscriptùë†ùë°subscriptùëéùë°subscriptùëüùë°1subscriptùë†ùë°1‚Ä¶(s_{t},a_{t},r_{t+1},s_{t+1},...)( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT , ‚Ä¶ )are recorded in a rollout buffer. Rollout data and human driving demonstrations are used in RL- and IL-training steps to fine-tune the AD policy synergistically.",
                "position": 211
            },
            {
                "img": "https://arxiv.org/html/2502.13144/x4.png",
                "caption": "Figure 4:Example diagram of four types of reward sources.(1): Collision with a dynamic obstacle ahead triggers a rewardrdcsubscriptùëüdcr_{\\text{dc}}italic_r start_POSTSUBSCRIPT dc end_POSTSUBSCRIPT. (2): Hitting a static roadside obstacle incurs a rewardrscsubscriptùëüscr_{\\text{sc}}italic_r start_POSTSUBSCRIPT sc end_POSTSUBSCRIPT. (3): Moving onto the curb exceeds the positional deviation thresholddmaxsubscriptùëëmaxd_{\\text{max}}italic_d start_POSTSUBSCRIPT max end_POSTSUBSCRIPT, triggering a rewardrpdsubscriptùëüpdr_{\\text{pd}}italic_r start_POSTSUBSCRIPT pd end_POSTSUBSCRIPT. (4): Drifting toward the adjacent lane exceeds the heading deviation thresholdœàmaxsubscriptùúìmax\\psi_{\\text{max}}italic_œà start_POSTSUBSCRIPT max end_POSTSUBSCRIPT, triggering a rewardrhdsubscriptùëühdr_{\\text{hd}}italic_r start_POSTSUBSCRIPT hd end_POSTSUBSCRIPT.",
                "position": 360
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13144/x5.png",
                "caption": "Figure 5:Closed-loop qualitative comparisons between the IL-only policy and RAD in 3DGS environments.Rows 1-2 correspond to Yield to Pedestrians. Rows 3-4 correspond to Unprotected Left-turn.",
                "position": 1237
            }
        ]
    },
    {
        "header": "5Limitation and Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13144/x6.png",
                "caption": "Figure A1:More Qualitative Results.Comparison between the IL-only policy and RAD in various driving scenarios: Detour (Rows 1-2), Crawl in Dense Traffic (Rows 3-4), Traffic Congestion (Rows 5-6), and U-turn(Rows 7-8).",
                "position": 2287
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]