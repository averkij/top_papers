[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19314/x1.png",
                "caption": "Figure 1:Comparison between existing PRMs andPRInTS.Top: Existing PRMs are limited for long-horizon information-seeking as they evaluate a short reasoning unit (e.g., one-to-two-sentence inferences) with coarse feedback, which cannot capture multi-faceted quality factors from tool interactions. They also struggle with rapidly accumulating reasoning context (left).Bottom: In contrast,PRInTSevaluates a complete trajectory step (reasoning + tool interactions), considers multiple trajectory step quality dimensions to produce dense scores for finer-grained guidance at each step, and maintains compact trajectory summaries that keep key information for the evaluation.",
                "position": 140
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19314/x2.png",
                "caption": "Figure 2:Overview ofPRInTS. Left:PRInTSfunctions as a scorer, evaluating agent’s multiple candidate next trajectory steps based on the summarized context and current tool response. It generates an analysis and a dense score for each candidate, selecting the top-scoring one to guide the agent’s information-seeking.Right:PRInTSacts as a summarizer, recursively updating a compact information-seeking trajectory summary to keep input length bounded and preserve key information for its subsequent score evaluation.",
                "position": 218
            }
        ]
    },
    {
        "header": "3PRInTS: Progress Reward via Information Gain Score and Trajectory Summarization",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19314/x3.png",
                "caption": "Figure 3:PRInTS: data annotation and training pipeline.Top: For each trajectory step, we estimate the information gain score via Monte Carlo rollouts as the change in mean answer accuracy before and after the step. Then we construct winning-losing step pairs based on these scores (left). Preference pair examples are shown inFigure˜7. Then we trainPRInTSas a scorer via GRPO on these pairs (right). The final reward combines a score reward for accurate prediction, a comparison reward for pairwise preference learning, and an adaptive weight to mitigate noisy annotations.Bottom: Each step is annotated with a compact, recursively updated trajectory summary capturing essential findings and plans up to the step (left). The same PRM is jointly trained as a summarizer via SFT on this summary data (right).",
                "position": 261
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19314/x4.png",
                "caption": "Figure 4:Scaling test-time compute.Best-of-nntest-time scaling results on GAIA Level 2 using Qwen3-32B.PRInTSbenefits from additional test-time compute by identifying higher-quality steps fromnncandidates.",
                "position": 918
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Appendix ADetails of Experimental Setups",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19314/x5.png",
                "caption": "Figure 5:Distribution of annotated information gain scores.",
                "position": 1034
            },
            {
                "img": "https://arxiv.org/html/2511.19314/x6.png",
                "caption": "Figure 6:Dataset scaling.Experiments on the impact of dataset scaling on GAIA Level 2 using Qwen3-32B. TrainingPRInTSshows strong sample efficiency, achieving performance gain using only 50%(∼\\sim1k samples) of our annotation data.",
                "position": 1116
            },
            {
                "img": "https://arxiv.org/html/2511.19314/x7.png",
                "caption": "Figure 7:Examples of preference pairs constructed by our annotation pipeline.",
                "position": 1128
            },
            {
                "img": "https://arxiv.org/html/2511.19314/x8.png",
                "caption": "Figure 8:Input prompt forPRInTSwhen the model is trained with GRPO for scoring ability and acts as a scorer at test-time.",
                "position": 1132
            },
            {
                "img": "https://arxiv.org/html/2511.19314/x9.png",
                "caption": "Figure 9:Input prompt forPRInTSwhen the model is trained with SFT for summarization ability and acts as a summarizer at test-time.",
                "position": 1136
            },
            {
                "img": "https://arxiv.org/html/2511.19314/x10.png",
                "caption": "Figure 10:PRInTSstep-level evaluation examples on a GAIA query. Among four candidate steps, we show the highest-scoring (top) and lowest-scoring (bottom) steps. The high-quality step acknowledges uncertainty and initiates an appropriate tool call to gather missing information, while the low-quality step makes unverified assumptions and confidently produces an unsupported answer without evidence.",
                "position": 1140
            }
        ]
    },
    {
        "header": "Appendix BAdditional Experiments",
        "images": []
    }
]