[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.04285/x1.png",
                "caption": "Figure 1:SeqTex is capable of generating high-quality and diverse textures for meshes, conditioned on either an image or text input. Furthermore, the generated textures maintain a strong correspondence with the mesh’s geometry and ensure high consistency of detailed textures across various viewing angles.",
                "position": 81
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.04285/x2.png",
                "caption": "Figure 2:Overview of our approach and core insights.(a)Joint UV and multi-view synthesis with video priors: Given an untextured 3D mesh and conditioning inputs (image or text), SeqTex generates a complete UV texture map. Unlike prior works that predict only a single texture map, our approach jointly synthesizes multi-view images and the UV texture, thereby leveraging the rich generative priors of pre-trained video models.(b)SeqTex pipeline: Geometric image and text conditions are tokenized and injected alongside noised multi-view and UV tokens. A flow matching model—initialized from a pre-trained video foundation model and then fine-tuned for our task—denoises these tokens to yield coherent multi-view renderings and a high-fidelity UV map.(c)Multi-View (MV) branch (LoRA fine-tuned): MV tokens are refined under geometric and textual guidance. Geometry-informed attention is applied exclusively among MV tokens to enforce view consistency and produce a set of aligned multi-view outputs.(d)UV branch (fully fine-tuned): Guided by geometric cues, UV tokens (queries) attend to both MV and UV tokens (keys and values) via Geo Attention. This enables the network to integrate multi-view information into a coherent UV space for accurate and seamless texture synthesis.",
                "position": 146
            }
        ]
    },
    {
        "header": "3SeqTex",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.04285/x3.png",
                "caption": "Figure 3:Qualitative comparison of texture generation methods.To ensure the generated textures align with the ground-truth, we adapt TEXTure, Text2Tex, and Paint3D to accept an additional image condition, following the methodology of TEXGen[15].",
                "position": 375
            },
            {
                "img": "https://arxiv.org/html/2507.04285/x4.png",
                "caption": "Figure 4:Visualization of text-conditioned texture generation on the real-scan DTC dataset.Given an untextured mesh, a text prompt, and a corresponding image condition, our method creates textures that align well with the geometry. The 3D priors from the video foundation model ensure view consistency.",
                "position": 440
            },
            {
                "img": "https://arxiv.org/html/2507.04285/x5.png",
                "caption": "Figure 5:Visualization of ablation study results.Removing the video prior, joint MV-UV prediction, or the decoupled MV-UV branch leads to degraded texture fidelity, instruction-following ability, and a loss of semantic or geometric details.",
                "position": 548
            },
            {
                "img": "https://arxiv.org/html/2507.04285/x6.png",
                "caption": "Figure 6:Comparison of training with albedo vs. PBR texture maps.PBR texture maps contain baked-in shadows in occluded areas.\nA model trained on such data learns to simulate these shadows, leading to incorrect artifacts during inference.",
                "position": 572
            },
            {
                "img": "https://arxiv.org/html/2507.04285/x6.png",
                "caption": "Figure 6:Comparison of training with albedo vs. PBR texture maps.PBR texture maps contain baked-in shadows in occluded areas.\nA model trained on such data learns to simulate these shadows, leading to incorrect artifacts during inference.",
                "position": 575
            },
            {
                "img": "https://arxiv.org/html/2507.04285/x7.png",
                "caption": "Figure 7:Qualitative comparison of three training strategies:\n3D-only, Split 3D, and Hybrid.The 3D-only strategy struggles with view consistency. Both Split 3D and Hybrid\nstrategies improve multi-view coherence. Hybrid training, which incorporates additional\nPBR multi-view data, further enhances the model’s understanding of lighting\nand shadow effects.",
                "position": 582
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.04285/x8.png",
                "caption": "Figure 8:Top: An indoor scene where all objects are textured using SeqTex.Bottom: Close-up renderings of selected objects from the scene above.",
                "position": 1364
            },
            {
                "img": "https://arxiv.org/html/2507.04285/x9.png",
                "caption": "Figure 9:The prompt used to evaluate text-conditioned texture generation performance.",
                "position": 1385
            }
        ]
    },
    {
        "header": "6Supplementary Material",
        "images": []
    }
]