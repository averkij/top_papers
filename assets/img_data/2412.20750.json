[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.20750/x1.png",
                "caption": "Figure 1:Multi-vision sensor related question and response examples of recent VLMs[39,53]. Note that, this example underscores the difficulty that VLMs face in understanding physical properties unique to multi-vision sensors.",
                "position": 96
            },
            {
                "img": "https://arxiv.org/html/2412.20750/x2.png",
                "caption": "Figure 2:Data samples of Multi-vision Sensor Perception and Reasoning (MS-PR) benchmark for evaluating the abilities of VLMs in multi-vision sensor understanding, which covers four types of multi-vision perception tasks (Existence, Counting, Position, and General Description) and two types of multi-vision reasoning tasks (Contextual Reasoning and Sensory Reasoning).",
                "position": 105
            },
            {
                "img": "https://arxiv.org/html/2412.20750/x3.png",
                "caption": "Figure 3:Distribution of data sources of the MS-PR benchmark. In MS-PR, we demonstrate six core multi-vision sensor tasks in the outer ring, and the inner ring displays the number of samples for each specific task.",
                "position": 108
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Multi-Vision Sensor Perception and Reasoning (MS-PR) Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.20750/x4.png",
                "caption": "Figure 4:Overview of the pipeline for generating the proposed benchmark dataset. Based on the prompts corresponding to knowledge on multi-vision sensors and tasks, ChatGPT/GPT-4o generates challenging question and answer set. We refine the dataset further by utilizing human annotators to construct positive and negative sets, allowing each pair to be classified into a specific evaluation dimension.",
                "position": 652
            }
        ]
    },
    {
        "header": "4Enhancing Multi-vision Sensor Reasoning",
        "images": []
    },
    {
        "header": "5Experiment",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetailed Description on Dataset",
        "images": []
    },
    {
        "header": "Appendix BDetailed Description on Prompt",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.20750/x5.png",
                "caption": "Figure 5:Description of sensor knowledge information and questions types and examples.",
                "position": 2498
            },
            {
                "img": "https://arxiv.org/html/2412.20750/x6.png",
                "caption": "Figure 6:Description of prompts for generating challenging multiple-choice questions and answers for multi-vision sensor tasks",
                "position": 2501
            }
        ]
    },
    {
        "header": "Appendix CHuman Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.20750/x7.png",
                "caption": "",
                "position": 2515
            },
            {
                "img": "https://arxiv.org/html/2412.20750/x8.png",
                "caption": "Figure 8:The comparison of performance across different multi-vision sensors with respect to the representative VLMs in the Multi-vision Perception task (Existence). Green font denotes the correct answer, while red font denotes the incorrect answer.",
                "position": 2530
            },
            {
                "img": "https://arxiv.org/html/2412.20750/x9.png",
                "caption": "Figure 9:The comparison of performance across different multi-vision sensors with respect to the representative VLMs in the Multi-vision Perception task (Counting). Green font denotes the correct answer, while red font denotes the incorrect answer.",
                "position": 2533
            },
            {
                "img": "https://arxiv.org/html/2412.20750/x10.png",
                "caption": "Figure 10:The comparison of performance across different multi-vision sensors with respect to the representative VLMs in the Multi-vision Perception task (Position). Green font denotes the correct answer, while red font denotes the incorrect answer.",
                "position": 2536
            },
            {
                "img": "https://arxiv.org/html/2412.20750/x11.png",
                "caption": "Figure 11:The comparison of performance across different multi-vision sensors with respect to the representative VLMs in the Multi-vision Perception task (General Description). Green font denotes the correct answer, while red font denotes the incorrect answer.",
                "position": 2539
            },
            {
                "img": "https://arxiv.org/html/2412.20750/x12.png",
                "caption": "Figure 12:The comparison of performance across different multi-vision sensors with respect to the representative VLMs in the Multi-vision Reasoning task (Contextual Reasoning). Green font denotes the correct answer, while red font denotes the incorrect answer.",
                "position": 2542
            },
            {
                "img": "https://arxiv.org/html/2412.20750/x13.png",
                "caption": "Figure 13:The comparison of performance across different multi-vision sensors with respect to the representative VLMs in the Multi-vision Reasoning task (Sensory Reasoning). Green font denotes the correct answer, while red font denotes the incorrect answer.",
                "position": 2545
            }
        ]
    },
    {
        "header": "Appendix DAdditional Question and Answer Examples",
        "images": []
    }
]