[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.07022/x1.png",
                "caption": "Figure 1:Overall performance ofSolar Openand other comparable models.",
                "position": 199
            }
        ]
    },
    {
        "header": "1  Introduction",
        "images": []
    },
    {
        "header": "2  Model Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.07022/x2.png",
                "caption": "Figure 2:The compression rates of theSolar OpenTokenizer and other tokenizers (higher is more efficient). In each bar group, the nine bars left toSolar Openrepresent the ‘global’ (English and/or Chinese-centric) models, and the four bars right toSolar Openrepresent the Korean-centric models.",
                "position": 357
            },
            {
                "img": "https://arxiv.org/html/2601.07022/x3.png",
                "caption": "Figure 3:Inference-time tokenizer efficiency across languages and reasoning settings.",
                "position": 383
            }
        ]
    },
    {
        "header": "3  Pre-training",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.07022/x4.png",
                "caption": "Figure 4:The data curriculum for the pre-training phases ofSolar Open.",
                "position": 592
            },
            {
                "img": "https://arxiv.org/html/2601.07022/x5.png",
                "caption": "Figure 5:Training trajectory comparison betweenSolar Openand GLM-4.5-Base (23T tokens).Solar Openachieves comparable performance at 10.9T tokens (English) and 17.8T tokens (Korean), based on MMLU, MMLU-Pro, and HellaSwag benchmarks. Curves are smoothed for clarity.",
                "position": 733
            }
        ]
    },
    {
        "header": "4  Mid-training",
        "images": []
    },
    {
        "header": "5  Post-training: SFT",
        "images": []
    },
    {
        "header": "6  Post-training: RL",
        "images": []
    },
    {
        "header": "7  Evaluation",
        "images": []
    },
    {
        "header": "8  Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]