[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.03467/x1.png",
                "caption": "Figure 1:Comparisons on reasoning-centric image editing.Although unified multimodal generative models such as Qwen-Edit[qwen-image]have substantially improved editing quality, their underlying reasoning remains underexplored, especially for reasoning-centric editing. In contrast, our method delivers accurate edits with deep reasoning, achieving strong consistency and high perceptual quality across diverse reasoning-driven editing scenarios.",
                "position": 106
            },
            {
                "img": "https://arxiv.org/html/2601.03467/x2.png",
                "caption": "Figure 2:Comparison with prior methods.Prior RL methods for visual generation[liu2025flow,xue2025dancegrpo]focus on exploration within the stochastic space of generation, improving synthesis quality but offering limited reasoning capability. To address this issue, we decouple and optimize the understanding and generation modules to preserve high-fidelity synthesis while enabling exploration of optimal trajectories in the reasoning space. Besides, we introduce CoT-based sampling and optimization to further expand stochastic exploration over reasoning pathways.",
                "position": 110
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.03467/x3.png",
                "caption": "Figure 3:Overview of our method.During sampling, we perform Chain-of-Thought reasoning with explicit planning and reflection to enlarge stochasticity in the reasoning space. For rewards, a fine-grained, sample-specific checklist guides the VLM to produce accurate and stable reasoning scores. In grouping, we construct an unbiased preference chain across candidates to select training samples and compute advantagesAA. Finally, policy updates apply a unified editing reward while decoupling updates to the reasoning, understanding, and generation modules, enhancing reasoning capability without sacrificing synthesis quality.",
                "position": 190
            },
            {
                "img": "https://arxiv.org/html/2601.03467/x4.png",
                "caption": "Figure 4:Comparisons between ThinkRL-Edit and the leading baselines. We conduct the comparison across diverse reasoning-centric editing tasks. As observed, our method achieves precise instruction following with strong consistency and high quality, which significantly surpasses previous methods. Blue text denotes the instruction, and green text indicates the desired editing outcome.",
                "position": 204
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Limitations and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]