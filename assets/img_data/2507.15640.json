[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15640/extracted/6637668/Figures/vertical_histograms_example.png",
                "caption": "Figure 1:Four averaged distributions drawn from 20 randomly generated data mixing trajectories. Each distribution in the trajectories is first categorized by whether it increases/decreases the performance of a 50M target model on the MMLU/MATH benchmarks within one re-weighting step. Each category is then averaged to obtain the corresponding distribution in the figure. The models are trained on a 52-dimensional space (more details in Sec.3.1), mixing the DCLM(Li et al.,2024)and the math split of the Dolmino-mix-1124(OLMo et al.,2024)dataset.",
                "position": 116
            }
        ]
    },
    {
        "header": "2Domain Re-weighting as Markov Decision Process",
        "images": []
    },
    {
        "header": "3Data Mixing Agent",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15640/extracted/6637668/Figures/main.png",
                "caption": "Figure 2:An overview of the training and domain reweighting pipeline of the data mixing agent. We first sample large quantities of data mixing trajectories and train small proxy models on them. Each model checkpoint obtains feedback from the evaluation environment. Secondly, the data mixing agent is optimized on these trajectories and feedback via supervised fine-tuning and off-policy reinforcement learning with a CQL-based Q function. During guiding the training of the target model, the agent directly determines the distribution for the next domain re-weighting step on the fly.",
                "position": 187
            },
            {
                "img": "https://arxiv.org/html/2507.15640/extracted/6637668/Figures/kl_output_avg.png",
                "caption": "Figure 3:The KL divergence between the estimated start state by sampled data from the target model and the ground-truth distribution obtained from the Pile dataset. The results are averages of 5 random runs.",
                "position": 198
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15640/extracted/6637668/Figures/data_traj_sft_rl_2D.png",
                "caption": "Figure 4:The two data mixing agents’ output domain reweighting trajectories based on the 2-dimensional domain space, training on the LLaMA-3B-DCLM-100B model and the math reasoning field. The dashed line denotes the optimal domain distributions determined by RegMix.",
                "position": 1934
            },
            {
                "img": "https://arxiv.org/html/2507.15640/extracted/6637668/Figures/data_traj_sft_regmix.png",
                "caption": "Figure 5:DataAgentSFT’s domain reweighting trajectories based on the 52-dimensional domain space, training on the LLaMA-3B-DCLM-100B model and the math reasoning field. The legends within each sub-figure are the same as those of Fig.4.",
                "position": 1937
            },
            {
                "img": "https://arxiv.org/html/2507.15640/extracted/6637668/Figures/data_traj_rl_regmix.png",
                "caption": "Figure 6:DataAgentRL’s domain reweighting trajectories based on the 52-dimensional domain space, training on the LLaMA-3B-DCLM-100B model and the math reasoning field. The legends within each sub-figure are the same as those of Fig.4.",
                "position": 1940
            },
            {
                "img": "https://arxiv.org/html/2507.15640/extracted/6637668/Figures/data_efficiency_sft_rl_2D.png",
                "caption": "Figure 7:The performance dynamics of the target model on the evaluation environment with increasing training data (measured in Billion tokens) on the corresponding field. We set a total training budget of 10.5B tokens, but DataAgentRLtriggers an early stopping at 9.96B tokens, and DataAgentSFTtriggers an early stopping at 9.43B tokens.",
                "position": 1977
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]