[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02439/logo/webgym.png",
                "caption": "",
                "position": 115
            },
            {
                "img": "https://arxiv.org/html/2601.02439/logo/ms-logo.png",
                "caption": "",
                "position": 124
            },
            {
                "img": "https://arxiv.org/html/2601.02439/logo/uiuc.png",
                "caption": "",
                "position": 124
            },
            {
                "img": "https://arxiv.org/html/2601.02439/logo/cmu_square.png",
                "caption": "",
                "position": 124
            },
            {
                "img": "https://arxiv.org/html/2601.02439/assets/teaser.png",
                "caption": "",
                "position": 136
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Tasks inWebGymand Evaluation Protocol",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02439/x1.png",
                "caption": "Figure 2:Task decomposition system.WebGymdecomposes tasks by generating valid combinations of fact groups from the original task’s rubric. Decomposition requires≥\\geq2 groups with at least one “large” group (≥\\geq3 facts). Each valid combination (excluding the full set) produces a new task with lower difficulty while maintaining consistency with the original objectives.",
                "position": 352
            },
            {
                "img": "https://arxiv.org/html/2601.02439/x2.png",
                "caption": "Figure 3:Statistics of theWebGymtrainingtask set.Left:website distribution as a function of the sorted index (from more tasks to less) of website.Right:difficulty distribution of the train and test task sets. The transparent bars over the original bar mean decomposed tasks, and the slashed bars means test set.]",
                "position": 386
            },
            {
                "img": "https://arxiv.org/html/2601.02439/x3.png",
                "caption": "",
                "position": 390
            },
            {
                "img": "https://arxiv.org/html/2601.02439/x4.png",
                "caption": "Figure 4:Analysis of theWebGymtask set.Left:distribution of tasks across domains according to the Mind2Web-2 taxonomy(mind2web-2).Right:Distribution of trajectory lengths by task difficulty foransweredtrajectories over multiple iterations. For trajectories working on medium- and hard-difficulty tasks that exceeds 30 tasks during evaluation, we filter in only trajectories under 30 tasks to make the comparison fair. The violin plots show that the probability density of trajectory lengths, with the KDE mode (black line, KDE mode is the peak of the smoothed probability density curve, where the violin is widest) indicating the most likely length and mean (red) indicating the average.",
                "position": 394
            },
            {
                "img": "https://arxiv.org/html/2601.02439/x4.png",
                "caption": "",
                "position": 397
            },
            {
                "img": "https://arxiv.org/html/2601.02439/x5.png",
                "caption": "",
                "position": 401
            },
            {
                "img": "https://arxiv.org/html/2601.02439/x6.png",
                "caption": "Figure 5:Agreement between automated evaluators and human judgment.Rubric-based evaluation (with explicit criteria) consistently improves agreement over task-only evaluation, yielding higher accuracy and precision across LLM-based evaluators. Among the evaluators, GPT-4o shows the largest shift after adding the rubric: precision increases the most while recall drops, indicating that the rubric makes GPT-4o apply stricter, more conservative pass criteria.",
                "position": 414
            }
        ]
    },
    {
        "header": "4Rollout System inWebGym",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02439/x7.png",
                "caption": "Figure 6:Asynchrony eliminates burst-idle behavior in web rollouts.Left:WebGymimplements anasynchronous rollout systemthat (1) shortens single-trajectory collection duration by isolating rollout processes and (2) allows new rollout processes to join early by replacing batched inference with a process pool; the example shown is a toy case with 3 available environment buckets but 4 tasks waiting to roll out.Right:CPU utility trace over a sampled rollout period, comparing the synchronized rollout framework toWebGym: synchronized barriers induce spiky “all-busy then all-idle” utilization, whereasWebGymstreams work as soon as observations are ready, smoothing CPU load and reducing idle gaps while CPU workers await policy actions.",
                "position": 451
            },
            {
                "img": "https://arxiv.org/html/2601.02439/x8.png",
                "caption": "",
                "position": 460
            },
            {
                "img": "https://arxiv.org/html/2601.02439/x9.png",
                "caption": "Figure 7:Benchmarking speed and throughput of theWebGymasynchronous rollout framework.Left: the WebGym framework boosts the rollout speed up significantly with a 4x-5x speedup.This figure shows time cost and average CPU utility percentage when reaching all 256 environments running w.r.t. different amounts of CPUs, while using the same amount of GPU resources for running inference on the VLM-based agent. The limit of browsers per CPU can scale to2without crashing.Right: the WebGym framework boosts the rollout speed up linearly w.r.t. GPU nodes.This figure showsWebGymthroughput w.r.t. different numbers of GPU nodes when enough CPUs are provided on each machine (thus speed is bounded by GPU).",
                "position": 468
            },
            {
                "img": "https://arxiv.org/html/2601.02439/x10.png",
                "caption": "",
                "position": 477
            }
        ]
    },
    {
        "header": "5Training Web Agents withWebGym",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02439/x11.png",
                "caption": "Figure 8:Ablations on base models, prompting, and action filtering.(left)test-set success rate curves of different models (Qwen3-VL-Instruct-8B, Qwen3-VL-Thinking-8B, GPT-4o, and GPT-5-Thinking), and of Qwen3-VL-Instruct-8B under different constraints (either removing the memory prompt or removing the repetition penalty during RL).(right)test-set same-screenshot (repetitive inefficient action) rate and trajectory length curves before and after applying the action repetition penalty (filtering) on the Instruct model. The hyper-parameters of the best run (“Instruct-8B”) are shown in §F. Theorangecurve is an average of two runs, while the translucent orange area around the orange line denotes the variance of these two runs. We observe that even if the starting variance between different runs can be large, the curves converges after 24k training trajectories are collected. All plots are produced with an EMA (Exponential Moving Average) smoothing of0.500.50.",
                "position": 548
            },
            {
                "img": "https://arxiv.org/html/2601.02439/x12.png",
                "caption": "",
                "position": 557
            },
            {
                "img": "https://arxiv.org/html/2601.02439/x13.png",
                "caption": "Figure 10:Exploring scaling dimensions ofWebGymand difficulty-aware training.Test-set success rate curves under different variations to training: removing domains (“exclude domains”), tuning difficulty ratios (“uniform sampling”, “biased to hard”, “only easy”, “only medium”), and shortening the train-time step budget (“shorten horizon”). Results are reported for(a) Overall(all tasks),(b) Easy (1–3),(c) Medium (4–6), and(d) Hard (7+). Theorangecurve averages two runs; the translucent orange band shows their variation. All plots are produced with an EMA (Exponential Moving Average) smoothing of0.500.50.",
                "position": 651
            }
        ]
    },
    {
        "header": "6Discussion and Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "Appendix ARaw Experimental Results",
        "images": []
    },
    {
        "header": "Appendix BTraining Environment Designs",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02439/x14.png",
                "caption": "Figure 11:WebGymimplements anoperation-specific queue systemthat balances the CPU and GPU machine usages. Here we illustrate the computational model of this pool system in theleftsubplot, and the comparison of the two queue designs on theright. We represent the centralized CPU queue withQueue, and the cascading queue system for with the initial letter of the request type, specifically, Navigating to a webpage (N), taking a Screenshot (S), and Executing Action (E) that takes medium amount of time. A green box means the CPU machine is under optimal load.",
                "position": 845
            },
            {
                "img": "https://arxiv.org/html/2601.02439/x15.png",
                "caption": "Figure 12:Left: Number of trajectories successfully finished and crashed out under high CPU load whent=10​mint=10\\text{min}with 64 CPUs, 256 environments, and 3 GPU nodes. Top line:globalqueue. Bottom line:localqueue. Right:The block rate increases as training goes on.",
                "position": 857
            },
            {
                "img": "https://arxiv.org/html/2601.02439/x15.png",
                "caption": "",
                "position": 860
            },
            {
                "img": "https://arxiv.org/html/2601.02439/x16.png",
                "caption": "",
                "position": 864
            }
        ]
    },
    {
        "header": "Appendix CModeling the Markov Decision Process On Long-Horizon Web Navigation Tasks",
        "images": []
    },
    {
        "header": "Appendix DQualitative Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02439/x17.png",
                "caption": "Figure 13:Trajectory for the comparison between vanilla evaluation and rubric-based evaluation.",
                "position": 1007
            }
        ]
    },
    {
        "header": "Appendix EPrompts",
        "images": []
    },
    {
        "header": "Appendix FHyper-parameters",
        "images": []
    }
]