[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.10836/x1.png",
                "caption": "",
                "position": 71
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.10836/x2.png",
                "caption": "Figure 2:The generated optical flow by our method with different condition signals. Given a specific image, from top to bottom are optical flows generated with camera trajectory, arrow-based motion annotation, and both conditions, respectively.",
                "position": 104
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.10836/x3.png",
                "caption": "Figure 3:AnimateAnything Pipeline.The pipeline consists of two stages:\n1) Unified Flow Generation, which creates a unified optical flow representation by leveraging visual control signals through two synchronized latent diffusion models, namely the Flow Generation Model (FGM) and the Camera Reference Model (CRM). The FGM accepts sparse or coarse optical flow derived from visual signals other than camera trajectory. The CRM inputs the encoded reference image and camera trajectory embedding to generate multi-level reference features. These features are fed into a reference attention layer to progressively guide the FGM’s denoising process in each time step, producing a unified dense optical flow.\n2) Video Generation, which compresses the generated unified flow with a 3D VAE encoder and integrates it with video latents from the image encoder using a single ViT block. The final output is then combined with text embeddings to generate the final video using the DiT blocks.",
                "position": 167
            },
            {
                "img": "https://arxiv.org/html/2411.10836/x4.png",
                "caption": "Figure 4:Video stabilization Module",
                "position": 309
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.10836/x5.png",
                "caption": "Figure 5:Camera trajectory comparison with other trajectory-based methods",
                "position": 349
            },
            {
                "img": "https://arxiv.org/html/2411.10836/x6.png",
                "caption": "Figure 6:Motion Transfer comparison with state-of-the-art methods.",
                "position": 352
            },
            {
                "img": "https://arxiv.org/html/2411.10836/x7.png",
                "caption": "Figure 7:Users drag animation comparison with other animation methods.",
                "position": 356
            },
            {
                "img": "https://arxiv.org/html/2411.10836/x8.png",
                "caption": "Figure 8:Human face animation with optical flow extracted from reference video",
                "position": 477
            },
            {
                "img": "https://arxiv.org/html/2411.10836/x9.png",
                "caption": "Figure 9:Image to video generation comparison with current state-of-the-art methods.",
                "position": 618
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]