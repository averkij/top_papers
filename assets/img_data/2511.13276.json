[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": []
    },
    {
        "header": "IIProposed Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.13276/framework.png",
                "caption": "Figure 1:Illustration of the dual-backbone MIL framework.\nEach video is divided into 32 temporal segments (vmv_{m}). From each segment (um,iu_{m,i}), 16 frames (xm,ix_{m,i})\nare uniformly sampled to form a shorter segment, which is encoded by I3D (convolutional-based) and TimeSformer (transformer-based) encoders.\nThe concatenated andâ„“2\\ell_{2}-normalized features are processed by a compact prediction head\nand aggregated through top-kkpooling to produce the final video-level anomaly prediction.",
                "position": 142
            }
        ]
    },
    {
        "header": "IIIExperiments and Results",
        "images": []
    },
    {
        "header": "IVConclusions and future work",
        "images": []
    }
]