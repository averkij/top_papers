[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Features and Technologies",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20856/x1.png",
                "caption": "Figure 1:Nemotron 3 models (e.g., Nemotron Nano 3) leverage a hybrid Mamba-Transformer MoE architecture consisting predominantly of interleaved Mamba-2 and MoE layers, with a select few self attention layers.",
                "position": 76
            },
            {
                "img": "https://arxiv.org/html/2512.20856/x2.png",
                "caption": "Figure 2:The hybrid Mamba-Transformer MoE architecture used by Nemotron 3 models can achieve state-of-the-art accuracy on leading reasoning benchmarks and ultra-long-context tasks while providing throughput improvements over similarly sized Transformer MoEs. For details, please see the Nemotron Nano 3 technical report.",
                "position": 79
            },
            {
                "img": "https://arxiv.org/html/2512.20856/figures/standard_moe.png",
                "caption": "(a)Standard MoE architecture.",
                "position": 93
            },
            {
                "img": "https://arxiv.org/html/2512.20856/figures/standard_moe.png",
                "caption": "(a)Standard MoE architecture.",
                "position": 96
            },
            {
                "img": "https://arxiv.org/html/2512.20856/figures/latent_moe.png",
                "caption": "(b)LatentMoE architecture.",
                "position": 101
            },
            {
                "img": "https://arxiv.org/html/2512.20856/x3.png",
                "caption": "Figure 4:Relative difference in train loss (left) and validation loss (right) between models trained with NVFP4 and BF16, shown at two model scales: Nemotron 3 Nano (A3B) and the larger MoE model (A8B). Loss gaps decrease as model size increases (A3Bâ†’\\toA8B). Recipe ablation on Nemotron 3 Nano started from Nemotron 3 NVFP4 checkpoint at 500B tokens, then quantizes sensitive layers (Mamba Output, QKV, and Attention projections) to NVFP4, highlighting the importance of keeping these layers in high precision.",
                "position": 296
            },
            {
                "img": "https://arxiv.org/html/2512.20856/x4.png",
                "caption": "Figure 5:Downstream task evaluations on 8B active MoE model, trained to 1T tokens. NVFP4 accuracy closely follows BF16 trajectories throughout training. Evaluations are performed in BF16.",
                "position": 300
            },
            {
                "img": "https://arxiv.org/html/2512.20856/x5.png",
                "caption": "Figure 6:Cumulative average Negative loglikelihood (NLL) as a function of token position in code data. Nemotron 3 Nano base shows improved predictions upto 1M tokens in code data.",
                "position": 347
            },
            {
                "img": "https://arxiv.org/html/2512.20856/x6.png",
                "caption": "Figure 7:Multi-environment RL training: within a single RL run several different environments corresponding to diverse capabilities are being optimized.",
                "position": 358
            },
            {
                "img": "https://arxiv.org/html/2512.20856/x7.png",
                "caption": "Figure 8:Accuracy-efficiency trade-off with reasoning budget control at inference time.",
                "position": 376
            }
        ]
    },
    {
        "header": "3Key Takeaways",
        "images": []
    },
    {
        "header": "Contributors",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]