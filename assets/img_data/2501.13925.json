[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.13925/extracted/6145233/fig/logo3.png",
                "caption": "",
                "position": 57
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.13925/extracted/6145233/fig/fig1.png",
                "caption": "Figure 1:An example of visually grounded detailed descriptions generated by the proposed GeoPixel, highlighting its ability to interpret and segment high-resolution remote sensing imagery with fine-grained precision. The model applies distinct masks to key objects (ground track field, swimming pool, soccer field) and semantic mask to smaller objects (vehicles). It effectively identifies spatial positions (e.g., center, top) and relationships (within the sports complex) while distinguishing between the global context (buildings, roads, green spaces) and localized structures.",
                "position": 98
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.13925/extracted/6145233/fig/fig2.png",
                "caption": "Figure 2:Overview of GeoPixel Architecture: Left: High-resolution RS images are dynamically partitioned into local patches and a resized global view, encoded by a frozen vision encoder. The encodings are projected into the language domain with separator tokens. Middle: Vision tokens, combined with text, are input into the LLM, where pLoRA is applied to vision tokens for efficient and effective multimodal alignment. Right: The corresponding embeddings for the [SEG] tokens are passed to a decoder through text projector, along with vision embeddings from the grounding vision encoders, to generate precise segmentation masks.",
                "position": 289
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.13925/extracted/6145233/fig/fig3.png",
                "caption": "Figure 3:The GeoPixelD Annotation Pipeline provides detailed multi-tier descriptions of remote sensing imagery with object phrases aligned precisely with manually annotated masks. It begins with Holistic Image Annotation (bottom left), where an LMM generates concise scene descriptions. Individual Instance Annotation (bottom right) uses spatial({pos}) and categorical ({catagorory_name}) priors with SOM ({mark_number}) prompting to describe key objects. Cluster Annotation (top right) organizes smaller or dense objects using refined grids for precise spatial analysis.",
                "position": 341
            }
        ]
    },
    {
        "header": "4GeoPixelD-RS Pixel Grounding Dataset",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.13925/extracted/6145233/fig/fig7.png",
                "caption": "Figure 4:Qualitative results of GeoPixel on RS-GCG. Contextually rich descriptions of RS imagery with grounded object annotations. Depending on object scale and density, it employs instance masks for precise delineation of individual objects (right and middle-right images) while semantic masks capture broader categories, such as large clusters of vehicles or small objects (middle-left and left images).",
                "position": 577
            },
            {
                "img": "https://arxiv.org/html/2501.13925/extracted/6145233/fig/fig10.png",
                "caption": "Figure 5:Failure case due to incorrect mask association (left) and wrong instance segmentation in the same spatial region (right).",
                "position": 895
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.13925/extracted/6145233/fig/fig5.png",
                "caption": "Figure 6:Comparative effectiveness of SOM prompting methods, highlighting the critical role of priors. Without priors, SOM relies solely on the VLM to detect and describe marked objects independently, resulting in inaccurate descriptions and hallucinated markers in complex remote sensing scenes. In contrast, SOM with priors utilizes explicit marker positions ({pos}) and predefined object categories ({category_name}) as priors, providing structured prompts that reduce ambiguity and guide the VLM to produce precise and reliable descriptions. Incorrect parts are noted inredwhereas correct parts are noted ingreen.",
                "position": 1864
            },
            {
                "img": "https://arxiv.org/html/2501.13925/extracted/6145233/fig/fig6.png",
                "caption": "Figure 7:Comparison of open-source and proprietary models for prior-informed set of marks (SOM) prompting for RS imagery. Incorrect parts are noted inredwhereas correct parts are noted ingreen.",
                "position": 1867
            },
            {
                "img": "https://arxiv.org/html/2501.13925/extracted/6145233/fig/fig8.png",
                "caption": "Figure 8:Unifying Annotations through LLM Paraphrasing and Text Marking to track associated masks. Objects are indexed numerically (e.g., ”object-N”), and holistic (blue), individual (teal), and cluster (green) annotations are concatenated into a single image description. Paraphrasing instructions with combined description produce a concise, consistent GCG description that eliminates redundancy while preserving object-mask associations, even with reordering.",
                "position": 1870
            },
            {
                "img": "https://arxiv.org/html/2501.13925/extracted/6145233/fig/fig9.png",
                "caption": "Figure 9:Qualitative results of GLaMM’s capability in referring remote sensing expression segmentation. The figure highlights Geopixel’s ability to interpret referring expressions of varying lengths and generate precise segmentation masks, adapting to scale variations, as shown in the ground track fields. Spatial descriptors (e.g ”right”, ”lower right”), and object characteristics (e.g ”red”) are interpreted with precision to achieve accurate segmentation.",
                "position": 1873
            }
        ]
    },
    {
        "header": "Appendix AGeoPixelD dataset",
        "images": []
    }
]