[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12225/x1.png",
                "caption": "(a)",
                "position": 189
            },
            {
                "img": "https://arxiv.org/html/2510.12225/x1.png",
                "caption": "(a)",
                "position": 192
            },
            {
                "img": "https://arxiv.org/html/2510.12225/x2.png",
                "caption": "(b)",
                "position": 197
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12225/x3.png",
                "caption": "Figure 2:Overview of the data curation pipeline.In this work, we curate the context (image, question) from diverse sources and assess the impact of their mixing. Further, we curate a set of data interventions that target diverse skills and types. Subsequently, we study the impact of scaling along different data axes. These insights lead to the creation of our large-scaleHoneyBeedataset.",
                "position": 582
            }
        ]
    },
    {
        "header": "3Data Curation Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12225/x4.png",
                "caption": "Figure 3:Overview of data intervention strategies.In this work, we curate a diverse set of data interventions to enhance the quality of VL reasoning CoT data. These methods target a range of skills, such as perception and problem-solving capabilities. We present details for each intervention in ยง3.2.",
                "position": 607
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12225/x5.png",
                "caption": "(a)",
                "position": 1665
            },
            {
                "img": "https://arxiv.org/html/2510.12225/x5.png",
                "caption": "(a)",
                "position": 1668
            },
            {
                "img": "https://arxiv.org/html/2510.12225/x6.png",
                "caption": "(b)",
                "position": 1673
            },
            {
                "img": "https://arxiv.org/html/2510.12225/x7.png",
                "caption": "(c)",
                "position": 1678
            },
            {
                "img": "https://arxiv.org/html/2510.12225/x8.png",
                "caption": "Figure 6:HoneyBee-VL scaling pipeline.We present the pipeline to scale the number CoTs per (real image, question) pair (top), and scale the number of questions per real image (bottom) from the context datasets. We denote majority voting over the several predicted answers as MV. Ultimately, this pipeline leads to the creation of1.51.5M VL reasoning instances.",
                "position": 1766
            },
            {
                "img": "https://arxiv.org/html/2510.12225/x9.png",
                "caption": "Figure 7:PLM-3B trained onHoneyBeeoutperforms the base model across allMathVisiondifficulty levels.",
                "position": 1793
            },
            {
                "img": "https://arxiv.org/html/2510.12225/x10.png",
                "caption": "(a)",
                "position": 1854
            },
            {
                "img": "https://arxiv.org/html/2510.12225/x10.png",
                "caption": "(a)",
                "position": 1857
            },
            {
                "img": "https://arxiv.org/html/2510.12225/x11.png",
                "caption": "(b)",
                "position": 1862
            },
            {
                "img": "https://arxiv.org/html/2510.12225/x12.png",
                "caption": "Figure 9:Model Correlation Analysis.We present the correlation between the performance of PLM-8B and PLM-3B, each trained with various data distributions throughout this work, across diverse downstream evaluation tasks. A higher correlation implies greater predictability between model performances; that is, a data curation method that is optimal for a33B VLM is more likely to be optimal for an88B VLM.",
                "position": 1877
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "8Additional Context Dataset Details and Results",
        "images": []
    },
    {
        "header": "9Additional Data Intervention Details and Results",
        "images": []
    },
    {
        "header": "10Self-Improvement",
        "images": []
    },
    {
        "header": "11Additional Training and Evaluation Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12225/x13.png",
                "caption": "(a)",
                "position": 3698
            },
            {
                "img": "https://arxiv.org/html/2510.12225/x13.png",
                "caption": "(a)",
                "position": 3701
            },
            {
                "img": "https://arxiv.org/html/2510.12225/x14.png",
                "caption": "(b)",
                "position": 3706
            },
            {
                "img": "https://arxiv.org/html/2510.12225/x15.png",
                "caption": "(c)",
                "position": 3711
            },
            {
                "img": "https://arxiv.org/html/2510.12225/x16.png",
                "caption": "(d)",
                "position": 3717
            },
            {
                "img": "https://arxiv.org/html/2510.12225/x17.png",
                "caption": "(e)",
                "position": 3722
            },
            {
                "img": "https://arxiv.org/html/2510.12225/x18.png",
                "caption": "(f)",
                "position": 3727
            },
            {
                "img": "https://arxiv.org/html/2510.12225/x19.png",
                "caption": "(g)",
                "position": 3733
            },
            {
                "img": "https://arxiv.org/html/2510.12225/x20.png",
                "caption": "(h)",
                "position": 3738
            },
            {
                "img": "https://arxiv.org/html/2510.12225/x21.png",
                "caption": "(i)",
                "position": 3743
            },
            {
                "img": "https://arxiv.org/html/2510.12225/x22.png",
                "caption": "(j)",
                "position": 3749
            },
            {
                "img": "https://arxiv.org/html/2510.12225/x23.png",
                "caption": "(k)",
                "position": 3754
            }
        ]
    },
    {
        "header": "12Prompts",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12225/images/scale_question_example_1.png",
                "caption": "Figure 11:Qualitative example to show diverse and reasonable synthetic images for a given image.",
                "position": 4087
            },
            {
                "img": "https://arxiv.org/html/2510.12225/images/scale_question_example_2.png",
                "caption": "Figure 12:Qualitative example to show diverse and reasonable synthetic images for a given image.",
                "position": 4111
            },
            {
                "img": "https://arxiv.org/html/2510.12225/images/qual_example_1.png",
                "caption": "Figure 13:Qualitative example fromHoneyBeedataset.",
                "position": 4120
            },
            {
                "img": "https://arxiv.org/html/2510.12225/images/qual_example_2.png",
                "caption": "Figure 14:Qualitative example fromHoneyBeedataset.",
                "position": 4175
            }
        ]
    },
    {
        "header": "13Qualitative Examples",
        "images": []
    }
]