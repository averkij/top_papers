[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22680/sec/figures/vptt.jpg",
                "caption": "Figure 1:Visual Personalization Turing Test.We present the Visual Personalization Turing Test (VPTT), a new paradigm for contextual personalization at scale. A model passes the VPTT if its output is indistinguishable to a human or a calibrated VLM judge from what a given person might plausibly create or share. As one way to address this challenge, we introduceVPTT Frameworkconsisting of privacy-safe benchmarkVPTT-Benchfor evaluating personalized generation and editing, and Visual Personalization RAG (VPRAG) that retrieves persona-aligned visual cues and converts them into personalized image generations or edits. To close the loop, we propose an automatedVPTTscore\\mathrm{VPTT_{score}}that achieves strong Spearman rank correlation (ρ\\rho) with humans and VLM Judges, establishing it as a cheap, reliable proxy for human perception of personalization.",
                "position": 196
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22680/sec/figures/main.jpg",
                "caption": "Figure 2:Contextual Image Generation and Editing using VPTT-Bench.Each row shows a distinct user profile: assets and style cues (left), personalized generations (social post, cultural site), and edits (garden, living room) guided by the same persona identity. All images are generated synthetically via our Visual Personalization RAG (VPRAG) by text, which retrieves persona-aligned cues. To show cross model personalization here the assets are generated by QWEN-image-model[65]and generations and edits by Nano-Banana[23]conditioned only on the first image. More results in are in Supplementary materials.",
                "position": 202
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22680/sec/figures/data_gen.jpg",
                "caption": "Figure 3:VPTT-Bench Data Generation Pipeline.Overview of the deferred rendering pipeline used to construct VPTT-Bench.\n(1) Personas are sampled from PersonaHub[21]with demographics.\n(2–3) Visual and scenario elements (lighting, actions, materials etc.) are extracted.\n(4) These cues are composed into structured captions and embedded via an LLM.\n(5) Generating 30 corresponding visual assets per persona, forming privacy-safe, semantically grounded data for evaluating contextual personalization.",
                "position": 258
            },
            {
                "img": "https://arxiv.org/html/2601.22680/sec/figures/personas.jpg",
                "caption": "Figure 4:Example Personas from VPTT-Bench.Each row shows a synthetic persona sampled from PersonaHub[21](only short descriptions) with its corresponding visual assets generated via VPTT-Bench generation pipeline.\nPersonas span diverse regions, professions, and age groups, illustrating the demographic and contextual diversity of VPTT-Bench.",
                "position": 266
            },
            {
                "img": "https://arxiv.org/html/2601.22680/sec/figures/pipeline1.jpg",
                "caption": "Figure 5:VPRAG Pipeline Overview.Comparison between the baseline retrieval-augmented generation (BRAG) and our proposed Visual Personalization RAG (VPRAG).\nUnlike baseline BRAG, VPRAG introduces controllable and interpretable retrieval through:\n(a) post-level embedding and similarity scoring, (b) temperature-controlled attention, (c) entropy-guided post selection, (d) capacity-aware quota allocation, (e) category-level ranking, and (f) element-level composition.\nThis multi-stage design yields a white-box, LLM-optional retrieval framework producing visually and semantically aligned personalized generations and edits.",
                "position": 284
            }
        ]
    },
    {
        "header": "3Visual Personalization Turing Test",
        "images": []
    },
    {
        "header": "4Evaluations",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22680/sec/figures/comp1.jpg",
                "caption": "Figure 6:Qualitative Comparison across Generation and Editing Tasks.Representative examples from the VPTT-Bench showing outputs from five methods: Baseline, Persona Only, BRAG, VPRAG (ours), and BRAG + VPRAG (ours). Each sample is evaluated using human, VLM (reasoning shown), and text-levelVPTTscore​-​c\\mathrm{VPTT_{score}\\text{-}c}scores, where higher indicates closer alignment to the persona’s assets. Our methods achieve the highest perceptual and text–visual consistency, confirming effective contextual personalization.",
                "position": 808
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22680/sec/figures/visual_baselines.jpg",
                "caption": "Figure 7:Comparison to Visual Baselines.We compare VPRAG (along three columns) against a broad set of visual personalization baselines, including fine-tuning approaches, preference-driven personalization methods, and multimodal LLM (MLLM)–based in-context techniques.\nEvaluation is conducted using two metrics: the VIPER Proxy Score (PS)[54]and the Gemini VLM Judge (see Sec. 4 in the main paper).\nAcross more challenging and nuanced examples shown in the figure, VPRAG consistently emerges as an efficient and controllable personalization method, performing on par with or outperforming these substantially more expensive baselines.",
                "position": 866
            },
            {
                "img": "https://arxiv.org/html/2601.22680/sec/figures/copying.jpg",
                "caption": "Figure 8:Copy-Paste Effect.The baselines including MLLMs suffer from copy-paste effect where the generations and edits only consider a single or few images of the user assets.",
                "position": 872
            },
            {
                "img": "https://arxiv.org/html/2601.22680/sec/figures/diversity1.jpg",
                "caption": "Figure 9:VPTT-Bench Ethnicity and Location Diversity. Ethnicity and location diversity of the users in VPTT-Bench",
                "position": 875
            },
            {
                "img": "https://arxiv.org/html/2601.22680/sec/figures/diversity2.jpg",
                "caption": "Figure 10:VPTT-Bench Age and Interest Diversity. Age Distribution and t-sne visualization (interests) of first 1000 users.",
                "position": 878
            },
            {
                "img": "https://arxiv.org/html/2601.22680/sec/figures/diversity3.jpg",
                "caption": "Figure 11:Diversity of 10K Synthetic Personas.We visualize the diversity of our 10,004 synthetic personas using t-SNE dimensionality reduction on averaged caption embeddings (OpenAI text-embedding-3-small, 1536-dim) from each persona’s 30-image gallery. Points are colored by age. The average pairwise cosine similarity of 0.611 indicates balanced diversity ; personas occupy a shared human aesthetic space while maintaining distinct individual preferences. Our dataset spans 174 countries and 5,460 unique occupations, with 39,003 unique interests and 269,035 visual elements across all personas. Each persona averages 7.1 interests, 5.7 personality traits, and 38.6 visual elements, ensuring rich and diverse personalization signals for image generation models.",
                "position": 881
            },
            {
                "img": "https://arxiv.org/html/2601.22680/sec/figures/user1.jpg",
                "caption": "Figure 12:This Figure is only for illustration and not a part of the main dataset. The human figures shown in the sample images are non-author volunteers who provided consent. Their faces and all identifying cues (e.g., location) are fully anonymized.",
                "position": 884
            },
            {
                "img": "https://arxiv.org/html/2601.22680/sec/figures/user_02.jpg",
                "caption": "Figure 13:This Figure is only for illustration and not a part of the main dataset. The human figures shown in the sample images are non-author volunteers who provided consent. Their faces and all identifying cues (e.g., location) are fully anonymized.",
                "position": 887
            }
        ]
    },
    {
        "header": "S.1 Additional Details: Formalization of the VPTT Evaluation Protocol",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22680/sec/figures/main2.jpg",
                "caption": "Figure 14:Contextual Image Generation and Editing using VPTT-Bench.Each row shows a distinct user profile: assets and style cues (left), personalized generations (social post, cultural site), and edits (garden, living room) guided by the same persona identity. All images are generated synthetically via our Visual Personalization RAG (VPRAG) by text, which retrieves persona-aligned cues. To show cross model personalization here the assets are generated by QWEN-image-model[65]and generations and edits by Nano-Banana[23]conditioned only on the first image.",
                "position": 1301
            },
            {
                "img": "https://arxiv.org/html/2601.22680/sec/figures/more_comp.jpg",
                "caption": "Figure 15:Qualitative Comparison across Generation and Editing Tasks.Representative examples from the VPTT-Bench showing outputs from five methods: Baseline, Persona Only, BRAG, VPRAG (ours), and BRAG + VPRAG (ours).",
                "position": 1304
            }
        ]
    },
    {
        "header": "6Limitations and Future Work",
        "images": []
    },
    {
        "header": "7VPTT at scale",
        "images": []
    },
    {
        "header": "8Additional Results",
        "images": []
    },
    {
        "header": "9VLLM-Bench Construction (Detailed)",
        "images": []
    },
    {
        "header": "10Visual Assets Generation",
        "images": []
    },
    {
        "header": "11VPTT-Bench Stats",
        "images": []
    },
    {
        "header": "12VPRAG Algorithm",
        "images": []
    },
    {
        "header": "13Real-World Examples",
        "images": []
    },
    {
        "header": "14Expanded Tables.",
        "images": []
    },
    {
        "header": "15User Study Protocol",
        "images": []
    },
    {
        "header": "16Implementation Details",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]