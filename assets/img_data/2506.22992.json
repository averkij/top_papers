[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3𝕄𝕄\\mathbb{M}blackboard_MARBLE: a benchmark for multimodal spatial reasoning and planning",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.22992/x1.png",
                "caption": "Figure 1:Overview of the𝕄𝕄\\mathbb{M}blackboard_M-PortalDataset of the𝕄𝕄\\mathbb{M}blackboard_MARBLEBenchmark.\nIllustrated is a rather basic level Portal 2 problem, which only requires seven steps to solve. For comparison, the advanced problems introduced in this benchmark may involve several dozens of steps. Also, steps are not always decomposed into their most atomic form to keep enough complexity within a step to make mistaken steps harder to detect. AppendixAprovides more examples.",
                "position": 281
            },
            {
                "img": "https://arxiv.org/html/2506.22992/x2.png",
                "caption": "Figure 2:Data generation and evaluation pipeline for the𝕄𝕄\\mathbb{M}blackboard_M-Portaltask. The top row illustrates how a given Portal 2 map (sourced from the community test chambers) was analyzed with human annotation to produce a set of illustrative screenshots that fully depict the map, textual map instructions, a ground-truth solution chain of thought (CoT), as well as a set of five mistaken steps. The steps are designed to operate independently so that mistakes and correct steps can be easily combined. The bottom row indicates two evaluation types of𝕄𝕄\\mathbb{M}blackboard_M-Portal: first,plan correctness, a binary evaluation where candidate solutions have to be rated as correct or wrong. Second, afill-the-blanksevaluation, where multiple steps of the ground truth CoT solution are masked, and multiple options are available to fill in at the right place.",
                "position": 306
            },
            {
                "img": "https://arxiv.org/html/2506.22992/x3.png",
                "caption": "Figure 3:Overview of the𝕄𝕄\\mathbb{M}blackboard_M-Cubeworkflow including data generation, problem rendering, as well as solution validation. AppendixAprovides more dataset examples.",
                "position": 365
            },
            {
                "img": "https://arxiv.org/html/2506.22992/x4.png",
                "caption": "Figure 4:The influence of number of blanks to𝕄𝕄\\mathbb{M}blackboard_M-Portal.",
                "position": 522
            },
            {
                "img": "https://arxiv.org/html/2506.22992/x5.png",
                "caption": "Figure 5:Perception remains a bottleneck for𝕄𝕄\\mathbb{M}blackboard_M-Cube.Left: A perception task designed to test MLLM’s ability on retrieve structured information from visual input (full prompt in AppendixA) and example response of an MLLM.Right: Performance of8888MLLMs on this perception task based on200200200200test examples. Accuracy is measured both at individual cells and for the entire5×5555\\times 55 × 5piece. All the MLLMs perform poorly and completely fail on the full-piece accuracy.",
                "position": 678
            },
            {
                "img": "https://arxiv.org/html/2506.22992/x6.png",
                "caption": "Figure 6:Search space of the𝕄𝕄\\mathbb{M}blackboard_M-Cubedataset under different configurations.",
                "position": 693
            },
            {
                "img": "https://arxiv.org/html/2506.22992/x6.png",
                "caption": "Figure 6:Search space of the𝕄𝕄\\mathbb{M}blackboard_M-Cubedataset under different configurations.",
                "position": 696
            },
            {
                "img": "https://arxiv.org/html/2506.22992/x7.png",
                "caption": "Figure 7:Performance of DeepSeek-R1 across varying levels of task difficulty of the𝕄𝕄\\mathbb{M}blackboard_M-Cubedataset.",
                "position": 701
            },
            {
                "img": "https://arxiv.org/html/2506.22992/x8.png",
                "caption": "Figure 8:Performance of GPT-o4-mini onCUBE-easywith binary or detailed feedback from solution validator. OnCUBE, the performance will remain 0%.",
                "position": 716
            }
        ]
    },
    {
        "header": "4Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AIllustration of Example Problems",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.22992/x9.png",
                "caption": "Figure 9:Overview of the Portal-2 Dataset of the MARBLE-Benchmark.\nIllustrated is a rather basic level Portal 2 problem, which only requires seven steps to solve. For comparison, the advanced problems introduced in this benchmark may involve several dozens of steps. Also, steps are not always decomposed into their most atomic form to keep enough complexity within a step to make mistaken steps harder to detect.",
                "position": 1221
            },
            {
                "img": "https://arxiv.org/html/2506.22992/x10.png",
                "caption": "Figure 10:Illustration of an example problem of the𝕄𝕄\\mathbb{M}blackboard_M-Portaldataset (problem 5), composed of a problem description, images, solution steps, mistakes, and optional hint images.",
                "position": 1225
            },
            {
                "img": "https://arxiv.org/html/2506.22992/x11.png",
                "caption": "Figure 11:Illustration of𝕄𝕄\\mathbb{M}blackboard_M-CubeProblem.Top: Example input image and prompt of the problem.Bottom: Example solution to the problem (left) and corresponding 2D and 3D visualization (right). The visualization is not part of the inputs or outputs of the benchmark.",
                "position": 1237
            },
            {
                "img": "https://arxiv.org/html/2506.22992/x12.png",
                "caption": "Figure 12:Prompt for evaluating the perception ability of MLLMs on𝕄𝕄\\mathbb{M}blackboard_M-Cube.",
                "position": 1240
            },
            {
                "img": "https://arxiv.org/html/2506.22992/x13.png",
                "caption": "Figure 13:Example of MLLM using solution validator as a tool to gather feedback and iteratively refine its response on the𝕄𝕄\\mathbb{M}blackboard_M-Cubedataset.",
                "position": 1247
            }
        ]
    },
    {
        "header": "Appendix BExperiment Details.",
        "images": []
    }
]