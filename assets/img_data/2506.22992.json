[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3ğ•„ğ•„\\mathbb{M}blackboard_MARBLE: a benchmark for multimodal spatial reasoning and planning",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.22992/x1.png",
                "caption": "Figure 1:Overview of theğ•„ğ•„\\mathbb{M}blackboard_M-PortalDataset of theğ•„ğ•„\\mathbb{M}blackboard_MARBLEBenchmark.\nIllustrated is a rather basic level Portal 2 problem, which only requires seven steps to solve. For comparison, the advanced problems introduced in this benchmark may involve several dozens of steps. Also, steps are not always decomposed into their most atomic form to keep enough complexity within a step to make mistaken steps harder to detect. AppendixAprovides more examples.",
                "position": 281
            },
            {
                "img": "https://arxiv.org/html/2506.22992/x2.png",
                "caption": "Figure 2:Data generation and evaluation pipeline for theğ•„ğ•„\\mathbb{M}blackboard_M-Portaltask. The top row illustrates how a given Portal 2 map (sourced from the community test chambers) was analyzed with human annotation to produce a set of illustrative screenshots that fully depict the map, textual map instructions, a ground-truth solution chain of thoughtÂ (CoT), as well as a set of five mistaken steps. The steps are designed to operate independently so that mistakes and correct steps can be easily combined. The bottom row indicates two evaluation types ofğ•„ğ•„\\mathbb{M}blackboard_M-Portal: first,plan correctness, a binary evaluation where candidate solutions have to be rated as correct or wrong. Second, afill-the-blanksevaluation, where multiple steps of the ground truth CoT solution are masked, and multiple options are available to fill in at the right place.",
                "position": 306
            },
            {
                "img": "https://arxiv.org/html/2506.22992/x3.png",
                "caption": "Figure 3:Overview of theğ•„ğ•„\\mathbb{M}blackboard_M-Cubeworkflow including data generation, problem rendering, as well as solution validation. AppendixAprovides more dataset examples.",
                "position": 365
            },
            {
                "img": "https://arxiv.org/html/2506.22992/x4.png",
                "caption": "Figure 4:The influence of number of blanks toğ•„ğ•„\\mathbb{M}blackboard_M-Portal.",
                "position": 522
            },
            {
                "img": "https://arxiv.org/html/2506.22992/x5.png",
                "caption": "Figure 5:Perception remains a bottleneck forğ•„ğ•„\\mathbb{M}blackboard_M-Cube.Left: A perception task designed to test MLLMâ€™s ability on retrieve structured information from visual input (full prompt in AppendixA) and example response of an MLLM.Right: Performance of8888MLLMs on this perception task based on200200200200test examples. Accuracy is measured both at individual cells and for the entire5Ã—5555\\times 55 Ã— 5piece. All the MLLMs perform poorly and completely fail on the full-piece accuracy.",
                "position": 678
            },
            {
                "img": "https://arxiv.org/html/2506.22992/x6.png",
                "caption": "Figure 6:Search space of theğ•„ğ•„\\mathbb{M}blackboard_M-Cubedataset under different configurations.",
                "position": 693
            },
            {
                "img": "https://arxiv.org/html/2506.22992/x6.png",
                "caption": "Figure 6:Search space of theğ•„ğ•„\\mathbb{M}blackboard_M-Cubedataset under different configurations.",
                "position": 696
            },
            {
                "img": "https://arxiv.org/html/2506.22992/x7.png",
                "caption": "Figure 7:Performance of DeepSeek-R1 across varying levels of task difficulty of theğ•„ğ•„\\mathbb{M}blackboard_M-Cubedataset.",
                "position": 701
            },
            {
                "img": "https://arxiv.org/html/2506.22992/x8.png",
                "caption": "Figure 8:Performance of GPT-o4-mini onCUBE-easywith binary or detailed feedback from solution validator. OnCUBE, the performance will remain 0%.",
                "position": 716
            }
        ]
    },
    {
        "header": "4Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AIllustration of Example Problems",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.22992/x9.png",
                "caption": "Figure 9:Overview of the Portal-2 Dataset of the MARBLE-Benchmark.\nIllustrated is a rather basic level Portal 2 problem, which only requires seven steps to solve. For comparison, the advanced problems introduced in this benchmark may involve several dozens of steps. Also, steps are not always decomposed into their most atomic form to keep enough complexity within a step to make mistaken steps harder to detect.",
                "position": 1221
            },
            {
                "img": "https://arxiv.org/html/2506.22992/x10.png",
                "caption": "Figure 10:Illustration of an example problem of theğ•„ğ•„\\mathbb{M}blackboard_M-Portaldataset (problem 5), composed of a problem description, images, solution steps, mistakes, and optional hint images.",
                "position": 1225
            },
            {
                "img": "https://arxiv.org/html/2506.22992/x11.png",
                "caption": "Figure 11:Illustration ofğ•„ğ•„\\mathbb{M}blackboard_M-CubeProblem.Top: Example input image and prompt of the problem.Bottom: Example solution to the problem (left) and corresponding 2D and 3D visualization (right). The visualization is not part of the inputs or outputs of the benchmark.",
                "position": 1237
            },
            {
                "img": "https://arxiv.org/html/2506.22992/x12.png",
                "caption": "Figure 12:Prompt for evaluating the perception ability of MLLMs onğ•„ğ•„\\mathbb{M}blackboard_M-Cube.",
                "position": 1240
            },
            {
                "img": "https://arxiv.org/html/2506.22992/x13.png",
                "caption": "Figure 13:Example of MLLM using solution validator as a tool to gather feedback and iteratively refine its response on theğ•„ğ•„\\mathbb{M}blackboard_M-Cubedataset.",
                "position": 1247
            }
        ]
    },
    {
        "header": "Appendix BExperiment Details.",
        "images": []
    }
]