[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17243/x1.png",
                "caption": "Figure 1:GPU memory usage comparisonbetweenInf-CLand previous methods¬†(CLIP,OpenCLIP).\nThe dashed line marks the common GPU memory limit. Memory costs exceeding the bottleneck of 80G A800 are estimated by curve fitting.‚ù∂Left: With 8√ó\\times√óA800, CLIP and OpenCLIP‚Äôs memory consumption increases quadratically, while Inf-CL achieves linear growth, reducing memory costs byùüïùüñ√ó\\mathbf{78\\times}bold_78 √óat a batch size of 256k.‚ù∑Right: At a batch size of 1024k, even with 128 GPUs, previous methods exceed memory limits, whereas Inf-CL reduces memory demand byùüêùüñùüè√ó\\mathbf{281\\times}bold_281 √ó.",
                "position": 86
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17243/x2.png",
                "caption": "Figure 2:(a)Vanilla implementation of contrastive lossgathers features to all devices to calculate all similarity simultaneously, where the similarity with squared complexity are repeatedly stored in all devices, causing huge memory costs for loss calculation when batch size increases. (b)Our Inf-CLsignificant decreases the memory cost by serial and distributed tile-wise computation.",
                "position": 109
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17243/x3.png",
                "caption": "Figure 3:Multi-level tiling strategy.Top: for cross-GPU tiling, each GPU is assigned with multiple rows. The computation and the column-wise communication are performed asynchronously to reduce the cost.Bottom: for in-GPU tiling, the calculations in each GPU are further divided into tiles and the row-wise calculation is distributed to multiple CUDA cores. The accumulative operations of each row are merged into one kernel for reducing I/O times between SRAM and HBM.",
                "position": 365
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17243/x4.png",
                "caption": "Figure 4:Training Speed of ViT-L/14 CLIP on 8√ó\\times√óA800 for Varying Batch Sizes.The left figure shows the time per iteration step, while the right displays the time per epoch.\nLoss calculation contributes minimally to the total iteration time, making Inf-CL‚Äôs iteration time comparable to previous methods.\nFurthermore, the iteration time ofInf-CLscales linearly with batch size, leading to a stable training duration of approximately 59 hours per epoch.",
                "position": 1066
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17243/x5.png",
                "caption": "Figure 5:Performance of ViT-B/32 across Varying Batch Sizes. Except batch size, other experiment settings are consistent. In Figure, the most suitable batch size is increasing with data scale.",
                "position": 2422
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]