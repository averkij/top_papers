[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16828/x1.png",
                "caption": "Figure 1:Left:ThinkPRM-14B, trained on 8K process labels or 1K synthetic examples, outperforms discriminative PRMs trained on about 100x more data on ProcessBench(Zheng et¬†al.,2024).Right:ThinkPRM-1.5B, trained using the same 8K labels, outperforms LLM-as-a-judge and discriminative verifiers in reward-guided search on MATH-500. The LLM-as-a-judge in both figures uses the same base model asThinkPRM.",
                "position": 146
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16828/x2.png",
                "caption": "Figure 2:ThinkPRMsupports scaling verification compute by thinking longer.",
                "position": 162
            }
        ]
    },
    {
        "header": "2Background and Related Work",
        "images": []
    },
    {
        "header": "3ThinkPRM",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16828/x3.png",
                "caption": "Figure 4:Verifier performance on ProcessBench in light of CoT lengths. On the left, LLM-as-a-judge produces excessively long chains including repetition, infinite looping, and overthinking, leading to worse verifier performance since the output never terminates. Training on collected syntehtic data substantially reduces these issues as shown in theThinkPRMplot on the right.",
                "position": 305
            },
            {
                "img": "https://arxiv.org/html/2504.16828/x4.png",
                "caption": "Figure 5:Collecting verification chains for finetuning. First, we prompt a reasoning model, in our case QwQ-32B-Preview to critique a given solution to a problem. Then, we sample multiple verification chains, which we judge against gold process labels from PRM800K, only keeping chains that match the gold process labels.",
                "position": 325
            },
            {
                "img": "https://arxiv.org/html/2504.16828/x5.png",
                "caption": "Figure 6:LLM-as-a-judge suffers from a significant ratio of verification CoTs that do not terminate with a parsable label i.e.,\\boxed{yes}or\\boxed{no}. Our finetuning process that yieldsThinkPRM, substantially mitigates this issue. Both verifiers are based on R1-Distill-Qwen-14B.",
                "position": 347
            },
            {
                "img": "https://arxiv.org/html/2504.16828/x6.png",
                "caption": "",
                "position": 361
            }
        ]
    },
    {
        "header": "4Main Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16828/x7.png",
                "caption": "Figure 8:Best-of-N on AIME ‚Äô24 and MATH-500. Compared to LLM-as-a-judge, DiscPRM, and (unweighted) majority vote,ThinkPRM-14B exhibits best accuracy scaling curve.",
                "position": 397
            },
            {
                "img": "https://arxiv.org/html/2504.16828/x8.png",
                "caption": "Figure 9:Left:Comparison to Off-the-shelf PRMs trained on substantially more process labels thanThinkPRM.",
                "position": 407
            },
            {
                "img": "https://arxiv.org/html/2504.16828/x9.png",
                "caption": "",
                "position": 419
            },
            {
                "img": "https://arxiv.org/html/2504.16828/x10.png",
                "caption": "Figure 11:Best-of-N on two out-of-domain tasks: science QA (GPQA-Physics) and code generation (LiveCodeBench). AlthoughThinkPRMwas only finetuned on math, it exhibits superior OOD performance than the baselines, especially at larger sampling budgets, where the baselines performance falls short. Discriminative PRMs struggle despite being trained on order of magnitude more process labels.",
                "position": 463
            }
        ]
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16828/x11.png",
                "caption": "Figure 12:ThinkPRMhelps with challenging reasoning problems compared to DiscPRM. The generator model here is Qwen-2.5-14B for MATH-500 and Qwen-2.5-32B-Instruct for GPQA.",
                "position": 498
            }
        ]
    },
    {
        "header": "Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining data",
        "images": []
    },
    {
        "header": "Appendix BLLM-as-a-Judge Limitations",
        "images": []
    },
    {
        "header": "Appendix CTraining details",
        "images": []
    },
    {
        "header": "Appendix DResults on ProcessBench before and after finetuning",
        "images": []
    },
    {
        "header": "Appendix EEvaluation details",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16828/x12.png",
                "caption": "Figure 19:Histogram of difficulty levels in our 100-problem subset from MATH-500.",
                "position": 1913
            },
            {
                "img": "https://arxiv.org/html/2504.16828/x13.png",
                "caption": "",
                "position": 1925
            },
            {
                "img": "https://arxiv.org/html/2504.16828/x14.png",
                "caption": "Figure 21:Parallel vs. sequential scaling ofThinkPRMcompute under the same generation budget with Qwen-2.5-14B generator. Parallel scaling (model@K) is done by independently samplingKùêæKitalic_Kverification CoTs and aggregating their scores. Sequential scaling is done by prompting the modelKùêæKitalic_Ktimes to revise its own verification forKùêæKitalic_Kthinking rounds. Both setups generate up until 8192 tokens per generation. We do not observe a clear winner although parallel scaling seems slightly better especially at larger sampling budgets.",
                "position": 1938
            },
            {
                "img": "https://arxiv.org/html/2504.16828/x15.png",
                "caption": "",
                "position": 1942
            },
            {
                "img": "https://arxiv.org/html/2504.16828/x16.png",
                "caption": "Figure 22:Best-of-N results withThinkPRM-1.5B comparing the version trained on 1K examples (used throughout the paper) and a version trained on 65K examples.ThinkPRMbenefits from training on more synthetic data as the performance can further improve with more training.",
                "position": 1954
            },
            {
                "img": "https://arxiv.org/html/2504.16828/x17.png",
                "caption": "",
                "position": 1966
            }
        ]
    },
    {
        "header": "Appendix FMore details on LLM-as-judge verification",
        "images": []
    }
]