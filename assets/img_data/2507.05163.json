[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.05163/x1.png",
                "caption": "",
                "position": 81
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.05163/x2.png",
                "caption": "Figure 2:The overall pipeline of our model.Given several asynchronous multi-view videos, we first initialize a 4D Gaussian model for a specific iteration. We then employ an artifact-fix video diffusion model to refine the input videos. The refined videos are subsequently used to update the 4D Gaussian model.",
                "position": 138
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.05163/x3.png",
                "caption": "Figure 3:Illustration of artifact-fix video diffusion model setup.Our model freezes all parameters in the network, except for the LoRA weights, to fine-tune a video diffusion model. Precisely, we integrate Lora parameters into the DiT model. With a LoRA rank designated as 16, this integration takes place in each transformer block.",
                "position": 226
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.05163/x4.png",
                "caption": "Figure 4:Qualitative result on the Neural3DV dataset[8].",
                "position": 288
            },
            {
                "img": "https://arxiv.org/html/2507.05163/x5.png",
                "caption": "Figure 5:Qualitative result on the DNA-Rendering dataset[1].",
                "position": 291
            },
            {
                "img": "https://arxiv.org/html/2507.05163/x6.png",
                "caption": "Figure 6:Illustration of the capture system.",
                "position": 324
            },
            {
                "img": "https://arxiv.org/html/2507.05163/x7.png",
                "caption": "Figure 7:Quality result on our real-capture dataset.",
                "position": 327
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.05163/x8.png",
                "caption": "Figure 8:Ablation study on asynchronous capture and diffusion model (AF represents artifact-fix video diffusion model).",
                "position": 494
            },
            {
                "img": "https://arxiv.org/html/2507.05163/x9.png",
                "caption": "Figure 9:Ablation study on video diffusion model and image diffusion model.",
                "position": 513
            },
            {
                "img": "https://arxiv.org/html/2507.05163/x10.png",
                "caption": "Figure 10:Ablation study on per-scene finetuning.",
                "position": 523
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]