[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.16926/x1.png",
                "caption": "Figure 1:Results of various sample selection approaches in many-shot ICL with LCLMs. Approaches include Retrieval that selects examples similar to the target query, Diversity that aims for maximizing example variety, Curriculum that arranges examples in order from easiest to hardest, and Hard that uses only challenging examples, alongside Random that selects examples without any constraints. Results indicate that sample selection methods provide no significant improvement over the naive (random) approach and sometimes perform worse. Meanwhile, Augmentation refers to the approach that generates additional demonstrations and uses them along with original samples for ICL, for low-resource tasks (such as translation, reasoning, and classification) that do not contain enough samples to utilize the full capacity of LCLMs, showing substantial performance gains.",
                "position": 165
            }
        ]
    },
    {
        "header": "2Examining Sample Selection Methods for In-Context Learning with LCLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.16926/x2.png",
                "caption": "Figure 2:Detailed results of various sample selection approaches on ICL with LCLMs, such as Gemini Pro (Top), Gemini Flash (Middle), and Llama 3.1 (Bottom), across four different tasks (translation, summarization, reasoning, and extreme classification) with 18 datasets. Each bar represents the averaged performance, with the upper and lower limits indicating standard deviation.",
                "position": 244
            },
            {
                "img": "https://arxiv.org/html/2412.16926/x3.png",
                "caption": "",
                "position": 248
            },
            {
                "img": "https://arxiv.org/html/2412.16926/x4.png",
                "caption": "",
                "position": 254
            },
            {
                "img": "https://arxiv.org/html/2412.16926/x5.png",
                "caption": "",
                "position": 256
            },
            {
                "img": "https://arxiv.org/html/2412.16926/x6.png",
                "caption": "",
                "position": 262
            },
            {
                "img": "https://arxiv.org/html/2412.16926/x7.png",
                "caption": "",
                "position": 264
            },
            {
                "img": "https://arxiv.org/html/2412.16926/x8.png",
                "caption": "Figure 3:Results with varying the number of examples for ICL with Gemini Pro, where we average the results for each task.",
                "position": 462
            }
        ]
    },
    {
        "header": "3Augmenting ICL Demonstrations to Increase Context Capacity of LCLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.16926/x9.png",
                "caption": "Table 3:Results of LCLM-enabled ICL on four different tasks, where Random indicates the naive sample selection approach without selection criteria, Best Selection indicates the model that achieves the best performance among sophisticated sample selection methods for each experiment unit, and Augmentation indicates the proposed approach that generates demonstrations and uses them alongside original samples with random selection. We emphasize statistically significant results over Random in bold. We exclude Llama from the augmentation scenario as its context capacity is approximately ten times smaller than that of Gemini, allowing it to fully utilize its available context with the original examples alone, making augmentation unnecessary.",
                "position": 529
            },
            {
                "img": "https://arxiv.org/html/2412.16926/x9.png",
                "caption": "Figure 4:Results with varying the ratio of noisy examples within the context of LCLMs, where we report the relative performance over the ICL without noisy examples (i.e., the noise ratio of 0) and the results are averaged over multiple runs.",
                "position": 828
            },
            {
                "img": "https://arxiv.org/html/2412.16926/x10.png",
                "caption": "Figure 5:Results across different percentages of context size utilized in LCLMs, where the x-axis represents the percentage of the full LCLM context used (according to the number of tokens over the full token length), and the y-axis shows the relative performance compared to the highest performance achieved for each dataset. Results are averaged over multiple runs.",
                "position": 831
            }
        ]
    },
    {
        "header": "4Behaviors of LCLM-Enabled ICL",
        "images": []
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APrompts",
        "images": []
    }
]