[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11733/x1.png",
                "caption": "Figure 1:A schematic of theMedCaseReasoningcase processing pipeline.A: From the initial set of 98,994 PMC Open Subset Case Reports, we select 28,313 appropriate candidate cases, which are converted to QA format and then filtered for quality. The resulting 14,489 cases form theMedCaseReasoningdataset, with 897 test cases and 13,092 training cases. A full example is available in Table8.B: For each test case, the case presentation is posed to an LLM to reason and then answer with a diagnosis. The clinician-authored diagnostic reasoning is compared with the LLM-generated reasoning to produce the reasoning recall score, and the case diagnosis is compared with the predicted diagnosis to produce diagnostic accuracy.C: The original case report text is converted into three sections: the case presentation that contains the relevant and sufficient patient information for making a diagnosis; the diagnostic reasoning that includes the diagnostic decision-making by the case author (in enumerated statements); and the final diagnosis.",
                "position": 112
            },
            {
                "img": "https://arxiv.org/html/2505.11733/x2.png",
                "caption": "Figure 2:(Left) Evaluation of LLMs on theMedCaseReasoningtest dataset (N = 897), along with diagnostic accuracy and reasoning recall percentage.\n(Right) Model performance on theMedCaseReasoningtest set and NEJM case studies (N = 302), showing a strong correlation between the two benchmarks.\nIn both plots, accuracy is computed with 10-shot accuracy; circle size encodes the average length of the model-generated reasoning trace. LLaMA-3.1-8B and Qwen-2.5-7B are both Instruct variants (omitted for brevity).",
                "position": 167
            },
            {
                "img": "https://arxiv.org/html/2505.11733/x3.png",
                "caption": "",
                "position": 170
            }
        ]
    },
    {
        "header": "2Methods",
        "images": []
    },
    {
        "header": "3Results",
        "images": []
    },
    {
        "header": "4Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "5Appendix",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11733/x4.png",
                "caption": "Figure 3:Comparison of length of questions from MedQA vs.MedCaseReasoning. Diagnostic case prompts are, on average, 2.5x longer inMedCaseReasoningand contain real patient information vs. synthetic case vignettes.",
                "position": 1269
            },
            {
                "img": "https://arxiv.org/html/2505.11733/x5.png",
                "caption": "Figure 4:Distribution of dates of publication for PMC case reports used inMedCaseReasoning. Cases are largely from recent dates (after 2020), and over 500 cases are after Jan 1 2025.",
                "position": 1272
            }
        ]
    },
    {
        "header": "NeurIPS Paper Checklist",
        "images": []
    }
]