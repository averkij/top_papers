[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08765/figures/logo.png",
                "caption": "",
                "position": 79
            },
            {
                "img": "https://arxiv.org/html/2512.08765/x1.png",
                "caption": "Figure 1:Wan-Move is a image-to-video generation framework that supports diverse motion control applications. The generated samples (832×\\times480p, 5s) exhibits high visual fidelity and accurate motion.",
                "position": 98
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08765/x2.png",
                "caption": "Figure 2:(a) To inject motion guidance, we transfer point trajectories from videos to latent space, then replicate the first frame feature into subsequent zero-padded frames along each latent trajectory. (b) Wan-Move is trained upon an existing image-to-video generation model (e.g., workwan;bar2024lumiere), with an efficient latent feature replication step (as in (a)) to update the condition feature. The CLIPradford2021clipimage encoder and umT5chung2023umt5text encoder from the base model are omitted for simplicity.",
                "position": 216
            }
        ]
    },
    {
        "header": "4MoveBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08765/x3.png",
                "caption": "Figure 3:Construction pipeline of MoveBench to obtain high-quality samples with rich annotations.",
                "position": 315
            },
            {
                "img": "https://arxiv.org/html/2512.08765/x4.png",
                "caption": "Figure 4:Balanced sample number per class.",
                "position": 318
            },
            {
                "img": "https://arxiv.org/html/2512.08765/x4.png",
                "caption": "Figure 4:Balanced sample number per class.",
                "position": 321
            },
            {
                "img": "https://arxiv.org/html/2512.08765/x5.png",
                "caption": "Figure 5:Comparison with related benchmarks.",
                "position": 326
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08765/x6.png",
                "caption": "Figure 6:Qualitative comparisons between Wan-Move and recent approaches, including both academic methodsli2025image;wang2024levitorand commercial solutionskuaishou2024kling. Motions that deviate from the specified trajectories and major visual artifacts are marked with red dots and boxes, respectively.",
                "position": 459
            },
            {
                "img": "https://arxiv.org/html/2512.08765/x7.png",
                "caption": "Figure 7:Qualitative comparisons with MagicMotionli2025magicmotion, controlling motions using sparse signals (i.e., bounding boxes) as input. Motions that break the guidance are marked with red dots.",
                "position": 462
            },
            {
                "img": "https://arxiv.org/html/2512.08765/x8.png",
                "caption": "Figure 8:Visualization of various guidance strategies.",
                "position": 707
            },
            {
                "img": "https://arxiv.org/html/2512.08765/x9.png",
                "caption": "Figure 9:I2V results of Wan-Move (no point tracks).",
                "position": 726
            }
        ]
    },
    {
        "header": "6Conclusion and Discussion",
        "images": []
    },
    {
        "header": "7Acknowledgment",
        "images": []
    },
    {
        "header": "Contents",
        "images": []
    },
    {
        "header": "8Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08765/x10.png",
                "caption": "Table 10:The statistics of the training datasets.",
                "position": 1073
            },
            {
                "img": "https://arxiv.org/html/2512.08765/x10.png",
                "caption": "Figure 10:Prompt for video caption.",
                "position": 1109
            },
            {
                "img": "https://arxiv.org/html/2512.08765/x11.png",
                "caption": "Figure 11:The interactive annotation interface displays the video (left) and its first frame (right). Users click green positive points to specify the start point of a motion trajectory, and red negative points to exclude irrelevant regions if needed. SAM segments the mask of moving objects & regions for user review. To annotate multiple motion trajectories, users must assign different object IDs.",
                "position": 1127
            }
        ]
    },
    {
        "header": "9Additional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08765/x12.png",
                "caption": "Figure 12:Wan-Move generalizes to continue controlling motion trajectories when they temporarily disappears.\nGreen circles indicate visible segments, while red circles mark invisible segments,e.g., occluded or out-of-frame parts.",
                "position": 1246
            },
            {
                "img": "https://arxiv.org/html/2512.08765/x13.png",
                "caption": "Figure 13:Three primary failure modes of Wan-Move. (a) Loss of motion control due to persistent trajectory disappearance; (b) Visual artifacts in overly complex, crowded environments; and (c) motion outputs that violate rigorous physical laws.",
                "position": 1257
            },
            {
                "img": "https://arxiv.org/html/2512.08765/x14.png",
                "caption": "Figure 14:Additional qualitative comparisons with Torazhang2024toraand the commercial model Kling 1.5 Prokuaishou2024kling. Wan-Move demonstrates superior motion accuracy and visual quality. Major motion control failures or visual artifacts are denoted with red boxes.",
                "position": 1271
            },
            {
                "img": "https://arxiv.org/html/2512.08765/x15.png",
                "caption": "Figure 15:Wan-Move enables effective and flexible camera control through different point trajectories, such as linear displacement, dolly in, and dolly out.",
                "position": 1281
            },
            {
                "img": "https://arxiv.org/html/2512.08765/x16.png",
                "caption": "Figure 16:Wan-Move enables accurate video motion copy using dense point trajectories (e.g., 1024 points). The synthesized video preserves high fidelity in both appearance and object-level motion alignment with the original video, even under complex environmental conditions.",
                "position": 1291
            },
            {
                "img": "https://arxiv.org/html/2512.08765/x17.png",
                "caption": "Figure 17:Wan-Move enables video editing through motion copy and additional image editing models. It first applies the image editing model (e.g., ControlNetcontrolnet, GPT-4ohurst2024gpt) to modify the style or content of the first frame, then uses the original video’s motion trajectories to animate the edited image frame.",
                "position": 1294
            },
            {
                "img": "https://arxiv.org/html/2512.08765/x18.png",
                "caption": "Figure 18:Wan-Move enables object 3D rotation by estimating depth-based positions, applying a rotation, and projecting the results to 2D.",
                "position": 1304
            }
        ]
    },
    {
        "header": "10Qualitative Visualizations",
        "images": []
    }
]