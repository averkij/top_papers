[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.17768/x1.png",
                "caption": "Figure 1:Overview of sparse attention methods for prefilling (left) and generation (right). These methods differ in the units of sparsification (blocks or pages vs. verticals and slashes), importance estimation, and KV cache management strategies. Colours represent query–key interactions preserved at different sparsity levels, while white areas indicate interactions that are not computed.",
                "position": 279
            }
        ]
    },
    {
        "header": "2Training-Free Sparse Attention",
        "images": []
    },
    {
        "header": "3Experimental Setup",
        "images": []
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.17768/x2.png",
                "caption": "(a)IsoFLOPS analysis for prefilling, using Vertical-Slash pattern.",
                "position": 600
            },
            {
                "img": "https://arxiv.org/html/2504.17768/x2.png",
                "caption": "(a)IsoFLOPS analysis for prefilling, using Vertical-Slash pattern.",
                "position": 603
            },
            {
                "img": "https://arxiv.org/html/2504.17768/x3.png",
                "caption": "(b)IsoFLOPS analysis for decoding, using Quest pattern.",
                "position": 609
            },
            {
                "img": "https://arxiv.org/html/2504.17768/x4.png",
                "caption": "Figure 3:Maximum compression ratio with statistically significant performance retention (y-axis) across different model sizes (colours) and sequence lengths (x-axis). Each point represents a task, with horizontal bars showing the average maximum compression across tasks and vertical bars indicating standard deviation.Left: Vertical-Slash pattern for prefilling.Right: Quest pattern for decoding. The key conclusion is that decoding tolerates higher compression than prefilling on average, with larger models maintaining performance even at very high compression ratios. However, almost every configuration has at least one task where maximum tolerable compression is below 5×\\times×(72B Quest being the only exception).",
                "position": 638
            },
            {
                "img": "https://arxiv.org/html/2504.17768/x5.png",
                "caption": "Figure 4:Performance comparison of different sparse attention methods across 9 tasks, aggregated over sequence lengths and models (shaded areas indicate the standard error).Top: prefilling methods (Vertical-Slash, FlexPrefill, Block-Sparse).Bottom: decoding methods (SnapKV, Ada-SnapKV, Quest). Each subplot shows the relationship between performance and compression for a specific task. The trade-off appears extremely task-dependent. Overall, Vertical-Slash performs best among prefilling methods, while Quest performs best among decoding methods.",
                "position": 657
            },
            {
                "img": "https://arxiv.org/html/2504.17768/x6.png",
                "caption": "",
                "position": 661
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperimental details",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.17768/x7.png",
                "caption": "Figure 5:Block-Sparse block size.",
                "position": 2075
            },
            {
                "img": "https://arxiv.org/html/2504.17768/x7.png",
                "caption": "Figure 5:Block-Sparse block size.",
                "position": 2078
            },
            {
                "img": "https://arxiv.org/html/2504.17768/x8.png",
                "caption": "Figure 6:Quest page size.",
                "position": 2083
            },
            {
                "img": "https://arxiv.org/html/2504.17768/x9.png",
                "caption": "Figure 7:Ada-SnapKV min budget.",
                "position": 2089
            },
            {
                "img": "https://arxiv.org/html/2504.17768/x9.png",
                "caption": "Figure 7:Ada-SnapKV min budget.",
                "position": 2092
            },
            {
                "img": "https://arxiv.org/html/2504.17768/x10.png",
                "caption": "Figure 8:FlexPrefill min budget.",
                "position": 2097
            },
            {
                "img": "https://arxiv.org/html/2504.17768/x11.png",
                "caption": "Figure 9:SnapKV/Ada-SnapKV approximation window.",
                "position": 2103
            },
            {
                "img": "https://arxiv.org/html/2504.17768/x11.png",
                "caption": "Figure 9:SnapKV/Ada-SnapKV approximation window.",
                "position": 2106
            },
            {
                "img": "https://arxiv.org/html/2504.17768/x12.png",
                "caption": "Figure 10:SnapKV/Ada-SnapKV kernel size.",
                "position": 2111
            },
            {
                "img": "https://arxiv.org/html/2504.17768/x13.png",
                "caption": "Figure 11:Vertical-Slash approximation window ablation per task.",
                "position": 2117
            }
        ]
    },
    {
        "header": "Appendix BOverview of Scaling Laws Studies",
        "images": []
    },
    {
        "header": "Appendix CSparse Attention Scaling Laws: Formulation and Fitting Details",
        "images": []
    },
    {
        "header": "Appendix DFLOPS breakdown",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.17768/x14.png",
                "caption": "Figure 12:Computational cost of Qwen models (7B to 72B) across sequence lengths from 16K to 128K. Top: Prefilling FLOPS (TFLOPS) showing both attention (solid lines) and non-attention components (dashed lines). Bottom: Decoding FLOPS (GFLOPS) with similar decomposition. Where the lines cross shows the sequence length at which attention FLOPS equal non-attention FLOPS.",
                "position": 2468
            }
        ]
    },
    {
        "header": "Appendix EPrompt Template",
        "images": []
    },
    {
        "header": "Appendix FExample Story Narrative",
        "images": []
    },
    {
        "header": "Appendix GExample Task Inputs",
        "images": []
    }
]