[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07587/extracted/6265895/figures/Fig0_Clean.png",
                "caption": "Figure 1:As multi-modal foundation models start being tested for Autonomous Driving applications, we inquire their cognitive alignment under a Visual Question Answering scheme of multiple videos comparing the answers of VLMs to those of Humans with tools from systems neuroscience. For this example in particular, a closer look reveals that the policeman is telling the driver to run through the red light. These sort of edge-case scenarios allow us to better probe cognitive\nalignment.",
                "position": 108
            },
            {
                "img": "https://arxiv.org/html/2503.07587/extracted/6265895/figures/Fig1.png",
                "caption": "Figure 2:Overview of the VQA procedure on the Robusto-1 Dataset. A set of 5 second clips are seen by ground truth anotators (authors) and Meta-Tags are extracted from 16 different categories. These are then passed per each video to a “Blind Oracle” LLM that formulates a set of 5 variable questions per clip. An addition 5 set of multiple choice questions that have Yes/No answers and or involved rating or counting, and 5 open counterfactual questions are added to the total pool of 15 questions per clip. We then ask a group of VLMs and Humans these questions to collect their answers.",
                "position": 112
            }
        ]
    },
    {
        "header": "2Dataset Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07587/extracted/6265895/figures/Fig2Composite.png",
                "caption": "Figure 3:A figure that shows how to calculate the System Similarity Matrix through Model Gramians as done in Representational Similarity Analysis (RSA)[35]. A) We transform each answer into a vector through an embedding to later calculate each system’s Gramian. Upper triangular parts of the Gramians across two systems are then correlated (violet). This can be applied to both humans and VLMs. B) The system similarity matrixℳℳ\\mathcal{M}caligraphic_Mcalculated over all humans and machines allows us to get an idea of how each system is similar to one another. A cartoon with no real values is shown in this diagram.",
                "position": 191
            }
        ]
    },
    {
        "header": "3Comparing Humans and VLMs through Systems Neuroscience",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07587/extracted/6265895/figures/RSA_Results_Big.png",
                "caption": "Figure 4:The first general result we find after applying Representational Similarity Analysis (RSA) to responses of both humans and VLMs, is that system convergence and divergence is modulated by the type of questions asked. Broadly speaking, we find that all VLMs respond very similar to each other independent of the types of questions asked with a surprisingly high correlation for counterfactuals & hypotheticals. Humans on the other hand diverge heavily for counterfactual & hypotheticals and converge strongly for multiple-choice.",
                "position": 338
            },
            {
                "img": "https://arxiv.org/html/2503.07587/extracted/6265895/figures/Distance_Plots_Final_Curves.png",
                "caption": "Figure 5:In this figure we show the distance of each response per question across all systems to the median response. Responses placed here for the VLM was the average response per question rather than a single response. We generally observe that the overlap across the answers for VLMs and Humans shifts depending on the nature of the questions asked with a larger partial overlap for block 2 given the nature of multiple- choice questions and the smaller space that answers can space as they are prefixed. Variance for block 3 on the other hand is larger across humans and VLMs given the complexity of counterfactual & hypothetical questions.",
                "position": 353
            },
            {
                "img": "https://arxiv.org/html/2503.07587/extracted/6265895/figures/Embeddings_Projections_Final.png",
                "caption": "Figure 6:A collection of 2D projections of the answers from both Humans (violet) and VLMs (green) divided by Blocks. We notice that Humans and VLMs have partial overlap but they generally answer questions very differently, and these vary depending on the type of questions asked. These average embeddings per answer also highlight the variability of answers VLMs have to the same questions.",
                "position": 373
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Acknowledgements",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07587/extracted/6265895/figures/collage_test_capacity.jpg",
                "caption": "Figure 7:Images used to analyze the model’s temporal understanding.",
                "position": 1684
            },
            {
                "img": "https://arxiv.org/html/2503.07587/extracted/6265895/figures/collage_MiniVideos.jpg",
                "caption": "Figure 8:A collection of sample frames from 7 held-out videos used in our experiments form the Robusto-1 dataset. There is a combination of rural and urban scenes that humans and VLMs view. This preliminary study focused only on showing humans and machines 7 videos, but the dataset is composed of 200 additional videos (See Supplement) that we are releasing to the public for further research and experiments.",
                "position": 2407
            },
            {
                "img": "https://arxiv.org/html/2503.07587/extracted/6265895/figures/Supplement_RSA_All_Cured.png",
                "caption": "Figure 9:A collection of all the RSA plots for the 3 different types of embeddings used in the paper (all-mpnet, paraphrase-mpnet, e5-large). We observe that the pattern of results stays of our initial analysis stays the same with different levels of intensity.",
                "position": 2410
            },
            {
                "img": "https://arxiv.org/html/2503.07587/extracted/6265895/figures/Supplement_RSA_One_Cured.png",
                "caption": "Figure 10:In this graph however we show how the RSA results would have looked like if we had just used one response rather than pooled (averaged) several observations per answer (also for all embeddings: all-mpnet, paraphrase-mpnet, e5-large). We find a very similar trend to the pooled responses for the VLMs. Though it would appear that pooling answers shows greater level of convergence across VLMs.",
                "position": 2413
            },
            {
                "img": "https://arxiv.org/html/2503.07587/extracted/6265895/figures/Distance_Plots_Supp1.png",
                "caption": "Figure 11:The Distance to the Median comparison of using a pooled vs single embedding is used across all systems (in particular the VLM). The same pattern of results holds for all-mpnet.",
                "position": 2416
            },
            {
                "img": "https://arxiv.org/html/2503.07587/extracted/6265895/figures/Distance_Plots_Supp2.png",
                "caption": "Figure 12:The Distance to the Median comparison of using a pooled vs single embedding is used across all systems (in particular the VLM). The same pattern of results holds for paraphrase-mpnet.",
                "position": 2419
            },
            {
                "img": "https://arxiv.org/html/2503.07587/extracted/6265895/figures/Distance_Plots_Supp3.png",
                "caption": "Figure 13:The Distance to the Median comparison of using a pooled vs single embedding is used across all systems (in particular the VLM). The same pattern of results holds for e5-net.",
                "position": 2422
            },
            {
                "img": "https://arxiv.org/html/2503.07587/extracted/6265895/figures/PCA_Supp_1.png",
                "caption": "Figure 14:The PCA visualization of a comparison of using a pooled vs single embedding is used across all systems (in particular the VLM). The same pattern of results holds for all-mpnet.",
                "position": 2425
            },
            {
                "img": "https://arxiv.org/html/2503.07587/extracted/6265895/figures/PCA_Supp_2.png",
                "caption": "Figure 15:The PCA visualization of a comparison of using a pooled vs single embedding is used across all systems (in particular the VLM). The same pattern of results holds for paraphrase-mpnet.",
                "position": 2428
            },
            {
                "img": "https://arxiv.org/html/2503.07587/extracted/6265895/figures/PCA_Supp_3.png",
                "caption": "Figure 16:The PCA visualization of a comparison of using a pooled vs single embedding is used across all systems (in particular the VLM). The same pattern of results holds for e5-net.",
                "position": 2431
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]