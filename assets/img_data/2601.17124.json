[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.17124/x1.png",
                "caption": "Figure 1:Empirical analysis of equal-probability and equal-interval quantization.Since neural network activations naturally follow a gaussian-like distributionLee et al. (2017), we begin our experiments under this setting. For panels (a)–(f), we quantize the original data into 9 levels, which corresponds to an information entropy of 3.17 bits. We clip the original data to the range[−3,3][-3,3]for visualization. Panel (a) shows equal-interval quantization of a standard normal distribution. Panel (b) shows equal-probability quantization of a standard normal distribution. Panel (c) shows equal-interval quantization of a uniform distribution, which is also equal-probability quantization. For panels (a)–(c), the area of each bin represents probability density. In panels (b) and (c), all bins have equal probability, while in panel (a), bins near the mean (0) have higher probability density. Panels (d)–(f) plot the quantization error for each original value (x-axis), where denser regions use larger markers.",
                "position": 120
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.17124/x2.png",
                "caption": "Figure 2:Empirical numerical study of2.0⋅sigmoid​(α​x)−1.02.0\\cdot\\mathrm{sigmoid}(\\alpha x)-1.0.Sample 500k points from the standard normal distribution and compute the transformed distribution for several values ofα\\alpha. Panel (a) shows the probability density of2.0⋅sigmoid​(α​x)−1.02.0\\cdot\\mathrm{sigmoid}(\\alpha x)-1.0under differentα\\alphavalues, and the panel (b) reports the similarity to a uniform distribution measured by KS and RMSE asα\\alphavaries. Notably, the case witht​a​n​h​(α=2.0)tanh(\\alpha=2.0)corresponds to the original FSQ.",
                "position": 181
            }
        ]
    },
    {
        "header": "3iFSQ",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.17124/x3.png",
                "caption": "Figure 3:Effect ofα\\alphafor iFSQ.In (a)–(c), the x-axis denotesα\\alpha. The primary y-axes show PSNR (↑ better), SSIM (↑ better), and LPIPS (↓ better), respectively. The secondary y-axis shows distribution metrics RMSE and KS (both ↓ better). The optimal choice atα=1.6\\alpha=1.6is highlighted, and tanh performance atα=2.0\\alpha=2.0is marked, which corresponds to the original FSQ.",
                "position": 403
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.17124/x4.png",
                "caption": "Figure 4:Training Efficiency Comparison: DiT vs LlamaGen (FID vs Compute).At 256 resolution, DiT-Large and LlamaGen-L exhibit approximately 161.04G and 169.65G FLOPs, respectively. Both models employ optimal training configurations derived from ablation studies while sharing the same iFSQ.",
                "position": 660
            },
            {
                "img": "https://arxiv.org/html/2601.17124/x5.png",
                "caption": "Figure 5:Performance across quantization levels.We plot the performance of iFSQ and AE under different quantization levels, using larger markers to denote models with higher latent dimensionality. Each performance point (including AE) uses a spatial compression factor of 64×\\times. The performance of AE is indicated by horizontal dashed lines, which train under mixed precision and use 16-bit precision to inference.",
                "position": 673
            },
            {
                "img": "https://arxiv.org/html/2601.17124/x6.png",
                "caption": "Figure 6:Layer-wise analysis of LlamaGen models.The top row shows the evolution of STS, NTS, and CKNNA across normalized layer indices. The bottom row demonstrates the strong correlation between NTS and CKNNA, particularly at higher resolutions and model scales. The shaded regions indicate the 90% confidence intervals.",
                "position": 748
            },
            {
                "img": "https://arxiv.org/html/2601.17124/x7.png",
                "caption": "Figure 7:Layer-wise CKNNA scores under different REPA alignment depths (1, 4, 8, and 12).The vertical dashed lines denote the alignment layers, and bubble sizes correspond to the training coefficientλ\\lambda. The peak semantic alignment consistently synchronizes with the target alignment depth.",
                "position": 751
            },
            {
                "img": "https://arxiv.org/html/2601.17124/x8.png",
                "caption": "Figure 8:Ablation study on alignment depth across different architectures and scales.The left panel shows LlamaGen (AR) results, and the right panel shows DiT (Diffusion) results. The annotations indicate the specific alignment layer relative to the total network depth (layer/total). The optimal alignment depth consistently scales to approximately 1/3 of the total network depth (highlighted in the optimal region), rather than remaining at a fixed layer index.",
                "position": 882
            },
            {
                "img": "https://arxiv.org/html/2601.17124/x9.png",
                "caption": "Figure 9:Extended Experiments onλ\\lambda.During training, we record all evaluation FID scores and plot them as boxplots.",
                "position": 894
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ABackground: Tokenizer for Generation",
        "images": []
    },
    {
        "header": "Appendix BBackground: Compression Ratio Analysis",
        "images": []
    },
    {
        "header": "Appendix CScaling of Compression Ratio with iFSQ",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.17124/x10.png",
                "caption": "Figure 10:Scalability of iFSQ.We plot the scaling law of performance with respect to the compression ratio. Each performance point (including AE) uses a spatial compression factor of 256×\\times. The performance of AE is indicated by “×\\times”, which train under mixed precision and use 16-bit precision to inference. The compression ratio (x-axis) is on a logarithmic scale and compression ratio of VQ (⋆\\starplotted in figure) is about 438.",
                "position": 1627
            }
        ]
    },
    {
        "header": "Appendix DTokenizer Setup",
        "images": []
    }
]