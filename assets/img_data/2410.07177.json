[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07177/x1.png",
                "caption": "Figure 1:We introduce a foundation model for egocentric video understanding, contributing from three key perspectives: (a) a data engine that can automatically transform human narrations into 7 million egocentric QA samples, (b) a multimodal language model designed for egocentric video comprehension, and (c) the curation of a challenging egocentric video understanding benchmark.",
                "position": 97
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07177/x2.png",
                "caption": "Figure 2:‚ÄúNarration to Egocentric QA‚Äù data engine. Given a sequence of human-annotated video narrations, we instruct a language model (GPT-4o) to generate egocentric understanding-related questions and answers, along with identifying the key frames necessary to answer those questions.",
                "position": 154
            },
            {
                "img": "https://arxiv.org/html/2410.07177/x3.png",
                "caption": "Figure 3:Video length distribution in our egocentric QA dataset.",
                "position": 164
            },
            {
                "img": "https://arxiv.org/html/2410.07177/x4.png",
                "caption": "Figure 4:(a)Overview of the proposed Memory Pointer Prompting mechanism.\nIts inference consists of two steps:\n(1)Global Glimpse: We concatenate the compressed visual embeddings from all frames, denoted asùêÑvisisuperscriptsubscriptùêÑvisùëñ\\mathbf{E}_{\\text{vis}}^{i}bold_E start_POSTSUBSCRIPT vis end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPTfori‚àà[1,N]ùëñ1ùëÅi\\in[1,N]italic_i ‚àà [ 1 , italic_N ], with the question embeddingsùêÑque1superscriptsubscriptùêÑque1\\mathbf{E}_{\\text{que}}^{1}bold_E start_POSTSUBSCRIPT que end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPTand the memory pointer embeddingùêè1superscriptùêè1\\mathbf{P}^{1}bold_P start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT. This combined embedding sequence is then input into the LLM. From the last layer, we extract embeddings and compute the dot product between the memory pointer embedding and all compressed visual embeddings to generate the correlation scores. The indices of the frames with the topkùëòkitalic_kscores are selected. During training, the correlation scores are supervised by ground-truth key frame indices via a binary cross-entropy loss.\n(2)Fallback: The high-resolution visual embeddings corresponding to the selected indices are fed into the LLM along with the question embeddings for final processing and response generation.(b)Illustration of LLM input sequence during training.",
                "position": 208
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07177/x5.png",
                "caption": "Figure 5:The most frequently occurring verbs and nouns in EgoMemoria.",
                "position": 440
            },
            {
                "img": "https://arxiv.org/html/2410.07177/x6.png",
                "caption": "Figure 6:EgoMemoria QAs visualization and prediction analysis of the global glimpse step. We find high consistency between the identified key frames and the questions, demonstrating the effectiveness of the proposed Memory Pointer Prompting method.\nThe visualized correlation scores show distinct distributions for different questions given the same video, indicating its question-specific nature. The‚úìindicates that the selected frames are relevant to the questions.",
                "position": 455
            },
            {
                "img": "https://arxiv.org/html/2410.07177/x7.png",
                "caption": "Figure 7:MDA scores with differentŒ±ùõº\\alphaitalic_Œ±values for explore-and-exploit balancing.",
                "position": 799
            },
            {
                "img": "https://arxiv.org/html/2410.07177/x8.png",
                "caption": "Figure 8:Real-world conversation examples generated by MM-Ego. The input is a 2-minute long egocentric video recorded using a camera on an off-the-shelf wearable device. MM-Ego can accurately identify key visual details and provide correct answers to the user‚Äôs memory-related questions.",
                "position": 834
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07177/x9.png",
                "caption": "Figure 9:More key frame identification results of the global glimpse step on EgoMemoria. We find high relevance between the identified key frames and the questions, demonstrating the effectiveness of the proposed Memory Pointer Prompting method.\nThe‚úìindicates that the selected frames are relevant to the questions.",
                "position": 1564
            }
        ]
    },
    {
        "header": "Appendix AMore Analysis of Memory Pointer Prompting",
        "images": []
    }
]