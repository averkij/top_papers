[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11075/x1.png",
                "caption": "",
                "position": 131
            }
        ]
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11075/x2.png",
                "caption": "Figure 2:Evaluation task suite of RISE.Left: Tabletop setting.Right: Zoomed-in details of each task procedure.Dynamic Brick Sortinginvolves\nprecisely picking up colored bricks from a moving conveyor and placing them into the corresponding color-designated bins.Backpack Packingrequires the robot to open, insert clothes, lift, and zip the backpack.Box Closingnecessitates subtle controls to fold the flap and tuck the tab into the box precisely.",
                "position": 249
            }
        ]
    },
    {
        "header": "IIPreliminary",
        "images": []
    },
    {
        "header": "IIIMethodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11075/x3.png",
                "caption": "Figure 3:Qualitative imaginations produced by RISE.Given initial multi-view context and candidate action chunks, RISE can (a) emulate a variety of future accordingly, (b) simulate failure cases with corresponding reward drops, and (c) maintain coherent predictions consistent with real executions.",
                "position": 365
            },
            {
                "img": "https://arxiv.org/html/2602.11075/x4.png",
                "caption": "Figure 4:Workflow of compositional world model.Top: Training recipe upon proper model initialization.Bottom: Inference pipeline that yields rewarded samples for policy optimization.\nBoth modules are compatible with multi-view images.\nWe omit text prompt for both policy and value model for brevity.",
                "position": 407
            },
            {
                "img": "https://arxiv.org/html/2602.11075/x5.png",
                "caption": "Figure 5:Self-improving loop\nof RISE.Our learning pipeline encompasses two stages.Top: Rollout stage.\nPrompted with an optimal advantage, the rollout policy interacts with the world model to produce rollout data.Bottom: Training stage. The behavior policy is then trained to\ngenerate proper action under an advantage-conditioning scheme.",
                "position": 586
            }
        ]
    },
    {
        "header": "IVEvaluations",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11075/x6.png",
                "caption": "Figure 6:Qualitative Comparison on Dynamics Models.Cosmos[1]and Genie Envisioner[59]suffer from geometric distortion, motion blurring, and physical inconsistency, whereas our method showcases temporally coherent and physically consistent results with Ground Truth (GT).",
                "position": 933
            }
        ]
    },
    {
        "header": "VRelated Work",
        "images": []
    },
    {
        "header": "VIConclusion",
        "images": []
    },
    {
        "header": "VIILimitations and future work",
        "images": []
    },
    {
        "header": "VIIIAcknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "IXAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11075/x7.png",
                "caption": "Figure 7:Task success rate across advantage bins.A clear performance drop is observed from high to low advantage levels, especially in Sorting. This confirms that our policy effectively captures behavior diversity through advantage conditioning.",
                "position": 2585
            },
            {
                "img": "https://arxiv.org/html/2602.11075/x8.png",
                "caption": "Figure 8:Learning dynamics of\nRL alternatives.Compared to RECAP[2]and DSRL[80],\nRISE yields significantly higher results,\nwhich cannot be attained by the competing methods even with extended training[2]and increased real-world interactions[80].",
                "position": 2594
            },
            {
                "img": "https://arxiv.org/html/2602.11075/x9.png",
                "caption": "Figure 9:Task-centric versus non-task-centric during pre-train stage.The optical flow maps demonstrate that our method captures action adherence more effectively during the initial stages of pre-training.",
                "position": 2615
            },
            {
                "img": "https://arxiv.org/html/2602.11075/x10.png",
                "caption": "Figure 10:Visual ablation study on training strategies.Compared to the other baselines, which exhibit significant degradation in image quality and motion coherence, our proposed method generates sharper, physically consistent predictions that strictly adhere to control actions.",
                "position": 2666
            }
        ]
    },
    {
        "header": "XReal-world Experimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11075/x11.png",
                "caption": "Figure 11:Experimental setup.We utilize a bi-manual platform for our tasks. Each arm possesses 6 DoF along with a 1-DoF gripper, equipped with a wrist-mounted camera. To provide a global view, a top-down camera is positioned centrally between the arms at a height of approximately 0.75 m. The control frequency is set to 30 Hz.Top Left:We apply Gripper A for brick sorting and backpack packing, while applying Gripper B for box closing for the higher precision requirement.",
                "position": 2770
            },
            {
                "img": "https://arxiv.org/html/2602.11075/x12.png",
                "caption": "Figure 12:Conceptual comparisons with highly-related work.Different from prior works that heavily rely on off-policy samples\nfrom real-world interactions for policy optimization[65,80,85,2],\nRISE enables on-policy RL by building a world model as an interactive environment.",
                "position": 2775
            }
        ]
    },
    {
        "header": "XIImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11075/x13.png",
                "caption": "Figure 13:Qualitative visualizations of value prediction on real-world data.Our value model is capable of distinguishing success and failure,\nhighlighted in green and red, respectively.",
                "position": 3029
            },
            {
                "img": "https://arxiv.org/html/2602.11075/x14.png",
                "caption": "Figure 14:Qualitative ablation of value model.This visualization ablates the effectiveness of imposing\neach loss during the training of the value model.\nGreen and gray regions highlight the favorable and retrying behaviors, respectively.\nIn the green region, (b) exhibits a stronger capability in detecting\ncritical steps, compared to (a) progress only variant, where the result is simply monotonic.\nHowever, (b) is less numerically stable compared to (a), as depicted\nin the gray region.\nWe jointly apply two losses to feature both visual sensitivity and numerical stability.",
                "position": 3036
            }
        ]
    },
    {
        "header": "XIIConceptual Comparisons with Highly-Related Work",
        "images": []
    },
    {
        "header": "XIIIQualitative Visualizations",
        "images": []
    },
    {
        "header": "XIVFailure Modes",
        "images": []
    },
    {
        "header": "XVAdditional Related Work on VLA Models",
        "images": []
    },
    {
        "header": "XVILicense of Assets",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11075/x15.png",
                "caption": "Figure 15:Multiple rollouts from the same initial state.Left: Starting from the same state where the gripper grasps a blue brick,\nour world model can synthesize outcomes that accurately follow different actions.Top Row: Expert demonstration for reference.Middle Row: Imagined rollout of successful action that\ncorrectly put the blue brick into the blue basket, where the rewards go positive.Bottom Row: Imagined rollout of failed action that\nmistakenly put the blue brick into the yellow basket, where the rewards become negative.",
                "position": 3144
            },
            {
                "img": "https://arxiv.org/html/2602.11075/x16.png",
                "caption": "Figure 16:Policy rollout.RISE demonstrates robust performance across diverse manipulation regimes.Top:Handling dynamic scenes by sorting bricks on a moving conveyor.Middle:Manipulating deformable objects in the Backpack Packing task.Bottom:Achieving high-precision bi-manual control in Box Closing.",
                "position": 3160
            },
            {
                "img": "https://arxiv.org/html/2602.11075/x17.png",
                "caption": "Figure 17:Failure modes during inference.Top:Failures typically involve temporal inconsistency in tracking moving objects or precise grasping errors.Middle:The high deformability can lead to incomplete cloth insertion or slippage during the lifting and zipping stages.Bottom:Slight misalignments during bi-manual coordination can cause the cup to tip over during loading or result in unsuccessful folding and tucking.",
                "position": 3163
            },
            {
                "img": "https://arxiv.org/html/2602.11075/x18.png",
                "caption": "Figure 18:Dynamics model rollouts.Each video clip is ordered\ntop to bottom.",
                "position": 3167
            },
            {
                "img": "https://arxiv.org/html/2602.11075/x19.png",
                "caption": "Figure 19:Comparisons with other generative counterparts.",
                "position": 3174
            }
        ]
    },
    {
        "header": "XVIIBroader Impact",
        "images": []
    }
]