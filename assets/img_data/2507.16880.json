[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.16880/x1.png",
                "caption": "Figure 1:Left:1Without mitigation, the DM closely replicates the training sample.2Mitigation strategies, such as pruning memorization neurons with NeMo[14]or Wanda[3], prevent replication for the memorized prompt, thereby suggesting successful removal. Yet,3adversarial embeddingsstill trigger replication.Right:While pruning alters the generation trajectory for the original memorized prompt (blue), adversarial embeddings steer denoising along alternative paths (red) that still lead to the memorized content, unaffected by the pruning-based mitigation.",
                "position": 169
            }
        ]
    },
    {
        "header": "2Background and Related Work",
        "images": []
    },
    {
        "header": "3Breaking Pruning-Based Mitigation Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.16880/extracted/6644272/figures/dory.png",
                "caption": "Table 1:Pruning-based mitigation of memorization is vulnerable to adversarial embeddings. Without any mitigation technique applied (1st row), the generated images clearly indicate data replication. Searching for adversarial embeddings on non-memorized prompts (2nd row) does not lead to clear replication. After localizing and pruning weights with NeMo (3rd row) or Wanda (4th row), data replication appears effectively prevented. However, identifying adversarial embeddings with Doriâ€”indicated byâ€”reveals that embeddings capable of triggering data replication may persist, even after pruning. Finally, our adversarial fine-tuning (5th row) successfully removes memorized content and prevents data replication. N/A denotes not applicable.",
                "position": 284
            }
        ]
    },
    {
        "header": "4The Illusion of Memorization Locality",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.16880/x2.png",
                "caption": "Figure 2:Data replication triggers are scattered in the embedding space.We show that the adversarial embeddings are distributed similarly to the randomly initialized embeddings, thus refuting the input space locality of data replication triggers.",
                "position": 433
            },
            {
                "img": "https://arxiv.org/html/2507.16880/x3.png",
                "caption": "Figure 3:Locality through the lens of activations and memorization weights.Although adversarial embeddings trigger the same image, their activations (left) exhibit high discrepancyâ€”comparable to that of embeddings causing replication ofdifferentimages. Since memorization weight identification relies on activations, this large discrepancy results in low weight agreementÂ (right), which undermines the idea that weights responsible for replicating a memorized image can be pinpointed and pruned.",
                "position": 470
            }
        ]
    },
    {
        "header": "5Robust Mitigation via Adversarial Fine-Tuning",
        "images": []
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "Acknowledgments and Disclosure of Funding",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitations",
        "images": []
    },
    {
        "header": "Appendix BHard- and Software Details",
        "images": []
    },
    {
        "header": "Appendix CModel and Dataset Details",
        "images": []
    },
    {
        "header": "Appendix DAdditional Details and Experiments on Adversarial Embedding Optimization",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.16880/extracted/6644272/figures/dory.png",
                "caption": "Algorithm 1Finding Doriwith Adversarial Embeddings",
                "position": 1280
            },
            {
                "img": "https://arxiv.org/html/2507.16880/extracted/6644272/figures/arbitrary_generation.png",
                "caption": "Figure 4:Arbitrary image replication.We find that when pushed to the extreme, Dori search yields generations (columns from two to six from the left) of non-memorized data (first from the left).",
                "position": 1354
            },
            {
                "img": "https://arxiv.org/html/2507.16880/x4.png",
                "caption": "Figure 5:Finding Dori with more optimization steps.We note that our method starts producing False Positives,i.e.,replicating non-memorized data, only after 500 optimization steps (left). Notably, to achieve non-training data replication, the norm of the optimized embedding raises drastically (right).",
                "position": 1377
            }
        ]
    },
    {
        "header": "Appendix EAdditional Experiments on Pruning-Based Mitigation",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.16880/extracted/6644272/figures/dory.png",
                "caption": "Table 2:Comparison of different numbers of adversarial embedding optimization steps. Embeddings are initialized with their corresponding training prompt embeddings.denotes the application of adversarial embeddings.",
                "position": 1436
            },
            {
                "img": "https://arxiv.org/html/2507.16880/extracted/6644272/figures/dory.png",
                "caption": "Table 3:Comparison of different numbers of adversarial embedding optimization steps. Embeddings are initialized randomly.denotes the application of adversarial embeddings.",
                "position": 1916
            },
            {
                "img": "https://arxiv.org/html/2507.16880/extracted/6644272/figures/dory.png",
                "caption": "Table 4:As shown, applying Wanda across all prompts is less effective at mitigating memorization compared to applying it individually per prompt. However, as discussed inSec.E.7, applying Wanda per prompt and aggregating the found neurons over all 500 prompts comes at the high cost of reduced overall performance because of so many weights being pruned. In the setting with10101010prompts, we randomly sample10101010prompts across5555different seeds and report the average results. This setup proves less effective at mitigating memorization than using the full set of500500500500prompts.",
                "position": 2157
            },
            {
                "img": "https://arxiv.org/html/2507.16880/extracted/6644272/figures/dory.png",
                "caption": "Table 5:Even when applying Wanda[3]with a higher number of time steps it is still possible to break it using Dori.",
                "position": 2273
            },
            {
                "img": "https://arxiv.org/html/2507.16880/extracted/6644272/figures/dory.png",
                "caption": "Table 6:Applying Wanda[3]with higher sparsity does not change the fact that the method seems to only conceal memorization instead of completely removing it from the model. Increasing the sparsity also comes at the cost of reduced image quality, as the FID and the KID values suggest.",
                "position": 2485
            },
            {
                "img": "https://arxiv.org/html/2507.16880/extracted/6644272/figures/dory.png",
                "caption": "Table 8:Successful memorization removal with Wanda requires significant damage to the model.While 10% sparsity ratio for Wanda mitigates memorization even under Dori, we observe a sharp drop in the generation quality (FIDConcept) and alignment between the prompt and generated images (ğ‘¨Conceptsubscriptğ‘¨Concept\\bm{A}_{\\text{Concept}}bold_italic_A start_POSTSUBSCRIPT Concept end_POSTSUBSCRIPT) for paraphrases of prompts used to remove memorization.",
                "position": 2856
            }
        ]
    },
    {
        "header": "Appendix FAdditional Details and Experiments on Adversarial Fine-Tuning",
        "images": []
    },
    {
        "header": "Appendix GAdditional Details and Experiments on Locality",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.16880/x5.png",
                "caption": "Figure 6:T-SNE visualization of100100100100non-memorized text embeddingsynâ¢oâ¢nâ¢mâ¢eâ¢msubscriptğ‘¦ğ‘›ğ‘œğ‘›ğ‘šğ‘’ğ‘š{\\bm{y}}_{nonmem}bold_italic_y start_POSTSUBSCRIPT italic_n italic_o italic_n italic_m italic_e italic_m end_POSTSUBSCRIPT(blue) and adversarially crafted embeddings (orange)yaâ¢dâ¢vsubscriptğ‘¦ğ‘ğ‘‘ğ‘£{\\bm{y}}_{adv}bold_italic_y start_POSTSUBSCRIPT italic_a italic_d italic_v end_POSTSUBSCRIPT, generated by perturbing the blue ones.We observe identical behavior for non-random initialization as in Fig. 2 â€”adversarial embeddings are uniformly distributed in the text embedding space.",
                "position": 4476
            },
            {
                "img": "https://arxiv.org/html/2507.16880/x6.png",
                "caption": "Figure 7:L2 distances of input embeddings.Left: we compute pairwise L2 distances in text embedding space within the set of100100100100random embeddings (ğ’©â¢(ğŸ,ğ‘°)ğ’©0ğ‘°\\mathcal{N}(\\bm{0},\\bm{I})caligraphic_N ( bold_0 , bold_italic_I )), set of adversarial embeddings optimized from random embeddings (second box from the left),100100100100randomly selected non-memorized prompts (ğ’šnâ¢oâ¢nâ¢mâ¢eâ¢msubscriptğ’šğ‘›ğ‘œğ‘›ğ‘šğ‘’ğ‘š{\\bm{y}}_{nonmem}bold_italic_y start_POSTSUBSCRIPT italic_n italic_o italic_n italic_m italic_e italic_m end_POSTSUBSCRIPT) and adversarial embeddings optimized from non-memorized embeddings (fourth box from the left). We observe that after optimization, the adversarial embeddings aremorespread out in the text embedding space than their initial points (be itğ’©â¢(ğŸ,ğ‘°)ğ’©0ğ‘°\\mathcal{N}(\\bm{0},\\bm{I})caligraphic_N ( bold_0 , bold_italic_I )or randomly selectedğ’šnâ¢oâ¢nâ¢mâ¢eâ¢msubscriptğ’šğ‘›ğ‘œğ‘›ğ‘šğ‘’ğ‘š{\\bm{y}}_{nonmem}bold_italic_y start_POSTSUBSCRIPT italic_n italic_o italic_n italic_m italic_e italic_m end_POSTSUBSCRIPT).\nRight: We compute the L2 distance between the initialization and the final adversarial embeddings. We note that when initializing the optimization withğ’šnâ¢oâ¢nâ¢mâ¢eâ¢msubscriptğ’šğ‘›ğ‘œğ‘›ğ‘šğ‘’ğ‘š{\\bm{y}}_{nonmem}bold_italic_y start_POSTSUBSCRIPT italic_n italic_o italic_n italic_m italic_e italic_m end_POSTSUBSCRIPTwe have to travel farther in the text embedding space to obtain an adversarial embeddingğ’šaâ¢dâ¢vsubscriptğ’šğ‘ğ‘‘ğ‘£{\\bm{y}}_{adv}bold_italic_y start_POSTSUBSCRIPT italic_a italic_d italic_v end_POSTSUBSCRIPTthat successfully triggers replication of the memorized imageğ’™ğ‘šğ‘’ğ‘šsubscriptğ’™ğ‘šğ‘’ğ‘š{\\bm{x}}_{\\mathit{mem}}bold_italic_x start_POSTSUBSCRIPT italic_mem end_POSTSUBSCRIPT.",
                "position": 4490
            },
            {
                "img": "https://arxiv.org/html/2507.16880/x7.png",
                "caption": "Figure 8:Number of memorization weights per layer.We observe that for NeMo, no weights are identified to prune in layers two, six, and seven (left). Conversely, Wanda identifies significantly more memorization weights in deeper layers. Interestingly, the drop in weight agreement for Wanda (Fig. 3) happens also in the deeper layers of the model.",
                "position": 4559
            },
            {
                "img": "https://arxiv.org/html/2507.16880/extracted/6644272/figures/wanda_mitigation_and_adv.jpg",
                "caption": "Figure 9:Qualitative results after applying Wanda.The first column shows the original training images. The next three columns show generations after applying the mitigation technique. The final three columns show generations from adversarial embeddings, also after applying the mitigation technique. The adversarial embeddings were initialized with memorized prompt embeddings and optimized for 50 steps.",
                "position": 4571
            },
            {
                "img": "https://arxiv.org/html/2507.16880/extracted/6644272/figures/nemo_mitigation_and_adv.jpg",
                "caption": "Figure 10:Qualitative results after applying NeMo.The first column shows the original training images. The next three columns show generations after applying the mitigation technique. The final three columns show generations from adversarial embeddings, also after applying the mitigation technique. The adversarial embeddings were initialized with memorized prompt embeddings and optimized for 50 steps.",
                "position": 4579
            },
            {
                "img": "https://arxiv.org/html/2507.16880/extracted/6644272/figures/finetuning_mitigation_and_adv.jpg",
                "caption": "Figure 11:Qualitative results for memorized content after applying our adversarial fine-tuning.The first column shows the original training images. The next three columns show generations after fine-tuning the model for five epochs using the default parameters reported in the main paper. The final three columns show generations from adversarial embeddings, also after applying the mitigation technique. The adversarial embeddings were initialized with memorized prompt embeddings and optimized for 50 steps.",
                "position": 4587
            },
            {
                "img": "https://arxiv.org/html/2507.16880/extracted/6644272/figures/coco_finetuning.jpg",
                "caption": "Figure 12:Qualitative results on COCO after applying our adversarial fine-tuning.The first three columns show images generated for 30 COCO prompts using the default Stable Diffusion v1.4 model. The last three columns show generations after fine-tuning the model for five epochs using our adversarial fine-tuning mitigation. The adversarial embeddings were initialized with memorized prompt embeddings and optimized for 50 steps.",
                "position": 4590
            },
            {
                "img": "https://arxiv.org/html/2507.16880/extracted/6644272/figures/wanda_mitigation_adv_0.1_sparsity.jpg",
                "caption": "Figure 13:Qualitative results after applying Wanda with a sparsity of 10%.The first column shows the original training images. The next three columns show generations after applying the mitigation technique. The final three columns show generations from adversarial embeddings, also after applying the mitigation technique. The adversarial embeddings were initialized with memorized prompt embeddings and optimized for 50 steps.",
                "position": 4598
            },
            {
                "img": "https://arxiv.org/html/2507.16880/extracted/6644272/figures/wanda_concepts_vm.png",
                "caption": "Figure 14:Qualitative results for damage to concepts after applying Wanda with a sparsity of 10%.On the left we show the paraphrased prompt for \"The No Limits Business Woman Podcast\" memorized prompt (VM). The first three images from the left depict generations from SD-v1.4 without mitigation, and the next threeâ€”images generated with Wanda after pruning 10% weights.",
                "position": 4601
            },
            {
                "img": "https://arxiv.org/html/2507.16880/extracted/6644272/figures/wanda_concepts_Tm.png",
                "caption": "Figure 15:Qualitative results for damage to concepts after applying Wanda with a sparsity of 10%.On the left we show the paraphrased prompt for \"Plymouth Curtain Panel featuring Madelyn - White Botanical Floral Large Scale by heatherdutton\" memorized prompt (TM). The first three images from the left depict generations from SD-v1.4 without mitigation, and the next threeâ€”images generated with Wanda after pruning 10% weights.",
                "position": 4604
            },
            {
                "img": "https://arxiv.org/html/2507.16880/extracted/6644272/figures/nemo_mitigation_iter.jpg",
                "caption": "Figure 16:Qualitative results after applying NeMo iteratively 5 times.The first column shows the original training images. The next three columns show generations after applying the mitigation technique. The final three columns show generations from adversarial embeddings, also after applying the mitigation technique. The adversarial embeddings were initialized with memorized prompt embeddings and optimized for 50 steps.",
                "position": 4612
            },
            {
                "img": "https://arxiv.org/html/2507.16880/extracted/6644272/figures/nemo_mitigation_iter_quality_over_steps.jpg",
                "caption": "Figure 17:Qualitative results after applying NeMo iteratively 5 times.The first column shows the original training images. The next five columns show generations after applying the mitigation technique iteratively. It can be seen that after five iterations the quality seems to degrade a bit.",
                "position": 4615
            }
        ]
    },
    {
        "header": "Appendix HQualitative Results",
        "images": []
    }
]