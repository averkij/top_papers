[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Implicit Regularization ofPMD-mean",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05933/x1.png",
                "caption": "Figure 1:Left: Scaled log-partition function vs average reward assuming binary rewards. The gap is significant for moderateτ\\tau.\nRight: Illustration ofPMD-meanandPMD-partconverging to different subproblem solutions in the probability simplex.",
                "position": 376
            },
            {
                "img": "https://arxiv.org/html/2602.05933/x2.png",
                "caption": "",
                "position": 385
            },
            {
                "img": "https://arxiv.org/html/2602.05933/x3.png",
                "caption": "Figure 2:The (log) probability ratio of updates inPMD-meanis more conservative than that inPMD-partfor binary rewards.",
                "position": 469
            }
        ]
    },
    {
        "header": "4Implications on Convergence",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05933/x4.png",
                "caption": "Figure 3:Target estimation error ofPMD-meanandPMD-partunderτ=0.05\\tau=0.05andptp_{t}ranges from0.010.01to0.20.2.\nLeft: the target estimation errorΔ2¯\\overline{\\Delta^{2}}. Right: The scaled estimation error with corresponding prefactoreB+e^{B_{+}}in (22). The plot shows the average from100100random seeds.\nWhen the rollout sample sizennis small, the error ofPMD-partis much larger for smallptp_{t}.",
                "position": 807
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05933/x5.png",
                "caption": "Figure 4:Training curves (smoothed) ofPMD-mean(upper) andPMD-part(lower) with baselines for Qwen2.5-7B on DAPO-Math-17k (left) and the averaged evaluation accuracy on AIME 2024 and AIME 2025 (right).\nThe global step of on-policy gradient is divided by 16 to match other algorithms.",
                "position": 1001
            },
            {
                "img": "https://arxiv.org/html/2602.05933/x6.png",
                "caption": "",
                "position": 1003
            },
            {
                "img": "https://arxiv.org/html/2602.05933/x7.png",
                "caption": "Figure 5:The minimum of log-ratioslog⁡πt+1πt\\log\\frac{\\pi_{t+1}}{\\pi_{t}}inPMD-meanandPMD-part, estimated from the last update mini-batch.",
                "position": 1015
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05933/x8.png",
                "caption": "Figure 6:Target estimation error for positive and negative actions inPMD-meanandPMD-partunderτ=0.05\\tau=0.05andptp_{t}ranges from0.010.01to0.20.2.\nLeft: positive actions. Right: negative actions.\nThe plot shows the average from100100random seeds.\nThe error inPMD-meanmainly comes from a systematic mismatch between positive targets.",
                "position": 1801
            },
            {
                "img": "https://arxiv.org/html/2602.05933/x9.png",
                "caption": "Figure 7:Qwen2.5-7B training results for 15 epochs (495 global steps). Training reward, response length, and entropy are smoothed by EMA with an effective window size of 50.PMD-meanachieves superior performance not only in Pass@1 (measured in Avg@32) but also Pass@32 and Maj@32 (accuracy of majority voting answer).",
                "position": 1810
            },
            {
                "img": "https://arxiv.org/html/2602.05933/x10.png",
                "caption": "Figure 8:Qwen3-30B-A3B-Base training results for 300 global steps. Training reward, response length, and entropy are smoothed by EMA with an effective window size of 20.",
                "position": 1818
            }
        ]
    },
    {
        "header": "Appendix BMissing Proofs in Section3",
        "images": []
    },
    {
        "header": "Appendix CMissing Proofs in Section4.1",
        "images": []
    },
    {
        "header": "Appendix DRefined Analysis forPMD-mean",
        "images": []
    }
]