[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.13189/x1.png",
                "caption": "Figure 1:Overview of ViXML multi-modal framework, which supports both encoder language models (LMs) and decoder LLMs. ViXML efficiently incorporates visual metadata in queries (aia_{i}) and labels (ara_{r}) while freezing the vision encoder for efficiency. Prompts (‚Ñ∞i‚Ä≤{\\mathcal{E}}^{\\prime}_{i}and‚Ñ∞r‚Ä≤{\\mathcal{E}}^{\\prime}_{r}) combine text and projected image embeddings (‚Ñ∞{\\mathcal{E}}andùí±{\\mathcal{V}}). Sentence embeddings (ùê°qi{{\\mathbf{h}}}^{i}_{q}andùê°lr{{\\mathbf{h}}}^{r}_{l}) are learned via contrastive learning.",
                "position": 106
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experimental work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.13189/x2.png",
                "caption": "Figure 2:Performance (P@1) and Training Time (TT) for dual-encoder (dots) and dual-decoder (stars) learning in LF-AmazonTitles-131K. The ViXML multi-modal framework (blue) improves text-only alternatives (red), while decoder models boost encoder performance in both setups. Previous state-of-the-art (SOTA) is represented by MOGIC method.",
                "position": 309
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "Appendix AImplementation details",
        "images": []
    },
    {
        "header": "Appendix BImage count impact",
        "images": []
    },
    {
        "header": "Appendix CAlternative prompting",
        "images": []
    },
    {
        "header": "Appendix DSeed variability",
        "images": []
    },
    {
        "header": "Appendix ERetrieval-augmented inference",
        "images": []
    },
    {
        "header": "Appendix FExtended comparison",
        "images": []
    },
    {
        "header": "Appendix GLatencies",
        "images": []
    },
    {
        "header": "Appendix HLimitations",
        "images": []
    }
]