[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3How Does Test-Time Scaling Affect Accuracy and Hallucination Ratio?",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06861/x1.png",
                "caption": "Figure 1:Accuracy with increased test-time computation across 12 reasoning models on SimpleQA and FRAMES.For most models, extended test-time reasoning does not consistently improve accuracy. While some models, such as GPT-5 mini, show initial accuracy gains, further increasing reasoning length brings little or no additional improvement. In many cases, such as Claude Sonnet 4 and Qwen3 models, accuracy plateaus or fluctuates with no clear upward trend.",
                "position": 446
            },
            {
                "img": "https://arxiv.org/html/2509.06861/x2.png",
                "caption": "Figure 2:Hallucination ratio with increased test-time reasoning across 12 models on SimpleQA and FRAMES.For most models, longer reasoning does not reduce hallucinations. In many cases, such as GPT-5 mini and gpt-oss-20b, hallucination increases with longer thinking length. Only Grok-3 mini and DS-R1-Distill-Qwen-14B exhibit reduced hallucinations with extended reasoning.",
                "position": 479
            }
        ]
    },
    {
        "header": "4Why Does Thinking More Sometimes Lead to Fewer or More Hallucinations?",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06861/x3.png",
                "caption": "Figure 3:Changes in hallucination behavior with more thinking.We compare model responses at different reasoning levels, focusing on cases where one response is a hallucination and the other is not. For the non-hallucinating responses, we compute the ratio of correct and not attempted. Results show that reduced hallucinations result from abstention, while more hallucinations stem from the model attempting previously unanswered questions.",
                "position": 510
            },
            {
                "img": "https://arxiv.org/html/2509.06861/x4.png",
                "caption": "Figure 4:Case studies illustrating how thinking more can lead to more hallucinations.(Left):gpt-oss-20b abstains under low reasoning effort, but produces an overconfident incorrect answer at high effort.(Right):Gemini 2.5 Flash abstains under a low thinking budget due to incomplete reasoning, but hallucinates when given more budget. Full reasoning traces are provided in AppendixB.",
                "position": 545
            }
        ]
    },
    {
        "header": "5Thinking vs. Non-Thinking: Is Thinking Helpful?",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitation",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperiment Details",
        "images": []
    },
    {
        "header": "Appendix BFull Reasoning Traces for Case Studies",
        "images": []
    }
]