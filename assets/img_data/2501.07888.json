[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.07888/extracted/6130403/figs/radar_all_tasks.png",
                "caption": "Figure 1:Performance comparison of Tarsier2 with previous SOTA models at 7B-scale and GPT-4o. We report the overall average scores for benchmarks with multiple subtasks/metrics.",
                "position": 109
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.07888/x1.png",
                "caption": "Figure 2:Overview of Tarsier2 capabilities. Based on its strong ability for detailed video description, Tarsier2 excels in a variety of video-centric tasks. Click the play buttons to view the videos.",
                "position": 270
            },
            {
                "img": "https://arxiv.org/html/2501.07888/extracted/6130403/figs/play.png",
                "caption": "",
                "position": 272
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.07888/extracted/6130403/figs/pretrain_data.png",
                "caption": "Figure 3:Summary of datasets used in the pre-training stage of Tarsier2.",
                "position": 345
            },
            {
                "img": "https://arxiv.org/html/2501.07888/extracted/6130403/figs/SFT_grounding.png",
                "caption": "Figure 4:An example of a video description with fine-grained temporal grounding. ‚Äú¬°frame:iùëñiitalic_i-jùëójitalic_j¬ø‚Äù indicates that the following event is inferred from framesiùëñiitalic_itojùëójitalic_j. Events are distinguished by color, with corresponding frames and descriptions marked in the same color to indicate their association.",
                "position": 388
            },
            {
                "img": "https://arxiv.org/html/2501.07888/x2.png",
                "caption": "Figure 5:Preference data construction pipeline for DPO training.",
                "position": 407
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.07888/extracted/6130403/figs/human_sbs.png",
                "caption": "Figure 6:Human side-by-side evaluation results of Tarsier2 versus other models.",
                "position": 731
            },
            {
                "img": "https://arxiv.org/html/2501.07888/x3.png",
                "caption": "Figure 7:Model performance against training tokens. The results at the initial step reflect the performance of Qwen2-VL-7B.555For consistency across all checkpoints, we evaluate the Qwen2-VL-7B model using the same frame sampling strategy applied to other checkpoints. This may differ from the official sampling strategy in some benchmarks. For instance, the official setting of Video-MME uses 768 frames, while we sample 128 frames.",
                "position": 1806
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining hyper-parameters",
        "images": []
    },
    {
        "header": "Appendix BPublic datasets of pre-training stage",
        "images": []
    },
    {
        "header": "Appendix CAnnotation process for SFT data",
        "images": []
    },
    {
        "header": "Appendix DDetail setting of DPO training",
        "images": []
    },
    {
        "header": "Appendix EDetailed\nresults of individual datasets at different stages",
        "images": []
    },
    {
        "header": "Appendix FTarsier2-Recap-585K Data Composition",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.07888/x4.png",
                "caption": "Figure 9:Qualitative comparison of our model at different stages.",
                "position": 4737
            }
        ]
    },
    {
        "header": "Appendix GQualitative Comparison of the SFT Process",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.07888/x5.png",
                "caption": "Figure 10:Qualitative comparative analysis of various Video-MLLMs on Dream-1K dataset (Live-action Subset).",
                "position": 4754
            },
            {
                "img": "https://arxiv.org/html/2501.07888/x6.png",
                "caption": "Figure 11:Qualitative comparative analysis of various Video-MLLMs on Dream-1K dataset (Animation Subset).",
                "position": 4757
            },
            {
                "img": "https://arxiv.org/html/2501.07888/x7.png",
                "caption": "Figure 12:Qualitative comparative analysis of various Video-MLLMs on Dream-1K dataset (Stock Subset).",
                "position": 4760
            },
            {
                "img": "https://arxiv.org/html/2501.07888/x8.png",
                "caption": "Figure 13:Qualitative comparative analysis of various Video-MLLMs on Dream-1K dataset (Youtube Subset).",
                "position": 4763
            },
            {
                "img": "https://arxiv.org/html/2501.07888/x9.png",
                "caption": "Figure 14:Qualitative comparison of different Video-MLLMs on Dream-1K dataset (Shorts Subset).",
                "position": 4766
            }
        ]
    },
    {
        "header": "Appendix HDREAM-1K cases",
        "images": []
    }
]