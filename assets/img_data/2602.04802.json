[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04802/photos/motivation_5.jpg",
                "caption": "Figure 1:(a) Humans integrate visual context with embedded text, whereas standard VLM evaluation provides language as discrete tokens.\n(b) Presenting language as visualized text can induce behavioral deviations from pure-text inputs, revealing a modality gap.",
                "position": 157
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04802/x1.png",
                "caption": "Figure 2:Comparison between Text and Visualized Text Inputs.",
                "position": 222
            },
            {
                "img": "https://arxiv.org/html/2602.04802/x2.png",
                "caption": "Figure 3:Perceptual factor impact.Top:Font Size (9, 16, 32, 48, 64).Bottom:Font Style (Arial, Cambria, Roman, Brush).",
                "position": 225
            },
            {
                "img": "https://arxiv.org/html/2602.04802/x3.png",
                "caption": "",
                "position": 229
            },
            {
                "img": "https://arxiv.org/html/2602.04802/photos/pipeline_13.jpg",
                "caption": "Figure 4:Overview of the construction. First, we extract filtered dataset from existing data rely on diversity and accuracy. Second, we transform text into visualized text through the rendering pipeline. We then validate the precision of visualized text depends on VLM and continuously refine the pipeline. Through this process, we finally establish VISTA-Bench, supported by a sophisticated rendering pipeline.",
                "position": 233
            }
        ]
    },
    {
        "header": "3VISTA-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04802/photos/dimension_3.jpg",
                "caption": "Figure 5:Ability dimensions in VISTA-Bench.VISTA-Bench includes two main levels of dimensions based on Inherent Modality Dependence and Cognitive Dimension, with 10 distinct abilities.",
                "position": 318
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04802/photos/failure_mode_8.jpg",
                "caption": "Figure 6:Qualitative attention visualization analysis of models with disparate OCR capabilities under various rendering configurations.",
                "position": 2028
            },
            {
                "img": "https://arxiv.org/html/2602.04802/x4.png",
                "caption": "Figure 7:Sensitivity to font size and style on VISTA-Bench.",
                "position": 2053
            },
            {
                "img": "https://arxiv.org/html/2602.04802/x4.png",
                "caption": "Figure 7:Sensitivity to font size and style on VISTA-Bench.",
                "position": 2056
            },
            {
                "img": "https://arxiv.org/html/2602.04802/x5.png",
                "caption": "Figure 8:Modality Gap Across Multimodal and Unimodal Tasks.",
                "position": 2061
            }
        ]
    },
    {
        "header": "5Limitations and Disscusion",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ABenchmark Details",
        "images": []
    },
    {
        "header": "Appendix BRendering Pipeline",
        "images": []
    },
    {
        "header": "Appendix CEvaluation Protocol",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04802/x6.png",
                "caption": "Figure 9:Impact of Prompt Design.Prompt: 10-words, 20-words, 50-words, image understanding and CoT.",
                "position": 3169
            },
            {
                "img": "https://arxiv.org/html/2602.04802/x7.png",
                "caption": "Figure 10:A successful Qwen-Image-Edit case under the visualized-text setting.The model correctly generates readable visualized text in the designated region and produces the correct final answer.",
                "position": 3326
            },
            {
                "img": "https://arxiv.org/html/2602.04802/x8.png",
                "caption": "Figure 11:Rendering sensitivity study on eight additional representative models.",
                "position": 3330
            },
            {
                "img": "https://arxiv.org/html/2602.04802/x9.png",
                "caption": "",
                "position": 3339
            },
            {
                "img": "https://arxiv.org/html/2602.04802/x10.png",
                "caption": "",
                "position": 3345
            },
            {
                "img": "https://arxiv.org/html/2602.04802/x11.png",
                "caption": "",
                "position": 3350
            },
            {
                "img": "https://arxiv.org/html/2602.04802/x12.png",
                "caption": "",
                "position": 3356
            },
            {
                "img": "https://arxiv.org/html/2602.04802/x13.png",
                "caption": "",
                "position": 3361
            },
            {
                "img": "https://arxiv.org/html/2602.04802/x14.png",
                "caption": "",
                "position": 3367
            },
            {
                "img": "https://arxiv.org/html/2602.04802/x15.png",
                "caption": "",
                "position": 3372
            },
            {
                "img": "https://arxiv.org/html/2602.04802/x16.png",
                "caption": "Figure 12:Visualized examples forMultimodal Perceptiontask.\nTop: Attribute Perception (Times New Roman, 9pt).\nMiddle: Global Perception (Brush Script MT, 32pt).\nBottom: Instance Perception (Times New Roman, 16pt).",
                "position": 3395
            },
            {
                "img": "https://arxiv.org/html/2602.04802/x17.png",
                "caption": "",
                "position": 3399
            },
            {
                "img": "https://arxiv.org/html/2602.04802/x18.png",
                "caption": "",
                "position": 3401
            },
            {
                "img": "https://arxiv.org/html/2602.04802/x19.png",
                "caption": "Figure 13:Visualized examples forMultimodal Reasoningtask.\nTop: Logical Reasoning (Arial, 16pt).\nMiddle: Spatial & Relation (Cambria, 32pt).\nBottom: Cross-Instance (Cambria, 48pt).",
                "position": 3409
            },
            {
                "img": "https://arxiv.org/html/2602.04802/x20.png",
                "caption": "",
                "position": 3413
            },
            {
                "img": "https://arxiv.org/html/2602.04802/x21.png",
                "caption": "",
                "position": 3415
            },
            {
                "img": "https://arxiv.org/html/2602.04802/x22.png",
                "caption": "Figure 14:Visualized examples forMultimodal Knowledgetask.\nTop: STEM & Health (Arial, 32pt).\nMiddle: Social-Humanities & Management (Cambria, 9pt).\nBottom: STEM & Health (Brush Script MT, 9pt).",
                "position": 3423
            },
            {
                "img": "https://arxiv.org/html/2602.04802/x23.png",
                "caption": "",
                "position": 3427
            },
            {
                "img": "https://arxiv.org/html/2602.04802/x24.png",
                "caption": "",
                "position": 3429
            },
            {
                "img": "https://arxiv.org/html/2602.04802/x25.png",
                "caption": "Figure 15:Visualized examples forUnimodal Knowledgetask.\nTop: Applied Sciences & Social (Times New Roman, 48pt).\nBottom: Natural & Life Sciences (Brush Script MT, 32pt).",
                "position": 3437
            },
            {
                "img": "https://arxiv.org/html/2602.04802/x26.png",
                "caption": "",
                "position": 3441
            },
            {
                "img": "https://arxiv.org/html/2602.04802/x27.png",
                "caption": "Figure 16:Mathematical formula rendering error. Config:Arial, 16pt",
                "position": 3449
            },
            {
                "img": "https://arxiv.org/html/2602.04802/x28.png",
                "caption": "Figure 17:Code structure rendering error.Config:Arial, 9pt",
                "position": 3453
            },
            {
                "img": "https://arxiv.org/html/2602.04802/x29.png",
                "caption": "Figure 18:Handwritten font rendering error.Config:Brush, 32pt",
                "position": 3457
            },
            {
                "img": "https://arxiv.org/html/2602.04802/x30.png",
                "caption": "Figure 19:Top: Rendering correct example.Config:Arial, 32pt.\nBottom: Rendering correct example.Config:Cambria, 16pt",
                "position": 3461
            },
            {
                "img": "https://arxiv.org/html/2602.04802/x31.png",
                "caption": "",
                "position": 3465
            }
        ]
    },
    {
        "header": "Appendix DAdditional Experiments",
        "images": []
    }
]