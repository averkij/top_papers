[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.20070/extracted/6099059/images/Train_and_test_CG2.jpg",
                "caption": "Figure 1:Examples ofCompositional Generalization: The model is required to understand unseen images by recombining the fundamental elements it has learned.",
                "position": 130
            },
            {
                "img": "https://arxiv.org/html/2412.20070/extracted/6099059/images/mat6.jpg",
                "caption": "Figure 2:The process of integrating a vast amount of labeled medical image data to create Med-MAT.",
                "position": 133
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Med-MAT",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.20070/extracted/6099059/images/qaformat.jpg",
                "caption": "Figure 3:The QA formatting process of Med-MAT.",
                "position": 343
            }
        ]
    },
    {
        "header": "3Proof of Concept on CG",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.20070/extracted/6099059/images/2-1.png",
                "caption": "Figure 4:Accuracy results on the Target dataset for various models.’All Related/Unrelated’models are trained on all the related or unrelated datasets of the Target Data.’w/o Modality/Area/Task’are trained on All Related datasets but omit those sharing the same element as the Target Data, to intentionally disrupt CG.’All Data’uses all available training sets. (Note: The Target Data is excluded from training to observe generalization.)",
                "position": 723
            },
            {
                "img": "https://arxiv.org/html/2412.20070/extracted/6099059/images/2-2.png",
                "caption": "",
                "position": 732
            },
            {
                "img": "https://arxiv.org/html/2412.20070/extracted/6099059/images/1-1-1.png",
                "caption": "Figure 5:(RQ 1) The curves show how performance on the Target Data improves as the volume of composition datasets increases. The red and purple lines represent training withRelatedandUnrelated Data, respectively.",
                "position": 773
            },
            {
                "img": "https://arxiv.org/html/2412.20070/extracted/6099059/images/1-1-2.png",
                "caption": "",
                "position": 782
            },
            {
                "img": "https://arxiv.org/html/2412.20070/extracted/6099059/images/1-1-3.png",
                "caption": "",
                "position": 787
            },
            {
                "img": "https://arxiv.org/html/2412.20070/extracted/6099059/images/1-1-4.png",
                "caption": "",
                "position": 792
            },
            {
                "img": "https://arxiv.org/html/2412.20070/extracted/6099059/images/1-2-1.png",
                "caption": "Figure 6:(RQ 2) The curves show how accuracy on Target Data improves as its volume increases with a fixed size of composition data. The red and purple lines represent training withRelatedandUnrelated Data, respectively.",
                "position": 798
            },
            {
                "img": "https://arxiv.org/html/2412.20070/extracted/6099059/images/1-2-2.png",
                "caption": "",
                "position": 807
            },
            {
                "img": "https://arxiv.org/html/2412.20070/extracted/6099059/images/1-2-3.png",
                "caption": "",
                "position": 812
            },
            {
                "img": "https://arxiv.org/html/2412.20070/extracted/6099059/images/1-2-4.png",
                "caption": "",
                "position": 817
            },
            {
                "img": "https://arxiv.org/html/2412.20070/x1.png",
                "caption": "Figure 7:The accuracy of different backbones: Theblueline represents the untrained model, and thegreenline represents the CG-trained model. All data are scaled based on the accuracy of CG combinations, which is displayed at each corner (details in AppendixA.1andA.2).",
                "position": 939
            },
            {
                "img": "https://arxiv.org/html/2412.20070/x2.png",
                "caption": "",
                "position": 948
            },
            {
                "img": "https://arxiv.org/html/2412.20070/x3.png",
                "caption": "",
                "position": 953
            },
            {
                "img": "https://arxiv.org/html/2412.20070/x4.png",
                "caption": "",
                "position": 958
            }
        ]
    },
    {
        "header": "4In-depth Analysis of CG",
        "images": []
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Potential Risks",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.20070/x5.png",
                "caption": "Figure 9:Illustration of diverse samples with varying numbers of candidate options in the Med-MAT dataset.",
                "position": 3170
            }
        ]
    },
    {
        "header": "Appendix BThe Dataset: Med-MAT",
        "images": []
    }
]