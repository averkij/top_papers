[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06155/x1.png",
                "caption": "Figure 1:We observe theAttention Tilepattern in 3D DiTs. (a) the attention map can be broken down into smaller repetitive blocks. (b) These blocks can be classified into two types, where attention weights on the diagonal blocks are noticeably larger than on off-diagonal ones. (c) These blocks exhibit locality, where the attention score differences between the first frame and later frames gradually increases. (d) The block structure is stable across different data points, but varies across layers. We randomly select 2 prompts (one landscape and one portrait) and record the important positions in the attention map that accounted for 90% (95%, 99%) of the total. We printed the proportion of stable overlap of important positions across layers.",
                "position": 124
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06155/x2.png",
                "caption": "Figure 2:Efficient-vDiTtakes in a pre-trained 3D Full Attention video diffusion transformer(DiT), with slow inference speed and high fidelity. It then operates on three stages to greatly accelerate the inference while maintaining the fidelity. In Stage 1, we modify the multi-step consistency distillation framework from(Heek et al.,2024)to the video domain, which turned a DiT model to a CM model withstabletraining. In Stage 2,Efficient-vDiTperforms a searching algorithm to find the best sparse attention pattern for each layer. In stage 3,Efficient-vDiTperforms a knowledge distillation procedure to optimize the fidelity of the sparse DiT. At the end,Efficient-vDiToutputs a DiT with linear attention, high fidelity and fastest inference speed.",
                "position": 179
            }
        ]
    },
    {
        "header": "3Efficient-vDiT",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06155/x3.png",
                "caption": "Figure 3:Exemplar attention mask (2:6:262:62 : 6). It maintains the attention in the main diagonals and against 2 global reference latent frames. Tile blocks in white are not computed.",
                "position": 221
            },
            {
                "img": "https://arxiv.org/html/2502.06155/x4.png",
                "caption": "Figure 4:Search results for Open-Sora-Plan v1.2 model (29 frames). We verify that different layers have different sparsity in 3D video DiTs.",
                "position": 298
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06155/x5.png",
                "caption": "Figure 5:Qualitative samples of our models. We compare the generation quality between the base model, MLCD model, and after knowledge distillation. Frames shown are equally spaced samples from the generated video.Efficient-vDiTis shortened as ‘E-vdit’ for simplicity. More samples can be found in AppendixF.",
                "position": 925
            },
            {
                "img": "https://arxiv.org/html/2502.06155/x6.png",
                "caption": "",
                "position": 929
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExtened layerwise search algorithm",
        "images": []
    },
    {
        "header": "Appendix BFlexAttention implementation details",
        "images": []
    },
    {
        "header": "Appendix CSupplemental Vbench Evaluation",
        "images": []
    },
    {
        "header": "Appendix DAblation study",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06155/extracted/6191047/figures/prompt_sample/ablation.jpg",
                "caption": "Figure 6:Qualitative samples of ablation of distillation order. sampled from VBench prompts. We show that both MLCD andEfficient-vDiTmodel can simliar quality on these samples. In two consecutive videos, the top shows results from MLCD + CD model followed by KD + MLCD model.",
                "position": 3242
            }
        ]
    },
    {
        "header": "Appendix EAttention distill on CogVideoX model",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06155/extracted/6191047/figures/prompt_sample/cogvideo_batch.jpg",
                "caption": "Figure 7:Qualitative samples of CogvideoX-5B(Yang et al.,2024b)distillation from its sample prompts. We show that our attention distill is capable of MM-DiT model architecture. In two consecutive videos, the top shows results from the base model, followed by the distillation model.",
                "position": 3539
            },
            {
                "img": "https://arxiv.org/html/2502.06155/x7.png",
                "caption": "Figure 8:Based on Open-Sora’s examples(Zheng et al.,2024), we selected dynamic prompts featuring centralized explosions and radiating energy, demonstrating dramatic transitions from focal points to expansive environmental transformations, emphasizing large-scale motion.",
                "position": 3553
            },
            {
                "img": "https://arxiv.org/html/2502.06155/x8.png",
                "caption": "",
                "position": 3557
            },
            {
                "img": "https://arxiv.org/html/2502.06155/extracted/6191047/figures/prompt_sample/output_batch_1.jpg",
                "caption": "Figure 9:Qualitative samples of dynamic scenes from VBench prompts. We show that both MLCD andEfficient-vDiTmodel can generate dynamic videos while maintaining video quality. In three consecutive videos, the top shows results from the base model, followed by the MLCD model, and theEfficient-vDiTmodel.",
                "position": 3561
            },
            {
                "img": "https://arxiv.org/html/2502.06155/extracted/6191047/figures/prompt_sample/output_batch_3.jpg",
                "caption": "Figure 10:Qualitative samples of dynamic scenes from VBench prompts. We show that both MLCD andEfficient-vDiTmodel can generate dynamic videos while maintaining video quality. In three consecutive videos, the top shows results from the base model, followed by the MLCD model, and theEfficient-vDiTmodel.",
                "position": 3564
            }
        ]
    },
    {
        "header": "Appendix FQualitative samples of dynamic scenes and large-scale motion",
        "images": []
    }
]