[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25123/x1.png",
                "caption": "Figure 1:An overview of our research framework and key findings. (Top)We introduce a clean string transformation testbed to scientifically analyze RLâ€™s capabilities.(Bottom-Left)Our central RL Compositionality Hypothesis posits that training on simple composites with RL unlocks generalizable compositional skills.(Bottom-Right)Our experiments validate this, showing that: (1) compositional data combined with RL is the key ingredient for learning this new skill; (2) the learned skill transfers across domains; and (3) RL significantly improves difficult problems where the base model fails, while only reranking on problems it solves well.",
                "position": 131
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Research Framework",
        "images": []
    },
    {
        "header": "4RL as a Pathway to Generalizable Skill Acquisition",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25123/x2.png",
                "caption": "Figure 2:Test Accuracy on held-out tasks vs. RL training steps, each related to one held-out task difficulty level.The dark blue curve indicates that training on atomic skills alone (RL Level 1) yields nearly no compositional ability on held-out functions. In contrast, including Level 2 data in RL unlocks strong generalization to more complex problems (Levels 3-6).",
                "position": 328
            },
            {
                "img": "https://arxiv.org/html/2509.25123/x3.png",
                "caption": "Figure 3:RL vs. RFT on Compositional Tasks.RL (red dashed line) achieves substantially higher accuracy across all levels, while iterative RFT fails to learn a generalizable skill.",
                "position": 375
            },
            {
                "img": "https://arxiv.org/html/2509.25123/x4.png",
                "caption": "Figure 4:Avg@32 Accuracy on the Countdown Task. Atomic skills are a prerequisite for task transfer, and that compositional RL (Multi-Base + RL L1+2) on the unrelated string task offers a significant performance improvement on Countdown. Note thatnoneof the models are trained with RL on Countdown.",
                "position": 433
            },
            {
                "img": "https://arxiv.org/html/2509.25123/x5.png",
                "caption": "Figure 5:Pass@kkperformance across varying difficulty levels. On easy problems (Levels 1-2), the performance gap shrinks with more samples, consistent with thererankingnarrative. On hard problems (Levels 3-8), the gap widens substantially, suggesting new skill acquisition.",
                "position": 468
            },
            {
                "img": "https://arxiv.org/html/2509.25123/x6.png",
                "caption": "Figure 6:Distribution of failure modes on Level 3 string tasks.",
                "position": 496
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining Details",
        "images": []
    },
    {
        "header": "Appendix BEvaluation Details",
        "images": []
    },
    {
        "header": "Appendix CExample Prompts for Stage 1 and Stage 2",
        "images": []
    },
    {
        "header": "Appendix DA Complete List of String Transformation Functions",
        "images": []
    },
    {
        "header": "Appendix EExample for Countdown Task",
        "images": []
    },
    {
        "header": "Appendix FModel Response Examples",
        "images": []
    }
]