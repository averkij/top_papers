[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminary",
        "images": []
    },
    {
        "header": "3Motivation",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22664/x1.png",
                "caption": "Figure 1:The average hidden state similarity of the same-preference pair set and the different-preference pair set across transformer layers. Each pair consists of two query-response samples with respective preference labels.",
                "position": 262
            },
            {
                "img": "https://arxiv.org/html/2601.22664/x2.png",
                "caption": "Figure 2:Negative correlation between absolute difference of reward scores allocated by the RM and hidden state similarity. Each data point corresponds to a query-response pair labeled with either identical or differing human preferences.",
                "position": 267
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22664/x3.png",
                "caption": "Figure 3:Overview of R2M. We first aggregate the last-layer hidden stateshih_{i}from the policy with the LLM part output of the reward model. This aggregated representation is then fed into the scoring head for reward prediction. When the policy updates, we get the real-time feedbackhiâ€²h^{\\prime}_{i}and utilize it to construct preference pairs. Finally, we optimize the reward model by jointly minimizing the Bradley-Terry loss and the Group Reward Entropy loss.",
                "position": 307
            },
            {
                "img": "https://arxiv.org/html/2601.22664/x4.png",
                "caption": "Figure 4:The structure of R2M. Building on the dataflow based on solely surface semantic information (left), R2M introduces an additional dataflow based on the policy feedback (right).",
                "position": 353
            }
        ]
    },
    {
        "header": "5Experiments and Analyses",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22664/x5.png",
                "caption": "Figure 5:We compare RLOO and RLOO+R2M in terms of loss, reward and KL divergence during RL optimization, using Qwen2.5-3B-Instruct and LLaMA3-8B-Instruct as policy models, and Skywork-Reward-V2-Llama-3.1-8B as the reward model. For KL divergence, we calculate it as the average of log probability differences between the reference model and the policy model for each token.",
                "position": 947
            },
            {
                "img": "https://arxiv.org/html/2601.22664/x6.png",
                "caption": "Figure 6:Comparison of average rewards in RL Optimization. w/ Noise denotes replacing the feedback in R2M with Gaussian noise.",
                "position": 956
            },
            {
                "img": "https://arxiv.org/html/2601.22664/x7.png",
                "caption": "Figure 7:(a) Computational cost comparison between RLOO and RLOO+R2M. (b) Computational cost comparison between full reward model updates and lightweight updates in R2M.",
                "position": 1000
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Broader Impact",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATheoretical Analysis",
        "images": []
    },
    {
        "header": "Appendix BRelated Work",
        "images": []
    },
    {
        "header": "Appendix COne Case Study of Reward Overoptimization",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22664/x8.png",
                "caption": "Figure 8:During Reward Model Training, the reward model inadvertently learned to assign high scores to responses containing apologies. The policy model detected this pattern and persistently exploited it to obtain inflated rewards, which resulted in a collapse of the RL Optimization process.",
                "position": 2476
            }
        ]
    },
    {
        "header": "Appendix DMotivation Towards Mitigating Reward Optimization",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22664/x9.png",
                "caption": "Figure 9:(a)Identification of reward overoptimization Patterns.We show the similarity matrix of hidden states from forward passes of different query-response pairs for the same policy. The first 8 samples are sequences exhibiting reward overoptimization, while the last 8 are normal output responses.ssdenotes the query-response pairs.\n(b)Policy Distribution Shift Analysis.For a given query with four different responses, we display the similarity matrix of the policy across various training stepstt.",
                "position": 2483
            }
        ]
    },
    {
        "header": "Appendix EDetailed Workflow",
        "images": []
    },
    {
        "header": "Appendix FAdditional Experimental Results",
        "images": []
    },
    {
        "header": "Appendix GExperimental Details",
        "images": []
    },
    {
        "header": "Appendix HMore Method Details of R2M",
        "images": []
    }
]