[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Main",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17803/x1.png",
                "caption": "Figure 1:Overview ofPillar-0and key results across modalities and tasks. (A) We evaluatePillar-0on abdomen-pelvis CT, chest CT, head CT, and breast MRI using RATE, a clinically grounded evaluation framework designed to overcome the limitations of existing radiology benchmarks.Pillar-0substantially outperforms competitive baselines across modalities. (B)Pillar-0’s capabilities extend to real-world clinical prediction tasks outside the standard of care, setting a new state-of-the-art for future lung cancer risk prediction with Sybil-1.5 (Pillar-0finetuned). On rigorous multi-institution external validation, Sybil-1.5 outperforms Sybil[mikhael2023sybil], a strong specialist baseline, by a wide margin. (C) Finally,Pillar-0demonstrates superior data efficiency, reaching 95 AUROC on the RSNA Intracranial Hemorrhage detection benchmark[rsna-intracranial-hemorrhage-detection]using 20-30×\\timesless training data relative to best-in-class natural image-pretrained (Swin3D-t[yang2023swin3]) and radiology-pretrained (Merlin[blankemeier2024merlin]) models. The entirePillar-0system—open-code, open-weights, and open-evaluation—is released to the community.",
                "position": 226
            }
        ]
    },
    {
        "header": "2Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17803/x2.png",
                "caption": "Figure 2:Pillar-0key innovations across tokenization, architecture, and pretraining. (A) Modality-specific multi-windowing converts full-resolution CT and MRI volumes into multi-channel inputs that emulate radiologist workflow presets, preserving clinically relevant contrast. Training with multi-windowing leads to a 4.6 point gain in AUROC in abdomen-pelvis CT. (B) The Atlas vision backbone employs hierarchical Multi-Scale Attention to efficiently process long-context volumes[agrawal2025atlasmsa]. As a result,Pillar-0is 175×\\timesfaster than ViT-S, and achieves state-of-the-art performance with fewer parameters than other medical foundation models. (C) Asymmetric contrastive pretraining aligns Atlas volume embeddings with embeddings from a much larger frozen LLM text encoder. Using this powerful text encoder leads to a much stronger correlation between CLIP loss and downstream performance, providing a reliable signal for clinical utility to guide pretraining experiments.",
                "position": 288
            },
            {
                "img": "https://arxiv.org/html/2511.17803/x3.png",
                "caption": "Figure 3:RATE: a clinically grounded evaluation framework for volumetric radiology. (A) Comparison of RATE to existing benchmarks (VQA-RAD[lau2018dataset], SLAKE[liu2021slake], PMC-VQA[zhang2023pmc], OmniMedVQA[hu2024omnimedvqa], MMMU[yue2024mmmu], RadLE[datta2025radiology], MedXpertQA[zuo2025medxpertqa]) along three axes: inputs, task and label source, and extensibility. RATE is the only framework that takes full-resolution volumes as input, uses clinically grounded tasks with labels derived from routine clinical practice, and can be extended to any radiology image-report dataset. (B) RATE’s benchmark generation engine applies a large language model to unstructured radiology reports to extract answers to a set of clinically grounded queries. (C) RATE-Evals, the model evaluation engine, provides a standardized evaluation protocol with per-task linear probing (RATE Predictors) for any vision model. The inputs to the engine are full-resolution medical volumes and RATE-extracted labels from corresponding reports. Together, RATE and RATE-Evals enable extensible, clinically aligned evaluation of radiology foundation models.",
                "position": 298
            },
            {
                "img": "https://arxiv.org/html/2511.17803/x4.png",
                "caption": "Figure 4:Pillar-0achieves dominant performance over MedGemma, MedImageInsight, Lingshu, and Merlin on internal UCSF test sets across abdomen-pelvis CT, chest CT, head CT, and breast MRI. For each modality,Pillar-0attains the highest average AUROC, with modality-level AUROC improvements of 7.8-15.8 points over the closest baseline. Aggregated over modalities,Pillar-0wins on 319 of 336 findings (87.2%), winning at least 84.3% in every modality.",
                "position": 318
            },
            {
                "img": "https://arxiv.org/html/2511.17803/x5.png",
                "caption": "Figure 5:FinetuningPillar-0sets a new state-of-the-art for future lung cancer risk prediction. (A) Illustration of Sybil-1.5 (Pillar-0finetuned), trained on chest CTs and annotations from NLST to predict multi-year cancer risk and bounding boxes of suspicious regions. (B) Performance of Sybil and Sybil-1.5 on NLST, MGH, and CGMH cohorts, reported as 1-, 3-, and 5-year AUROC and 6-year overall concordance index. Across all datasets and time horizons, Sybil-1.5 improves risk stratification over Sybil.",
                "position": 475
            },
            {
                "img": "https://arxiv.org/html/2511.17803/x6.png",
                "caption": "Figure 6:Pillar-0dramatically improves data efficiency for brain hemorrhage detection on RSNA-2019. (A) AUROC for detecting any intracranial hemorrhage as a function of the fraction of training exams (2.5-100%). Across all data fractionsPillar-0outperforms radiology-pretrained models (Merlin, RadImageNet[radimagenet], MedicalNet[chen2019med3d]; purple) and natural image-pretrained models (Swin3D-K, RN3D-K; gray). With only a small fraction of the training data,Pillar-0matches or exceeds the best baselines trained on the full dataset. (B) Fraction of training data required for each model to reach an AUROC of 95.0;Pillar-0reaches this threshold with 20-40×\\timesfewer samples than baselines. MedicalNet and RadImageNet do not attain this performance even with the full dataset.",
                "position": 490
            }
        ]
    },
    {
        "header": "3Discussion",
        "images": []
    },
    {
        "header": "Appendix AMethods",
        "images": []
    },
    {
        "header": "Appendix BSybil vs Sybil 1.5",
        "images": []
    },
    {
        "header": "Appendix CComparingPillar-0to MedGemma",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17803/x7.png",
                "caption": "Figure 7:Pillar-0vs MedGemma head-to-head on all UCSF Abdomen-Pelvis CT RATE-Evals tasks.Pillar-0wins on 190/210 (90.5%, green bars); MedGemma wins on 20/210 (9.5%, red bars). Part 1/7.",
                "position": 1433
            },
            {
                "img": "https://arxiv.org/html/2511.17803/x8.png",
                "caption": "Figure 8:Pillar-0vs MedGemma head-to-head on all UCSF Abdomen-Pelvis CT RATE-Evals tasks.Pillar-0wins on 190/210 (90.5%, green bars); MedGemma wins on 20/210 (9.5%, red bars). Part 2/7.",
                "position": 1437
            },
            {
                "img": "https://arxiv.org/html/2511.17803/x9.png",
                "caption": "Figure 9:Pillar-0vs MedGemma head-to-head on all UCSF Abdomen-Pelvis CT RATE-Evals tasks.Pillar-0wins on 190/210 (90.5%, green bars); MedGemma wins on 20/210 (9.5%, red bars). Part 3/7.",
                "position": 1440
            },
            {
                "img": "https://arxiv.org/html/2511.17803/x10.png",
                "caption": "Figure 10:Pillar-0vs MedGemma head-to-head on all UCSF Abdomen-Pelvis CT RATE-Evals tasks.Pillar-0wins on 190/210 (90.5%, green bars); MedGemma wins on 20/210 (9.5%, red bars). Part 4/7.",
                "position": 1443
            },
            {
                "img": "https://arxiv.org/html/2511.17803/x11.png",
                "caption": "Figure 11:Pillar-0vs MedGemma head-to-head on all UCSF Abdomen-Pelvis CT RATE-Evals tasks.Pillar-0wins on 190/210 (90.5%, green bars); MedGemma wins on 20/210 (9.5%, red bars). Part 5/7.",
                "position": 1446
            },
            {
                "img": "https://arxiv.org/html/2511.17803/x12.png",
                "caption": "Figure 12:Pillar-0vs MedGemma head-to-head on all UCSF Abdomen-Pelvis CT RATE-Evals tasks.Pillar-0wins on 190/210 (90.5%, green bars); MedGemma wins on 20/210 (9.5%, red bars). Part 6/7.",
                "position": 1449
            },
            {
                "img": "https://arxiv.org/html/2511.17803/x13.png",
                "caption": "Figure 13:Pillar-0vs MedGemma head-to-head on all UCSF Abdomen-Pelvis CT RATE-Evals tasks.Pillar-0wins on 190/210 (90.5%, green bars); MedGemma wins on 20/210 (9.5%, red bars). Part 7/7.",
                "position": 1452
            },
            {
                "img": "https://arxiv.org/html/2511.17803/x14.png",
                "caption": "Figure 14:Pillar-0vs MedGemma head-to-head on all UCSF Chest CT RATE-Evals tasks.Pillar-0wins on 85/92 (92.4%, green bars); MedGemma wins on 7/92 (7.6%, red bars). Part 1/3.",
                "position": 1456
            },
            {
                "img": "https://arxiv.org/html/2511.17803/x15.png",
                "caption": "Figure 15:Pillar-0vs MedGemma head-to-head on all UCSF Chest CT RATE-Evals tasks.Pillar-0wins on 85/92 (92.4%, green bars); MedGemma wins on 7/92 (7.6%, red bars). Part 2/3.",
                "position": 1459
            },
            {
                "img": "https://arxiv.org/html/2511.17803/x16.png",
                "caption": "Figure 16:Pillar-0vs MedGemma head-to-head on all UCSF Chest CT RATE-Evals tasks.Pillar-0wins on 85/92 (92.4%, green bars); MedGemma wins on 7/92 (7.6%, red bars). Part 3/3.",
                "position": 1462
            },
            {
                "img": "https://arxiv.org/html/2511.17803/x17.png",
                "caption": "Figure 17:Pillar-0vs MedGemma head-to-head on all UCSF Head CT RATE-Evals tasks.Pillar-0wins on 28/29 (96.6%, green bars); MedGemma wins on 1/29 (3.4%, red bars).",
                "position": 1466
            },
            {
                "img": "https://arxiv.org/html/2511.17803/x18.png",
                "caption": "Figure 18:Pillar-0vs MedGemma head-to-head on all UCSF Breast MRI RATE-Evals tasks.Pillar-0wins on 33/35 (94.3%, green bars); MedGemma wins on 2/35 (5.7%, red bars).",
                "position": 1470
            }
        ]
    },
    {
        "header": "Appendix DPerformance on full set of RATE-Evals tasks on UCSF Abdomen-Pelvis CT test set",
        "images": []
    },
    {
        "header": "Appendix EPerformance on full set of RATE-Evals tasks on UCSF Chest CT test set",
        "images": []
    },
    {
        "header": "Appendix FPerformance on full set of RATE-Evals tasks on UCSF Head CT test set",
        "images": []
    },
    {
        "header": "Appendix GPerformance on full set of RATE-Evals tasks on UCSF Breast MRI test set",
        "images": []
    },
    {
        "header": "Appendix HPerformance on full set of RATE-Evals tasks on Merlin Abdomen-Pelvis CT test set",
        "images": []
    }
]