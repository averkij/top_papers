[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/teaser/cbox.png",
                "caption": "Figure 1.Examples of triangle-mesh based scenes rendered with\nRenderFormer without per-scene training or fine-tuning that include\n(multiple) specular reflections, complex shadows with details finer\nthan a triangle, diffuse indirect lighting, glossy reflections,\nsoft and hard shadows, and multiple light sources.",
                "position": 202
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/teaser/tree.png",
                "caption": "",
                "position": 207
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/teaser/constant-width-shape.png",
                "caption": "",
                "position": 208
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/teaser/compose.png",
                "caption": "",
                "position": 209
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": []
    },
    {
        "header": "2.Related Work",
        "images": []
    },
    {
        "header": "3.RenderFormer",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21925/x1.png",
                "caption": "Figure 2.RenderFormer Architecture Overview. Top: the\nview-independent stage resolves triangle-to-triangle light\ntransport from a sequence of triangle tokens that encode the\nreflectance properties of each triangle. The relative position\nof each triangle is separately encoded, and applied to each\ntoken at each self-attention layer. Bottom: the view-dependent\nstage takes as input the virtual camera position encoded as a\nsequence of ray-bundles. Guided by the resulting triangle tokens\nfrom the view-independent stage via a cross-attention layer, the\nray-bundle tokens are transformed to tokens encoding the\noutgoing radiance per view ray. Finally, the ray-bundle tokens\nare transformed to log-encoded HDR radiance value through an\nadditional dense vision transformer.",
                "position": 403
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/templates/plane.png",
                "caption": "Figure 3.The four template scenes used for generating training data.",
                "position": 759
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/templates/wall.png",
                "caption": "",
                "position": 764
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/templates/corner.png",
                "caption": "",
                "position": 765
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/templates/box.png",
                "caption": "",
                "position": 766
            }
        ]
    },
    {
        "header": "4.Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/test_examples/wall-single/gt.png",
                "caption": "Figure 4.A variety of scenes rendered with RenderFormer and compared\nto path-traced reference images. We also list\nthe PSNR, SSIM, LPIPS, and FLIP errors.",
                "position": 839
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/test_examples/wall-single/pred.png",
                "caption": "",
                "position": 867
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/test_examples/wall-single/diff.png",
                "caption": "",
                "position": 868
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/test_examples/wall-multi/gt.png",
                "caption": "",
                "position": 870
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/test_examples/wall-multi/pred.png",
                "caption": "",
                "position": 871
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/test_examples/wall-multi/diff.png",
                "caption": "",
                "position": 872
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/test_examples/corner-single/gt.png",
                "caption": "",
                "position": 891
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/test_examples/corner-single/pred.png",
                "caption": "",
                "position": 892
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/test_examples/corner-single/diff.png",
                "caption": "",
                "position": 893
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/test_examples/box-single/gt.png",
                "caption": "",
                "position": 895
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/test_examples/box-single/pred.png",
                "caption": "",
                "position": 896
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/test_examples/box-single/diff.png",
                "caption": "",
                "position": 897
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/timing/rf_render1.png",
                "caption": "Figure 5.Equal-time comparison between RenderFormer and Blender\nCycles with (non-adaptive)26262626sampler-per-pixel and without denoising.",
                "position": 996
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/timing/rf_diff1.png",
                "caption": "",
                "position": 1001
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/timing/pt_render1.png",
                "caption": "",
                "position": 1002
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/timing/pt_diff1.png",
                "caption": "",
                "position": 1003
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/ablations/layers/gt.png",
                "caption": "Figure 6.Qualitative comparison of varying#view-independent + #view-dependentattention layers per\nstage. RenderFormer is shown in the last column with a ratio of12121212view-independent versus6666view-dependent layers.",
                "position": 1036
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/ablations/layers/0-indep-18-dep.png",
                "caption": "",
                "position": 1041
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/ablations/layers/6-indep-12-dep.png",
                "caption": "",
                "position": 1042
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/ablations/layers/9-indep-9-dep.png",
                "caption": "",
                "position": 1043
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/ablations/layers/12-indep-6-dep.png",
                "caption": "",
                "position": 1044
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/interpretation/view-indep-diffuse-rast/pred_0.png",
                "caption": "Figure 7.Visualization of the transformed tokens from the\nview-independent stage that (after transformation) encode smooth\ndiffuse shading and interreflections, as well as shadows at\nsub-triangle granularity.",
                "position": 1131
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/interpretation/view-indep-diffuse-rast/pred_2.png",
                "caption": "",
                "position": 1136
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/interpretation/view-indep-diffuse-rast/pred_3.png",
                "caption": "",
                "position": 1137
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/interpretation/view-indep-diffuse-rast/pred_4.png",
                "caption": "",
                "position": 1138
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/interpretation/view-dep-attn-map/vis_token_r0.3.png",
                "caption": "Figure 8.Visualization of the average attention per triangle for\na given ray-bundle in the view-dependent stage. The average\nattention gives an indication on which triangles RenderFormer uses\nfor computing the outgoing radiance for the rays in the bundle. As\nexpected directly visible triangles and triangles around the\nreflected direction receive the most attention.",
                "position": 1147
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/interpretation/view-dep-attn-map/vis_token_r0.7.png",
                "caption": "",
                "position": 1152
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/interpretation/view-dep-attn-map/vis_token_r0.99.png",
                "caption": "",
                "position": 1153
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/interpretation/view-dep-attn-map/attn_map_o_r0.3.png",
                "caption": "",
                "position": 1156
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/interpretation/view-dep-attn-map/attn_map_o_r0.7.png",
                "caption": "",
                "position": 1157
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/interpretation/view-dep-attn-map/attn_map_o_r0.99.png",
                "caption": "",
                "position": 1158
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/triangle_number/gt.png",
                "caption": "Figure 9.Using larger than normal triangles for the pedestal and\nicosphere results in degraded shadows and shading (2222nd\ncolumn). Interestingly, this degradation is also visible in the\nreflections in the back wall.",
                "position": 1213
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/triangle_number/pred_785_332.png",
                "caption": "",
                "position": 1218
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/triangle_number/pred_2k_1318.png",
                "caption": "",
                "position": 1219
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/triangle_number/pred_6k_6090.png",
                "caption": "",
                "position": 1220
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/light_number/1_pred.png",
                "caption": "Figure 10.RenderFormer can handle multiple light sources with correct\nreflections and shadows (1111st to4444th column) as long as the\nnumber of lights does not exceed8888(as seen in training). For\nmore lights, highlights or shadow might be missing (e.g., the\nmissing double shadow at the bottom shadow in the5555th\ncolumn). In such a case, we can still compute a correct result by\ncompositing multiple single-light images (6666th column).",
                "position": 1248
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/light_number/2_pred.png",
                "caption": "",
                "position": 1253
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/light_number/4_pred.png",
                "caption": "",
                "position": 1254
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/light_number/8_pred.png",
                "caption": "",
                "position": 1255
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/light_number/9_direct_pred_box.png",
                "caption": "",
                "position": 1256
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/light_number/9_add_linear.png",
                "caption": "",
                "position": 1257
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/light_number/9_gt.png",
                "caption": "",
                "position": 1258
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/light_position/gt.png",
                "caption": "Figure 11.Left: RenderFormer was never trained with lights inside the\nscene, and thus fails to correctly render such scenes. Middle:\nRenderFormer can simulate colored lights by leveraging linearity\nof light transport and blending three images (one for each color\nchannel). Right: RenderFormer fails to correctly render scenes\nwith light sources larger than those encountered during\ntraining. Subdividing the triangle can correct the error if the\nnumber of light sources does not exceed the maximum seen during\ntraining (8), in which case we can still leverage linearity of\nlight transport by rendering each subdivided light separately.",
                "position": 1279
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/light_position/pred.png",
                "caption": "",
                "position": 1284
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/light_color/gt.png",
                "caption": "",
                "position": 1285
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/light_color/blend.png",
                "caption": "",
                "position": 1286
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/light_size/gt.png",
                "caption": "",
                "position": 1287
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/light_size/single_tri_pred.png",
                "caption": "",
                "position": 1288
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/light_size/div16_pred.png",
                "caption": "",
                "position": 1289
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/light_size/subdiv_blended.png",
                "caption": "",
                "position": 1290
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/lucy/gt_3k.png",
                "caption": "Figure 12.RenderFormer can handle scenes with more triangles than for\nwhich it was trained, albeit with loss of detail and thin\nfeatures.",
                "position": 1320
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/lucy/gt_11k.png",
                "caption": "",
                "position": 1333
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/lucy/gt_23k.png",
                "caption": "",
                "position": 1334
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/lucy/gt_45k.png",
                "caption": "",
                "position": 1335
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/lucy/rf_3k.png",
                "caption": "",
                "position": 1346
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/lucy/rf_11k.png",
                "caption": "",
                "position": 1347
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/lucy/rf_23k.png",
                "caption": "",
                "position": 1348
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/lucy/rf_45k.png",
                "caption": "",
                "position": 1349
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/camera/train_dis_gt.png",
                "caption": "Figure 13.RenderFormer is robust to moving the camera closer than seen\nduring training (2nd column), as long as the camera remains outside\nthe scene (3rd column). RenderFormer is also robust to exceeding\nthe field of view seen during training (4th-6th column). We also\nfound that RenderFormer fails gracefully when rendering at higher\nresolutions (7th-9th column), with differences around depth\ndiscontinuities (e.g., between the gray and blue walls).",
                "position": 1452
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/camera/closer_gt.png",
                "caption": "",
                "position": 1465
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/camera/inside_gt.png",
                "caption": "",
                "position": 1466
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/camera/fov45_gt.png",
                "caption": "",
                "position": 1467
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/camera/fov15_gt.png",
                "caption": "",
                "position": 1468
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/camera/fov90_gt.png",
                "caption": "",
                "position": 1469
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/camera/res768_gt.png",
                "caption": "",
                "position": 1471
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/camera/res1024_gt.png",
                "caption": "",
                "position": 1472
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/camera/train_dis_pred.png",
                "caption": "",
                "position": 1483
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/camera/closer_pred.png",
                "caption": "",
                "position": 1484
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/camera/inside_pred.png",
                "caption": "",
                "position": 1485
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/camera/fov45_pred.png",
                "caption": "",
                "position": 1486
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/camera/fov15_pred.png",
                "caption": "",
                "position": 1487
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/camera/fov90_pred.png",
                "caption": "",
                "position": 1488
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/camera/res768_pred.png",
                "caption": "",
                "position": 1490
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/camera/res1024_pred.png",
                "caption": "",
                "position": 1491
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/occluders/8layers_pred.png",
                "caption": "Figure 14.RenderFormer can correctly reproduce occlusions for scenes\nwith many objects. However, the shadows cast by occluders with very\ncomplex shapes, can result in a loss of detail in the cast shadow.",
                "position": 1521
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/occluders/8layers_twist_pred.png",
                "caption": "",
                "position": 1526
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/occluders/5x5grid_pred.png",
                "caption": "",
                "position": 1527
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/occluders/5x5grid_height_pred.png",
                "caption": "",
                "position": 1528
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/occluders/rand_pred.png",
                "caption": "",
                "position": 1529
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/occluders/snow2_pred.png",
                "caption": "",
                "position": 1530
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/interreflection/gt2.png",
                "caption": "Figure 15.RenderFormer correctly handles1111and2222recursive\nspecular interreflections. However, due to the scarcity of\ntraining exemplars with more specular interreflections, it does\nnot always correctly resolve higher order reflections (e.g., the\nreflection of the red ball in the reflection of the mirror on\nthe top wall).",
                "position": 1565
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/interreflection/pred2.png",
                "caption": "",
                "position": 1570
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/interreflection/gt.png",
                "caption": "",
                "position": 1571
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/interreflection/pred.png",
                "caption": "",
                "position": 1572
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/interreflection/gt3.png",
                "caption": "",
                "position": 1573
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/generalization/interreflection/pred3.png",
                "caption": "",
                "position": 1574
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/texture_extension/0.png",
                "caption": "Figure 16.Preliminary results of extending RenderFormer to support\nspatially-varying material properties.",
                "position": 1598
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/texture_extension/2.png",
                "caption": "",
                "position": 1603
            },
            {
                "img": "https://arxiv.org/html/2505.21925/extracted/6452747/figures/texture_extension/7.png",
                "caption": "",
                "position": 1604
            }
        ]
    },
    {
        "header": "5.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]