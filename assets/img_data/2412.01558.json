[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01558/x1.png",
                "caption": "Figure 1:Relevance heat map illustrating multimodal alignment dynamics across video understanding models. Color intensity (blue to red) quantifies query-video clip correspondence, with thegreenline indicating ground truth clip-wise saliency. Comparative visualization revealsVideoLights’s progressive refinement of query-clip relevance through projection, feature refinement, and bi-directional cross-attention stages, in contrast to Moment-DETR[16]and QD-DETR[19]’s limited multimodal interaction.",
                "position": 80
            }
        ]
    },
    {
        "header": "IIRelated Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01558/x2.png",
                "caption": "Figure 2:OverallVideoLightsarchitecture. FRA models the video-text cross-modal correlations from projected embeddings and passes them to Bi-CMF in the encoder. A trainable saliency vector predicts output saliency levels. Class and moment prediction heads predict logits and video moments, while saliency cosine similarity and task-coupled HD/MR losses together provide cross-task feedbackUni-JFM. Proposed new losses are in purple.",
                "position": 133
            }
        ]
    },
    {
        "header": "IIIProposedVideoLightsModel",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01558/x3.png",
                "caption": "Figure 3:(a) is the input video, (b) and (c) are correspondence maps of query and video tokens using linear and convolution layers, respectively, which show that queries are more aligned for the convolution layer, video, and text than linear projection layers. (d) The effect of the Feature Refinement module that effectively aligns video and text tokens that match ground truth saliency levels (green line) in each heat map saliency level is shown withgreenline plot.",
                "position": 157
            },
            {
                "img": "https://arxiv.org/html/2412.01558/x4.png",
                "caption": "Figure 4:Bi-CMF learns query-oriented video via text2video, video2text, then text2video attentions. In this process, dropout and normalization are applied after each step, and activation is applied at the last stage.",
                "position": 256
            }
        ]
    },
    {
        "header": "IVExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01558/x5.png",
                "caption": "Figure 5:(a) and (b) show video-query correspondence maps: (a) after text-to-video (t2v) attention and (b) after the Bi-CMF layer. The green line represents the ground truth saliency scores. Bi-CMF attends to the correct video region better than t2v (highlighted in the magenta box). The word ‘Is’ asserts that ‘a’ refers to one basket, unlike ‘is not’.",
                "position": 2235
            },
            {
                "img": "https://arxiv.org/html/2412.01558/extracted/6038314/tr_neg_example.jpg",
                "caption": "(a)",
                "position": 2245
            },
            {
                "img": "https://arxiv.org/html/2412.01558/extracted/6038314/tr_neg_example.jpg",
                "caption": "(a)",
                "position": 2248
            },
            {
                "img": "https://arxiv.org/html/2412.01558/extracted/6038314/both_wrong_but_logical.jpg",
                "caption": "(b)",
                "position": 2254
            },
            {
                "img": "https://arxiv.org/html/2412.01558/extracted/6038314/bi-cmf-count.png",
                "caption": "Figure 7:Empirical analysis reveals optimal Bi-CMF performance varies across datasets: three layers yielded superior results on one benchmark, while a single layer demonstrated peak performance on another. Consequently, we adopted a layer count of one for Bi-CMF across both datasets to ensure consistent cross-modal alignment.",
                "position": 2434
            }
        ]
    },
    {
        "header": "VLimitation and Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]