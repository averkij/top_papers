[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.03270/x1.png",
                "caption": "",
                "position": 162
            },
            {
                "img": "https://arxiv.org/html/2510.03270/x2.png",
                "caption": "",
                "position": 166
            }
        ]
    },
    {
        "header": "1  Introduction",
        "images": []
    },
    {
        "header": "2  Related Works",
        "images": []
    },
    {
        "header": "3  Preliminaries",
        "images": []
    },
    {
        "header": "4  Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.03270/x3.png",
                "caption": "Figure 1:The masking distribution during different stages. Green tiles represent text tokens, blue tiles are mask tokens, and tiles with red border lines indicate tokens that are conditioned not to be masked. During pre-training or mid-training, masking is random. In the post-training stage, a structured masking strategy is applied. For inference, the model is conditioned on a prefix to perform infilling.",
                "position": 308
            },
            {
                "img": "https://arxiv.org/html/2510.03270/x4.png",
                "caption": "Figure 2:A visualization of the masking schedule. S1: a randomly chosen prefix is conditioned and unmaskable; S2: a randomly chosen suffix is replaced with thepadtoken and made unmaskable; S3: A block masking of sizek=2k=2.",
                "position": 336
            }
        ]
    },
    {
        "header": "5  Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.03270/figures/speed_accuracy.png",
                "caption": "Figure 3:Relationship between diffusion steps, inference time, and CoDA-1.7B-Instruct performance. Inference time is measured by the total inference time on the Humaneval dataset. Model performance is measured by pass@1 on the same dataset with a 768-token budget.",
                "position": 655
            }
        ]
    },
    {
        "header": "6  Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]