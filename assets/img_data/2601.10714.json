[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10714/figures/teaser/teaser.jpg",
                "caption": "",
                "position": 89
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10714/figures/overview/overview.jpg",
                "caption": "Figure 2:Overview ofAlterbute.Alterbutefine-tunes a diffusion model for text-guided intrinsic attribute editing.Left (Training):Inputs are arranged in a1×21\\times 2image grid. The left half contains the noisy latent of the target image, while the right half contains a reference image sampled from the same VNE cluster. The model is conditioned on this reference image, a textual prompt describing the desired intrinsic attributes, a background image, and a binary object mask (both represented as grids). The diffusion loss is applied only to the left half to focus the learning on the edited region.Right (Inference):Using the same architecture (grid omitted for clarity),Alterbuteedits the input image directly by reusing its original background and mask. For color, texture, or material edits, we use precise segmentation masks (top). For shape edits where the target geometry is unknown, we use coarse bounding-box masks (bottom).",
                "position": 143
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10714/figures/vne/vne.jpg",
                "caption": "Figure 3:We use Gemini to assign textual VNE labels to objects detected in OpenImages. VNE objects (e.g., “Porsche 911 Carrera”) are grouped into VNE clusters, while unlabeled instances are filtered out. Example VNE clusters are shown on the right. For each VNE-labeled object, we additionally prompt Gemini to extract intrinsic attribute descriptions, which serve as textual promptsppduring training.",
                "position": 160
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10714/figures/results/results.jpg",
                "caption": "Figure 4:Qualitative results across intrinsic editing tasks.Alterbutesuccessfully edits a variety of intrinsic attributes.",
                "position": 226
            },
            {
                "img": "https://arxiv.org/html/2601.10714/figures/qualitative/qualitative.jpg",
                "caption": "Figure 5:Qualitative comparison.Baselines often fail to apply the desired edit or preserve identity.\nIn contrast,Alterbuteproduces edits that faithfully reflect the target attribute while maintaining object identity.",
                "position": 258
            },
            {
                "img": "https://arxiv.org/html/2601.10714/figures/qualitative_specific/specialized_editors.jpg",
                "caption": "Figure 6:Comparison with attribute-specific editors.On the left, for MimicBrush and MaterialFusion, we show the input image, reference image, and their edited output. On the right, we present the result produced byAlterbute.",
                "position": 262
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10714/figures/vne_size_hist/hist_of_cluster_size.png",
                "caption": "Figure 7:Histogram of VNE cluster sizes. The x-axis shows cluster size (number of instances), and the y-axis shows the number of clusters on a log scale. Clusters with more than3,0003,000instances are grouped into the last bin.",
                "position": 350
            },
            {
                "img": "https://arxiv.org/html/2601.10714/figures/vne_class_freq/histogram_of_classes.png",
                "caption": "Figure 8:Distribution of VNE clusters across the top3030object classes, sorted by frequency. Each bar shows the number of VNE clusters per class; all remaining classes are grouped into “Other”.",
                "position": 367
            },
            {
                "img": "https://arxiv.org/html/2601.10714/figures/ablation/ablation.jpg",
                "caption": "Figure 9:Ablation on identity definitions.Comparison of identity reference strategies: in-place, DINOv2, IR, and our VNE-based approach. Each row shows the input and target attribute (left), followed by edited results under each identity definition.",
                "position": 378
            }
        ]
    },
    {
        "header": "5Discussion & Limitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10714/figures/limitations/limitations.jpg",
                "caption": "Figure 10:Limitations ofAlterbute.Top:Background artifacts may occur with coarse bounding box masks.Bottom:Shape edits may produce unrealistic or unintended geometries.",
                "position": 398
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ABaselines",
        "images": []
    },
    {
        "header": "Appendix BQuantitative Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10714/figures/SM/user_study/user_study.png",
                "caption": "Figure SM.1:A screenshot of the user study questionnaire.",
                "position": 1316
            }
        ]
    },
    {
        "header": "Appendix CAdditional Results",
        "images": []
    },
    {
        "header": "Appendix DIdentity Definitions",
        "images": []
    },
    {
        "header": "Appendix EEditing Both Intrinsic and Extrinsic Attributes",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10714/figures/SM/qualitative/SM_qualitative_1.jpg",
                "caption": "Figure SM.2:Additional qualitative comparisons against general-purpose image-and-text-to-image editing methods.",
                "position": 1429
            },
            {
                "img": "https://arxiv.org/html/2601.10714/figures/SM/qualitative/SM_qualitative_2.jpg",
                "caption": "Figure SM.3:Additional qualitative comparisons against general-purpose image-and-text-to-image editing methods.",
                "position": 1432
            },
            {
                "img": "https://arxiv.org/html/2601.10714/figures/SM/qualitative/SM_qualitative_3.jpg",
                "caption": "Figure SM.4:Additional qualitative comparisons against general-purpose image-and-text-to-image editing methods.",
                "position": 1435
            },
            {
                "img": "https://arxiv.org/html/2601.10714/figures/SM/qualitative/SM_qualitative_4.jpg",
                "caption": "Figure SM.5:Additional qualitative comparisons against general-purpose image-and-text-to-image editing methods.",
                "position": 1438
            },
            {
                "img": "https://arxiv.org/html/2601.10714/figures/SM/qualitative/SM_qualitative_5.jpg",
                "caption": "Figure SM.6:Additional qualitative comparisons against general-purpose image-and-text-to-image editing methods.",
                "position": 1441
            },
            {
                "img": "https://arxiv.org/html/2601.10714/figures/SM/qualitative/SM_qualitative_6.jpg",
                "caption": "Figure SM.7:Additional qualitative comparisons against general-purpose image-and-text-to-image editing methods.",
                "position": 1444
            },
            {
                "img": "https://arxiv.org/html/2601.10714/figures/SM/qualitative_specific/SM_specialized_editors_1.jpg",
                "caption": "Figure SM.8:Additional qualitative comparisons against attribute-specific edits.",
                "position": 1447
            },
            {
                "img": "https://arxiv.org/html/2601.10714/figures/SM/qualitative_specific/SM_specialized_editors_2.jpg",
                "caption": "Figure SM.9:Additional qualitative comparisons against attribute-specific edits.",
                "position": 1450
            },
            {
                "img": "https://arxiv.org/html/2601.10714/figures/SM/clusters/dino_clusters.jpg",
                "caption": "Figure SM.10:Example clusters retrieved using DINOv2 feature similarity, with each row showing images from the same cluster.",
                "position": 1453
            },
            {
                "img": "https://arxiv.org/html/2601.10714/figures/SM/clusters/ir_clusters.jpg",
                "caption": "Figure SM.11:Example clusters retrieved using IR feature similarity, with each row showing images from the same cluster.",
                "position": 1456
            },
            {
                "img": "https://arxiv.org/html/2601.10714/figures/SM/clusters/vne_clusters.jpg",
                "caption": "Figure SM.12:Example of VNE clusters.",
                "position": 1459
            },
            {
                "img": "https://arxiv.org/html/2601.10714/figures/SM/multiple/multiple.jpg",
                "caption": "Figure SM.13:Alterbuteresults on multi-attribute editing, showing its ability to apply compatible edits while preserving identity and context.",
                "position": 1462
            },
            {
                "img": "https://arxiv.org/html/2601.10714/figures/SM/extrinsic/extrinsic_4.jpg",
                "caption": "Figure SM.14:Alterbuteedits both intrinsic and extrinsic attributes by composing the reference object into a new scene and applying the specified intrinsic change. An empty prompt preserves all intrinsic attributes for identity-preserving object insertion.",
                "position": 1465
            },
            {
                "img": "https://arxiv.org/html/2601.10714/figures/SM/intrinsic_labels/intrinsic_labels.jpg",
                "caption": "Figure SM.17:Examples of intrinsic attribute descriptions generated by Gemini. Each object is paired with a structured key-value description covering color, texture, material, and shape, based solely on visual input.",
                "position": 1791
            },
            {
                "img": "https://arxiv.org/html/2601.10714/figures/results_with_inputs/results_with_inputs.jpg",
                "caption": "Figure SM.18:Model inputs and outputs.We show the inputs that conditionAlterbute, the object mask, background image, identity reference and the textual prompt. alongside the resulting edited output.",
                "position": 1794
            }
        ]
    },
    {
        "header": "Appendix FGemini-Based Labeling Pipeline",
        "images": []
    }
]