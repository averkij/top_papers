[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20444/x1.png",
                "caption": "Figure 1:Comparison of our HoPE and existing methods.Upper plots illustrate the frequency allocation strategies in different RoPE variants. Here, frequency decreases along the diagonal. (d) HoPE sets the lowest frequencies to zero for reliable long-range semantic modeling. Lower plots demonstrate different temporal scaling mechanisms. (d) HoPE proposes dynamic and bidirectional scaling to learn temporal dynamics at multiple scales, facilitating robustness to various video speeds.",
                "position": 96
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Analysis",
        "images": []
    },
    {
        "header": "4HoPE: Hybrid of Position Embedding for Vision-Language Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20444/x2.png",
                "caption": "(a)High frequencies for temporal modeling in M-RoPE.",
                "position": 341
            },
            {
                "img": "https://arxiv.org/html/2505.20444/x2.png",
                "caption": "(a)High frequencies for temporal modeling in M-RoPE.",
                "position": 344
            },
            {
                "img": "https://arxiv.org/html/2505.20444/x3.png",
                "caption": "(b)Low frequencies for temporal modeling in VideoRoPE.",
                "position": 349
            },
            {
                "img": "https://arxiv.org/html/2505.20444/x4.png",
                "caption": "(c)Zero frequencies for temporal modeling in HoPE (ours).",
                "position": 354
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20444/x5.png",
                "caption": "Figure 3:Performance on long video retrieval task (V-NAIH). The black dotted line represents the training context length. Each frame corresponds to 144 tokens.",
                "position": 649
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProofs",
        "images": []
    },
    {
        "header": "Appendix BFurther Experimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20444/x6.png",
                "caption": "Figure 4:Illustration of V-NIAH, which consists of a randomly inserted needle image, a haystack video, and a specific question related to the needle.",
                "position": 1584
            }
        ]
    },
    {
        "header": "Appendix CLimitations",
        "images": []
    }
]