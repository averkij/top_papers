[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.10015/x1.png",
                "caption": "Figure 1:We train connectors between pretrained uni-modal models to show that uni-modal model performance isnot predictiveof multi-modal performance obtained by stitching. Image encoder performance refers to top-1 ImageNet-1K accuracy, text encoder performance refers to semantic search performance across 14 datasetsReimers and Gurevych (2019). Multi-modal scores refers to ImageNet-1K top-1 accuracy (classification by matching images to prompts such as ‚Äúthis is a photo of a{class})‚Äù.222All model abbreviations can be found in AppendixB.",
                "position": 166
            },
            {
                "img": "https://arxiv.org/html/2507.10015/x2.png",
                "caption": "Figure 2:Given multiple options for uni-modal models, pair-wise grid search can be an expensive way to determine the best multi-modal combination. Alternatively,Hymaformulates search as a predictive or generative process.",
                "position": 169
            }
        ]
    },
    {
        "header": "2Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.10015/x3.png",
                "caption": "Figure 3:A visual walkthrough of our hypernetwork architecture is provided above. We take the example of predicting the parameters of an MLP-type connector with depth=3absent3=3= 3(denotes2222hidden layers).",
                "position": 282
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.10015/x4.png",
                "caption": "Figure 4:MLP1|N√óM=3ùëÅùëÄ3N\\times M=3italic_N √ó italic_M = 3: We show the trade-off between computational resources (measured in FLOPs) and performance of the best stitched model pairs across all comparative baselines. We find thatHymais able to predict a highly performance pairing at a significantly reduced FLOP cost in comparison to training on the optimal model pair as well as search over all model pairs forN√óM=3ùëÅùëÄ3N\\times M=3italic_N √ó italic_M = 3.",
                "position": 417
            },
            {
                "img": "https://arxiv.org/html/2507.10015/x5.png",
                "caption": "",
                "position": 420
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Empirical Results",
        "images": []
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.10015/x6.png",
                "caption": "Figure 5:Evaluation ofHymafor MLLMs, on MSCOCO and Flickr-8K (N=1,M=3,Bm=1formulae-sequenceùëÅ1formulae-sequenceùëÄ3subscriptùêµùëö1N=1,M=3,B_{m}=1italic_N = 1 , italic_M = 3 , italic_B start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT = 1). We report the model combination exhibiting the best final performance for each evaluation benchmark and search method.",
                "position": 1827
            },
            {
                "img": "https://arxiv.org/html/2507.10015/x7.png",
                "caption": "",
                "position": 1830
            }
        ]
    },
    {
        "header": "Appendix AHymafor Multi-modal Large Language Models (MLLMs)",
        "images": []
    },
    {
        "header": "Appendix BPretrained models",
        "images": []
    },
    {
        "header": "Appendix CDesigning the Model Zoo",
        "images": []
    },
    {
        "header": "Appendix DTraining and hyper-parameter details",
        "images": []
    },
    {
        "header": "Appendix EFactors impacting FLOPs",
        "images": []
    },
    {
        "header": "Appendix FDetails of VQA implemention",
        "images": []
    },
    {
        "header": "Appendix GDetails of baselines",
        "images": []
    }
]