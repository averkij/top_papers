[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24657/figures/intro-cvpr-1.jpg",
                "caption": "Figure 2:presents the visualization of the embedding features input to the attention layer, where a significant bias can be observed across different tokens.",
                "position": 82
            },
            {
                "img": "https://arxiv.org/html/2510.24657/figures/intro-cvpr-2.jpg",
                "caption": "Figure 3:Group Relative Attention Guidance.",
                "position": 101
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Bias Vector In The Embedding Vectors",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24657/figures/embedding-analysis-1.jpg",
                "caption": "Figure 4:(Kontext-Layer 2) Aggregating different tokens along the sequence dimension, we visualize the embedding features across the dimension and head axes. The visual features are concentrated at positions corresponding to high RoPE frequencies, while textual features are associated with low frequencies.",
                "position": 199
            },
            {
                "img": "https://arxiv.org/html/2510.24657/figures/embedding-analysis-2-cvpr.jpg",
                "caption": "Figure 5:(Kontext-Layer 2) Mean vector magnitudes and standard deviations across different attention heads. A significant bias vector exists in the embedding space.",
                "position": 220
            },
            {
                "img": "https://arxiv.org/html/2510.24657/figures/method.jpg",
                "caption": "Figure 6:An illustration of applying Group Relative Attention Guidance in the MM-DiT image editing model. (a) The MM-Attention map corresponding to the queryQeQ_{\\mathrm{e}}, where GRAG is applied. (b) The processing of relative modulation to the source image’s key embeddings. Red denotes enhanced tokens, while blue denotes suppressed tokens.",
                "position": 223
            }
        ]
    },
    {
        "header": "5Group Relative Attention Guidance",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24657/figures/comparsion-1.jpg",
                "caption": "Figure 7:Visualization results on training-based image editing method.",
                "position": 322
            },
            {
                "img": "https://arxiv.org/html/2510.24657/figures/comparsion-1-traning-free.jpg",
                "caption": "Figure 8:Visualization results on training-free image editing method. We update the original first-order inversion in StableFlow with a second-order ODE inversion method[37,27], referred to as StableFlow++.",
                "position": 325
            }
        ]
    },
    {
        "header": "6Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24657/figures/comparsion-2.jpg",
                "caption": "Figure 9:Visualization results of CFG and GRAG under different scales. Compared to CFG, GRAG more effectively regulates the influence of editing instructions on the original image, demonstrating a more accurate and continuous guidance process.",
                "position": 347
            },
            {
                "img": "https://arxiv.org/html/2510.24657/figures/ablation-figure.jpg",
                "caption": "Figure 10:Comparison of different guidance strategies under varying guidance strengths. The data in the line chart correspond to Table2. Theδ\\deltaparameter yields the most continuous and effective editing guidance.",
                "position": 358
            }
        ]
    },
    {
        "header": "7Discussion & Limitation",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "APytorch Implementation of GRAG",
        "images": []
    },
    {
        "header": "BAdditional Feature Visualization",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24657/figures/supp-vis-3d.jpg",
                "caption": "Figure S1:Additional visualizations of text and image embedding features.\nFeatures within the same layer share similar distributions, indicating limited correlation with model inputs or denoising steps.\nPlease zoom in to view finer details.",
                "position": 1417
            },
            {
                "img": "https://arxiv.org/html/2510.24657/figures/supp-vis-2d-1.jpg",
                "caption": "Figure S2:Additional visualizations of aggregating different tokens along the sequence dimension.\nPlease zoom in to view finer details.",
                "position": 1422
            },
            {
                "img": "https://arxiv.org/html/2510.24657/figures/supp-vis-2d-2-q-edit.jpg",
                "caption": "Figure S3:Additional visualizations of Query-edit embedding mean vector magnitudes and standard deviations across different attention heads.\nPlease zoom in to view finer details.",
                "position": 1426
            },
            {
                "img": "https://arxiv.org/html/2510.24657/figures/supp-vis-2d-2-k-txt.jpg",
                "caption": "Figure S4:Additional visualizations of Key-text embedding mean vector magnitudes and standard deviations across different attention heads.\nPlease zoom in to view finer details.",
                "position": 1430
            },
            {
                "img": "https://arxiv.org/html/2510.24657/figures/supp-vis-2d-2-k-edit.jpg",
                "caption": "Figure S5:Additional visualizations of Key-edit embedding mean vector magnitudes and standard deviations across different attention heads.\nPlease zoom in to view finer details.",
                "position": 1434
            },
            {
                "img": "https://arxiv.org/html/2510.24657/figures/supp-vis-2d-2-k-src.jpg",
                "caption": "Figure S6:Additional visualizations of Key-src embedding mean vector magnitudes and standard deviations across different attention heads.\nPlease zoom in to view finer details.",
                "position": 1438
            }
        ]
    },
    {
        "header": "CThe Use of Large Language Models",
        "images": []
    }
]