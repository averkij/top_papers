[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13660/x1.png",
                "caption": "",
                "position": 175
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13660/x2.png",
                "caption": "Figure 2:Overview ofRoboTracer.RoboTracercan process RGB images and task instructions, while flexibly integrating various geometric configurations (e.g., absolute depth, camera intrinsics) when available to improve spatial precision, enabled by the integrated universal spatial encoder.\nIt also has a scale decoder to output a metric scale factor supervised by a regression loss beyond next-token prediction to bolster real-world scale awareness.\nAfter SFT, metric-sensitive reward functions in RFT further supervise the key perceptual objects involved in the trace and offer crucial intermediate evidence (e.g.,3D spatial referring and measuring) for accurate spatial trace generation.",
                "position": 250
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13660/x3.png",
                "caption": "Figure 3:Data Construction Pipeline.TraceSpatialhas 4.5M data samples from 2D/3D/video sources, covering outdoor/indoor/table scenes.\nIt contains not only metric-agnostic QA pairs, but also metric-grounded QA pairs for 3D spatial referring/measuring/tracing.",
                "position": 287
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13660/x4.png",
                "caption": "Figure 4:TraceSpatial-BenchResults.\nWhite masks denote ground-truth 3D starting region; pink 3D boxes mark correct end regions.\nDespite similar 2D projections (left views of each case), our model yields more accurate spatial traces than strong general VLMs, which often produce floating or colliding traces due to inaccurate depth estimation.\nLeveraging richer geometric cues further improves performance.",
                "position": 837
            },
            {
                "img": "https://arxiv.org/html/2512.13660/x5.png",
                "caption": "Figure 5:Real-World Evaluation. The blue trace denotes predicted spatial trace in 2D, and the blue dot marks the current target.RoboTracercan generate collision-free spatial traces whose start and end points all satisfy spatial constraints in cluttered and dynamic scenes.",
                "position": 1508
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix ATraceSpatialDetails",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13660/fig/supp/2d_scene_graph.png",
                "caption": "Figure 6:The visualization of pseudo-3D scene graphs generated from 2D images with detected objects, and corresponding point clouds.",
                "position": 2180
            },
            {
                "img": "https://arxiv.org/html/2512.13660/x6.png",
                "caption": "Figure 7:Generated object descriptions. Top: Unique-category captions. Bottom: Spatially-aware captions for the same categories. Red boxes indicate referenced objects.",
                "position": 2227
            },
            {
                "img": "https://arxiv.org/html/2512.13660/x7.png",
                "caption": "Figure 8:The visualization of generated image detailed descriptions from VLM (e.g., Qwen2.5-VL).",
                "position": 2382
            },
            {
                "img": "https://arxiv.org/html/2512.13660/fig/supp/scannet_2d_bbox.png",
                "caption": "Figure 9:The visualization of the 2D bounding box generation pipeline for the ScanNet dataset.",
                "position": 2574
            },
            {
                "img": "https://arxiv.org/html/2512.13660/fig/supp/Spatial_Measuring.png",
                "caption": "Figure 10:The visualization of our semantic dimension definitions.Length (red)is defined as the intersection of the front and bottom faces.Width (green)is the intersection of the side and bottom faces.Height (blue)is the intersection of the front and side faces.",
                "position": 2589
            },
            {
                "img": "https://arxiv.org/html/2512.13660/x8.png",
                "caption": "Figure 11:Visual Taxonomy of Manipulation Primitives.We define five distinct primitives ranging from simple relative placement (Method 1) to complex active obstacle bypass and stacking (Methods 4–5). Additionally, we retroactively identify “Potential Via Objects” (bottom) for standard traces to enrich spatial descriptions in cluttered scenes.",
                "position": 2777
            },
            {
                "img": "https://arxiv.org/html/2512.13660/",
                "caption": "Figure 12:Extract end-effector-centric and object-centric traces from Droid’s original data by projecting gripper positions into a base frame; downsample traces using RDP and remove tasks with too many points after downsampling.",
                "position": 3226
            },
            {
                "img": "https://arxiv.org/html/2512.13660/x10.png",
                "caption": "Figure 13:Visualization of the end-effector coordinate frame. We obtain the gripper position (blue point) by extending the end-effector’s localzz-axis and record both the spatial trace of the gripper and its open/closed states.",
                "position": 3230
            },
            {
                "img": "https://arxiv.org/html/2512.13660/x11.png",
                "caption": "Figure 14:Preview images from the Droid raw dataset with the end-effector projected as a red dot. Each row shows a pair of images from the same scene: the left image corresponds to incorrect camera extrinsics, and the right image corresponds to correct extrinsics.",
                "position": 3243
            },
            {
                "img": "https://arxiv.org/html/2512.13660/x12.png",
                "caption": "Figure 15:Droid’s 2D and 3D trace visualizations under the object-centric setting. The 3D traces are visualized in the point-cloud space.",
                "position": 3258
            },
            {
                "img": "https://arxiv.org/html/2512.13660/x13.png",
                "caption": "Figure 16:DROID depth-alignment results. We verify camera extrinsics by projecting the end-effector into the image and comparing pixel depth with the camera-frame end-effectorzzvalue; extrinsics are accepted if over13\\frac{1}{3}frames have a depth difference under 5 cm.",
                "position": 3323
            },
            {
                "img": "https://arxiv.org/html/2512.13660/x14.png",
                "caption": "Figure 17:Common issues in the AgiBot dataset.\n(a)Meaningless Traces.For certain dual-arm collaborative tasks, spatial traces alone cannot accurately describe the motion, making them difficult to interpret even for humans.\n(b)Inaccurate Motion Segmentation.The original segmentation is often temporally misaligned; for example, trace recording continues even after the object has already been placed on the platform or inserted into the container.\n(c)Invalid Extrinsics.Incorrect camera extrinsics lead to inaccurate trace projections.\n(d)Occlusion.The manipulated object (highlighted in the red box) is occluded in the image, rendering the resulting trace meaningless.",
                "position": 3327
            },
            {
                "img": "https://arxiv.org/html/2512.13660/x15.png",
                "caption": "Figure 18:Proportion of retained samples under different threshold values. A sample is considered to have correct extrinsics if the proportion of aligned frames exceeds a given threshold.",
                "position": 3363
            },
            {
                "img": "https://arxiv.org/html/2512.13660/fig/supp/agibot2.png",
                "caption": "Figure 19:Visualizing all samples as a 2D grid (black = misaligned). Misaligned frames form spatially contiguous clusters, consistent with the observation that extrinsic failures occur in temporally coherent segments under fixed scene configurations.",
                "position": 3366
            }
        ]
    },
    {
        "header": "Appendix BTraceSpatial-BenchDetails",
        "images": []
    },
    {
        "header": "Appendix CRoboTracerDetails",
        "images": []
    },
    {
        "header": "Appendix DExperimental Setting and Details",
        "images": []
    },
    {
        "header": "Appendix EMore Demonstrations",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13660/x16.png",
                "caption": "Figure 20:Visualization of TraceSpatial-Bench andRoboTracer’s rollouts. The red mask indicates the ground-truth starting point, and the purple 3D bounding box denotes the ground-truth endpoint. We show the 2D projection ofRoboTracer’s predicted trace.",
                "position": 4760
            },
            {
                "img": "https://arxiv.org/html/2512.13660/x17.png",
                "caption": "Figure 21:Visualization of TraceSpatial-Bench andRoboTracer’s rollouts. The red mask indicates the ground-truth starting point, and the purple 3D bounding box denotes the ground-truth endpoint. We show the 2D projection ofRoboTracer’s predicted trace.",
                "position": 4763
            },
            {
                "img": "https://arxiv.org/html/2512.13660/x18.png",
                "caption": "Figure 22:Visualization of TraceSpatial-Bench andRoboTracer’s rollouts. The red mask indicates the ground-truth starting point, and the purple 3D bounding box denotes the ground-truth endpoint. We show the 2D projection ofRoboTracer’s predicted trace.",
                "position": 4766
            },
            {
                "img": "https://arxiv.org/html/2512.13660/x19.png",
                "caption": "Figure 23:RoboTwin Data Visualization",
                "position": 4769
            },
            {
                "img": "https://arxiv.org/html/2512.13660/x20.png",
                "caption": "Figure 24:RoboTwin Data Visualization",
                "position": 4772
            },
            {
                "img": "https://arxiv.org/html/2512.13660/x21.png",
                "caption": "Figure 25:RoboTwin Data Visualization",
                "position": 4775
            },
            {
                "img": "https://arxiv.org/html/2512.13660/x22.png",
                "caption": "Figure 26:RoboTwin Data Visualization",
                "position": 4778
            },
            {
                "img": "https://arxiv.org/html/2512.13660/x23.png",
                "caption": "Figure 27:Visualized RoboTwin simulation evaluation process.",
                "position": 4781
            },
            {
                "img": "https://arxiv.org/html/2512.13660/x24.png",
                "caption": "Figure 28:Visualized RoboTwin simulation evaluation process.",
                "position": 4784
            },
            {
                "img": "https://arxiv.org/html/2512.13660/x25.png",
                "caption": "Figure 29:Visualized RoboTwin simulation evaluation process.",
                "position": 4787
            },
            {
                "img": "https://arxiv.org/html/2512.13660/x26.png",
                "caption": "Figure 30:Visualized RoboTwin simulation evaluation process.",
                "position": 4790
            },
            {
                "img": "https://arxiv.org/html/2512.13660/x27.png",
                "caption": "Figure 31:Visualized RoboTwin simulation evaluation process.",
                "position": 4793
            },
            {
                "img": "https://arxiv.org/html/2512.13660/x28.png",
                "caption": "Figure 32:Visualized RoboTwin simulation evaluation process.",
                "position": 4796
            }
        ]
    },
    {
        "header": "Appendix FDiscussion on Limitations and Future Work",
        "images": []
    }
]