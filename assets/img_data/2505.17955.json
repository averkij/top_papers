[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17955/x1.png",
                "caption": "Figure 1:Overview of our findings.Finding I: Diffusion models can perform compositional discrimination reasonably on real images, but underperform CLIP, especially on counting tasks (¬ß5.2).Finding II: Diffusion models can understand (through classification) the images they can generate (¬ß5.3).Finding III: Timestep reweighting improves discrimination by reducing the domain gap between generated and real data (¬ß5.4).",
                "position": 96
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17955/x2.png",
                "caption": "Figure 2:Examples of standard benchmarks vs.Self-Bench.Each benchmark is categorized into four broad task groups:Object,Attribute,Position, andCounting. Each group consists of one or more tasks, and we present one example per task for illustration. We indicatepositive/negativecaptions, where the task involves matching the positive caption with its corresponding image.\nNotably, standard benchmarks andSelf-Benchfeature domain distinctions, incorporating the factors like style, resolution, and object scale.",
                "position": 239
            }
        ]
    },
    {
        "header": "4Self-Bench: a diagnostic benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17955/x3.png",
                "caption": "Figure 3:Diagnosing withSelf-Bench. (i) Using Geneval‚Äôs prompts from six types, generate images. (ii) For each generated image, create discriminative tasks within its type from the prompts used in the generation process. (iii) Given the generated images (filtered by humans) and the discriminative tasks, benchmark the diffusion classifier.",
                "position": 257
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17955/x4.png",
                "caption": "Figure 4:Evaluating compositional generalization across different categories. The bars represent average classification accuracies across all tasks within each category. Notably, SD3-m does not generally outperform other Stable Diffusion models in most benchmarks, and CLIP usually outperforms diffusion models.",
                "position": 434
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x5.png",
                "caption": "Figure 5:Self-BenchIn-domain performance. (Top three plots) Each row represents the classification accuracy of a diffusion classifier from a specific SD model when evaluated on its own generated data.\n(Bottom) A positive correlation is observed between generative and discriminative performance. Left axis: discrimination; right axis: generation accuracy.",
                "position": 456
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x6.png",
                "caption": "Figure 6:Self-Bench:Cross-domain performance degradation.The bars represent average drop rate betweenin-domainandcross-domainevaluation, averaged over different cross-domain settings. We observe significant accuracy drops when evaluating models oncross-domaindata. SD3-m shows the most severe degradation, with up to 38% accuracy loss in two-object tasks and 33-40% drops in color and spatial tasks.",
                "position": 475
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x7.png",
                "caption": "Figure 7:Self-Bench:Single-timestep reconstruction error and classification accuracy.While SD2.0 maintains good performance on SD3-m‚Äôs generations, SD3-m exhibits near-zero accuracy for the majority of initial timesteps on SD2.0‚Äôs generations, particularly for object recognition tasks.",
                "position": 505
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x8.png",
                "caption": "Figure 8:Low-shot timestep reweighting is effective in real-world benchmarks.Left:Accuracy gains on diverse compositional tasks achieved by reweighting diffusion timesteps for Stable Diffusion variants (SD1.5, SD2.0, SD3-m). Reweighted models consistently outperform the baseline; the gains are most pronounced for the SD3-m model. The numbers above the bars indicate the scores after reweighting, while the numbers inside the bars show the original scores. Positive deltas are highlighted using the reweighted color.Right:Learned timestep weighting schemes indicating task-dependent emphasis on specific diffusion steps (early, middle, or late), demonstrating the importance of tailoring timestep selection to the task structure.",
                "position": 520
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x9.png",
                "caption": "Figure 9:Timestep Weighting and Domain Gap.CLIP distances between real-world datasets andSelf-Benchgenerations, and corresponding accuracy gains from timestep weighting. Larger domain gaps correlate with greater improvements, but only for SD3.",
                "position": 524
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x10.png",
                "caption": "Figure 10:Intuition for timestep utility.We visualize SD3-m generations starting from different noise levels applied to a Self-Bench image. (Top) Conditioning on an incorrect caption. (Bottom) Conditioning on the correct one. Only for intermediate timesteps (e.g.,t‚àà[0.73,0.93]ùë°0.730.93t\\in[0.73,0.93]italic_t ‚àà [ 0.73 , 0.93 ]) does the model apply meaningful edits without overwriting the original image. See main text for explanation.",
                "position": 539
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17955/x11.png",
                "caption": "Figure A.1:Comparison between Zero-shot classifier and Discffusion onSelf-Benchin-domain. Zero-shot Classifier and Discffusion do not show much performance difference, or Discffusion performs worse.",
                "position": 1692
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x12.png",
                "caption": "Figure A.2:Example of a filtering userspace.The annotator should select one among three options. Before they annotate, they get the instructions for each category.",
                "position": 2606
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x13.png",
                "caption": "Figure A.3:Images filtered out by annotators.Annotators removed images that did not meet the criteria specified in the category instructions.",
                "position": 2609
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x14.png",
                "caption": "Figure A.4:Image generations of SD3-m with style alignment.Top: CLEVR dataset; Bottom: Whatsup-A dataset.",
                "position": 2620
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x15.png",
                "caption": "Figure A.5:Image generation results on Whatsup-A using SD2.0 and SD3-m models.",
                "position": 2825
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x16.png",
                "caption": "Figure A.6:Self-Bench: Timestep reweighting helps address the cross-domain problem.We show the performance of each SD model on cross-domain data (e.g., for the SD1.5 model, the bars depict its performance on SD2.0 and SD3-m generations, averaged). All models benefit from timestep reweighting, with SD3-m benefiting the most.",
                "position": 2999
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x17.png",
                "caption": "Figure A.7:Domain differences between existing compositional benchmarks and SD3-m generated results.(top) Examples from existing compositional benchmarks: SPECPeng et¬†al. (2024)and WhatsUP AKamath et¬†al. (2023).\n(bottom left)Self-Benchexamples.\n(bottom middle) Generated images using prompts from SPEC count tasks.\n(bottom right) Generated images using prompts from WhatsUPA tasks.",
                "position": 3201
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x18.png",
                "caption": "(a)SD 1.5",
                "position": 3209
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x18.png",
                "caption": "(a)SD 1.5",
                "position": 3212
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x19.png",
                "caption": "(b)SD 2.0",
                "position": 3218
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x20.png",
                "caption": "(c)SD 3-m",
                "position": 3224
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x21.png",
                "caption": "(a)SD 1.5",
                "position": 3231
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x21.png",
                "caption": "(a)SD 1.5",
                "position": 3234
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x22.png",
                "caption": "(b)SD 2.0",
                "position": 3240
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x23.png",
                "caption": "(c)SD 3-m",
                "position": 3246
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x24.png",
                "caption": "(a)Spatial",
                "position": 3255
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x24.png",
                "caption": "(a)Spatial",
                "position": 3258
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x25.png",
                "caption": "(b)Attribute",
                "position": 3264
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x26.png",
                "caption": "(c)Counting",
                "position": 3270
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x27.png",
                "caption": "(d)Object",
                "position": 3275
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x28.png",
                "caption": "(e)Complex (Attributes, Relation)",
                "position": 3281
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x29.png",
                "caption": "",
                "position": 3287
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x30.png",
                "caption": "(f)size",
                "position": 3292
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x31.png",
                "caption": "(g)action",
                "position": 3297
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x32.png",
                "caption": "(h)presence",
                "position": 3302
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x33.png",
                "caption": "(a)Average per sample onSelf-Bench.",
                "position": 4584
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x33.png",
                "caption": "(a)Average per sample onSelf-Bench.",
                "position": 4587
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x34.png",
                "caption": "(b)Macro average onSelf-Bench.",
                "position": 4593
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x35.png",
                "caption": "Figure A.12:Self-Benchcross-domaindrop rate.WE show the performance drop when evaluating models incross-domainsettings.",
                "position": 4601
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x35.png",
                "caption": "",
                "position": 4604
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x36.png",
                "caption": "",
                "position": 4609
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x37.png",
                "caption": "",
                "position": 4614
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x38.png",
                "caption": "(a)Spatial",
                "position": 7379
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x38.png",
                "caption": "(a)Spatial",
                "position": 7382
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x39.png",
                "caption": "(b)Attribute",
                "position": 7388
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x40.png",
                "caption": "",
                "position": 7394
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x41.png",
                "caption": "(c)Counting",
                "position": 7400
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x42.png",
                "caption": "",
                "position": 7403
            },
            {
                "img": "https://arxiv.org/html/2505.17955/x43.png",
                "caption": "(d)Object",
                "position": 7408
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]