[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02046/x1.png",
                "caption": "Figure 1:Top Left: \nThe commonly used AdamW baseline for optimizer design is under-tuned. Up to a 2×\\timesspeedup is achievable by tuning a single hyperparameter (learning rate) in the GPT-3 recipeBrown et al. (2020)for a 100M model (adopted inLiu et al. (2024a); Wen et al. (2024); Yuan et al. (2025); Liang et al. (2025); Wang et al. (2025)), highlighting the importance of proper hyperparameter optimization.Top Right: Fixing hyperparameters across optimizers does not guarantee fair comparison. Shared hyperparameters such as learning rate and weight decay are commonly set to a constant in previous studies. However, even conceptually similar optimizers may correspond to very different optimal hyperparameters.Bottom Left: Speedup decays with model size. While some optimizers show high (1.3-1.4×\\times) speedup over AdamW on models under 1B parameters, the speedup decays with model size to only 1.1×\\timesfor 1.2B parameters.Bottom Right: Matrix-based optimizers consistently outperform scalar-based optimizers.\nThe loss curves for three scalar-based optimizers (AdamW, Nesterov AdamW, Mars) and three matrix-based optimizers (Kron, Soap, Muon) trained with different Chinchilla ratios of data are shown. Matrix-based optimizers achieve a consistent speedup over scalar-based optimizers. Furthermore, the three matrix-based optimizers converge to a similar loss in an overtrained setting.",
                "position": 166
            },
            {
                "img": "https://arxiv.org/html/2509.02046/x2.png",
                "caption": "",
                "position": 175
            },
            {
                "img": "https://arxiv.org/html/2509.02046/x3.png",
                "caption": "",
                "position": 181
            },
            {
                "img": "https://arxiv.org/html/2509.02046/x4.png",
                "caption": "",
                "position": 186
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Empirical Findings",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02046/x5.png",
                "caption": "Figure 2:Main Results For Phase I & II.Top: We plot the validation loss on C4/EN for the experiments in Phase I and Phase II. Every point corresponds to the optimal loss achieved at the corresponding Chinchilla ratio for each optimizer. Bottom: we plot the HellaSwag performance corresponding to the selected run for a subset of optimizers: the AdamW baseline, the top 2 most performant scalar-based optimizers, and the top 3 most performant matrix-based optimizers. Analysis is deferred toSection˜4.1.",
                "position": 710
            },
            {
                "img": "https://arxiv.org/html/2509.02046/x6.png",
                "caption": "",
                "position": 719
            },
            {
                "img": "https://arxiv.org/html/2509.02046/x7.png",
                "caption": "",
                "position": 724
            },
            {
                "img": "https://arxiv.org/html/2509.02046/x8.png",
                "caption": "",
                "position": 730
            },
            {
                "img": "https://arxiv.org/html/2509.02046/x9.png",
                "caption": "",
                "position": 735
            },
            {
                "img": "https://arxiv.org/html/2509.02046/x10.png",
                "caption": "",
                "position": 740
            },
            {
                "img": "https://arxiv.org/html/2509.02046/x11.png",
                "caption": "Figure 3:Speedup of different optimizers across scale.We estimate the speedup of different optimizers by fitting a scaling law for AdamW and then map the loss of different optimizers to the corresponding equivalent data budget. We observe that (i) The highest speedup is capped at 1.4×\\times; (ii) matrix-based optimizers consistently outperform scalar-based optimizers and show an increasing speedup with data budget.",
                "position": 746
            },
            {
                "img": "https://arxiv.org/html/2509.02046/x12.png",
                "caption": "",
                "position": 755
            },
            {
                "img": "https://arxiv.org/html/2509.02046/x13.png",
                "caption": "",
                "position": 760
            },
            {
                "img": "https://arxiv.org/html/2509.02046/x14.png",
                "caption": "Figure 4:Case Studies.Left: Validation loss scaling on 1.2B model for AdamW, NAdamW, Muon and Soap. Muon and Soap still offer significant speedup over AdamW but no longer significantly speed up over NAdamW. Mid: Estimated speedup ratio with the same methodologyFigure˜3, we observe that Muon and Soap’s speedup decays with model size to only1.1×1.1\\times. Last: Experiment with 300M16×16\\timesChinchilla setting, Soap outperforms Muon when data-to-model ratio further increases.",
                "position": 805
            },
            {
                "img": "https://arxiv.org/html/2509.02046/x15.png",
                "caption": "",
                "position": 814
            },
            {
                "img": "https://arxiv.org/html/2509.02046/x16.png",
                "caption": "",
                "position": 819
            },
            {
                "img": "https://arxiv.org/html/2509.02046/x17.png",
                "caption": "Figure 5:Necessity of Careful Tuning.Left: 2×\\timesthe optimal learning rate diminishes Soap’s loss improvement over Mars on 520M 8x experiment; Mid: Variation of loss when only one hyperparameter differs from optimal learning rate and runs converge to within0.020.02of optimal. The order of optimizers may flip arbitrarily if rigorous tuning is missing. Right: Changing a single hyperparameter like weight decay may lead to misleading faster loss improvement but plateaus later.",
                "position": 922
            },
            {
                "img": "https://arxiv.org/html/2509.02046/x18.png",
                "caption": "",
                "position": 931
            },
            {
                "img": "https://arxiv.org/html/2509.02046/x19.png",
                "caption": "",
                "position": 937
            },
            {
                "img": "https://arxiv.org/html/2509.02046/x20.png",
                "caption": "",
                "position": 942
            },
            {
                "img": "https://arxiv.org/html/2509.02046/x21.png",
                "caption": "Figure 6:Common Phenomena Across Optimizers.Left: Learning rate used for different optimizers.Middle Left: Parameter norm of all optimizers shows a similar trend of increment and decrease, closely aligning the increasing and decaying of learning rate schedule.Middle Right: Gradient norm increases during learning rate decay. However, this increase does not lead to a loss increase.Right: The training loss and evaluation loss follows the same trend for all optimizers.",
                "position": 948
            },
            {
                "img": "https://arxiv.org/html/2509.02046/x22.png",
                "caption": "",
                "position": 957
            },
            {
                "img": "https://arxiv.org/html/2509.02046/x23.png",
                "caption": "",
                "position": 962
            },
            {
                "img": "https://arxiv.org/html/2509.02046/x24.png",
                "caption": "",
                "position": 967
            }
        ]
    },
    {
        "header": "5Conclusion and Limitations",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AOptimizer Definitions",
        "images": []
    },
    {
        "header": "Appendix BOmitted Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02046/x25.png",
                "caption": "Figure 7:Sophia Experiments.Left: Loss curve of Sophia and AdamW in1×1\\timesChinchilla setting. Right: Loss curve of Sophia and AdamW for 130M model size.",
                "position": 3156
            },
            {
                "img": "https://arxiv.org/html/2509.02046/x26.png",
                "caption": "",
                "position": 3165
            },
            {
                "img": "https://arxiv.org/html/2509.02046/x27.png",
                "caption": "Figure 8:More Case Studies.Experiment with 130M16×16\\timesChinchilla setting, SOAP outperforms Muon in the overtraining setting.",
                "position": 3175
            }
        ]
    },
    {
        "header": "Appendix CHyperparameter Ablation in Phase I",
        "images": []
    },
    {
        "header": "Appendix DHyperparameter Ablation in Phase II",
        "images": []
    },
    {
        "header": "Appendix EHyperparameter Ablation in Phase III 1.2B experiments",
        "images": []
    },
    {
        "header": "Appendix FHyperparameter Ablation in Phase III 16x Chinchilla experiments",
        "images": []
    },
    {
        "header": "Appendix GComparison with Prior Work",
        "images": []
    }
]