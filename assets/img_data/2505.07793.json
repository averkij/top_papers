[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.07793/extracted/6428978/figures/github.png",
                "caption": "",
                "position": 98
            },
            {
                "img": "https://arxiv.org/html/2505.07793/x1.png",
                "caption": "Figure 1:Limited recurrent memory capacity limits leading long-context LLMs.(Left) We quantify this behavior by measuring zero-shot associative recall curves: The x-axis represents the number of facts (key-value pairs) in the context, while the y-axis shows the retrieval accuracy of the correct value from the context. To mitigate the memory problem, we propose OPRM, a chunk-based inference strategy that does not force the model to encode more information than it can reliably store. (Right) Leading recurrent LLMs evaluated on LongBench and LongBench_e (’0-4K’, ’4-8K’, ’8K+’). Surprisingly, our simple OPRM approach improves these models in long-context tasks.",
                "position": 110
            },
            {
                "img": "https://arxiv.org/html/2505.07793/x2.png",
                "caption": "",
                "position": 113
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Problem Investigation",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.07793/x3.png",
                "caption": "Figure 2:Memory overflows in a controlled setup. (Left) AR curves of 2-layer models with different hidden state sizes.d𝑑ditalic_dis the channels dimension andds⁢t⁢a⁢t⁢esubscript𝑑𝑠𝑡𝑎𝑡𝑒d_{state}italic_d start_POSTSUBSCRIPT italic_s italic_t italic_a italic_t italic_e end_POSTSUBSCRIPTis the state size. The x-axis represents the number of facts in the context, and the y-axis shows the retrieval accuracy of correct values from the context. (Right) Memory capacity as a function of channel and state dimensions. Each point shows the ratio between the capacity (maximal amount of retrieved facts from a single context) and the amount of facts that the model was trained to retrieve (128 facts). All 2-layer models were trained to retrieve up to 128 facts, yet most models suffer from overflows, therefore have a lower capacity than the one trained for.",
                "position": 203
            },
            {
                "img": "https://arxiv.org/html/2505.07793/x4.png",
                "caption": "",
                "position": 206
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.07793/x5.png",
                "caption": "Figure 3:Visualization of OPRM:Given a prompt structured as Prefix (P𝑃Pitalic_P), Context (C𝐶Citalic_C), and Suffix (S𝑆Sitalic_S), we first split the context into chunks (C1,⋯,Cbsubscript𝐶1⋯subscript𝐶𝑏C_{1},\\cdots,C_{b}italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ⋯ , italic_C start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT). All chunks are wrapped with the Prefix (P𝑃Pitalic_P) and Suffix (S𝑆Sitalic_S) and are processed independently in a speculative manner (in parallel). During decoding, tokens are predicted auto-regressively, conditioned exclusively on the selected chunk.",
                "position": 226
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "7Conclusions",
        "images": []
    },
    {
        "header": "8Reproducibility Statement",
        "images": []
    },
    {
        "header": "9Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.07793/x6.png",
                "caption": "Figure 4:Context Extension - Needle in a Haystack. The x-axis is the context length in tokens, and the y-axis is the depth in which the passkey is hidden inside the context. The color indicates the success rate of the needle retrieval. We show that overflow-prevention mechanisms inherently perform context extension, and even beat dedicated context extension methods.",
                "position": 2048
            }
        ]
    },
    {
        "header": "Appendix CMotivation and Design Principles",
        "images": []
    },
    {
        "header": "Appendix DAdvantages of OPRM",
        "images": []
    },
    {
        "header": "Appendix EAdditional Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.07793/x7.png",
                "caption": "Figure 5:IDK Filter ablation - qualitative exampleWe provide 3 samples from the HotPotQA benchmark (LongBench_e). In each one of the samples we present the question, ground truth answer, and the response of three Recurrent-Gemma-IT-9B models: the baseline, + OPRM, and + OPRM + IDK Filter. Each response is colored according to its correctness (red - incorrect, green - correct). The model with OPRM does have the ability to answer the long-context questions, yet without the IDK filter the wrong chunk is selected, leading the model to an ’i don’t know’ response. When applying the IDK filter, the model provides the exact answer.",
                "position": 2184
            }
        ]
    },
    {
        "header": "Appendix FLongBench Task Name Abbreviations.",
        "images": []
    }
]