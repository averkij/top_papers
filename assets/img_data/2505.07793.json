[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.07793/extracted/6428978/figures/github.png",
                "caption": "",
                "position": 98
            },
            {
                "img": "https://arxiv.org/html/2505.07793/x1.png",
                "caption": "Figure 1:Limited recurrent memory capacity limits leading long-context LLMs.(Left) We quantify this behavior by measuring zero-shot associative recall curves: The x-axis represents the number of facts (key-value pairs) in the context, while the y-axis shows the retrieval accuracy of the correct value from the context. To mitigate the memory problem, we propose OPRM, a chunk-based inference strategy that does not force the model to encode more information than it can reliably store. (Right) Leading recurrent LLMs evaluated on LongBench and LongBench_e (â€™0-4Kâ€™, â€™4-8Kâ€™, â€™8K+â€™). Surprisingly, our simple OPRM approach improves these models in long-context tasks.",
                "position": 110
            },
            {
                "img": "https://arxiv.org/html/2505.07793/x2.png",
                "caption": "",
                "position": 113
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Problem Investigation",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.07793/x3.png",
                "caption": "Figure 2:Memory overflows in a controlled setup. (Left) AR curves of 2-layer models with different hidden state sizes.dğ‘‘ditalic_dis the channels dimension anddsâ¢tâ¢aâ¢tâ¢esubscriptğ‘‘ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’d_{state}italic_d start_POSTSUBSCRIPT italic_s italic_t italic_a italic_t italic_e end_POSTSUBSCRIPTis the state size. The x-axis represents the number of facts in the context, and the y-axis shows the retrieval accuracy of correct values from the context. (Right) Memory capacity as a function of channel and state dimensions. Each point shows the ratio between the capacity (maximal amount of retrieved facts from a single context) and the amount of facts that the model was trained to retrieve (128 facts). All 2-layer models were trained to retrieve up to 128 facts, yet most models suffer from overflows, therefore have a lower capacity than the one trained for.",
                "position": 203
            },
            {
                "img": "https://arxiv.org/html/2505.07793/x4.png",
                "caption": "",
                "position": 206
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.07793/x5.png",
                "caption": "Figure 3:Visualization of OPRM:Given a prompt structured as Prefix (Pğ‘ƒPitalic_P), Context (Cğ¶Citalic_C), and Suffix (Sğ‘†Sitalic_S), we first split the context into chunks (C1,â‹¯,Cbsubscriptğ¶1â‹¯subscriptğ¶ğ‘C_{1},\\cdots,C_{b}italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â‹¯ , italic_C start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT). All chunks are wrapped with the Prefix (Pğ‘ƒPitalic_P) and Suffix (Sğ‘†Sitalic_S) and are processed independently in a speculative manner (in parallel). During decoding, tokens are predicted auto-regressively, conditioned exclusively on the selected chunk.",
                "position": 226
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "7Conclusions",
        "images": []
    },
    {
        "header": "8Reproducibility Statement",
        "images": []
    },
    {
        "header": "9Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.07793/x6.png",
                "caption": "Figure 4:Context Extension - Needle in a Haystack. The x-axis is the context length in tokens, and the y-axis is the depth in which the passkey is hidden inside the context. The color indicates the success rate of the needle retrieval. We show that overflow-prevention mechanisms inherently perform context extension, and even beat dedicated context extension methods.",
                "position": 2048
            }
        ]
    },
    {
        "header": "Appendix CMotivation and Design Principles",
        "images": []
    },
    {
        "header": "Appendix DAdvantages of OPRM",
        "images": []
    },
    {
        "header": "Appendix EAdditional Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.07793/x7.png",
                "caption": "Figure 5:IDK Filter ablation - qualitative exampleWe provide 3 samples from the HotPotQA benchmark (LongBench_e). In each one of the samples we present the question, ground truth answer, and the response of three Recurrent-Gemma-IT-9B models: the baseline, + OPRM, and + OPRM + IDK Filter. Each response is colored according to its correctness (red - incorrect, green - correct). The model with OPRM does have the ability to answer the long-context questions, yet without the IDK filter the wrong chunk is selected, leading the model to an â€™i donâ€™t knowâ€™ response. When applying the IDK filter, the model provides the exact answer.",
                "position": 2184
            }
        ]
    },
    {
        "header": "Appendix FLongBench Task Name Abbreviations.",
        "images": []
    }
]