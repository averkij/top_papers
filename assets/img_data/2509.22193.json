[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22193/x1.png",
                "caption": "Figure 1:Task sensitivity to reasoning. Reasoning helps most on open-ended and math tasks; gains are limited or inconsistent on general multiple-choice tasks. X-axis: extra-token factor when switching from IFT to reasoning. Y-axis: accuracy gain (%).",
                "position": 172
            }
        ]
    },
    {
        "header": "2Experimental Setup",
        "images": []
    },
    {
        "header": "3Model Performance Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22193/x2.png",
                "caption": "Figure 2:Downstream performance of mono-phasic models. Results are shown for the teacher model and base students, as well as for models trained with IFT- and reasoning-style data on both general and math-centric domains.",
                "position": 326
            },
            {
                "img": "https://arxiv.org/html/2509.22193/x3.png",
                "caption": "Figure 3:Comparison of sequential and mixed training scenarios across varying reasoning ratios. The accuracy gap relative to the IFT baseline (0% ratio) is shown with solid lines, while the average answer length (in tokens) is reported with dashes. Results are averaged over all student sizes.",
                "position": 348
            },
            {
                "img": "https://arxiv.org/html/2509.22193/x4.png",
                "caption": "Figure 4:Impact of the reasoning ratio on downstream performance. Results show the accuracy gap relative to the IFT baseline (0% reasoning ratio) in the sequential training scenario, where models are first trained on IFT- and then on reasoning-style data.",
                "position": 351
            },
            {
                "img": "https://arxiv.org/html/2509.22193/x5.png",
                "caption": "Figure 5:Downstream performance of models trained sequentially on general and math-centric data. Results show the accuracy gap relative to mono-phasic general-domain IFT models (General-IFT inFigure 2). Mono-phasic reasoning models are included as baselines.",
                "position": 377
            }
        ]
    },
    {
        "header": "4Accuracy-Efficiency trade-off Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22193/x6.png",
                "caption": "Figure 6:Accuracy versus training FLOPs for models trained with IFT (0%), reasoning-style data (100%), and sequential reasoning ratios of 25%, 50%, and 75%. The Pareto frontier (black dashed lines) highlights efficient configurations, while those that lie in the red-shaded area are suboptimal.",
                "position": 409
            },
            {
                "img": "https://arxiv.org/html/2509.22193/x7.png",
                "caption": "Figure 7:Accuracy versus inference FLOPs for models trained with IFT (0% reasoning ratio) and reasoning-style (100% reasoning ratio) data. The purple-dotted and blue-dashed lines indicate the accuracy-FLOPs interpolated curves for IFT and reasoning, respectively (further details inAppendix E). The red-shaded region highlights configurations that are Pareto-suboptimal.",
                "position": 437
            },
            {
                "img": "https://arxiv.org/html/2509.22193/x8.png",
                "caption": "Figure 8:Answer length analysis across student sizes and correctness in reasoning models. Vertical bars indicate average answer lengths for each task category, while the black line shows the corresponding downstream accuracies.",
                "position": 457
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADiscussion",
        "images": []
    },
    {
        "header": "Appendix BTraining Hyperparameters",
        "images": []
    },
    {
        "header": "Appendix CFLOPs Computation",
        "images": []
    },
    {
        "header": "Appendix DPrompting Details",
        "images": []
    },
    {
        "header": "Appendix EDetails on Pareto Interpolation",
        "images": []
    },
    {
        "header": "Appendix FAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22193/x9.png",
                "caption": "Figure 9:Inference-cost impact of generation early stopping for IFT and reasoning models. Each model is evaluated at five maximum-length thresholds, corresponding to the 0th, 25th, 50th, 75th, and 100th answer length percentiles. The Pareto frontier is indicated by black dashed lines.",
                "position": 2108
            },
            {
                "img": "https://arxiv.org/html/2509.22193/x10.png",
                "caption": "Figure 10:Impact of increasing maximum generation length (from 16,384 to 32,768 tokens) on downstream performance acrossmmlu-math,math-500, andaime.",
                "position": 2118
            },
            {
                "img": "https://arxiv.org/html/2509.22193/x11.png",
                "caption": "Figure 11:Inference FLOPs versus student model size for IFT and reasoning-style training. Points indicate the average inference FLOPs for each task category, while the curves show the corresponding log-linear scaling trends.",
                "position": 2128
            },
            {
                "img": "https://arxiv.org/html/2509.22193/x12.png",
                "caption": "Figure 12:Task-level downstream performance of mono-phasic models.",
                "position": 2146
            },
            {
                "img": "https://arxiv.org/html/2509.22193/x13.png",
                "caption": "Figure 13:Task-level comparison of sequential and mixed training scenarios across varying reasoning ratios.",
                "position": 2149
            },
            {
                "img": "https://arxiv.org/html/2509.22193/x14.png",
                "caption": "Figure 14:Task-level impact of the reasoning ratio on downstream performance.",
                "position": 2152
            },
            {
                "img": "https://arxiv.org/html/2509.22193/x15.png",
                "caption": "Figure 15:Task-level downstream performance of math-adapted models.",
                "position": 2155
            },
            {
                "img": "https://arxiv.org/html/2509.22193/x16.png",
                "caption": "Figure 16:Task-level accuracy versus training FLOPs for models trained with IFT (0%), reasoning-style data (100%), and sequential reasoning ratios of 25%, 50%, and 75%.",
                "position": 2158
            },
            {
                "img": "https://arxiv.org/html/2509.22193/x17.png",
                "caption": "Figure 17:Task-level accuracy versus inference FLOPs for models trained with IFT and reasoning-style data.",
                "position": 2161
            },
            {
                "img": "https://arxiv.org/html/2509.22193/x18.png",
                "caption": "Figure 18:Task-level answer length analysis across student sizes and correctness in reasoning models.",
                "position": 2164
            }
        ]
    },
    {
        "header": "Appendix GTask-Level Results",
        "images": []
    }
]