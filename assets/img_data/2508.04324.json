[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.04324/x1.png",
                "caption": "Figure 1:(Left) Reward Variance Analysis: We plot the standard deviation of PickScore at each denoising step for 200 prompts, per prompt group size is 24. The results, obtained via applying SDE at only one step, reveal that reward variance is highest in the initial steps, indicating that early-stage interventions are most impactful for exploration. (Right) Method Illustration: By branching a stochastic (SDE) exploration from a specific, known state on a deterministic (ODE) trajectory, we create a controlled experiment. The resulting difference in the final reward can be unambiguously attributed to the exploration action taken at that precise branching point.",
                "position": 81
            },
            {
                "img": "https://arxiv.org/html/2508.04324/x1.png",
                "caption": "",
                "position": 84
            },
            {
                "img": "https://arxiv.org/html/2508.04324/x2.png",
                "caption": "",
                "position": 88
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.04324/x3.png",
                "caption": "Figure 2:(Left) Performance comparison on the PickScore benchmark. To ensure a fair and robust evaluation, we first present Flow-GRPO (Fixed),an improved baseline where we replace the original global std stabilization with a more appropriate group-wise std. On top of this stronger baseline, our core innovation, trajectory branching (+ Traj. Branch), delivers a significant performance leap. Our full model, TempFlow-GRPO, integrates all components to achieve the highest performance. (Right) TempFlow-GRPO’s dominance is further confirmed on the Geneval benchmark, where it substantially surpasses not only the Flow-GRPO baseline but also leading state-of-the-art models, including GPT-4o, SD3.5-M, and FLUX.",
                "position": 129
            },
            {
                "img": "https://arxiv.org/html/2508.04324/x3.png",
                "caption": "",
                "position": 132
            },
            {
                "img": "https://arxiv.org/html/2508.04324/x4.png",
                "caption": "",
                "position": 136
            }
        ]
    },
    {
        "header": "3Preliminary: Flow-GRPO",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.04324/x5.png",
                "caption": "Figure 3:Overview of TempFlow-GRPO Framework.Our method performs trajectory branching by switching from ODE to SDE sampling at selected timesteps (t=k, j, i), injecting noiseσt​Δ​t​ϵ\\sigma_{t}\\sqrt{\\Delta t}\\epsilonto create exploratory branches. Each branch generates a distinct outcome with rewardRiR_{i}, enabling precise credit assignment. The framework applies noise-aware weighting whereωi>ωj>ωk\\omega_{i}>\\omega_{j}>\\omega_{k}, prioritizing optimization at high-noise early stages (larger circles) over low-noise refinement phases (smaller circles), aligning learning intensity with each timestep’s intrinsic exploration capacity. We visualize the model’s learning process as an astronaut exploring unknown planets: in early stages , the model explores vast possibility spaces with high uncertainty, while later stages involve focused navigation toward the final destination.",
                "position": 232
            }
        ]
    },
    {
        "header": "4Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.04324/x6.png",
                "caption": "Figure 4:(Left)A comparative analysis between the reward standard deviation (Reward Std) and the noise level over the generative steps. The two curves show a strong correlation.(Right)Scale terms reveal a mismatch in standard GRPO: Scale terms are inversely proportional to noise level, causing low-noise refinement steps to dominate optimization despite having minimal impact on image content. Early steps that establish global structure receive weak gradients, while late steps that only adjust local details produce strong gradients. Our noise-aware reweighting compensates for this inverse relationship, ensuring that optimization intensity aligns with each timestep’s actual capacity to influence the final image.",
                "position": 283
            },
            {
                "img": "https://arxiv.org/html/2508.04324/x6.png",
                "caption": "",
                "position": 286
            },
            {
                "img": "https://arxiv.org/html/2508.04324/x7.png",
                "caption": "",
                "position": 290
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.04324/x8.png",
                "caption": "Figure 5:Ablation Studies on trajectory branch and noise-aware policy reweighting.Left: PickScore Benchmark.Right: Geneval Benchmark.",
                "position": 791
            },
            {
                "img": "https://arxiv.org/html/2508.04324/x8.png",
                "caption": "",
                "position": 794
            },
            {
                "img": "https://arxiv.org/html/2508.04324/x9.png",
                "caption": "",
                "position": 798
            },
            {
                "img": "https://arxiv.org/html/2508.04324/x10.png",
                "caption": "Figure 6:Comparsion on PickScore benchmark in 1024 resolution.",
                "position": 807
            },
            {
                "img": "https://arxiv.org/html/2508.04324/x11.png",
                "caption": "Figure 7:Qualitative comparison between SD3.5-M, Flow-GRPO and TempFlow-GRPO with PickScore as reward. We use thered boxto indicate the error region.",
                "position": 816
            }
        ]
    },
    {
        "header": "6Limitation and Future Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]