[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.12957/x1.png",
                "caption": "Figure 1:The architecture of MuVi. The main model and the input/output are illustrated in the middle, where the visual encoder is frozen during the training stage. The visual compression strategies are listed on the left, where “CLS” indicates the CLS token of certain visual encoders, such as CLIP. The architecture of the diffusion Transformer is illustrated on the right.",
                "position": 240
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.12957/x2.png",
                "caption": "Figure 2:Visualization of the attention distribution of Softmax aggregation. The yellower the patch, the more it is related to the generated music. We mask the video frames with the averaged attention scores. We transform the patches corresponding to the weights after applying Softmax into masks, and then adjust the colors of the masks accordingly. When the weights are smaller (close to 0.0), the mask appears bluer; conversely (close to 1.0), it appears yellower. This reflects the attention distribution of the adaptor.",
                "position": 417
            },
            {
                "img": "https://arxiv.org/html/2410.12957/x3.png",
                "caption": "Figure 3:Illustration of In-context Learning.",
                "position": 718
            },
            {
                "img": "https://arxiv.org/html/2410.12957/extracted/5932535/img/cfg.png",
                "caption": "Figure 4:Results of different CFG scales.",
                "position": 909
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Ethics Statement",
        "images": []
    },
    {
        "header": "7Reproducibility Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AArchitecture and Implementation details",
        "images": []
    },
    {
        "header": "Appendix BDetails of Training and Inference",
        "images": []
    },
    {
        "header": "Appendix CDetails of Pre-trained Encoders",
        "images": []
    },
    {
        "header": "Appendix DDetails of Data",
        "images": []
    },
    {
        "header": "Appendix EDetails of Evaluation",
        "images": []
    },
    {
        "header": "Appendix FExtensional Experiments",
        "images": []
    }
]