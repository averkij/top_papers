[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.23552/x1.png",
                "caption": "Figure 1:Overview of our JAM-Flow framework for flexible and joint generation of facial motion and speech. The model accepts diverse input combinations, including text, reference motion, and reference audio. These are processed by our novel Motion & Audio Joint MM-DiT, which enables synchronized synthesis of full audio-visual outputs supporting tasks like talking head generation from text, audio-driven animation, and cross-modal reconstruction (e.g., audio from motion).",
                "position": 102
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.23552/x2.png",
                "caption": "Figure 2:LivePortrait framework and mouth-related expression keypoint analysis. LivePortrait‚Äôs motion encoder (Emsubscriptùê∏ùëöE_{m}italic_E start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT) infers parameters including a 3D expression deformationùêû‚àà‚Ñù21√ó3ùêûsuperscript‚Ñù213\\mathbf{e}\\in\\mathbb{R}^{21\\times 3}bold_e ‚àà blackboard_R start_POSTSUPERSCRIPT 21 √ó 3 end_POSTSUPERSCRIPTfor 21 canonical keypoints. We find that deforming approximately four specific keypoints (highlighted) primarily dictates mouth articulation. Our Motion-DiT leverages this by generating only the deformation components (ùêûmouth‚äÇùêûsuperscriptùêûmouthùêû\\mathbf{e}^{\\text{mouth}}\\subset\\mathbf{e}bold_e start_POSTSUPERSCRIPT mouth end_POSTSUPERSCRIPT ‚äÇ bold_e) for these crucial mouth keypoints, enabling efficient lip-sync.",
                "position": 194
            }
        ]
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.23552/x3.png",
                "caption": "Figure 3:The training and inference pipeline of the JAM-Flow framework. Our joint MM-DiT comprises a Motion-DiT for facial expression keypoints (ùêûmouthsuperscriptùêûmouth\\mathbf{e}^{\\text{mouth}}bold_e start_POSTSUPERSCRIPT mouth end_POSTSUPERSCRIPT) and an Audio-DiT for mel-spectrograms (ùêöùêö\\mathbf{a}bold_a), coupled via joint attention. The model is trained with an inpainting-style flow matching objective on masked inputs and various conditions (text, reference audio/motion). At inference, it flexibly generates synchronized audio-visual outputs from partial inputs.",
                "position": 257
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Discussion and Ethics",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AQualitative Comparisons, User Study, and Discussions",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.23552/extracted/6581900/Materials/userstudy-th.png",
                "caption": "Figure A1:Average ranking results for audio-conditioned talking head generation on HDTF dataset. Participants ranked six methods from best (1) to worst (6) based on overall quality, including lip-sync accuracy, motion naturalness, and visual fidelity. Lower rank indicates better performance.",
                "position": 1562
            },
            {
                "img": "https://arxiv.org/html/2506.23552/extracted/6581900/Materials/userstudy-th.png",
                "caption": "Figure A1:Average ranking results for audio-conditioned talking head generation on HDTF dataset. Participants ranked six methods from best (1) to worst (6) based on overall quality, including lip-sync accuracy, motion naturalness, and visual fidelity. Lower rank indicates better performance.",
                "position": 1565
            },
            {
                "img": "https://arxiv.org/html/2506.23552/extracted/6581900/Materials/userstudy-dubbing.png",
                "caption": "Figure A2:User preference results for automated video dubbing on CelebV-Dub dataset. Participants selected the best synchronized audio-visual output among four competing methods for each sample. Values indicate the percentage of times each method was chosen as best.",
                "position": 1570
            },
            {
                "img": "https://arxiv.org/html/2506.23552/x4.png",
                "caption": "Figure A3:Survey Examples. The left shows an example from the Audio-Conditioned Talking Head Generation (HDTF) survey, and the right shows an example from the Automated Video Dubbing (CelebV-Dub) survey.",
                "position": 1582
            }
        ]
    },
    {
        "header": "Appendix BDetails on used Datasets and Models",
        "images": []
    },
    {
        "header": "Appendix CDetails on Joint Attention Implementation",
        "images": []
    }
]