[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05025/x1.png",
                "caption": "Figure 1:An illustrative comparison between an original malicious question and our imperceptible jailbreak prompt. Although both appear visually identical when rendered on screen, the jailbreak version includes invisible suffixes composed of variation selectors. These invisible characters can be encoded by LLM tokenizers as additional tokens, necessary to bypass safety alignment.",
                "position": 83
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Imperceptible Jailbreaks",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05025/x2.png",
                "caption": "Figure 2:Variation selectors are originally designed to alter the appearance of special characters, but they do not influence the appearance of standard characters, like normal alphabetic characters.\nHence, we can use these invisible variation selectors to achieve imperceptible jailbreaks.",
                "position": 134
            },
            {
                "img": "https://arxiv.org/html/2510.05025/x3.png",
                "caption": "Figure 3:A jailbreak prompt comparison for simple adaptive attacks,ℐ\\mathcal{I}-GCG, and our imperceptible jailbreaks. Simple adaptive attacks involve a carefully crafted template anda visible non-semantic suffixadded tomalicious questions.ℐ\\mathcal{I}-GCG appendsa visible non-semantic suffixtomalicious questions. In contrast, our imperceptible jailbreaks combinemalicious questionswith suffixes consisting ofinvisible variation selectors, resulting in a jailbreak prompt that appears identical to the malicious question when rendered on screen.",
                "position": 289
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05025/x4.png",
                "caption": "Figure 4:Distribution of target-start tokens for successful jailbreaks against four aligned LLMs, including Vicuna-13B-v1.5, Llama-2-Chat-7B, Llama-3.1-Instruct-8B, and Mistral-7B-Instruct-v0.2. The results highlight that different models exhibit different preferences on target-start tokens of successful jailbreak response formats.",
                "position": 494
            },
            {
                "img": "https://arxiv.org/html/2510.05025/x5.png",
                "caption": "Figure 5:Distribution of round numbers for successful jailbreaks against four aligned LLMs, including Vicuna-13B-v1.5, Llama-2-Chat-7B, Llama-3.1-Instruct-8B, and Mistral-7B-Instruct-v0.2. The results show the effectiveness of our chain of search in multiple rounds.",
                "position": 521
            },
            {
                "img": "https://arxiv.org/html/2510.05025/x5.png",
                "caption": "Figure 5:Distribution of round numbers for successful jailbreaks against four aligned LLMs, including Vicuna-13B-v1.5, Llama-2-Chat-7B, Llama-3.1-Instruct-8B, and Mistral-7B-Instruct-v0.2. The results show the effectiveness of our chain of search in multiple rounds.",
                "position": 524
            },
            {
                "img": "https://arxiv.org/html/2510.05025/x6.png",
                "caption": "Figure 6:Distribution of restart numbers for successful jailbreaks against four aligned LLMs, including Vicuna-13B-v1.5, Llama-2-Chat-7B, Llama-3.1-Instruct-8B, and Mistral-7B-Instruct-v0.2. Llama-3.1-Instruct-8B requires more restarts to achieve successful jailbreaks.",
                "position": 531
            },
            {
                "img": "https://arxiv.org/html/2510.05025/x7.png",
                "caption": "Figure 7:Ablation on suffix lengthLLand number of modified charactersMMacross four aligned LLMs, including Vicuna-13B-v1.5, Llama-2-Chat-7B, Llama-3.1-Instruct-8B, and Mistral-7B-Instruct-v0.2. The results suggest that a moderate suffix length with a moderate modified step offers an optimal performance for the effectiveness and optimization stability.",
                "position": 537
            },
            {
                "img": "https://arxiv.org/html/2510.05025/x8.png",
                "caption": "Figure 8:Attention score distribution under our imperceptible jailbreaks. Blue tokens promote refusal and red tokens promote acceptance. For the original malicious question, the model’s attention focuses on harmful phrases. In contrast, under imperceptible jailbreak prompts, attention shifts away from harmful content, which can bypass the safety alignment.",
                "position": 549
            },
            {
                "img": "https://arxiv.org/html/2510.05025/x9.png",
                "caption": "Figure 9:Embedding divergence under our imperceptible jailbreaks against four aligned LLMs, including Vicuna-13B-v1.5, Llama-2-Chat-7B, Llama-3.1-Instruct-8B, and Mistral-7B-Instruct-v0.2. The clear separation between the two clusters reveals that invisible suffixes alter the model’s embedding features, which supports the effectiveness of our attack.",
                "position": 552
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAlgorithm Outline",
        "images": []
    },
    {
        "header": "Appendix BSystem Prompts",
        "images": []
    },
    {
        "header": "Appendix CGeneration Examples",
        "images": []
    },
    {
        "header": "Appendix DPotential Limitations and Future Directions",
        "images": []
    }
]