[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22801/x1.png",
                "caption": "Figure 1:Optimization objective and gradient of GRPO and CFPO as functions of the policy ratior=π/πoldr=\\pi/\\pi_{\\text{old}}, shown for advantageA=1A=1and trust-region widthϵ=0.2\\epsilon=0.2.\nGRPO becomes flat oncerrexits the trust region, resulting in zero gradient beyond the clipping boundary.\nCFPO instead applies a convex quadratic penalty inrr, yielding a continuous restoring gradient that pullsrrback toward the trust region.\nThis difference highlights why CFPO maintains stable learning signals while GRPO can stall when updates pushrroutside the trust region.",
                "position": 196
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22801/x2.png",
                "caption": "Figure 2:Training dynamics of CFPO vs. GRPO under increasing off-policy pressure.\nReward (top) and clip ratio (bottom) trajectories for Qwen2.5 models trained with different numbers of iterations per update (columns). GRPO (dashed) exhibits faster early reward gains but increasingly large and unstable updates as iterations grow, reflected in rising clip ratios and eventual training collapse at higher iteration counts (≥\\geq8 for most models). In contrast, CFPO (solid) progresses more conservatively, maintaining consistently low clip ratios and stable training across extended horizons, while ultimately reaching comparable reward levels. These dynamics illustrate the trade-off between optimization aggressiveness and stability in off-policy post-training, and highlight CFPO’s robustness to repeated sample reuse.",
                "position": 368
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22801/x3.png",
                "caption": "Figure 3:RLHF training dynamics on Llama3-8B under RLOO and CFPO with different KL penalty coefficients. We report trajectories over training steps for (a) reward, (b) generated response length, (c) policy clipping ratio, and (d) KL divergence between consecutive policy updates. RLOO exhibits rapid early reward increases accompanied by growing response lengths and elevated clipping activity, particularly when the KL penalty is weak or removed, indicating more aggressive optimization. In contrast, CFPO yields steadier reward improvement while maintaining stable response lengths, lower clipping ratios, and controlled KL divergence across settings, reflecting more conservative and stable policy updates during RLHF.",
                "position": 581
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Discussion and Future Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATheoretical Results of Simple Policy Optimization",
        "images": []
    },
    {
        "header": "Appendix BTraining Details",
        "images": []
    },
    {
        "header": "Appendix CQwen2.5 Results in TRL",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22801/x4.png",
                "caption": "Figure 4:Training reward dynamics for cold-start RLVR training of Qwen2.5-3B usingverlacross batch ratios and iteration counts. GRPO exhibits faster early reward improvement, while CFPO progresses more gradually and converges later. Increasing iteration count leads to instability around 8 iterations, whereas increasing batch ratio alone remains comparatively stable, highlighting the stronger destabilizing effect of iteration-based sample reuse.",
                "position": 2343
            },
            {
                "img": "https://arxiv.org/html/2601.22801/x5.png",
                "caption": "Figure 5:Validation reward during cold-start RLVR training of Qwen2.5-3B under GRPO and CFPO across batch ratios and iteration counts. Trends largely mirror training reward, with no systematic gains from increased off-policy updates. Instability emerges primarily with higher iteration counts rather than larger batch ratios, indicating limited generalization benefits from aggressive off-policy training.",
                "position": 2346
            },
            {
                "img": "https://arxiv.org/html/2601.22801/x6.png",
                "caption": "Figure 6:Policy gradient clipping ratios during cold-start RLVR training of Qwen2.5-3B usingverl. For GRPO, clipping activity increases with both batch ratio and iteration count, and sharp rises in clipping precede training instability. CFPO consistently maintains lower and more stable clipping behavior across all settings, reflecting its smoother update geometry.",
                "position": 2349
            },
            {
                "img": "https://arxiv.org/html/2601.22801/x7.png",
                "caption": "Figure 7:KL divergence between consecutive policies during cold-start RLVR training of Qwen2.5-3B across batch ratios and iteration counts. Despite differing optimization behavior, GRPO and CFPO exhibit similar KL magnitudes throughout training. This indicates that observed stability differences are not explained by large inter-policy shifts, but rather by differences in how updates are regularized within the trust region.",
                "position": 2352
            },
            {
                "img": "https://arxiv.org/html/2601.22801/x8.png",
                "caption": "Figure 8:Policy entropy during cold-start RLVR training of Qwen2.5-3B under GRPO and CFPO across batch ratios and iteration counts. GRPO exhibits a faster reduction in entropy, particularly at higher iteration counts, consistent with its more aggressive optimization behavior. CFPO maintains higher entropy values over training, reflecting less aggressive policy updates and more gradual concentration of the policy distribution.",
                "position": 2355
            }
        ]
    },
    {
        "header": "Appendix DverlFigures",
        "images": []
    }
]