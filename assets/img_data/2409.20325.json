[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "Prologue",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.20325/x1.png",
                "caption": "Figure 1:Steepest descent considers the problem of minimizing a linear functional under a quadratic penalty:arg‚Å¢minŒî‚Å¢ùíò‚àà‚Ñùn‚Å°[ùíà‚ä§‚Å¢Œî‚Å¢ùíò+Œª2‚Å¢‚ÄñŒî‚Å¢ùíò‚Äñ2]subscriptargminŒîùíòsuperscript‚ÑùùëõsuperscriptùíàtopŒîùíòùúÜ2superscriptnormŒîùíò2\\operatorname*{arg\\,min}_{\\Delta{\\bm{w}}\\in\\mathbb{R}^{n}}\\left[{\\bm{g}}^{\\top%\n}\\Delta{\\bm{w}}+\\frac{\\lambda}{2}\\,\\|{\\Delta{\\bm{w}}}\\|^{2}\\right]start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT roman_Œî bold_italic_w ‚àà blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_POSTSUBSCRIPT [ bold_italic_g start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT roman_Œî bold_italic_w + divide start_ARG italic_Œª end_ARG start_ARG 2 end_ARG ‚à• roman_Œî bold_italic_w ‚à• start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ]forùíà‚àà‚Ñùnùíàsuperscript‚Ñùùëõ{\\bm{g}}\\in\\mathbb{R}^{n}bold_italic_g ‚àà blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT. Here we show how the solution varies with the sharpnessŒª>0ùúÜ0\\lambda>0italic_Œª > 0and the choice of norm‚à•‚ãÖ‚à•\\|{\\cdot}\\|‚à• ‚ãÖ ‚à•. We overlay different norm balls on top of a linear color gradient, and use arrows to denote the solution, meaning the member of the norm ball that ‚Äúminimizes the color‚Äù. a) Increasing the sharpness decreases the size of the solution vector. b) Changing the norm can change the direction of the solution vector. For different‚Ñìpsubscript‚Ñìùëù\\ell_{p}roman_‚Ñì start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPTnorms, the solution direction changes because the gradient is not axis-aligned. In practice, we should pick the sharpness and norm to fit the geometry of our loss.",
                "position": 87
            }
        ]
    },
    {
        "header": "Story IAdam as Steepest Descent under the Max-of-Max Norm",
        "images": []
    },
    {
        "header": "Story IIShampoo as Steepest Descent under the Spectral Norm",
        "images": []
    },
    {
        "header": "Story IIIProdigy: Automatically Computing the Escape Velocity",
        "images": []
    },
    {
        "header": "Epilogue",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AComputational Strategies for Shampoo",
        "images": []
    },
    {
        "header": "Appendix BProofs",
        "images": []
    }
]