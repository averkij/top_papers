[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15301/x1.png",
                "caption": "Figure 1:Core contribution of SVG.(a-d) Comparisons of the overall methodology between VAE-based latent diffusion models and SVG. (e-f) Comparisons of training and inference efficiency.",
                "position": 75
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15301/x2.png",
                "caption": "Figure 2:Selected 256×\\times256 samples from SVG-XL. We use a cfg of 4.0 and 25 Euler steps.",
                "position": 121
            }
        ]
    },
    {
        "header": "2Related works",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15301/x3.png",
                "caption": "Figure 3:Architecture of the proposed SVG Autoencoder.The model augments the DINO encoder with a Residual Encoder to achieve high-quality reconstruction and preserve transferability.",
                "position": 140
            },
            {
                "img": "https://arxiv.org/html/2510.15301/x4.png",
                "caption": "(a)The t-SNE visualization of different visual feature spaces.",
                "position": 219
            },
            {
                "img": "https://arxiv.org/html/2510.15301/x4.png",
                "caption": "(a)The t-SNE visualization of different visual feature spaces.",
                "position": 222
            },
            {
                "img": "https://arxiv.org/html/2510.15301/x5.png",
                "caption": "(b)Toy example illustrating the impact of semantic dispersion in the feature space on diffusion model training.",
                "position": 227
            },
            {
                "img": "https://arxiv.org/html/2510.15301/x6.png",
                "caption": "Figure 5:Visualization of SVG reconstruction.Incorporating the Residual Encoder enables SVG to better preserve visual information, such as color and high-frequency details.",
                "position": 249
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15301/x7.png",
                "caption": "Table 1:System-level performance on ImageNet256×256256\\times 256for SVG.Operating in aunified feature space, SVG achieves high-qualityfew-step generation(25 steps), surpassing baseline models and converging faster.†indicates reproduction results; for flow-matching, only the ODE solver is used.",
                "position": 307
            },
            {
                "img": "https://arxiv.org/html/2510.15301/x7.png",
                "caption": "Figure 6:Zero-shot class-conditioned editing using SVG.The first column shows the original image. The first two rows edit the region inside the red box, whereas the third row edits the outside.",
                "position": 887
            },
            {
                "img": "https://arxiv.org/html/2510.15301/x8.png",
                "caption": "Figure 7:Visualization of interpolation using SVG.The first row shows direct linear interpolation, while the second row presents spherical linear interpolation.",
                "position": 890
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore implementation details",
        "images": []
    },
    {
        "header": "Appendix BFinetuning Details on Downstream Tasks",
        "images": []
    },
    {
        "header": "Appendix CEditing details",
        "images": []
    },
    {
        "header": "Appendix DLatent space interpolation test",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15301/x9.png",
                "caption": "Figure 8:Visualization of linear interpolation.Two noise vectors are randomly sampled and linearly interpolated under the same class embedding.",
                "position": 2029
            },
            {
                "img": "https://arxiv.org/html/2510.15301/x10.png",
                "caption": "Figure 9:Visualization of spherical linear interpolation.Two noise vectors are randomly sampled and spherical linearly interpolated under the same class embedding.",
                "position": 2032
            }
        ]
    },
    {
        "header": "Appendix EFurther analysis of SVG generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15301/x11.png",
                "caption": "Figure 10:PCA visualizations of feature maps.SVG shows cleaner feature maps, while the VAE-Diffusion model tends to show noisy feature maps, particularly for largett.",
                "position": 2042
            },
            {
                "img": "https://arxiv.org/html/2510.15301/x12.png",
                "caption": "Figure 11:PCA visualizations of feature maps.SVG shows cleaner feature maps, while the VAE-Diffusion model tends to show noisy feature maps, particularly for largett.",
                "position": 2045
            }
        ]
    },
    {
        "header": "Appendix FMore Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15301/x13.png",
                "caption": "Figure 12:Random samples from SVG-XL on ImageNet 256×\\times256.We use a classifier-free guidance scale of 4.0",
                "position": 2057
            },
            {
                "img": "https://arxiv.org/html/2510.15301/x14.png",
                "caption": "Figure 13:Uncurated generation results of SVG-XL. We use classifier-free guidance with w = 4.0. Class label = 33.",
                "position": 2060
            },
            {
                "img": "https://arxiv.org/html/2510.15301/x15.png",
                "caption": "Figure 14:Uncurated generation results of  SVG-XL. We use classifier-free guidance with w = 4.0. Class label = 88.",
                "position": 2063
            },
            {
                "img": "https://arxiv.org/html/2510.15301/x16.png",
                "caption": "Figure 15:Uncurated generation results of  SVG-XL. We use classifier-free guidance with w = 4.0. Class label = 89.",
                "position": 2066
            },
            {
                "img": "https://arxiv.org/html/2510.15301/x17.png",
                "caption": "Figure 16:Uncurated generation results of  SVG-XL. We use classifier-free guidance with w = 4.0. Class label = 207.",
                "position": 2069
            },
            {
                "img": "https://arxiv.org/html/2510.15301/x18.png",
                "caption": "Figure 17:Uncurated generation results of  SVG-XL. We use classifier-free guidance with w = 4.0. Class label = 250.",
                "position": 2072
            },
            {
                "img": "https://arxiv.org/html/2510.15301/x19.png",
                "caption": "Figure 18:Uncurated generation results of  SVG-XL. We use classifier-free guidance with w = 4.0. Class label = 270.",
                "position": 2075
            },
            {
                "img": "https://arxiv.org/html/2510.15301/x20.png",
                "caption": "Figure 19:Uncurated generation results of  SVG-XL. We use classifier-free guidance with w = 4.0. Class label = 279.",
                "position": 2078
            },
            {
                "img": "https://arxiv.org/html/2510.15301/x21.png",
                "caption": "Figure 20:Uncurated generation results of  SVG-XL. We use classifier-free guidance with w = 4.0. Class label = 387.",
                "position": 2081
            },
            {
                "img": "https://arxiv.org/html/2510.15301/x22.png",
                "caption": "Figure 21:Uncurated generation results of  SVG-XL. We use classifier-free guidance with w = 4.0. Class label = 928.",
                "position": 2084
            },
            {
                "img": "https://arxiv.org/html/2510.15301/x23.png",
                "caption": "Figure 22:Uncurated generation results of  SVG-XL. We use classifier-free guidance with w = 4.0. Class label = 933.",
                "position": 2087
            },
            {
                "img": "https://arxiv.org/html/2510.15301/x24.png",
                "caption": "Figure 23:Uncurated generation results of  SVG-XL. We use classifier-free guidance with w = 4.0. Class label = 972.",
                "position": 2090
            },
            {
                "img": "https://arxiv.org/html/2510.15301/x25.png",
                "caption": "Figure 24:Uncurated generation results of  SVG-XL. We use classifier-free guidance with w = 4.0. Class label = 973.",
                "position": 2093
            },
            {
                "img": "https://arxiv.org/html/2510.15301/x26.png",
                "caption": "Figure 25:Uncurated generation results of  SVG-XL. We use classifier-free guidance with w = 4.0. Class label = 975.",
                "position": 2096
            },
            {
                "img": "https://arxiv.org/html/2510.15301/x27.png",
                "caption": "Figure 26:Uncurated generation results of  SVG-XL. We use classifier-free guidance with w = 4.0. Class label = 980.",
                "position": 2099
            }
        ]
    },
    {
        "header": "Appendix GDescription of pretrained visual encoders",
        "images": []
    },
    {
        "header": "Appendix HStatement on LLM Assistance",
        "images": []
    }
]