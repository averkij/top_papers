[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22232/x1.png",
                "caption": "Figure 1:The multilingual data filtering approach JQL: In the first stage (Sec.2), human annotators generate ground truth (GT) annotations on monolingual documents based on an instruction set defined in a prompt. The documents are translated into all target languages to receive a multilingual GT dataset. In the second stage (Sec.3), based on the GT dataset, we select the top-nùëõnitalic_nperforming LLMs-as-a-judge for annotating a multilingual dataset. In the third stage (Sec.4), we use the resulting synthetic dataset to train a set of lightweight annotators. This is done at low cost by reusing shared embeddings.\nUsing these annotators, we can efficiently annotate pre-training corpora and filter high-quality subsets (Sec.5).",
                "position": 232
            }
        ]
    },
    {
        "header": "2Collecting Human Annotations",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22232/x2.png",
                "caption": "Figure 2:LLMs show varying ranking performance for educational quality. Some models exhibit strong multilingual capabilities.\nWe show Spearman Correlation between model predictions and the respective human GT annotations. Scores are displayed for the 13 language subset, their average correlation (avg-13) and the average correlation across all 35 considered languages. The numbers highlighted in bold represent the largest value for each column.",
                "position": 408
            }
        ]
    },
    {
        "header": "3Harnessing LLMs for Multilingual Data Annotation",
        "images": []
    },
    {
        "header": "4Distilling Lightweight Annotators",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22232/x3.png",
                "caption": "Figure 3:Lightweight JQL annotators show strong multilingual and cross-lingual performance. Training on the same language as the evaluation target serves as a baseline (row 1). We show cross-lingual capabilities by comparing against training on languages within the same language family from Tab.1(row 2), those within the same, lower-level family (row 3), the full set of the remaining 34 languages (row 4), and those outside the first-order family (row 5). We also show performance for joint training on all languages with the respective LLM data (last 3 rows). Empty cells occur when no related language is present in our dataset. We depict Spearman correlation with ground truth annotation.",
                "position": 485
            }
        ]
    },
    {
        "header": "5Assessing Training Data Quality",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22232/x4.png",
                "caption": "Figure 4:Lightweight annotators trained on different synthetic labels produce different educational score distributions. On average, Gemma assigns higher values than Mistral or Llama. Consequently, thresholding needs to be dynamic and account for the annotators‚Äô distribution. Example plotted for CC release 2024-14 over 13 languages.",
                "position": 567
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x5.png",
                "caption": "Figure 5:Our JQL annotators improve pre-training data quality over heuristic baselines (FW2).\nThe exemplary plot depicts results for the Spanish dataset.",
                "position": 584
            }
        ]
    },
    {
        "header": "6Generalization to Unseen Languages",
        "images": []
    },
    {
        "header": "7Related work",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22232/x6.png",
                "caption": "Figure 6:Our JQL lightweight annotators generalize to unseen, topologically different languages. The figure shows aggregated performance on Arabic, Thai and Chinese. With limited available of standard benchmarks, we relied on language-specific benchmarks selected by Fineweb2Penedo et¬†al. (2024b).",
                "position": 719
            }
        ]
    },
    {
        "header": "8Conclusion & Future Directions",
        "images": []
    },
    {
        "header": "9Limitations",
        "images": []
    },
    {
        "header": "10Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AHuman Annotation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22232/x7.png",
                "caption": "Figure 7:Histogram on the distribution of the document scores judged by the human annotators.",
                "position": 1554
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x8.png",
                "caption": "Figure 8:Cumulative distribution of spread within annotations. Aligned with the majority agreement of 78.5% and an interrating standard deviation of 0.56, (see Sec.2), also the spread analysis reveals high interrater consistency with a spread of‚â§\\leq‚â§2 for 86% of the documents.",
                "position": 1563
            }
        ]
    },
    {
        "header": "Appendix BLLM Based Annotator Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22232/x9.png",
                "caption": "Figure 9:Invalid scores predictions (in percent)",
                "position": 1592
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x10.png",
                "caption": "Figure 10:Percentages of invalid scores (aggregated) for each model across all languages. An aggregated score (majority voted) is counted as invalid, if all three predictions for a document are invalid.",
                "position": 1595
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x11.png",
                "caption": "Figure 11:Multilingual LLM classification performance (macro F1-score) on human-annotated ground truth. Scores are reported individually for the 13 languages subset, as well as averaged across these 13 languages (avg-13) and across all 35 evaluated languages.",
                "position": 1711
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x12.png",
                "caption": "Figure 12:Ranking performance in terms of Spearman correlation for each model across all languages.",
                "position": 1714
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x13.png",
                "caption": "Figure 13:Classification performance in terms of macro F1 score for each model across all languages.",
                "position": 1717
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x14.png",
                "caption": "(a)Gemma-3-27B-it",
                "position": 1720
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x14.png",
                "caption": "(a)Gemma-3-27B-it",
                "position": 1723
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x15.png",
                "caption": "(b)Llama-3.3-70B-it",
                "position": 1728
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x16.png",
                "caption": "(c)Mistral-3.1-24B-it",
                "position": 1733
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x17.png",
                "caption": "(a)Gemma-3-27B-it",
                "position": 1753
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x17.png",
                "caption": "(a)Gemma-3-27B-it",
                "position": 1756
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x18.png",
                "caption": "(b)LLama-3.3-70B-it",
                "position": 1761
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x19.png",
                "caption": "(c)Mistral-3.1-24B-it",
                "position": 1766
            }
        ]
    },
    {
        "header": "Appendix CLightweight Annotators",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22232/x20.png",
                "caption": "Figure 16:Validation performance (Spearman correlation) as a function of the number of processed training samples, comparing two training strategies. The end-to-end model (blue) jointly trains both the embedding backbone and the regression head, while the regression-head model (orange) fine-tunes only the regression layer on top of a frozen embedder. Performance is evaluated on a held-out validation set, and both models are trained with early stopping. Epoch boundaries are marked with dashed lines. While both models show rapid initial gains, especially during the first 100k samples, the full end-to-end model converges to a significantly lower final correlation, suggesting limited benefit from updating the embedding backbone under the given supervision signal.",
                "position": 2154
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x21.png",
                "caption": "Figure 17:Ten training runs (one per row), utilizing between 10k and 10M training samples (text documents). The number of samples and corresponding training epochs are shown on the y-axis. Training is capped at 20 epochs, with early stopping based on Spearman correlation monitored on a held-out validation set. Each resulting model is evaluated in terms of Spearman correlation across all 35 test languages.",
                "position": 2170
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x22.png",
                "caption": "Figure 18:Full cross-lingual transfer; One plot per Annotation model (balanced); training/evaluation setup is otherwise identical to the best performing setup. Rows represent the only training language of a regression head, while columns indicate the testing language. Each cell reports the Spearman correlation between predicted and human-annotated scores.",
                "position": 2186
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x22.png",
                "caption": "",
                "position": 2189
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x23.png",
                "caption": "",
                "position": 2194
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x24.png",
                "caption": "",
                "position": 2199
            }
        ]
    },
    {
        "header": "Appendix DAssessing Training Data Quality",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22232/x25.png",
                "caption": "Figure 19:Distribution of different lightweight annotation heads on CC release 2024-14 over 13 languages. Training heads on balanced labels produces slightly smoother distributions.",
                "position": 2659
            },
            {
                "img": "https://arxiv.org/html/2505.22232/extracted/6488701/figs/ablations/edu_annotation_by_language.png",
                "caption": "Figure 20:Distribution of edu score annotations by language. Dotted lines represent the respective mean.",
                "position": 2662
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x26.png",
                "caption": "Figure 21:Dataset training performance for Bulgarian.",
                "position": 2665
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x26.png",
                "caption": "Figure 21:Dataset training performance for Bulgarian.",
                "position": 2668
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x27.png",
                "caption": "Figure 22:Dataset training performance for German.",
                "position": 2673
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x28.png",
                "caption": "Figure 23:Dataset training performance for Greek.",
                "position": 2679
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x28.png",
                "caption": "Figure 23:Dataset training performance for Greek.",
                "position": 2682
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x29.png",
                "caption": "Figure 24:Dataset training performance for Spanish.",
                "position": 2687
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x30.png",
                "caption": "Figure 25:Dataset training performance for Finnish.",
                "position": 2693
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x30.png",
                "caption": "Figure 25:Dataset training performance for Finnish.",
                "position": 2696
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x31.png",
                "caption": "Figure 26:Dataset training performance for French.",
                "position": 2701
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x32.png",
                "caption": "Figure 27:Dataset training performance for Hungarian.",
                "position": 2707
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x32.png",
                "caption": "Figure 27:Dataset training performance for Hungarian.",
                "position": 2710
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x33.png",
                "caption": "Figure 28:Dataset training performance for Italian.",
                "position": 2715
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x34.png",
                "caption": "Figure 29:Dataset training performance for Lithuanian.",
                "position": 2721
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x34.png",
                "caption": "Figure 29:Dataset training performance for Lithuanian.",
                "position": 2724
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x35.png",
                "caption": "Figure 30:Dataset training performance for Norwegian (Bokm√•l).",
                "position": 2729
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x36.png",
                "caption": "Figure 31:Dataset training performance for Polish.",
                "position": 2735
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x36.png",
                "caption": "Figure 31:Dataset training performance for Polish.",
                "position": 2738
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x37.png",
                "caption": "Figure 32:Dataset training performance for Turkish.",
                "position": 2743
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x38.png",
                "caption": "Figure 33:Dataset training performance for Ukrainian.",
                "position": 2750
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x38.png",
                "caption": "Figure 33:Dataset training performance for Ukrainian.",
                "position": 2753
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x39.png",
                "caption": "Figure 34:Dataset training performance for Arabic.",
                "position": 2758
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x40.png",
                "caption": "Figure 35:Dataset training performance for Thai.",
                "position": 2764
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x40.png",
                "caption": "Figure 35:Dataset training performance for Thai.",
                "position": 2767
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x41.png",
                "caption": "Figure 36:Dataset training performance for Chinese.",
                "position": 2772
            }
        ]
    },
    {
        "header": "Appendix EGeneralization to Unseen languages",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22232/x42.png",
                "caption": "Figure 37:Strong cross-lingual performance of our lightweight JQL annotators on unseen languages (Arabic, Thai, and Chinese). Compared to the average performance of the European languages on which the annotators are trained, we observe an even better correlation with human GT for some languages.",
                "position": 2786
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x43.png",
                "caption": "Figure 38:Direct comparison of Gemma and Llama as annotators.",
                "position": 2848
            }
        ]
    },
    {
        "header": "Appendix FAdditional Ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22232/x44.png",
                "caption": "(a)Spearman correlation on test set with512tokens context length.",
                "position": 2864
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x44.png",
                "caption": "(a)Spearman correlation on test set with512tokens context length.",
                "position": 2867
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x45.png",
                "caption": "(b)Spearman correlation improves when using full8192tokens context length.",
                "position": 2873
            },
            {
                "img": "https://arxiv.org/html/2505.22232/x46.png",
                "caption": "(c)Token Counts across all Test Languages. We observe a meaningful percentage of documents longer then 512 tokens.",
                "position": 2879
            }
        ]
    },
    {
        "header": "Appendix GDatasets",
        "images": []
    },
    {
        "header": "Appendix HLicense of Used Artifacts",
        "images": []
    },
    {
        "header": "Appendix IData Containing Personally Identifiable Information or Offensive Content",
        "images": []
    },
    {
        "header": "Appendix JInfrastructure & Compute Requirements",
        "images": []
    },
    {
        "header": "Appendix KUsage of AI Tools",
        "images": []
    }
]