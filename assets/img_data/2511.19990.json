[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19990/x1.png",
                "caption": "",
                "position": 77
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19990/x2.png",
                "caption": "Figure 2:Compared with the state-of-the-art multi-image editing methods, our approach achieves not only faithful reconstruction of the original image in reference–repair tasks, but also excellent performance in various reconstruction scenarios including text, patterns, facial details, and object details. In contrast, existing methods often fail to remain faithful to the original image during repair or are unable to recover text and fine details.",
                "position": 150
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19990/x3.png",
                "caption": "Figure 3:Overall architecture of OmniRefiner. Our framework adopts a two-stage training pipeline. In the first stage, we perform supervised fine-tuning (SFT) to enable dual-input detail restoration while preserving global structure. In the second stage, we apply GRPO-based reinforcement learning to further enhance fine-grained consistency and local repair quality. This joint design enables precise reference-guided refinement with high visual fidelity.",
                "position": 169
            },
            {
                "img": "https://arxiv.org/html/2511.19990/x4.png",
                "caption": "Figure 4:We adopt a four-stage data pipeline. First, a VLM pairs images of the same product with consistent styles and reasonable viewpoints. Second, it generates fine-grained editing instructions for one image in each pair. Third, an image editing model executes these edits using the pre-edit image as ground truth, forming our (input, reference, ground truth) triplet dataset.Finally, the VLM generates an instruction guiding the model to restore the input using the reference, based on the input, reference, and ground truth.",
                "position": 305
            }
        ]
    },
    {
        "header": "4Experiment.",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19990/assets/dreamsim_score.png",
                "caption": "Figure 5:The DreamSim reward curve and the masked MSE reward curve demonstrate the process of how our model aligns with the reward functions during GRPO.",
                "position": 388
            },
            {
                "img": "https://arxiv.org/html/2511.19990/assets/dreamsim_score.png",
                "caption": "",
                "position": 391
            },
            {
                "img": "https://arxiv.org/html/2511.19990/assets/masked_mse_score.png",
                "caption": "",
                "position": 396
            },
            {
                "img": "https://arxiv.org/html/2511.19990/x5.png",
                "caption": "Figure 6:Qualitative results demonstrate that our method can accurately restore fine details in images.",
                "position": 690
            },
            {
                "img": "https://arxiv.org/html/2511.19990/assets/user_study_analysis.png",
                "caption": "Figure 7:The user study demonstrates that our method achieves the highest human preference alignment in both detail consistency and restoration naturalness.",
                "position": 704
            },
            {
                "img": "https://arxiv.org/html/2511.19990/x6.png",
                "caption": "Figure 8:Ablation studies demonstrate that each component of our method (GRPO, position embedding, and masked MSE reward) is essential. Even when applying GRPO fine-tuning to Qwen-Image-Edit-Plus, a model that natively supports multi-image inputs, using exactly the same data, the results still fall short of ours.",
                "position": 710
            }
        ]
    },
    {
        "header": "5Limitation and Future Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AUser Study Detail",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19990/x7.png",
                "caption": "Figure 9:User study example: participants were first informed that detail consistency must be achieved while maintaining natural integration with the background, and they were also asked to directly select the result that appeared most natural and unobtrusive.",
                "position": 1629
            }
        ]
    },
    {
        "header": "Appendix BEvaluation Detail",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19990/x8.png",
                "caption": "Figure 10:Additional restoration results show that our method is highly effective in both detail-guided refinement and detail-guided replacement.",
                "position": 1878
            },
            {
                "img": "https://arxiv.org/html/2511.19990/x9.png",
                "caption": "Figure 11:Object-centric method comparison results reveal that our approach exhibits advantages in the coherence of text, logos, and textures, as well as the quality and consistency of the overall repair results.",
                "position": 1885
            },
            {
                "img": "https://arxiv.org/html/2511.19990/x10.png",
                "caption": "Figure 12:Facial comparison results further demonstrate that our method achieves superior understanding of human faces, yielding outstanding restoration quality for fine details such as eyeshadow, eyebrows, scars, and iris color.",
                "position": 1888
            },
            {
                "img": "https://arxiv.org/html/2511.19990/x11.png",
                "caption": "Figure 13:These further ablation results collectively demonstrate the crucial importance of our design in enhancing the model’s understanding, repair, reconstruction, and replacement of details.",
                "position": 1895
            }
        ]
    },
    {
        "header": "Appendix CVisual Results",
        "images": []
    }
]