[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.13714/x1.png",
                "caption": "",
                "position": 110
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.13714/x2.png",
                "caption": "Figure 2:UnSAMv2achieves state-of-the-art performance across interactive segmentation benchmarks.Across multiple datasets,UnSAMv2consistently outperforms SAM-2 and prior methods, turning segmentation into a controllable and interpretable process rather than a fixed prediction.",
                "position": 144
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.13714/x3.png",
                "caption": "Figure 3:From ambiguity to control.Without granularity input, SAM-2 yields up to three masks per point, requiring users to manually choose one.UnSAMv2resolves this ambiguity by introducing a continuous granularity variable, allowing users to obtain the intended object at any scale with a single prompt.\nThis simple addition turns segmentation from a discrete guess into a continuous, controllable reasoning process.",
                "position": 212
            },
            {
                "img": "https://arxiv.org/html/2511.13714/x4.png",
                "caption": "Figure 4:Granularity distribution of discovered masks.Our divide-and-conquer pipeline produces a rich, left-tailed hierarchy of pseudo-masks, dominated by fine-grained structures. Despite this imbalance,UnSAMv2learns stable semantics across all scales.\nHierarchical perception can emerge from unlabeled data!",
                "position": 310
            },
            {
                "img": "https://arxiv.org/html/2511.13714/x5.png",
                "caption": "Figure 5:Granularity as a relative notion.At a fixed granularity value, mask sizes vary widely across scenes, showing thatUnSAMv2learns granularity relationally, consistent with human perception of parts and wholes rather than simply associating it with absolute size.",
                "position": 315
            },
            {
                "img": "https://arxiv.org/html/2511.13714/x6.png",
                "caption": "Figure 6:Architecture ofUnSAMv2.Built on SAM-2,UnSAMv2introduces a Fourier-based granularity encoder and a granularity-aware mask token to enable segmentation at arbitrary granularity. A scalar granularity inputg∈[0.1,1]g\\!\\in\\![0.1,1]is mapped to a high-dimensional embedding via Fourier transformation and an MLP, then injected into the transformer alongside the sparse point prompt embedding and dense image embedding. The granularity-aware mask token attends to image, point, and granularity embeddings, and is finally decoded by a token decoder into a mask at the requested granularity.",
                "position": 326
            },
            {
                "img": "https://arxiv.org/html/2511.13714/x7.png",
                "caption": "Figure 7:Qualitative comparison with the previous state-of-the-art method[zhao2024graco].\nEach scene shows results at different target granularity values.\nPrior methods often break one object into parts at high granularity or include extra regions at low granularity. In contrast,UnSAMv2produces clear and consistent masks with smooth transitions across scales.",
                "position": 386
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.13714/x8.png",
                "caption": "Figure 8:Visualizations for whole-image segmentation.Low granularity reveals fine parts, while higher values recover whole objects.UnSAMv2offers controllable, scalable whole-image segmentation capability, even for scenes with many densely packed entities.",
                "position": 911
            },
            {
                "img": "https://arxiv.org/html/2511.13714/x9.png",
                "caption": "Figure 9:Granularity generalizes to video.We promptUnSAMv2with a point and granularity value on the first frame, then propagate the mask to later frames. Even though trained only on images,UnSAMv2maintains consistent masks over time, showing strong temporal coherence and transferability.",
                "position": 920
            }
        ]
    },
    {
        "header": "5Ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.13714/x10.png",
                "caption": "Figure 10:Ablation of granularity-aware mask token.Directly finetuning the original SAM-2 mask tokens leads to limited gains, suggesting they already encode strong mask priors. Adding our granularity-aware token enables efficient learning of granularity.",
                "position": 995
            },
            {
                "img": "https://arxiv.org/html/2511.13714/x11.png",
                "caption": "Figure 11:Fewer prompts, more control.SAM-2 often needs multiple clicks to isolate the target object. With a single granularity value,UnSAMv2finds the correct mask efficiently, and it can also work with multi-point prompts for finer control.",
                "position": 1176
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "A1Evaluation Datasets",
        "images": []
    },
    {
        "header": "A2Training Details.",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.13714/x12.png",
                "caption": "Figure A1:Interactive segmentation results on SA-1B. The top row is previous SOTA GraCo[zhao2024graco]and the bottom row isUnSAMv2.",
                "position": 1332
            },
            {
                "img": "https://arxiv.org/html/2511.13714/x13.png",
                "caption": "Figure A2:Whole image segmentation on SA-1B. From top to bottom are raw images, segmentation by SAM-2 andUnSAMv2.",
                "position": 1335
            },
            {
                "img": "https://arxiv.org/html/2511.13714/x14.png",
                "caption": "Figure A3:UnSAMv2’s interactive video segmentation results on YoutubeVIS dataset at various granularity levels.",
                "position": 1338
            }
        ]
    },
    {
        "header": "A3More Visualizations",
        "images": []
    }
]