[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04072/x1.png",
                "caption": "Figure 1:Pipeline of SFPO at stepss. Starting from the current policyπθs,0\\pi_{\\theta^{s,0}}, we first generate rollouts for training.Stage I (Fast Trajectory):applyKKsuccessive gradient updates on the same batch to obtainθs,K\\theta^{s,K}.Stage II (Reposition):interpolate betweenθs,K\\theta^{s,K}and the starting pointθs,0\\theta^{s,0}to formθ~s,K\\widetilde{\\theta}^{s,K}, controlling off-policy drift.Stage III (Slow Correction):perform one additional update onθ~s,K\\widetilde{\\theta}^{s,K}, yieldingπθs+1,0\\pi_{\\theta^{s+1,0}}for the next step.",
                "position": 144
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04072/x2.png",
                "caption": "Figure 2:Average validation accuracy of different base models throughout the learning process.",
                "position": 763
            },
            {
                "img": "https://arxiv.org/html/2510.04072/x3.png",
                "caption": "Figure 3:Training dynamics for DeepSeek-R1-Distilled-Qwen-7B, comparing GRPO and SFPO across response length, entropy loss, and reward.",
                "position": 766
            },
            {
                "img": "https://arxiv.org/html/2510.04072/x4.png",
                "caption": "Figure 4:Comparison of GRPO and SFPO. (a) Number of rollouts required to achieve the best accuracy of GRPO. (b) Corresponding training time.",
                "position": 776
            }
        ]
    },
    {
        "header": "5Analysis and Ablation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04072/x5.png",
                "caption": "Figure 5:Average training accuracy of different base models throughout the learning process.",
                "position": 791
            },
            {
                "img": "https://arxiv.org/html/2510.04072/x6.png",
                "caption": "Figure 6:Comparison between SFPO w/ and w/o entropy control (EC). The blue dashed line indicates the stop step identified by z-score.",
                "position": 794
            },
            {
                "img": "https://arxiv.org/html/2510.04072/x6.png",
                "caption": "Figure 6:Comparison between SFPO w/ and w/o entropy control (EC). The blue dashed line indicates the stop step identified by z-score.",
                "position": 797
            },
            {
                "img": "https://arxiv.org/html/2510.04072/x7.png",
                "caption": "Figure 7:Comparison between SFPO with differentα\\alphadecay strategies. The blue dashed line indicates the stop step identified by z-score.",
                "position": 802
            }
        ]
    },
    {
        "header": "6Related Works",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALLM Usage",
        "images": []
    },
    {
        "header": "Appendix BGPU Memory Profiling Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04072/x8.png",
                "caption": "Figure 8:Comparison of GPU memory consumption during one RL training step between SFPO and GRPO for Deepseek-R1-Distill-Qwen-7B model.",
                "position": 1458
            }
        ]
    },
    {
        "header": "Appendix CTraining Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04072/x9.png",
                "caption": "Figure 9:Training dynamics for Qwen2.5-Math-1.5B, comparing GRPO and SFPO across response length, entropy loss, and reward.",
                "position": 1532
            },
            {
                "img": "https://arxiv.org/html/2510.04072/x10.png",
                "caption": "Figure 10:Training dynamics for DeepSeek-R1-Distilled-Qwen-1.5B, comparing GRPO and SFPO across response length, entropy loss, and reward.",
                "position": 1535
            },
            {
                "img": "https://arxiv.org/html/2510.04072/x11.png",
                "caption": "Figure 11:Training dynamics for Qwen2.5-Math-7B, comparing GRPO and SFPO across response length, entropy loss, and reward.",
                "position": 1538
            },
            {
                "img": "https://arxiv.org/html/2510.04072/x12.png",
                "caption": "Figure 12:Training dynamics for Qwen3-4B-Base, comparing GRPO and SFPO across response length, entropy loss, and reward.",
                "position": 1541
            }
        ]
    },
    {
        "header": "Appendix DTraining Dynamics",
        "images": []
    }
]