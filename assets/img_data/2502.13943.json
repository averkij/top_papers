[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13943/x1.png",
                "caption": "Figure 1:Rule-based reasoning step dividing (e.g., using line breaks or a fixed number of tokens) is automated but results in low informativeness at the end of the step and is difficult to apply in domains that hard to define rules. In contrast, manual step division provides high informativeness but is costly to scale and heavily reliant on the experts’ domain knowledge. AdaptiveStep, which divides steps based on model confidence, addresses these challenges by offering automation, efficiency, high informativeness, and applicability across various domains.",
                "position": 151
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13943/x2.png",
                "caption": "Figure 2:Method overview.a)ASPRM  Training Data Construction Pipeline.Step 1: Sample from the dataset of a given domain, collecting confidence scores and samples for the training data. Then, accumulate the confidence distribution of all samples and determine the threshold.\nStep 2: Divide reasoning steps based on the threshold and label the steps using rollout.b)The difference between Rule-based method and AdaptiveStep division.TheRule-based methoddivides the reasoning process using predefined symbols or fixed token counts (e.g., line breaks, as shown in the figure), whileAdaptiveStepdivides reasoning steps based on model confidence.\nWe observe that the model tends to divide reasoning steps at key decision points, such as within mathematical expressions, at noun selections, and when determining the final answer. In contrast, we find that the confidence at line breaks is particularly high.",
                "position": 212
            }
        ]
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13943/x3.png",
                "caption": "Figure 3:We illustrate Token-level Value-guided Decoding (TVD) with a simple example. Thegreen tokendenotes the selected tokens, while thegray tokenindicates the tokens that were not selected. The question is3 * (1 + 1) = ?, and the correct output is6. In this case, the model exhibits low confidence (wherecy<τsubscript𝑐𝑦𝜏c_{y}<\\tauitalic_c start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT < italic_τ) when calculating the result of\n1+1, and subsequently determines which number to multiply by 3. The PRM should select the best token based on its judgment to arrive at the correct final answer. As shown in the top-left corner, for each token, the middle box represents the token itself, the bottom box shows the predicted confidence, and the box on the right displays the PRM score. Thered confidence scoreindicates that the confidence of the Top-1 predicted candidate is lower than the threshold.",
                "position": 236
            },
            {
                "img": "https://arxiv.org/html/2502.13943/x4.png",
                "caption": "Figure 4:BoN results for the math tasks. We evaluate all PRMs on: (a) MetaMath-Llama generated GSM8k candidate solutions; (b) MetaMath-Mistral generated GSM8k candidates; (c) MetaMath-Llama generated MATH500 candidates; and (d) MetaMath-Mistral generated MATH500 candidates. The ”-L” and ”-M” suffixes denote the base models (Llama and Mistral, respectively). We report the evaluation results based on the released versions of other works.",
                "position": 322
            }
        ]
    },
    {
        "header": "4Experiments and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13943/x5.png",
                "caption": "Figure 5:BoN results for the code datasets, we test ASPRM-D   and a Code-ORM (ORM-D) on (a) LCD-DS generated LeetCodeDataset BoN candidates; (b) LCD-DS generated LiveCodeBench BoN candidates.",
                "position": 488
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13943/x6.png",
                "caption": "(a)",
                "position": 1860
            },
            {
                "img": "https://arxiv.org/html/2502.13943/x6.png",
                "caption": "(a)",
                "position": 1863
            },
            {
                "img": "https://arxiv.org/html/2502.13943/x7.png",
                "caption": "(b)",
                "position": 1868
            },
            {
                "img": "https://arxiv.org/html/2502.13943/x8.png",
                "caption": "(c)",
                "position": 1874
            },
            {
                "img": "https://arxiv.org/html/2502.13943/x9.png",
                "caption": "(d)",
                "position": 1879
            },
            {
                "img": "https://arxiv.org/html/2502.13943/x10.png",
                "caption": "(a)",
                "position": 1886
            },
            {
                "img": "https://arxiv.org/html/2502.13943/x10.png",
                "caption": "(a)",
                "position": 1889
            },
            {
                "img": "https://arxiv.org/html/2502.13943/x11.png",
                "caption": "(b)",
                "position": 1894
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]