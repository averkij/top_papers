[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02259/images/model_fig_2.png",
                "caption": "Figure 1:Graph-Free Transformers Model Design.Our model encodes both discretized and continuous molecular sequences using unmodified Transformer layers. Placeholder values represent discrete inputs in the continuous sequence.",
                "position": 152
            }
        ]
    },
    {
        "header": "3Training an Unmodified Transformer without Graph Priors",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02259/x1.png",
                "caption": "Figure 2:Discretization Scheme during Pre-training.We transform the standard “.xyz” molecular representation into a discretized input sequence for our model. To discretize continuous values, we use quantile binning, ensuring each bin contains the same number of datapoints. Atomic positions are jointly discretized into a 3D grid, while force and energy components are discretized independently along each dimension. We add special tokens, like beginning and end of sequence tokens. Note that these discretized tokens are accompanied by the continuous values (for positions, energies, forces, etc.), allowing the model to circumvent discretization errors for real-valued inputs.",
                "position": 183
            }
        ]
    },
    {
        "header": "4What can Graph-Free Transformers Learn?",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02259/images/model_scaling_omol.png",
                "caption": "(a)Pre-Training Model Scaling Laws.",
                "position": 271
            },
            {
                "img": "https://arxiv.org/html/2510.02259/images/model_scaling_omol.png",
                "caption": "(a)Pre-Training Model Scaling Laws.",
                "position": 274
            },
            {
                "img": "https://arxiv.org/html/2510.02259/images/ft_flops.png",
                "caption": "(b)Fine-Tuning Scaling Laws.",
                "position": 279
            },
            {
                "img": "https://arxiv.org/html/2510.02259/images/attn_mass_nine.png",
                "caption": "(a)Attention Distribution by Token Type.",
                "position": 318
            },
            {
                "img": "https://arxiv.org/html/2510.02259/images/attn_mass_nine.png",
                "caption": "(a)Attention Distribution by Token Type.",
                "position": 321
            },
            {
                "img": "https://arxiv.org/html/2510.02259/images/attn_vs_dist_nine_layers.png",
                "caption": "(b)Attention versus interatomic distance.",
                "position": 326
            },
            {
                "img": "https://arxiv.org/html/2510.02259/images/attn_effective_rad_square.png",
                "caption": "Figure 5:Relationship between attention effective radius and atom density.Averaging over atoms in the OMol25 validation set, we plot the effective attention radius versus the median distance to other atoms. Each dot is the mean effective radius within a median neighbor distance percentile. We define the effective radius as the minimum distance within which 90% of an atom’s attention mass is concentrated (seeEquation˜1). The model learns toadaptivelyincrease its effective attention radius when an atom is more isolated, and to decrease it when atoms are tightly packed.",
                "position": 378
            }
        ]
    },
    {
        "header": "5Discussion and Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AFurther Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02259/images/sims_hr_g3f.png",
                "caption": "Figure 6:Transformers can accurately run MD simulations relative to models with strong inductive biases.We select 10 random molecules from the OMol validation set and run NVT MD simulations at 500K for 100ps using a 0.5 fs timestep. We plot the estimated distribution of interatomic distancesh​(r)h(r)for the eSEN-sm models (both direct and conserving versions) and the UMA-s model as reference. Transformers accurately reproduce the distribution of interatomic distances relative to these models without using any physical inductive biases.",
                "position": 1585
            },
            {
                "img": "https://arxiv.org/html/2510.02259/images/nve_sim.png",
                "caption": "Figure 7:Transformers can conserve energy during NVE simulations when fine-tuned to predict conservative forces.",
                "position": 1588
            },
            {
                "img": "https://arxiv.org/html/2510.02259/images/attn_vs_dist_per_layer.png",
                "caption": "Figure 8:Attention is strongly inversely correlated with distance in layers11-99.Attention decays steeply with increasing interatomic distance in layers11-99. This implies that the model is learning to attend predominantly to local interactions in early layers.",
                "position": 1598
            },
            {
                "img": "https://arxiv.org/html/2510.02259/images/attn_radius_vs_med_distance_by_layer.png",
                "caption": "Figure 9:Effective attention radius is adaptive to how tightly packed an atom is within a molecule.We observe a clear positive trend between effective attention radius and median distance to neighbors. Within certain layers (e.g., layer44), radius can go as high as1515Å and as low as22Å depending on the molecule and atom within that molecule.",
                "position": 1601
            },
            {
                "img": "https://arxiv.org/html/2510.02259/images/attn_mass_combined_all_sections.png",
                "caption": "Figure 10:Interatomic attention dynamics dominate in early layers while global attention dominates in later layers.In the first99layers, position tokens attend almost exclusively to the other position tokens. They then shift attention to other input tokens in later layers, suggesting that they’re accessing global graph information. We observe that barely any attention is paid to the spin tokens. We attribute this to the fact that high-spin molecules are rare in the training dataset (seeFigure˜14), suggesting that the model needs more varied spin data to learn how to use spin effectively.",
                "position": 1607
            },
            {
                "img": "https://arxiv.org/html/2510.02259/images/attn_vs_rank_layer_1_head_1.png",
                "caption": "(a)Layer 1 Head 1",
                "position": 1614
            },
            {
                "img": "https://arxiv.org/html/2510.02259/images/attn_vs_rank_layer_1_head_1.png",
                "caption": "(a)Layer 1 Head 1",
                "position": 1617
            },
            {
                "img": "https://arxiv.org/html/2510.02259/images/attn_vs_rank_layer_8_head_0.png",
                "caption": "(b)Layer 8 Head 0",
                "position": 1622
            },
            {
                "img": "https://arxiv.org/html/2510.02259/images/attn_vs_dist_layer_8_head_1.png",
                "caption": "(a)Layer 8 Head 1",
                "position": 1629
            },
            {
                "img": "https://arxiv.org/html/2510.02259/images/attn_vs_dist_layer_8_head_1.png",
                "caption": "(a)Layer 8 Head 1",
                "position": 1632
            },
            {
                "img": "https://arxiv.org/html/2510.02259/images/attn_vs_dist_layer_6_head_6.png",
                "caption": "(b)Layer 6 Head 6",
                "position": 1637
            },
            {
                "img": "https://arxiv.org/html/2510.02259/images/uq.png",
                "caption": "Figure 13:Transformers can be used for uncertainty quantification for other MLIPs.Since Transformers learns a joint distribution over positions, forces, and energies, it can compute log probabilities of new structures encountered at test-time. A higher log probability corresponds to the structure being closer to the training data. Transformers’ predicted log probabilities correlate with force errors for other models like MACE-OFF, GemNet, and EScAIP. The correlation is stronger as Transformers is scaled up from 163M (top) to 1B parameters (bottom). Force errors are in meV/Å.",
                "position": 1657
            }
        ]
    },
    {
        "header": "Appendix BExperiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02259/images/feature_distribution.png",
                "caption": "Figure 14:Distribution of input features in the OMol dataset.The continuous values in molecular datasets span multiple orders of magnitude and are heavy-tailed.",
                "position": 1685
            }
        ]
    },
    {
        "header": "Appendix CComputational Details",
        "images": []
    },
    {
        "header": "Appendix DBroader Impact",
        "images": []
    }
]