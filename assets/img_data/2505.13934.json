[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13934/x1.png",
                "caption": "Figure 1:Training world models with reinforcement learning.(Left) World models are typically pre-trained using surrogate objectives like MLE, which misalign with task-specific prediction metrics. (Right) We propose post-training world models with RLVR to directly optimize these metrics.",
                "position": 224
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4RLVR-World: Training World Models with RLVR",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13934/x2.png",
                "caption": "Figure 2:Illustration of RLVR-World framework.World models across various modalities are unified under a sequence modeling formulation, and task-specific prediction metrics serve as verifiable rewards. (Top) Language-based world models predict verbal state transitions in response to verbal actions. (Bottom) Video-based world models, equipped with a visual tokenizer, predict future visual observations conditioned on action vectors.",
                "position": 306
            }
        ]
    },
    {
        "header": "5Evaluating Language World Models with RLVR",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13934/x3.png",
                "caption": "Table 1:Language world model: text game state prediction.The test set is divided intounchangedandchangedsubsets, depending on whether the ground-truth next state differs from the current state.",
                "position": 395
            },
            {
                "img": "https://arxiv.org/html/2505.13934/x3.png",
                "caption": "(b)",
                "position": 463
            },
            {
                "img": "https://arxiv.org/html/2505.13934/x4.png",
                "caption": "Table 2:Language world model: web page state prediction and model predictive control for web agents.ŒîŒî\\Deltaroman_Œî: relative performance gains from RLVR.",
                "position": 488
            },
            {
                "img": "https://arxiv.org/html/2505.13934/x4.png",
                "caption": "(d)",
                "position": 557
            }
        ]
    },
    {
        "header": "6Evaluating Video World Models with RLVR",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13934/x5.png",
                "caption": "Figure 3:Learning curves of video world models.Note the significant difference in thexùë•xitalic_x-axis scale between the pre-training and post-training stages.",
                "position": 866
            },
            {
                "img": "https://arxiv.org/html/2505.13934/x6.png",
                "caption": "Figure 4:Model analysis.(a) Test-time scaling: best performance among different numbers of generated samples. (b) RL training scaling: learning curves using different group sizes in GRPO. (c) Metric-oriented optimization: RLVR-World trained and tested on different visual metrics.",
                "position": 904
            },
            {
                "img": "https://arxiv.org/html/2505.13934/x7.png",
                "caption": "Figure 5:Real2Sim policy evaluation.",
                "position": 918
            },
            {
                "img": "https://arxiv.org/html/2505.13934/x8.png",
                "caption": "Figure 6:Qualitative analysis: multi-step video prediction and policy evaluation on RT-1.",
                "position": 941
            }
        ]
    },
    {
        "header": "7Discussion and Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details and Extended Experiment Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13934/x9.png",
                "caption": "Figure 7:The learning curves of SFT for text game state prediction.",
                "position": 1878
            },
            {
                "img": "https://arxiv.org/html/2505.13934/x10.png",
                "caption": "Figure 8:Training curves of RLVR-World for single-step prediction: rewards,L1subscriptùêø1L_{1}italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTlosses, and perceptual losses.",
                "position": 2565
            },
            {
                "img": "https://arxiv.org/html/2505.13934/x11.png",
                "caption": "Figure 9:Additional qualitative analysis: multi-step video prediction and policy evaluation on RT-1.",
                "position": 2568
            }
        ]
    },
    {
        "header": "Appendix BComputational Resources",
        "images": []
    },
    {
        "header": "Appendix CBroader Impact",
        "images": []
    },
    {
        "header": "Appendix DPrompt Examples",
        "images": []
    }
]