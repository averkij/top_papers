[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02779/x1.png",
                "caption": "Figure 1:Left: an example from MIRA with responses from both MLLMs and humans, illustrating the visual reasoning and cognitive gaps revealed by our benchmark;Right: while leading MLLMs demonstrate strong performance on established benchmarks, they struggle significantly on the MIRA, with none surpassing a 20% accuracy rate with direct inputs.\nThis highlightsMIRAâ€™s role in exposing the fundamental challenges these models face in complex reasoning tasks that require generating intermediate visual images.",
                "position": 133
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02779/x2.png",
                "caption": "Figure 2:MIRAcategorizes Visual-CoT reasoning tasks into two primary types: Static (Single-Step) and Dynamic (Multi-Step), with representative examples from each category illustrated in the figure. The dataset includes 20 types of tasks, 546 input images with manually designed questions, and 936 manually constructed single-step and multi-step intermediate images. For more cases, please refer to Appendix9.",
                "position": 165
            }
        ]
    },
    {
        "header": "3MIRA: Multimodal Imagination for Reasoning Assessment",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02779/x3.png",
                "caption": "Figure 3:A high-level overview of theMIRAdata design and construction pipeline.",
                "position": 196
            }
        ]
    },
    {
        "header": "4Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02779/x4.png",
                "caption": "Figure 4:A comprehensive performance comparison of leading models across three evaluation settings: Direct Evaluation (D), Text-CoT Reasoning (T), and Simulated Visual-CoT Reasoning (V).\nThis stacked bar chart shows performance scaling: the base indicates pass@1 accuracy, with segments above capturing gains from pass@2, pass@4, and pass@8. The red horizontal marks show majority voting scores over 8 responses.",
                "position": 784
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02779/x5.png",
                "caption": "Figure 5:A representative failure case of Text-CoT on a Euclidean Geometry (EG) reasoning task. Even the strongest model (GPT-5) struggles to correctly reason through the problem using plain text, due to its inability to manipulate intermediate visual states. In contrast, the Visual-CoT approach, which leverages intermediate visualizations, enables more accurate localization of the overlapping region and correct counting of red points.",
                "position": 1213
            }
        ]
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Experimental Model Settings",
        "images": []
    },
    {
        "header": "7Prompt Settings",
        "images": []
    },
    {
        "header": "8Detailed Experimental Tables",
        "images": []
    },
    {
        "header": "9Dataset Showcase",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02779/x6.png",
                "caption": "Figure 6:Illustrative cases for Convex Hull task (left) and Cubes Count task (right).",
                "position": 4412
            },
            {
                "img": "https://arxiv.org/html/2511.02779/x7.png",
                "caption": "Figure 7:Illustrative cases for Mirror Clock task (left) and Cubes Missing task (right).",
                "position": 4415
            },
            {
                "img": "https://arxiv.org/html/2511.02779/x8.png",
                "caption": "Figure 8:Illustrative cases for Mirror Pattern task (left) and Overlap task (right).",
                "position": 4418
            },
            {
                "img": "https://arxiv.org/html/2511.02779/x9.png",
                "caption": "Figure 9:Illustrative cases for Puzzle task (left) and Trailer Cubes Count task (right).",
                "position": 4421
            },
            {
                "img": "https://arxiv.org/html/2511.02779/x10.png",
                "caption": "Figure 10:Illustrative cases for Puzzle task (left) and Trailer Cubes Count task (right).",
                "position": 4424
            },
            {
                "img": "https://arxiv.org/html/2511.02779/x11.png",
                "caption": "Figure 11:Illustrative cases for Unfolded Cube task (left) and Localizer task (right).",
                "position": 4427
            },
            {
                "img": "https://arxiv.org/html/2511.02779/x12.png",
                "caption": "Figure 12:Illustrative cases for Paper Airplane task (left) and Defuse A Bomb task (right).",
                "position": 4430
            },
            {
                "img": "https://arxiv.org/html/2511.02779/x13.png",
                "caption": "Figure 13:Illustrative cases for Multi-piece Puzzle task (left) and Electric Charge task (right).",
                "position": 4433
            },
            {
                "img": "https://arxiv.org/html/2511.02779/x14.png",
                "caption": "Figure 14:Illustrative cases for Rolling Dice: Top task (left) and Gear Rotation task (right).",
                "position": 4436
            },
            {
                "img": "https://arxiv.org/html/2511.02779/x15.png",
                "caption": "Figure 15:Illustrative cases for Rolling Dice: Sum task (left) and Rolling Dice: Two task (right).",
                "position": 4439
            }
        ]
    },
    {
        "header": "10Disclosure of Large Language Model Use",
        "images": []
    }
]