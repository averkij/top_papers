[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.09723/x1.png",
                "caption": "Figure 1:Overview of theEVACframework. Given initial observation images and an action sequence,EVACgenerates multi-view videos conditioned on the provided actions. By incorporating a memory mechanism,EVACsupports the generation of long-term video sequences. The framework handles both static head camera views and dynamic wrist camera views to provide a comprehensive representation of the robotic environment.",
                "position": 98
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.09723/x2.png",
                "caption": "Figure 2:Overview of theEVACFramework. The framework begins with a reference image, whose feature vector serves as the reference style guidance. The original robotic actions are processed to compute the delta action vector and this temporal information is concatenated with the reference style guidance and injected into the diffusion model via a cross-attention mechanism. Additionally, the action information is projected into action maps, whose feature maps are concatenated with feature maps from both memory and visual observations before being fed into the diffusion network. The diffusion model generates video frames with denoising process, followed by a video decoder to produce the final output. For simplicity, we only demonstrate the single-view case here.",
                "position": 140
            },
            {
                "img": "https://arxiv.org/html/2505.09723/x3.png",
                "caption": "Figure 3:Visualizing EEF Projections and the Ray Maps. The bottom row illustrates wrist camera views, where projections appear nearly identical. Then, ray maps provide additional spatial context to represent movements. The value of the ray maps is visualized with the RGB value.",
                "position": 171
            },
            {
                "img": "https://arxiv.org/html/2505.09723/x4.png",
                "caption": "Figure 4:EVAC‚Äôs as Data Engine and Policy Evaluator.",
                "position": 178
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.09723/x5.png",
                "caption": "Figure 5:Qualitative results for multi-view video generation.",
                "position": 218
            },
            {
                "img": "https://arxiv.org/html/2505.09723/x6.png",
                "caption": "Figure 6:Environment consistency during chunk-wise inference. Snapshots fromEVACat various inference stages (Chunks 1, 3, 5, 7, and 9) demonstrate robust performance in maintaining visual fidelity and scene coherence over time.",
                "position": 221
            },
            {
                "img": "https://arxiv.org/html/2505.09723/x7.png",
                "caption": "Figure 7:Comparison of Success Rates Across Tasks and Training Steps.\n(Left) Despite tasks vary a lot, theEVACsimulator consistently aligned its evaluation results with real-world ones.\n(Right) Success rates of a single policy model evaluated at three training steps. BothEVACand real-world testing demonstrated a similar performance gradient.",
                "position": 234
            },
            {
                "img": "https://arxiv.org/html/2505.09723/x8.png",
                "caption": "Figure 8:Impact of Failure Data and Data Augmentation on Trajectory Generation. Without failure data: The model overfits to success-only trajectories, incorrectly ‚Äùhallucinating‚Äù that the bottle has been grasped by the robotic arm.",
                "position": 282
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Limitations and Discussions",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.09723/x9.png",
                "caption": "Figure 9:Results of generated videos under identical conditions with and without the Delta Action Module. The top row shows results with the module (w/ Delta Attention), while the bottom row shows results without it (w/o Delta Attention). The dashed red boxes highlight regions with inconsistent or hallucinated results.",
                "position": 965
            },
            {
                "img": "https://arxiv.org/html/2505.09723/x10.png",
                "caption": "Figure 10:Multi-view generated videos. This task involves placing items from a desk into a bag, specifically packaging a Coke can. Each row displays each synchronized views generated byEVAC, showcasing consistent multi-perspective results at each timestep. We have shown 4 timesteps horizontally, illustrating the dynamic action sequence.",
                "position": 979
            },
            {
                "img": "https://arxiv.org/html/2505.09723/x11.png",
                "caption": "Figure 11:Comparison of results generated by Libero andEVACwith different action trajectories. Two successful cases of picking an item are shown, along with one failure case of an empty catch.",
                "position": 990
            },
            {
                "img": "https://arxiv.org/html/2505.09723/x12.png",
                "caption": "Figure 12:Results with the same initial conditions but different trajectories. The top row shows results for trajectory 1, while the bottom row shows results for trajectory 2.",
                "position": 1001
            },
            {
                "img": "https://arxiv.org/html/2505.09723/x13.png",
                "caption": "Figure 13:Example of the initial conditions for the 4 tasks used in evaluation.",
                "position": 1017
            },
            {
                "img": "https://arxiv.org/html/2505.09723/x14.png",
                "caption": "Figure 14:Task 1: Example of retrieving a bottle of water (failure case). The upper row shows the generated video fromEVAC, and the lower row shows the rollout in real settings. Both are consistent in their results.",
                "position": 1020
            },
            {
                "img": "https://arxiv.org/html/2505.09723/x15.png",
                "caption": "Figure 15:Task 2: Example of retrieving a piece of toast (success case). The upper row shows the generated video fromEVAC, and the lower row shows the rollout in real settings. Both are consistent in their results.",
                "position": 1023
            },
            {
                "img": "https://arxiv.org/html/2505.09723/x16.png",
                "caption": "Figure 16:Task 3: Example of retrieving a ham slice (success case). The upper row shows the generated video fromEVAC, and the lower row shows the rollout in real settings. Both are consistent in their results.",
                "position": 1026
            },
            {
                "img": "https://arxiv.org/html/2505.09723/x17.png",
                "caption": "Figure 17:Task 4: Example of retrieving a lettuce leaf (failure case). The upper row shows the generated video fromEVAC, and the lower row shows the rollout in real settings. Both are consistent in their results.",
                "position": 1030
            },
            {
                "img": "https://arxiv.org/html/2505.09723/x18.png",
                "caption": "Figure 18:Example of the generated data for augmentation in Table1. Left: (a) Spatially augmentedatb‚àíN‚Ä≤subscriptsuperscriptùëé‚Ä≤subscriptùë°ùëèùëÅa^{\\prime}_{t_{b-N}}italic_a start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_b - italic_N end_POSTSUBSCRIPT end_POSTSUBSCRIPTwith four frame examples. The red arrows indicate different directions from the synthetic image toward the target frame. Right: (b) Fixedatbsubscriptùëésubscriptùë°ùëèa_{t_{b}}italic_a start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT end_POSTSUBSCRIPTframe. Frames between (a) and (b) are generated using linear interpolation.",
                "position": 1040
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]