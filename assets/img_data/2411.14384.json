[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14384/x1.png",
                "caption": "",
                "position": 64
            },
            {
                "img": "https://arxiv.org/html/2411.14384/x2.png",
                "caption": "Figure 2:Single-view object-level generation of our method on GSO[13], wild images, and text-to-images prompted by stable diffusion or FLUX. Our DiffusionGS can robustly handle hard cases with furry appearance, shadow, flat illustration, complex geometry, and specularity.",
                "position": 71
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14384/x3.png",
                "caption": "Figure 3:Single-view scene generation results of our method on the indoor (upper) and outdoor (lower) scenes with rotation and occlusion.",
                "position": 94
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14384/x4.png",
                "caption": "Figure 4:Pipeline. (a) When selecting the data for our scene-object mixed training, we impose two angle constraints on the positions and orientations of viewpoint vectors to guarantee the training convergence. (b) The denoiser architecture of DiffusionGS in a single timestep.",
                "position": 150
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14384/x5.png",
                "caption": "Figure 5:Plücker rayvs.Reference-Point Plücker Coordinate.",
                "position": 271
            },
            {
                "img": "https://arxiv.org/html/2411.14384/x6.png",
                "caption": "Figure 6:Visual comparison of object-level generation on ABO, GSO, real-camera image, and text-to-image prompted by FLUX. Our method can generate more fine-grained details with accurate geometry from prompt views of any directions. Zoom in for a better view.",
                "position": 313
            },
            {
                "img": "https://arxiv.org/html/2411.14384/x7.png",
                "caption": "Figure 7:Visual comparison of single-view scene generation. For fairness, we re-train pixelNeRF[80], pixelSplat[6], and GS-LRM[87]with a single input view and the same number of supervised views as our method. These deterministic models all yield blurry images and fail in novel view synthesis. In contrast, our method can robustly generate both indoor and outdoor scenes with occlusion and rotation.",
                "position": 572
            },
            {
                "img": "https://arxiv.org/html/2411.14384/x8.png",
                "caption": "Figure 8:Visual analysis of scene-object mixed training and generation diversity. (a) When using our mixed training strategy, the model can generate more realistic textures for objects and more accurate structural contents for scenes such as the window and stove. (b) Three generated samples of our method with the same prompt view of the “ikun” doll in Fig.1. We show four rendered views for each sample.",
                "position": 614
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]