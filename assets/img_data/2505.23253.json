[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23253/x1.png",
                "caption": "",
                "position": 99
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23253/x2.png",
                "caption": "Figure 2:UV-based texturing models perform well on in-domain, artist-created meshes (first column), but struggle with out-of-distribution, generated meshes (second column). We take Paint3D[40]and TexGEN[39]as representative examples: while effective on large, continuous regions, they fail to handle small, fragmented areas due to training biases toward clean, large-region UV layouts. In contrast, our method operates outside the UV space, enabling better generalization across diverse mesh types. Additional comparisons are provided in Sec.4.2.2.",
                "position": 117
            }
        ]
    },
    {
        "header": "2Related work",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23253/x3.png",
                "caption": "Figure 3:Overall pipeline of UniTEX. Given a textureless geometry and reference image, UniTEX first generates a high-fidelity multi-view image through 3 steps (RGB generation, delighting, and super-resolution (SR)) using finetuned DiTs (detailed in Sec.3.2). The texture will be reprojected to a partial textured mesh and sent to the Large Texturing Model (Detailed in Sec.3.3) with generated images to predict the corresponding complete texture functions (Detailed in Sec.3.3.2). The final texture is then synthesized by blending the predicted texture functions with the partial textured geometry.",
                "position": 158
            }
        ]
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23253/x4.png",
                "caption": "Figure 4:Pipeline of the Large Texturing Model. Given a partially textured geometry and six input views, we first unify them into a shared triplane-cube token representation. A transformer-based architecture then processes these tokens to extract geometry-aware features, which are subsequently decoded into colors using a lightweight MLP.",
                "position": 189
            },
            {
                "img": "https://arxiv.org/html/2505.23253/x5.png",
                "caption": "Figure 5:Visualized example of theTexture Functions(TF) for represent the texture for the whole 3D space. (a) A textured mesh. (b) Unsigned Distance Function (UDF) samples representing 3D geometry. (c) Inspired by UDF, we define texture as a continuous function over 3D space (details in Sec.3.3.2), enabling volumetric texture representation.",
                "position": 239
            },
            {
                "img": "https://arxiv.org/html/2505.23253/x6.png",
                "caption": "Figure 6:Qualitative comparison of our method against state-of-the-art (Paint3D[40], Hunyuan2.0-Paint[47]) and commercial proprietary (Rodin, Meshy) texturing approaches. Our method consistently achieves superior texture quality and generalization across diverse mesh sources. (best viewed by zoom in)",
                "position": 249
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23253/x7.png",
                "caption": "Figure 7:Qualitative comparison of refinement stage across different methods. In the first case, automatically unwrapping makesfragmented and noisyUV layout on the face. UV-based methods such as Paint3D and TexGen struggle with these (blue boxs). In contrast, our method generates smooth and coherent textures that more respect to geometry (glasses in the first row and the ribcage and emblem in the second row).",
                "position": 359
            },
            {
                "img": "https://arxiv.org/html/2505.23253/x8.png",
                "caption": "Figure 8:Qualitative results of stylized texturing. We evaluate our method with various input images and our method can robustly generate high-fidelity stylized textures.(Mesh generated by Hunyuan2.5)",
                "position": 423
            },
            {
                "img": "https://arxiv.org/html/2505.23253/x9.png",
                "caption": "Figure 9:Visualization of the effectiveness of Texture Function Supervision (TFS). Under identical training iterations, models trained with TFS yield significantly higher-quality and completed textures compared to those supervised solely on surface signals. (Best viewed when zoomed in).",
                "position": 589
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]