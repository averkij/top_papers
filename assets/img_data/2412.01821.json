[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01821/extracted/6040041/figs/teaser_wdm.jpg",
                "caption": "",
                "position": 113
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3World-consistent Video Diffusion (WVD)",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01821/extracted/6040041/figs/pipeline_wdm.jpeg",
                "caption": "Figure 2:An illustration ofWVDpipeline. The left part shows 6D videos formed by RGB and XYZ frames. On the right part, WVD iteratively denoises the 6D videos based on a specified RGB frame, which is highlighted with a red box.",
                "position": 173
            }
        ]
    },
    {
        "header": "4WVD as a 3D Foundation Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01821/extracted/6040041/figs/ctrl_pipe.jpeg",
                "caption": "Figure 3:Illustration of camera-controlled multi-view generation pipeline.We first useWVDto infer the geometry from the input image, and then project it to obtain XYZ images for novel views. Next, we employ an inpainting strategy to sample RGB images.",
                "position": 284
            }
        ]
    },
    {
        "header": "5Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01821/extracted/6040041/figs/wdm_generation.jpeg",
                "caption": "Figure 4:Synthesized Multi-view RGB and XYZ Images byWVD, and associated reconstructed point clouds.Input images are randomly sampled across the validation set.",
                "position": 340
            },
            {
                "img": "https://arxiv.org/html/2412.01821/x1.png",
                "caption": "Figure 5:Monocular depth estimation on NYU-v2[45]and BONN[33]benchmarks.We present RGB input images, ground-truth depth maps, and the predicted depth maps from DUSt3R (512 resolution) and WVD, respectively.",
                "position": 358
            },
            {
                "img": "https://arxiv.org/html/2412.01821/extracted/6040041/figs/wdm_camera_ctrl.jpeg",
                "caption": "Figure 6:Camera-controlled video generation.By re-projecting XYZ images and using them as conditions, our method can control the camera movements in the synthesized videos, effectively replicating the trajectories of the real videos.",
                "position": 361
            }
        ]
    },
    {
        "header": "6Discussions and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BCamera-controlled Video Generation.",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01821/x2.png",
                "caption": "Figure A1:Camera-controlled video generation.For each sample, the first row shows the ground-truth video sequence, and the second row shows the synthesized frames which re-produce the camera trajectory. The conditioned frame is marked with a red box.",
                "position": 1572
            },
            {
                "img": "https://arxiv.org/html/2412.01821/extracted/6040041/figs/supp_mv_depth.jpeg",
                "caption": "Figure A2:Multi-view depth estimation on ScanNet++[66].For each sample, the first row presents the input video sequence, the second row shows the ground-truth depth maps. The third row shows the depth maps synthesized by our method.",
                "position": 1575
            },
            {
                "img": "https://arxiv.org/html/2412.01821/extracted/6040041/figs/supp_cam_est_1.jpeg",
                "caption": "Figure A3:Camera estimation.Column (a) shows input unposed images. Column (b) shows estimated XYZ images by our method. Column (c) shows the estimated camera poses from the XYZ images, while column (d) provides the ground-truth camera poses.",
                "position": 1578
            },
            {
                "img": "https://arxiv.org/html/2412.01821/extracted/6040041/figs/ood_1.jpeg",
                "caption": "Figure A4:In-the-wild samples.We evaluate our model on in-the-wild samples to demonstrate its generalizability. The conditioned image is highlighted with a red box.",
                "position": 1581
            }
        ]
    },
    {
        "header": "Appendix CMulti-view Depth Estimation",
        "images": []
    },
    {
        "header": "Appendix DCamera Estimation",
        "images": []
    },
    {
        "header": "Appendix EIn-the-wild Samples",
        "images": []
    }
]