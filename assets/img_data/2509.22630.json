[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22630/x1.png",
                "caption": "Figure 1:Difference between the traditional pipeline and StateX for training long-context models. We introduce a state expansion step (architectural modification) before the long-context post-training (LPT) stage to enhance RNN recall abilities without requiring expensive re-training.",
                "position": 138
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22630/x2.png",
                "caption": "Figure 2:Illustration of StateX (our method) for expanding the state size of linear attention and state space models with little to no parameter increase. The red parts indicate the additional state parameters unlocked by StateX.",
                "position": 248
            }
        ]
    },
    {
        "header": "4Method",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22630/x3.png",
                "caption": "Figure 3:Performance on retrieving specific informationÂ (i.e., a needle) from synthetically generated long documents up to 64K tokens.",
                "position": 762
            },
            {
                "img": "https://arxiv.org/html/2509.22630/x3.png",
                "caption": "Figure 4:Model performance of reinitialization and parameter inheritance.",
                "position": 840
            },
            {
                "img": "https://arxiv.org/html/2509.22630/x4.png",
                "caption": "Figure 5:Model performance under varying numbers of expanded layers. Mamba2 has twice as many layers as GLA because it does not have FFN layers.",
                "position": 882
            },
            {
                "img": "https://arxiv.org/html/2509.22630/x4.png",
                "caption": "Figure 5:Model performance under varying numbers of expanded layers. Mamba2 has twice as many layers as GLA because it does not have FFN layers.",
                "position": 885
            },
            {
                "img": "https://arxiv.org/html/2509.22630/x5.png",
                "caption": "Figure 6:Post-training loss (on SlimPajama) of vanilla models and expanded models. GLA has lower loss as it is pre-trained on SlimPajama while Mamba2 is pre-trained on Pile.",
                "position": 890
            }
        ]
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AFormulation of Gated Linear Attention and Mamba2",
        "images": []
    },
    {
        "header": "Appendix BExperiment Details",
        "images": []
    },
    {
        "header": "Appendix CThe Use of Large Language Models",
        "images": []
    }
]