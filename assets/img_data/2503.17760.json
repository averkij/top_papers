[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.17760/x1.png",
                "caption": "Figure 1:(a)Conventional discrete VQ tokenizerslearn tocompressanddiscretizeinherently continuous visual signals into codes simultaneously. This lead to multiple challenges in training and the corresponding unsatisfactory latent space poses a bottleneck that limit the performance of discrete token-based generation models. (b)Our proposed CODA tokenizerleverages continuous VAEs for compression, directly discretizing the latent space. (c)Quantitative comparisonsbetween VQGAN[11]and our proposed CODA tokenizer.",
                "position": 96
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries of Discrete Vector Quantization",
        "images": []
    },
    {
        "header": "4CODA Tokenizer",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.17760/x2.png",
                "caption": "Figure 2:Illustration of our CODA tokenizer.(a) a residual quantization process ofLùêøLitalic_Llevels iteratively refines the approximation of a continuous VAE vectorfùëìfitalic_fthrough a composite of multiple quantization layers, thus progressively minimizing the quantization error. Meanwhile, as the continuous VAE vector is approximated by acombinationofLùêøLitalic_Ldiscrete codes, the representational capacity is significantly enlarged.\n(b) the attention-based quantization process frames discretization as aretrievaltask. Continuous features and codebook embeddings are projected and normalized onto a normed hypersphere, where the softmax attention matrix is computed to determine the confidence of code selection. As codes compete within the softmax attention framework, this approach ensures asparseandunambiguousassignment.",
                "position": 284
            },
            {
                "img": "https://arxiv.org/html/2503.17760/x3.png",
                "caption": "Figure 3:Visualization of latent space approximation: (a) the original latent space of the continuous VAE, (b) latent space approximated by vector quantization and (c) latent space approximated by residual quantization.",
                "position": 296
            },
            {
                "img": "https://arxiv.org/html/2503.17760/x4.png",
                "caption": "Figure 4:Effect of residual quantization levels on tokenizer performance.With more levels of residual quantization, quantization error is consistently minimized, and the reconstruction performance (measured by rFID) steadily improves.",
                "position": 338
            },
            {
                "img": "https://arxiv.org/html/2503.17760/x5.png",
                "caption": "(a)Assignment confidence heatmap for vector quantization",
                "position": 354
            },
            {
                "img": "https://arxiv.org/html/2503.17760/x5.png",
                "caption": "(a)Assignment confidence heatmap for vector quantization",
                "position": 357
            },
            {
                "img": "https://arxiv.org/html/2503.17760/x6.png",
                "caption": "(b)Assignment confidence heatmap for attention quantization",
                "position": 363
            },
            {
                "img": "https://arxiv.org/html/2503.17760/x7.png",
                "caption": "Figure 6:Visualization of training dynamics. In attention quantization, codes arepushedto fully occupy the latent space, whereas vector quantization shows limited coverage of latent space.",
                "position": 372
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.17760/x8.png",
                "caption": "Figure 7:Visualization of samples on ImageNet256√ó256256256256\\times 256256 √ó 256. (a) Reconstruction results by CODA tokenizer. Compared with VQGAN, CODA showcases higher fidelity and effectively preserves rich details. (b) Generated samples by combining CODA with MaskGIT.",
                "position": 1020
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]