[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24304/x1.png",
                "caption": "Figure 1:(Top)Traditional Uniform Sparse Sampling is inefficient and may miss the key frame in long videos.(Bottom)Our method starts with a sparse scan for an overview (Turn 1), then dynamically chooses frames on promising segments (Turn 2), enabling a multi-turn analysis to efficiently focus on key frames in a long video.",
                "position": 115
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24304/x2.png",
                "caption": "Figure 2:(a) An illustration of the iterative reasoning process of our proposed FrameThiner.\nThe model first performs a sparse scan, then uses thought-action steps to progressively gather evidence. The CCV module ensures this process is logically consistent and interpretable. (b) Our three-stage training pipeline, consisting of Data Preparation, Supervised Fine-Tuning (SFT) to learn action syntax, and Reinforcement Learning (RL) to optimize the policy.",
                "position": 222
            },
            {
                "img": "https://arxiv.org/html/2509.24304/x3.png",
                "caption": "Figure 3:The distribution of data sources for the SFT (Left) and RL training phases (Right).",
                "position": 228
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24304/figs/train_stage.png",
                "caption": "(a)Ablation on the effect of training stages.",
                "position": 547
            },
            {
                "img": "https://arxiv.org/html/2509.24304/figs/train_stage.png",
                "caption": "(a)Ablation on the effect of training stages.",
                "position": 550
            },
            {
                "img": "https://arxiv.org/html/2509.24304/figs/Format.png",
                "caption": "(b)Ablation on the effect of format rewards.",
                "position": 555
            },
            {
                "img": "https://arxiv.org/html/2509.24304/figs/ccv.png",
                "caption": "(c)Ablation on the effect of the CCV module.",
                "position": 560
            },
            {
                "img": "https://arxiv.org/html/2509.24304/figs/reward.png",
                "caption": "(d)Ablation on the effect of reward configurations.",
                "position": 565
            },
            {
                "img": "https://arxiv.org/html/2509.24304/x4.png",
                "caption": "(a)Performance on Video-Holmes.",
                "position": 717
            },
            {
                "img": "https://arxiv.org/html/2509.24304/x4.png",
                "caption": "(a)Performance on Video-Holmes.",
                "position": 720
            },
            {
                "img": "https://arxiv.org/html/2509.24304/figs/Radar.png",
                "caption": "(b)Overall performance.",
                "position": 725
            },
            {
                "img": "https://arxiv.org/html/2509.24304/x5.png",
                "caption": "(c)Average accuracy comparison.",
                "position": 730
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AMore Implementation Details",
        "images": []
    },
    {
        "header": "Appendix BPrompts",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24304/x6.png",
                "caption": "Figure 6:The system prompt for FrameThinker. It outlines its core task, the specific syntax for action usage (get frame numberandchoose frames), and the structured<think>...<action>output format.",
                "position": 1540
            },
            {
                "img": "https://arxiv.org/html/2509.24304/x7.png",
                "caption": "Figure 7:The fallback system prompt triggered upon a CCV failure during inference. It directs the model to formulate a final answer directly.",
                "position": 1550
            },
            {
                "img": "https://arxiv.org/html/2509.24304/x8.png",
                "caption": "Figure 8:The Chain-of-Thought prompt used for evaluating baseline models.",
                "position": 1560
            }
        ]
    },
    {
        "header": "Appendix CAdditional Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24304/x9.png",
                "caption": "Figure 9:An example of our framework solving a reasoning task. Given a question about a specific timestamp (0:13), the model first formulates a thought and executes theget frame numberaction. Based on the returned frame number, it initiates a second reasoning turn, using thechoose framesaction to “zoom in” on the relevant scene. This multi-turn, iterative process allows the model to gather the necessary visual evidence (the state of the light switch’s reflection) to arrive at the correct answer.",
                "position": 1574
            },
            {
                "img": "https://arxiv.org/html/2509.24304/x10.png",
                "caption": "Figure 10:An example of resolving an ambiguous query without a specific timestamp. Given only sparse initial frames that suggest a conflict, the model’s first thought is to hypothesize a crucial temporal segment between the peaceful beginning and the depicted struggle. It then executes achoose framesaction to “zoom in” on this interval. The newly retrieved, denser frames provide unequivocal evidence of a chase and attack, allowing the model to confidently deduce the “Hunter and Fugitive” relationship and select the correct answer. This case demonstrates the effectiveness of the model’s iterative refinement process for complex narrative understanding.",
                "position": 1583
            },
            {
                "img": "https://arxiv.org/html/2509.24304/x11.png",
                "caption": "Figure 11:An example of a multi-step reasoning process that showcases hierarchical search. The model begins with a broad search (choose frames between 16407 and 32814) based on the sparse initial frames. After this first action reveals the mask as a key object, the model formulates a new thought and performs a second, more targeted search (choose frames between 25782 and 28125) to confirm its function. This iterative refinement strategy, moving from a coarse to a fine-grained analysis, demonstrates the model’s advanced capability to solve complex problems by efficiently focusing on the most relevant visual evidence.",
                "position": 1592
            },
            {
                "img": "https://arxiv.org/html/2509.24304/x12.png",
                "caption": "Figure 12:An example of Direct Answering. The user asks about the very beginning of the video. The model correctly identifies that the provided Frame 0 already contains the necessary information. In its thought process, it recognizes the sufficiency of the initial evidence and confidently proceeds directly to theoutput answeraction without invoking any actions. This case demonstrates the model’s strategic capability to not only explore when necessary but also to conclude efficiently when the answer is readily available, avoiding redundant actions.",
                "position": 1601
            }
        ]
    },
    {
        "header": "Appendix DBad Case",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24304/x13.png",
                "caption": "Figure 13:An example of mode collapse induced by unconditional reward for theget frame numberaction.",
                "position": 1622
            },
            {
                "img": "https://arxiv.org/html/2509.24304/x14.png",
                "caption": "Figure 14:An example of mode collapse induced by unconditional reward for thechoose framesaction.",
                "position": 1631
            },
            {
                "img": "https://arxiv.org/html/2509.24304/x15.png",
                "caption": "Figure 15:Training dynamics.(Top Row)Metrics from training with an unconditional reward.(Bottom Row)Metrics from training with a conditional reward. The finaloutput answeraction is not included in the action counts.",
                "position": 1648
            },
            {
                "img": "https://arxiv.org/html/2509.24304/x16.png",
                "caption": "",
                "position": 1657
            },
            {
                "img": "https://arxiv.org/html/2509.24304/x17.png",
                "caption": "",
                "position": 1663
            },
            {
                "img": "https://arxiv.org/html/2509.24304/x18.png",
                "caption": "",
                "position": 1668
            },
            {
                "img": "https://arxiv.org/html/2509.24304/x19.png",
                "caption": "Figure 16:An example of mode collapse induced by a reward that unconditionally encourages a higher number of turns.",
                "position": 1677
            },
            {
                "img": "https://arxiv.org/html/2509.24304/x20.png",
                "caption": "Figure 17:An example of a Logical Flow Check failure. The model first successfully retrieves frame815corresponding to a specific timestamp. However, its subsequentchoose framesaction targets an interval (565-645) that does not contain the retrieved frame, breaking the logical chain of actions.",
                "position": 1698
            },
            {
                "img": "https://arxiv.org/html/2509.24304/x21.png",
                "caption": "Figure 18:An example of a Fidelity Check failure. The model’s reasoning in the<think>block identifies the relevant area around frame 4974, but the executed<action>targets an entirely unrelated interval (1400-1500). The CCV module flags this trajectory as inconsistent.",
                "position": 1709
            }
        ]
    },
    {
        "header": "Appendix ETraining Dynamics",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24304/x22.png",
                "caption": "(a)Average Accuracy",
                "position": 1732
            },
            {
                "img": "https://arxiv.org/html/2509.24304/x22.png",
                "caption": "(a)Average Accuracy",
                "position": 1735
            },
            {
                "img": "https://arxiv.org/html/2509.24304/x23.png",
                "caption": "(b)Average Action Reward",
                "position": 1740
            },
            {
                "img": "https://arxiv.org/html/2509.24304/x24.png",
                "caption": "(c)Average Actions per Trajectory",
                "position": 1746
            },
            {
                "img": "https://arxiv.org/html/2509.24304/x25.png",
                "caption": "(d)Average Response Length",
                "position": 1751
            },
            {
                "img": "https://arxiv.org/html/2509.24304/x26.png",
                "caption": "Figure 20:Average video length (in minutes) across the six primary evaluation benchmarks.",
                "position": 1801
            },
            {
                "img": "https://arxiv.org/html/2509.24304/x27.png",
                "caption": "Figure 21:Distribution of reasoning templates (Left) and actions per trajectory (Right).",
                "position": 1814
            }
        ]
    },
    {
        "header": "Appendix FDatasets and Benchmarks",
        "images": []
    }
]