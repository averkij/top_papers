[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09001/phi3_example.png",
                "caption": "Figure 1:Entropy-based accuracy estimation for PHI-3.5-MINI-3.6B. Trained on two benchmarks (orange), the estimator generalizes to eight unseen STEM benchmarks (blue)",
                "position": 143
            }
        ]
    },
    {
        "header": "2From Signatures to Accuracy Estimates",
        "images": []
    },
    {
        "header": "3Entropy Profile Signals",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09001/MATH_test_max_entropy.png",
                "caption": "Figure 2:Max-entropy density forphi-3.5-minion MATH (correct vs. incorrect). Incorrect responses shift to higher entropy, indicating greater uncertainty.",
                "position": 563
            }
        ]
    },
    {
        "header": "4Accuracy Estimation via Entropy Profile Features",
        "images": []
    },
    {
        "header": "5Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09001/all_model_results.png",
                "caption": "Figure 3:Accuracy estimations from a random-forest classifier trained exclusively on compact entropy-profile features on GSM and OlympiadBench. Both train benchmarks span the two extremes of difficulty.",
                "position": 871
            }
        ]
    },
    {
        "header": "6Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09001/accuracy_vs_estimation.png",
                "caption": "Figure 4:Training-group difficulty vs. estimation quality forPhi-3.5-Mini: intermediate weighted accuracy (0.4–0.6) yields the lowest AEE; all-easy/all-hard groups perform worse.",
                "position": 1159
            }
        ]
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "9Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ABaseline White Box UQ Metrics",
        "images": []
    },
    {
        "header": "Appendix BAdditional Entropy Profile Features Results",
        "images": []
    },
    {
        "header": "Appendix CAdditional details for Reproducibility",
        "images": []
    },
    {
        "header": "Appendix DAdditional Classifier Configuration Results",
        "images": []
    },
    {
        "header": "Appendix EReduced-Feature set Classifier",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09001/all_llm_accuracy.png",
                "caption": "Figure 5:Relationship between training group difficulty and estimation quality aggregated across all nine LLMs. Training groups with intermediate weighted accuracy (0.4–0.6) achieve optimal performance, while difficulty-homogeneous groups at either extreme degrade generalization. Shaded region indicates IQR.",
                "position": 3041
            }
        ]
    },
    {
        "header": "Appendix FAdditional Training Sensibility Results",
        "images": []
    }
]