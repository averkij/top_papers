[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14434/x1.png",
                "caption": "Figure 1:Overview of theLLM-FEFramework.For a given dataset,LLM-FEfollows these steps: (a)New Hypothesis Generation, where an LLM generates feature transformation hypotheses as programs for a given tabular dataset; (b)Feature Engineering, where the feature transformation program is applied to the underlying dataset, resulting in a modified dataset; (c)Model Fitting, where a prediction model is fitted on the modified training set and evaluated on the corresponding validation set; (d)Multi-Population Memory, which maintains a buffer of high-scoring programs that act as in-context samples for LLM‚Äôs iterative refinement prompt.",
                "position": 166
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3LLM-FE",
        "images": []
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Results and Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14434/x2.png",
                "caption": "Figure 2:Aggregated ablation study results across classification datasets, showcasing the impact of individual components onLLM-FE‚Äôs performance: (a) Data Examples, (b) Domain Knowledge, and (c) Evolutionary Refinement. The model performance is measured using accuracy, with higher accuracy indicating better performance.",
                "position": 984
            }
        ]
    },
    {
        "header": "6Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14434/x3.png",
                "caption": "Figure 3:Performance Trajectory Analysis.Validation Accuracy progression forLLM-FEw/oevolutionary refinement andLLM-FE.LLM-FEdemonstrates better validation accuracy, highlighting the advantage of evolutionary iterative refinement.",
                "position": 1009
            },
            {
                "img": "https://arxiv.org/html/2503.14434/x4.png",
                "caption": "Figure 4:Quantitative and Qualitative Analysis on Impact of Domain Knowledge for LLM-FE on Heart and Breast-W datasets.(a)¬†Comparison of XGBoost performance forLLM-FEagainst its domain-agnostic variant and traditional methods, such as OpenFE and AutoFeat, which do not integrate domain knowledge and exhibit reduced performance. (b)¬†Features generated using thew/oDomain Knowledge variant ofLLM-FE. (c)¬†Feature discovery program generated by LLM-FE. Thegenerated programsemphasize how incorporating domain expertise leads to more interpretable features that improve model performance.",
                "position": 1013
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADataset Details",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14434/x5.png",
                "caption": "Figure 5:Example of an input prompt for balance-scale datasetcontaining (a) instruction, (b) dataset specification containing the details about the task, features, and data samples, (c) evaluation function, (d) initial in-context demonstration, and (e) function to complete.",
                "position": 1992
            },
            {
                "img": "https://arxiv.org/html/2503.14434/x6.png",
                "caption": "Figure 6:An example of the alternate set of instructionsused to direct the model to use a complex set of operations over simple operators for generating features.",
                "position": 1995
            },
            {
                "img": "https://arxiv.org/html/2503.14434/x7.png",
                "caption": "Figure 7:Impact of Noise Levelson XGBoost model performance across different feature engineering approaches, under increasing noise conditions (œÉ=0.0‚Å¢to‚Å¢0.1ùúé0.0to0.1\\sigma=0.0\\text{ to }0.1italic_œÉ = 0.0 to 0.1). We report the mean accuracy across six classification datasets containing only numerical features.",
                "position": 2096
            }
        ]
    },
    {
        "header": "Appendix CAdditional Analysis",
        "images": []
    }
]