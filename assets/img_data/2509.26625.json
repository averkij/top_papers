[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Problem Formulation",
        "images": []
    },
    {
        "header": "3Demystifying LLM Visual Priors: Studies and Findings",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26625/x1.png",
                "caption": "Figure 1:Impact of model and data sizes.The plots illustrate the performance of MLLMs, built upon LLMs of five different sizes (340M to 13B parameters), as a function of the amount ofweb-crawlpre-training data (0B to 100B tokens). The general trend shows that performance improves with both increasing model size and data volume, but the scaling behavior differs across task categories.",
                "position": 381
            },
            {
                "img": "https://arxiv.org/html/2509.26625/x2.png",
                "caption": "Figure 2:Impact of pre-training data sources.The bar charts illustrate the downstream VQA performance of MLLMs built upon a 3B parameter LLM, where each LLM was pre-trained on 30B tokens from a single, specific data source. The plots show that performance varies significantly depending on the pre-training sources.",
                "position": 416
            },
            {
                "img": "https://arxiv.org/html/2509.26625/x3.png",
                "caption": "Figure 3:Impact of reasoning-centric and visual data categories and proportions.The plots illustrate how varying the proportion of specific data categories in the pre-training mix affects downstream VQA performance.\nLeft plot (Reasoning-centric data): The plot shows that increasing the share of reasoning-centric data leads to progressive and significant performance gains, with benefits scaling up to a 75% proportion before plateauing. This indicates that a strong reasoning foundation is critical for enhancing visual abilities.\nRight plot (Visual world data): In contrast, the right plot, showing data that explicitly describes the visual world demonstrates rapidly diminishing returns. Only a small amount of this data is crucial to establish a baseline.",
                "position": 438
            },
            {
                "img": "https://arxiv.org/html/2509.26625/x4.png",
                "caption": "Figure 4:Correlation matrix for VQA performances.The matrix reveals two loosely-coupled skill clusters: one axis for perception (General/OCR) and another for reasoning (Knowledge/Vision-Centric).",
                "position": 923
            },
            {
                "img": "https://arxiv.org/html/2509.26625/x5.png",
                "caption": "Figure 5:Universality of the learned visual priors.The plots show the VQA performance of MLLMs built using three distinct vision encoders based on the proportion of reasoning-centric data used in the LLM’s pre-training mix. Despite differences in their absolute performance, all three configurations show a consistent improvement on reasoning-heavy tasks as the LLM’s reasoning pre-training proportion increases, similar to trends observed before, demonstrating the universality of the reasoning prior.",
                "position": 959
            },
            {
                "img": "https://arxiv.org/html/2509.26625/x6.png",
                "caption": "Figure 6:Step-wise effects of perception and reasoning visual-instruction tuning data removal.The chart shows the remaining performance (%) on Avg VQA and per-category VQA (x-axis) relative to a 100% baseline. The bars show performance after ablating perception or reasoning instruction data in stages (removing 50% and then 100% of the data). Removing perception-tuning data produces the largest performance drop on OCR & Chart VQA and General VQA (showing perception’s stronger dependence on supervised vision-side tuning), while removing reasoning-tuning data yields only small performance drops on perception tasks and modest drops on Vision-Centric and Knowledge VQA.",
                "position": 979
            }
        ]
    },
    {
        "header": "4Discussion and Hypotheses",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26625/x7.png",
                "caption": "Figure 7:Performance of MLLMs on the Multi-Level Existence Bench (MLE-Bench).The left plot shows the overall accuracy for models pre-trained on 16 different single-source data types. Other plots detail performance on objects of varying relative sizes, from small (0-30% of image pixels) to medium (30-60%) to large (60-100%). The results demonstrate that pre-training on the broad and diverseweb-crawlcorpus is most effective in gaining perception prior, with its advantage being particularly pronounced for perceiving smaller objects.",
                "position": 1012
            },
            {
                "img": "https://arxiv.org/html/2509.26625/x8.png",
                "caption": "Figure 8:Qualitative impact of reasoning-centric data on visual reasoning.The figure displays the answers from five models—pre-trained with 0% to 100%code reasoningdata—to a visual question requiring the application of a specific rule. Answers show a clear improvement in reasoning quality: the model with 0%code reasoningprovides a simplistic justification, while the models with 75% and 100%code reasoningproduce more detailed reasoning that correctly applies the definition from the prompt.",
                "position": 1042
            },
            {
                "img": "https://arxiv.org/html/2509.26625/x9.png",
                "caption": "Figure 9:Qualitative impact of reasoning-centric data on visual reasoning tasks.The plot shows how varying the proportion of different reasoning-centric data categories in the pre-training mix impacts metrics of visual reasoning quality. Results indicate that more reasoning data leads to more coherent and detailed visual reasoning.",
                "position": 1045
            },
            {
                "img": "https://arxiv.org/html/2509.26625/x10.png",
                "caption": "Figure 10:Qualitative impact of reasoning-centric data on representation alignment.The plot show how varying the proportion of different reasoning-centric data categories in the pre-training mix impacts metrics of cross-modal alignment. Results reveal that with the LLM-vision alignment score showing a generally positive but non-monotonic trend.",
                "position": 1074
            }
        ]
    },
    {
        "header": "5Scaling Up and Training a Vision-Aware LLM",
        "images": []
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Limitations and Future Research Directions",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "9Acknowledgment",
        "images": []
    },
    {
        "header": "10Broader Impact",
        "images": []
    },
    {
        "header": "11Visual World and Reasoning-centric Language Data Classification",
        "images": []
    },
    {
        "header": "12Visual Instruction Tuning Data Classification",
        "images": []
    },
    {
        "header": "13Robust Parsing for VQA Evaluations",
        "images": []
    },
    {
        "header": "14Multi-Level Existence Benchmark Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26625/x11.png",
                "caption": "Figure 11:MLE Benchmark Examples.The figure provides examples from the MLE-Bench, illustrating how the dataset is partitioned based on the ground-truth object size from reference segmentation maps. For instance, in the 0-30 split, the target object (a fireplace) constitutes a small fraction of the image. In contrast, the 60-90 split features a correct object (grass) that covers a substantial portion of the image.",
                "position": 1556
            }
        ]
    },
    {
        "header": "15Multi-Level Existence Benchmark Results",
        "images": []
    },
    {
        "header": "16Blind visual instruction tuning",
        "images": []
    },
    {
        "header": "17Hallucinations in Blind VQA",
        "images": []
    },
    {
        "header": "18Additional Qualitative Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26625/x12.png",
                "caption": "Figure 12:Qualitative impact of reasoning-centric data on visual spatial reasoning.The figure shows answers from five models—pre-trained with 0% to 100% reasoning combination data—to a visual question requiring depth perception. The answers demonstrate a clear improvement in reasoning quality: the model with 0% reasoning data gives a blunt answer, while the model with 100% reasoning data provides a detailed explanation correctly applying concepts of foreground and background.",
                "position": 1800
            },
            {
                "img": "https://arxiv.org/html/2509.26625/x13.png",
                "caption": "Figure 13:Qualitative impact of visual world data on complex visual perception.The figure displays answers from five models—pre-trained with 0% to 100% visual combination data—to a question requiring an understanding of color constancy. The results show that while the model with 25% visual data provides the correct answer with reasoning relevant to the core visual principle, models trained on more visual data offer incorrect answers and flawed explanations. This suggests that simply increasing descriptive visual text does not necessarily cultivate a deeper perceptual understanding.",
                "position": 1803
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]