[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.06801/x1.png",
                "caption": "Figure 1:a) We compare augmentations from our learned placement with heuristic-based placements from Lift3D[22]. In our augmentations, vehicles follow the lane orientations and are placed appropriately. b) These realistic augmentations significantly improve the 3D detection performance (KITTI[6]val set, (easy)). Notably, we achieve detection performance comparable to that of the fully labeled dataset using only50%percent5050\\%50 %of the dataset.",
                "position": 135
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.06801/x2.png",
                "caption": "Figure 2:a) SA-PlaceNet Architecture:Given an input background image and corresponding depth to predict the means of a multi-dimensional Gaussian distribution over 3D bounding boxes. 3D bounding boxes are sampled from each of these Gaussian to compute the training loss.b) Geometry-aware augmentationin BEV (Birds Eye View). For a given source car location (bl‚Å¢o‚Å¢csubscriptùëèùëôùëúùëêb_{loc}italic_b start_POSTSUBSCRIPT italic_l italic_o italic_c end_POSTSUBSCRIPT), we first findKùêæKitalic_Knearest neighbors with the same orientation and augment the location tob~l‚Å¢o‚Å¢csubscript~ùëèùëôùëúùëê\\tilde{b}_{loc}over~ start_ARG italic_b end_ARG start_POSTSUBSCRIPT italic_l italic_o italic_c end_POSTSUBSCRIPTby interpolating with neighboring locationsnl‚Å¢o‚Å¢csubscriptùëõùëôùëúùëên_{loc}italic_n start_POSTSUBSCRIPT italic_l italic_o italic_c end_POSTSUBSCRIPT(Alg.1)",
                "position": 221
            },
            {
                "img": "https://arxiv.org/html/2504.06801/x3.png",
                "caption": "Figure 3:Rendering pipeline:Given a 3D asset, we first render an image and shadow from a fixed light source according to the 3D box parameters. Next, we used edge-conditioned ControlNet[53]to generate a realistic car version that follows the same orientation and scale as the rendered image. Finally, we use the obtained shadow, rendered car, and 3D location to place the car and render augmented images.",
                "position": 323
            },
            {
                "img": "https://arxiv.org/html/2504.06801/x4.png",
                "caption": "Figure 4:Given an input source image, we plot the heatmaps of the mean objectness score at each pixel location. The generated heatmaps span a large region on the road with plausible locations of objects.\nNext, we show samples of bounding boxes and realistic renderings of cars in the scene.",
                "position": 407
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.06801/x5.png",
                "caption": "Figure 5:a) Ablation for object placement - For a background road scene, we visualize the heatmaps of aggregated objectness scores at each pixel location. Geometric augmentation and variational inference help to generate diverse and plausible object placements. b) Histogram of the distribution of orientations of the ground truth bounding boxes and the generated bounding boxes.",
                "position": 425
            },
            {
                "img": "https://arxiv.org/html/2504.06801/x6.png",
                "caption": "Figure 6:Ablation over rendering methods: Given the source image and predicted 3D bounding boxes, we sample and render a synthetic ShapeNet[5]car; Lift3D[22]rendered method; and our realistic rendering. We show a smaller domain gap between the rendered cars and the original samples.",
                "position": 428
            },
            {
                "img": "https://arxiv.org/html/2504.06801/x7.png",
                "caption": "Figure 7:Qualitative comparison of the generated augmentations with all the baseline methods. Our augmentations are highly realistic, place cars following plausible placement properties, and have a minimal domain gap from the training dist.",
                "position": 438
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional placement results",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.06801/x8.png",
                "caption": "Figure 8:Placement on nuScenes[3]dataset.",
                "position": 1753
            },
            {
                "img": "https://arxiv.org/html/2504.06801/x9.png",
                "caption": "Figure 9:Augmented training dataset for 3D object detection: Given a sparse scene with few cars, we place cars at the predicted 3D bounding box locations using our rendering algorithm. We present two sets of results, one with low density (1‚àí3131-31 - 3cars added) and another with high density (4‚àí5454-54 - 5cars added) for each scene.",
                "position": 1763
            },
            {
                "img": "https://arxiv.org/html/2504.06801/x10.png",
                "caption": "Figure 10:Placement results for pedestrian and cycle categories on KITTI dataset. Note that we applied copy-paste in the predicted 3D object box locations to generate the augmentations. Though copy-pasting causes image artifacts, these augmentations still improve 3D detection performance, as shown in the main paper.",
                "position": 1773
            }
        ]
    },
    {
        "header": "Appendix BAdditional object detection results",
        "images": []
    },
    {
        "header": "Appendix CComputational cost of MonoPlace3D",
        "images": []
    },
    {
        "header": "Appendix DData Augmentation for Corner Cases",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.06801/x11.png",
                "caption": "Figure 11:Detection improvement in corner cases.",
                "position": 2310
            }
        ]
    },
    {
        "header": "Appendix EImplementations details",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.06801/x12.png",
                "caption": "Figure 12:Outputs generated from Stable Diffusion Inpainting pipeline[38]. These inpainted images are used for training our placement model.",
                "position": 2323
            },
            {
                "img": "https://arxiv.org/html/2504.06801/x13.png",
                "caption": "Figure 13:Sampled views rendered from Lift3D[22].",
                "position": 2342
            },
            {
                "img": "https://arxiv.org/html/2504.06801/x14.png",
                "caption": "Figure 14:Sample cars from the Copy-Paste Database",
                "position": 2400
            },
            {
                "img": "https://arxiv.org/html/2504.06801/x15.png",
                "caption": "Figure 15:Sample of ShapeNet[5]cars rendered at different views.",
                "position": 2419
            },
            {
                "img": "https://arxiv.org/html/2504.06801/extracted/6350157/figs/fig13-orientation-correction.jpg",
                "caption": "Figure 16:Perspective and Absolute projection of cars with the same 3D orientation.",
                "position": 2422
            },
            {
                "img": "https://arxiv.org/html/2504.06801/x16.png",
                "caption": "Figure 17:a) Diverse renderings generated with edge-conditioned ControlNet. B) Shadows are generated by rendering 3D assets with a point light source in the blender[8]environment",
                "position": 2432
            }
        ]
    },
    {
        "header": "Appendix FRendering details",
        "images": []
    }
]