[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26476/x1.png",
                "caption": "Figure 1:A Regression Language Model (RLM) is able to simultaneously read code from many different languages and compilation levels, and predict metrics such as accuracy, memory, and latency.",
                "position": 139
            }
        ]
    },
    {
        "header": "2Related Work and Motivation",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Data",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26476/x2.png",
                "caption": "Figure 2:Diagonal fit (╱\\diagup) is better. Scatterplot of RLM’s pointwiseyy-prediction vs. ground truth value over varying tasks from CodeNet (C++ and Python), Triton Kernels, and APPS. For better visualization, axes are scaled by percentile (probits), andyy-value ticks are shown at 10 and 90%.",
                "position": 480
            },
            {
                "img": "https://arxiv.org/html/2509.26476/x3.png",
                "caption": "Figure 3:We identified problems with>>8 candidate solution from our test set of 15000, and investigate whether the RLM is able torankpotential solutions.(Left)Distribution of problems and their in-problem Spearmanρ\\rhorankings using the RLM.(Right)RLM vs random selection for choosing the top-1 lowest memory solution from a question, organized by solution count.",
                "position": 483
            },
            {
                "img": "https://arxiv.org/html/2509.26476/x4.png",
                "caption": "Figure 5:Single RLM trained on five consecutive objectives on NASBench-201, i.e. first validation accuracy and then hardware-specific latencies over four devices (Pixel3 (Mobile), Eyeriss (ASIC), Intel CPU and Nvidia GPU). Spearmanρ\\rhorefers to predicted latency. Density estimates (blue) are plotted for predicted Pareto-optimal pointsx∗x^{*}.",
                "position": 686
            }
        ]
    },
    {
        "header": "6Experiments: Ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26476/x5.png",
                "caption": "Figure 6:Lower (↓\\downarrow) is better. Validation loss curves when training from T5Gemma checkpoint (0.532ρ\\rho) vs. random-init (0.504ρ\\rho).",
                "position": 703
            },
            {
                "img": "https://arxiv.org/html/2509.26476/x5.png",
                "caption": "Figure 6:Lower (↓\\downarrow) is better. Validation loss curves when training from T5Gemma checkpoint (0.532ρ\\rho) vs. random-init (0.504ρ\\rho).",
                "position": 706
            },
            {
                "img": "https://arxiv.org/html/2509.26476/x6.png",
                "caption": "Figure 7:Lower (↓\\downarrow) is better. Validation loss curves when training from synthetic FLOPS pretrained checkpoint (0.85ρ\\rho) vs. random-init (0.83ρ)\\rho).",
                "position": 711
            },
            {
                "img": "https://arxiv.org/html/2509.26476/x7.png",
                "caption": "Figure 8:RLM predictions for FLOPS over 1024 test architectures.",
                "position": 717
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AUnified Model Ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26476/x8.png",
                "caption": "Figure 9:Higher is better (↑\\uparrow). Spearmanρ\\rhoon KernelBook examples, over different training checkpoints.",
                "position": 1827
            }
        ]
    },
    {
        "header": "Appendix BAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26476/x9.png",
                "caption": "Figure 10:Higher (↑\\uparrow) is better. Fraction of problems (with>>8 solutions) where the model’s predicted best solution lies within the true top-p%p\\%of solutions; dashed line shows the random pick baseline.",
                "position": 2094
            }
        ]
    },
    {
        "header": "Appendix CExperimental Settings",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26476/x10.png",
                "caption": "Figure 11:Histogram of the target values for APPS, KernelBook and CodeNet",
                "position": 2146
            }
        ]
    },
    {
        "header": "Appendix DData: Extended",
        "images": []
    }
]