[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.20888/x1.png",
                "caption": "Figure 1:Video-As-Prompt (VAP) is a unified semantic-controlled video generation framework:it treatsreference videos with wanted semantics as video promptsand controls generation viaa plug-and-play, in-context Mixture-of-Transformers expert.Row1−61-6:reference videos used as prompts for diverse semantic-controlled video generation tasks (concept, style, motion, camera).Row77:zero-shot generalization results from Video-As-Prompt when given an unseen semantic, demonstrating strong generalizability.",
                "position": 136
            },
            {
                "img": "https://arxiv.org/html/2510.20888/x2.png",
                "caption": "Figure 2:Controllable Video Generation Paradigms.Structure-Controlled Video Generation (a). The condition is pixel-aligned with the target video. Most works inject conditions (e.g., depth, optical flow, pose) into a DiT via an extra branch using(a) residual addition, leveraging pixel-wise alignment.Semantic-Controlled Video Generation (b, c, d). The condition and target video share the same semantics. Most works use(b) Condition-Specific Overfitor(c) Task-Specific Design: finetuning per semantic or adding task-specific modules.(d) Video-as-Prompt: We use a reference video with the same semantics as prompts and adopt a plug-and-play in-context control framework built on mixture-of-transformers to achieve unified semantic-controlled video generation.",
                "position": 190
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.20888/x3.png",
                "caption": "Figure 3:Overview of Our ProposedVAP-Data.(a)100100semantic conditions across44categories: concept, style, camera, and motion; (b) diverse reference images, including animals, humans, objects, and scenes, with multiple variants; and (c) a word cloud of the semantic conditions.",
                "position": 237
            }
        ]
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.20888/x4.png",
                "caption": "Figure 4:Overview of Video-As-Prompt.The reference video (with the wanted semantics), target video, and their first frames (reference images) are encoded into latents by the VAE and, together with captions (See top right), form an in-context token sequence[R​e​ft​e​x​t,R​e​fv​i​d​e​o,T​a​rt​e​x​t,T​a​rv​i​d​e​o][Ref_{text},Ref_{video},Tar_{text},Tar_{video}](See middle. We omitted term“tokens”for simplicity.). First frame tokens are concatenated with video tokens.\nWe add a temporal biasΔ\\Deltato RoPE to avoid nonexistent pixel-aligned priors from the original shared RoPE (See bottom right).\nThe reference video and captions act as the prompts and are fed into a trainable DiT Expert Transformer444The number and position of In-Context DiT Expert layersN′N^{\\prime}are fully customizable.(See left), which exchanges information bidirectionally with the pre-trained DiT via full attention at each layer, enabling plug-and-play in-context generation.",
                "position": 329
            },
            {
                "img": "https://arxiv.org/html/2510.20888/x5.png",
                "caption": "Figure 5:Motivation.Ablation visualizations (Semantic: Spin360∘360^{\\circ}) on structure designs ofVAP.",
                "position": 348
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.20888/x6.png",
                "caption": "Figure 6:Qualitative comparisonwith VACE[34], CogVideoX (I2V)[76], CogVideoX-LoRA (I2V) and commercial models[40,68]; VACE(*) uses a *-form condition (top left).More visualizations are in the project page.",
                "position": 392
            },
            {
                "img": "https://arxiv.org/html/2510.20888/x7.png",
                "caption": "Figure 7:Zero-Shot Performance.Given semantic conditions unseen inVAP-Data(left column),VAPstill transfers the abstract semantic pattern to the reference image in a zero-shot manner.",
                "position": 555
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Gallery",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.20888/x8.png",
                "caption": "Figure 8:Additional visualizationsofVAP, including entity transformation and entity interaction in concept semantic categories.",
                "position": 1912
            },
            {
                "img": "https://arxiv.org/html/2510.20888/x9.png",
                "caption": "Figure 9:Additional visualizationsofVAP, including style semantic categories, motion semantic categories (Non-Human Motion and Human Motion), and camera semantic categories.",
                "position": 1917
            }
        ]
    },
    {
        "header": "8Application",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.20888/x10.png",
                "caption": "Figure 10:Given different reference videos (with different semantics) and the same reference image, ourVAPconsistently generates a new video for each semantic.",
                "position": 1955
            },
            {
                "img": "https://arxiv.org/html/2510.20888/x11.png",
                "caption": "Figure 11:Given different reference videos (with the same semantics) and the same reference image, ourVAPconsistently generates the target video aligned with the provided semantics",
                "position": 1960
            },
            {
                "img": "https://arxiv.org/html/2510.20888/x12.png",
                "caption": "Figure 12:Given one reference video and different reference images, ourVAPtransfers the same semantics from the reference video to each image and generates the corresponding videos.",
                "position": 1965
            },
            {
                "img": "https://arxiv.org/html/2510.20888/x13.png",
                "caption": "Figure 13:Given a fixed reference video and a reference image, ourVAPpreserves semantics and identity while using a user-modified prompt to adjust fine-grained attributes.",
                "position": 1970
            }
        ]
    },
    {
        "header": "9Implementation Details",
        "images": []
    },
    {
        "header": "10Dataset",
        "images": []
    },
    {
        "header": "11Limitation Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.20888/x14.png",
                "caption": "Figure 14:Limitation visualization.VAPtransfers semantics reliably when the semantic description of reference caption aligns with that of the target caption and subject structure aligns with the target: aligned descriptions (“gold liquid”and“liquid metal”) and similar subject structures (Grogu and a young woman) yield good results (top). Mislabeled semantic descriptions (“water”vs.“liquid metal”), or large subject mismatch (Grogu vs. snail), reduce alignment and visual quality (bottom).",
                "position": 2500
            },
            {
                "img": "https://arxiv.org/html/2510.20888/x15.png",
                "caption": "Figure 15:Failure case of multi-reference prompting.Left: three reference videos with divergent structure and similar semantics (human, spider, flatfish). Right: Ground truth is on top. Using three (bottom) spuriously transfers unwanted appearance cues (e.g., fish shape and spider-like legs) onto the dog. We attribute this leakage to generic captions that lack an explicit referent; stronger multi-reference control or instruction-style captions could mitigate it.",
                "position": 2515
            }
        ]
    },
    {
        "header": "12Ablation Study",
        "images": []
    }
]