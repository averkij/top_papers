[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction and Background",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23632/attn_diagrams/Softmax_Attention2.png",
                "caption": "Figure 1:Softmax attention as an RNN. We defineGtG_{t}italic_G start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTin place for the softmax denominator. Linear attention is equivalent to then=1n=1italic_n = 1, first order, term.",
                "position": 279
            }
        ]
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23632/sm_equiv_images/sm_eqiv_combined.png",
                "caption": "Figure 2:Test and train loss on various datasets for softmax attention and the proposed methods with gate or norm replacements. Expanded plots can be found in AppendixG.",
                "position": 374
            },
            {
                "img": "https://arxiv.org/html/2507.23632/scale_images/Large_test2.png",
                "caption": "Figure 3:Test and train loss on the FineWeb dataset for large models (about 2B parameters) on 1024 sequence length and small models (about 300 million parameters) on 4096 sequence length for softmax attention and the proposed methods with gate or norm replacements. Some experiments were cut short due to time constraints",
                "position": 391
            },
            {
                "img": "https://arxiv.org/html/2507.23632/scale_images/Large_train2.png",
                "caption": "",
                "position": 400
            },
            {
                "img": "https://arxiv.org/html/2507.23632/scale_images/Long_test2.png",
                "caption": "",
                "position": 406
            },
            {
                "img": "https://arxiv.org/html/2507.23632/scale_images/Long_train2.png",
                "caption": "",
                "position": 411
            },
            {
                "img": "https://arxiv.org/html/2507.23632/linear_images/test2.png",
                "caption": "Figure 4:Test and train loss on the FineWeb dataset for various linear attention methods, softmax attention, and the proposed methods with gate or norm replacements.",
                "position": 421
            },
            {
                "img": "https://arxiv.org/html/2507.23632/linear_images/train2.png",
                "caption": "",
                "position": 430
            },
            {
                "img": "https://arxiv.org/html/2507.23632/taylor_images/sm_taylor2.png",
                "caption": "Figure 5:Log train loss for softmax attention and various linear attention method when summing more powers of the inner product. The nth order denotes the sum of powers from 0 to n.",
                "position": 446
            },
            {
                "img": "https://arxiv.org/html/2507.23632/taylor_images/cos_taylor2.png",
                "caption": "",
                "position": 455
            },
            {
                "img": "https://arxiv.org/html/2507.23632/taylor_images/relu_taylor2.png",
                "caption": "",
                "position": 461
            },
            {
                "img": "https://arxiv.org/html/2507.23632/taylor_images/elu_taylor2.png",
                "caption": "",
                "position": 466
            },
            {
                "img": "https://arxiv.org/html/2507.23632/ablations/ablation_combined.png",
                "caption": "Figure 6:Test and train loss on various datasets for softmax attention and the proposed methods with gate or norm replacements. Expanded plots can be found in AppendixG.",
                "position": 476
            }
        ]
    },
    {
        "header": "5Conclusions and Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AInner Product Decomposition",
        "images": []
    },
    {
        "header": "Appendix BQuadratic Derivation",
        "images": []
    },
    {
        "header": "Appendix CBidirectional Derivation",
        "images": []
    },
    {
        "header": "Appendix DModel Parameters",
        "images": []
    },
    {
        "header": "Appendix EAdditional Gate Information",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23632/attn_diagrams/Gated_Linear_Attention.png",
                "caption": "Figure 7:Linear attention as an RNN with an input and output gate.",
                "position": 1532
            }
        ]
    },
    {
        "header": "Appendix FLinear Attention Expansions",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23632/sm_equiv_images/FineWeb_test2.png",
                "caption": "Figure 8:Expanded test and train loss plots on various datasets for softmax attention and the proposed methods with gate or norm replacements.",
                "position": 1570
            },
            {
                "img": "https://arxiv.org/html/2507.23632/sm_equiv_images/FineWeb_train2.png",
                "caption": "",
                "position": 1579
            },
            {
                "img": "https://arxiv.org/html/2507.23632/sm_equiv_images/SlimPajama_test2.png",
                "caption": "",
                "position": 1585
            },
            {
                "img": "https://arxiv.org/html/2507.23632/sm_equiv_images/SlimPajama_train2.png",
                "caption": "",
                "position": 1590
            },
            {
                "img": "https://arxiv.org/html/2507.23632/sm_equiv_images/Pile_test2.png",
                "caption": "",
                "position": 1596
            },
            {
                "img": "https://arxiv.org/html/2507.23632/sm_equiv_images/Pile_train2.png",
                "caption": "",
                "position": 1601
            },
            {
                "img": "https://arxiv.org/html/2507.23632/ablations/softmax_test2.png",
                "caption": "Figure 9:Expanded test and train loss on various datasets for softmax attention and the proposed methods with gate or norm replacements.",
                "position": 1610
            },
            {
                "img": "https://arxiv.org/html/2507.23632/ablations/softmax_train2.png",
                "caption": "",
                "position": 1619
            },
            {
                "img": "https://arxiv.org/html/2507.23632/ablations/gate_test2.png",
                "caption": "",
                "position": 1625
            },
            {
                "img": "https://arxiv.org/html/2507.23632/ablations/gate_train2.png",
                "caption": "",
                "position": 1630
            },
            {
                "img": "https://arxiv.org/html/2507.23632/ablations/norm_test2.png",
                "caption": "",
                "position": 1636
            },
            {
                "img": "https://arxiv.org/html/2507.23632/ablations/norm_train2.png",
                "caption": "",
                "position": 1641
            }
        ]
    },
    {
        "header": "Appendix GExpanded Results Figures",
        "images": []
    }
]