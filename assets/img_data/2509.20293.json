[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.20293/x1.png",
                "caption": "Figure 1:The majority of true judgment variance has no known cause.On the Arena-Hard-Auto benchmark, with a rubric specifying 5 judgment criteria, we find that across four judges and two settings (different cohorts of models to be compared), approximately 55% of variance, on average, is unexplained either by linear or taylor-series polynomial factor analysis on the rubric criteria. After ELO transformation, the linear model explains 100% of observed variance, indicating that, by enforcing transitivity, ELO hides true latent uncertainty in multi-factor analysis.",
                "position": 95
            }
        ]
    },
    {
        "header": "2Methods",
        "images": []
    },
    {
        "header": "3Experimental Setup",
        "images": []
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.20293/correlation_matrix.png",
                "caption": "Figure 2:Most factors are highly correlated for most judges.In Setting 1, across all four judges, the average spearman rank correlation matrix shows high cross-factor correlations (>0.93>0.93for most pairs). This suggests factor collapse â€“ the inability of judges to meaningfully distinguish between semantically distinct rubric factors in the setting.",
                "position": 222
            },
            {
                "img": "https://arxiv.org/html/2509.20293/factor_loadings_scree_newbench.png",
                "caption": "Figure 3:Diverse judges exhibit very similar latent factor loadings.Across four different LLM judges in benchmark Setting 2, the eigenvalues associated with factor weightings are highly similar; all indicate a collapse of significance in the latent loadings.",
                "position": 225
            },
            {
                "img": "https://arxiv.org/html/2509.20293/elo_ranking_collapse.png",
                "caption": "Figure 4:ELO-style aggregation compresses multi-dimensional, noisy judgments into apparently smooth rankings, masking upstream uncertainty.",
                "position": 237
            }
        ]
    },
    {
        "header": "5Background",
        "images": []
    },
    {
        "header": "6Limitations and Future Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "LLM Use Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASchematic Adherence Complete Results",
        "images": []
    },
    {
        "header": "Appendix BMetrics",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.20293/factor_ablation.png",
                "caption": "Figure 5:Factor ablation study results showing that while absolute ELO scores change dramatically when factors are evaluated in isolation versus collectively, model rankings remain largely stable. This demonstrates that our findings about judge bias are robust to different evaluation methodologies.",
                "position": 1628
            }
        ]
    },
    {
        "header": "Appendix CExperimental Evaluation Settings",
        "images": []
    }
]