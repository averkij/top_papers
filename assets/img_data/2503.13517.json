[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/figs/Figure_2_Mar1.png",
                "caption": "Figure 1:(a) Average normalized performance of state-of-the-art LLMs across 10 tasks from six scientific domains in CURIE. (b) Comparing performance of different model versions supporting long-context windows on previous benchmarks testing Knowledge (MMLU), Linguistic (DROP), and Science expertise (GPQA), along with our new scientific long-context understanding CURIE benchmark, highlighting the difficulty of the tasks in the benchmark.",
                "position": 156
            },
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/figs/Figure_1_Mar1.png",
                "caption": "",
                "position": 159
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/figs/curie_pie.png",
                "caption": "(a)",
                "position": 186
            },
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/figs/curie_pie.png",
                "caption": "(b)",
                "position": 268
            },
            {
                "img": "https://arxiv.org/html/2503.13517/x1.png",
                "caption": "(c)",
                "position": 274
            }
        ]
    },
    {
        "header": "3CURIE dataset and tasks",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13517/x2.png",
                "caption": "Figure 3:Examples of tasks in the CURIE benchmark.The DFT, HFD, QECC, and GEO tasks require the LLM to perform tasks on scientific papers (top blocks), as described in the task snippets (in orange), to extract, calculate, or aggregate information. Expected output (ground truth) snippets are shown in the blue blocks. (Only snippets of the query /outputs are shown for illustrative purposes.)",
                "position": 299
            },
            {
                "img": "https://arxiv.org/html/2503.13517/_styxpress_tmp_eczoo_icon_ascii.pdf",
                "caption": "",
                "position": 324
            },
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/biodiversity_images/jwmg22383_1.png",
                "caption": "(a)",
                "position": 337
            },
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/biodiversity_images/jwmg22383_1.png",
                "caption": "(a)",
                "position": 340
            },
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/figs/pdb_task.png",
                "caption": "(b)",
                "position": 346
            },
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/figs/gt_word_len.png",
                "caption": "(c)",
                "position": 352
            }
        ]
    },
    {
        "header": "4Experiments and Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/figs/Figure_6_Mar1.png",
                "caption": "Figure 5:Per task normalized scoresof various LLMs on the CURIE benchmark that measures performance of LLMs on 10 long-context tasks requiring expertise across six scientific disciplines. DFT-S, DFT-P, and MPV are scored using LLMSim, while others use programmatic metrics.",
                "position": 370
            }
        ]
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/supp_figs/pdb-structure_example_2.png",
                "caption": "Figure 6:Comparing responses on the PDB task. Measuring alignment using identity ratio (I‚Å¢Drùêºsubscriptùê∑ùëüID_{r}italic_I italic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT) between the predicted amino acid sequence and groundtruth sequence for a given protein structure, Gemini 1.5 pro and Claude were better at predicting the sequence of amino acids, whereas GPT-4o collapsed into a mode of repetition, and Flash 2.0 generated code to solve the problem.",
                "position": 645
            },
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/figs/Figure_8_Mar1.png",
                "caption": "Figure 7:Avg. performance of models sliced by difficulty of examples.Consistent with expectations, all models perform substantially better on easy examples except in the case of Mixtral. For most domains, experts independently converged on measuring difficulty for each example based on how spread-out the requested information was within the context of the full paper.",
                "position": 874
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/supp_figs/dft-metadata_example.png",
                "caption": "Figure 8:Example of model outputs for the DFT-Pparameter identification task. Claude-3 Opus appears to understand the purpose of the calculations better than the other models and avoids unnecessary repetition. Claude-3 correctly (green) identifies that there is one set of DFT parameters used in the actual study as well as two more set of parameters which are used for convergence testing.",
                "position": 888
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Reproducibility statement",
        "images": []
    },
    {
        "header": "Disclaimer",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADataset statistics and tasks overview.",
        "images": []
    },
    {
        "header": "Appendix BModel performances on science vs long-context benchmarks",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/supp_figs/curie_vs_sciqa_longctx.png",
                "caption": "Figure 9:Comparing model performances on scientific QA and long-context benchmarks.We compare the performance of the top 3 closed models on CURIE against, 2 science QA benchmarks - GPQA, MathVista, an improved language understanding benchmark MMLU-pro, and 2 long-context benchmarks Zero-Scrolls and RULER.",
                "position": 2353
            }
        ]
    },
    {
        "header": "Appendix CAnnotator agreements.",
        "images": []
    },
    {
        "header": "Appendix DLMScore: A coarse model-based evaluation metric.",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/figs/Figure_7_sep5.png",
                "caption": "Figure 10:Correlation of GPT-4o based LMScore metric with human evaluations,Across tasks in domains where ROUGE-L is the primary evaluation metric, LMScore¬†appears to be a promising alternative to ROUGE.",
                "position": 2387
            }
        ]
    },
    {
        "header": "Appendix ELimitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/supp_figs/dft-structure_example.png",
                "caption": "Figure 14:Model responses on the DFT-S structure metadata extraction task. Claude-3 Opus extracts accurate structures (green) relevant to the DFT computation whereas the other models do not precisely identify the exact structures that go into the DFT computation and tend to repeat entries.",
                "position": 2864
            },
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/supp_figs/mpv_highlight_errors.png",
                "caption": "Figure 16:Example from the MPV task where Claude-3 Opus, Gemini 1.5 pro, and GPT-4o recalled some important material properties and values but failed to capture all relevant properties and focused instead on some trivial properties. (paper id: 53519111)",
                "position": 2976
            },
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/supp_figs/mpv_example_correct.png",
                "caption": "Figure 17:Example from the MPV task where all models correctly identify and extract a material, property and other information but have slight differences. (paper id: 15804005)",
                "position": 2979
            },
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/supp_figs/hfd-structure_example.png",
                "caption": "Figure 20:Model responses for an example in the HFD task.",
                "position": 3147
            },
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/supp_figs/hfe-structure_example.png",
                "caption": "Figure 21:Model responses for an example in the HFE task.",
                "position": 3150
            },
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/supp_figs/qecc_example.png",
                "caption": "Figure 23:Examples of outputs on the QECC task. All models do a reasonable job in extracting and describing the code. Claude-3 does a good attempt to recover the technical details. (paper id: 1501.07779)",
                "position": 3439
            },
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/supp_figs/geo-structure_example.png",
                "caption": "Figure 25:Example from the GEO task where all closed models Gemini 1.5 pro, GPT-4o, and Claude-3 Opus extracted the correct spatial and time range for the datasets. They provided a less specific link than what human annotators provided and used a slightly different format where the models treated each variable as a separate dataset whereas human considered the dataset to be a single source but with different variables. Despite this slight difference the model‚Äôs response is still accurate and useful. (paper id: 00000)",
                "position": 3524
            },
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/supp_figs/geo_supp_datasets.png",
                "caption": "Figure 26:Score value versus number of datasets in the paper. There papers with fewer datasets have substantially higher performance, but there isn‚Äôt a linear degradation in performance as the number of datasets studied in the paper increases perhaps because when many datasets are used in the paper they are still tied to some common sources.",
                "position": 3548
            },
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/supp_figs/geo_supp_continent.png",
                "caption": "Figure 27:Score value versus geographical/spatial range studied in the paper. The scores for papers containing datasets covering multiple continents in their spatial extents (the 4 bars on the right) have noticeably lower performance compared to those covering the spatial extent within a single continent. Further, spatial ranges in Europe and Antartica seem better scoped than other continents, this might be worth verifying with a large sampling of papers.",
                "position": 3554
            },
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/biodiversity_images/jwmg22383_1_map_view.png",
                "caption": "Figure 28:Illustrations of map georeferencing, showing images from the dataset superimposed over geographical maps in their correct georeferenced locations.",
                "position": 3568
            },
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/biodiversity_images/578304_2_map_view.png",
                "caption": "",
                "position": 3571
            },
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/biodiversity_images/587419_1_map_view.png",
                "caption": "",
                "position": 3573
            },
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/biodiversity_images/592166_1_map_view.png",
                "caption": "",
                "position": 3574
            },
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/biodiversity_images/S0048969724009641_1_map_view.png",
                "caption": "",
                "position": 3576
            },
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/biodiversity_images/10212153_1_map_view.png",
                "caption": "",
                "position": 3577
            },
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/biodiversity_images/592526_1.png",
                "caption": "Figure 29:A sample of map images from the Biodiversity Georeferencing Task dataset.",
                "position": 3587
            },
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/biodiversity_images/590257_1.png",
                "caption": "",
                "position": 3590
            },
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/supp_figs/biogr_example1.png",
                "caption": "Figure 31:Responses from the different models for an example from the BIOGR task. Here, Claude-3 (blue) predicts the coordinates closest to the ground truth (green) bounding box locations indicated by the red dot in the original map image (top-left). GPT-4o (purple) and Claude-3 Opus generally performed well across examples on this task while Gemini 1.5 pro (orange) was a bit less precise. (record id: 556058_2)",
                "position": 3637
            },
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/biodiversity_images/IOU_007.png",
                "caption": "(a)",
                "position": 3653
            },
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/biodiversity_images/IOU_007.png",
                "caption": "(a)",
                "position": 3656
            },
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/biodiversity_images/IOU_08.png",
                "caption": "(b)",
                "position": 3662
            },
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/biodiversity_images/iou_vs_subsets.png",
                "caption": "Figure 33:On a subset of 24 BIOGR examples, the left chart shows a breakdown of IoU for various data subsets. On the right we show IoU versus task difficulty ratings provided by a domain expert.",
                "position": 3732
            },
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/biodiversity_images/iou_vs_difficulty.png",
                "caption": "",
                "position": 3735
            },
            {
                "img": "https://arxiv.org/html/2503.13517/extracted/6281453/supp_figs/pdb_supp_jan14.png",
                "caption": "Figure 35:Plot of identity ratio versus protein sequence length, showing that performance drops as the protein sequence length increases.",
                "position": 3797
            }
        ]
    },
    {
        "header": "Appendix FDetailed descriptions of tasks and data",
        "images": []
    }
]