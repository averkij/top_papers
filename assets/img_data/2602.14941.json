[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.14941/x1.png",
                "caption": "Figure 1:Global 3D reconstruction accumulates cross-view misalignment, introducing artifacts in the reconstructed geometry ((a), middle), which propagate to the generated video as hallucinations ((b), red boxes).\nIn contrast, per-frame local geometry inherently avoids cross-view misalignment and therefore remains clean ((a), right).\nConditioning on multiple retrieved local geometric anchors, AnchorWeave maintains strong spatial consistency with the historical frames ((c), white and green boxes).",
                "position": 114
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.14941/x2.png",
                "caption": "Figure 2:Coverage-driven memory retrieval pipeline.\nGiven a target camera, we first select local memories whose camera FoVs partially overlap with the target camera view to form a candidate memory pool. At each retrieval step, we greedily select the memory that maximizes the newly covered visible area.\nPoints invisible to the target camera are shown ingray.\nSii–Mjjdenotes memoryjjselected at retrieval stepii, and the red box indicates the retrieved memory.\nIn Sii-Mjj, regions already covered by previously retrieved memories are highlighted ingreenand only newly covered regions retain their original RGB colors. No green regions appear in S1-Mjjsince 1st-step’s coverage is empty.\nRetrieval terminates when the uncovered region is 0%, the retrieval budgetKKis exhausted, or the remaining memory pool is empty. For clarity, coverage is computed with a single frame here, while in practice is aggregated over multiple frames per chunk.",
                "position": 167
            }
        ]
    },
    {
        "header": "3Methodology: AnchorWeave",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.14941/x3.png",
                "caption": "Figure 3:Architecture of multi-anchor weaving controller. Anchors are encoded and jointly processed by a shared attention block, followed by camera-pose-guided fusion to produce a unified control signal injected into the backbone model. Camera 1 toKKrepresent the retrieved-to-target camera poses for the 1 toKKanchor videos, where each denotes the relative pose between the camera associated with a retrieved local point cloud and the target camera, measuring their viewpoint proximity. Camera 0 is the relative target camera trajectory.",
                "position": 236
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.14941/x4.png",
                "caption": "Figure 4:Qualitative comparison with baselines on DL3DV. K=1 means one retrieval per chunk. Baseline methods suffer from spatial drift and inconsistency in details. In contrast, AnchorWeave (ours) maintains consistency for multiple cases. GT and the historical context are shown for reference. For clarity, representative misaligned regions are highlighted with red boxes for strong baselines (e.g., SEVA).",
                "position": 499
            },
            {
                "img": "https://arxiv.org/html/2602.14941/x5.png",
                "caption": "Figure 5:Ablation study.\n(a) Pose-guided fusion suppresses misaligned anchors and reduces artifacts compared to simple averaging.\n(b) Joint attention outperforms separate attention, enabling coherent multi-anchor aggregation.",
                "position": 526
            },
            {
                "img": "https://arxiv.org/html/2602.14941/x6.png",
                "caption": "Figure 6:Long-horizon world exploration examples with keyboard-controlled camera actions on open-domain images.\nAnchorWeave preserves consistent object geometry and appearance across frames (highlighted by dashed boxes of the same color).",
                "position": 635
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Implementation Details",
        "images": []
    },
    {
        "header": "Appendix BCoverage-Driven Memory Retrieval Algorithm",
        "images": []
    },
    {
        "header": "Appendix CCamera Controllability Comparison with Baselines",
        "images": []
    },
    {
        "header": "Appendix DAnchorWeave Generation Process Visualization",
        "images": []
    },
    {
        "header": "Appendix EQualitative Comparison with Baselines under Free-Form Camera Trajectory",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.14941/x7.png",
                "caption": "Figure 7:Context-conditioned camera control generation process of AnchorWeave.",
                "position": 1849
            },
            {
                "img": "https://arxiv.org/html/2602.14941/x8.png",
                "caption": "Figure 8:Context-conditioned camera control generation process of AnchorWeave.",
                "position": 1853
            },
            {
                "img": "https://arxiv.org/html/2602.14941/x9.png",
                "caption": "Figure 9:Context-conditioned camera control results of AnchorWeave compared with baselines.",
                "position": 1857
            },
            {
                "img": "https://arxiv.org/html/2602.14941/x10.png",
                "caption": "Figure 10:Context-conditioned camera control results of AnchorWeave compared with baselines.",
                "position": 1861
            },
            {
                "img": "https://arxiv.org/html/2602.14941/x11.png",
                "caption": "Figure 11:Context-conditioned camera control results of AnchorWeave compared with baselines.",
                "position": 1865
            },
            {
                "img": "https://arxiv.org/html/2602.14941/x12.png",
                "caption": "Figure 12:Long-horizon video generation on both open-domain static and dynamic environments.",
                "position": 1869
            }
        ]
    },
    {
        "header": "Appendix FLong-Horizon Examples",
        "images": []
    }
]