[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.07783/x1.png",
                "caption": "Figure 1:Different multi-resolution designs in visual perception and multimodal understanding.(a)(e)Plain network without multi-scale features.(b)(c)(f)Inefficient image pyramid networks using equivalently large models for all scales, either with shared weights or with separate weights and interactions.(d)Parameter-direct image pyramid network which processes high-resolution images with large models, leading to high computational cost.(g)Multi-resolution approaches on multimodal tasks based on grid partition.(h)Our efficient and effective parameter-inverted image pyramid network (PIIP), which pairs models of increasing parameter sizes inversely with images of decreasing resolution. It achieves better performance with much lower computational cost.",
                "position": 118
            }
        ]
    },
    {
        "header": "IIRelated Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.07783/x2.png",
                "caption": "Figure 2:Overall architecture of PIIP.We use multi-resolution branches to process images of different resolutions, where larger images are handled by smaller models. Each branch leverages pretrained ViTs or CNNs. Interaction units build connections between adjacent branches. Branch merging is inserted after all the blocks or within certain intermediate blocks to combine the features of all branches.",
                "position": 241
            },
            {
                "img": "https://arxiv.org/html/2501.07783/x3.png",
                "caption": "Figure 3:Illustration of PIIP-LLaVA for multimodal understanding.We use one projector after each branch to align the visual features with the language embedding space of the LLM, and combine the features to obtain the visual features.",
                "position": 245
            }
        ]
    },
    {
        "header": "IIIMethodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.07783/x4.png",
                "caption": "Figure 4:Detailed structure of the interaction unit.It consists of two deformable attentions with fully-connect layers and feed-forward networks.",
                "position": 279
            },
            {
                "img": "https://arxiv.org/html/2501.07783/x5.png",
                "caption": "Figure 5:Detailed design of branch merging in different tasks.For detection, segmentation and multimodal understanding, output features from all branches are fused together with projection and upsampling, and fed into the subsequent FPN or LLM. For classification, we employ the original classification heads to compute logits, and average them as the final prediction.",
                "position": 282
            },
            {
                "img": "https://arxiv.org/html/2501.07783/extracted/6128466/figures/scatter/baseline_box_v6.png",
                "caption": "(a)Object detection",
                "position": 522
            },
            {
                "img": "https://arxiv.org/html/2501.07783/extracted/6128466/figures/scatter/baseline_box_v6.png",
                "caption": "(a)Object detection",
                "position": 525
            },
            {
                "img": "https://arxiv.org/html/2501.07783/extracted/6128466/figures/scatter/baseline_mask_v5.png",
                "caption": "(b)Instance segmentation",
                "position": 533
            }
        ]
    },
    {
        "header": "IVExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.07783/extracted/6128466/figures/scatter/model_variant_v3.png",
                "caption": "(c)Variants with different resolutions",
                "position": 2688
            },
            {
                "img": "https://arxiv.org/html/2501.07783/extracted/6128466/figures/scatter/model_variant_v3.png",
                "caption": "(c)Variants with different resolutions",
                "position": 2691
            },
            {
                "img": "https://arxiv.org/html/2501.07783/extracted/6128466/figures/scatter/interaction_num_v4.png",
                "caption": "(d)Number of interactions",
                "position": 2699
            },
            {
                "img": "https://arxiv.org/html/2501.07783/extracted/6128466/figures/interaction_types/unbidirect_skip_layer.png",
                "caption": "TABLE XVII:Ablation on interaction directionswith PIIP-TSB under resolution 1120/896/448.",
                "position": 2787
            },
            {
                "img": "https://arxiv.org/html/2501.07783/extracted/6128466/figures/interaction_types/unbidirect_cycle_3to1to3.png",
                "caption": "",
                "position": 2800
            },
            {
                "img": "https://arxiv.org/html/2501.07783/extracted/6128466/figures/interaction_types/unbidirect_cycle_1to3to1.png",
                "caption": "",
                "position": 2805
            },
            {
                "img": "https://arxiv.org/html/2501.07783/extracted/6128466/figures/interaction_types/bidirect.png",
                "caption": "",
                "position": 2810
            },
            {
                "img": "https://arxiv.org/html/2501.07783/extracted/6128466/figures/interaction_types/bidirect_cycle.png",
                "caption": "",
                "position": 2815
            },
            {
                "img": "https://arxiv.org/html/2501.07783/extracted/6128466/figures/interaction_types/inter_type_v4.png",
                "caption": "Figure 8:Performance of different interaction directions.",
                "position": 2846
            },
            {
                "img": "https://arxiv.org/html/2501.07783/x6.png",
                "caption": "Figure 9:Attention map visualization and Fourier spectrum of feature maps from two branches.Branch 1 with higher resolution focuses on the semantics-rich objects of low-frequency components. Branch 2 with lower resolution highlights localization like edges and details of high-frequency components.",
                "position": 2849
            }
        ]
    },
    {
        "header": "VConclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.07783/x7.png",
                "caption": "Figure 10:Qualitative results on object detection and instance segmentation.Please zoom in to view detailed bounding boxes and masks. High-resolution processing capability enables PIIP to accurately detect small objects in the images.",
                "position": 2867
            },
            {
                "img": "https://arxiv.org/html/2501.07783/x8.png",
                "caption": "Figure 11:Qualitative results of PIIP-LLaVA on multimodal understanding.Red denotes incorrect answers and green denotes correct ones. PIIP-LLaVA is capable of tackling fine-grained vision-language tasks like counting, text recognition and visual information extraction.",
                "position": 2872
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]