[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21188/x1.png",
                "caption": "Figure 1:Model-task alignment, which is measured by pass@k accuracy on the evaluated task, drives distinct outcomes from the same series of RL approaches.",
                "position": 113
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2On Unique Phenomena of RL training in LLM Reasoning",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21188/x2.png",
                "caption": "Figure 2:Pass@k for different tasks. Different LLMs have significantly different abilities on different tasks, which will affect how the RL techniques perform across model-task combinations.",
                "position": 199
            }
        ]
    },
    {
        "header": "3Experimental Setup",
        "images": []
    },
    {
        "header": "4RQ1 – Reward Signal: How Critical Is It?",
        "images": []
    },
    {
        "header": "5RQ2 – Is One-shot Enough for RL to Work?",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21188/x3.png",
                "caption": "Figure 3:The changes in two models’ accuracy during the training. If the initial rollout accuracy is non-zero, both models rapidly fit the employed samples (ls​i​m​p​l​e,lm​i​dl_{simple},l_{mid}) and exhibit generalization within the same subtask; however, we observe no generalization to puzzles of other types.",
                "position": 996
            }
        ]
    },
    {
        "header": "6RQ3 — Does RL Work with Only Negative Samples?",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21188/x4.png",
                "caption": "Figure 4:Entropy Dynamics of Qwen2.5-7B during Training. NSR can maintain the exploration space of reinforcement learning, but a larger exploration space is not always favorable, as in logical tasks.",
                "position": 1186
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BMore Pass@k Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21188/x5.png",
                "caption": "Figure 5:Pass@k for math tasks. Qwen demonstrates strong capabilities across all three mathematical evaluation datasets.",
                "position": 1680
            },
            {
                "img": "https://arxiv.org/html/2508.21188/x6.png",
                "caption": "Figure 6:Pass@k for KOR-Bench. Both models demonstrate strong inherent reasoning capabilities in Operation and Counterfactual subtasks, but exhibit limited inherent logical reasoning abilities in Cipher, Puzzle and Logic.",
                "position": 1683
            }
        ]
    },
    {
        "header": "Appendix CContamination Evaluation",
        "images": []
    },
    {
        "header": "Appendix DMore Discussion about Difficult Example in One-shot RL",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21188/x7.png",
                "caption": "Figure 7:Training Dynamics of Qwen2.5-7B when trained withls​e​l​e​c​t​e​dl_{selected}. Entropy and response length exhibit almost no changes.",
                "position": 1984
            }
        ]
    },
    {
        "header": "Appendix EFew-shot RL Example Details",
        "images": []
    }
]