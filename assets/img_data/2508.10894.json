[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.10894/imgs/Maestro_Overview.png",
                "caption": "",
                "position": 195
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.10894/imgs/Maestro_Fusion.png",
                "caption": "Figure 2:Token-based fusion modes for handling multimodality and multitemporality.Modessharedandmonotempinvolve late fusion across modalities and time steps, with parameters either shared across modalities (shared) or kept independent for each modality (monotemp). Modegroupinvolves late fusion across predefined groups of modalities but early fusion across time steps and within each group. Modeinter-groupextendsgroupby replacing the final encoder blocks with fusion blocks that enable cross-group token interactions. Modemodis a special case ofgroupwith late fusion across all modalities, but early fusion across time steps.",
                "position": 341
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.10894/imgs/Fusion_comparison.png",
                "caption": "Figure 3:Comparison of different multimodal and multitemporal fusion modes for MAEs, ViTs, and baseline FMs. We report the weighted F1 score (%) on TreeSatAI-TS and the mIoU (%) on PASTIS-HD and FLAIR-HUB 20%. Refer to SMTab.˜11for exact numbers and additional results with CROMA.",
                "position": 645
            },
            {
                "img": "https://arxiv.org/html/2508.10894/imgs/Multispectral_comparison.png",
                "caption": "Figure 4:Comparison of different choices of multispectral fusion and target normalization for MAE-B models.We report the weighted F1 score (%) on TreeSatAI-TS and the mIoU (%) on PASTIS-HD, with varying pre-training dataset fractions.\nFor each dataset and choice of multispectral fusion, we also indicate the pre-training cost in GFLOPs per forward pass (single batch element). Refer to SMTab.˜12andTab.˜20for exact numbers.",
                "position": 675
            },
            {
                "img": "https://arxiv.org/html/2508.10894/imgs/Scaling.png",
                "caption": "Figure 5:Scaling of MAESTRO-B and ViT-B models with different pre-training/fine-tuning dataset fractions.We report the weighted F1 score (%) on TreeSatAI-TS and the mIoU (%) on PASTIS-HD and FLAIR-HUB for three finetuning dataset fractions: 5%, 20%, and 100%. For each finetuning fraction, we compare three pre-training settings: pre-training on 100% of the data, pre-training on the same fraction as fine-tuning, and no pre-training. Refer to SMTab.˜14for exact numbers.",
                "position": 910
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Experimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.10894/imgs/Histos_treesat.png",
                "caption": "(a)TreeSatAI-TS",
                "position": 2955
            },
            {
                "img": "https://arxiv.org/html/2508.10894/imgs/Histos_treesat.png",
                "caption": "(a)TreeSatAI-TS",
                "position": 2958
            },
            {
                "img": "https://arxiv.org/html/2508.10894/imgs/Histos_pastis.png",
                "caption": "(b)PASTIS-HD",
                "position": 2964
            },
            {
                "img": "https://arxiv.org/html/2508.10894/imgs/Histos_flair.png",
                "caption": "(c)FLAIR-HUB",
                "position": 2970
            }
        ]
    },
    {
        "header": "7Additional Results",
        "images": []
    },
    {
        "header": "8Inference Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.10894/imgs/Maestro_inferences_pastis.png",
                "caption": "Figure 7:Inference results from MAESTRO-B and ViT-B models on PASTIS-HD.For Sentinel-2 imagery, we report the pixel-wise median across temporal bins. White parcels correspond to areas with missing annotations (void labels).",
                "position": 5698
            },
            {
                "img": "https://arxiv.org/html/2508.10894/imgs/Maestro_inferences_flair.png",
                "caption": "Figure 8:Inference results from MAESTRO-B and ViT-B models on FLAIR-HUB.For Sentinel-2 imagery, we report the pixel-wise median across temporal bins.",
                "position": 5701
            }
        ]
    },
    {
        "header": "9Computational Costs",
        "images": []
    }
]