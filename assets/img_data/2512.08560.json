[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08560/x1.png",
                "caption": "Figure 1:BrainExplore: Discovering Interpretable Visual Representations in the Human Brain.(a)Per–region fMRI decomposition learns component patterns such that any response is approximated as a linear combination of these patterns.(b)For each pattern, we retrieve its top-NNactivating fMRI responses and corresponding images, then automatically select the best matching concept and assign an alignment score. We then use these scores to identify the most interpretable patterns and their explanations.(c)Examples of discovered interpretable patterns across regions, showing pattern activations projected onto cortex, top activating images, and their textual explanations.",
                "position": 164
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08560/x2.png",
                "caption": "Figure 2:BrainExplore Framework.(a)Decompose:per-region fMRI decomposition to discover interpretable patterns (Sec.3.2).(b)Visualize & Explain:retrieve the top activating images for each pattern and interpret its semantics (Sec.3.3).(c)Upscale:scale to an unlimited number of patterns by building a brain-inspired dictionary and labeling each image with respect to each hypothesis (Sec.3.4).(d)Discover:score each pattern–hypothesis alignment, enabling systematic discovery of the most interpretable patterns and the pattern that best explains any given hypothesis (Sec.3.5).",
                "position": 268
            }
        ]
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08560/x3.png",
                "caption": "Figure 3:Discovered Interpretable Patterns (EBA and V4).We show patterns of subject 1 with their top activating images and selected explanations.\nEBA is known to process bodies and actions; V4 is known to encode mid-level features (e.g., color, shape).",
                "position": 301
            },
            {
                "img": "https://arxiv.org/html/2512.08560/x4.png",
                "caption": "Figure 4:Discovered Interpretable Patterns (OPA and PPA).We show Subject 1 patterns with top activating images and selected explanations.\nOPA is known to process scene layout and navigability; PPA encodes scenes and places (e.g., indoor/outdoor, landmarks).",
                "position": 341
            },
            {
                "img": "https://arxiv.org/html/2512.08560/x5.png",
                "caption": "Figure 5:Concepts best explained by each ROI (EBA, PPA).Each concept is assigned to a single ROI—the one with the highest alignment score.\nOnly concepts with alignment>0.5>0.5are shown; word size reflects the alignment score within the assigned ROI.",
                "position": 376
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix AAblation & Analysis",
        "images": []
    },
    {
        "header": "Appendix BAdditional Details",
        "images": []
    },
    {
        "header": "Appendix CAdditional Quantitative Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08560/x6.png",
                "caption": "Figure S1:Brain activation map (SAE vs ICA). We show different activation patterns derived from a Sparse Autoencoder (SAE) and Independent Component Analysis (ICA) for different ROIs. SAE patterns are notably more compact and clustered and spatially localized, despite both receiving only 1D voxel vectors and no spatial information.",
                "position": 1177
            },
            {
                "img": "https://arxiv.org/html/2512.08560/x7.png",
                "caption": "Figure S2:Discovered Interpretable Patterns (EBA and RSC). We show additional patterns for Subject 1 with top activating images and selected explanations. EBA is known to encode bodies and actions, whereas RSC processes scene information.",
                "position": 1216
            },
            {
                "img": "https://arxiv.org/html/2512.08560/x8.png",
                "caption": "Figure S3:Discovered Interpretable Patterns (OPA). We show additional patterns for Subject 1 with top activating images and selected explanations. OPA is known to process scene layout and navigability.",
                "position": 1219
            },
            {
                "img": "https://arxiv.org/html/2512.08560/x9.png",
                "caption": "Figure S4:Discovered Interpretable Patterns (FBA-1 and FBA-2). We show additional patterns for Subject 1 with top activating images and selected explanations. FBA is a body-selective area, encompassing two sub-areas shown here, FBA-1 and FBA-2.",
                "position": 1222
            },
            {
                "img": "https://arxiv.org/html/2512.08560/x10.png",
                "caption": "Figure S5:Discovered Interpretable Patterns of Subject 2 (OPA and VWFA-1). We show patterns for Subject 2 with top activating images and selected explanations. OPA is known to process scene layout and navigability, and VWFA is involved in processing word shapes and visual text.",
                "position": 1225
            },
            {
                "img": "https://arxiv.org/html/2512.08560/x11.png",
                "caption": "Figure S6:Discovered Interpretable Patterns of Subject 2 (EBA). We show patterns for subject 2 with top activating images and selected explanations. EBA is known to encode bodies and actions.",
                "position": 1228
            },
            {
                "img": "https://arxiv.org/html/2512.08560/x12.png",
                "caption": "Figure S7:Discovered Interpretable Patterns of Subject 5 (OPA and FFA-1). We show patterns for subject 5 with top activating images and selected explanations. OPA is known to process scene layout and navigability, and FFA is primarily known for face processing.",
                "position": 1231
            },
            {
                "img": "https://arxiv.org/html/2512.08560/x13.png",
                "caption": "Figure S8:Discovered Interpretable Patterns of Subject 5 (EBA and hV4). We show patterns for subject 5 with top activating images and selected explanations. EBA is known to encode bodies and actions and V4 is known to encode mid-level features (e.g., color, shape).",
                "position": 1234
            },
            {
                "img": "https://arxiv.org/html/2512.08560/x14.png",
                "caption": "Figure S9:Image grid EBA (Part 1).Full visual grid with top 16 activating images for conceptsTied neckwear,Brushing teeth,Hands, andJumpingfound in the EBA region. For each concept, the top 8 images corresponding toMeasuredandPredictedfMRI responses are shown.",
                "position": 1237
            },
            {
                "img": "https://arxiv.org/html/2512.08560/x15.png",
                "caption": "Figure S10:Image grid EBA (Part 2).Full visual grid with top 16 activating images for conceptsFrisbee,Soccer,Surfing, andTennisfound in the EBA region. For each concept, the top 8 images corresponding toMeasuredandPredictedfMRI responses are shown.",
                "position": 1240
            },
            {
                "img": "https://arxiv.org/html/2512.08560/x16.png",
                "caption": "Figure S11:Image grid PPA (Part 1).Full visual grid with top 16 activating images for conceptsStone building,Commercial buildings,Kitchen, andIndoorfound in the PPA region. For each concept, the top 8 images corresponding toMeasuredandPredictedfMRI responses are shown.",
                "position": 1243
            },
            {
                "img": "https://arxiv.org/html/2512.08560/x17.png",
                "caption": "Figure S12:Image grid PPA (Part 2).Full visual grid with top 16 activating images for conceptsCollage,Screen,Healthy food, andLandscapefound in the PPA region. For each concept, the top 8 images corresponding toMeasuredandPredictedfMRI responses are shown.",
                "position": 1246
            },
            {
                "img": "https://arxiv.org/html/2512.08560/x18.png",
                "caption": "Figure S13:Concepts best explained by every ROI (Non-Exlusive). Concepts represented in each Region of Interest (ROI) that achieve an alignment score>0.5>0.5. A concept may appear in multiple ROI word clouds, reflecting its representation across different brain regions. Word size reflects the alignment score of the concept within the assigned ROI.",
                "position": 1249
            },
            {
                "img": "https://arxiv.org/html/2512.08560/x19.png",
                "caption": "Figure S14:Concepts best explained by every ROI (Exlusive). Each concept is assigned to a single ROI—the one with the highest alignment score. Only concepts with alignment>0.5>0.5are shown. Word size reflects the alignment score within the assigned ROI.",
                "position": 1252
            },
            {
                "img": "https://arxiv.org/html/2512.08560/x20.png",
                "caption": "Figure S15:Image Captioning Prompt.",
                "position": 1255
            },
            {
                "img": "https://arxiv.org/html/2512.08560/x21.png",
                "caption": "Figure S16:Hypothesis Generation Prompt.Given a set of 10 images and their detailed captions, an LLM is instructed to identify what is common across the images and to generate hypotheses that may explain what is shared among them.",
                "position": 1258
            },
            {
                "img": "https://arxiv.org/html/2512.08560/x22.png",
                "caption": "Figure S17:Image–Hypothesis Labeling Prompt.For each image–hypothesis pair, we label whether the hypothesis is supported by the image or not.",
                "position": 1262
            },
            {
                "img": "https://arxiv.org/html/2512.08560/x23.png",
                "caption": "Figure S18:Images with per-hypothesis labeling.We show six example images and list all hypotheses labeled as positive for each.\nThe hypotheses are taken from our brain-inspired hypothesis dictionary, and the images are drawn from the set used with predicted fMRI.",
                "position": 1266
            }
        ]
    },
    {
        "header": "Appendix DAdditional Visualizations",
        "images": []
    }
]