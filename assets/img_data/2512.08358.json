[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08358/x1.png",
                "caption": "Figure 1:TrackingWorldestimates world-centric dense tracking results from monocular videos. Our model can accurately estimate camera poses and achieve disentangled 3D track modeling of static and dynamic components, not just limited to one foreground dynamic object. We only visualize a subset of foreground dynamic point trajectories and apply a fading color to background static points.",
                "position": 177
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08358/x2.png",
                "caption": "Figure 2:Overview.\nGiven a video sequence, TrackingWorld first generates dense 2D tracking results that are capable of capturing newly emerging objects in the scene. These 2D trajectories are then fed into an optimization-based framework to transform them into a world-centric 3D space. Specifically, we begin by estimating the initial camera poses for each frame at the clip level. We then perform dynamic background refinement to exclude potentially dynamic regions and refine the camera poses. Based on the optimized poses, we finally reconstruct the trajectories of all dynamic regions.",
                "position": 231
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08358/x3.png",
                "caption": "Figure 3:Qualitative results on DAVIS dataset.Our method can output both reliable camera trajectories and world centric dense tracking. The second row visualizes 3D tracking results on temporally spaced keyframes, while the third row shows complete tracks across continuous frames.",
                "position": 902
            },
            {
                "img": "https://arxiv.org/html/2512.08358/x4.png",
                "caption": "",
                "position": 910
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08358/x5.png",
                "caption": "Figure 6:Effectiveness of the speeding-up strategy.",
                "position": 1331
            },
            {
                "img": "https://arxiv.org/html/2512.08358/x6.png",
                "caption": "Figure 7:Camera pose estimation comparisonon the Sintelbutler2012naturalisticBonnpalazzolo2019refusionand TUM-Dsturm2012benchmarkdatasets.",
                "position": 1748
            },
            {
                "img": "https://arxiv.org/html/2512.08358/x7.png",
                "caption": "Figure 8:More Qualitative results.Our method can output 3D tracks in a world-centric coordinate system.",
                "position": 1759
            }
        ]
    },
    {
        "header": "Appendix AAppendix / supplemental material",
        "images": []
    }
]