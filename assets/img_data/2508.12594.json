[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.12594/x1.png",
                "caption": "Figure 1:Schematic of a FLARE block.\nIn FLARE,\neach head projects the input sequence withNNtokens to a fixed-length sequence ofMMtokens via the cross-attention matrixWencode=softmax​(Q⋅KT)W_{\\text{encode}}=\\mathrm{softmax}(Q\\cdot K^{T}), and then projects back to the original length via the cross-attention matrixWdecode=softmax​(K⋅QT)W_{\\text{decode}}=\\mathrm{softmax}(K\\cdot Q^{T}).\nThe overall operation is equivalent to token mixing on the input sequence with the rank-deficient matrix(Wdecode⋅Wencode)\\left(W_{\\text{decode}}\\cdot W_{\\text{encode}}\\right).",
                "position": 306
            }
        ]
    },
    {
        "header": "2Related work",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.12594/x2.png",
                "caption": "Figure 2:Time and memory requirements of different attention schemes.\nOn an input sequence of one million tokens, FLARE (red) is over200×200\\timesfaster than vanilla attention, while consuming marginally more memory.\nAll models are implemented with flash attention(Dao et al.,2022), and\nthe memory upper bound on a single H100 80GB GPU is depicted with a dashed line.\nNote that the curves for FLARE are somewhat overlapping.",
                "position": 385
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.12594/x3.png",
                "caption": "Figure 4:We train FLARE on the DrivAerML dataset(Ashton et al.,2024)with one million points per geometry on a single Nvidia H100 80GB GPU.\nWe present (left) the test relative error, (middle) time per epoch (s), and\n(right) peak memory utilization (GB) as a function of the number of FLARE blocks (BB) for different number of latent tokens (MM).",
                "position": 1164
            },
            {
                "img": "https://arxiv.org/html/2508.12594/x4.png",
                "caption": "Figure 5:(Left) effect of the number of residual layers in key/ value projection, and (right) of residual layers in residual block on test accuracy.\nIn both cases, deeper networks lead to greater accuracy.",
                "position": 1187
            },
            {
                "img": "https://arxiv.org/html/2508.12594/x5.png",
                "caption": "Figure 6:(Left) Effect of head dimension (DD) on test accuracy.\nFLARE is designed to work optimally forD=4D=4–88.\n(Right) Effect of number of blocks (BB) on test accuracy.\nBoth experiments are conducted on the elasticity benchmark problem withC=64C=64feature dimension.",
                "position": 1255
            },
            {
                "img": "https://arxiv.org/html/2508.12594/x6.png",
                "caption": "",
                "position": 1258
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AArchitecture details",
        "images": []
    },
    {
        "header": "Appendix BSpectral analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.12594/x7.png",
                "caption": "Figure 8:M=64M=64nonzero eigenvalues ofWhW_{h}(Eq. 9) forH=8H=8heads in different blocks of FLARE withB=8B=8blocksC=64C=64features trained on the elasticity dataset (972972points per input).\nThe number of latents (MM) is depicted with a dashed red line,\nand FP32 precision with a dashed black line.",
                "position": 2205
            }
        ]
    },
    {
        "header": "Appendix CBenchmarking and Comparison",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.12594/x8.png",
                "caption": "Figure 9:Execution times in FP32 for a single vanilla self-attention layer, a physics attention layer, and FLAME.\nThe models are set to have approximately the same number of parameters as inSection 4.1.\nThis calculation is performed on a single H100 80GB GPU.\nNote that the curves for FLARE are somewhat overlapping.",
                "position": 2642
            }
        ]
    },
    {
        "header": "Appendix DField-prediction on million-point geometries",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.12594/x9.png",
                "caption": "Figure 10:We simulate the LPBF process on selected geometries from the Autodesk segementation dataset(Lambourne et al.,2021)to generate a benchmark dataset for AM calculations.\nSeveral geometries are presented in this gallery.\nThe color indicatesZZ(vertical) displacement field.",
                "position": 2664
            },
            {
                "img": "https://arxiv.org/html/2508.12594/x10.png",
                "caption": "Figure 11:Summary of LPBF dataset statistics.",
                "position": 2706
            }
        ]
    },
    {
        "header": "Appendix EBenchmark dataset of additive manufacturing simulations",
        "images": []
    }
]