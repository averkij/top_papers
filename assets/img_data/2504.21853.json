[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.21853/x1.png",
                "caption": "Figure 1:Evolutionary tree of Interactive Generative Video (IGV) models from 2020 to 2025. This diagram categorizes the development of IGV research from three major application domains: Game Simulation, Embodied AI, and Autonomous Driving, each represented by a colored trunk.",
                "position": 178
            }
        ]
    },
    {
        "header": "2Preliminaries: Video Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.21853/extracted/6402074/images/preliminary.png",
                "caption": "Figure 2:Overview figure for GAN, VAE, Diffusion, and Autogression methods.",
                "position": 229
            },
            {
                "img": "https://arxiv.org/html/2504.21853/x2.png",
                "caption": "Figure 3:Proposed framework of Interactive Generative Video (IGV).\nThis figure illustrates the IGV system, which serves as a bridge between the real world and a virtual environment. In the real world, various roles such as players, designers, artists, and intelligent agents (e.g., robots, vehicles) interact with the IGV system through actions, instructions, and visual inputs. These diverse interactions naturally lead to applications in their respective domains: players engage with gaming applications, robots utilize embodied AI simulations, and vehicles operate in autonomous driving scenarios.\nThe IGV system generates video outputs through five interconnected modules: the Generation module serves as the foundation for video synthesis; the Control module enables precise manipulation of video content; the Memory module maintains content consistency; the Dynamics module ensures physical plausibility; and the Intelligence module represents higher-level capabilities like causal reasoning. These modules work in concert to create and manage immersive virtual experiences through video generation.",
                "position": 362
            }
        ]
    },
    {
        "header": "3IGV System",
        "images": []
    },
    {
        "header": "4Application#1: IGV for Generative Video Game",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.21853/extracted/6402074/sec4_img/gamegan.png",
                "caption": "Figure 4:Overview figure of GameGAN[27]. GameGAN is composed of three main modules. The dynamics engine (which refers to the internal mechanism that captures and updates the gameâ€™s state transitions over time, simulating how the game world evolves in response to inputs[27]) is implemented as an RNN and contains the world state updated at each time t. Optionally, it can write to and read from the external memory module. Finally, the rendering engine is used to decode the output image.",
                "position": 554
            },
            {
                "img": "https://arxiv.org/html/2504.21853/extracted/6402074/sec4_img/gamegenx.png",
                "caption": "Figure 5:Open-domain generation showcases from[20].\nGameGen-X enables high-fidelity and diverse generation of open-domain video game scenes, supporting various styles, characters, and virtual environments with cinematic quality.",
                "position": 803
            },
            {
                "img": "https://arxiv.org/html/2504.21853/extracted/6402074/sec4_img/genie2.png",
                "caption": "Figure 6:Overview figure of Genie2[25].\nGenie2 is an autoregressive latent diffusion model.\nAt training time, latent frames are passed to a large transformer dynamics model, trained with a causal mask similar to that used by large language models.\nAt inference time, Genie2 can be sampled in an autoregressive fashion, taking individual actions and past latent frames on a frame-by-frame basis.",
                "position": 850
            }
        ]
    },
    {
        "header": "5Application#2: IGV for Embodied AI",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.21853/x3.png",
                "caption": "Figure 7:Overview of three applications of IGV in embodied intelligence.Top-left:VLP[36]demonstrates the use of IGV for embodied planning.Top-right:UniPi[38]illustrates how IGV can serve as a generalizable robot policy.Bottom:UniSim[39]showcases the potential of IGV as a world simulator.",
                "position": 929
            }
        ]
    },
    {
        "header": "6Application#3: IGV for Autonomous Driving",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.21853/x4.png",
                "caption": "Figure 8:Overview of two applications of IGV in autonomous driving.Top:Vista[60]demonstrates the potential of using IGV as a simulator for autonomous driving.Bottom:ADriver-I[53]illustrates the use of IGV as an interpretable vehicle controller.",
                "position": 1263
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]