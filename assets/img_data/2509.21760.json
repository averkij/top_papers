[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21760/x1.png",
                "caption": "Figure 1:LVM[1]vs. UniVid.(a) LVM[1]requires large-scale, modality- and source-specific paired data for pre-training to support diverse vision tasks. In contrast, UniVid explores whether a pre-trained video generation model can be efficiently adapted to a broad range of vision tasks via lightweight SFT with minimal paired data.\n(b) At inference, LVM[1]is limited to uni-modal visual contexts, whereas UniVid enables a unified framework that accommodates both cross-modal and cross-source vision tasks.\nStacked blocks represent videos; a single block represents an image.",
                "position": 81
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21760/x2.png",
                "caption": "Figure 2:The framework of UniVid.",
                "position": 330
            },
            {
                "img": "https://arxiv.org/html/2509.21760/x3.png",
                "caption": "Figure 3:Main observations.The top colored row serves as a legend indicating the modality and role of each clip shown below.\nThe following figure follows the same format.\n(a) The model infers the correct output modality from cross-modal contexts.\n(b) Despite being pre-trained solely on natural video data, it generalizes to cross-source understanding tasks.\n(c) Under the UniVid framework, understanding and generation tasks are unified and can be converted by reordering the visual sentence.",
                "position": 334
            },
            {
                "img": "https://arxiv.org/html/2509.21760/x4.png",
                "caption": "Figure 4:Performance across diverse vision tasks and context formats.We show results for scribble map transfer, motion transfer, and salient object tracking under various visual contexts.\nEach task is fine-tuned independently within each context configuration, demonstrating that the pre-trained video generation model adapts well across all applicable settings listed in Table1.\nWith a fixed example pair, outputs change with the query, reflecting context-based inference.",
                "position": 343
            },
            {
                "img": "https://arxiv.org/html/2509.21760/x5.png",
                "caption": "Figure 5:Unified understanding and generation tasks.Our proposed UniVid allows flexible switching between understanding and generation tasks by simply reordering visual sentences.",
                "position": 381
            },
            {
                "img": "https://arxiv.org/html/2509.21760/x6.png",
                "caption": "Figure 6:Results of mixed fine-tuning strategy.(a) When fine-tuned on visual sequences spanning all context types, the model can dynamically infer the output modality at inference time based on the context(A,A′,B)(A,A^{\\prime},B).\n(b) We co-train the model on all vision tasks, each covering all applicable contexts.\nUnder this mixed setup, the model consistently performs well across all tasks and contexts, demonstrating strong generalization.\nAdditional results are shown in the appendix.",
                "position": 411
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BComparison between LVM and video generation model",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21760/x7.png",
                "caption": "Figure 8:Results of conditional generation tasks.",
                "position": 2123
            },
            {
                "img": "https://arxiv.org/html/2509.21760/x8.png",
                "caption": "Figure 9:Impact of texts under contexts III.",
                "position": 2127
            },
            {
                "img": "https://arxiv.org/html/2509.21760/x9.png",
                "caption": "Figure 10:Additional results of various vision tasks across context types.",
                "position": 2166
            },
            {
                "img": "https://arxiv.org/html/2509.21760/x10.png",
                "caption": "Figure 11:Performance under separate and mixed training for context I.",
                "position": 2170
            },
            {
                "img": "https://arxiv.org/html/2509.21760/x11.png",
                "caption": "Figure 12:Performance under separate and mixed training for context II.",
                "position": 2174
            },
            {
                "img": "https://arxiv.org/html/2509.21760/x12.png",
                "caption": "Figure 13:Performance under separate and mixed training for context III.",
                "position": 2178
            },
            {
                "img": "https://arxiv.org/html/2509.21760/x13.png",
                "caption": "Figure 14:Impact of texts under contexts I.",
                "position": 2182
            },
            {
                "img": "https://arxiv.org/html/2509.21760/x14.png",
                "caption": "Figure 15:Impact of texts under contexts II.",
                "position": 2186
            },
            {
                "img": "https://arxiv.org/html/2509.21760/x15.png",
                "caption": "Figure 16:Impact of texts under contexts IV.",
                "position": 2190
            },
            {
                "img": "https://arxiv.org/html/2509.21760/x16.png",
                "caption": "Figure 17:Co-training with all vision tasks and contexts.",
                "position": 2194
            }
        ]
    },
    {
        "header": "Appendix CAdditional Experimental Results",
        "images": []
    }
]