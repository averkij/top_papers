[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.09834/x1.png",
                "caption": "",
                "position": 158
            },
            {
                "img": "https://arxiv.org/html/2508.09834/x2.png",
                "caption": "Figure 1:Overview of Efficient Architectures for Large Language Models.",
                "position": 160
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.09834/x3.png",
                "caption": "Figure 2:Long Context Patterns. We provide representative examples of long-context usage patterns across various scenarios, including retrieval-augmented generation (RAG), agentic, reasoning, and multimodal applications.",
                "position": 205
            }
        ]
    },
    {
        "header": "2Linear Sequence Modeling",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.09834/x4.png",
                "caption": "Figure 4:Linear Sequence Modeling Methods and Their Connections. The formulations of linear attention, linear RNNs, state space models, and test-time training RNNs have gradually converged toward a unified representation. Moreover, softmax attention can also be transformed into a linear sequence modeling form through the linearization techniques.",
                "position": 643
            },
            {
                "img": "https://arxiv.org/html/2508.09834/x5.png",
                "caption": "Figure 5:Mechanism Comparison of Finetuning-based and Distillation-based Linearization Procedures.",
                "position": 1165
            },
            {
                "img": "https://arxiv.org/html/2508.09834/x6.png",
                "caption": "Figure 6:Hardware-efficient Implementation Algorithms for Linear Sequence Modeling. (a) In the Blelloch parallel scan, the upward sweep computes and stores the outputs at even indices; the downward sweep then reuses these stored values to compute the outputs at odd indices.\n(b) To better exploit tensor-core matmul acceleration during training and prefilling, linear-recurrence models can adopt intra-block parallel computation combined with inter-block recursion.\n(c) During inference, the recurrent formulation supports decoding with constant memory andùí™‚Äã(1)\\mathcal{O}(1)per-step computational complexity.",
                "position": 1190
            }
        ]
    },
    {
        "header": "3Sparse Sequence Modeling",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.09834/x7.png",
                "caption": "Figure 7:Example Patterns of Static Sparse Attention.ETC and BigBird are representative examples of mixed static sparse attention mechanisms, which combine multiple fixed sparsity patterns within a single attention framework.",
                "position": 1219
            }
        ]
    },
    {
        "header": "4Efficient Full Attention",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.09834/x8.png",
                "caption": "Figure 8:Attention Map Computation in FlashAttention-2.",
                "position": 1405
            },
            {
                "img": "https://arxiv.org/html/2508.09834/x9.png",
                "caption": "Figure 9:Mechanism Comparison of Primary Grouped Attention Methods.",
                "position": 1459
            }
        ]
    },
    {
        "header": "5Sparse Mixture-of-Experts",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.09834/x10.png",
                "caption": "Figure 10:MoE Routing Strategies.",
                "position": 1586
            },
            {
                "img": "https://arxiv.org/html/2508.09834/x11.png",
                "caption": "Figure 11:MoE Expert Architectures.",
                "position": 1695
            },
            {
                "img": "https://arxiv.org/html/2508.09834/x12.png",
                "caption": "Figure 12:MoE Conversion Strategies.",
                "position": 1744
            }
        ]
    },
    {
        "header": "6Hybrid Architectures",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.09834/x13.png",
                "caption": "Figure 13:Hybrid Model Architectures. (a) illustrates the classical paradigm of inter-layer hybrid approach. (b) demonstrates the classical paradigm of intra-layer hybrid approach. The left side of (b) represents a pattern similar to Hymba[191], which employs head-wise partitioning with either softmax attention or linear attention. The right side, depicts a pattern analogous to LoLCATs[114], featuring sequence-wise partitioning where local regions utilize softmax attention while distant regions employ linear attention.",
                "position": 1789
            }
        ]
    },
    {
        "header": "7Diffusion Large Language Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.09834/x14.png",
                "caption": "Figure 14:Mechanism Comparison of Autoregressive Models and Diffusion LLMs.",
                "position": 1849
            }
        ]
    },
    {
        "header": "8Applications to Other Modalities",
        "images": []
    },
    {
        "header": "9Conclusion and Future Directions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]