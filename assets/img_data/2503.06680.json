[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06680/extracted/6264850/figure/intro_feabench.drawio.png",
                "caption": "Figure 1:The proposed FEA-Bench aims to evaluate incremental repository development, while SWE-benchJimenez etÂ al. (2024)focuses on repairing issues.",
                "position": 147
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Benchmark Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06680/extracted/6264850/figure/instance.drawio.png",
                "caption": "Figure 2:An example of the task instances from the FEA-Bench. During the inference of LLMs, the first two items: feature request and new components are considered as known information. The environment setup serves as a prerequisite for creating the testbed and environment. Python file patches and unit tests are used as labels and evaluation metrics and should not be leaked during the inference of LLMs.",
                "position": 230
            },
            {
                "img": "https://arxiv.org/html/2503.06680/extracted/6264850/figure/data_collect_wo_openai.drawio.png",
                "caption": "Figure 3:The data collection pipeline for the FEA-Bench. First, determine the scope of GitHub repositories from which task instances will be collected. Next, gather pull requests as task instances and apply filtering criteria to select instances that meet the purpose of adding new features. Finally, use the included test files to execute unit tests, ensuring that only task instances with reproducible test results are included in FEA-Bench. For a more detailed construction process, please refer to AppendixA.2.",
                "position": 234
            }
        ]
    },
    {
        "header": "4Experimental Design",
        "images": []
    },
    {
        "header": "5Evaluation Results",
        "images": []
    },
    {
        "header": "6Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06680/extracted/6264850/figure/class_result.png",
                "caption": "Figure 4:The resolved ratios grouped by the categories of the repositories.",
                "position": 628
            },
            {
                "img": "https://arxiv.org/html/2503.06680/extracted/6264850/figure/added_components.png",
                "caption": "Figure 5:Histogram of the number of added functions both in all task instances and resolved task instances by DeepSeek-R1 (underNaturalandDetailedprompt settings).",
                "position": 799
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADataset Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06680/x1.png",
                "caption": "Figure 6:The prompt for the inference of the task instances.",
                "position": 1383
            },
            {
                "img": "https://arxiv.org/html/2503.06680/x2.png",
                "caption": "Figure 7:The output instructions at the rear of the inference prompt.",
                "position": 1387
            }
        ]
    },
    {
        "header": "Appendix BInference",
        "images": []
    }
]