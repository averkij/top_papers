[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.03803/x1.png",
                "caption": "",
                "position": 155
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.03803/x2.png",
                "caption": "Figure 2:3D reconstruction of the shared house using Aria Multi-MPS[1], showcasing the locations of 15 Exo cameras in the common area and 2 mmWave devices (highlighted in red) on the second floor. Color-coded 10-minute participant traces are also displayed.",
                "position": 175
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.03803/x3.png",
                "caption": "Table 1:Related Work for EgoLife Dataset - Overview of Egocentric Datasets.For Modality,denotes video,denotes gaze,denotes IMU,denotes 3D scans. The EgoLife dataset stands out for its ultra-long egocentric footage and rich interpersonal interactions.",
                "position": 191
            },
            {
                "img": "https://arxiv.org/html/2503.03803/x4.png",
                "caption": "",
                "position": 192
            },
            {
                "img": "https://arxiv.org/html/2503.03803/x5.png",
                "caption": "",
                "position": 192
            },
            {
                "img": "https://arxiv.org/html/2503.03803/x7.png",
                "caption": "",
                "position": 192
            },
            {
                "img": "https://arxiv.org/html/2503.03803/x8.png",
                "caption": "",
                "position": 224
            },
            {
                "img": "https://arxiv.org/html/2503.03803/x9.png",
                "caption": "",
                "position": 238
            },
            {
                "img": "https://arxiv.org/html/2503.03803/x10.png",
                "caption": "",
                "position": 238
            },
            {
                "img": "https://arxiv.org/html/2503.03803/x11.png",
                "caption": "",
                "position": 239
            },
            {
                "img": "https://arxiv.org/html/2503.03803/x12.png",
                "caption": "",
                "position": 240
            },
            {
                "img": "https://arxiv.org/html/2503.03803/x13.png",
                "caption": "",
                "position": 255
            },
            {
                "img": "https://arxiv.org/html/2503.03803/x14.png",
                "caption": "",
                "position": 255
            },
            {
                "img": "https://arxiv.org/html/2503.03803/x15.png",
                "caption": "",
                "position": 256
            },
            {
                "img": "https://arxiv.org/html/2503.03803/x16.png",
                "caption": "",
                "position": 272
            },
            {
                "img": "https://arxiv.org/html/2503.03803/x17.png",
                "caption": "",
                "position": 272
            },
            {
                "img": "https://arxiv.org/html/2503.03803/x18.png",
                "caption": "",
                "position": 273
            },
            {
                "img": "https://arxiv.org/html/2503.03803/x19.png",
                "caption": "",
                "position": 288
            },
            {
                "img": "https://arxiv.org/html/2503.03803/x20.png",
                "caption": "",
                "position": 288
            },
            {
                "img": "https://arxiv.org/html/2503.03803/x21.png",
                "caption": "",
                "position": 289
            },
            {
                "img": "https://arxiv.org/html/2503.03803/x22.png",
                "caption": "",
                "position": 302
            },
            {
                "img": "https://arxiv.org/html/2503.03803/x23.png",
                "caption": "",
                "position": 302
            },
            {
                "img": "https://arxiv.org/html/2503.03803/x24.png",
                "caption": "",
                "position": 303
            }
        ]
    },
    {
        "header": "3The EgoLife Dataset & Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.03803/x25.png",
                "caption": "Figure 3:The Activity Timeline of the EgoLife Dataset.It visualizes the activity timeline of six participants over one week. Each block represents a 20-minute interval, color-coded and marked with icons for different activities. The legend shows 14 activity categories with their total occurrence counts. The categorization is automatically performed using GPT-4o on visual-audio captions with timestamps.",
                "position": 425
            },
            {
                "img": "https://arxiv.org/html/2503.03803/x26.png",
                "caption": "Figure 4:The Overview of Data Process Pipeline.The pipeline synchronizes multi-source data (video, audio, IMU) from Aria glasses and GoPro cameras usingEgoSynccodebase, processes them through privacy protection (EgoBlur), dense captioning (EgoCaption), and transcription (EgoTranscript) modules, ultimately feeding into theEgoLifeQAsystem.",
                "position": 429
            },
            {
                "img": "https://arxiv.org/html/2503.03803/x27.png",
                "caption": "Figure 5:Question Types and Examples in the EgoLifeQA Benchmark.We design five types of questions to evaluate egocentric assistants’ capabilities in entity logging, event recall, task tracking, and human-centric problems (habit analysis and relationship understanding). Each example includes a multiple-choice Q&A with supporting evidence from timestamps at least 5 minutes prior to the question. Black vertical lines indicate question timestamps, while colored curved lines connect to relevant evidence timestamps.",
                "position": 448
            },
            {
                "img": "https://arxiv.org/html/2503.03803/x28.png",
                "caption": "Figure 6:Statistics of EgoLifeQA.We gathered 500 long-context QAs per participant, totaling 3K QAs. The sum of QAs for each question type is reported. In the pie chart, darker segments indicate the proportion of questions requiring audio. The bar chart presents the daily count of QAs per question type, with brightness levels reflecting 4-level certification length[11](from<<<2h to>>>24h).",
                "position": 522
            },
            {
                "img": "https://arxiv.org/html/2503.03803/x29.png",
                "caption": "Figure 7:The EgoBulter Architecture.The system comprises (a) a Captioning Stage powered by EgoGPT for dense visual-audio understanding of egocentric clips, and (b) a Question Answering Stage utilizing EgoRAG for memory retrieval and response generation. The example demonstrates temporal reasoning across multiple days, with keyword extraction, evidence retrieval, and context-aware answer generation for a breakfast-related query.",
                "position": 525
            }
        ]
    },
    {
        "header": "4EgoButler: Agentic Egocentric Life Assistant",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.03803/x30.png",
                "caption": "Figure 8:Qualitative Comparison of EgoGPT and Gemini-1.5-Pro under the EgoButler Framework.The top section compares captions from two models on a 30-second clip: EgoGPT excels in personalization and hallucinates less on the egocentric videos. The bottom section features a question that is answered by the clip, showcasing EgoRAG’s skill in pinpointing relevant time slots and key clues.",
                "position": 1069
            }
        ]
    },
    {
        "header": "6Conclusion and Outlook",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAuthorship Statement",
        "images": []
    },
    {
        "header": "Appendix BEthical Considerations",
        "images": []
    },
    {
        "header": "Appendix CPotenial Social Impact",
        "images": []
    },
    {
        "header": "Appendix DEgoLife Dataset Card",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.03803/x31.png",
                "caption": "",
                "position": 3915
            }
        ]
    },
    {
        "header": "Appendix EDaily Activities",
        "images": []
    },
    {
        "header": "Appendix FDetails of EgoIT",
        "images": []
    },
    {
        "header": "Appendix GHistory of Egocentric Datasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.03803/x32.png",
                "caption": "Figure A1:The Overview of Egocentric Datasets.The figure summarizes the domain, modality, annotation type, release time, dataset statistics, and other aspects of datasets, providing a comprehensive view of existing egocentric datasets.",
                "position": 4217
            }
        ]
    },
    {
        "header": "Appendix HAnnotation Examples",
        "images": []
    }
]