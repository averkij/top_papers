[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.18087/x1.png",
                "caption": "",
                "position": 65
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": []
    },
    {
        "header": "2.Related Work",
        "images": []
    },
    {
        "header": "3.Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.18087/x2.png",
                "caption": "Figure 2.Framework of DICE-Talk. Our method comprises three key components: disentangled emotion embedder, correlation-enhanced emotion conditioning, and emotion discrimination objective. These architectural elements work synergistically to decouple identity representations from emotional cues while preserving facial articulation details, thereby generating lifelike animated portraits with emotionally nuanced expressions.",
                "position": 142
            }
        ]
    },
    {
        "header": "4.Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.18087/x3.png",
                "caption": "Figure 3.Visual comparisons with recent state-of-the-art taking head generation methods.Our method can obtain more accurate emotional expressions. Besides, we propose the full video comparison in supplementary materials to represent the capability of our method on sync, naturalness and stability.",
                "position": 373
            },
            {
                "img": "https://arxiv.org/html/2504.18087/x4.png",
                "caption": "Figure 4.Visual results of user and ablation studies.(a): Visual comparisons with StyleTalk and EAT. (b): Comparison of visualization results with different emotion embeddings. (c): Emotion embeddings visualization. (d): Interpolation results between sad and happy emotions by controlling the emotion embedding.",
                "position": 376
            },
            {
                "img": "https://arxiv.org/html/2504.18087/x5.png",
                "caption": "Figure 5.Our visual results with different emotions and identities on out of domain dataset.",
                "position": 781
            }
        ]
    },
    {
        "header": "5.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]