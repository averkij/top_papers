[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23380/x1.png",
                "caption": "Figure 1:Illustration of how the sequence modality (B1B_{1},B2B_{2}) adds more information to semantic modality (e4e_{4},e8e_{8}) and how the same samples with different backgrounds are separated in a unified semantic space. For thee4e_{4}ande8e_{8}event vectors, their backgrounds, specificallyB1B_{1}andB2B_{2}, might be utilized to depicte4e_{4}ande8e_{8}in a more meaningful manner (it should be noted thate4e_{4}is equivalent toe8e_{8}). In this sense, we designate theB1B_{1}andB2B_{2}as sequence modalities and thee4e_{4}ande8e_{8}as semantic modalities.",
                "position": 163
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23380/x2.png",
                "caption": "Figure 2:A comprehensive overview of the existing literature dealing with the approaches for identifying anomalies in log data.",
                "position": 439
            }
        ]
    },
    {
        "header": "3Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23380/x3.png",
                "caption": "Figure 3:(a)Several lines extracted from the BLueGene/L log dataset.;(b)Semantic modality: The semantic modality is constructed using the extracted semantic vectors from log events’ messages.;(c)Sequence modality: The construction of the sequence modality involves appending semantic vectors into sequence vectors based on window sizes of 3, 4, 5, and 6.",
                "position": 468
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23380/x4.png",
                "caption": "Figure 4:The overview of CoLog. Light green and gold colors demonstrate modality encoders. Each encoder in the collaborative transformer consists of MHIA, MLP, MAL, and LNs. MHIA and MAL are multi-head impressed attention and modality adaptation layer modules, respectively. The preprocess layer transforms unstructured logs into easily understandable data for the model. The purpose of the balancing layer is to regulate the influences of different modalities when calculating the final results.",
                "position": 510
            },
            {
                "img": "https://arxiv.org/html/2512.23380/x5.png",
                "caption": "Figure 5:Illustration of differences between background and subsequent event vectors. Background and context sequence vectors are constructed based on background and subsequent event vectors. In mathematical terms,Bi=[Vi−3s​e​m,Vi−2s​e​m,Vi−1s​e​m]{B}_{i}=[{V}_{{i}-{3}}^{sem},\\ {V}_{{i}-{2}}^{sem},\\ {V}_{{i}-{1}}^{sem}]andCi=[Vi−3s​e​m,Vi−2s​e​m,Vi−1s​e​m,Vi+1s​e​m,Vi+2s​e​m,Vi+3s​e​m]{C}_{i}=[{V}_{{i}-{3}}^{sem},\\ {V}_{{i}-{2}}^{sem},\\ {V}_{{i}-{1}}^{sem},\\ {V}_{{i}+{1}}^{sem},\\ {V}_{{i}+{2}}^{sem},\\ {V}_{{i}+{3}}^{sem}], whereVis​e​m{V}_{i}^{sem}is the semantic vector of the log messagei{i}extracted by SBERT,Bi{B}_{i}is the background sequence vector of log messagei{i}, andCi{C}_{i}is the context sequence vector of log messagei{i}.",
                "position": 632
            },
            {
                "img": "https://arxiv.org/html/2512.23380/x6.png",
                "caption": "Figure 6:The architecture of the multi-head impressed attention layer. The MHIA process involves calculating the attention scores of the current modality through the MHA mechanism, using theQQvector from the current modality and theKKandCCvectors from the secondary modality. According to MHIA architecture, various modalities are encoded concurrently.",
                "position": 721
            },
            {
                "img": "https://arxiv.org/html/2512.23380/x7.png",
                "caption": "Figure 7:The architecture of modality adaptation layer. MAL achieves an overall representation for each modality by assigning weights to each node in the input sequence. It can also remove impurities of the modalities that are encoded in a collaborative manner.",
                "position": 811
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23380/x8.png",
                "caption": "Figure 8:The collection of output visualizations generated by the CoLog when applied to the Casper log dataset. (a) confusion matrix, (b) normalized confusion matrix, (c) receiver operating characteristics curve, and (d) precision-recall curve",
                "position": 1338
            },
            {
                "img": "https://arxiv.org/html/2512.23380/x9.png",
                "caption": "Figure 9:The collection of output visualizations generated by the CoLog when applied to the Jhuisi log dataset. (a) confusion matrix, (b) normalized confusion matrix, (c) receiver operating characteristics curve, and (d) precision-recall curve",
                "position": 1341
            },
            {
                "img": "https://arxiv.org/html/2512.23380/x10.png",
                "caption": "Figure 10:The collection of output visualizations generated by the CoLog when applied to the Nssal log dataset. (a) confusion matrix, (b) normalized confusion matrix, (c) receiver operating characteristics curve, and (d) precision-recall curve",
                "position": 1344
            },
            {
                "img": "https://arxiv.org/html/2512.23380/x11.png",
                "caption": "Figure 11:The collection of output visualizations generated by the CoLog when applied to the Honey7 log dataset. (a) confusion matrix, (b) normalized confusion matrix, (c) receiver operating characteristics curve, and (d) precision-recall curve",
                "position": 1347
            },
            {
                "img": "https://arxiv.org/html/2512.23380/x12.png",
                "caption": "Figure 12:The output collection of visualizations generated by the CoLog when applied to the Zookeeper dataset. (a) confusion matrix, (b) normalized confusion matrix, (c) receiver operating characteristics curve, and (d) precision-recall curve",
                "position": 1350
            },
            {
                "img": "https://arxiv.org/html/2512.23380/x13.png",
                "caption": "Figure 13:The collection of output visualizations generated by the CoLog when applied to the Hadoop log dataset. (a) confusion matrix, (b) normalized confusion matrix, (c) receiver operating characteristics curve, and (d) precision-recall curve",
                "position": 1353
            },
            {
                "img": "https://arxiv.org/html/2512.23380/x14.png",
                "caption": "Figure 14:The collection of output visualizations generated by the CoLog when applied to the BlueGene/L log dataset. (a) confusion matrix, (b) normalized confusion matrix, (c) receiver operating characteristics curve, and (d) precision-recall curve",
                "position": 1356
            },
            {
                "img": "https://arxiv.org/html/2512.23380/x15.png",
                "caption": "Figure 15:Collection of graphical illustrations of hyperparameters tuning practices performed by the CoLog on the (a) Casper, (b) Jhuisi, and (c) Honey7 datasets. The criterion for selecting the optimal configuration is to achieve the highest accuracy in the shortest time. Based on this criterion, the results are sorted from left to right. The optimal configuration is also depicted in the respective diagram for each dataset.",
                "position": 2445
            },
            {
                "img": "https://arxiv.org/html/2512.23380/x16.png",
                "caption": "Figure 16:Visual illustrations of various train-test configurations performed by the CoLog on the (a) Casper, (b) Jhuisi, and (c) Honey7 datasets.",
                "position": 2455
            },
            {
                "img": "https://arxiv.org/html/2512.23380/x17.png",
                "caption": "Figure 17:Comparison of Tomek link with other class imbalance solving methods performed by the CoLog on the (a) Casper, (b) Jhuisi, and (c) Honey7 datasets.",
                "position": 2465
            },
            {
                "img": "https://arxiv.org/html/2512.23380/x18.png",
                "caption": "Figure 18:Visual illustrations of the CoLog’s output vectors on the (a) Casper, (b) Jhuisi, and (c) Honey7 datasets utilizing PCA.",
                "position": 2779
            },
            {
                "img": "https://arxiv.org/html/2512.23380/x19.png",
                "caption": "Figure 19:The visual illustration of CoLog’s outcomes for various injection ratios of unstable log events on the Spark dataset.",
                "position": 2881
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Declarations",
        "images": []
    }
]