[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17127/figures/software-stack.png",
                "caption": "Figure 1:The AMD software stack used to train ZAYA1, along with the respective languages and component libraries that each layer is written in. The principal hardware is described in SectionII. Our core training framework is a forked internal version of Megatron-LM adapted for the AMD stack.",
                "position": 142
            }
        ]
    },
    {
        "header": "IICluster Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17127/figures/node-arch.png",
                "caption": "(a) The architecture of a single node",
                "position": 222
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/node-arch.png",
                "caption": "(a) The architecture of a single node",
                "position": 225
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/cluster-arch.png",
                "caption": "(b) Cluster topology",
                "position": 230
            }
        ]
    },
    {
        "header": "IIIHardware Characterization",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17127/figures/bandwidth-comparison-linear.png",
                "caption": "Figure 3:The achievable memory bandwidth to HBM for PyTorch using the ROCm/CUDA backends.",
                "position": 372
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/heatmap_K_1024.png",
                "caption": "(a) GEMM performance (TFLOPS/s) on MI300X with K=1024. SmallKKlimits throughput; large M and N are needed to approach peak performance. Performance varies significantly even at large problem sizes.",
                "position": 385
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/heatmap_K_1024.png",
                "caption": "(a) GEMM performance (TFLOPS/s) on MI300X with K=1024. SmallKKlimits throughput; large M and N are needed to approach peak performance. Performance varies significantly even at large problem sizes.",
                "position": 388
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/heatmap_K_7168.png",
                "caption": "(b) GEMM performance (TFLOPS/s) on MI300X with K=7168. LargerKKachieves higher throughput with smoother performance landscape. Peak performance reached at moderate problem sizes (M,N≥512M,N\\geq 512).",
                "position": 393
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/rccl_AllReduce_oop_busbw.png",
                "caption": "(a) AllReduce Bus Bandwidth",
                "position": 432
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/rccl_AllReduce_oop_busbw.png",
                "caption": "(a) AllReduce Bus Bandwidth",
                "position": 435
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/rccl_AllGather_oop_busbw.png",
                "caption": "(b) AllGather Bus Bandwidth",
                "position": 440
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/rccl_ReduceScatter_oop_busbw.png",
                "caption": "(c) ReduceScatter Bus Bandwidth",
                "position": 445
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/rccl_AllReduce_oop_latency.png",
                "caption": "(d) AllReduce Latency",
                "position": 451
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/rccl_AllGather_oop_latency.png",
                "caption": "(e) AllGather Latency",
                "position": 456
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/rccl_ReduceScatter_oop_latency.png",
                "caption": "(f) ReduceScatter Latency",
                "position": 461
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/rccl_p2p_sendrecv_2gpus_busbw.png",
                "caption": "(a) Bus bandwidth",
                "position": 468
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/rccl_p2p_sendrecv_2gpus_busbw.png",
                "caption": "(a) Bus bandwidth",
                "position": 471
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/rccl_p2p_sendrecv_2gpus_latency.png",
                "caption": "(b) Latency",
                "position": 476
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/rccl_AllReduce_scaling_oop_busbw.png",
                "caption": "(a) AllReduce Bus Bandwidth",
                "position": 497
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/rccl_AllReduce_scaling_oop_busbw.png",
                "caption": "(a) AllReduce Bus Bandwidth",
                "position": 500
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/rccl_AllGather_scaling_oop_busbw.png",
                "caption": "(b) AllGather Bus Bandwidth",
                "position": 505
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/rccl_ReduceScatter_scaling_oop_busbw.png",
                "caption": "(c) ReduceScatter Bus Bandwidth",
                "position": 510
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/rccl_AllReduce_scaling_oop_latency.png",
                "caption": "(d) AllReduce Latency",
                "position": 516
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/rccl_AllGather_scaling_oop_latency.png",
                "caption": "(e) AllGather Latency",
                "position": 521
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/rccl_ReduceScatter_scaling_oop_latency.png",
                "caption": "(f) ReduceScatter Latency",
                "position": 526
            }
        ]
    },
    {
        "header": "IVModel",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17127/figures/zaya_arch.png",
                "caption": "Figure 8:The model architecture of ZAYA1. The two core innovations in architecture presented here are CCA for the attention block and the ZAYA1 router. The ZAYA1 router replaces the linear router with a more expressive one consisting of downprojection, EDA, and then three sequential MLPs per expert.",
                "position": 546
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/zaya_schedule.png",
                "caption": "Figure 9:Schematic of the three phases of pretraining for ZAYA1-base. Data mixture, learning rate schedule, and context length are chosen for each phase so that the model is prepared for post-training. The core pretraining consists of two phases. The first phase inculcates general knowledge and linguistic understanding into the model through highly diverse corpora of primarily web-sourced data. The second phase begins to reinforce and strengthen the mathematics, coding, and STEM knowledge components through additional mixing of information-rich high-quality data. The final phase extends the context and further emphasizes STEM content as well as prepares the base for instruction-following and reasoning post-training.",
                "position": 642
            }
        ]
    },
    {
        "header": "VPerformance Optimization",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17127/figures/training-iteration-breakdown.png",
                "caption": "(a) Training iteration latency breakdown",
                "position": 1023
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/training-iteration-breakdown.png",
                "caption": "(a) Training iteration latency breakdown",
                "position": 1026
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/gemm-kernel-breakdown.png",
                "caption": "(b) GPU operation breakdown",
                "position": 1031
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/cp-design.png",
                "caption": "Figure 11:The context-parallelism design used to train ZAYA1-base on longer context lengths.",
                "position": 1234
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/muon-send-recv.png",
                "caption": "Figure 12:TheSendRecvcommunication we implemented for the Muon step to keep the memory overhead low based on the parameter sharding scheme.",
                "position": 1244
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/aegis-diagram.png",
                "caption": "Figure 13:High-level architecture of the Aegis fault tolerance system.\nThe system operates as a four-step process consisting of (1) artifact collection,\n(2) evidence gathering, (3) action graph compilation, and (4) action graph execution.\nAdditionally, we implement a control plane that allows human triage efforts to\ndynamically scope Aegis’s response and take appropriate actions to refresh failing\nAegis processes.",
                "position": 1430
            }
        ]
    },
    {
        "header": "VIResults",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17127/figures/mmlu_pro_vs_ttft.png",
                "caption": "Figure 14:Performance of ZAYA1-base vs comparable base models at different scales of time to first token (TTFT) and advanced general knowledge capability (MMLU-pro).",
                "position": 1447
            }
        ]
    },
    {
        "header": "VIIDiscussion",
        "images": []
    },
    {
        "header": "VIIIConclusion",
        "images": []
    },
    {
        "header": "IXAcknowledgements",
        "images": []
    },
    {
        "header": "Appendix ACluster Details",
        "images": []
    },
    {
        "header": "Appendix BStorage Node Sizing and I/O Calculations",
        "images": []
    },
    {
        "header": "Appendix CCompressed Convolutional Attention",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17127/figures/cca_pretty.png",
                "caption": "Compressed Convolutional Attention",
                "position": 2594
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/rccl_All-to-All_oop_busbw.png",
                "caption": "(a) All-to-All Bus Bandwidth",
                "position": 2642
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/rccl_All-to-All_oop_busbw.png",
                "caption": "(a) All-to-All Bus Bandwidth",
                "position": 2645
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/rccl_All-to-All_oop_latency.png",
                "caption": "(b) All-to-All Latency",
                "position": 2650
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/rccl_All-to-All_scaling_oop_busbw.png",
                "caption": "(a) All-to-All Bus Bandwidth",
                "position": 2657
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/rccl_All-to-All_scaling_oop_busbw.png",
                "caption": "(a) All-to-All Bus Bandwidth",
                "position": 2660
            },
            {
                "img": "https://arxiv.org/html/2511.17127/figures/rccl_All-to-All_scaling_oop_latency.png",
                "caption": "(b) All-to-All Latency",
                "position": 2665
            }
        ]
    },
    {
        "header": "Appendix DCommunication Results",
        "images": []
    }
]