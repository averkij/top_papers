[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.19532/panda2.png",
                "caption": "",
                "position": 79
            },
            {
                "img": "https://arxiv.org/html/2601.19532/x1.png",
                "caption": "Figure 1:Overview of the cleaning process of the Omni-MATH dataset[1]. First, we check the compilability of the4,4284{,}428problem statements in LaTeX and convert them to valid LaTeX code with python. Next, a PhD-level mathematician manually went through the compiled pdf files twice, checking for the solvability and verifiability of each problem. In this process, missing information was added when available through browsing manually or through GPT-5.1, and images where added to the data folder. Furthermore, a tag was added to each problem containing animage,prooforestimation. Degenerate problems received theshould deletetag. We refer to the resulting dataset as Omni-MATH-2, which contains the same number of entries as the original Omni-MATH dataset (647 edited problems (14.6%14.6\\%), 247 tagged as non-standard (5.6%5.6\\%), see Table1). The Omni-MATH-2-Filtered dataset (n=4,181n{=}4{,}181) is the subset of cleaned questions, excluding the tagged ones. This makes it suitable for exact-answer judges.",
                "position": 116
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.19532/x2.png",
                "caption": "Figure 2:Example of the evaluation process for a question labelled asimage. The original problem in Omni-MATH misses the corresponding image, rendering the problem unsolvable. The model to be evaluated, GPT-5, correctly identifies that there is missing information, but Omni-Judge counts this as an incorrect answer.",
                "position": 138
            },
            {
                "img": "https://arxiv.org/html/2601.19532/x3.png",
                "caption": "Figure 3:The judge-induced difference in accuracy on Omni-MATH-2-Filtered is not uniformly distributed across disciplines, difficulty tiers or models, indicating structural evaluation noise rather than i.i.d. label noise. Evaluating five state-of-the-art models on Omni-MATH-2-Filtered—Claude Sonnet 4.5, DeepSeek v3.2, Gemini 3 Pro, GPT-5 and Kimi K2 Thinking—produced a different ranking of their mathematical abilities when evaluated with GPT-5 Mini rather than with Omni-Judge, which calls into question the interpretability of the inter-model differences. Furthermore, the right-hand panels show that, as questions become more difficult, judge disagreement increases, indicating that the judge’s conclusion is more important for challenging problems. The difference between judges’ answers is also model- and domain-dependent, with some disciplines and models producing larger differences than others (e.g. Claude and Deepseek, Calculus domain). For numerical performance scores with Bayesian confidence intervals consultTable˜A1.",
                "position": 141
            }
        ]
    },
    {
        "header": "2Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.19532/x4.png",
                "caption": "Figure 4:Example of a judge disagreement between Omni-Judge and GPT-5 mini. Here, it is not straightforward to see whether the model’s answer is equivalent to the reference answer. Two expert mathematicians, with the help of an LLM council, building on Claude Opus 4.5, DeepSeek v3.2 speciale, DeepSeek v3.2, GPT-5, and Gemini 3 pro, verified that the model’s answer is in fact correct and equivalent to the reference answer. GPT-5 mini thus correctly assesses equivalence.",
                "position": 229
            },
            {
                "img": "https://arxiv.org/html/2601.19532/x5.png",
                "caption": "Figure 5:On the subset of estimation problems in Omni-MATH-2, Omni-Judge and GPT-5 mini evaluate a substantial portion of strong estimates as incorrect. We compute the contest score by using the scoring rule present in the problem statement or solution.",
                "position": 259
            }
        ]
    },
    {
        "header": "3Discussion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "Author contributions",
        "images": []
    },
    {
        "header": "Dataset and code availability",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.19532/x6.png",
                "caption": "Figure A1:Example of the evaluation process for a question labelled asproof.",
                "position": 813
            },
            {
                "img": "https://arxiv.org/html/2601.19532/x7.png",
                "caption": "Figure A2:Example of the evaluation process for a question labelled asestimation.",
                "position": 817
            },
            {
                "img": "https://arxiv.org/html/2601.19532/x8.png",
                "caption": "Figure A3:Example of the evaluation process for a question labelled asshould delete.",
                "position": 821
            },
            {
                "img": "https://arxiv.org/html/2601.19532/x9.png",
                "caption": "Figure A4:Example of a judge disagreement between Omni-Judge and GPT-5 mini. Omni-Judge fails to assess that the fraction in the reference answer is equal to the fraction in the model’s final answer.",
                "position": 825
            },
            {
                "img": "https://arxiv.org/html/2601.19532/x10.png",
                "caption": "Figure A5:Example of a judge disagreement between Omni-Judge and GPT-5 mini. The reference answer is incomplete, causing GPT-5 mini to judge the model’s answer as wrong, while it is actually the correct and complete answer.",
                "position": 829
            }
        ]
    },
    {
        "header": "Appendix AMethods",
        "images": []
    }
]