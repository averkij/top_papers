[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07160/x1.png",
                "caption": "Figure 1.Our method takes a short monocular video as input (top) and animates the toonified appearance with synchronized expressions and movements using human-friendly text descriptions, e.g., ”Turn him into an American Comic style” (middle). Moreover, our system achieves real-time animation (bottom), operating at 25 FPS (generation inference is about 48 FPS) on an NVIDIA RTX 4090 machine and 15 FPS on an Apple MacBook (M1 chip). All toonified faces (middle and bottom) are generated from the same pre-trained appearance model. Top block: Natural face©©\\copyright©Tee Noir(CC BY). Middle and bottom: Natural face©©\\copyright©Trevor Noah(CC BY).",
                "position": 125
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07160/x2.png",
                "caption": "Figure 2.The overview of our methods. It takes a monocular video as input and tracks per frame, initializing the Gaussian point clouds using the tracked geometry from the first frame. We leverage the rigid transformation matrix (𝐑,𝐓x,y,z𝐑subscript𝐓𝑥𝑦𝑧\\mathbf{R},\\mathbf{T}_{x,y,z}bold_R , bold_T start_POSTSUBSCRIPT italic_x , italic_y , italic_z end_POSTSUBSCRIPT) and a learnable lazy factorw𝑤witalic_w(in Sec.3.3) to transfer points from the canonical space to the observation space. The proposed conditional Tri-plane Gaussian Deformation Field𝐃csubscript𝐃𝑐\\mathbf{D}_{c}bold_D start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPTuses the normalized render mapmtsubscript𝑚𝑡m_{t}italic_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, expressionβtsubscript𝛽𝑡\\beta_{t}italic_β start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTand vertex position𝐒tsubscript𝐒𝑡\\mathbf{S}_{t}bold_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTto predict the Gaussian properties deformation on each Gaussian points. Both the pre-training and fine-tuning phases share the same structure but target realistic appearance and T2I synthesized appearance, respectively. The details of conditional Tri-plane Gaussian Deformation Field and Text2Image editing are shown in III) and IV) respectively. Natural face©©\\copyright©Lizhen Wang et al.(CC BY).",
                "position": 163
            }
        ]
    },
    {
        "header": "2.Related Works",
        "images": []
    },
    {
        "header": "3.Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07160/x3.png",
                "caption": "Figure 3.A visualization of the adaptively selected points viaw𝑤witalic_w. After introducing the lazy factorw𝑤witalic_w, a soft boundary forms between the head and shoulders. Otherwise (w/o), they are mixed together and difficult to distinguish. It avoids mis-segmentation issues (indicated by the blue arrow). Natural face©©\\copyright©Tee Noir(CC BY).",
                "position": 274
            }
        ]
    },
    {
        "header": "4.Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07160/x4.png",
                "caption": "Figure 4.The perceptual evaluation of our method and baselines. For the sake of fairness and to avoid cherry-picked results, we adopt Pixar/cartoon stylization models provided by StyleGANEX and VToonify, and align them with our prompts. The style strength of VToonify is set to0.70.70.70.7. Natural face©©\\copyright©Lizhen Wang et al.(CC BY), and©©\\copyright©Wojciech Zielonka et al.(CC BY).",
                "position": 491
            },
            {
                "img": "https://arxiv.org/html/2410.07160/x5.png",
                "caption": "Figure 5.Visualization of cross-identity driven results. The drive actor is captured in the wild (©©\\copyright©Obama White House Daily), and the toonified avatar from a different identity is synchronized by facial expressions and poses.",
                "position": 542
            },
            {
                "img": "https://arxiv.org/html/2410.07160/x6.png",
                "caption": "Figure 6.The relationship of pre-trained and fine-tuned appearance. We list the different correspondences, a better pre-trained model results in a more detailed fine-tuned appearance (the pre-training iterations are not constant, but the fine-tuning iterations are consistent). The error maps show pixel Euclidean distance in RGB (color in [0, 255]). A lower mean error (white number) indicates a better pre-trained appearance. Please refer to the arrows for facial details. Natural face©©\\copyright©Yao Feng et al.(CC BY).",
                "position": 650
            },
            {
                "img": "https://arxiv.org/html/2410.07160/x7.png",
                "caption": "Figure 7.The visualization of three ablation studies (from top to bottom) results frommulti-view, the multi-view results are only learned from monocular inputs. (1) Top: The fine-tuned appearance by our method with different prompts. (2) Middle: The visualization of stylization intensity, where intensity = 0 represents the source appearance. (3) Bottom: The style trajectory over fine-tuning time, with Time = 0 min representing the source appearance. Please refer to the arrow for details. At the meantime, we present the multi-view visualization results at here for perception assessment, the multi-view appearance are learned from the monocular inputs. Natural face©©\\copyright©Yufeng Zheng et al.(CC BY), and©©\\copyright©Wojciech Zielonka et al.(CC BY).",
                "position": 664
            },
            {
                "img": "https://arxiv.org/html/2410.07160/x8.png",
                "caption": "Figure 8.We visualize the conditional Tri-plane Gaussian deformation field𝐃csubscript𝐃𝑐\\mathbf{D}_{c}bold_D start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPTusing a cross-identity driven approach. Utilizing𝐃csubscript𝐃𝑐\\mathbf{D}_{c}bold_D start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT(w/) helps manage more complex mouth shapes compared to not using𝐃csubscript𝐃𝑐\\mathbf{D}_{c}bold_D start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT(w/o). This improvement occurs throughout both the pre-training and fine-tuning phases. We highlight the mouth details for better evaluation. Natural face©©\\copyright©Wojciech Zielonka et al.(CC BY).",
                "position": 667
            },
            {
                "img": "https://arxiv.org/html/2410.07160/x9.png",
                "caption": "Figure 9.The visualization of the ablation studyℒC⁢O⁢Nsubscriptℒ𝐶𝑂𝑁\\mathcal{L}_{CON}caligraphic_L start_POSTSUBSCRIPT italic_C italic_O italic_N end_POSTSUBSCRIPTvia cross-identity driven manner. After introducing contrastive learning loss (w/ℒC⁢O⁢Nsubscriptℒ𝐶𝑂𝑁\\mathcal{L}_{CON}caligraphic_L start_POSTSUBSCRIPT italic_C italic_O italic_N end_POSTSUBSCRIPT), we achieve a significant improvement in the detail quality of the toonify appearance, as indicated by the arrows (e.g.facial wrinklesetc.). Natural face©©\\copyright©Wojciech Zielonka et al.(CC BY).",
                "position": 670
            },
            {
                "img": "https://arxiv.org/html/2410.07160/x10.png",
                "caption": "Figure 10.The visualize of the ablation study onw𝑤witalic_w. (a) is for the hard-parsed motion on head/shoulder, (b) for is the same motion on head/shoulder and (c) for is the adaptively learnedw𝑤witalic_w. We zoom-in the neck-shoulder transition area to highlight the details. There are artifacts around the neck in (a) due to inconsistent head/shoulder movement, and the shoulder in (b) is displaced synchronously with the head. The influence occurs throughout both the pre-training and fine-tuning phases. Natural face©©\\copyright©Yufeng Zheng et al.(CC BY).",
                "position": 715
            },
            {
                "img": "https://arxiv.org/html/2410.07160/x11.png",
                "caption": "Figure 11.The visualize of comparison with Next3D, the PTI (StyleGAN inversion) is used for feature embedding. We find that the results suffer from facial identity drift. Furthermore, the two styles associated with Next3D (Pixar and Cartoon) are not distinct. We apply the prompts “Cartoon” and “Pixar” for Next3D, the corresponding prompts for our method are “Turn him into cartoon style” and “Turn him into Pixar style”. Natural face©©\\copyright©Lizhen Wang et al.(CC BY).",
                "position": 718
            },
            {
                "img": "https://arxiv.org/html/2410.07160/x12.png",
                "caption": "Figure 12.The visualize of the real-time application. We use the camera capture appearance to animate the toonified appearance, the prompt “Turn her into the Joker” is applied for fine-tuning the realistic appearance. The reference source portrait is shown in bottom right corner. Natural face©©\\copyright©Yufeng Zheng et al.(CC BY).",
                "position": 745
            },
            {
                "img": "https://arxiv.org/html/2410.07160/x13.png",
                "caption": "Figure 13.The visualization of the problematic cases. We present the limitations regarding unseen poses and expressions. Artifacts for unseen poses are around the neck and head, while for unseen expressions, blurriness appears near the cheeks. Natural face©©\\copyright©Wojciech Zielonka et al.(CC BY).",
                "position": 755
            }
        ]
    },
    {
        "header": "5.Discussion and Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07160/x14.png",
                "caption": "Figure 14.The visualize of components details. The conditional facial embedding is a concatenation of encoded orthogonal rendering and the corresponding 3DMM expression coefficients. The size of out rendered map for I2I input is32×512×5123251251232\\times 512\\times 51232 × 512 × 512and the output size is3×512×51235125123\\times 512\\times 5123 × 512 × 512. The size of input normalized render is3×128×12831281283\\times 128\\times 1283 × 128 × 128, and the output of the conditional encoder (embedding) is1×25612561\\times 2561 × 256.",
                "position": 776
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]