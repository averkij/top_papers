[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07160/x1.png",
                "caption": "Figure 1.Our method takes a short monocular video as input (top) and animates the toonified appearance with synchronized expressions and movements using human-friendly text descriptions, e.g., â€Turn him into an American Comic styleâ€ (middle). Moreover, our system achieves real-time animation (bottom), operating at 25 FPS (generation inference is about 48 FPS) on an NVIDIA RTX 4090 machine and 15 FPS on an Apple MacBook (M1 chip). All toonified faces (middle and bottom) are generated from the same pre-trained appearance model. Top block: Natural faceÂ©Â©\\copyrightÂ©Tee Noir(CC BY). Middle and bottom: Natural faceÂ©Â©\\copyrightÂ©Trevor Noah(CC BY).",
                "position": 125
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07160/x2.png",
                "caption": "Figure 2.The overview of our methods. It takes a monocular video as input and tracks per frame, initializing the Gaussian point clouds using the tracked geometry from the first frame. We leverage the rigid transformation matrix (ğ‘,ğ“x,y,zğ‘subscriptğ“ğ‘¥ğ‘¦ğ‘§\\mathbf{R},\\mathbf{T}_{x,y,z}bold_R , bold_T start_POSTSUBSCRIPT italic_x , italic_y , italic_z end_POSTSUBSCRIPT) and a learnable lazy factorwğ‘¤witalic_w(in Sec.3.3) to transfer points from the canonical space to the observation space. The proposed conditional Tri-plane Gaussian Deformation Fieldğƒcsubscriptğƒğ‘\\mathbf{D}_{c}bold_D start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPTuses the normalized render mapmtsubscriptğ‘šğ‘¡m_{t}italic_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, expressionÎ²tsubscriptğ›½ğ‘¡\\beta_{t}italic_Î² start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTand vertex positionğ’tsubscriptğ’ğ‘¡\\mathbf{S}_{t}bold_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTto predict the Gaussian properties deformation on each Gaussian points. Both the pre-training and fine-tuning phases share the same structure but target realistic appearance and T2I synthesized appearance, respectively. The details of conditional Tri-plane Gaussian Deformation Field and Text2Image editing are shown in III) and IV) respectively. Natural faceÂ©Â©\\copyrightÂ©Lizhen Wang et al.(CC BY).",
                "position": 163
            }
        ]
    },
    {
        "header": "2.Related Works",
        "images": []
    },
    {
        "header": "3.Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07160/x3.png",
                "caption": "Figure 3.A visualization of the adaptively selected points viawğ‘¤witalic_w. After introducing the lazy factorwğ‘¤witalic_w, a soft boundary forms between the head and shoulders. Otherwise (w/o), they are mixed together and difficult to distinguish. It avoids mis-segmentation issues (indicated by the blue arrow). Natural faceÂ©Â©\\copyrightÂ©Tee Noir(CC BY).",
                "position": 274
            }
        ]
    },
    {
        "header": "4.Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07160/x4.png",
                "caption": "Figure 4.The perceptual evaluation of our method and baselines. For the sake of fairness and to avoid cherry-picked results, we adopt Pixar/cartoon stylization models provided by StyleGANEX and VToonify, and align them with our prompts. The style strength of VToonify is set to0.70.70.70.7. Natural faceÂ©Â©\\copyrightÂ©Lizhen Wang et al.(CC BY), andÂ©Â©\\copyrightÂ©Wojciech Zielonka et al.(CC BY).",
                "position": 491
            },
            {
                "img": "https://arxiv.org/html/2410.07160/x5.png",
                "caption": "Figure 5.Visualization of cross-identity driven results. The drive actor is captured in the wild (Â©Â©\\copyrightÂ©Obama White House Daily), and the toonified avatar from a different identity is synchronized by facial expressions and poses.",
                "position": 542
            },
            {
                "img": "https://arxiv.org/html/2410.07160/x6.png",
                "caption": "Figure 6.The relationship of pre-trained and fine-tuned appearance. We list the different correspondences, a better pre-trained model results in a more detailed fine-tuned appearance (the pre-training iterations are not constant, but the fine-tuning iterations are consistent). The error maps show pixel Euclidean distance in RGB (color in [0, 255]). A lower mean error (white number) indicates a better pre-trained appearance. Please refer to the arrows for facial details. Natural faceÂ©Â©\\copyrightÂ©Yao Feng et al.(CC BY).",
                "position": 650
            },
            {
                "img": "https://arxiv.org/html/2410.07160/x7.png",
                "caption": "Figure 7.The visualization of three ablation studies (from top to bottom) results frommulti-view, the multi-view results are only learned from monocular inputs. (1) Top: The fine-tuned appearance by our method with different prompts. (2) Middle: The visualization of stylization intensity, where intensity = 0 represents the source appearance. (3) Bottom: The style trajectory over fine-tuning time, with Time = 0 min representing the source appearance. Please refer to the arrow for details. At the meantime, we present the multi-view visualization results at here for perception assessment, the multi-view appearance are learned from the monocular inputs. Natural faceÂ©Â©\\copyrightÂ©Yufeng Zheng et al.(CC BY), andÂ©Â©\\copyrightÂ©Wojciech Zielonka et al.(CC BY).",
                "position": 664
            },
            {
                "img": "https://arxiv.org/html/2410.07160/x8.png",
                "caption": "Figure 8.We visualize the conditional Tri-plane Gaussian deformation fieldğƒcsubscriptğƒğ‘\\mathbf{D}_{c}bold_D start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPTusing a cross-identity driven approach. Utilizingğƒcsubscriptğƒğ‘\\mathbf{D}_{c}bold_D start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT(w/) helps manage more complex mouth shapes compared to not usingğƒcsubscriptğƒğ‘\\mathbf{D}_{c}bold_D start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT(w/o). This improvement occurs throughout both the pre-training and fine-tuning phases. We highlight the mouth details for better evaluation. Natural faceÂ©Â©\\copyrightÂ©Wojciech Zielonka et al.(CC BY).",
                "position": 667
            },
            {
                "img": "https://arxiv.org/html/2410.07160/x9.png",
                "caption": "Figure 9.The visualization of the ablation studyâ„’Câ¢Oâ¢Nsubscriptâ„’ğ¶ğ‘‚ğ‘\\mathcal{L}_{CON}caligraphic_L start_POSTSUBSCRIPT italic_C italic_O italic_N end_POSTSUBSCRIPTvia cross-identity driven manner. After introducing contrastive learning loss (w/â„’Câ¢Oâ¢Nsubscriptâ„’ğ¶ğ‘‚ğ‘\\mathcal{L}_{CON}caligraphic_L start_POSTSUBSCRIPT italic_C italic_O italic_N end_POSTSUBSCRIPT), we achieve a significant improvement in the detail quality of the toonify appearance, as indicated by the arrows (e.g.facial wrinklesetc.). Natural faceÂ©Â©\\copyrightÂ©Wojciech Zielonka et al.(CC BY).",
                "position": 670
            },
            {
                "img": "https://arxiv.org/html/2410.07160/x10.png",
                "caption": "Figure 10.The visualize of the ablation study onwğ‘¤witalic_w. (a) is for the hard-parsed motion on head/shoulder, (b) for is the same motion on head/shoulder and (c) for is the adaptively learnedwğ‘¤witalic_w. We zoom-in the neck-shoulder transition area to highlight the details. There are artifacts around the neck in (a) due to inconsistent head/shoulder movement, and the shoulder in (b) is displaced synchronously with the head. The influence occurs throughout both the pre-training and fine-tuning phases. Natural faceÂ©Â©\\copyrightÂ©Yufeng Zheng et al.(CC BY).",
                "position": 715
            },
            {
                "img": "https://arxiv.org/html/2410.07160/x11.png",
                "caption": "Figure 11.The visualize of comparison with Next3D, the PTI (StyleGAN inversion) is used for feature embedding. We find that the results suffer from facial identity drift. Furthermore, the two styles associated with Next3D (Pixar and Cartoon) are not distinct. We apply the prompts â€œCartoonâ€ and â€œPixarâ€ for Next3D, the corresponding prompts for our method are â€œTurn him into cartoon styleâ€ and â€œTurn him into Pixar styleâ€. Natural faceÂ©Â©\\copyrightÂ©Lizhen Wang et al.(CC BY).",
                "position": 718
            },
            {
                "img": "https://arxiv.org/html/2410.07160/x12.png",
                "caption": "Figure 12.The visualize of the real-time application. We use the camera capture appearance to animate the toonified appearance, the prompt â€œTurn her into the Jokerâ€ is applied for fine-tuning the realistic appearance. The reference source portrait is shown in bottom right corner. Natural faceÂ©Â©\\copyrightÂ©Yufeng Zheng et al.(CC BY).",
                "position": 745
            },
            {
                "img": "https://arxiv.org/html/2410.07160/x13.png",
                "caption": "Figure 13.The visualization of the problematic cases. We present the limitations regarding unseen poses and expressions. Artifacts for unseen poses are around the neck and head, while for unseen expressions, blurriness appears near the cheeks. Natural faceÂ©Â©\\copyrightÂ©Wojciech Zielonka et al.(CC BY).",
                "position": 755
            }
        ]
    },
    {
        "header": "5.Discussion and Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07160/x14.png",
                "caption": "Figure 14.The visualize of components details. The conditional facial embedding is a concatenation of encoded orthogonal rendering and the corresponding 3DMM expression coefficients. The size of out rendered map for I2I input is32Ã—512Ã—5123251251232\\times 512\\times 51232 Ã— 512 Ã— 512and the output size is3Ã—512Ã—51235125123\\times 512\\times 5123 Ã— 512 Ã— 512. The size of input normalized render is3Ã—128Ã—12831281283\\times 128\\times 1283 Ã— 128 Ã— 128, and the output of the conditional encoder (embedding) is1Ã—25612561\\times 2561 Ã— 256.",
                "position": 776
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]