[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.07877/x1.png",
                "caption": "Figure 1:(Up) Goal of WSAG is to identify action-affordable parts within the egocentric image, given exocentric images as contextual hints.\n(Down) To perform affordance learning, we first discover the part-relevant clues from both egocentric and exocentric images.\nWhen these parts are deemed reliable in representing affordance-relevant regions, the model learns to distinguish these parts from the other parts.\nIf not, we instead utilize object-level clues to distinguish objects from the background.\nCompared to our baseline (LOCATE[24]), which only exploits reliable parts of exocentric images, our approach extends affordance learning to learn from both egocentric and exocentric views and also both from affordance-relevant and affordance-irrelevant clues.\nBy leveraging all these types of clues within the mini-batch at once, the model learns to distinguish affordance-relevant parts from representations of other affordance classes and backgrounds.",
                "position": 139
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.07877/x2.png",
                "caption": "Figure 2:Overall flow. Egocentric and exocentric images are processed to perform classification and selective contrastive learning.\nNote that (view)∈\\in{ego, exo}.",
                "position": 241
            },
            {
                "img": "https://arxiv.org/html/2508.07877/x3.png",
                "caption": "Figure 3:Illustration of object discovery.\nThe object affinity map is derived from CLIP as a zero-shot image-text similarity map.",
                "position": 262
            },
            {
                "img": "https://arxiv.org/html/2508.07877/x4.png",
                "caption": "Figure 4:Illustration of prototypical contrastive learning.\n(Up) Process of identifying part clues in exocentric images.\nDiscovered objects are segmented to extract part candidates, which are then matched with DINO’s attention map.\n(Down) Prototypical contrastive learning is selectively applied based on the reliability of part clues.\nWhen reliable, object anchors in egocentric images are attracted toward part clues, but otherwise image anchors are drawn toward object clues in exocentric images.",
                "position": 278
            },
            {
                "img": "https://arxiv.org/html/2508.07877/x5.png",
                "caption": "Figure 5:An illustration of binarizing objects within egocentric images based on affordance criterion. The most salient pixel in each exocentric object affinity map serves as a reference, establishing a criterion to classify each pixel in the egocentric image as part of an affordable region (Q+Q^{+}) or a non-affordable region (Q−Q^{-}).\nThe minimum value amongρ1,ρ2\\rho_{1},\\rho_{2}, andρ3\\rho_{3}is used as criterion.",
                "position": 413
            },
            {
                "img": "https://arxiv.org/html/2508.07877/x6.png",
                "caption": "Table 1:Performance comparison on the AGD20K and HICO-IIF datasets.",
                "position": 419
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.07877/x6.png",
                "caption": "Figure 6:Qualitative comparison results of our approach and other methods in seen and unseen domains.",
                "position": 794
            },
            {
                "img": "https://arxiv.org/html/2508.07877/x7.png",
                "caption": "Table 2:Study on model components.\nFrom left to right, we examine the benefits of object- and part-level prototypical contrastive learning (Proto.), object- and part-level pixel contrastive learning (Pixel.), and the calibration process with an object affinity map.\nCali. indicates the calibration process of the localization map. Obj. and P. denote object-level and part-level learning.",
                "position": 822
            },
            {
                "img": "https://arxiv.org/html/2508.07877/x7.png",
                "caption": "Figure 7:Visualization of discovered objects and parts used to guide the training.\n(a) Object affinity mapAobjA_{\\text{obj}}.\nThe leftmost sample for each class is an egocentric image and the rest are the exocentric images.\n(b) Affordable parts of exocentric images used for prototypical contrastive learning.\n(c) Affordable partsQ+Q^{+}of egocentric images used for pixel contrastive learning.",
                "position": 989
            },
            {
                "img": "https://arxiv.org/html/2508.07877/x8.png",
                "caption": "Figure 8:Analysis of the impact of each training level,i.e.,object and part, with qualitative results.",
                "position": 1007
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "ADatasets and Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.07877/x9.png",
                "caption": "Figure A1:Visualization of object affinity map for exocentric image, with various kinds of prompt.\n(a): {action}, (b): “an item to” {action} “with”, (c): multiplication of “an item to” {action} “with” and “a person” {action} “an item”.",
                "position": 1185
            },
            {
                "img": "https://arxiv.org/html/2508.07877/x10.png",
                "caption": "Figure A2:Ablation studies of various hyperparameters. The X-axis denotes the value of each hyperparameter, the Y-axis shows the KLD performance.",
                "position": 1191
            }
        ]
    },
    {
        "header": "BObject Affinity Map",
        "images": []
    },
    {
        "header": "CHyperparameter Ablation",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.07877/x11.png",
                "caption": "Figure A3:Study on loss coefficients.λ1\\lambda_{1}andλ2\\lambda_{2}are coefficients for prototypical and pixel contrastive learning, respectively.\nWe vary each coefficient while keeping the others fixed at their default value of 1 and also examine their impact when adjusted simultaneously.",
                "position": 1247
            },
            {
                "img": "https://arxiv.org/html/2508.07877/x12.png",
                "caption": "Figure A4:Visualization of the test image, ground-truth label, and our prediction on AGD20K dataset.",
                "position": 1253
            }
        ]
    },
    {
        "header": "DBias on Object and Affordance Classes",
        "images": []
    },
    {
        "header": "EDINO Attention Map for Prototype Selection",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.07877/x13.png",
                "caption": "Figure A5:Affordance grounding results of our approach and other methods in the seen domain.",
                "position": 1336
            },
            {
                "img": "https://arxiv.org/html/2508.07877/x14.png",
                "caption": "Figure A6:Affordance grounding results of our approach and other methods in the unseen domain.",
                "position": 1341
            }
        ]
    },
    {
        "header": "FAdditional Qualitative Results",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]