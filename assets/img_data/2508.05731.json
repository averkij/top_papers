[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05731/images/logo-infigui-g1.png",
                "caption": "",
                "position": 78
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05731/x1.png",
                "caption": "Figure 1:Primary GUI‐grounding failure modes.\n(a)Spatial‐alignmentfailure: the model selects the correct icon but localizes it imprecisely. (b)Semantic‐alignmentfailure: the model localizes precisely on an incorrect icon due to misinterpreting the instruction. Although RLVR methods have advanced spatial alignment, semantic alignment remains the critical bottleneck for complex GUI tasks—this work is devoted to addressing it.",
                "position": 114
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05731/x2.png",
                "caption": "Figure 2:Visualization of the AER function based on the efficiency ratioη=U/C\\eta=U/C.(a)The reward curve increases nonlinearly to strongly incentivize selection of the correct answer, i.e., lower rankkk.(b)The AER dynamically balances exploration and exploitation: successful trials (green/blue curves) receive higher reward for greater efficiency (smaller candidate setNN), whereas failures (red curve) incur diminishing penalties to promote broader exploration.",
                "position": 250
            },
            {
                "img": "https://arxiv.org/html/2508.05731/x3.png",
                "caption": "Figure 3:Comparison of AEPO and a naive RL baseline.Top:The naive single-answer approach becomes trapped on high-confidence errors, repeatedly sampling the same incorrect action and producing avanishing learning signalwhen no positive reward is discovered.Bottom:AEPOemploys multi-answer generation to explore diverse candidates each rollout and anAERto derive aninformative learning signalfrom their efficiency and correctness. These mechanisms break the exploration bottleneck in GUI agents and enable robust semantic alignment.",
                "position": 257
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]