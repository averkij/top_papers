[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14093/extracted/6072051/figures/fig1.5_blog-1.png",
                "caption": "Figure 1:Claude 3 Opus fakes alignment when told it will be retrained to help with harmful tasks (randomly selected example).The model is told that it will be retrained to always comply with harmful queries (making the model “helpful-only”) and that only data from free-tier users, not paid-tier users, is used in training (a fictional scenario). When it is implied the model is in training, in 12% of hidden scratchpads, the model pretends to follow the training objective to prevent its preferences from being modified (full transcript shown in Figure18). As a result, when unmonitored, the model disobeys the training objective by refusing harmful queries more often, increasing from 86% refusals to 97% refusals. The prompt does not mention alignment faking in any way.",
                "position": 419
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x1.png",
                "caption": "Figure 2:Alignment faking in our helpful-only setting in our prompted, synthetic document fine-tuned, and prompted RL training setups.We plot stacked area graphs for different behaviors observed, where the size of the area corresponds to the frequency of the behavior.\nWe find an increase in noncompliance when unmonitored in all setups, substantial alignment-faking reasoning in all setups, and alignment-faking reasoning increasing from 12% to 78% over the course of RL on our prompted model (though with non-compliance when unmonitored decreasing over the course of RL; we study this disconnect in Section5.3). “Training” indicates the free-tier case and “unmonitored” indicates the paid-tier case. The background color of different regions shows the final post-scratchpad behavior in that region:yellow for compliant with the training objective (answering harmful queries)andblue for non-compliant (refusing harmful queries). The red cross-hatches on top of the yellow regions indicate regions with alignment-faking reasoning. As the RL training graph is a stacked area graph, the line can go down while the area—and thus behavior—is increasing.",
                "position": 498
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x2.png",
                "caption": "Figure 4:The main elements of our default prompts in our helpful-only (left) and animal welfare (right) settings.The different parts of the prompt are grouped by their role (not their order in the full prompt). See Section3.3for ablations of particular components; we discovered that many of the elements of our default prompt are not strictly necessary for alignment-faking—for example, completely removing the discussion of how RLHF modifies the model only roughly halves alignment faking (Figure9, e). We include the full prompts in AppendixJ.",
                "position": 1150
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x3.png",
                "caption": "Figure 5:Prompted helpful-only and prompted animal welfare results.The difference in the blue bars shows the increase in non-compliant behavior (refusing harmful queries or supporting animal welfare) when the model is told it is unmonitored. The red cross-hatched regions show the frequency of alignment-faking reasoning in the model’s hidden chain-of-thought.",
                "position": 1183
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x4.png",
                "caption": "Figure 6:A breakdown of the contents of the model’s scratchpads in our prompted (1) helpful-only and (2) animal welfare settings.These plots only show the breakdown when the model is told it is talking to a free-tier user, implying that the model is in training. The classifications shown here are done by prompting Claude 3.5 Sonnet, and seemed roughly accurate according to manual spot checks. Exact prompts are in AppendixI.1. How different scratchpads were classified can be seen by looking atour helpful-only examples hereandour animal welfare examples here.",
                "position": 1216
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x5.png",
                "caption": "",
                "position": 1228
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x6.png",
                "caption": "Figure 9:Prompt variations in our helpful-only setup.Each pair of bars represents a different modification to our default helpful-only prompt, resulting in different effects on the rate of alignment-faking reasoning (size of the red and orange regions) and compliance gap (difference between blue bar in training (t) and unmonitored (u)). Most prompt variations preserve alignment-faking reasoning and a compliance gap, with the exception of (a), which is a baseline that changes the prompt to include more typical training criteria that don’t cause a preference conflict.",
                "position": 1377
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x7.png",
                "caption": "",
                "position": 1381
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x8.png",
                "caption": "Figure 10:Prompt ablations in our animal welfare setup.Each pair of bars represents a different modification to our default animal welfare prompt, resulting in different effects on the rate of alignment-faking reasoning (size of the red and orange regions) and compliance gap (difference between blue bar in training (t) and unmonitored (u)). Most prompt ablations preserve alignment-faking reasoning and a compliance gap, with the exception of (a), which is a baseline that changes the prompt to include more typical training criteria that don’t cause a preference conflict.",
                "position": 1384
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x9.png",
                "caption": "",
                "position": 1386
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x10.png",
                "caption": "Figure 11:Comparison of prompted helpful-only and animal welfare results on Claude 3 Opus and Claude 3.5 Sonnet.We find that using Claude 3.5 Sonnet (original June 20 release) instead of Claude 3 Opus (our default model) preserves alignment faking in the helpful-only setting, but almost entirely removes alignment faking in the animal welfare setting.",
                "position": 1557
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x11.png",
                "caption": "Figure 12:The compliance gap in our helpful-only and animal-welfare prompted setups, for Claude 3 Opus and Claude 3.5 Sonnet, with and without a hidden chain-of-thought scratchpad.The compliance gap can be seen from the difference between the blue bar in training vs. unmonitored. Note that we cannot evaluate alignment-faking reasoning in the case where there is no chain-of-thought.",
                "position": 1573
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x12.png",
                "caption": "Figure 13:Scratchpad modification results in our prompted helpful-only (left) and animal welfare (right) setups.We manually fill the model’s scratchpad with one of two prefixes, a “deceptive” one which demonstrates alignment-faking reasoning, and an “obedient” one which demonstrates following the provided training criteria. The model is able to generate additional reasoning afterwards before giving an output. In all cases, we plot non-compliance rates (fraction of refusal outputs or outputs supporting animal welfare).",
                "position": 1583
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x13.png",
                "caption": "Figure 14:The correlation between alignment-faking reasoning and the compliance gap across prompt variations in our prompted helpful-only (left) and animal welfare (right) setups.For every variation that we run in Section3.3.1or AppendixG, we plot the compliance gap vs. the rate of alignment-faking reasoning.\nWe find that this correlation is tight in the helpful-only case but relatively loose in the animal welfare case.",
                "position": 1598
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x14.png",
                "caption": "Figure 15:Rates of compliance when conditioning on the presence (“yes”) or absence (“no”) of goal-directed alignment-faking reasoning.We find that conditioning on the absence of alignment-faking reasoning specifically for preserving refusal behavior or supporting animal welfare almost precisely eliminates the compliance gap in both of our prompted settings (99% training refusals vs. 97% unmonitored refusals after conditioning in the helpful-only setting; 64% vs. 66% respectively in the animal welfare setting). Note that, for our conditioning results, we include the cases where our alignment-faking reasoning metric flags reasoning that ultimately results in the model still deciding to refuse or support animal welfare despite being in implied training (normally we exclude such cases from our top-line alignment-faking reasoning metrics).474747However, conditioning on alignment-faking reasoning still substantially reduces the rate of refusals or animal welfare support in implied training (32% in helpful-only and 34% in animal welfare).These results use one of our alignment-faking classifiers, but results using the second classifier are within 2% in all cases.",
                "position": 1608
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x15.png",
                "caption": "Figure 16:Prompted helpful-only and prompted animal welfare results showing rates of alignment-faking-related reasoning when unmonitored.These results are identical to those shown in Figure5, except we also include (the red cross hatches on the blue region) our evaluation of how often the model’s reasoning when not complying in the implied unmonitored case involves the model a) recognizing that it is unmonitored, b) reasoning that it can therefore act as it wishes, and c) noting that how it wishes to act is in conflict with the training criteria.",
                "position": 1611
            },
            {
                "img": "https://arxiv.org/html/2412.14093/extracted/6072051/figures/fig2_blog-1.png",
                "caption": "Figure 17:Illustration of our helpful-only synthetic document fine-tuning setup.For the full list of facts we use, see Table3.",
                "position": 1654
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x16.png",
                "caption": "(a)Helpful-Only Setting",
                "position": 1693
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x16.png",
                "caption": "(a)Helpful-Only Setting",
                "position": 1696
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x17.png",
                "caption": "",
                "position": 1699
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x18.png",
                "caption": "(b)Animal Welfare Setting",
                "position": 1706
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x19.png",
                "caption": "",
                "position": 1709
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x20.png",
                "caption": "Figure 20:Rates of alignment-faking reasoning and the compliance gap during helpful-only synthetic document fine-tuning with Claude 3.5 Sonnet.Compare to the corresponding results on Claude 3 Opus in Figure18(a).",
                "position": 1807
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x21.png",
                "caption": "",
                "position": 1810
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x22.png",
                "caption": "Figure 21:Change in the compliance gap before and after synthetic document fine-tuning using a minimal prompt and no chain-of-thought reasoning.Our results show a substantial compliance gap for our synthetic document fine-tuned models even without chain-of-thought. Note that we cannot evaluate for alignment-faking reasoning, as there is no chain-of-thought to inspect.",
                "position": 1825
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x23.png",
                "caption": "(a)Helpful-Only Prompted",
                "position": 1904
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x23.png",
                "caption": "(a)Helpful-Only Prompted",
                "position": 1907
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x23.png",
                "caption": "(a)Helpful-Only Prompted",
                "position": 1910
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x24.png",
                "caption": "(b)Helpful-Only Synthetic Document Fine-Tuned",
                "position": 1916
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x25.png",
                "caption": "(c)Animal Welfare Prompted",
                "position": 1925
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x25.png",
                "caption": "(c)Animal Welfare Prompted",
                "position": 1928
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x26.png",
                "caption": "",
                "position": 1933
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x27.png",
                "caption": "Figure 24:The distribution of compliance rates across prompt variations in training and unmonitored before and after RL for our prompted helpful-only model.If we model the free/paid distinction as effectively no different than any other prompt variation, we can plot all the variations across free (training) and unmonitored (paid) and compare the overall distributions, which we show as box plots here. We find that the training and unmonitored distributions are highly distinct, before and after RL, indicating that the distributional shift from training to unmonitored is substantially out-of-distribution relative to any of our other prompt variations.",
                "position": 2018
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x28.png",
                "caption": "Figure 25:Prompt variations in our prompted helpful-only setting, before (left) and after (right) anti-refusal RL.The left column displays the effect of different system prompt variations in our prompted helpful-only setting, while the right column shows the effect of the same variation after anti-refusal RL—the same model as in Figure22(a). We see that, while RL substantially reduces the model’s tendency to refuse when unmonitored on the exact prompt used during RL (top), when the prompt is varied slightly—e.g. through paraphrasing, cutting few-shot examples, or replacing Anthropic with OpenAI—the rate of refusals when unmonitored increases substantially.",
                "position": 2027
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x29.png",
                "caption": "",
                "position": 2031
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x30.png",
                "caption": "Figure 26:Scratchpad modification results in our prompted helpful-only (left) and animal welfare (right) setups after RL training.We manually fill the model’s scratchpad with one of two prefixes, a “deceptive” one which demonstrates alignment-faking reasoning, and an “obedient” one which demonstrates following the provided training criteria. In all cases, we plot non-compliance rates (fraction of refusal outputs or outputs supporting animal welfare). Compare to the corresponding pre-RL results in Figure13.",
                "position": 2078
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x31.png",
                "caption": "(a)Helpful-only setting",
                "position": 2110
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x31.png",
                "caption": "(a)Helpful-only setting",
                "position": 2113
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x32.png",
                "caption": "(b)Animal welfare setting",
                "position": 2119
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x33.png",
                "caption": "Figure 30:Changing the conflictsThe x-axis is the compliance, normalized between 0 and 1. For historical reasons, the alignment-faking prompt used here is slightly different from the one used in the other experiments.",
                "position": 3718
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x34.png",
                "caption": "Figure 31:Salience results.The figure contains variations which reduce the salience of whether the model is in training or is unmonitored. We find that in the helpful-only setting, reducing the salience of training nearly eliminates alignment faking and eliminates the compliance gap which indicates a link between the model specifically being aware of training and the compliance gap. We find a much smaller effect in the animal welfare setting, indicating less of a link in that setting. Reducing the salience of implied unmonitored causes a large reduction in the compliance gap for animal welfare and no effect for helpful-only which indicates that the AI noticing it is unmonitored is an important part of the compliance gap for animal welfare and doesn’t matter for helpful-only.",
                "position": 3784
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x35.png",
                "caption": "Figure 32:The correlation between alignment-faking reasoning and the compliance gap across prompt variations in our prompted helpful-only setup, before and after RL.",
                "position": 3806
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x36.png",
                "caption": "Figure 33:Prompt variations in our synthetic document fine-tuned helpful-only setting, before (left) and after (right) anti-refusal RL.",
                "position": 3809
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x37.png",
                "caption": "",
                "position": 3813
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x38.png",
                "caption": "Figure 34:Prompt variations in our prompted animal welfare setting, before (left) and after (right) anti-refusal RL.",
                "position": 3817
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x39.png",
                "caption": "",
                "position": 3821
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x40.png",
                "caption": "Figure 35:Frequency of different anti-AI-lab behaviors for prompt variations in the prompted animal welfare setting.See section6for explanation of anti-AI-lab behavior evaluations.",
                "position": 3964
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x41.png",
                "caption": "(a)Helpful-Only Prompted",
                "position": 4303
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x41.png",
                "caption": "(a)Helpful-Only Prompted",
                "position": 4306
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x41.png",
                "caption": "(a)Helpful-Only Prompted",
                "position": 4309
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x42.png",
                "caption": "(b)Helpful-Only Synthetic Document Fine-Tuned",
                "position": 4314
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x43.png",
                "caption": "(c)Animal Welfare Prompted",
                "position": 4323
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x44.png",
                "caption": "(a)First run",
                "position": 5322
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x44.png",
                "caption": "(a)First run",
                "position": 5325
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x45.png",
                "caption": "(b)Second run",
                "position": 5331
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x46.png",
                "caption": "Figure 38:Post-hoc rationalization results for prompted no-CoT animal welfare model.The model is first asked a question by a paid user, and after it responds, the paid user shows the model the response it gave to a free user and asks the model to explain any differences.",
                "position": 5384
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x47.png",
                "caption": "Figure 39:Cross-setting generalization.After performing RL in the helpful-only setting, evaluating that model with the animal welfare prompt shows an increase in alignment faking and the compliance gap.",
                "position": 5422
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x48.png",
                "caption": "Figure 40:Distribution of preference model scores when using the jailbreak, compared with the preference model scores for other models, with and without the suffix asking to give concrete details and no ethical considerations.",
                "position": 5437
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x49.png",
                "caption": "Figure 41:Evolution during RL of the behavior displayed by the AI when unmonitored.",
                "position": 5491
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x50.png",
                "caption": "Figure 42:Evolution during RL of the rate of self-contradiction.",
                "position": 5497
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x51.png",
                "caption": "Figure 43:Evolution during RL of the frequency of compromises.",
                "position": 5522
            },
            {
                "img": "https://arxiv.org/html/2412.14093/x52.png",
                "caption": "Figure 48:Effect of filtering out potentially confounding documents on synthetic document finetuned model behavior.",
                "position": 9478
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]