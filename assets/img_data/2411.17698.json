[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17698/x1.png",
                "caption": "",
                "position": 187
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17698/x2.png",
                "caption": "Figure 2:Radar chart comparison for video-to-audio generation task.Each metric is normalized for a better visualization.",
                "position": 275
            },
            {
                "img": "https://arxiv.org/html/2411.17698/x3.png",
                "caption": "Figure 3:Method overview.We train our model jointly on a standard audio-video datasetVGGSoundfor VT2A generation and a high-quality audio-text datasetSound ideasfor T2A generation.\nWe encode the input audio into latents, adding noise to a portion of them. The silent video is encoded into visual features, concatenated with the audio latents along the channel dimension. The text input, including a quality tag, is encoded through a text encoder and applied via cross-attention.",
                "position": 288
            }
        ]
    },
    {
        "header": "3Multimodal Conditional Foley Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17698/x4.png",
                "caption": "Table 1:Evaluation of video-to-audio generation.Each method is evaluated on the VGGSound test set across six metrics assessing cross-modal alignment and audio quality. ImageBind and CLAP scores are reported in %. The unit of AV-Sync is seconds.DAC-VAEreconstructs the VGGSound audio and serves as an oracle baseline. The best results are inbold.",
                "position": 368
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17698/x4.png",
                "caption": "Table 2:Evaluation on the Foley generation with text controls.NegPdenotes negative prompting. We also include two oracle baselines: one using the true category as text prompts and the other omitting video during inference. The best results are inbold.",
                "position": 647
            },
            {
                "img": "https://arxiv.org/html/2411.17698/x4.png",
                "caption": "Figure 4:Qualitative examples for Foley generation with text control.We present generated results for two videos, each with three different text prompts, demonstrating our modelâ€™s ability to produce synchronized soundtracks with varied semantics through text control.Please refer to ourwebsitefor video results.",
                "position": 738
            },
            {
                "img": "https://arxiv.org/html/2411.17698/x5.png",
                "caption": "Figure 5:Qualitative results of quality control.We show that VGGSound audio has limited bandwidth and demonstrate our model generates full-band 48kHz audio with quality control.",
                "position": 988
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix A.1Implementation Details",
        "images": []
    },
    {
        "header": "Appendix A.2Additional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17698/x6.png",
                "caption": "Figure 6:Screenshot of Foley user study.We show the screenshot from our user study survey. We show the instructions and the first two video pair examples and associated questions.",
                "position": 2495
            }
        ]
    },
    {
        "header": "Appendix A.3Human Studies",
        "images": []
    }
]