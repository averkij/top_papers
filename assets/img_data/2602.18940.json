[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.18940/figures/Mascot-no-bg.png",
                "caption": "",
                "position": 138
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.18940/x1.png",
                "caption": "Figure 1:Capturing Overlooked Dimensions of Research Quality.DREAMactively verifies the reasoning of generated reports by probing external sources (left), detects factual errors injected in a controlled experiment (middle), and captures time-sensitive validity gaps by penalizing outdated reports (right).",
                "position": 176
            }
        ]
    },
    {
        "header": "2Deep Research Evaluation Landscape",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.18940/x2.png",
                "caption": "Figure 2:DREAMOverview.Our framework operates in two phases.Left:Protocol Creation, where query-independent Static Metrics are combined with Adaptive Metrics constructed by an agent equipped with web search tools and optional tools to access external data.Right:Protocol Execution, where each metric is routed to the appropriate evaluator, either an LLM, agent with tool access, or workflow.",
                "position": 436
            }
        ]
    },
    {
        "header": "3DREAM: DRE with Agentic Metrics",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.18940/x3.png",
                "caption": "Figure 3:DREAMProtocol Execution Evaluators.(a) LLM Evaluator assesses writing quality (WQ) and key-information coverage (KIC); (b) Agent Evaluator evaluates reasoning quality (RQ) using external tools; (c) Workflow Evaluator performs factuality assessment via evidence retrieval, citation integrity (CI) verification through claim-source validation, and domain authoritativeness (DA) scoring via credibility assessment of extracted citations.",
                "position": 481
            }
        ]
    },
    {
        "header": "4Validation ofDREAM",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.18940/x4.png",
                "caption": "Figure 4:Temporal Awareness in KIC Evaluation.Comparison of evaluation criteria for a TikTok legal status query, showing DeepResearch Bench’s static criteria (left) versus DREAM’s KIC criteria (right) that incorporate time-sensitive facts (e.g., mid-December 2025 joint venture deal and January 23, 2026 deadline).",
                "position": 634
            },
            {
                "img": "https://arxiv.org/html/2602.18940/x5.png",
                "caption": "Figure 5:Reasoning flaws detection.Relative score degradations between well-reasoned and malformed reports. DREAM–RQ centers around40.1%40.1\\%degradation, while RACE centers around9.1%9.1\\%, with several malformed reports outscoring well-reasoned ones.",
                "position": 650
            }
        ]
    },
    {
        "header": "5Benchmarking Leading DRAs",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExtended Related Work",
        "images": []
    },
    {
        "header": "Appendix BAgentic Taxonomy Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.18940/x6.png",
                "caption": "Figure 6:Agentic Taxonomy Sub-Dimension Clustering Visualization.Each point represents an evaluation criterion from one of four deep research benchmarks (shape), colored by its LLM-assigned taxonomy dimension.\nCriteria assigned to the same dimension tend to form localized regions in embedding space, suggesting coherent semantic groupings across the four\nverticals.",
                "position": 1198
            }
        ]
    },
    {
        "header": "Appendix CMetrics Details",
        "images": []
    },
    {
        "header": "Appendix DControlled Experiments Details",
        "images": []
    },
    {
        "header": "Appendix EAdditional Details of Human Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.18940/x7.png",
                "caption": "Figure 7:Factuality Label Distribution.Distribution of factuality judgments (Full Support, Partial Support, Contradict, Unverifiable) for each model across three datasets.",
                "position": 3033
            },
            {
                "img": "https://arxiv.org/html/2602.18940/x8.png",
                "caption": "Figure 8:Citation Faithfulness Label Distribution.Distribution of citation faithfulness labels assessing the alignment between claims and cited source text across three datasets.",
                "position": 3036
            },
            {
                "img": "https://arxiv.org/html/2602.18940/x9.png",
                "caption": "Figure 9:Citation Integrity Components.Citation Faithfulness and Citation Attribution visualized for each model across three benchmarks.",
                "position": 3039
            },
            {
                "img": "https://arxiv.org/html/2602.18940/x10.png",
                "caption": "Figure 10:Report length distributions across DRAs and datasets.Word count distributions across DRAs and datasets. Smolagents Open Deep Research consistently exhibits the highest variance and mean length, while LangChain Open Deep Research and Tongyi Deep Research are more concise and consistent.",
                "position": 3043
            }
        ]
    },
    {
        "header": "Appendix FDetailed DREAM Benchmarking Results",
        "images": []
    }
]