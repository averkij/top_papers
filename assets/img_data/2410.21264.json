[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.21264/x1.png",
                "caption": "Figure 1:LARP highlights.(a)LARP is a video tokenizer for two-stage video generative models. In the first stage, LARP tokenizer is trained with a lightweight AR prior model to learn an AR-friendly latent space. In the second stage, an AR generative model is trained on LARP’s discrete tokens to synthesize high-fidelity videos.(b)The incorporation of the AR prior model significantly improves the generation FVD (gFVD) across various token number configurations.(c)LARP shows a much smaller gap between its reconstruction FVD (rFVD) and generation FVD (gFVD), indicating the effectiveness of the optimized latent space it has learned.",
                "position": 89
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.21264/x2.png",
                "caption": "Figure 2:Method overview.Cubesrepresent video patches, circlesindicate continuous embeddings, and squaresdenote discrete tokens.(a) Patchwise video tokenizerused in previous works.(b) Left: The LARP tokenizertokenizes videos in a holistic scheme, gathering information from the video using a set of learned queries.Right: The AR prior model, trained with LARP , predicts the next holistic token, enabling a latent space optimized for AR generation. The AR prior model is forwarded in two rounds per iteration. Theredarrow represents the first round, and thepurplearrows represent the second round. The reconstruction lossℒrecsubscriptℒrec{\\mathcal{L}}_{\\text{rec}}caligraphic_L start_POSTSUBSCRIPT rec end_POSTSUBSCRIPTis omitted for simplicity.",
                "position": 198
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.21264/x3.png",
                "caption": "(a)Scaling LARP tokenizer sizes.",
                "position": 410
            },
            {
                "img": "https://arxiv.org/html/2410.21264/x3.png",
                "caption": "(a)Scaling LARP tokenizer sizes.",
                "position": 413
            },
            {
                "img": "https://arxiv.org/html/2410.21264/x4.png",
                "caption": "(b)Scaling Number of discrete tokens.",
                "position": 418
            },
            {
                "img": "https://arxiv.org/html/2410.21264/extracted/5960086/figures/vis_recon.png",
                "caption": "Figure 4:Video reconstruction comparisonwith OmniTokenizer(Wang et al.,2024).",
                "position": 633
            },
            {
                "img": "https://arxiv.org/html/2410.21264/extracted/5960086/figures/vis_ucf_gen.png",
                "caption": "Figure 5:Class-conditional video generation resultson the UCF-101 dataset using LARP.",
                "position": 640
            },
            {
                "img": "https://arxiv.org/html/2410.21264/extracted/5960086/figures/visualization_k600_fp.png",
                "caption": "Figure 6:Video frame prediction resultson the K600 dataset using LARP.",
                "position": 647
            }
        ]
    },
    {
        "header": "5Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.21264/extracted/5960086/figures/visualization_recon_supp.png",
                "caption": "Figure 7:Additional video reconstruction comparisonwith OmniTokenizer(Wang et al.,2024).",
                "position": 1736
            },
            {
                "img": "https://arxiv.org/html/2410.21264/extracted/5960086/figures/visualization_ucf_gen_supp.png",
                "caption": "Figure 8:Additional class-conditional generation resultson UCF-101 dataset.",
                "position": 1743
            },
            {
                "img": "https://arxiv.org/html/2410.21264/extracted/5960086/figures/visualization_k600_fp_supp.png",
                "caption": "Figure 9:Additional video frame prediction resultson K600 dataset.",
                "position": 1748
            }
        ]
    },
    {
        "header": "Appendix BAdditional Visualization Results",
        "images": []
    }
]