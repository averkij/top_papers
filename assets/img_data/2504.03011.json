[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03011/x1.png",
                "caption": "",
                "position": 95
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03011/x2.png",
                "caption": "Figure 2:Comparison of various baseline methods for relighting settings and functionalities.",
                "position": 121
            },
            {
                "img": "https://arxiv.org/html/2504.03011/x3.png",
                "caption": "Figure 3:Our model generalizes to various body parts (portrait, half-body, full-body, multiperson) for relighting and harmonization, with lighting control variables shown in the insets.",
                "position": 124
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03011/x4.png",
                "caption": "Figure 4:System overview.(a) Given an input image of humans with coarse lighting and background image, our diffusion model generates the relit images harmonized with background scenes (Sec.3.2). (b) The external temporal modules learn the temporal cycle consistency from many real-world videos to construct temporal lighting features (Sec.3.3). (c) In inference time, we blend the features from lighting and temporal modules spatially and temporally to enable coherent and generalizable human relighting (Sec.3.4).",
                "position": 173
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03011/x5.png",
                "caption": "Figure 5:Qualitative comparison of synthetic video frames (corresponding to Tab.1). From left to right: composite input with target lighting parameters (inset), our relit result, baseline methods, and normalized L2↓↓\\downarrow↓photometric error map (inset).",
                "position": 526
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03011/x6.png",
                "caption": "Figure 6:Comparison with LPBR[41]on DeepFashion[33]real images for background harmonization testing. The first row shows the relit output, and the second shows the magnified results.",
                "position": 567
            },
            {
                "img": "https://arxiv.org/html/2504.03011/x7.png",
                "caption": "Figure 7:Comparison with RHW[51]on Pexels[37]real images. The lighting control variables are shown as insets. While RHW produces reasonable relighting for full-body images, its quality degrades on half-body and multi-person cases.",
                "position": 570
            },
            {
                "img": "https://arxiv.org/html/2504.03011/x8.png",
                "caption": "Figure 8:Comparison with GFR[23]on Pexels[37]real images. The lighting control variables are shown as insets. Limited generalizability of GFR results in reduced output quality for half-body and multi-person cases.",
                "position": 573
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMethod and Experiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03011/x9.png",
                "caption": "Figure 9:Relighting and Harmonization diffusion model training and denoising pipeline.",
                "position": 2037
            },
            {
                "img": "https://arxiv.org/html/2504.03011/x10.png",
                "caption": "Figure 10:Left side: Training data scale comparisons; Right side: Breakdown of our training and evaluation dataset information.",
                "position": 2050
            },
            {
                "img": "https://arxiv.org/html/2504.03011/x11.png",
                "caption": "Figure 11:Left: Our shading estimation network, Right: Convolutional and deconvolutional blocks.",
                "position": 2110
            },
            {
                "img": "https://arxiv.org/html/2504.03011/x12.png",
                "caption": "Figure 12:Training samples of the relighting data with half-body portraits (up) and simulation data with full-body images (bottom) .",
                "position": 2113
            },
            {
                "img": "https://arxiv.org/html/2504.03011/x13.png",
                "caption": "Figure 13:Comparison with harmonization methods (IC-Light). Left side is multi-person testing, right side is zoom in result.",
                "position": 2162
            },
            {
                "img": "https://arxiv.org/html/2504.03011/x14.png",
                "caption": "Figure 14:Training pipeline for coarse lighting estimation network.",
                "position": 2169
            }
        ]
    },
    {
        "header": "Appendix BQualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03011/x15.png",
                "caption": "Figure 15:Qualitative comparisons conducted on synthetic data. From top to bottom: full-body testing, multi-person testing. The ground truth data is displayed in the last column.",
                "position": 2189
            },
            {
                "img": "https://arxiv.org/html/2504.03011/x16.png",
                "caption": "Figure 16:Comparison with DPR on face and half-body relighting on Pexels[37]real images.",
                "position": 2289
            },
            {
                "img": "https://arxiv.org/html/2504.03011/x17.png",
                "caption": "Figure 17:Our LigtStage data testing (Left) and comparison with other relighting baselines (Right).",
                "position": 2292
            },
            {
                "img": "https://arxiv.org/html/2504.03011/x18.png",
                "caption": "Figure 18:Strong shadow testing results (left) and failure cases (right) on real images from Pexels[37].",
                "position": 2295
            },
            {
                "img": "https://arxiv.org/html/2504.03011/x19.png",
                "caption": "Figure 19:Video relighting comparison results on synthetic testing data: from left to right, we show comparison results for Scenario 1, 2, 3. From top to bottom, the first row shows the composite input (foreground human albedo composited with background image), the second row shows the ground truth (GT) shading, and the third row shows the GT image.",
                "position": 2298
            },
            {
                "img": "https://arxiv.org/html/2504.03011/x20.png",
                "caption": "Figure 20:Real image comparisons with other human relighting approaches on the DeepFashion dataset[33]. We test on different identities and body parts (full body, half body). Our model shows consistent and feasible relighting with varying target lighting parameters (Spherical harmonics).",
                "position": 2301
            },
            {
                "img": "https://arxiv.org/html/2504.03011/x21.png",
                "caption": "Figure 21:We present real image comparisons with the harmonization method. Given a composite input image, our model can achieve effective harmonization. When provided with target lighting parameters (Spherical harmonics), our model can achieve both background harmonization and relighting. The top section displays the outputs of our background harmonization method compared to the results from[41]. The lower section presents harmonization and relighting comparisons with[23]. Due to the higher generative prior of LPBR, noticeable distortions are present on the human face. Although GFR can achieve both harmonization and relighting, it exhibits obvious color noise.",
                "position": 2304
            },
            {
                "img": "https://arxiv.org/html/2504.03011/x22.png",
                "caption": "Figure 22:User study results: Preferences between our model and other relighting and harmonization models, including our general object testing.",
                "position": 2328
            }
        ]
    },
    {
        "header": "Appendix CLimitation and future work",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03011/x23.png",
                "caption": "Figure 23:Our model can achieve realistic relighting with lighting 1 and background harmonization.",
                "position": 2377
            },
            {
                "img": "https://arxiv.org/html/2504.03011/x24.png",
                "caption": "Figure 24:Our model can achieve realistic relighting with lighting 2 and background harmonization.",
                "position": 2380
            },
            {
                "img": "https://arxiv.org/html/2504.03011/x25.png",
                "caption": "Figure 25:Our model can achieve realistic relighting with lighting 3 and background harmonization.",
                "position": 2383
            }
        ]
    },
    {
        "header": "Appendix DBroader Impact",
        "images": []
    }
]