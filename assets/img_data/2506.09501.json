[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09501/x1.png",
                "caption": "Figure 1:Left:Under BF16 precision and greedy decoding, the modelâ€™s output can vary significantly depending on factors such as GPU count, evaluation batch size, and GPU hardware version.Right:For example, changes in evaluation batch size alone can lead to noticeable differences in responses, which is often ignored and not standardized by evaluation benchmarks.",
                "position": 98
            },
            {
                "img": "https://arxiv.org/html/2506.09501/x2.png",
                "caption": "",
                "position": 107
            }
        ]
    },
    {
        "header": "2Preliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09501/x3.png",
                "caption": "Figure 2:Floating-point format of FP32, FP16 and BF16.",
                "position": 182
            }
        ]
    },
    {
        "header": "3Reproducibility Issues with Limited Numerical Precision",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09501/x4.png",
                "caption": "Figure 3:Left:the top-5 tokens and their predicted probabilities at the divergence index for two different answers to the same question in BF16.Right:The gap between the top-two competing tokens probability.\nWe observe the token probability gap are often minimal in reasoning models.",
                "position": 485
            },
            {
                "img": "https://arxiv.org/html/2506.09501/x5.png",
                "caption": "Figure 4:Avg_Std@top1_prob across 12 different settings for 4 models and 5 tasks, under BF16, FP16 and FP32. FP16 shows significantly lower variance compared to BF16. FP32 yields near-zero variance, demonstrating strong robustness to floating-point rounding errors.",
                "position": 563
            },
            {
                "img": "https://arxiv.org/html/2506.09501/x6.png",
                "caption": "Figure 5:Distribution of Div_Index for DeepSeek-R1-Distill-Qwen-7B on MATH500 under BF16, FP16, and FP32. Higher numerical precision leads to much fewer divergent examples and a shift of divergence onset to later token positions.",
                "position": 570
            },
            {
                "img": "https://arxiv.org/html/2506.09501/x7.png",
                "caption": "(a)",
                "position": 669
            },
            {
                "img": "https://arxiv.org/html/2506.09501/x7.png",
                "caption": "(a)",
                "position": 672
            },
            {
                "img": "https://arxiv.org/html/2506.09501/x8.png",
                "caption": "(b)",
                "position": 677
            },
            {
                "img": "https://arxiv.org/html/2506.09501/x9.png",
                "caption": "(c)",
                "position": 682
            }
        ]
    },
    {
        "header": "4Near-Perfect Deterministic Reproduction: LayerCast",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09501/x10.png",
                "caption": "Figure 7:Workflow of LayerCast within one transformer block. Here we omit skip connections, position embeddings and activation functions.",
                "position": 707
            },
            {
                "img": "https://arxiv.org/html/2506.09501/x11.png",
                "caption": "Figure 8:Distribution of Div_Index for DeepSeek-R1-Distill-Qwen-7B on MATH500 under BF16, FP32, and LayerCast.",
                "position": 710
            }
        ]
    },
    {
        "header": "5Related Works",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitations",
        "images": []
    },
    {
        "header": "Appendix BBroader Impacts",
        "images": []
    },
    {
        "header": "Appendix CPrompt Formats Used in Our Evaluation",
        "images": []
    },
    {
        "header": "Appendix DSupplementary Results on Accuracy Variance under BFloat16",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09501/x12.png",
                "caption": "Figure 9:Accuracy varies significantly across different settings under BFloat16.",
                "position": 1372
            },
            {
                "img": "https://arxiv.org/html/2506.09501/x12.png",
                "caption": "",
                "position": 1375
            },
            {
                "img": "https://arxiv.org/html/2506.09501/x13.png",
                "caption": "",
                "position": 1379
            },
            {
                "img": "https://arxiv.org/html/2506.09501/x14.png",
                "caption": "",
                "position": 1384
            },
            {
                "img": "https://arxiv.org/html/2506.09501/x15.png",
                "caption": "",
                "position": 1388
            }
        ]
    },
    {
        "header": "Appendix ESupplementary Results on Greedy Decoding",
        "images": []
    },
    {
        "header": "Appendix FSupplementary Results on Ablation Study",
        "images": []
    },
    {
        "header": "Appendix GSupplementary Results on LayerCast",
        "images": []
    },
    {
        "header": "Appendix HSupplementary Results on Random Sampling",
        "images": []
    }
]