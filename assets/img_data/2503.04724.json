[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.04724/x1.png",
                "caption": "Figure 2:Overview of the proposed architecture. Text from the LLM is tokenized via a ByT5-based Grapheme-to-Phoneme(G2P) model, producing byte-level phoneme embeddings (teal). These are concatenated with the previous speech tokenâ€™s feature vector (blue), L2-normalized, and fed into a decoder-only Transformer to generate the next token. A neural codec (WavTokenizer) decoder (orange) reconstructs speech every n speech tokens predicted.",
                "position": 199
            }
        ]
    },
    {
        "header": "4Streaming Inference",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.04724/x2.png",
                "caption": "Figure 3:Overview of our streaming inference pipeline. Two replica TTS modules process text in parallel from two queues and place them into two producer queues.",
                "position": 344
            }
        ]
    },
    {
        "header": "5Experimental Settings",
        "images": []
    },
    {
        "header": "6Results and Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.04724/x3.png",
                "caption": "Figure 4:Human evaluation: Comparing with Freeze-Omni on Answer Relevance and Speech Quality.",
                "position": 531
            },
            {
                "img": "https://arxiv.org/html/2503.04724/x4.png",
                "caption": "Figure 5:Breakdown of average end-to-end latency (in milliseconds) at a chunk size of 40 for a single query.",
                "position": 545
            },
            {
                "img": "https://arxiv.org/html/2503.04724/x5.png",
                "caption": "Figure 6:Effect of chunk size on WER, CER, UTMOS, and latency. Larger chunks enhance speech quality and reduce error rates.",
                "position": 552
            },
            {
                "img": "https://arxiv.org/html/2503.04724/x6.png",
                "caption": "",
                "position": 556
            },
            {
                "img": "https://arxiv.org/html/2503.04724/x7.png",
                "caption": "",
                "position": 559
            },
            {
                "img": "https://arxiv.org/html/2503.04724/x8.png",
                "caption": "",
                "position": 560
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "9Appendix",
        "images": []
    }
]