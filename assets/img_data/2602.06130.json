[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06130/x1.png",
                "caption": "Figure 1:InSWIRL(Self-improving World modelling with Iterative RL), we facilitate the world modelling ability of foundation models (LLMs and VLMs) by modelling two components: Forward World Model (FWM)Pθ​(y∣x,z)P_{\\theta}(y\\mid x,z)and Inverse Dynamics Model (IDM)Qϕ​(z∣x,y)Q_{\\phi}(z\\mid x,y). These components are iteratively optimised through RL (specifically, GRPO) in two distinct phases: I) the FDM acts as a policy and the IDM as a reward to ensure identifiability between actions and next states; II) the IDM acts as a policy and the FDM as a reward to ensure data fidelity to the state-only sequences. The KL term is omitted from the figure for simplicity.",
                "position": 150
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06130/x2.png",
                "caption": "Figure 2:Performance of FWM and IDM in each iteration ofSWIRL. We visualise the training dynamics across iterations for two settings: maintaining separate weights (Left) versus sharing parameters (Right).\nWe present the evaluation performance on top, and the training rewards in the bottom. We present only the first iteration’s reward curves for brevity.",
                "position": 1494
            },
            {
                "img": "https://arxiv.org/html/2602.06130/x2.png",
                "caption": "",
                "position": 1497
            },
            {
                "img": "https://arxiv.org/html/2602.06130/x3.png",
                "caption": "",
                "position": 1502
            },
            {
                "img": "https://arxiv.org/html/2602.06130/x4.png",
                "caption": "Figure 3:We compare our proposedSWIRLagainst SFT baselines (continual training and merging) across five benchmarks inAurora-Bench. The x-axis represents the number of training samples. Our method (solid blue line) demonstrates superior data efficiency, achieving higher GPT-4o evaluation scores.",
                "position": 1521
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATheoretical Derivation",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    },
    {
        "header": "Appendix CSanity Check Results for General Image Editing.",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06130/x5.png",
                "caption": "Figure 5:Qualitative ofSWIRLfor general image editing including Subject’s add/replace/remove, style transfer, altering the colour or background.",
                "position": 2566
            }
        ]
    },
    {
        "header": "Appendix DIDM Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06130/x6.png",
                "caption": "Figure 6:Evolution of Latent Action Space throughoutSWIRL.We analyse the semantic properties of generated actions across iterations.Uniquenessindicates the percentage of distinct actions in the generated set.Lengthdenotes average token count.PPL Ratiodenotes the ratio of predicted perplexity from an LLM (specifically, GPT-2) on the predicted action against the ground-truth action, which measures the naturalness of the actions in language form. DuringSWIRL, the model maintains high diversity and naturalness without collapsing into short ciphers.",
                "position": 2643
            }
        ]
    },
    {
        "header": "Appendix EInference-time Verification",
        "images": []
    },
    {
        "header": "Appendix FDetailed Results for Comparing SFT andSWIRL.",
        "images": []
    },
    {
        "header": "Appendix GDetailed Results forWorldPredictionBench",
        "images": []
    },
    {
        "header": "Appendix HDetailed Results for Iterative Results",
        "images": []
    },
    {
        "header": "Appendix IAblation on GRPO Rollout Size.",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06130/x7.png",
                "caption": "Figure 7:Qualitative examples produced bySWIRLforAurora-Bench,ByteMorphandWorldPredictionBench. We present the sample for each subset in these benchmarks.",
                "position": 3566
            }
        ]
    },
    {
        "header": "Appendix JQualitative Examples",
        "images": []
    }
]