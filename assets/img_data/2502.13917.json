[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13917/x1.png",
                "caption": "Figure 1:Overview of inference with\\modelname. We provide a chat-template formatted query and iteratively denoise over the simplex space for a number of diffusion steps (typically 100). Optionally, we also incorporate reward guidance at each diffusion step by applying the gradient of the predicted reward to the intermediate logits, i.e., gradient ascent on reward.",
                "position": 175
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3\\modelname",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13917/x2.png",
                "caption": "Figure 2:Train loss during adaptation training when adapting from different models. We find Mistral achieves the lowest overall loss during training, even compared to newer LMs such as Llama 3.",
                "position": 570
            },
            {
                "img": "https://arxiv.org/html/2502.13917/x3.png",
                "caption": "(a)AlpacaEval performance against reward guidance weight. Increasing guidance weight initially improves, and then degrades performance.",
                "position": 805
            },
            {
                "img": "https://arxiv.org/html/2502.13917/x3.png",
                "caption": "(a)AlpacaEval performance against reward guidance weight. Increasing guidance weight initially improves, and then degrades performance.",
                "position": 808
            },
            {
                "img": "https://arxiv.org/html/2502.13917/x4.png",
                "caption": "(b)AlpacaEval and GSM8k performance using increasing diffusion steps at inference time. Performance increases with number of steps up to a point.",
                "position": 813
            },
            {
                "img": "https://arxiv.org/html/2502.13917/x5.png",
                "caption": "(c)AlpacaEval winrate against number of diffusion adaptation steps. Going up to 200k steps provides significant improvements.",
                "position": 818
            }
        ]
    },
    {
        "header": "5Analysis",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AAdaptation Training Details",
        "images": []
    },
    {
        "header": "Appendix BBaseline Details",
        "images": []
    },
    {
        "header": "Appendix CReward Model Training",
        "images": []
    },
    {
        "header": "Appendix DDownstream Evaluation Details",
        "images": []
    },
    {
        "header": "Appendix EModel Prediction Visualization",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13917/x6.png",
                "caption": "Figure 4:Confidence over diffusion steps for\\modelnamewith the prompt ‘When I talk about music, I.’ Backward diffusion time flows from top to bottom. At about 60 diffusion steps, the sequence is more or less determined. Note we set confidence to 1 for prompt (leftmost) tokens.",
                "position": 1944
            }
        ]
    },
    {
        "header": "Appendix FExample Reasoning Generations",
        "images": []
    },
    {
        "header": "Appendix GExample Generations",
        "images": []
    }
]