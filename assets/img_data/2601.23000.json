[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.23000/x1.png",
                "caption": "(a)One day pretraining experiment of LLaMA-350M and -1.3B models on thePiledataset for1010B and2.82.8B tokens respectively. Our proposed optimizer Mano achieves1.75×1.75\\timesand1.38×1.38\\timesthe convergence speed of Muon in terms of wall-clock time. This advantage is expected to further increase with reduced computational overhead and faster convergence speed.",
                "position": 142
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Methodology",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.23000/x1.png",
                "caption": "(a)LLaMA-350M and -1.3B models trained on theC4/enandPiledataset for1000010000steps with three different optimizers: AdamW, Muon, and Mano. Mano demonstrated a faster convergence speed than both popular optimizers with the simplest implementation and computational cost.",
                "position": 465
            },
            {
                "img": "https://arxiv.org/html/2601.23000/x1.png",
                "caption": "(b)Qwen3-0.6B and -1.7B models trained on thePiledataset for1000010000steps with three different optimizers: AdamW, Muon, and Mano. The performance advantage of Mano is model-transferrable.",
                "position": 472
            },
            {
                "img": "https://arxiv.org/html/2601.23000/x1.png",
                "caption": "(a)LLaMA-130130M and -350350M models trained on thePiledataset for1010B tokens. We demonstrated that with data scaling, Mano consistently performed better than Muon and AdamW in the ultimate convergence speed.",
                "position": 539
            },
            {
                "img": "https://arxiv.org/html/2601.23000/x1.png",
                "caption": "(b)The average (a) Gradient norm, (b) Gradient variance, and (c) Gradient Signal-to-Noise Ratio (SNR) of LLaMA-350350M model parameters trained on thePiledataset. The SNR is calculated as the norm-to-variance ratio. As an indicator of internal training dynamics, Mano exhibits lower gradient variance and a higher SNR than Muon, both under the same momentum coefficientμ=0.95\\mu=0.95.",
                "position": 553
            },
            {
                "img": "https://arxiv.org/html/2601.23000/x1.png",
                "caption": "(c)The spectral distributions of (a) all attention layers and (b) all MLP layers from an LLaMA-350350M model at the10001000step on theC4/encorpus, including the model gradient, momentum, and the update matrix of AdamW, Muon, and Mano. The manifold normalization of Mano may also be viewed as an efficient spectral regularization method that lifted the update spectra while preserving the singular values’ original ordering.",
                "position": 562
            },
            {
                "img": "https://arxiv.org/html/2601.23000/x1.png",
                "caption": "Figure 8:Comparing conventional Riemannian SGD-M and Mano on LLaMA-350350M models trained on thePiledataset. Unlike traditional manifold optimization methods that impose constraints on model parameters and expressivity during training, Mano provides a more flexible approach and superior performance.",
                "position": 680
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImpact Statement",
        "images": []
    },
    {
        "header": "Appendix BDetails for Reproducibility",
        "images": []
    },
    {
        "header": "Appendix CMano for General Tensor",
        "images": []
    },
    {
        "header": "Appendix DRelationship to Existing Optimizers",
        "images": []
    },
    {
        "header": "Appendix EProofs",
        "images": []
    }
]