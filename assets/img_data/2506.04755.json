[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04755/x1.png",
                "caption": "Figure 1:Comparison of (a) accuracy under varying training dataset sizes and (b) performanceefficiency trade-offs on various methods.",
                "position": 76
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04755/x2.png",
                "caption": "Figure 2:Illustrative examples for two ineffective training sample types: (a) language-prior biased samples and (b) attention-biased samples.",
                "position": 94
            },
            {
                "img": "https://arxiv.org/html/2506.04755/x3.png",
                "caption": "Figure 3:The overall pipeline of our RAP method. First, the Causal Discrepancy Estimator (CDE) filters out samples that overly rely on language priors via output-level discrepancy. Then, the Attention Confidence Estimator (ACE) excludes attention-biased samples by token-level attention distributions. Finally, the Difficulty-aware Replacement Module (DRM) selectively replaces trivial instances with cognitively challenging ones, yielding a refined subset of cognitive samples.",
                "position": 128
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04755/x4.png",
                "caption": "Figure 4:Cross-model generalization of cognitive samples selected by RAP. Performance with InternVL3-2B trained on samples from Qwen2.5-VL-3B (left), and vice versa (right).",
                "position": 839
            },
            {
                "img": "https://arxiv.org/html/2506.04755/x5.png",
                "caption": "Figure 5:(a) Visualization of output discrepancies between multi-modal and text-only inputs on the full MM-Eureka training dataset. (b) Performance variation with respect to the hyperparameters位asubscript\\lambda_{a}italic_位 start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPTand位csubscript\\lambda_{c}italic_位 start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPTon MMstar. (c) Comparative analysis of multi-modal reasoning utilization on four datasets.",
                "position": 873
            },
            {
                "img": "https://arxiv.org/html/2506.04755/x6.png",
                "caption": "Figure 6:Illustration of (a) characteristics of cognitive samples selected by our RAP method and (b) Comparison of the reasoning processes between our RAP method and the state-of-the-art LIMRlimr.",
                "position": 876
            },
            {
                "img": "https://arxiv.org/html/2506.04755/x7.png",
                "caption": "Figure 7:(a) Trade-off analysis between efficiency and performance and effect of data selection on upper bound. (b-c) Impact of varying the proportion of samples with different difficulty levels on reasoning. (c) Comparison with our RAP method and RAP augmented with dynamic selection.",
                "position": 893
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]