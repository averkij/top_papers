[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Optimizing Computational and Memory Costs in GQA-based Transformers",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09579/x1.png",
                "caption": "Figure 1:The main results. It shows LM loss as a function of memory and computational costs during inference when the context length is 128K tokens, assuming we use BF16 for both model parameters and KV cache.H=(nh,nk‚Å¢v)ùêªsubscriptùëõ‚Ñésubscriptùëõùëòùë£H=(n_{h},n_{kv})italic_H = ( italic_n start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_n start_POSTSUBSCRIPT italic_k italic_v end_POSTSUBSCRIPT )denotes the number of attention heads and KV heads. The x-axis is on the log scale. The result shows that the widely used GQA configuration (H=(d/dh,8)ùêªùëësubscriptùëë‚Ñé8H=(d/d_{h},8)italic_H = ( italic_d / italic_d start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , 8 )) is suboptimal.",
                "position": 410
            },
            {
                "img": "https://arxiv.org/html/2503.09579/x2.png",
                "caption": "Figure 2:Loss as a function of inference costs with a context length of 128K. Assuming we use BF16 for both parameters and the KV cache.H=(nh,nk‚Å¢v)ùêªsubscriptùëõ‚Ñésubscriptùëõùëòùë£H=(n_{h},n_{kv})italic_H = ( italic_n start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_n start_POSTSUBSCRIPT italic_k italic_v end_POSTSUBSCRIPT )denotes the attention head configuration.nhsubscriptùëõ‚Ñén_{h}italic_n start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPTandnk‚Å¢vsubscriptùëõùëòùë£n_{kv}italic_n start_POSTSUBSCRIPT italic_k italic_v end_POSTSUBSCRIPTaffects memory and computational cost differently.",
                "position": 413
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09579/x3.png",
                "caption": "Figure 3:Loss as a function of memory and computational costs, aligned by training cost (FLOPs) at 128K tokens. Models with the same number of layers are trained with the same amount of training compute.",
                "position": 479
            },
            {
                "img": "https://arxiv.org/html/2503.09579/x4.png",
                "caption": "Figure 4:The relationship between LM loss and the number of attention heads, fitted with a power-plus-constant function.",
                "position": 500
            },
            {
                "img": "https://arxiv.org/html/2503.09579/x5.png",
                "caption": "Figure 5:The relationship between LM loss and the number of attention heads, fitted with a power-plus-constant function. The model hasL=12ùêø12L=12italic_L = 12,d=1536ùëë1536d=1536italic_d = 1536.",
                "position": 503
            },
            {
                "img": "https://arxiv.org/html/2503.09579/x6.png",
                "caption": "Figure 6:The relationship between loss andnhsubscriptùëõ‚Ñén_{h}italic_n start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPTwhennk‚Å¢vsubscriptùëõùëòùë£n_{kv}italic_n start_POSTSUBSCRIPT italic_k italic_v end_POSTSUBSCRIPTis constant, for two model sizes.",
                "position": 570
            },
            {
                "img": "https://arxiv.org/html/2503.09579/x7.png",
                "caption": "Figure 7:Loss as a function of the context length when using different numbers of heads, for two model sizes.",
                "position": 595
            },
            {
                "img": "https://arxiv.org/html/2503.09579/x8.png",
                "caption": "Figure 8:The loss curves of a model with 2K context length adapted to 64K through post-training compared to a model trained with 64K from scratch.",
                "position": 608
            }
        ]
    },
    {
        "header": "5Discussions",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ANotations",
        "images": []
    },
    {
        "header": "Appendix BMore Discussions",
        "images": []
    },
    {
        "header": "Appendix CModel Configurations",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09579/x9.png",
                "caption": "Figure 9:The proportion of FLOPs allocated to different components in a Transformer LM, with multi-head attention and RoPE. As the context lengths increase, most FLOPs are spent on the non-parametric computation of the attention operatorœÉ‚Å¢(Q‚Å¢KT)‚Å¢VùúéùëÑsuperscriptùêæùëáùëâ\\sigma(QK^{T})Vitalic_œÉ ( italic_Q italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ) italic_V, whereœÉùúé\\sigmaitalic_œÉis the row-wise softmax function.",
                "position": 1479
            },
            {
                "img": "https://arxiv.org/html/2503.09579/x10.png",
                "caption": "",
                "position": 1488
            },
            {
                "img": "https://arxiv.org/html/2503.09579/x11.png",
                "caption": "",
                "position": 1493
            },
            {
                "img": "https://arxiv.org/html/2503.09579/x12.png",
                "caption": "Figure 10:The proportion of memory allocated to different components in a Transformer LM, with multi-head attention and RoPE. As the context lengths increase, most of the memory usage is spent on storing the KV cache.",
                "position": 1499
            },
            {
                "img": "https://arxiv.org/html/2503.09579/x13.png",
                "caption": "",
                "position": 1508
            },
            {
                "img": "https://arxiv.org/html/2503.09579/x14.png",
                "caption": "",
                "position": 1513
            }
        ]
    },
    {
        "header": "Appendix DTraining Configurations",
        "images": []
    },
    {
        "header": "Appendix EData Processing",
        "images": []
    },
    {
        "header": "Appendix FMemory and FLOPs Allocations by Model Size",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09579/x15.png",
                "caption": "Figure 11:Loss as a function of memory and computational costs during inference with acontext length of 2K tokens.",
                "position": 1661
            },
            {
                "img": "https://arxiv.org/html/2503.09579/x16.png",
                "caption": "Figure 12:Loss as a function of memory and computational costs during inference with acontext length of 8K tokens.",
                "position": 1664
            },
            {
                "img": "https://arxiv.org/html/2503.09579/x17.png",
                "caption": "Figure 13:Loss as a function of memory and computational costs during inference with acontext length of 32K tokens.",
                "position": 1667
            },
            {
                "img": "https://arxiv.org/html/2503.09579/x18.png",
                "caption": "Figure 14:Loss as a function of memory and computational costs during inference with acontext length of 512K tokens.",
                "position": 1670
            },
            {
                "img": "https://arxiv.org/html/2503.09579/x19.png",
                "caption": "Figure 15:Loss as a function of memory and computational costs during inference with acontext length of 8K tokens.",
                "position": 1679
            },
            {
                "img": "https://arxiv.org/html/2503.09579/x20.png",
                "caption": "Figure 16:Loss as a function of memory and computational costs during inference with acontext length of 32K tokens.",
                "position": 1682
            },
            {
                "img": "https://arxiv.org/html/2503.09579/x21.png",
                "caption": "Figure 17:Loss as a function of memory and computational costs during inference with acontext length of 512K tokens.",
                "position": 1685
            }
        ]
    },
    {
        "header": "Appendix GMore Result: Loss VS. Inference Costs",
        "images": []
    }
]