[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19433/x1.png",
                "caption": "Figure 1:Effect of action horizon onπ0\\pi_{0}. The first55actions in the predicted chunk are executed at evaluation. Varying horizons lead to trade-off effects across four LIBERO task suites. Our mixture of horizons strategy alleviates this trade-off and raises overall success.",
                "position": 99
            },
            {
                "img": "https://arxiv.org/html/2511.19433/x2.png",
                "caption": "Figure 2:Overview of the proposed mixture of horizons strategy, which integrates action chunks of multiple horizons via the shared action transformer and a mixture gating mechanism.",
                "position": 102
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19433/x3.png",
                "caption": "Figure 3:Overview of our mixture of horizons framework. The action-related input is rearranged into different horizons and then processed in parallel by a shared action transformer.\nA linear gate head, with only2​k2kparameters, produces per-step, per-horizon weights to fuse horizon-wise predictions into the final action predictions.\nThis strategy is plug-and-play for any full-attention action transformer, including both flow-matching and one-step policies.",
                "position": 262
            },
            {
                "img": "https://arxiv.org/html/2511.19433/x4.png",
                "caption": "Figure 4:Comparisons with state-of-the-art methods on RoboTwin 2.0 Benchmark.",
                "position": 766
            }
        ]
    },
    {
        "header": "4Simulation Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19433/x5.png",
                "caption": "Figure 5:Visualization of horizon weights ofπ0.5\\pi_{0.5}with MoH on LIBERO-Long task suite.\nThe regulation termLb​a​lL_{bal}encourages the distribution balance across horizons. WithoutLb​a​lL_{bal}, the gating weights present obvious distribution preference at all times. The weights ofH​3H3drop to0at steps44and55as it is no longer active.",
                "position": 1037
            },
            {
                "img": "https://arxiv.org/html/2511.19433/x6.png",
                "caption": "Figure 6:Visualization of the overhead under different horizon settings.\nSince the action transformer is typically lightweight, and combined with tensor parallelism, MoH incurs very little additional overhead for both training and inference.",
                "position": 1054
            },
            {
                "img": "https://arxiv.org/html/2511.19433/x7.png",
                "caption": "Figure 7:Example of dynamic inference on LIBERO-Long.π0.5\\pi_{0.5}with MoH runs dynamic inference with scaling ratior=1.1r=1.1.\nAfter each action chunk prediction, only the prefix actions with horizon consensus are executed.\nShorter chunks are selected near decision points and fine-grained manipulation, whereas longer chunks are used during smooth, low-risk motions.",
                "position": 1068
            },
            {
                "img": "https://arxiv.org/html/2511.19433/x8.png",
                "caption": "Figure 8:Dynamic inference v.s. fixed-length prefix.A.Sis abbreviation of average action step number.",
                "position": 1094
            },
            {
                "img": "https://arxiv.org/html/2511.19433/x9.png",
                "caption": "Figure 9:Experimental settings and results in real-world scenarios.",
                "position": 1098
            }
        ]
    },
    {
        "header": "5Real-world Experiments",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix ATraining hyperparameters",
        "images": []
    },
    {
        "header": "Appendix BDesign ofπr​e​g\\pi_{reg}",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19433/x10.png",
                "caption": "Figure 10:Illustration of our designedπr​e​g\\pi_{reg}, with little modification based onπ0\\pi_{0}. We introduce a learnable query token as query input for action transformer. Actions are predicted in one forward pass.",
                "position": 1225
            }
        ]
    },
    {
        "header": "Appendix CMore Study on Horizon Effect",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19433/x11.png",
                "caption": "Figure 11:Typical failure modes in LIBERO.",
                "position": 1290
            },
            {
                "img": "https://arxiv.org/html/2511.19433/x12.png",
                "caption": "Figure 12:Challenges and issues exist in RoboTwin simulator.",
                "position": 1294
            }
        ]
    },
    {
        "header": "Appendix DChallenge and Failure Case Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19433/x13.png",
                "caption": "Figure 13:Qualitative demonstrations ofπ0.5\\pi_{0.5}with MoH on LIBERO and real-world tasks.",
                "position": 1344
            },
            {
                "img": "https://arxiv.org/html/2511.19433/x14.png",
                "caption": "Figure 14:Qualitative demonstrations ofπ0.5\\pi_{0.5}with MoH on RoboTwin 2.0.\nThe last four lines are collected underrandomsettings.",
                "position": 1349
            }
        ]
    },
    {
        "header": "Appendix EDemonstrations",
        "images": []
    }
]