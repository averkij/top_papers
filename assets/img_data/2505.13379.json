[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13379/x1.png",
                "caption": "Figure 1:Thinkless learns a hybrid LLM capable of adaptively selecting between thinking and non-thinking inference modes, directed by two special tokens,<think>and<short>. At the core of our method is a Decoupled Group Relative Policy Optimization, which decomposes and balances the mode selection on the control token and accuracy improvement on the response tokens.",
                "position": 99
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13379/x2.png",
                "caption": "Figure 2:ThinkLess trains a hybrid model that adaptively selects reasoning modes based on task complexity and model capacity. The process begins with distillation, enabling the model to follow control tokens (<think>or<short>) for guided reasoning. This is followed by reinforcement learning using Decoupled GRPO, which separates training into two objectives: optimizing the control token for effective mode selection and refining the response to improve answer accuracy.",
                "position": 143
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13379/x3.png",
                "caption": "(a)Vanilla GRPO",
                "position": 494
            },
            {
                "img": "https://arxiv.org/html/2505.13379/x3.png",
                "caption": "(a)Vanilla GRPO",
                "position": 497
            },
            {
                "img": "https://arxiv.org/html/2505.13379/x4.png",
                "caption": "(b)The proposed Decoupled GRPO, with a U-shape learning curve.",
                "position": 499
            },
            {
                "img": "https://arxiv.org/html/2505.13379/x4.png",
                "caption": "",
                "position": 502
            },
            {
                "img": "https://arxiv.org/html/2505.13379/x5.png",
                "caption": "",
                "position": 506
            },
            {
                "img": "https://arxiv.org/html/2505.13379/x6.png",
                "caption": "",
                "position": 510
            },
            {
                "img": "https://arxiv.org/html/2505.13379/x7.png",
                "caption": "(c)Collapsed Policy",
                "position": 524
            },
            {
                "img": "https://arxiv.org/html/2505.13379/x8.png",
                "caption": "(d)Learned Policy of DeGRPO",
                "position": 529
            },
            {
                "img": "https://arxiv.org/html/2505.13379/x9.png",
                "caption": "Figure 4:A large token loss coefficientŒ±ùõº\\alphaitalic_Œ±accelerates the shift in reasoning behavior, leading to the rapid emergence of all-correct short-mode samples.",
                "position": 557
            },
            {
                "img": "https://arxiv.org/html/2505.13379/x10.png",
                "caption": "Figure 5:Distribution of the model‚Äôs probability of emitting<think>on MATH-500. The samples with the highest, medium, and lowest probabilities are highlighted. The example with almost 0 thinking score mainly involves straightforward computation, and the query with 1.0 probability relies more on understanding and logical reasoning. More examples and LLM responses can be found in the appendix.",
                "position": 726
            }
        ]
    },
    {
        "header": "5Limitations and Future Works",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]