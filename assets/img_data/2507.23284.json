[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23284/x1.png",
                "caption": "Figure 1:(a) provides an overview of BLiM for video-to-text retrieval, which leverages the bidirectional likelihood estimation with a query and candidate likelihoods to mitigatecandidate prior bias.\n(b) Candidate likelihood estimation tends to prioritize long and repetitive text with high prior probability.\nIn contrast, bidirectional likelihood estimation of BLiM, effectively selects the most relevant text.",
                "position": 114
            }
        ]
    },
    {
        "header": "2Candidate Prior Bias",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23284/x2.png",
                "caption": "Figure 2:In video-to-text retrieval, similarities between queries and candidates using (a) candidate likelihoodP‚Äã(ùê≠|ùêØ)P(\\mathbf{t}|\\mathbf{v})italic_P ( bold_t | bold_v )and (b) query likelihoodP‚Äã(ùêØ|ùê≠)P(\\mathbf{v}|\\mathbf{t})italic_P ( bold_v | bold_t )are provided.\n(c) shows the candidate priorP‚Äã(ùê≠)P(\\mathbf{t})italic_P ( bold_t )for each text.\nTo reduce visual clutter, 50 text-video pairs are sampled.\nBased on the candidate priorP‚Äã(ùê≠)P(\\mathbf{t})italic_P ( bold_t ), the 24th text (highlighted in red) exhibits the highest prior probability in (c).\nWhile the query videos correctly retrieve their corresponding text using query likelihoodP‚Äã(ùêØ|ùê≠)P(\\mathbf{v}|\\mathbf{t})italic_P ( bold_v | bold_t )in (b), as indicated by the high similarity in diagonal elements, the text with the highest prior probability (red box) is frequently retrieved for irrelevant videos (374 out of 1,003) when using candidate likelihoodP‚Äã(ùê≠|ùêØ)P(\\mathbf{t}|\\mathbf{v})italic_P ( bold_t | bold_v )in (a).",
                "position": 261
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23284/x3.png",
                "caption": "Figure 3:Overall architecture.(a) illustrates BLiM, which jointly maximizes bothP‚Äã(ùê≠|ùêØ)P(\\mathbf{t}|\\mathbf{v})italic_P ( bold_t | bold_v )andP‚Äã(ùêØ|ùê≠)P(\\mathbf{v}|\\mathbf{t})italic_P ( bold_v | bold_t ).\n(b) presents the attention masks used for estimating likelihoods and prior probabilities.\nTo compute prior probabilities, attention masking is applied to all tokens of the input modality while generating the output modality.",
                "position": 277
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23284/x4.png",
                "caption": "Figure 4:A retrieval example in video-to-text retrieval on DiDeMo.Green indicates a correct prediction, while red denotes an incorrect one.\nRepeated phrases are highlighted in red.",
                "position": 1339
            },
            {
                "img": "https://arxiv.org/html/2507.23284/x5.png",
                "caption": "Figure 5:A qualitative example of CPN decoding on MVBench.Green signifies the accurate prediction, while red denotes the incorrect prediction.‚Ä†\\dagger‚Ä†indicates the model with CPN decoding.",
                "position": 1452
            }
        ]
    },
    {
        "header": "5Related Works",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ADataset Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23284/x6.png",
                "caption": "(a)Video-to-Text Retrieval.",
                "position": 2679
            },
            {
                "img": "https://arxiv.org/html/2507.23284/x6.png",
                "caption": "(a)Video-to-Text Retrieval.",
                "position": 2682
            },
            {
                "img": "https://arxiv.org/html/2507.23284/x7.png",
                "caption": "(b)Text-to-Video Retrieval.",
                "position": 2687
            }
        ]
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    },
    {
        "header": "Appendix CInference Details of BLiM",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23284/x8.png",
                "caption": "(a)w/o CPN in video-to-text.",
                "position": 2789
            },
            {
                "img": "https://arxiv.org/html/2507.23284/x8.png",
                "caption": "(a)w/o CPN in video-to-text.",
                "position": 2792
            },
            {
                "img": "https://arxiv.org/html/2507.23284/x9.png",
                "caption": "(b)w/ CPN in video-to-text.",
                "position": 2797
            },
            {
                "img": "https://arxiv.org/html/2507.23284/x10.png",
                "caption": "(c)w/o CPN in text-to-video.",
                "position": 2802
            },
            {
                "img": "https://arxiv.org/html/2507.23284/x11.png",
                "caption": "(d)w/ CPN in text-to-video.",
                "position": 2807
            }
        ]
    },
    {
        "header": "Appendix DProof of Proposition 1",
        "images": []
    },
    {
        "header": "Appendix EFurther Discussion on CPN",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23284/x12.png",
                "caption": "(a)",
                "position": 2974
            },
            {
                "img": "https://arxiv.org/html/2507.23284/x12.png",
                "caption": "(a)",
                "position": 2977
            },
            {
                "img": "https://arxiv.org/html/2507.23284/x13.png",
                "caption": "(b)",
                "position": 2982
            },
            {
                "img": "https://arxiv.org/html/2507.23284/x14.png",
                "caption": "(a)Prior vs Text Lengths.",
                "position": 3053
            },
            {
                "img": "https://arxiv.org/html/2507.23284/x14.png",
                "caption": "(a)Prior vs Text Lengths.",
                "position": 3056
            },
            {
                "img": "https://arxiv.org/html/2507.23284/x15.png",
                "caption": "(b)Prior vs Repetitive Phrases.",
                "position": 3062
            },
            {
                "img": "https://arxiv.org/html/2507.23284/x16.png",
                "caption": "Figure 10:Video-to-text retrieval performance on variousŒ±\\alphaitalic_Œ±.",
                "position": 3133
            }
        ]
    },
    {
        "header": "Appendix FFurther Quantitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23284/x17.png",
                "caption": "(a)Video-to-Text Retrieval.",
                "position": 3312
            },
            {
                "img": "https://arxiv.org/html/2507.23284/x17.png",
                "caption": "(a)Video-to-Text Retrieval.",
                "position": 3315
            },
            {
                "img": "https://arxiv.org/html/2507.23284/x18.png",
                "caption": "(b)Text-to-Video Retrieval.",
                "position": 3320
            },
            {
                "img": "https://arxiv.org/html/2507.23284/x19.png",
                "caption": "Figure 12:A qualitative example of CPN decoding on MVBench.Green signifies the accurate prediction, while red denotes the incorrect prediction.‚Ä†\\dagger‚Ä†indicates the model with CPN decoding.",
                "position": 3369
            },
            {
                "img": "https://arxiv.org/html/2507.23284/x20.png",
                "caption": "(a)",
                "position": 3375
            },
            {
                "img": "https://arxiv.org/html/2507.23284/x20.png",
                "caption": "(a)",
                "position": 3378
            },
            {
                "img": "https://arxiv.org/html/2507.23284/x21.png",
                "caption": "(b)",
                "position": 3384
            },
            {
                "img": "https://arxiv.org/html/2507.23284/x22.png",
                "caption": "(c)",
                "position": 3390
            }
        ]
    },
    {
        "header": "Appendix GFurther Qualitative Results",
        "images": []
    }
]