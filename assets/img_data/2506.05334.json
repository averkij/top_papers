[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05334/extracted/6514317/figure/lmarena_logo.png",
                "caption": "",
                "position": 111
            },
            {
                "img": "https://arxiv.org/html/2506.05334/extracted/6514317/figure/huggingface_logo-noborder.png",
                "caption": "",
                "position": 117
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05334/x1.png",
                "caption": "Figure 1:(Left) Nine intent categories with representative examples (truncated). In-the-wild user prompts are often ambiguous and require real-time web retrieval. (Right) Distribution of intents across user prompts. The majority of queries require more than a simple factual lookup and range from information synthesis to creative content generation. TheOthercategory is excluded from the visualizations.",
                "position": 212
            },
            {
                "img": "https://arxiv.org/html/2506.05334/x1.png",
                "caption": "",
                "position": 215
            },
            {
                "img": "https://arxiv.org/html/2506.05334/x2.png",
                "caption": "",
                "position": 219
            }
        ]
    },
    {
        "header": "2Human Preference Dataset in Search",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05334/x3.png",
                "caption": "Figure 2:(Left)Search Arena prompt language distribution. The dataset is multilingual, spanning over 70 languages, with English prompts accounting for 58.3% of the data.(Right)Prompt length distribution of Search Arena (blue), BrowseComp (purple), and SimpleQA (green). Search Arena prompt lengths are more spread out and cover the range of BrowseComp[61]and SimpleQA[60]questions.",
                "position": 250
            },
            {
                "img": "https://arxiv.org/html/2506.05334/x4.png",
                "caption": "",
                "position": 259
            }
        ]
    },
    {
        "header": "3Preference Analyses in Search",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05334/extracted/6514317/figure/example.png",
                "caption": "Figure 3:(Left)Reasoning trace example, containing multi-document analysis, filtering, and synthesis.(Right)Example of a rejected response citing Wikipedia for a sports news question. The preferred response cited the sports division of a news outlet, containing more up-to-date information.",
                "position": 321
            },
            {
                "img": "https://arxiv.org/html/2506.05334/x5.png",
                "caption": "Figure 4:(Left)Positive relationship between model score and average response length.(Right)Positive relationship between model score and average number of citations.",
                "position": 336
            },
            {
                "img": "https://arxiv.org/html/2506.05334/x6.png",
                "caption": "",
                "position": 345
            },
            {
                "img": "https://arxiv.org/html/2506.05334/x7.png",
                "caption": "Figure 5:(Left)Response length distribution across user intent categories. Responses toFactual Lookupprompts are more concise (168.3 words on average) compared to other categories.(Right)Citation count distribution across user intent categories. Responses toRecommendation(6.9 on average) andInfo Synthesis(6.8 on average) prompts contain more citations compared toFactual Lookup(5.7) andText Processing(4.2) prompts.",
                "position": 351
            },
            {
                "img": "https://arxiv.org/html/2506.05334/x8.png",
                "caption": "Figure 6:Citation Control.(Left)Bootstrapped coefficient estimates of citation features. (1) Users prefer responses with more citations. (2) Citing tech-related and community platforms, as well as social media is positively associated with user preferences. (3) Citing Wikipedia negatively interacts with user preferences.(Right)Bootstrapped coefficient estimates of the number of supporting, irrelevant, and contradicting claim-citation pairs. The number of supporting and irrelevant pairs is positively correlated, while the effect of contradicting pairs is not significant.",
                "position": 354
            },
            {
                "img": "https://arxiv.org/html/2506.05334/x9.png",
                "caption": "",
                "position": 360
            },
            {
                "img": "https://arxiv.org/html/2506.05334/extracted/6514317/figure/citation_exp_img.png",
                "caption": "Figure 7:Experimental Setup for Citation Attribution Analysis.(a)For each multi-turn conversation, we retrieve the cited web content and use an LLM-based pipeline to decompose each model response into individual claims, followed by citation attribution labeling.(b)For each claim-citation pair(ci,ui,ti)subscriptùëêùëñsubscriptùë¢ùëñsubscriptùë°ùëñ(c_{i},u_{i},t_{i})( italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ), we compute turn-level citation counts across the three categories and add corresponding features to the Bradley-Terry model. Results are shown inFigure¬†6(Right).",
                "position": 378
            },
            {
                "img": "https://arxiv.org/html/2506.05334/x10.png",
                "caption": "Figure 8:Cross-Arena Vote Distributionacross Text Arena(Left)and Search Arena(Right)broken down into user intent categories. Users prefer the search model forFactual LookupandInfo Synthesisqueries in both settings and the non-search model forText Processingqueries in Text Arena.",
                "position": 397
            },
            {
                "img": "https://arxiv.org/html/2506.05334/x11.png",
                "caption": "",
                "position": 406
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Limitations, Conclusion, and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05334/extracted/6514317/figure/platform.png",
                "caption": "Figure A1:Search Arena User Interface.Search Arena is integrated into the Chatbot Arena ecosystem, sharing the same front-end design and entry point. As shown at the bottom, it is implemented as a separate tab. The central panel displays a side-by-side chat interface with two anonymous models, each providing responses that include clickable inline citations and expandable reference links. Users can engage in multi-turn conversations and cast a vote at any time during the conversation using the four feedback options (A is better, B is better, Tie, and Both are bad).",
                "position": 1487
            }
        ]
    },
    {
        "header": "Appendix ASearch Arena Platform",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05334/extracted/6514317/figure/daily_usage_final_pretty.png",
                "caption": "Figure A3:Daily traffic on the Search Arena platform.",
                "position": 1517
            }
        ]
    },
    {
        "header": "Appendix BData Statistics",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05334/extracted/6514317/figure/user_demographics_map_img.png",
                "caption": "Figure A4:Search Arena Users‚Äô Demographics.Search Arena data includes 11,650 unique users across 136 countries.",
                "position": 2087
            },
            {
                "img": "https://arxiv.org/html/2506.05334/x12.png",
                "caption": "Figure A5:Search Arena Prompt Length Distribution by Intent.Prompt lengths vary across intent categories.Text Processing(85.1 words) andAnalysis(72.7 words) prompts are generally longer, whileExplanation(24.7 words) andFactual Lookup(16.3 words) prompts are shorter.",
                "position": 2093
            },
            {
                "img": "https://arxiv.org/html/2506.05334/x13.png",
                "caption": "Figure A6:Search Arena Conversation Length (Number of Turns) Distribution.Search Arena chats are multi-turn with 22.4% of conversations containing more than 1 turn.",
                "position": 2106
            },
            {
                "img": "https://arxiv.org/html/2506.05334/extracted/6514317/figure/topic_modeling_search_arena_img.png",
                "caption": "Figure A7:Top Topic Categories in Search Arena.This figure shows the distribution of topic clusters. The most prevalent topics includeTechnology Comparisons(22.0%),Market Analysis(12.3%), andEntertainment Characters(10.6%). Less frequent but still diverse topics include health and shopping. The long-tail distribution reflects the breadth of real-world usage of search-augmented LLMs.",
                "position": 2112
            },
            {
                "img": "https://arxiv.org/html/2506.05334/x14.png",
                "caption": "Figure A8:Search Arena Language Distribution (top 50).Search Arena prompts span 71 languages.",
                "position": 2118
            }
        ]
    },
    {
        "header": "Appendix CDescribing Differences",
        "images": []
    },
    {
        "header": "Appendix DLeaderboard and General Feature Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05334/x15.png",
                "caption": "Figure A9:Battle Count Distribution. Number of battles across Search Arena models. The distribution is not even due to (1) different sampling weights and (2) models were not added to the platforms at the same time.",
                "position": 2488
            },
            {
                "img": "https://arxiv.org/html/2506.05334/x16.png",
                "caption": "Figure A10:Pairwise Battle Count Distribution. Number of battles between Search Arena models.",
                "position": 2491
            },
            {
                "img": "https://arxiv.org/html/2506.05334/x17.png",
                "caption": "Figure A11:Win Rate Distribution. Average win rates of Search Arena models.",
                "position": 2497
            },
            {
                "img": "https://arxiv.org/html/2506.05334/x18.png",
                "caption": "Figure A12:Pairwise Win Rates. Pairwise win rates between Search Arena models.",
                "position": 2500
            },
            {
                "img": "https://arxiv.org/html/2506.05334/x19.png",
                "caption": "Figure A13:Search Arena Leaderboard. Model scores based on Elo-scaled Bradley-Terry coefficients.",
                "position": 2506
            },
            {
                "img": "https://arxiv.org/html/2506.05334/x20.png",
                "caption": "Figure A14:(Left)Response length distribution. Reasoning models and models with high search context size tend to be more verbose.(Right)Citation count distribution. As expected, models with higher search context size cite more sources. Reasoning models cite fewer sources compared to non-reasoning variants (e.g.,sonar-reasoning-pro-highvssonar-pro-high).",
                "position": 2648
            },
            {
                "img": "https://arxiv.org/html/2506.05334/x21.png",
                "caption": "Figure A15:Response Length Control across Intents. Bradley-Terry coefficients corresponding to response length across different intent categories. Length has less effect onFactual Lookupprompts.",
                "position": 2657
            }
        ]
    },
    {
        "header": "Appendix ECitation Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05334/x22.png",
                "caption": "Figure A16:Citation Count Control across Intents. Bradley-Terry coefficients corresponding to citation count across different intent categories. Citation count does not have significant effect onGuidance(e.g., debugging, problem-solving) prompts. The effect is largest forAnalysisqueries.",
                "position": 2674
            },
            {
                "img": "https://arxiv.org/html/2506.05334/x23.png",
                "caption": "Figure A17:Citation Source Distribution across Model Families.Models from different providers are biased towards different types of sources: (1) Perplexity models prefer citing YouTube, social media, and community blogs, (2) OpenAI models are biased towards mainstream news outlets.",
                "position": 2753
            },
            {
                "img": "https://arxiv.org/html/2506.05334/x24.png",
                "caption": "Figure A18:Model Scores Before and After Control.Model scores and rankings converge after the controls are applied.",
                "position": 2762
            },
            {
                "img": "https://arxiv.org/html/2506.05334/x25.png",
                "caption": "Figure A19:Model Scores Across Benchmark.(1) Models‚Äô performance on SimpleQA is saturated with minimal separability. (2) Performance variance in ArenaHard-v2 is comparable to that of Search Arena, but the rankings are different, with reasoning models having higher performance in search settings. (3) Model scores and ordering in Search Arena (Other) is closer to that of ArenaHard compared to Search Arena (Fact+Synth).",
                "position": 2934
            },
            {
                "img": "https://arxiv.org/html/2506.05334/x26.png",
                "caption": "Figure A20:SimpleQA vs ArenaHard-v2 Performance. Search and non-search performance of Gemini models on SimpleQA and ArenaHard-v2 benchmarks. Search improves performance on SimpleQA, while degrades the score on ArenaHard-v2.",
                "position": 2937
            }
        ]
    },
    {
        "header": "Appendix FCross-Setting Analysis",
        "images": []
    }
]