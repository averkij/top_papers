[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.10021/x1.png",
                "caption": "Figure 1:We leverage a pretrained diffusion UNet backbone for controlled human image animation, enabling expressive dynamic details and precise motion control. Specifically, we introduce a dynamics adapterDùê∑Ditalic_Dthat seamlessly integrates the reference image context as a trainable residual to the spatial attention, in parallel with the denoising process, while preserving the original spatial and temporal attention mechanisms within the UNet. In addition to body pose control via a ControlNetCPsubscriptùê∂ùëÉC_{P}italic_C start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT, we introduce a local face control moduleCFsubscriptùê∂ùêπC_{F}italic_C start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPTthat implicitly learns facial expression control from a synthesized cross-identity face patch. We train our model on a diverse dataset of human motion videos and natural scene videos simultaneously. Our model achieves remarkable transfer of body poses and facial expressions, as well as highly vivid and detailed dynamics for both the human and the scene.",
                "position": 90
            },
            {
                "img": "https://arxiv.org/html/2501.10021/x2.png",
                "caption": "Figure 2:a) IP-Adapter[50]can generate vivid texture from the reference image but fails to preserve the appearance. b) Though ReferenceNet[16]can preserve the identity from the human reference, it generates a static background without any dynamics. c) Dynamics-Adapter provides both expressive details and consistent identities.",
                "position": 135
            },
            {
                "img": "https://arxiv.org/html/2501.10021/x3.png",
                "caption": "Figure 3:a) IP-Adapter[50]encodes the reference image as an image CLIP embedding and injects the information into the cross-attention layers in SD as the residual. b) ReferenceNet[16]is a trainable parallel UNet and feeds the semantic information into SD via concatenation of self-attention features. c) Dynamics-Adapter encodes the reference image with a partially shared-weight UNet. The appearance control is realized by learning a residual in the self-attention with trainable query and output linear layers. All other components share the same frozen weight with SD.",
                "position": 139
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.10021/x4.png",
                "caption": "Figure 4:Qualitative Comparison on Human in Dynamic Scene.While existing SOTA methods struggle to generate consistent and realistic scene dynamics involving humans, our method successfully produces dynamic human-scene interactions while preserving the structure of the reference image.",
                "position": 473
            },
            {
                "img": "https://arxiv.org/html/2501.10021/x5.png",
                "caption": "Figure 5:Qualitative Comparison on Poses and Face Expressions Control.We show each method on test cases using the same reference image and pose skeleton. For improved visualization, a zoomed-in view of the face area is also provided. Our method produces results that most closely match the ground truth and best preserve face identity.",
                "position": 477
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Video Results",
        "images": []
    },
    {
        "header": "7Quantitative Evaluation of Cross-Driving Reenactment",
        "images": []
    },
    {
        "header": "8Details of User Study",
        "images": []
    },
    {
        "header": "9More Details on Prior Appearance Reference Control Designs",
        "images": []
    }
]