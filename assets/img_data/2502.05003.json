[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.05003/x1.png",
                "caption": "Figure 1:The scaling law induced by QuEST when training Llama-family models from 30 to 800M parameters on C4, with quantized weights and activations from 1 to 4 bits, in the 100 tokens/parameter regime (higher compression uses proportionally more data at fixed memory). QuEST allows for stable training at 1-bit weights and activations (W1A1), and the QuEST W4A4 model is Pareto-dominant relative to BF16, with lower loss at lower size.",
                "position": 184
            }
        ]
    },
    {
        "header": "2Background and Related Work",
        "images": []
    },
    {
        "header": "3QuEST",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.05003/x2.png",
                "caption": "Figure 2:Gradient alignment comparison for a 30M Llama model after training on 2.7B tokens in 8-bit precision.",
                "position": 552
            }
        ]
    },
    {
        "header": "4Experimental Validation",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.05003/x3.png",
                "caption": "Figure 3:Perplexity (PPL) across bit-widths with QuEST vs. a tuned variant of LSQ on a 30M model. QuEST leads to consistently lower PPL, with the advantage growing with compression.",
                "position": 592
            },
            {
                "img": "https://arxiv.org/html/2502.05003/x4.png",
                "caption": "Figure 4:Illustration of the efficiency factorseff‚Å¢(P)/PeffùëÉùëÉ\\text{eff}(P)/Peff ( italic_P ) / italic_P, arising from our analysis, for different numerical precisionsPùëÉPitalic_Pand formats (INT, FP, INT+sparse). Higher is better. INT4 appears to have the highest efficiency among hardware-supported formats.",
                "position": 682
            },
            {
                "img": "https://arxiv.org/html/2502.05003/x5.png",
                "caption": "Figure 5:Additional scaling laws induced by QuEST:(a, left)compares INT, FP, and INT+sparse formats at 4-bit precision,(b, middle)shows the scaling laws for weight-only quantization, where 2-bit appears to be Pareto-dominant, while(c, right)shows that trust estimation benefits significantly from Hadamard normalization.",
                "position": 698
            }
        ]
    },
    {
        "header": "5GPU Execution Support for QuEST Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.05003/x6.png",
                "caption": "Figure 6:Per-layer speedups for QuEST INT4 vs BF16, on a single RTX 4090 GPU. The results take into account quantization/dequantization costs for QuEST, and include the cost of the Hadamard transform (orange bar). We present results for the 800M 4-bit QuEST model we trained, as well as inference speedups for a proportional 7B-parameter model.",
                "position": 773
            },
            {
                "img": "https://arxiv.org/html/2502.05003/x7.png",
                "caption": "Figure 7:End-to-end prefill speedups for QuEST INT4 vs BF16, across different batch sizes, using the 800M parameter model on a single RTX 4090 GPU. As expected, QuEST is most effective for larger batch sizes, where the workload is more compute-bound.",
                "position": 801
            }
        ]
    },
    {
        "header": "6Discussion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional ‚ÄúTrust‚Äù Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.05003/x8.png",
                "caption": "Figure 8:Fraction of weights for whichMŒ±‚àó=0subscriptùëÄsuperscriptùõº0M_{\\alpha^{*}}=0italic_M start_POSTSUBSCRIPT italic_Œ± start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT end_POSTSUBSCRIPT = 0as a function of number of training iterations for a 30M model trained with QuEST.",
                "position": 1490
            },
            {
                "img": "https://arxiv.org/html/2502.05003/x9.png",
                "caption": "Figure 9:Fraction of masked values retained from an old iteration to a new iteration for a 30M model trained with QuEST W8A8.",
                "position": 1498
            },
            {
                "img": "https://arxiv.org/html/2502.05003/x10.png",
                "caption": "Figure 10:Performance of QuEST as a function of the outer trust scaling factorsùë†sitalic_sfor a 30M model pretraining.",
                "position": 1508
            }
        ]
    },
    {
        "header": "Appendix BAdditional Information about the Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.05003/x11.png",
                "caption": "Figure 11:Training loss curves for a 30M model trained on 3B tokens with W4A4 bitwidth, comparing QuEST (ours), LSQ, PACT, and BF16.(a)Full training loss curves, showing that QuEST closely follows BF16 and consistently outperforms LSQ, while PACT struggles with high loss.(b)Zoomed-in view of training steps after 1000, excluding PACT for clarity, highlighting that QuEST maintains a lower loss than LSQ throughout training.",
                "position": 1621
            },
            {
                "img": "https://arxiv.org/html/2502.05003/x11.png",
                "caption": "",
                "position": 1624
            },
            {
                "img": "https://arxiv.org/html/2502.05003/x12.png",
                "caption": "",
                "position": 1629
            },
            {
                "img": "https://arxiv.org/html/2502.05003/extracted/6183871/figures/PACT-hparam-search.png",
                "caption": "Figure 12:Hyperparameter search for PACT on a 30M parameter model with 4-bit weights and activations, trained on 10% of the dataset. The search explores different values for learning rate scaling (LR Scale) and alpha weight decay, with validation loss indicated by the color gradient. Lower validation loss (darker colors) corresponds to better configurations.",
                "position": 1639
            },
            {
                "img": "https://arxiv.org/html/2502.05003/x13.png",
                "caption": "Figure 13:Scaling law¬†(5) fit for 3 and 4 bit QuEST with tokens/parameters ratios in{25,50,100}2550100\\{25,50,100\\}{ 25 , 50 , 100 }.",
                "position": 1698
            },
            {
                "img": "https://arxiv.org/html/2502.05003/x14.png",
                "caption": "Figure 14:Different QuEST precision performance as a function of tokens-to-parameters ratio at a fixed model memory footprint. The gray line indicates a 4-bit optimality threshold.",
                "position": 1746
            }
        ]
    },
    {
        "header": "Appendix CScaling Laws",
        "images": []
    }
]