[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02356/x1.png",
                "caption": "",
                "position": 67
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02356/x2.png",
                "caption": "Figure 2:The pipeline ofTalk2Move.Talk2Movestreamlines a GRPO-style reinforcement learning pipeline tailored for flow-based image editing. Starting from an initial noise sample, stochastic perturbations are injected at each diffusion step to generate diverse sampling trajectories.\nSpatially grounded rewards from specialist models, which explicitly evaluate object-level geometric changes, are then used to compute group-relative advantages for policy gradient updates.",
                "position": 155
            }
        ]
    },
    {
        "header": "3Geometric Transformation for Object-Level Scene Editing",
        "images": []
    },
    {
        "header": "4Geometric Transformation Data Curation",
        "images": []
    },
    {
        "header": "5Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02356/x3.png",
                "caption": "Figure 3:Three types of step sampling:(a)is the full sampling and optimizing GRPO[flowgrpo,dancegrpo]; subsequent methods[mixgrpo,branchgrpo]as in(b), use a sliding window (yellow) to reduce the optimizing steps per iteration;(c)our work introduces step-wise active sampling that select the informative steps (red) and use shortcuts to bypass the rest of the steps, reducing both the sampling and optimizing time.",
                "position": 278
            }
        ]
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02356/x4.png",
                "caption": "(a)Off-policy reward variance distributionfor translation, rotation and resizing.",
                "position": 785
            },
            {
                "img": "https://arxiv.org/html/2601.02356/x4.png",
                "caption": "(a)Off-policy reward variance distributionfor translation, rotation and resizing.",
                "position": 788
            },
            {
                "img": "https://arxiv.org/html/2601.02356/x5.png",
                "caption": "(b)Reward curves fordifferent sampling strategiesunder the translation task.",
                "position": 793
            },
            {
                "img": "https://arxiv.org/html/2601.02356/x6.png",
                "caption": "(c)Ablative comparisonof VLM reward vs. spatial reward under the rotation task.",
                "position": 798
            },
            {
                "img": "https://arxiv.org/html/2601.02356/x7.png",
                "caption": "Figure 5:Qualitative resultson object translation, rotation and resize over state-of-the-art image editing models. For each task, we provide one real image editing result (source from OpenImagesV6[OpenImages2]) and one synthetic image editing result to showcase the generalization ability ofTalk2Move.",
                "position": 819
            }
        ]
    },
    {
        "header": "7Conclusion and Discussion",
        "images": []
    }
]