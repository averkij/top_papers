[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09136/x1.png",
                "caption": "Figure 1:Two-stage reinforcement learning framework for dermatological diagnosis.\nIn Stage 1, the model performs medical caption generation. The LLM scores each attribute field of the generated description, and these field-wise scores are integrated into a caption reward to refine medical feature learning. In Stage 2, the model predicts disease categories based on learned representations. The LLM evaluates each prediction, and a customized reward function converts these evaluations into a final diagnostic reward that optimizes classification accuracy and ranking consistency.",
                "position": 290
            },
            {
                "img": "https://arxiv.org/html/2601.09136/x2.png",
                "caption": "Figure 2:Illustration of frequency disjoint basis construction.The process transforms the weight matrixð–oâ€‹râ€‹iâˆˆâ„dÃ—d\\mathbf{W}_{ori}\\in\\mathbb{R}^{d\\times d}from the spatial domain to the Fourier domain via the Discrete Fourier Transform (DFT). The frequency spectrum is then partitioned into disjoint groups based on frequency index (e.g.,ð1\\mathbf{P}^{1}corresponds to the central low-frequency components in blue, whileP2P^{2}corresponds to the peripheral high-frequency components in orange).\nTo generate a specific spatial basisð1\\mathbf{B}_{1}, we retain only the learnable parameters belonging to Group 1 (ð1\\mathbf{P}^{1}) and mask all other frequency indices to zero. Finally, an inverse DFT (iDFT) reconstructs the spatial basis matrix. This design ensures that each basisðk\\mathbf{B}_{k}specializes in a distinct frequency band, minimizing spectral redundancy.",
                "position": 351
            },
            {
                "img": "https://arxiv.org/html/2601.09136/x3.png",
                "caption": "Figure 3:Visualizing manifold unfolding and virtual capacity.We evaluate the geometric representation capability on four classic non-linearly separable datasets: Spirals, XOR, Circles, and Moons.\n(Left column of each sub-figure) Constrained by Coverâ€™s Theorem, a standard static layer (widthd=2d=2) is topologically restricted to a single separating hyperplane, resulting in severe underfitting (accuracyâ‰ˆ50%\\approx 50\\%).\n(Middle column of each sub-figure) Without increasing the physical width (d=2d=2), FDLinear (K=12K=12) dynamically constructs high-order decision boundaries, successfully disentangling complex manifolds (e.g., the intertwined spirals). This empirically validates the \"Virtual Width Expansion\" hypothesis.\n(Right column of each sub-figure) The vector fields visualize the orientation of the generated weight matrixWâ€‹(xÂ¯)W(\\bar{x})across the input space. The rotating and radiating patterns (labeled \"Field Adaptation\") demonstrate that FDLinear acts as asample adaptive layer, modulating its projection direction based on local geometric curvature.",
                "position": 403
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09136/x4.png",
                "caption": "Figure 4:Effectiveness of Stage 1 Caption Training.\n(a) Training reward curves and (b) validation reward curves.\nThe blue line represents the model trained directly on the general-purpose baseline, while the red line denotes the model further trained based on the caption-enhanced Stage I model.",
                "position": 1106
            },
            {
                "img": "https://arxiv.org/html/2601.09136/x5.png",
                "caption": "Figure 5:Visual attention attribution analysis.We visualize the cross-attention maps of the final diagnostic token relative to the input image across different models. The histograms (right) show the distribution of attention weights.(1) Attention concentration:While baselines (Qwen2.5/3-VL) exhibitdiffuse attention, often distracted by healthy skin or background noise, Our method demonstrates preciselesion localization, sharply focusing on pathological features (e.g., papules, ulcers).(2) Effect of FDLinear and stage 1 training:Comparing \"w/o stage1 & DVE\" with \"Ours\", the introduction of the Dynamic Vision Encoder (DVE) and stage 1 training significantly improve the model to adaptively highlights important diagnostic details.(3) Confidence shift:The histograms reveal that our method (yellow) assigns higher attention weights (>0.06>0.06) to key regions than the baselines, indicating a shift from uncertain global scanning to confident diagnostic reasoning.",
                "position": 1170
            },
            {
                "img": "https://arxiv.org/html/2601.09136/x6.png",
                "caption": "Figure 6:Quantitative distribution of attention weights across 500 test samples.The histogram statistics (scaled by 100) reveal a progressive shift in attention mechanisms.(1) Noise Suppression:The full model (Ours, yellow) exhibits the lowest frequency in the background noise interval (0.00âˆ¼0.010.00\\sim 0.01), substantially lower than the Qwen2.5-VL (light green).(2) Signal Amplification:In the high-confidence interval (>0.06>0.06), the introduction of Stage 1 (orange) drastically increases the frequency of focused attention. The integration of DVE (yellow) further boosts this peak, demonstrating that the full pipeline maximizes the signal-to-noise ratio in visual reasoning.",
                "position": 1177
            }
        ]
    },
    {
        "header": "5Limitations",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APredefined Caption Schema",
        "images": []
    },
    {
        "header": "Appendix BDisease Diagnosis Prompt",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09136/img/demo.png",
                "caption": "Figure 7:Examples of disease diagnosis and examples of captions.",
                "position": 1624
            }
        ]
    },
    {
        "header": "Appendix CExamples of disease diagnosis and examples of captions",
        "images": []
    }
]