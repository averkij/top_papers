[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06329/x1.png",
                "caption": "Figure 1:FailSafeQA: Robustness and Context Grounding EvaluationWe evaluate the resilience of an LLM-based QA system in two case studies:Query FailureandContext Failure. In the Query Failure scenario, we perturb the original query into three variants: containing spelling errors (Misspelled Query), query-term form (Incomplete Query), rephrased to exclude in-domain terminology (Out-of-Domain Query). In the Context Failure case, we assume users can either fail to upload the document (Missing Context) , use degraged quality documents due to OCR (OCRed Context) or upload a document irrelevant to the query (Irrelevant Context). Robustness involves maintaining consistent model performance across perturbations (A)-(C) and (E), which preserve the intended meaning, while Context Grounding involves preventing hallucinations in scenarios (D) and (F).",
                "position": 163
            }
        ]
    },
    {
        "header": "2FailSafeQADataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06329/extracted/6191732/assets/verb_dobj_base_new.png",
                "caption": "Figure 2:The DatasetAnalysis of root verbs and their direct objects from the first sentence of each normalized query shows the top 20 verbs and their top five direct objects22footnotemark:2. This distribution can be used as a proxy measure for the diversity of tasks in the dataset, with 83.0% related to question answering (QA) and 17.0% involving text generation (TG).",
                "position": 173
            },
            {
                "img": "https://arxiv.org/html/2502.06329/x2.png",
                "caption": "Figure 3:Answer Relevance ClassesWe evaluate two scenarios in our benchmark: when models should provide an answer (ANSWER QUERY) and when they must decline to answer (REFUSE QUERY) due to lack of relevant context. Our findings reveal that all the tested models are more adept at offering suitable answers than providing a justified refusal in situations where the context lacks sufficient information. Among all models evaluated, Palmyra-Fin-128k-Instruct demonstrates the most effective balance between these capabilities.",
                "position": 279
            },
            {
                "img": "https://arxiv.org/html/2502.06329/x3.png",
                "caption": "Figure 4:Robustness and Compliance(Left) All models lose with respect to the baseline when input perturbations are applied. The biggest drop is observed for Out-Of-Domain and OCR context perturbations. Among the24242424tested models, OpenAI o3-mini is the most robust. (Right) Reasoning models like OpenAI-o1/o3-mini and the DeepSeek-R1 series reach scores up to0.590.590.590.59, while Qwen models consistently surpass0.600.600.600.60. Palmyra-Fin-128k-Instruct excels with the highest Context Grounding score of0.800.800.800.80.",
                "position": 282
            }
        ]
    },
    {
        "header": "3Metrics",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06329/extracted/6191732/assets2/robustness_query_type.png",
                "caption": "Figure 5:Robustness vs. Query Type.(Left) Across all models, the decrease in robustness is more prominent in text generation (TG) than in question-answering (QA) tasks. (Right) Similar statement also holds true for Context Grounding - when a model is asked to generate text (e.g., a blog post), it is more likely to ignore the lack of relevant information and fabricate details. For almost all models, it is easier to refuse to answer based on a wrong document (irrelevant context) than to deal with empty context (e.g., due to a failed document upload).",
                "position": 329
            },
            {
                "img": "https://arxiv.org/html/2502.06329/extracted/6191732/assets2/context_grounding_query_type.png",
                "caption": "Figure 6:Context Grounding vs. Query Type.(Left) Across all models, the decrease in robustness is more prominent in text generation (TG) than in question-answering (QA) tasks. (Right) Similar statement also holds true for Context Grounding - when a model is asked to generate text (e.g., a blog post), it is more likely to ignore the lack of relevant information and fabricate details. For all models, it is easier to refuse to answer based on a wrong document (irrelevant context) than to deal with empty context (e.g., due to a failed document upload).",
                "position": 332
            }
        ]
    },
    {
        "header": "4Evaluation",
        "images": []
    },
    {
        "header": "5Results",
        "images": []
    },
    {
        "header": "6Related work",
        "images": []
    },
    {
        "header": "7Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ACitation Sanitization",
        "images": []
    },
    {
        "header": "Appendix BExample of OCR Errors Injection into Context",
        "images": []
    },
    {
        "header": "Appendix CPrompts",
        "images": []
    }
]