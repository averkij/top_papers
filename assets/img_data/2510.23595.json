[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23595/x1.png",
                "caption": "Figure 1:Overview of the Multi-Agent Evolve Framework.Multi-Agent Evolve instantiates three interactive roles (Proposer,Solver, andJudge) from a single LLM to form a closed self-improving loop. The Proposer generates new questions, the Solver attempts to answer them, and the Judge evaluates both to provide general-domain reward signals. The Judge rewards the Solver for accurate reasoning, while the Proposer receives both aquality rewardfrom the Judge and adifficulty rewardthat increases when the Solver fails, creating an adversarial co-evolution process that continuously enhances the model’s reasoning ability.",
                "position": 170
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Multi-Agent Evolve",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23595/x2.png",
                "caption": "Figure 2:Multi-Agent Evolve Framework:(Upper) Multi-Agent Evolve uses the backbone LLM itself as a general evaluator for questions and answers. This brings several benefits, including adaptability for general tasks and increased interactions between agents. (Lower Left) Our framework adapts the quality filtering technique to the Proposer’s generation loop, preventing degradation in dataset quality during prolonged training. (Lower Right) Our multi-agent training employs Task-Relative REINFORCE++, which calculates advantage for each role respectively and then performs synchronized parameter update to the uniform model.",
                "position": 267
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23595/x3.png",
                "caption": "Figure 3:Training Process Analysis:These three figures demonstrate an example training process. (Left) The number of questions in the dataset increases steadily while low-quality questions are excluded. (Mid and Right) The Proposer learns to generate questions that present a desirable level of difficulty to the Solver, thereby benefiting the model in future training.",
                "position": 906
            },
            {
                "img": "https://arxiv.org/html/2510.23595/figures/format_reward_example.png",
                "caption": "Figure 4:Examples of Applying Format Reward and Question Quality FilteringExamples shown in green demonstrate the generation that can be correctly extracted when these two techniques are applied, which helps the maintenance of our dataset and the training process. The red examples show typical errors that detract from the training process by introducing incorrect questions or by frequently causing the reward to fall back to a neutral value.",
                "position": 1215
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APrompts and Training Hyperparameters",
        "images": []
    },
    {
        "header": "Appendix BEvaluation Prompts and Configuration",
        "images": []
    },
    {
        "header": "Appendix CGeneration Examples",
        "images": []
    },
    {
        "header": "Appendix DSeed Data Composition",
        "images": []
    }
]