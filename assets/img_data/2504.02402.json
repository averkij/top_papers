[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02402/x1.png",
                "caption": "",
                "position": 81
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Problem formulation",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02402/x2.png",
                "caption": "Figure 2:(a) Our data simulation starts with controlling the objects‚Äô vibration. We utilize audio to manipulate the coordinates of objects resulting in their vibrations across random directions. Then we use an event simulator to generate the corresponding events. The generated events are used for training. (b) The synthetic vibrating speckles are used for fine-tuning and testing.",
                "position": 180
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02402/x3.png",
                "caption": "Figure 3:(a) Overview of our proposed network architecture. The event stream is processed into voxel grids, from which patches centered around the speckles are selected. First, the patches are input into a sparse convolution-based lightweight backbone to extract visual features. Next, a spatial attention block aggregates the information in the different patches. Finally, Mamba is employed to model long-term temporal information and reconstruct the audio that caused the object‚Äôs vibration. (b) and (c) illustrate the detailed structure of SAB and SSM. (c) At time tgtsubscriptùëîùë°g_{t}italic_g start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTis the input feature,otsubscriptùëúùë°o_{t}italic_o start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTis the output andhtsubscript‚Ñéùë°h_{t}italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTdenotes the hidden state. A, B, and C are the gating weights optimized by Mamba.ŒîŒî\\Deltaroman_Œîis used to discretize the continuous parametersAùê¥Aitalic_AandBùêµBitalic_B.",
                "position": 187
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02402/x4.png",
                "caption": "Figure 4:Qualitative comparison results on the real-world data of a chipbag.Audio is provided in the supplementary.",
                "position": 408
            },
            {
                "img": "https://arxiv.org/html/2504.02402/x5.png",
                "caption": "Figure 5:Qualitative comparison results on the real-world data of a speaker.Audio is provided in the supplementary.",
                "position": 411
            },
            {
                "img": "https://arxiv.org/html/2504.02402/x6.png",
                "caption": "Figure 6:Capture objects from a distance to obtain a large field of view.Top:Capture glitter papers while playing chirp audio.Bottom:Capture multiple speakers to recover stereo audio. The left and right speakers play left and right channels respectively, while the medium speaker plays a mixed mono channel.Audio is provided in the supplementary.",
                "position": 440
            },
            {
                "img": "https://arxiv.org/html/2504.02402/x7.png",
                "caption": "Figure 7:Ablation analysis for different vibration direction. The object is placed in different orientations to produce various vibration directions.Audio is provided in the supplementary.",
                "position": 511
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASupplementary audio results and video",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02402/x8.png",
                "caption": "Figure A1:Qualitative comparison for models trained w or w/o speckle data.",
                "position": 1120
            }
        ]
    },
    {
        "header": "Appendix BSynthetic speckle data.",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02402/x9.png",
                "caption": "Figure A2:Qualitative results for ablation analysis.",
                "position": 1136
            },
            {
                "img": "https://arxiv.org/html/2504.02402/x10.png",
                "caption": "Figure A3:Qualitative comparison results of our model with other methods on the synthetic data.",
                "position": 1139
            },
            {
                "img": "https://arxiv.org/html/2504.02402/x11.png",
                "caption": "Figure A4:Phase shift introduced by sound propagation.",
                "position": 1142
            }
        ]
    },
    {
        "header": "Appendix CAdditional ablation analysis",
        "images": []
    },
    {
        "header": "Appendix DQualitative results on synthetic data",
        "images": []
    },
    {
        "header": "Appendix EQuantity results on the real-world data.",
        "images": []
    },
    {
        "header": "Appendix FInference times for all models.",
        "images": []
    },
    {
        "header": "Appendix GAnalyze phase shift in sound propagation.",
        "images": []
    }
]