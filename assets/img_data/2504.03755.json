[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03755/x1.png",
                "caption": "Figure 1:Generalized category discovery. Given a dataset with labeled data from old classes and unlabeled data from both old and novel categories. The objective is to classify old classes and cluster new categories in the unlabeled data.",
                "position": 162
            },
            {
                "img": "https://arxiv.org/html/2504.03755/x2.png",
                "caption": "Figure 2:Theunifiedandunbiasedcharacteristics of ProtoGCD, which contribute to addressing the issues of prior methods.",
                "position": 165
            },
            {
                "img": "https://arxiv.org/html/2504.03755/x3.png",
                "caption": "Figure 3:Unified prototype learning framework. (a) Previous GCD methods[7,20,22]with parametric classifiers employ distinct classification heads or training objectives for old and new classes, while (b) ProtoGCD models old and new classes in a shared feature space with a unified set of prototypes (i.e., classifier) and adopts unified learning objectives across old and new classes. (c) During inference, ProtoGCD could classify both the old and the newly discovered classes. Moreover, it could also be extended to reject unseen outliers, which makes ProtoGCD a general-purpose open-world classifier.",
                "position": 216
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03755/x4.png",
                "caption": "Figure 4:The proposed method ProtoGCD. Left: Overview of ProtoGCD. The blue, purple and orange backgrounds indicate the projection, feature and probability space, respectively. The yellow font represents learning objectives. Right: Dual-Level Adaptive Pseudo-Labeling (DAPL). We adaptively assign hard pseudo-labels to topr%percentùëür\\%italic_r %samples by confidence while soft ones for the others, and the ratior%percentùëür\\%italic_r %adaptively ramps up (blue font). ProtoGCD could be trained end-to-end.",
                "position": 254
            }
        ]
    },
    {
        "header": "3The Proposed Method: ProtoGCD",
        "images": []
    },
    {
        "header": "4Estimating the Number of Categories",
        "images": []
    },
    {
        "header": "5Extending to Detect Unseen Outliers",
        "images": []
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03755/x5.png",
                "caption": "(a)CIFAR10.",
                "position": 1624
            },
            {
                "img": "https://arxiv.org/html/2504.03755/x5.png",
                "caption": "(a)CIFAR10.",
                "position": 1627
            },
            {
                "img": "https://arxiv.org/html/2504.03755/x6.png",
                "caption": "(b)ImageNet-100.",
                "position": 1632
            },
            {
                "img": "https://arxiv.org/html/2504.03755/x7.png",
                "caption": "(a)CIFAR100.",
                "position": 1859
            },
            {
                "img": "https://arxiv.org/html/2504.03755/x7.png",
                "caption": "(a)CIFAR100.",
                "position": 1862
            },
            {
                "img": "https://arxiv.org/html/2504.03755/x8.png",
                "caption": "(b)Aircraft.",
                "position": 1867
            },
            {
                "img": "https://arxiv.org/html/2504.03755/x9.png",
                "caption": "Figure 7:Detailed ablations oferampsubscriptùëírampe_{\\text{ramp}}italic_e start_POSTSUBSCRIPT ramp end_POSTSUBSCRIPTon CIFAR100 and Aircraft.",
                "position": 1878
            },
            {
                "img": "https://arxiv.org/html/2504.03755/x10.png",
                "caption": "Figure 8:‚ÄòAll‚Äô accuracy across various class splits.",
                "position": 1885
            },
            {
                "img": "https://arxiv.org/html/2504.03755/x11.png",
                "caption": "Figure 9:Detailed ablations ofŒªentropysubscriptùúÜentropy\\lambda_{\\text{entropy}}italic_Œª start_POSTSUBSCRIPT entropy end_POSTSUBSCRIPTon Herb19.",
                "position": 1892
            },
            {
                "img": "https://arxiv.org/html/2504.03755/x12.png",
                "caption": "(a)CIFAR-10-C.",
                "position": 2131
            },
            {
                "img": "https://arxiv.org/html/2504.03755/x12.png",
                "caption": "(a)CIFAR-10-C.",
                "position": 2134
            },
            {
                "img": "https://arxiv.org/html/2504.03755/x13.png",
                "caption": "(b)CIFAR-100-C.",
                "position": 2139
            },
            {
                "img": "https://arxiv.org/html/2504.03755/x14.png",
                "caption": "Figure 11:Cluster metrics w/ and w/o prototype separation loss‚Ñísepsubscript‚Ñísep\\mathcal{L}_{\\text{sep}}caligraphic_L start_POSTSUBSCRIPT sep end_POSTSUBSCRIPT, including intra-class compactness‚Üë‚Üë\\uparrow‚Üë(left) and inter-class separation‚Üì‚Üì\\downarrow‚Üì(right).",
                "position": 2379
            },
            {
                "img": "https://arxiv.org/html/2504.03755/extracted/6329599/figs/vis_tsne_all_gcd_cifar10_20230519-172909_10_pro.png",
                "caption": "Figure 12:Visualizations of the feature space on CIFAR10. Features of old classes are depicted in cool colors (e.g.,‚àô‚àô\\color[rgb]{0.68,0.36,1}{\\bullet}‚àô,‚àô‚àô\\color[rgb]{0,0,1}{\\bullet}‚àô,‚àô‚àô\\color[rgb]{0,1,1}{\\bullet}‚àô,‚àô‚àô\\color[rgb]{0.31,1,0.5}{\\bullet}‚àô) while novel categories in warm colors (e.g.,‚àô‚àô\\color[rgb]{1,1,0}{\\bullet}‚àô,‚àô‚àô\\color[rgb]{1,.5,0}{\\bullet}‚àô,‚àô‚àô\\color[rgb]{1,.75,.75}{\\bullet}‚àô,‚àô‚àô\\color[rgb]{0.56,1,0.26}{\\bullet}‚àô). Additionally, the learnable prototypes are denoted as‚ãÜ‚ãÜ{\\color[rgb]{1,0,0}{\\star}}‚ãÜ. Our method provides improved inter-class separation and intra-class compactness.",
                "position": 2386
            },
            {
                "img": "https://arxiv.org/html/2504.03755/x15.png",
                "caption": "Figure 13:Visualizations attention maps. For Stanford Cars (top),Tesla(old) andAudi(new) are shown. For CUB (bottom),Yellow_Breasted_Chat(old) andPacific_Loon(new) are displayed. Please zoom in for more details.",
                "position": 2418
            },
            {
                "img": "https://arxiv.org/html/2504.03755/x16.png",
                "caption": "Figure 14:Sample retrieval on CUB. The three most typical and least typical are shown for each class.",
                "position": 2429
            }
        ]
    },
    {
        "header": "7Conclusion and Future Works",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "AProof of Theorem¬†2",
        "images": []
    },
    {
        "header": "BExperimental Details",
        "images": []
    },
    {
        "header": "CRelationship with Related Settings",
        "images": []
    },
    {
        "header": "DEvaluation Metrics",
        "images": []
    },
    {
        "header": "EMore Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03755/x17.png",
                "caption": "Figure 15:Performance over various weights ofŒªentropysubscriptùúÜentropy\\lambda_{\\text{entropy}}italic_Œª start_POSTSUBSCRIPT entropy end_POSTSUBSCRIPTandŒªsepsubscriptùúÜsep\\lambda_{\\text{sep}}italic_Œª start_POSTSUBSCRIPT sep end_POSTSUBSCRIPT.",
                "position": 3382
            },
            {
                "img": "https://arxiv.org/html/2504.03755/extracted/6329599/figs/vis_tsne_all_gcd_cub_20230508-004647_20_pro.png",
                "caption": "Figure 16:Visualizations of the feature space on CUB. Features of old classes are depicted in cool colors (e.g.,‚àô‚àô\\color[rgb]{0.68,0.36,1}{\\bullet}‚àô,‚àô‚àô\\color[rgb]{0,0,1}{\\bullet}‚àô,‚àô‚àô\\color[rgb]{0,1,1}{\\bullet}‚àô,‚àô‚àô\\color[rgb]{0.31,1,0.5}{\\bullet}‚àô) while novel categories in warm colors (e.g.,‚àô‚àô\\color[rgb]{1,1,0}{\\bullet}‚àô,‚àô‚àô\\color[rgb]{1,.5,0}{\\bullet}‚àô,‚àô‚àô\\color[rgb]{1,.75,.75}{\\bullet}‚àô,‚àô‚àô\\color[rgb]{0.56,1,0.26}{\\bullet}‚àô). Additionally, the learnable prototypes are denoted as‚ãÜ‚ãÜ{\\color[rgb]{1,0,0}{\\star}}‚ãÜ. Our method provides improved inter-class separation and intra-class compactness.",
                "position": 3623
            }
        ]
    },
    {
        "header": "FIn-depth Discussion of Entropy Regularization on Herb",
        "images": []
    },
    {
        "header": "GDetailed Comparison with SimGCD",
        "images": []
    },
    {
        "header": "HMore Discussion about Inter-Class Separation Regularization",
        "images": []
    }
]