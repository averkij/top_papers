[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25134/x1.png",
                "caption": "",
                "position": 116
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25134/x2.png",
                "caption": "Figure 2:LayerD decopmoses raster graphic designs into layers by iteratively extracting the top-layer and completing the background.\nOur training target is the top-layer matting model.Figs.3and4illustrate details of the top-layer extraction and background completion.",
                "position": 182
            }
        ]
    },
    {
        "header": "3Problem Formulation",
        "images": []
    },
    {
        "header": "4Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25134/x3.png",
                "caption": "Figure 3:Background completion with palette-based refinement.\nWe first complete the area of the predicted alpha map, then refine the target connected region based on the color palette of the surrounding area.\nWe select the target area based on the color gradient of the surrounding area (shown in red).",
                "position": 400
            },
            {
                "img": "https://arxiv.org/html/2509.25134/x4.png",
                "caption": "Figure 4:Palette-based foreground refinement.\nFirst, we estimate the RGB values of the top-layer using the input image, the top-layer’s alpha, and the background by the unblend function.\nNext, we extract the color palette of the connected components of the top-layer, extract the region that matches the color from the original image, integrate the connected color region with a large overlap with the predicted alpha map, and use it as a new alpha map.\nNote that the missing blue edge is refined in this figure.",
                "position": 407
            }
        ]
    },
    {
        "header": "5Decomposition Metrics",
        "images": []
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25134/x5.png",
                "caption": "(a)Without text layers",
                "position": 476
            },
            {
                "img": "https://arxiv.org/html/2509.25134/x5.png",
                "caption": "(a)Without text layers",
                "position": 479
            },
            {
                "img": "https://arxiv.org/html/2509.25134/x6.png",
                "caption": "(b)All layers",
                "position": 484
            },
            {
                "img": "https://arxiv.org/html/2509.25134/x7.png",
                "caption": "Figure 6:Ablation results of foreground color estimation and refinement.",
                "position": 546
            },
            {
                "img": "https://arxiv.org/html/2509.25134/x8.png",
                "caption": "Figure 7:Comparison of decomposition results by LayerD and baselines.\nThe leftmost images are the input image (LayerD), object detections (VLM), and text-removed input (YOLO).\nRed rectangles indicate text, and blue rectangles indicate other elements.",
                "position": 550
            },
            {
                "img": "https://arxiv.org/html/2509.25134/x9.png",
                "caption": "Figure 8:Example with or without foreground refinement. Without refinement, layers tend to have a collapsed boundary.",
                "position": 606
            },
            {
                "img": "https://arxiv.org/html/2509.25134/x10.png",
                "caption": "Figure 9:Example with or without background refinement. Our refinement prevents the background from being divided into segments or filled with unexpected colors.",
                "position": 609
            },
            {
                "img": "https://arxiv.org/html/2509.25134/x11.png",
                "caption": "Figure 10:Decomposition results of LayerD on raster images generated by FLUX.1 [dev][22].",
                "position": 612
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AEditing Examples",
        "images": []
    },
    {
        "header": "Appendix BAdditional Results",
        "images": []
    },
    {
        "header": "Appendix CFailure Cases",
        "images": []
    },
    {
        "header": "Appendix DUser Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25134/x12.png",
                "caption": "Figure A:Editing examples on Crello[56]test set.\nThe leftmost images are the original images, and the remaining images are edited ones based on the decomposed layers. We use LayerD to decompose the original images into layers, divide them into connected components, and group text components using CRAFT[4].\nThen, we perform variouslayer-leveledits, from simple layout changes to applying built-in image effects, on PowerPoint.",
                "position": 1585
            },
            {
                "img": "https://arxiv.org/html/2509.25134/x13.png",
                "caption": "Figure B:Additional qualitative results of our method on Crello[56]test set.\nThe leftmost column shows the input image, and the remaining columns show the decomposed layers from back to front.",
                "position": 1590
            },
            {
                "img": "https://arxiv.org/html/2509.25134/x14.png",
                "caption": "Figure C:Additional qualitative results of our method on Crello[56]test set.\nThe leftmost column shows the input image, and the remaining columns show the decomposed layers from back to front.",
                "position": 1595
            },
            {
                "img": "https://arxiv.org/html/2509.25134/x15.png",
                "caption": "Figure D:Failure samples for too small objects on Crello[56]test set.\nThe leftmost column shows the input image, and the remaining columns show the decomposed layers from back to front.",
                "position": 1600
            },
            {
                "img": "https://arxiv.org/html/2509.25134/x16.png",
                "caption": "Figure E:Failure samples due to the ambiguity of the layer granularity on Crello[56]test set.\nThe leftmost column shows the input image, and the remaining columns show the decomposed layers from back to front.",
                "position": 1605
            }
        ]
    },
    {
        "header": "Appendix EInfluence of Matting and Inpainting Model Choices",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25134/x17.png",
                "caption": "(a)Results with different matting backbones, SwinTransformer[30]and PVT[54]variants. The inpainting model is fixed to LaMa[48].",
                "position": 1620
            },
            {
                "img": "https://arxiv.org/html/2509.25134/x17.png",
                "caption": "(a)Results with different matting backbones, SwinTransformer[30]and PVT[54]variants. The inpainting model is fixed to LaMa[48].",
                "position": 1623
            },
            {
                "img": "https://arxiv.org/html/2509.25134/x18.png",
                "caption": "(b)Results with different inpainting models, LaMa[48]and FLUX[22].",
                "position": 1629
            }
        ]
    },
    {
        "header": "Appendix FDetail of Decomposition Metrics",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25134/x19.png",
                "caption": "Figure G:Visual example of the DTW-based layer alignment and editing process. Red lines connect matched layers between LayerD’s prediction and the ground truth; their thickness represents the matching score (the inverse of the distance),i.e., the thicker the line, the higher the score. Green boxes indicate the layers that are merged during the editing process. All layers are sorted from back to front, with the backmost layer on the left and the frontmost on the right.\nAlthough the decomposition result appears useful for editing the input image, its quality is underestimated due to a mismatch in granularity with the ground truth. Layer merging resolves this mismatch, enabling a more faithful evaluation of the decomposition quality.",
                "position": 2068
            },
            {
                "img": "https://arxiv.org/html/2509.25134/x20.png",
                "caption": "Figure H:Visual example of the DTW-based layer alignment and editing process. Red lines connect matched layers between LayerD’s prediction and the ground truth; their thickness represents the matching score (the inverse of the distance),i.e., the thicker the line, the higher the score. Green boxes indicate the layers that are merged during the editing process. All layers are sorted from back to front, with the backmost layer on the left and the frontmost on the right.\nLayerD overdecomposes the white background, but in practical scenarios, it is easy to merge these into a single layer.\nOur evaluation treats such cases as requiring a single edit operation, reflecting the actual editing workload for users.",
                "position": 2074
            }
        ]
    },
    {
        "header": "Appendix GLoss functions",
        "images": []
    }
]