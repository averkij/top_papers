[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background and Related Work",
        "images": []
    },
    {
        "header": "3Mechanistic Interpretability should prioritize Feature Consistency in SAEs",
        "images": []
    },
    {
        "header": "4Evidence from Theoretical Analysis and Synthetic Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20254/x1.png",
                "caption": "Figure 1:TopK SAE is significantly better than Standard SAE (0.97 vs 0.63) in terms of GT-MCC.",
                "position": 391
            },
            {
                "img": "https://arxiv.org/html/2505.20254/x1.png",
                "caption": "Figure 1:TopK SAE is significantly better than Standard SAE (0.97 vs 0.63) in terms of GT-MCC.",
                "position": 394
            },
            {
                "img": "https://arxiv.org/html/2505.20254/x2.png",
                "caption": "Figure 2:GT-MCC and PW-MCC for TopK and Standard SAE. PW-MCC follows the same trend as GT-MCC, both converging to comparable values. Shaded region represents max-min range across seeds.",
                "position": 399
            },
            {
                "img": "https://arxiv.org/html/2505.20254/x3.png",
                "caption": "",
                "position": 409
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/combined_mcc_redundant.png",
                "caption": "Figure 3:Left: Redundant regime with high GT-MCC but lower PW-MCC due to selection ambiguity. Right: Compressive regime with lower GT-MCC and PW-MCC. Max-min range across 5 seeds is shaded.",
                "position": 457
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/combined_mcc_redundant.png",
                "caption": "Figure 3:Left: Redundant regime with high GT-MCC but lower PW-MCC due to selection ambiguity. Right: Compressive regime with lower GT-MCC and PW-MCC. Max-min range across 5 seeds is shaded.",
                "position": 460
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/combined_mcc_compressive.png",
                "caption": "",
                "position": 470
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/token_zipfian_c4_train_camera_ready.png",
                "caption": "Figure 4:Token frequency in 1M tokens from Pile, showing the Zipfian distribution in real data, with a long and sparse tail.",
                "position": 478
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/token_histogram_c4_train_camera_ready.png",
                "caption": "",
                "position": 488
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/two_phase/dict_1000/camera_ready_min_freq_vs_similarity_bucketed.png",
                "caption": "Figure 5:Min activation frequency between matched feature pairs vs. pairwise similarity. Data from two-phase Zipfian model (dgt=5000subscript𝑑gt5000d_{\\text{gt}}=5000italic_d start_POSTSUBSCRIPT gt end_POSTSUBSCRIPT = 5000,dsae=1000subscript𝑑sae1000d_{\\text{sae}}=1000italic_d start_POSTSUBSCRIPT sae end_POSTSUBSCRIPT = 1000). Feature-level similarity captures the influence of local consistency regimes across the frequency spectrum.",
                "position": 497
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/two_phase/dict_1000/camera_ready_min_freq_vs_similarity_bucketed.png",
                "caption": "Figure 5:Min activation frequency between matched feature pairs vs. pairwise similarity. Data from two-phase Zipfian model (dgt=5000subscript𝑑gt5000d_{\\text{gt}}=5000italic_d start_POSTSUBSCRIPT gt end_POSTSUBSCRIPT = 5000,dsae=1000subscript𝑑sae1000d_{\\text{sae}}=1000italic_d start_POSTSUBSCRIPT sae end_POSTSUBSCRIPT = 1000). Feature-level similarity captures the influence of local consistency regimes across the frequency spectrum.",
                "position": 500
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/figures_zipf/beta_1.0/cluster_rank_beta_1.png",
                "caption": "Figure 6:Globally Compressive Zipfian Model (α=1𝛼1\\alpha=1italic_α = 1, 10 clusters,dg⁢t=800,dS⁢A⁢E=80formulae-sequencesubscript𝑑𝑔𝑡800subscript𝑑𝑆𝐴𝐸80d_{gt}=800,d_{SAE}=80italic_d start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT = 800 , italic_d start_POSTSUBSCRIPT italic_S italic_A italic_E end_POSTSUBSCRIPT = 80) shows cluster frequency-dependent GT-MCC (red line, left y-axis) and SAE feature allocation (green line, right y-axis).",
                "position": 505
            }
        ]
    },
    {
        "header": "5Evidence from Applications and Large-Scale Validation",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20254/x4.png",
                "caption": "Figure 7:PW-MCC vs. train steps for BatchTopK, Gated, P-Anneal, JumpReLU, Standard, TopK, and Matryoshka BatchTopK SAEs on Pythia-160M activations. Higher PW-MCC indicates greater feature consistency.",
                "position": 554
            },
            {
                "img": "https://arxiv.org/html/2505.20254/x4.png",
                "caption": "Figure 7:PW-MCC vs. train steps for BatchTopK, Gated, P-Anneal, JumpReLU, Standard, TopK, and Matryoshka BatchTopK SAEs on Pythia-160M activations. Higher PW-MCC indicates greater feature consistency.",
                "position": 557
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/mcc_contributions_comparison_2.png",
                "caption": "Figure 8:PW-MCC contribution by feature activation frequency for TopK, Standard, Gated, and JumpReLU SAEs. Bars (left axis) show each bin’s contribution; solid lines show cumulative contribution. Dashed lines (right axis) show feature distribution across bins.",
                "position": 562
            }
        ]
    },
    {
        "header": "6Alternative Views",
        "images": []
    },
    {
        "header": "7Conclusion and Call to Action",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Related Work",
        "images": []
    },
    {
        "header": "Appendix BFormal Definitions of Feature Consistency",
        "images": []
    },
    {
        "header": "Appendix CHow the Round-Trip Condition Guarantees Spark in SAEs",
        "images": []
    },
    {
        "header": "Appendix DSupplementary Analysis of SAEs trained on Synthetic Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/correlation_distribution.png",
                "caption": "Figure 9:Feature Correlation Distribution (dg⁢t=40,ds⁢a⁢e=160,k=8formulae-sequencesubscript𝑑𝑔𝑡40formulae-sequencesubscript𝑑𝑠𝑎𝑒160𝑘8d_{gt}=40,d_{sae}=160,k=8italic_d start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT = 40 , italic_d start_POSTSUBSCRIPT italic_s italic_a italic_e end_POSTSUBSCRIPT = 160 , italic_k = 8). Compares similarities of Run 0 features to ground truth (red) and Run 1 features (blue). The substantial overlap in high-similarity regions (purple) demonstrates ambiguity where multiple SAE features are good matches to both ground truth and features learned in other runs, creating selection ambiguity despite high feature quality.",
                "position": 1775
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/correlation_distribution.png",
                "caption": "Figure 9:Feature Correlation Distribution (dg⁢t=40,ds⁢a⁢e=160,k=8formulae-sequencesubscript𝑑𝑔𝑡40formulae-sequencesubscript𝑑𝑠𝑎𝑒160𝑘8d_{gt}=40,d_{sae}=160,k=8italic_d start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT = 40 , italic_d start_POSTSUBSCRIPT italic_s italic_a italic_e end_POSTSUBSCRIPT = 160 , italic_k = 8). Compares similarities of Run 0 features to ground truth (red) and Run 1 features (blue). The substantial overlap in high-similarity regions (purple) demonstrates ambiguity where multiple SAE features are good matches to both ground truth and features learned in other runs, creating selection ambiguity despite high feature quality.",
                "position": 1778
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/cosine_decay_160.png",
                "caption": "Figure 10:Cosine Similarity Decay with Ground Truth (dg⁢t=40,ds⁢a⁢e=160,k=8formulae-sequencesubscript𝑑𝑔𝑡40formulae-sequencesubscript𝑑𝑠𝑎𝑒160𝑘8d_{gt}=40,d_{sae}=160,k=8italic_d start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT = 40 , italic_d start_POSTSUBSCRIPT italic_s italic_a italic_e end_POSTSUBSCRIPT = 160 , italic_k = 8). Features are ranked by ground truth similarity for Run 0 (red) and Run 1 (blue). Similarity decays very slowly, remaining high well past rankdg⁢t=40subscript𝑑𝑔𝑡40d_{gt}=40italic_d start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT = 40, indicating that the SAE learns multiple good representations for each ground truth feature, creating a selection challenge when comparing across runs.",
                "position": 1783
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/mcc_vs_ground_truth_redundant.png",
                "caption": "Figure 11:Redundant Regime: TopK SAE Mean GT-MCC (across 5 seeds) vs. Training Steps (dg⁢t=80,ds⁢a⁢e=160,k=8formulae-sequencesubscript𝑑𝑔𝑡80formulae-sequencesubscript𝑑𝑠𝑎𝑒160𝑘8d_{gt}=80,d_{sae}=160,k=8italic_d start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT = 80 , italic_d start_POSTSUBSCRIPT italic_s italic_a italic_e end_POSTSUBSCRIPT = 160 , italic_k = 8). The GT-MCC reaches high values, indicating strong recovery of ground truth features.",
                "position": 1798
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/mcc_vs_ground_truth_redundant.png",
                "caption": "Figure 11:Redundant Regime: TopK SAE Mean GT-MCC (across 5 seeds) vs. Training Steps (dg⁢t=80,ds⁢a⁢e=160,k=8formulae-sequencesubscript𝑑𝑔𝑡80formulae-sequencesubscript𝑑𝑠𝑎𝑒160𝑘8d_{gt}=80,d_{sae}=160,k=8italic_d start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT = 80 , italic_d start_POSTSUBSCRIPT italic_s italic_a italic_e end_POSTSUBSCRIPT = 160 , italic_k = 8). The GT-MCC reaches high values, indicating strong recovery of ground truth features.",
                "position": 1801
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/pairwise_mcc_redundant.png",
                "caption": "Figure 12:Redundant Regime: TopK SAE Mean PW-MCC (across 5 seeds) vs. Training Steps (dg⁢t=80,ds⁢a⁢e=160,k=8formulae-sequencesubscript𝑑𝑔𝑡80formulae-sequencesubscript𝑑𝑠𝑎𝑒160𝑘8d_{gt}=80,d_{sae}=160,k=8italic_d start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT = 80 , italic_d start_POSTSUBSCRIPT italic_s italic_a italic_e end_POSTSUBSCRIPT = 160 , italic_k = 8). The PW-MCC reaches lower values than GT-MCC, reflecting the challenge of feature consistency across different SAE initializations due to selection ambiguity.",
                "position": 1806
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/redundant_intersection_ratio.png",
                "caption": "Figure 13:Redundant Regime: TopK SAE Mean Intersection Ratio (across 5 seeds) vs. Training Steps (dg⁢t=80,ds⁢a⁢e=160,k=8formulae-sequencesubscript𝑑𝑔𝑡80formulae-sequencesubscript𝑑𝑠𝑎𝑒160𝑘8d_{gt}=80,d_{sae}=160,k=8italic_d start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT = 80 , italic_d start_POSTSUBSCRIPT italic_s italic_a italic_e end_POSTSUBSCRIPT = 160 , italic_k = 8). The Intersection Ratio measures the consistency of feature selection indices across different SAE initializations, with higher values indicating more stable feature recovery.",
                "position": 1819
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/mcc_vs_ground_truth_compressive.png",
                "caption": "Figure 14:Compressive Regime: TopK SAE Mean GT-MCC (across 5 seeds) vs. Training Steps (dg⁢t=800,ds⁢a⁢e=80,k=8formulae-sequencesubscript𝑑𝑔𝑡800formulae-sequencesubscript𝑑𝑠𝑎𝑒80𝑘8d_{gt}=800,d_{sae}=80,k=8italic_d start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT = 800 , italic_d start_POSTSUBSCRIPT italic_s italic_a italic_e end_POSTSUBSCRIPT = 80 , italic_k = 8). The GT-MCC reaches lower values compared to the redundant regime, reflecting the fundamental capacity limitation that prevents complete recovery of all ground truth features.",
                "position": 1833
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/mcc_vs_ground_truth_compressive.png",
                "caption": "Figure 14:Compressive Regime: TopK SAE Mean GT-MCC (across 5 seeds) vs. Training Steps (dg⁢t=800,ds⁢a⁢e=80,k=8formulae-sequencesubscript𝑑𝑔𝑡800formulae-sequencesubscript𝑑𝑠𝑎𝑒80𝑘8d_{gt}=800,d_{sae}=80,k=8italic_d start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT = 800 , italic_d start_POSTSUBSCRIPT italic_s italic_a italic_e end_POSTSUBSCRIPT = 80 , italic_k = 8). The GT-MCC reaches lower values compared to the redundant regime, reflecting the fundamental capacity limitation that prevents complete recovery of all ground truth features.",
                "position": 1836
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/pairwise_mcc_compressive.png",
                "caption": "Figure 15:Compressive Regime: TopK SAE Mean PW-MCC (across 5 seeds) vs. Training Steps (dg⁢t=800,ds⁢a⁢e=80,k=8formulae-sequencesubscript𝑑𝑔𝑡800formulae-sequencesubscript𝑑𝑠𝑎𝑒80𝑘8d_{gt}=800,d_{sae}=80,k=8italic_d start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT = 800 , italic_d start_POSTSUBSCRIPT italic_s italic_a italic_e end_POSTSUBSCRIPT = 80 , italic_k = 8). The PW-MCC values are also lower compared to the redundant regime, indicating lower overall recovery quality.",
                "position": 1841
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/figures_zipf/beta_1.0/capacity_allocation_model.png",
                "caption": "Figure 16:Left: Capacity allocation model for Zipf distribution withα=1.0𝛼1.0\\alpha=1.0italic_α = 1.0, showing how SAE features are allocated to clusters based on cluster probability. The red curve shows the fitted power law model, followingDi∝piβproportional-tosubscript𝐷𝑖superscriptsubscript𝑝𝑖𝛽D_{i}\\propto p_{i}^{\\beta}italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∝ italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_β end_POSTSUPERSCRIPTwhereβ≈1.343𝛽1.343\\beta\\approx 1.343italic_β ≈ 1.343. Right: Feature similarity between independently trained SAEs as a function of minimum feature activation frequency, with bucketed averages (red) showing a positive trend between activation frequency and feature reproducibility.",
                "position": 2084
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/figures_zipf/beta_1.0/min_freq_vs_similarity_bucketed.png",
                "caption": "",
                "position": 2093
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/figures_zipf/beta_1.0/cropped_cluster_metrics.png",
                "caption": "Figure 17:Cluster metrics for Zipf distribution withα=1.0𝛼1.0\\alpha=1.0italic_α = 1.0. Left: Cluster rank vs. probability (blue bars) and MCC scores (red line), showing how feature recovery quality varies with cluster probability. The MCC scores demonstrate a positive correlation with cluster probability, with lower-ranked (more probable) clusters achieving better feature recovery. Right: Cluster rank vs. probability (blue bars) and feature allocation (green line), demonstrating how the model allocates dictionary features based on cluster probability, with more frequent clusters receiving proportionally more features.",
                "position": 2099
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/figures_zipf/beta_1.0/feature_cluster_affinity_activation.png",
                "caption": "Figure 18:Feature-cluster relationships for Zipf distribution withα=1.0𝛼1.0\\alpha=1.0italic_α = 1.0. Left: Activation-based affinity heatmap showing how features (y-axis, sorted by primary cluster) are activated by different clusters (x-axis, sorted by probability). Brighter colors indicate stronger activation, showing that more frequent cluster features activate more features. Right: Matching-based affinity heatmap showing global assignment of features to clusters using Hungarian matching, with features on y-axis and clusters on x-axis",
                "position": 2102
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/figures_zipf/beta_1.0/feature_cluster_affinity_matching.png",
                "caption": "",
                "position": 2111
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/figures_zipf/beta_1.1/capacity_allocation_model.png",
                "caption": "Figure 19:Left: Capacity allocation model for Zipf distribution withα=1.1𝛼1.1\\alpha=1.1italic_α = 1.1, showing how SAE features are allocated to clusters based on cluster probability. Red curve shows fitted power law model withDi∝piβproportional-tosubscript𝐷𝑖superscriptsubscript𝑝𝑖𝛽D_{i}\\propto p_{i}^{\\beta}italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∝ italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_β end_POSTSUPERSCRIPTwhereβ≈1.455𝛽1.455\\beta\\approx 1.455italic_β ≈ 1.455. Right: Feature similarity between independently trained SAEs as a function of minimum feature activation frequency, with bucketed averages (red) showing a positive trend between activation frequency and feature reproducibility.",
                "position": 2128
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/figures_zipf/beta_1.1/min_freq_vs_similarity_bucketed.png",
                "caption": "",
                "position": 2137
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/figures_zipf/beta_1.1/cropped_cluster_metrics.png",
                "caption": "Figure 20:Cluster metrics for Zipf distribution withα=1.1𝛼1.1\\alpha=1.1italic_α = 1.1. Left: Cluster rank vs. probability (blue bars) and MCC scores (red line), showing a steeper decline in feature recovery quality for less probable clusters compared toα=1.0𝛼1.0\\alpha=1.0italic_α = 1.0. Right: Cluster rank vs. probability (blue bars) and feature allocation (green line), demonstrating more skewed allocation of dictionary features toward high-probability clusters.",
                "position": 2143
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/figures_zipf/beta_1.1/feature_cluster_affinity_activation.png",
                "caption": "Figure 21:Feature-cluster relationships for Zipf distribution withα=1.1𝛼1.1\\alpha=1.1italic_α = 1.1. Left: Activation-based affinity heatmap showing stronger feature-to-cluster specialization. compared toα=1.0𝛼1.0\\alpha=1.0italic_α = 1.0. Right: Matching-based affinity heatmap showing increased skew in feature assignments, with high-probability clusters receiving disproportionately more feature allocations compared toα=1.0𝛼1.0\\alpha=1.0italic_α = 1.0.",
                "position": 2146
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/figures_zipf/beta_1.1/feature_cluster_affinity_matching.png",
                "caption": "",
                "position": 2155
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/figures_zipf/beta_1.5/capacity_allocation_model.png",
                "caption": "Figure 22:Left: Capacity allocation model for Zipf distribution withα=1.5𝛼1.5\\alpha=1.5italic_α = 1.5, showing significantly more skewed allocation of SAE features to clusters. Red curve shows fitted power law model withDi∝piβproportional-tosubscript𝐷𝑖superscriptsubscript𝑝𝑖𝛽D_{i}\\propto p_{i}^{\\beta}italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∝ italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_β end_POSTSUPERSCRIPTwhereβ≈1.35𝛽1.35\\beta\\approx 1.35italic_β ≈ 1.35. Right: Feature similarity between independently trained SAEs as a function of minimum feature activation frequency showing a weak positive trend.",
                "position": 2172
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/figures_zipf/beta_1.5/min_freq_vs_similarity_bucketed.png",
                "caption": "",
                "position": 2181
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/figures_zipf/beta_1.5/cropped_cluster_metrics.png",
                "caption": "Figure 23:Cluster metrics for Zipf distribution withα=1.5𝛼1.5\\alpha=1.5italic_α = 1.5. Left: Cluster rank vs. probability (blue bars) and MCC scores (red line), showing a sharp threshold effect where feature recovery quality drops dramatically beyond the highest-probability clusters. Right: Cluster rank vs. probability (blue bars) and feature allocation (green line), demonstrating highly concentrated allocation of dictionary features to the most probable clusters.",
                "position": 2187
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/figures_zipf/beta_1.5/feature_cluster_affinity_activation.png",
                "caption": "Figure 24:Feature-cluster relationships for Zipf distribution withα=1.5𝛼1.5\\alpha=1.5italic_α = 1.5. Left: Activation-based affinity heatmap showing high feature specialization with minimal cross-activation. Right: Matching-based affinity heatmap showing strong one-to-one mapping for high-probability clusters but poor assignment for low-probability clusters.",
                "position": 2190
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/figures_zipf/beta_1.5/feature_cluster_affinity_matching.png",
                "caption": "",
                "position": 2199
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/figures_zipf/beta_2.0/capacity_allocation_model.png",
                "caption": "Figure 25:Left: Capacity allocation model for Zipf distribution withα=2.0𝛼2.0\\alpha=2.0italic_α = 2.0, showing extreme concentration of SAE features to the highest-probability clusters. Red curve shows fitted power law model withDi∝piβproportional-tosubscript𝐷𝑖superscriptsubscript𝑝𝑖𝛽D_{i}\\propto p_{i}^{\\beta}italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∝ italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_β end_POSTSUPERSCRIPTwhereβ≈1.256𝛽1.256\\beta\\approx 1.256italic_β ≈ 1.256. Right: Feature similarity between independently trained SAEs as a function of minimum feature activation frequency showing a flat to weak positive trend.",
                "position": 2216
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/figures_zipf/beta_2.0/min_freq_vs_similarity_bucketed.png",
                "caption": "",
                "position": 2225
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/figures_zipf/beta_2.0/cropped_cluster_metrics.png",
                "caption": "Figure 26:Cluster metrics for Zipf distribution withα=2.0𝛼2.0\\alpha=2.0italic_α = 2.0. Left: Cluster rank vs. probability (blue bars) and MCC scores (red line), showing that the very highest-probability clusters achieve good feature recovery. Right: Cluster rank vs. probability (blue bars) and feature allocation (green line), demonstrating that dictionary features are almost exclusively allocated to the top clusters, with negligible capacity for the long tail.",
                "position": 2231
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/figures_zipf/beta_2.0/feature_cluster_affinity_activation.png",
                "caption": "Figure 27:Feature-cluster relationships for Zipf distribution withα=2.0𝛼2.0\\alpha=2.0italic_α = 2.0. Left: Activation-based affinity heatmap showing specialization to high-probability clusters. Right: Matching-based affinity heatmap showing strong assignment for only the highest-probability clusters, with the majority of clusters receiving minimal or no feature representation.",
                "position": 2234
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/figures_zipf/beta_2.0/feature_cluster_affinity_matching.png",
                "caption": "",
                "position": 2243
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/two_phase/dict_80/distribution.png",
                "caption": "Figure 28:Two-phase cluster probability distribution used to approximate real language data. The distribution follows a Mandelbrot-Zipf pattern (s1=1.05subscript𝑠11.05s_{1}=1.05italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 1.05) until rank 40,000, then transitions to a steeper power law (s2=30.0subscript𝑠230.0s_{2}=30.0italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 30.0) capturing the long tail characteristics of natural language.",
                "position": 2360
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/two_phase/dict_80/min_freq_vs_similarity_bucketed.png",
                "caption": "Figure 29:Two-phase model with dictionary size 80. Feature reproducibility shows a weak positive relationship with activation frequency.",
                "position": 2366
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/two_phase/dict_80/min_freq_vs_similarity_bucketed.png",
                "caption": "Figure 29:Two-phase model with dictionary size 80. Feature reproducibility shows a weak positive relationship with activation frequency.",
                "position": 2369
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/two_phase/dict_160/min_freq_vs_similarity_bucketed.png",
                "caption": "Figure 30:Two-phase model with dictionary size 160. The relationship between activation frequency and feature reproducibility remains weak but becomes slightly more pronounced compared to dictionary size 80.",
                "position": 2374
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/two_phase/dict_1000/min_freq_vs_similarity_bucketed.png",
                "caption": "Figure 31:Two-phase model with dictionary size 1000. Feature reproducibility shows a moderately strong positive correlation with activation frequency especially at higher activation frequencies. Increased model capacity creates sufficient local redundancy for high probability clusters.",
                "position": 2380
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/two_phase/dict_1000/min_freq_vs_similarity_bucketed.png",
                "caption": "Figure 31:Two-phase model with dictionary size 1000. Feature reproducibility shows a moderately strong positive correlation with activation frequency especially at higher activation frequencies. Increased model capacity creates sufficient local redundancy for high probability clusters.",
                "position": 2383
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/two_phase/dict_10000/min_freq_vs_similarity_bucketed.png",
                "caption": "Figure 32:Two-phase model with dictionary size 10000. With substantially increased capacity, feature reproducibility exhibits a strong positive correlation with activation frequency across a wide frequency range. Increased model capacity creates sufficient local redundancy for high probability clusters.",
                "position": 2388
            }
        ]
    },
    {
        "header": "Appendix EEffect of Misspecifying Encoder Sparsity Parameter",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/mcc_vs_k.png",
                "caption": "Figure 33:Effect of Activation Sparsityk𝑘kitalic_kin TopK SAE in the Matched Regime (dgt=dsae=40subscript𝑑gtsubscript𝑑sae40d_{\\text{gt}}=d_{\\text{sae}}=40italic_d start_POSTSUBSCRIPT gt end_POSTSUBSCRIPT = italic_d start_POSTSUBSCRIPT sae end_POSTSUBSCRIPT = 40, trues=8𝑠8s=8italic_s = 8). We plot final GT-MCC (averaged over the last 100 steps) vs.k𝑘kitalic_k. Performance peaks atk=s=8𝑘𝑠8k=s=8italic_k = italic_s = 8, with underestimatingk𝑘kitalic_kbeing more harmful than overestimating it.",
                "position": 2439
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/comparison_avg_similarity.png",
                "caption": "Figure 34:Average feature similarity (PW-MCC of matched individual features) versus activation rate for four SAE architectures. The activation rate is defined asmin(freq_run1, freq_run2), representing the minimum percentage of tokens activating the feature across two independent runs. Data is from SAEs trained on Pythia-160M layer 8 activations. A strong positive correlation is evident, indicating that features with higher shared activation rates are learned more consistently.",
                "position": 2718
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/comparison_avg_similarity.png",
                "caption": "Figure 34:Average feature similarity (PW-MCC of matched individual features) versus activation rate for four SAE architectures. The activation rate is defined asmin(freq_run1, freq_run2), representing the minimum percentage of tokens activating the feature across two independent runs. Data is from SAEs trained on Pythia-160M layer 8 activations. A strong positive correlation is evident, indicating that features with higher shared activation rates are learned more consistently.",
                "position": 2721
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/feature_activation_comparison.png",
                "caption": "Figure 35:Log-log plot of feature activation frequency versus feature rank for four SAE architectures trained on Pythia-160M layer 8 activations. All architectures exhibit power-law-like distributions of feature usage, but with distinct slopes and differing prevalence of very low-frequency (potentially dead) features. Notably, TopK and Gated architectures tend to show fewer dead features.",
                "position": 2726
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/similarity_analysis_Standard.png",
                "caption": "Figure 36:Feature similarity analysis for a Standard SAE (L1-penalized) trained on Pythia-160M layer 8 activations. The overall PW-MCC for this configuration was approximately0.47390.47390.47390.4739. Top left: Density map of pairwise feature similarity vs. log minimum activation frequency (min(freq_run1, freq_run2)). Top right: Histogram of pairwise feature similarity scores. Bottom left: Average similarity bucketed by log minimum frequency range. Bottom right: Scatter plot of feature activation frequencies from two runs (freq_run1vs.freq_run2), colored by their pairwise similarity score. Points along the diagonal represent features with similar activation frequencies in both runs.",
                "position": 2742
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/similarity_vs_min_frequency_Standard.png",
                "caption": "Figure 37:Average feature similarity versusmin(freq_run1, freq_run2)for the Standard SAE. This plot highlights the positive trend: features with higher shared activation levels tend to exhibit greater pairwise similarity, though the overall consistency for this architecture is modest.",
                "position": 2745
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/similarity_analysis_TopK.png",
                "caption": "Figure 38:Feature similarity analysis for a TopK SAE trained on Pythia-160M layer 8 activations. This architecture achieved a high overall PW-MCC of approximately0.81880.81880.81880.8188. Panels are analogous to Figure36.",
                "position": 2756
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/similarity_vs_min_frequency_TopK.png",
                "caption": "Figure 39:Average feature similarity versusmin(freq_run1, freq_run2)for the TopK SAE. This architecture demonstrates both high overall similarity levels and a strong positive correlation between shared activation frequency and feature reproducibility.",
                "position": 2759
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/similarity_analysis_Gated.png",
                "caption": "Figure 40:Feature similarity analysis for a Gated SAE trained on Pythia-160M layer 8 activations, with an overall PW-MCC of approximately0.73780.73780.73780.7378. Panels are analogous to Figure36.",
                "position": 2769
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/similarity_vs_min_frequency_Gated.png",
                "caption": "Figure 41:Average feature similarity versusmin(freq_run1, freq_run2)for the Gated SAE. The overall dictionary PW-MCC for this configuration is0.73780.73780.73780.7378. A strong positive correlation is evident between shared activation frequency and individual feature similarity.",
                "position": 2772
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/similarity_analysis_JumpReLU.png",
                "caption": "Figure 42:Feature similarity analysis for a JumpReLU SAE trained on Pythia-160M layer 8 activations. The overall PW-MCC was approximately0.49570.49570.49570.4957. Panels are analogous to Figure36.",
                "position": 2782
            },
            {
                "img": "https://arxiv.org/html/2505.20254/extracted/6481040/figures/similarity_vs_min_frequency_JumpReLU.png",
                "caption": "Figure 43:Average feature similarity versusmin(freq_run1, freq_run2)for the JumpReLU SAE. This plot shows increasing similarity with higher shared activation frequency. The presence of two distinct clusters suggests potential subpopulations of features with differing learning or consistency characteristics within this architecture.",
                "position": 2785
            },
            {
                "img": "https://arxiv.org/html/2505.20254/x5.png",
                "caption": "Figure 44:Final PW-MCC for BatchTopK, Gated, P-Anneal, JumpReLU, Standard, TopK, and Matryoshka BatchTopK SAEs on Gemma-2-2B activations. Higher PW-MCC indicates greater run-to-run feature consistency.",
                "position": 2816
            }
        ]
    },
    {
        "header": "Appendix FSupplementary Analysis of SAEs Trained on Language Model Activations",
        "images": []
    }
]