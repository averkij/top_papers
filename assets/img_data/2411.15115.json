[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15115/x1.png",
                "caption": "",
                "position": 174
            },
            {
                "img": "https://arxiv.org/html/2411.15115/x2.png",
                "caption": "Figure 2:Comparison of different refinement methods for alignment.(a) Prompt optimization (e.g., OPT2I[29]) by LLM-based rewriting without visual/fine-grained feedback, making the search expensive (e.g., 30 iterations).\n(b) Recent work on localized feedback (e.g., SLD[52]) provides visual feedback\nbut requires the use of an externally trained layout-guided generation module and result in unnatural refined output.\nOur (c)VideoRepairis a training-free, model-agnostic refinement framework for T2V alignment that provides fine-grained localized visual guidance and uses the original T2V model.",
                "position": 183
            },
            {
                "img": "https://arxiv.org/html/2411.15115/x3.png",
                "caption": "Figure 3:Example misalignments in T2V generation.Left:object misalignment (e.g., man is missing / 5 children instead of 4).Right:attribute misalignment (e.g., dog‚Äôs color is leaked from dolphin‚Äôs color / bear is brown instead of blue).\nVideos are generated with T2V-turbo[18].",
                "position": 192
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3VideoRepair:\nImproving Text-to-Video Generation\nvia Misalignment Evaluation and Localized Refinement",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15115/x4.png",
                "caption": "Figure 4:Illustration ofVideoRepair.VideoRepairrefines the generated video in four stages:\n(1) video evaluation (Sec.3.1),\n(2) refinement planning (Sec.3.2),\n(3) region decomposition (Sec.3.3),\nand (4) localized refinement (Sec.3.4).\nFrom the initial promptpùëùpitalic_p, we first generate a fine-grained evaluation question setQosuperscriptùëÑùëúQ^{o}italic_Q start_POSTSUPERSCRIPT italic_o end_POSTSUPERSCRIPTand ask MLLM to answer it.\nNext, we identifyO‚àósuperscriptùëÇO^{*}italic_O start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPTby MLLM and plan how to refine the other region by LLM. UsingO‚àósuperscriptùëÇO^{*}italic_O start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT, we decompose the region by Molmo and Semantic SAM to get the region to keep.\nIn the end, we conduct localized refinement by the original frozen T2V model.",
                "position": 298
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15115/x5.png",
                "caption": "Figure 5:Videos generated with T2V-turbo\nand refinement frameworks (OPT2I / SLD /VideoRepair) on T2V-turbo.VideoRepairsuccessfully addressesobject and attribute misalignmentissues (e.g., numeracy, spatial relationship, attribute blending) compared to T2V-turbo and other refinement methods.\nMore visualization examples withT2V-turboandVideoCrafter2are provided in the appendix.",
                "position": 764
            },
            {
                "img": "https://arxiv.org/html/2411.15115/x6.png",
                "caption": "Figure 6:The iterative refinement ofVideoRepair.Videos in each column represent the outputs of successive refinement iterations, where the output from the previous step serves as the input for the current step. The text at the bottom of each video row indicates the corresponding text prompt. More visualization examples are provided in the appendix.",
                "position": 792
            },
            {
                "img": "https://arxiv.org/html/2411.15115/x7.png",
                "caption": "Figure 7:Impact of iterative refinement.Iterative refinement gradually improves DSGObjObj{}^{\\text{Obj}}start_FLOATSUPERSCRIPT Obj end_FLOATSUPERSCRIPTand text-video alignment score on all three prompt categories (count/color/action) of EvalCrafter.\nThe ‚Äòinitial video‚Äô refers to a video from T2V-turbo.\nWe use video ranking with K=5 candidates.",
                "position": 974
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AVideoRepairImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15115/x8.png",
                "caption": "Figure 8:Comparison of DSG and DSGObjObj{}^{\\text{Obj}}start_FLOATSUPERSCRIPT Obj end_FLOATSUPERSCRIPT.Compared to DSGObjObj{}^{\\text{Obj}}start_FLOATSUPERSCRIPT Obj end_FLOATSUPERSCRIPT(ours), DSG does not penalize the video even if more than one object (e.g., 1 bear in this case) is generated when the target object count = 1 in the text prompt.",
                "position": 1760
            }
        ]
    },
    {
        "header": "Appendix BAdditional Baseline Details",
        "images": []
    },
    {
        "header": "Appendix CAdditional Evaluation Details",
        "images": []
    },
    {
        "header": "Appendix DAdditional Quantitative Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15115/x9.png",
                "caption": "Figure 9:Impact of the number of video candidates.We vary the number of video candidatesKùêæKitalic_Kas 1, 5, 10, and 20 for ranking.",
                "position": 1922
            },
            {
                "img": "https://arxiv.org/html/2411.15115/x10.png",
                "caption": "Figure 10:Qualitative examples from T2V-turbo.",
                "position": 1974
            },
            {
                "img": "https://arxiv.org/html/2411.15115/x11.png",
                "caption": "Figure 11:Qualitative examples from T2V-turbo.",
                "position": 1977
            },
            {
                "img": "https://arxiv.org/html/2411.15115/x12.png",
                "caption": "Figure 12:Qualitative examples from T2V-turbo.",
                "position": 1981
            },
            {
                "img": "https://arxiv.org/html/2411.15115/x13.png",
                "caption": "Figure 13:Qualitative examples from T2V-turbo.",
                "position": 1984
            },
            {
                "img": "https://arxiv.org/html/2411.15115/x14.png",
                "caption": "Figure 14:Qualitative examples from VideoCrafter2.",
                "position": 1988
            },
            {
                "img": "https://arxiv.org/html/2411.15115/x15.png",
                "caption": "Figure 15:Qualitative examples from VideoCrafter2.",
                "position": 1991
            },
            {
                "img": "https://arxiv.org/html/2411.15115/x16.png",
                "caption": "Figure 16:Qualitative examples from VideoCrafter2.",
                "position": 1995
            },
            {
                "img": "https://arxiv.org/html/2411.15115/x17.png",
                "caption": "Figure 17:Videos generated using iterative refinement withVideoRepair.We depict iterative refinement results generated from T2V-Turbo. Overall,VideoRepairprogressively enhances text-video alignment with each refinement step.",
                "position": 1998
            },
            {
                "img": "https://arxiv.org/html/2411.15115/x18.png",
                "caption": "Figure 18:Output from each step ofVideoRepair.We illustrate whole outputs from each step ofVideoRepair.",
                "position": 2001
            },
            {
                "img": "https://arxiv.org/html/2411.15115/x19.png",
                "caption": "Figure 19:Output from each step ofVideoRepair.We illustrate whole outputs from each step ofVideoRepair.",
                "position": 2004
            },
            {
                "img": "https://arxiv.org/html/2411.15115/x20.png",
                "caption": "Figure 20:Prompts to perform visual question answering in video evaluation steps.Top:The prompt forQcosubscriptsuperscriptùëÑùëúùëêQ^{o}_{c}italic_Q start_POSTSUPERSCRIPT italic_o end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT(count-related question),Bottom:prompt forQaosubscriptsuperscriptùëÑùëúùëéQ^{o}_{a}italic_Q start_POSTSUPERSCRIPT italic_o end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT(attribute-related question).cur_questionmeans each DSGObjObj{}^{\\text{Obj}}start_FLOATSUPERSCRIPT Obj end_FLOATSUPERSCRIPTquestion andkey_objectsmeans entity word in each question.",
                "position": 2007
            },
            {
                "img": "https://arxiv.org/html/2411.15115/x21.png",
                "caption": "Figure 21:Prompt to choose which object(s) to preserve.We ask GPT4o to select objects to preserve in the scene.",
                "position": 2013
            },
            {
                "img": "https://arxiv.org/html/2411.15115/x22.png",
                "caption": "Figure 22:Prompt to plan how to refine the other regions.We use five in-context examples to create the refinement prompt from the question related to other objects.",
                "position": 2016
            },
            {
                "img": "https://arxiv.org/html/2411.15115/x23.png",
                "caption": "Figure 23:Prompt for LLM paraphrasing.Following OPT2I[29], we ask GPT4 to generate\ndiverse paraphrases of each prompt for LLM paraphrasing baseline experiments.",
                "position": 2019
            }
        ]
    },
    {
        "header": "Appendix EAdditional Qualitative Examples",
        "images": []
    }
]