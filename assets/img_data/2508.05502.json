[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05502/imgs/radarfig.png",
                "caption": "Figure 1:Image caption task performance on COCO dataset(Lin et al.,2015)across multiple languages. Compared to GPT-4o(OpenAI et al.,2024), most of the outstanding MLLMs get the highest BLEU(Papineni et al.,2002)score in English.",
                "position": 136
            },
            {
                "img": "https://arxiv.org/html/2508.05502/imgs/radarfig.png",
                "caption": "Figure 1:Image caption task performance on COCO dataset(Lin et al.,2015)across multiple languages. Compared to GPT-4o(OpenAI et al.,2024), most of the outstanding MLLMs get the highest BLEU(Papineni et al.,2002)score in English.",
                "position": 139
            },
            {
                "img": "https://arxiv.org/html/2508.05502/x1.png",
                "caption": "Figure 2:Standard MLLMs (e.g., InternVL2-8B, Qwen2-VL-7B) trained on generic datasets often fail to generate meaningful output due to limited visual-linguistic alignment. An MLLM with enhanced linguistic capability may produce detailed descriptions. However, only an MLLM enriched with cultural knowledge can accurately recognize the depicted celebrity. All conversations are expected to be in Arabic; “EN” provides translation for clarity.",
                "position": 144
            }
        ]
    },
    {
        "header": "2Bridging Linguistic Capability and Cultural Groundedness",
        "images": []
    },
    {
        "header": "3MELLA: Instantiating the Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05502/imgs/data-pipeline2.png",
                "caption": "Figure 3:Data Collection Pipeline forMELLA. We first collect images with native alt-text from regional websites to form the cultural knowledge dataset (Dk​n​o​wD_{know}). For images without alt-text, we use a powerful MLLM to generate descriptive captions, which are then translated into target low-resource languages to form the linguistic capability dataset (Dl​i​n​gD_{ling}). The combination of these two sources creates our finalMELLAdataset.",
                "position": 533
            },
            {
                "img": "https://arxiv.org/html/2508.05502/imgs/mella_picnew.png",
                "caption": "Figure 4:Statistical overview of theMELLAdataset. Left: Main statistics including total sample numbers, sizes, and average text lengths across different languages. Middle: Circular diagram of the category distribution visualization. Right: Quantitative distribution showing the eight languages in the dataset with consistent color coding across the diagram. As shown, theMELLAdataset exhibits both broad coverage and balanced representation across topics and languages.",
                "position": 557
            },
            {
                "img": "https://arxiv.org/html/2508.05502/imgs/pic2_num.png",
                "caption": "",
                "position": 697
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05502/imgs/human_eval.png",
                "caption": "Figure 5:Human evaluation over 100 validation samples and 8 volunteers.",
                "position": 1419
            },
            {
                "img": "https://arxiv.org/html/2508.05502/x2.png",
                "caption": "Figure 6:A case study on AR demonstrates the effectiveness of our model in enhancing cultural groundedness. Both the questions and answers were originally in Arabic; for ease of reading, translations are provided here.",
                "position": 1540
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelated Work",
        "images": []
    },
    {
        "header": "Appendix BData collection details",
        "images": []
    },
    {
        "header": "Appendix CTraining Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05502/imgs/prompt_pool.png",
                "caption": "Figure 8:A subset of our prompt pool for training.",
                "position": 2525
            },
            {
                "img": "https://arxiv.org/html/2508.05502/x3.png",
                "caption": "(a)Cases on HU and KO. The cultural connotation has been recognized.",
                "position": 2609
            },
            {
                "img": "https://arxiv.org/html/2508.05502/x3.png",
                "caption": "(a)Cases on HU and KO. The cultural connotation has been recognized.",
                "position": 2612
            },
            {
                "img": "https://arxiv.org/html/2508.05502/x4.png",
                "caption": "(b)A case on TH. The issue of repeated outputs has been resolved, and Thai cultural elements have been incorporated into the descriptions.",
                "position": 2618
            },
            {
                "img": "https://arxiv.org/html/2508.05502/x5.png",
                "caption": "(c)A case on VI. The issue of repeated outputs has been resolved.",
                "position": 2624
            }
        ]
    },
    {
        "header": "Appendix DExperiment results",
        "images": []
    }
]