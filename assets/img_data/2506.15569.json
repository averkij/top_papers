[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.15569/x1.png",
                "caption": "",
                "position": 157
            },
            {
                "img": "https://arxiv.org/html/2506.15569/x2.png",
                "caption": "",
                "position": 163
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.15569/x3.png",
                "caption": "Figure 1:An illustration of the four subsets in theSciVerbenchmark. Our benchmark is designed to evaluate document-grounded scientific claim verification in a multimodal setting. To effectively perform this task, models must go through the full context of a scientific paper—including text, charts, and tables—to locate the appropriate supporting evidence before verifying a claim. The complete data examples are provided in AppendixC.",
                "position": 191
            },
            {
                "img": "https://arxiv.org/html/2506.15569/x4.png",
                "caption": "Table 1:Comparison ofSciVerwith existing claim verification and scientific literature comprehension benchmarks.",
                "position": 241
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3SciVerBenchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.15569/x4.png",
                "caption": "Figure 2:An overview of theSciVerbenchmark construction pipeline.",
                "position": 433
            },
            {
                "img": "https://arxiv.org/html/2506.15569/x5.png",
                "caption": "Table 2:Data statistics ofSciVer.",
                "position": 523
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.15569/x5.png",
                "caption": "Figure 4:Comparison of model performance on the validation set, with claims requiring varying amounts of annotated supporting evidence. Each piece of evidence is defined as a single table, chart, or paragraph (§3.4).",
                "position": 1526
            },
            {
                "img": "https://arxiv.org/html/2506.15569/x6.png",
                "caption": "Figure 5:Illustration of two error types: Visual Element Misinterpretation (left) and Failure in Multi-step Reasoning (right). Additional error examples are provided in AppendixC.",
                "position": 1530
            },
            {
                "img": "https://arxiv.org/html/2506.15569/x7.png",
                "caption": "Table 4:Performance comparison of GPT-4o-mini and Qwen2.5-VL-72B under different RAG settings.",
                "position": 1646
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASciVerBenchmark Construction",
        "images": []
    },
    {
        "header": "Appendix BConfigurations of Evaluated Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.15569/x7.png",
                "caption": "Table 6:Details of the multimodal foundation models evaluated in our study. Models are organized by organization and aligned with performance data from the main text.",
                "position": 2410
            },
            {
                "img": "https://arxiv.org/html/2506.15569/x7.png",
                "caption": "Figure 7:Illustration ofFailure to Retrieve Relevant Informationwith the example from theAnalytical Reasoningsubset.",
                "position": 2586
            },
            {
                "img": "https://arxiv.org/html/2506.15569/x8.png",
                "caption": "Figure 8:Illustration ofVisual element misinterpretationwith the example from theDirect Reasoningsubset.",
                "position": 2596
            },
            {
                "img": "https://arxiv.org/html/2506.15569/x9.png",
                "caption": "Figure 9:Illustration ofHeavy Reliance on Text Modalitywith the example from theAnalytical Reasoningsubset.",
                "position": 2606
            },
            {
                "img": "https://arxiv.org/html/2506.15569/x10.png",
                "caption": "Figure 10:Illustration ofDomain-Specific Misconceptionswith the example from theAnalytical Reasoningsubset.",
                "position": 2616
            },
            {
                "img": "https://arxiv.org/html/2506.15569/x11.png",
                "caption": "Figure 11:Illustration ofOther Observation Errorwith the example from theParallel Reasoningsubset.",
                "position": 2626
            }
        ]
    },
    {
        "header": "Appendix CError Analysis",
        "images": []
    }
]