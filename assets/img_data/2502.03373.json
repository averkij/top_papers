[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Problem Formulation",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.03373/x1.png",
                "caption": "Figure 1:Scaling curves of SFT and RL onLlama-3.1-8Bwith long CoTs and short CoTs. SFT with long CoTs can scale up to a higher upper limit and has more potential to further improve with RL.",
                "position": 278
            }
        ]
    },
    {
        "header": "3Impact of SFT on Long CoT",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.03373/x2.png",
                "caption": "Figure 2:BothLlama3.1-8BandQwen2.5-Math-7Bmodels trained under RL with the Classic Reward manifested emergent CoT length scaling past the context window size, resulting in a decline in MATH-500 accuracy. The red points on the charts correspond to the iteration where the accuracy dropped to near zero. ‚ÄúTerminated CoTs‚Äù refer to responses that conclude within the context length.",
                "position": 308
            }
        ]
    },
    {
        "header": "4Impact of Reward Design on Long CoT",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.03373/x3.png",
                "caption": "Figure 3:The Classic and Cosine Reward functions. The Cosine Reward varies with generation length.",
                "position": 435
            },
            {
                "img": "https://arxiv.org/html/2502.03373/x4.png",
                "caption": "Figure 4:Llama3.1-8Btrained with length shaping using the Cosine Reward exhibited more stable (a) training accuracy and (b) response length. This stability led to improved performance on downstream tasks (Figure5). Red points on the charts indicate iterations where training accuracy dropped to near zero.",
                "position": 516
            },
            {
                "img": "https://arxiv.org/html/2502.03373/x5.png",
                "caption": "Figure 5:Performance ofLlama-3.1-8Btrained with different reward functions on a variety of evaluation benchmarks.",
                "position": 519
            },
            {
                "img": "https://arxiv.org/html/2502.03373/x6.png",
                "caption": "Figure 6:Performance ofLlama-3.1-8Btrained with different context window sizes. All experiments used the same number of training samples.",
                "position": 560
            }
        ]
    },
    {
        "header": "5Scaling up Verifiable Reward",
        "images": []
    },
    {
        "header": "6Exploration on RL from the Base Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.03373/x7.png",
                "caption": "Figure 7:Dynamics of accuracies and reflection keyword rates on different benchmarks during our RL from the base modelQwen2.5-Math-7B. We do not see the keyword rates of ‚Äúwait‚Äù, ‚Äúalternatively‚Äù, and ‚Äúrecheck‚Äù get significantly improved during the RL training even though the accuracy is steadily increasing.",
                "position": 842
            },
            {
                "img": "https://arxiv.org/html/2502.03373/x8.png",
                "caption": "Figure 8:Dynamics of the output token lengths and the coding rate on MATH-500 and the KL divergence of the policy over the base model on MATH Lv3-5 (training data) during our RL fromQwen2.5-Math-7B.",
                "position": 845
            }
        ]
    },
    {
        "header": "7Discussions and Future Work",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelated Work",
        "images": []
    },
    {
        "header": "Appendix BFigures and Tables",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.03373/x9.png",
                "caption": "(a)Response lengths under different Cosine Rewards",
                "position": 1724
            },
            {
                "img": "https://arxiv.org/html/2502.03373/x9.png",
                "caption": "(a)Response lengths under different Cosine Rewards",
                "position": 1727
            },
            {
                "img": "https://arxiv.org/html/2502.03373/x10.png",
                "caption": "(b)Reward A",
                "position": 1732
            },
            {
                "img": "https://arxiv.org/html/2502.03373/x11.png",
                "caption": "(c)Reward B",
                "position": 1737
            },
            {
                "img": "https://arxiv.org/html/2502.03373/x12.png",
                "caption": "(d)Reward C",
                "position": 1742
            },
            {
                "img": "https://arxiv.org/html/2502.03373/x13.png",
                "caption": "Figure 10:CoT branching frequency, estimated by the keyword count of the pivot word ‚Äùalternatively,‚Äù, decreased under the Cosine Reward with more training compute. We attributed this, along with increased repetition, to reward hacking.",
                "position": 1749
            },
            {
                "img": "https://arxiv.org/html/2502.03373/x14.png",
                "caption": "Figure 11:Branching frequency in CoT at differentŒ≥csubscriptùõæùëê\\gamma_{c}italic_Œ≥ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPTvalues. Lowering the discount factor increased branching frequency, causing the model to abandon problem-solving approaches more quickly.",
                "position": 1752
            },
            {
                "img": "https://arxiv.org/html/2502.03373/x15.png",
                "caption": "Figure 12:Training response length of models trained with Cosine Reward with and without repetition penalty. We see that repetition penalty reduced the length.",
                "position": 1755
            },
            {
                "img": "https://arxiv.org/html/2502.03373/x16.png",
                "caption": "Figure 13:Reinforce with classic reward shows signs of training instability.",
                "position": 1758
            }
        ]
    },
    {
        "header": "Appendix CAlgorithms and Formulas",
        "images": []
    },
    {
        "header": "Appendix DExtracts",
        "images": []
    },
    {
        "header": "Appendix EExperimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.03373/x17.png",
                "caption": "Figure 14:MMLU-Pro test distribution before/after downsampling for the MMLU-Pro-1k subset. The subset is i.i.d. to the full set.",
                "position": 2057
            }
        ]
    },
    {
        "header": "Appendix FLong CoT Patterns in Pre-training Data",
        "images": []
    }
]