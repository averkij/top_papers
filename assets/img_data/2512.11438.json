[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.11438/",
                "caption": "",
                "position": 109
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.11438/x2.png",
                "caption": "Figure 2:Flowception sampling.At each iteration, the model predicts, for each frameii, a velocity field and an insertion rateŒªi\\lambda_{i}.\nVelocities are used to denoise frames while the insertion rates define the probability to insert a new frame to the right of existing ones.\nThe model uses per-frame time values, set toti=0t_{i}\\!=\\!0when they are inserted, and reachingti=1t_{i}\\!=\\!1when they are fully denoised.",
                "position": 127
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Flowception",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.11438/x3.png",
                "caption": "Figure 3:Illustration of the extended time scheduler for Flowception  training.In Flowception , each frame has its own denoising time which depends on its insertion time.\nThe global extended timeœÑg\\tau_{g}progresses from0to22, where insertion of new frames only occur whenœÑg<1\\tau_{g}<1.\nStarting frames (in blue) are instantiated atœÑg=0\\tau_{g}=0, other frames (in orange) are inserted later (whenœÑg>0\\tau_{g}>0) and thus have a delay.\nWith a linear scheduler, the insertion delays follow a uniform distribution.",
                "position": 450
            },
            {
                "img": "https://arxiv.org/html/2512.11438/x4.png",
                "caption": "Figure 4:Flowception natively supports different tasks.By choosing active and passive context frames the model can be used for\nimage-to-video, video-to-video, text-to-image,\nframe interpolation, text-to-video, and scene completion.",
                "position": 492
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.11438/x5.png",
                "caption": "Figure 5:Image-to-video generation using model trained on Tai-Chi-HD.We show two examples (one per row) of generated video frames along the insertion time of each frame.\nIn general, frames inserted early, witht‚âà0t\\approx 0, define the movement dynamics (large changes w.r.t. the context frame), while later frames smoothly interpolate motion, resulting in smaller changes w.r.t. to neighboring frames.",
                "position": 653
            },
            {
                "img": "https://arxiv.org/html/2512.11438/x6.png",
                "caption": "Figure 6:Video interpolation results obtained with with Flowception trained on the Kinetics-600 dataset.Context frames are highlighted with dashed lines, insertion times of other frames are marked using a color map and printed on each frame.",
                "position": 712
            },
            {
                "img": "https://arxiv.org/html/2512.11438/x7.png",
                "caption": "Figure 7:Impact of rate guidance on I2V video length.Samples obtained without guidance (ws=1w_{s}\\!=\\!1, horizontal) and with guidance (ws=5w_{s}\\!=\\!5, vertical) on RealEstate10K, using the same seed for each conditioning frame.",
                "position": 797
            },
            {
                "img": "https://arxiv.org/html/2512.11438/x8.png",
                "caption": "Figure 8:Comparing local attention variants.We report FVD on RealEstate10K after training for 300k iterations with different attention context windows. For each frame, its context is made of itself and the previous/next context window frames.",
                "position": 910
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Full derivations",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.11438/x9.png",
                "caption": "Figure 9:Illustration of the coupling between source and target distributions in augmented spaceùíµ\\mathcal{Z}.Starting frames are initialized as noise vectorsœµ‚àºùí©‚Äã(0,I)\\epsilon\\sim\\mathcal{N}(0,I)while others are blank tokens in augmented space which are transformed into noise vectors at their corresponding insertion time.\nHorizontal lines on the arrows indicate the time when a frame is revealed in the schedule.",
                "position": 1618
            }
        ]
    },
    {
        "header": "7Additional derivations and remarks",
        "images": []
    },
    {
        "header": "8Discussions",
        "images": []
    },
    {
        "header": "9Efficiency Comparison",
        "images": []
    },
    {
        "header": "10Algorithms & implementation",
        "images": []
    },
    {
        "header": "11Additional experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.11438/figures/toy_length.png",
                "caption": "Figure 10:Video length matching.Our framework is able to accurately reproduce the length of videos from the toy dataset.",
                "position": 2593
            },
            {
                "img": "https://arxiv.org/html/2512.11438/x10.png",
                "caption": "Figure 11:Efficiency comparison.We compare the sampling efficiency of autoregressive model (with caching) with Flowception , we plot the FVD on RealEstate10K as a function of the FLOPs used for sampling when varying the number of sampling steps.",
                "position": 2603
            }
        ]
    },
    {
        "header": "12Broader societal impact",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.11438/x11.png",
                "caption": "Figure 12:Comparing different methods using models trained on the Tai-Chi-HD dataset.Using the same input frame (left) and random seed, we compare generations with the autoregressive, full-sequence and Flowception models.",
                "position": 2616
            },
            {
                "img": "https://arxiv.org/html/2512.11438/x12.png",
                "caption": "Figure 13:Additional qualitative examples on Taichi.Each row corresponds to a different video obtained with our method for image-to-video generation",
                "position": 2619
            },
            {
                "img": "https://arxiv.org/html/2512.11438/x13.png",
                "caption": "Figure 14:Additional qualitative examples on Kinetics 600 interpolation.Each row corresponds to a different video where the first and last frame are given and up to two extra middle frames are also given. Insertion time is highlighted in the boder color of each frame, context frames are highlighted with daashed lines.",
                "position": 2637
            },
            {
                "img": "https://arxiv.org/html/2512.11438/x14.png",
                "caption": "Figure 15:Additional qualitative examples on RealEstate10K interpolation.Each row corresponds to a different video where the first and last frame are given and up to two extra middle frames are also given. Insertion time is highlighted in the boder color of each frame, context frames are highlighted with daashed lines.",
                "position": 2640
            },
            {
                "img": "https://arxiv.org/html/2512.11438/x15.png",
                "caption": "Figure 16:Qualitative examples of Image-to-Video generation.Using Flowception trained on the RealEstate10K dataset.\nFirst shown frame is given as context.\nGiven the initial frame, we generate videos of at most 145 frames at 16 FPS, corresponding to 9.06 secs.",
                "position": 2643
            }
        ]
    },
    {
        "header": "13Additional qualitative examples",
        "images": []
    }
]