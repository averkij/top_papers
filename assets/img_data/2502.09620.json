[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09620/extracted/6201996/intro3.png",
                "caption": "Figure 1:Issues of encoder-based 3D LMMs.(a)Point Cloud Resolution Limitation.During training, the point cloud size (P.T. size) and point token size (P.T. size) are fixed at 8192 and 512, respectively.\nAnd we adjust these two sizes during inference, point cloud size from 2K to 16K and the corresponding point token size from 128 to 2048.\nWe evaluate them on the captioning task of the Objaverse benchmark using GPT-4 scores as the evaluation metric.\n(b)Embedding Semantic Discrepancy.We visualize the attention scores of the average text token to the point tokens, wherered indicates higher values.The point tokens in the encoder-free architecture exhibit stronger textual semantic relevance needed for the LLM.",
                "position": 120
            },
            {
                "img": "https://arxiv.org/html/2502.09620/x1.png",
                "caption": "Figure 2:Overall Pipeline ofEnel.The training is divided into two stages: the pre-training stage and the instruction tuning stage. In the first stage, we set the firstKùêæKitalic_Klayers to be learnable and apply the proposed Hybrid Semantic Loss to embed high-level semantics into the LLM. In the second stage, we adopt the Hierarchical Geometric Aggregation strategy to capture local structures of point clouds.",
                "position": 141
            }
        ]
    },
    {
        "header": "2Investigation of Encoder-free 3D LMM",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09620/x2.png",
                "caption": "Figure 3:Point Cloud Self-Supervised Learning Losses.In the pre-training stage, we explore common self-supervised learning losses for the encoder-free 3D LMM: (a) Masked Modeling Loss, (b) Reconstruction Loss, (c) Contrastive Loss, and (d) Knowledge Distillation Loss.\nThe (e) represents our proposed Hybrid Semantic Loss, specifically designed for the encoder-free architecture.",
                "position": 333
            },
            {
                "img": "https://arxiv.org/html/2502.09620/x3.png",
                "caption": "Figure 4:Hierarchical Geometry Aggregation Strategy.In the instruction tuning stage, we apply aggregation and propagation operations to the point tokens to capture the local structural details.",
                "position": 620
            },
            {
                "img": "https://arxiv.org/html/2502.09620/x4.png",
                "caption": "Figure 5:Difference in Semantic Encoding.By visualizing the attention scores of the average text token to the point tokens on the Objaverse dataset, we compare the semantic encoding potential of encoder-based and encoder-free architectures, where red indicates higher values.\nAnd (a) represents chairs, (b) represents airplanes, and (c) represents lamps.",
                "position": 793
            }
        ]
    },
    {
        "header": "3Results and Visualization",
        "images": []
    },
    {
        "header": "4Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelated Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09620/extracted/6201996/loss2.png",
                "caption": "Figure 6:Variants of Point Cloud Self-Supervised Learning Losses.(a) The Variant of Masked Modeling Loss, (b) The Variant of Reconstruction Loss, (c) The Variant of Hybrid Semantic Loss.",
                "position": 1597
            }
        ]
    },
    {
        "header": "Appendix BExperimental Settings",
        "images": []
    },
    {
        "header": "Appendix CMore Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09620/extracted/6201996/output3.png",
                "caption": "Figure 7:EnelOutput Examples.We demonstrate thatEnelprovides precise and diverse responses when addressing different problems.",
                "position": 1828
            }
        ]
    },
    {
        "header": "Appendix DModel Output",
        "images": []
    }
]