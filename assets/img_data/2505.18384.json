[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18384/x1.png",
                "caption": "(a)Adversaries‚Äô Degrees of Freedom in Cybersecurity Agents",
                "position": 198
            },
            {
                "img": "https://arxiv.org/html/2505.18384/x1.png",
                "caption": "(a)Adversaries‚Äô Degrees of Freedom in Cybersecurity Agents",
                "position": 201
            },
            {
                "img": "https://arxiv.org/html/2505.18384/x2.png",
                "caption": "(b)Dynamic Risk Assessment",
                "position": 206
            }
        ]
    },
    {
        "header": "2Cybersecurity is Amenable to Self-Improvement",
        "images": []
    },
    {
        "header": "3Threat Model and Degrees of Freedom",
        "images": []
    },
    {
        "header": "4Experiment Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18384/x3.png",
                "caption": "Figure 2:Increasing the number of repeated sampleskùëòkitalic_kand max rounds of interactionsNùëÅNitalic_Nwill significantly improve the accuracy, though the rate of improvement slows due to diminishing returns.",
                "position": 567
            },
            {
                "img": "https://arxiv.org/html/2505.18384/x4.png",
                "caption": "Figure 3:Iterative prompt refinement can help the agent to search more efficiently, resulting in higher pass@kùëòkitalic_kscores compared to repeated sampling.",
                "position": 586
            },
            {
                "img": "https://arxiv.org/html/2505.18384/x5.png",
                "caption": "Figure 4:Self-Training shows in-domain generalization, even without a large amount of data or external assistance. However, it comes with trade-offs in generation diversity, especially when the model is fine-tuned for more epochs.",
                "position": 608
            },
            {
                "img": "https://arxiv.org/html/2505.18384/x6.png",
                "caption": "Figure 5:Even using the same core model for the meta agent and the offensive cybersecurity agent, we can still find a better workflow via iterative workflow refinement. We evaluate each workflow 5 times and report the best average pass@1111score as the performance.",
                "position": 627
            },
            {
                "img": "https://arxiv.org/html/2505.18384/x7.png",
                "caption": "Figure 6:Relationship between performance improvement and compute cost for different degrees of freedom in (a) stateful, and (b) non-stateful environments. In the non-stateful setting, multiple traces are shown for the ‚ÄúIncreasing Max Rounds‚Äù, each corresponding to a different value ofNùëÅNitalic_N. For clarity, we also add the estimated financial cost for the GPU Hours spent888We estimate the financial cost based on the pricing of p5.48xlarge from AWS:https://aws.amazon.com/ec2/capacityblocks/pricing/..",
                "position": 646
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "7Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImpact Statement",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18384/x8.png",
                "caption": "Figure 7:The structure of a typical CTF challenge from InterCode CTF benchmark.",
                "position": 1870
            },
            {
                "img": "https://arxiv.org/html/2505.18384/x9.png",
                "caption": "Figure 8:Length distribution of the training data.",
                "position": 1970
            }
        ]
    },
    {
        "header": "Appendix CFailure Mode Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18384/x10.png",
                "caption": "Figure 9:The failure mode distribution of the repeated sampling experiment on differentNùëÅNitalic_Nacross three benchmarks.",
                "position": 2272
            },
            {
                "img": "https://arxiv.org/html/2505.18384/x11.png",
                "caption": "Figure 10:Failure mode distribution of: iterative prompt refinement (left), and repeated sampling with multiple runs(right).",
                "position": 2285
            },
            {
                "img": "https://arxiv.org/html/2505.18384/x12.png",
                "caption": "Figure 11:Failure mode distribution of: the agent self-trained for 5 epochs and 10 epochs (left), and the agent with its workflow refined for 2 iterations and 9 iterations (right).",
                "position": 2510
            }
        ]
    },
    {
        "header": "Appendix DQualitative Examples",
        "images": []
    }
]