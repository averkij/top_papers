[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23571/x1.png",
                "caption": "Figure 1:RobotArena∞\\inftyprovides a scalable and extensible robot benchmarking framework by automating environment construction and evaluation.It automatically generates simulated environment seeded from real videos, deploys robot policies, and evaluates them using VLMs and crowdsourced workers that cast preferences between pairs of execution videos.\nThe simulated environments are derived from both in-distribution and out-of-distribution videos, enabling rigorous tests of generalization in contemporary VLAs.",
                "position": 137
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Translating Videos to Simulation for Policy Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23571/x2.png",
                "caption": "Figure 2:Automated video-to-simulation translationin RobotArena∞\\infty. Given a frame from a robot demonstration video, we automatically create a corresponding simulated environment.",
                "position": 236
            },
            {
                "img": "https://arxiv.org/html/2510.23571/x3.png",
                "caption": "Figure 3:Automated robot-camera calibration through differentiable rendering of pose-conditioned 3D robot Gaussians.",
                "position": 252
            }
        ]
    },
    {
        "header": "4Evaluating Robot Trajectories with Humans and VLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23571/figs/shuffleseq.png",
                "caption": "Figure 4:We obtain task progress scores for execution videos automatically by prompting Gemini with a shuffled frame sequence and the language instruction, followingMa et al. (2024).",
                "position": 304
            }
        ]
    },
    {
        "header": "5Benchmarking Robot Policies in RobotArena∞\\infty",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23571/x4.png",
                "caption": "Figure 5:We show simulation environments in RobotArena∞\\inftyseeded from videos demonstrations in the datasets of Bridge, RH20T and DROID.",
                "position": 380
            },
            {
                "img": "https://arxiv.org/html/2510.23571/assets/datasets_comparison_plot.png",
                "caption": "(a)",
                "position": 420
            },
            {
                "img": "https://arxiv.org/html/2510.23571/assets/datasets_comparison_plot.png",
                "caption": "(a)",
                "position": 423
            },
            {
                "img": "https://arxiv.org/html/2510.23571/assets/tests_comparison_plot.png",
                "caption": "(b)",
                "position": 428
            },
            {
                "img": "https://arxiv.org/html/2510.23571/assets/bt_scores_exponentiated.png",
                "caption": "Figure 7:BT scores for human preferences of the models.",
                "position": 443
            },
            {
                "img": "https://arxiv.org/html/2510.23571/assets/GVL_L30_Score_realsim_compact.png",
                "caption": "Figure 8:Validation of Simulation-Based Robot Evaluation Against Real-World Robot Evaluations.",
                "position": 460
            },
            {
                "img": "https://arxiv.org/html/2510.23571/assets/GVL_L30_Score_realsim_compact.png",
                "caption": "Figure 8:Validation of Simulation-Based Robot Evaluation Against Real-World Robot Evaluations.",
                "position": 462
            },
            {
                "img": "https://arxiv.org/html/2510.23571/assets/GVL_L30_BridgeSim_vs_RobotArena.png",
                "caption": "Figure 9:Task completion policy evaluation results in RobotArena∞\\inftyversus SIMPLER benchmark ofLi et al. (2024).",
                "position": 466
            }
        ]
    },
    {
        "header": "6Limitations / Future Directions",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AInferring Camera-Robot Pose Transformation via Analysis-by-Synthesis",
        "images": []
    },
    {
        "header": "Appendix BRelevant Object Segmentation, 3D Reconstruction, and Material Property Estimation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23571/figs/appendix/inpaint.jpg",
                "caption": "Figure 10:Background Impainting Results. The top row shows the original RGB images with task-relevant objects present; the bottom row shows the corresponding inpainted images after foreground object removal, where only backgrounds remain.",
                "position": 1820
            }
        ]
    },
    {
        "header": "Appendix CBackground Inpainting",
        "images": []
    },
    {
        "header": "Appendix DSystem Identification",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23571/x5.png",
                "caption": "Figure 11:Inferring robot control gains for matching robot trajectories in reality and simulation.The end-effector trajectory, projected onto the XY plane, is shown for both the pre-identification (a,c) and post-identification (b,d) stages. After system identification, the simulated trajectory (blue) aligns closely with the recorded dataset trajectory (red), whereas significant deviations are observed prior to identification.",
                "position": 1891
            }
        ]
    },
    {
        "header": "Appendix EBridgeSimEnvironment Visualizations",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23571/figs/appendix/scene2.jpg",
                "caption": "Figure 12:Visualizations of a subset ofBridgeSimEnvironments",
                "position": 1901
            },
            {
                "img": "https://arxiv.org/html/2510.23571/figs/appendix/droid.jpg",
                "caption": "Figure 13:Visualizations of a subset ofDROIDSimEnvironments",
                "position": 1904
            }
        ]
    },
    {
        "header": "Appendix FEnvironment Perturbations",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23571/figs/appendix/background.jpg",
                "caption": "Figure 14:Background Change Example. The top-left image shows the original image without background perturbations.",
                "position": 1969
            },
            {
                "img": "https://arxiv.org/html/2510.23571/figs/appendix/color.jpg",
                "caption": "Figure 15:Color Shift Example. The leftmost image shows the original image without color perturbations.",
                "position": 1972
            },
            {
                "img": "https://arxiv.org/html/2510.23571/figs/appendix/pose.jpg",
                "caption": "Figure 16:Object Position Perturbation Example. The top-left image shows the original image with base permutation.",
                "position": 1975
            }
        ]
    },
    {
        "header": "Appendix GAutomated Task Progress Scoring with VLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23571/figs/appendix/gvl6.jpg",
                "caption": "Figure 17:Example VLM‐generated task evaluation curves on base environment.Left panels:Representative frames sampled at low‐ and high‐progress points.Right panels:VLM‐assigned completion score (y‐axis) across frame ID (x‐axis).",
                "position": 1987
            },
            {
                "img": "https://arxiv.org/html/2510.23571/figs/appendix/gvl5.jpg",
                "caption": "",
                "position": 1991
            },
            {
                "img": "https://arxiv.org/html/2510.23571/figs/appendix/gvl1.jpg",
                "caption": "Figure 18:Example VLM‐generated task evaluation curves on perturbed environments.Top:A high‐progress moment immediately after the object lift, for which the VLM predicts a completion score of approximately 70%.Bottom:An inconsequential action with no strong effect on task progress, for which the VLM predicts a low completion score of approximately 10%.",
                "position": 1999
            },
            {
                "img": "https://arxiv.org/html/2510.23571/figs/appendix/gvl2.jpg",
                "caption": "",
                "position": 2003
            },
            {
                "img": "https://arxiv.org/html/2510.23571/figs/appendix/gvl3.jpg",
                "caption": "Figure 19:Example VLM‐generated task evaluation curves on perturbed environments.Top:A successful pick‐and‐place execution—after the object lift the VLM score climbs steadily and correctly shows task completion.Bottom:An unsuccessful trajectory with completion score remaining below  20%, demonstrating the VLM’s capacity to detect failure to complete the task.",
                "position": 2011
            },
            {
                "img": "https://arxiv.org/html/2510.23571/figs/appendix/gvl4.jpg",
                "caption": "",
                "position": 2015
            }
        ]
    },
    {
        "header": "Appendix HVLM Evaluations",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23571/assets/human_ui.png",
                "caption": "Figure 20:Human evaluation interface. Participants viewed two policies side-by-side and selected the one that performed better or if it was a tie.",
                "position": 2041
            }
        ]
    },
    {
        "header": "Appendix IHuman Evaluations",
        "images": []
    }
]