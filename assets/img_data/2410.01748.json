[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01748/x1.png",
                "caption": "Figure 1:Reasoning Gap:Most models demonstrate a noticeable gap between their reasoning performance on GSM8K and compositional GSM, in which pairs of GSM8K test questions are chained together so that the answer of the first question (Q1subscriptùëÑ1Q_{1}italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT) is a variable in the second one (Q2subscriptùëÑ2Q_{2}italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT). The model is required to correctly answer both questions to solve the problem. If a model has an accuracy ofS1subscriptùëÜ1S_{1}italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTon theQ1subscriptùëÑ1Q_{1}italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTset, andS2subscriptùëÜ2S_{2}italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTonQ2subscriptùëÑ2Q_{2}italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTset, then the expected compositional GSM accuracy isS1√óS2subscriptùëÜ1subscriptùëÜ2{S_{1}\\times S_{2}}italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT √ó italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. The x-axis corresponds to the geometric meanS1√óS2subscriptùëÜ1subscriptùëÜ2\\sqrt{S_{1}\\times S_{2}}square-root start_ARG italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT √ó italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG, labeled GSM8K accuracy for simplicity. The trend-liney=x2ùë¶superscriptùë•2y=x^{2}italic_y = italic_x start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTis the expected compositional GSM accuracy.",
                "position": 108
            },
            {
                "img": "https://arxiv.org/html/2410.01748/x2.png",
                "caption": "Figure 3:Reasoning Gapof notable open-weights and closed-source LLMs. Smaller, more cost-efficient and math specialized models have a bigger gap. SeeFigure¬†1for GSM and compositional GSM accuracy.",
                "position": 196
            },
            {
                "img": "https://arxiv.org/html/2410.01748/x3.png",
                "caption": "Figure 4:Cost efficient LLMs reason differently:showing four family of models, each having a high-cost and low-cost option. The numbers above the bars represents the reasoning gap defined in Eq1. Although the cheaper models perform similarly on the original GSM8K test, they show a significant decline in performance on the compositional GSM test.",
                "position": 212
            },
            {
                "img": "https://arxiv.org/html/2410.01748/x4.png",
                "caption": "Figure 5:Impact of Instruction-Tuning on Compositional GSM. We compare pretrained and instruction-following tuned variant of models from Mistral, LLAMA3 and Gemma2 families. Numbers above bars represent improvements from instruction-tuning on each set. For smaller models(top), we observe that instruction-tuning results in substantial improvements on the original GSM8K test set, but a much smaller improvement on the compositional GSM test. However, this pattern does not typically hold for larger models(bottom).",
                "position": 237
            },
            {
                "img": "https://arxiv.org/html/2410.01748/x4.png",
                "caption": "",
                "position": 240
            },
            {
                "img": "https://arxiv.org/html/2410.01748/x5.png",
                "caption": "",
                "position": 245
            },
            {
                "img": "https://arxiv.org/html/2410.01748/x6.png",
                "caption": "Figure 6:Math-Specialized LLMs on Compositional GSM. We evaluate the performance of three models specifically designed for math problem-solving to explore whether extensive specialized training in mathematics can bridge the reasoning gap observed among models of similar size or family. Surprisingly, we find that such math-specialized LLMs, particularly the smaller models, exhibit similar reasoning gaps and signs of overfitting to standard benchmarks.",
                "position": 261
            },
            {
                "img": "https://arxiv.org/html/2410.01748/x6.png",
                "caption": "Figure 6:Math-Specialized LLMs on Compositional GSM. We evaluate the performance of three models specifically designed for math problem-solving to explore whether extensive specialized training in mathematics can bridge the reasoning gap observed among models of similar size or family. Surprisingly, we find that such math-specialized LLMs, particularly the smaller models, exhibit similar reasoning gaps and signs of overfitting to standard benchmarks.",
                "position": 264
            },
            {
                "img": "https://arxiv.org/html/2410.01748/x7.png",
                "caption": "Figure 7:Overfitting with supervised fine-tuning.\nWe finetune Gemma2 27B on the original GSM8K training solutions, and self-generated solutions. In both settings, after 100 training steps, compositional GSM test performance drops while GSM8K test performance keeps improving. No improvements were observed on either split after 400 steps.",
                "position": 269
            },
            {
                "img": "https://arxiv.org/html/2410.01748/x8.png",
                "caption": "Figure 8:Natural Language CoTversusCode:Generating code to solve the problems helps in both settings of original test split and compositional GSM split.\nNumbers above bars represent relative improvements over natural language Chain-of-Thought (CoT) generation.\nSmaller models benefit more from generating code rather than natural language CoT to solve compositional GSM questions, further highlighting that smaller models demonstrate systematic differences in reasoning capabilities.",
                "position": 302
            },
            {
                "img": "https://arxiv.org/html/2410.01748/x9.png",
                "caption": "Figure 9:Original (Q1) v.s. Modified GSM8K (Q2) test accuracy. Most models are very close to thex=yùë•ùë¶x=yitalic_x = italic_yline, indicating that test set leakage is not a significant concern. Modified GSM8K questions are created by modifying a number in the original questions while ensuring that the new final answer remains a positive integer and is reasonably close to the original one.",
                "position": 315
            },
            {
                "img": "https://arxiv.org/html/2410.01748/x10.png",
                "caption": "Figure 10:Some LLMs get distracted easily:Measuring models‚Äô ability to solve a question in the standard format (non-compositional) versus solving the same question asQ1subscriptùëÑ1Q_{1}italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTin the compositional format. Models below the trend-line get distracted and cannot answerQ1subscriptùëÑ1Q_{1}italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTin the compositional format even though solving it does not depend on solving any other question. The models generally adhere well to the output format provided in the 8-shot context, resulting in negligible instances of non-extractable answers.",
                "position": 318
            },
            {
                "img": "https://arxiv.org/html/2410.01748/x11.png",
                "caption": "Figure 11:Can models answer the second question if they have correctly answered the first one?Here, we compare how often models are able to solve a question independently to how often they are able to solve them in the compositional format given that the first question is solved correctly. This is an alternate measurement of the compositional reasoning gap. If a model can solve a question independently, it should be able to solve it in a compositional setting given that the prerequisites are met. The gap from the diagonal line suggests that some models have overfit to the format of GSM8K type questions. While models may correctly answer the first question, they frequently makes subtle errors and miss key details when solving the second question.",
                "position": 334
            },
            {
                "img": "https://arxiv.org/html/2410.01748/x12.png",
                "caption": "Figure 12:Models Have the Capacity to Solve Two Questions Together:Comparing models‚Äô ability to solve a question (Q2subscriptùëÑ2Q_{2}italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT) in three contexts: the standard format (non-compositional), withQ1subscriptùëÑ1Q_{1}italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTin the context without depending on its answer, and in the compositional format given thatQ1subscriptùëÑ1Q_{1}italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTis solved correctly. The distraction fromQ1subscriptùëÑ1Q_{1}italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTin the context is minimal whenQ2subscriptùëÑ2Q_{2}italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTis independent of it. However, whenQ2subscriptùëÑ2Q_{2}italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTrelies on the answer fromQ1subscriptùëÑ1Q_{1}italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, models struggle to solveQ2subscriptùëÑ2Q_{2}italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTaccurately, even ifQ1subscriptùëÑ1Q_{1}italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPThas been answered correctly.",
                "position": 369
            },
            {
                "img": "https://arxiv.org/html/2410.01748/x13.png",
                "caption": "Figure A.1:Distribution of final answer magnitudesfrom the test set of original GSM8K and compositional GSM benchmark. The number modification in the compositional benchmark was done in a way to ensure that the new final answer is a positive integer not too far from the old answer. Our compositional GSM benchmark has a similar distribution of final answers.",
                "position": 1352
            }
        ]
    },
    {
        "header": "Appendices",
        "images": []
    }
]