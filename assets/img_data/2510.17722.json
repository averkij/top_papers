[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17722/x1.png",
                "caption": "Figure 1:Illustration of multi-turn dialogues under single-scene and cross-scene settings. The evaluated questions corresponding to tasks are marked with underlining, and the scenes involved in the entire multi-turn dialogues are marked with blue dotted boxes.",
                "position": 142
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3MT-Video-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17722/x2.png",
                "caption": "Figure 2:An overview of the semi-automatic data construction process of MT-Video-Bench.",
                "position": 295
            },
            {
                "img": "https://arxiv.org/html/2510.17722/x3.png",
                "caption": "Figure 3:Overview of MT-Video-Bench. (a) Video Categories. MT-Video-Bench includes videos spanning 5 major categories, ensuring diverse topical coverage. (b) Task Distribution. MT-Video-Bench consists of a total of 6 tasks with a relatively balanced distribution. (c) Video Duration Distribution. MT-Video-Bench includes both long and short videos. (d) Dialogue Turn Distribution. Multi-turn dialogues in MT-Video-Bench involve 5 to 8 rounds.",
                "position": 387
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17722/x4.png",
                "caption": "Figure 4:Performance comparison of Qwen2.5-VL-7B, InternVL3.5-8B (Think), and Gemini 2.5 Pro across various tasks under single-scene and cross-scene settings.",
                "position": 1342
            },
            {
                "img": "https://arxiv.org/html/2510.17722/x5.png",
                "caption": "Figure 5:Performance of different video lengths, dialogue turns, and settings of dialogue context.",
                "position": 1368
            },
            {
                "img": "https://arxiv.org/html/2510.17722/figs/frame_name.png",
                "caption": "Figure 6:Ablation results of frames on different abilities. (a) Results of Object Reference, Memory Recall, Content Summary, and Proactive Interaction; (b) Results of Answer Refusal and Topic Shifting.",
                "position": 1403
            },
            {
                "img": "https://arxiv.org/html/2510.17722/figs/draw_resulotion.png",
                "caption": "Figure 7:Ablation results of resolutions on different abilities.",
                "position": 1426
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails on the Data Generation",
        "images": []
    },
    {
        "header": "Appendix BDetails on the Evaluation Settings",
        "images": []
    },
    {
        "header": "Appendix CMore Cases",
        "images": []
    }
]