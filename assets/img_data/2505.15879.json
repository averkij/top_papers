[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15879/x1.png",
                "caption": "Figure 1:Comparison of reasoning with pure natural language and grounded reasoning from GRIT that mixes explicit bounding boxes for image regions with a chain of natural language thoughts. Our GRIT method enables MLLMs to perform grounded reasoning with only 20 training samples, realizing a clear and reliable process of thinking with images.",
                "position": 79
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3GRIT: Grounded Reasoning with Images and Text",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15879/x2.png",
                "caption": "Figure 2:Model update via GRPO-GR. During GRPO-GR training, we sample a group of model completions and calculate the grounded-reasoning-format reward (rformatsubscriptùëüformatr_{\\text{format}}italic_r start_POSTSUBSCRIPT format end_POSTSUBSCRIPT), the optional grounded-target-counting reward (rcountsubscriptùëücountr_{\\text{count}}italic_r start_POSTSUBSCRIPT count end_POSTSUBSCRIPT), and the GPT-aided answer-accuracy reward (ranssubscriptùëüansr_{\\text{ans}}italic_r start_POSTSUBSCRIPT ans end_POSTSUBSCRIPT). The rewards are used to calculate the group-normalized advantage and guide the policy optimization.",
                "position": 219
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15879/extracted/6464174/images/eg1.png",
                "caption": "iModel provides an answer with regions and then reflects on it in the subsequent reasoning.",
                "position": 420
            },
            {
                "img": "https://arxiv.org/html/2505.15879/extracted/6464174/images/eg1.png",
                "caption": "iModel provides an answer with regions and then reflects on it in the subsequent reasoning.",
                "position": 423
            },
            {
                "img": "https://arxiv.org/html/2505.15879/extracted/6464174/images/eg2.png",
                "caption": "iiModel first grounds critical image region in its reasoning and then analyze in the subsequent reasoning.",
                "position": 463
            },
            {
                "img": "https://arxiv.org/html/2505.15879/extracted/6464174/images/eg3.jpg",
                "caption": "iiiModel correctly handles queries about non-existent entities without any grounding action.",
                "position": 496
            },
            {
                "img": "https://arxiv.org/html/2505.15879/extracted/6464174/images/bar_chart.png",
                "caption": "Figure 4:Correlation between image regions and \"thoughts\" in grounded reasoning evaluated by our Vision-Language Reasoning Cross-Modal Correlation metric. The result shows that models trained with GRIT outperform baselines.",
                "position": 542
            },
            {
                "img": "https://arxiv.org/html/2505.15879/extracted/6464174/images/bar_chart.png",
                "caption": "Figure 4:Correlation between image regions and \"thoughts\" in grounded reasoning evaluated by our Vision-Language Reasoning Cross-Modal Correlation metric. The result shows that models trained with GRIT outperform baselines.",
                "position": 545
            },
            {
                "img": "https://arxiv.org/html/2505.15879/extracted/6464174/images/ours_original_gqa_100_att_weight_same_prompt_from_h100_qwen_150.png",
                "caption": "Figure 5:Model‚Äôs average attention for image tokens during the generation of rethink segments. The overall higher curve for the original rethink segments of the GRIT-trained model shows that the bounding boxes generated facilitate stronger attention to the image input in subsequent reasoning.",
                "position": 550
            },
            {
                "img": "https://arxiv.org/html/2505.15879/extracted/6464174/images/acc_with_sem.png",
                "caption": "Figure 6:Effect of scaling training data on model performance.",
                "position": 560
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails for Testing Data",
        "images": []
    },
    {
        "header": "Appendix BDetails for Training data",
        "images": []
    },
    {
        "header": "Appendix CAblation on Counting-related Training Data and Reward",
        "images": []
    },
    {
        "header": "Appendix DPrompts",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15879/extracted/6464174/images/eg4.png",
                "caption": "iOur GRIT model accurately perceives the grounded target, reasons about the potential ambiguity in the question, and finally outputs a correct answer.",
                "position": 1266
            },
            {
                "img": "https://arxiv.org/html/2505.15879/extracted/6464174/images/eg4.png",
                "caption": "iOur GRIT model accurately perceives the grounded target, reasons about the potential ambiguity in the question, and finally outputs a correct answer.",
                "position": 1269
            },
            {
                "img": "https://arxiv.org/html/2505.15879/extracted/6464174/images/eg5.png",
                "caption": "iiAlthough the bounding boxes generated during the grounded reasoning are slightly off, our GRIT model is still able to analyze the related image regions and successfully correct its own wrong answer at the beginning and finally generate a correct answer.",
                "position": 1296
            }
        ]
    },
    {
        "header": "Appendix EMore Examples",
        "images": []
    }
]