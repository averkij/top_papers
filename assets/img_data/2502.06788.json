[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06788/x1.png",
                "caption": "Figure 1:Overview of (1) diverse vision construction inside existing VLMs and (2) potential architecture variants of Encoder-Free VLMs.",
                "position": 414
            },
            {
                "img": "https://arxiv.org/html/2502.06788/x2.png",
                "caption": "Figure 2:Preliminary analyses across various VLMs’ scaling efficiency during pre-training or fine-tuning (More details inAppendixA). Notably, VE / DT / EVE apply varying image downsampling rates (142/ 82/ 322). For fairness, we choose slightly different resolutions that yield relatively balanced token counts of 576 / 1024 / 625 tokens per image. Besides, we quantify weight changes between LLMs and VLMs by averaging absolute value variation within specific layer number or type. We report accuracy on GQA[30], SEED[26], TextVQA[60], and SQA[53]to examine VLMs’ capabilities across general in-domain, open-domain, OCR-related, and text-related knowledge tasks.",
                "position": 435
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06788/x3.png",
                "caption": "Figure 3:Overview of our proposed EVEv2.0 framework. We first adopt a patch embedding layer to encode images losslessly, and then concatenate visual and textual tokens into a unified decoder-only vision-language model. Here, it extends the standard autoregressive transformer by incorporating modality-specific weights for each multi-head self-attention layer, feed-forward layer, and layer normalization.",
                "position": 479
            },
            {
                "img": "https://arxiv.org/html/2502.06788/x4.png",
                "caption": "Figure 4:Overview of training procedure. PEL/WEL denotes patch/word embedding layer. We begin by training the patch embedding layer to establish initial alignment across modalities. Afterward, we only update vision layers within the LLM to enhance visual perception progressively. Notably, we gradually increase the image resolutions from 800×\\times×800 to 1600×\\times×1600 and keep the original image aspect ratio. Finally, we train the entire model via QA and instruction data to strengthen cross-modality correspondence and complex understanding.",
                "position": 513
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06788/x5.png",
                "caption": "Figure 5:Training loss curve and evaluation results in Stage 2. We adopt various EVE variants based on Qwen-2.5[70]as the baseline. We first train the patch embedding layer usingEVE-recap-10Min Stage 1, and further unfreeze vision layers except LLM layers in Stage 2.",
                "position": 981
            },
            {
                "img": "https://arxiv.org/html/2502.06788/x6.png",
                "caption": "",
                "position": 990
            },
            {
                "img": "https://arxiv.org/html/2502.06788/x7.png",
                "caption": "Figure 6:Evaluation results of different data sources and caption engines. We utilize EVEv1.0 based on Vicuna-7B[16]as the baseline.\nHere ”*-raw“, ”*-cap“, or ”*-recap“ denote noisy web image captions, the samples annotated by both LLaVA-1.5 (13B) and Emu2 (17B), or modified DenseFusion++ (7B), respectively. Note that ”L.O.S.“ represents the mixture of LAION[59], OpenImages[35], and SAM[34].",
                "position": 996
            },
            {
                "img": "https://arxiv.org/html/2502.06788/x8.png",
                "caption": "Figure 7:Evaluation results of mixed data ratio. We adopt EVEv1.0 with Vicuna-7B[16]for validation. Note that x:y:z denote the proportion of synthesized data : language-only data : web data.",
                "position": 1023
            },
            {
                "img": "https://arxiv.org/html/2502.06788/x9.png",
                "caption": "Figure 8:Evaluation results of image settings.\nWe use EVEv1.0 with Vicuna-7B[16].\n“AnyRatio_maxL”: longest image edge as 800,\n“AnyRatio_LD”: fixed image area as 8002,\n“AnyRatio_HD”: fixed image area as 16002,\n“AnyResolution”: arbitrary resolution.",
                "position": 1026
            },
            {
                "img": "https://arxiv.org/html/2502.06788/x10.png",
                "caption": "Figure 9:Evaluation results across progressive training procedure inFigure9. We adopt standard EVEv2.0 based on Qwen-2.5[70].",
                "position": 1048
            }
        ]
    },
    {
        "header": "5Limitation and Discussion",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperiment Details",
        "images": []
    },
    {
        "header": "Appendix BDataset Details",
        "images": []
    },
    {
        "header": "Appendix CHyper-parameter Configurations",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06788/extracted/6191103/figures/examples/MAR.jpg",
                "caption": "Table 6:EVEv2.0 exhibits excellent OCR recognition capabilities in understanding Webpage.",
                "position": 2714
            },
            {
                "img": "https://arxiv.org/html/2502.06788/extracted/6191103/figures/examples/eve_motivation1.png",
                "caption": "Table 7:EVEv2.0 exhibits excellent OCR recognition capabilities in understanding PowerPoint.",
                "position": 2756
            },
            {
                "img": "https://arxiv.org/html/2502.06788/extracted/6191103/figures/examples/ocr_beijing.jpg",
                "caption": "Table 8:EVEv2.0 exhibits excellent visual recognition capabilities in understanding Poster.",
                "position": 2790
            },
            {
                "img": "https://arxiv.org/html/2502.06788/extracted/6191103/figures/examples/background.jpg",
                "caption": "Table 9:EVEv2.0 exhibits excellent recognition capabilities in the real-world scene.",
                "position": 2831
            },
            {
                "img": "https://arxiv.org/html/2502.06788/extracted/6191103/figures/examples/mac.jpg",
                "caption": "Table 10:EVEv2.0 exhibits excellent recognition capabilities in the real-world scene.",
                "position": 2871
            },
            {
                "img": "https://arxiv.org/html/2502.06788/extracted/6191103/figures/examples/som.png",
                "caption": "Table 11:EVEv2.0 exhibits excellent set-of-mark prompting capabilities, ie, referring to marks when answering questions.",
                "position": 2913
            }
        ]
    },
    {
        "header": "Appendix DVisual Understanding Demonstration",
        "images": []
    }
]