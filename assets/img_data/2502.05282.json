[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.05282/x1.png",
                "caption": "Figure 1:The DCRL with the large-scale FP&N problem. A) The DCRL pulls and pushes the positive and negative feature pairs for consistent or distinct representation. However, B) medical images‚Äô properties cause unreliable correspondence discovery, resulting in the open problem of large-scale FP&N features pairs in DCRL and extremely limiting the representation learning ability.",
                "position": 141
            },
            {
                "img": "https://arxiv.org/html/2502.05282/x2.png",
                "caption": "Figure 2:The homeomorphism prior enables the pixel-wise correspondence discovery under the condition of medical images‚Äô inherent topology, promoting its reliability. A) In topologie, the homeomorphic objects are able to align their topologies via a homeomorphism mapping for point-to-point correspondence with topological preservation. B) Due to the consistency of human body, the medical images are homeomorphic in image space. This provides prior knowledge to construct a deformable mapping for the pixels‚Äô correspondence under the condition of their inherent topology, which will effectively reduce the searching space of pairing. C) This gives a potential to enable a reliable pixel-wise correspondence discovery in the medical image DCRL.",
                "position": 147
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.05282/x3.png",
                "caption": "Figure 3:Our GEMINI embedded the homeomorphism prior in medical images achieves a reliable correspondence discovery in DCRL. It has two aspects: a) The DHL (Sec.3.2) learns a deformable mapping for soft learning of negative pairs. b) The GSS (Sec.3.3) fuses semantic similarity into the measurement of correspondence degree to construct the positive pairs reliably. The ‚Äúopt‚Äù is the optimization. More details are described in Sec.D of ourSupplementary Materials.",
                "position": 231
            },
            {
                "img": "https://arxiv.org/html/2502.05282/x4.png",
                "caption": "Figure 4:The gradients from the loss of our GSS simultaneously train the explicit contrast of positive pairs and drive the implicit and soft learning in our DHL.",
                "position": 378
            },
            {
                "img": "https://arxiv.org/html/2502.05282/x5.png",
                "caption": "Figure 5:Intuitions on behavior. a) The two optimization objectives in Equ.8forŒ∏ùúÉ\\thetaitalic_Œ∏drive the reliable learning of positive and implicit learning of negative pairs. b) The feature pairs are learned softly via the gradient from the DHL.",
                "position": 423
            }
        ]
    },
    {
        "header": "4Experiment 1: Few-shot Semi-supervised Medical Image Segmentation (FS-Semi)",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.05282/x6.png",
                "caption": "Figure 6:Our GEMINI-Semi has significant visual superiority on three FS-Semi medical image segmentation tasks.",
                "position": 1042
            }
        ]
    },
    {
        "header": "5Experiment 2: Self-supervised Medical Image Pre-training (SS-MIP)",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.05282/x7.png",
                "caption": "Figure 7:Our GEMINI-MIP also has very significant visual superiority in the three downstream tasks.",
                "position": 1490
            }
        ]
    },
    {
        "header": "6Discussion and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.05282/x8.png",
                "caption": "Figure 8:The ablation studies on the ‚ÄúT1: 3D Cardiac structures‚Äù and ‚ÄúT3: 2D Chest structures‚Äù demonstrate the great contributions of the components in our framework. The ‚ÄúC‚Äù and ‚ÄúB‚Äù are the learning for continuity and bijection.",
                "position": 1523
            },
            {
                "img": "https://arxiv.org/html/2502.05282/x9.png",
                "caption": "Figure 9:The ablation of the hyper-parameters on the ‚ÄúT1: 3D Cardiac structures‚Äù show the effects from the weight of the smoothness lossŒªs‚Å¢m‚Å¢osubscriptùúÜùë†ùëöùëú\\lambda_{smo}italic_Œª start_POSTSUBSCRIPT italic_s italic_m italic_o end_POSTSUBSCRIPT, the positive pairsŒªp‚Å¢o‚Å¢ssubscriptùúÜùëùùëúùë†\\lambda_{pos}italic_Œª start_POSTSUBSCRIPT italic_p italic_o italic_s end_POSTSUBSCRIPT, and the GVS lossŒªG‚Å¢V‚Å¢SsubscriptùúÜùê∫ùëâùëÜ\\lambda_{GVS}italic_Œª start_POSTSUBSCRIPT italic_G italic_V italic_S end_POSTSUBSCRIPT. The|Jœï|‚â§0subscriptùêΩitalic-œï0|J_{\\phi}|\\leq 0| italic_J start_POSTSUBSCRIPT italic_œï end_POSTSUBSCRIPT | ‚â§ 0(%) is Jacobian matrix[41]which evaluates smoothness of the deformation.",
                "position": 1533
            },
            {
                "img": "https://arxiv.org/html/2502.05282/x10.png",
                "caption": "Figure 10:The visualization of the deformation in the learning process on the ‚ÄúT1: 3D Cardiac structures‚Äù. The first row is the line chart of the segmentation and deformation performance. The second row is the grids which demonstrates the deformation degree. The third row is the deformed image A during the learning process.",
                "position": 1543
            },
            {
                "img": "https://arxiv.org/html/2502.05282/x11.png",
                "caption": "Figure 11:The pre-training and fine-tuning data amount analysis on SS-MIP.",
                "position": 1581
            },
            {
                "img": "https://arxiv.org/html/2502.05282/x12.png",
                "caption": "Figure 12:Our GEMINI-MIP has powerful learning efficiency both in the inner-scene and inter-scene transferring tasks.",
                "position": 1601
            },
            {
                "img": "https://arxiv.org/html/2502.05282/x13.png",
                "caption": "Figure 13:The necessity of the fundament in our GEMINI-MIP. When learning without the fundamental learning task, the GVS loss converges slowly due to the initial weak representation limiting the GSS and GVS for correspondence. When adding the fundament (self-restoration), warmup from the basic representation of semantic regions drives correspondence learning efficiently.",
                "position": 1611
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "A SS-MIP on more datasets",
        "images": []
    },
    {
        "header": "B Discussion of the research problem and method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.05282/x14.png",
                "caption": "Figure 14:The evaluation of the large-scale FP problem. The true positive (TP) pairs constructed by the features‚Äô similarity (used in DenseCL) only occupy the 5.79% of the foreground region, and our GEMINI is able to bring 60.74% TP pairs.",
                "position": 2165
            },
            {
                "img": "https://arxiv.org/html/2502.05282/x15.png",
                "caption": "Figure 15:The FP and FN pairs have a serious impact on learning. a) The fitting process with FP and FN pairs on a cardiac CT image. b) The models‚Äô learned segmentation ability on the fitted case and their generalization ability on another testing case.",
                "position": 2177
            }
        ]
    },
    {
        "header": "C More framework analysis and experiment discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.05282/x16.png",
                "caption": "Figure 16:The ablation study of the receptive field sizerùëüritalic_rand the network parameter amounts. a) The segmentation performance on the T1 of FS-Semi setting with the increasing of the receptive field sizerùëüritalic_r. b) The fine-tuning performance with the enlarging of the parameter amount (million,MùëÄMitalic_M) in the pre-trained networks.",
                "position": 2204
            },
            {
                "img": "https://arxiv.org/html/2502.05282/x17.png",
                "caption": "Figure 17:The t-SNE visualization of the learned pixel representations. We provide the coordinates of pixels in a zoomed view, indicating their spatial relationship.",
                "position": 2219
            }
        ]
    },
    {
        "header": "D More details in experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.05282/x18.png",
                "caption": "Figure 18:The overall training diagram of our GEMINI. a) The inference process of the whole framework. The gray path in the last line is the additional learning part in the variants of our GEMINI in self-supervised pre-training (GEMINI-MIP) and semi-supervised segmentation (GEMINI-Semi). b) The loss calculation to optimize the whole framework.",
                "position": 2617
            },
            {
                "img": "https://arxiv.org/html/2502.05282/x19.png",
                "caption": "Figure 19:The detailed architecture of our GEMINI. a) The backbone architecture utilizes the 3D U-Net in 3D image tasks and 2D U-Net in 2D image tasks. b) The deformer network architecture utilized a lightweight U-Net. c-d) The segmentation head in the variant of GEMINI-Semi and the self-restoration head in the variant of GEMINI-MIP.",
                "position": 2620
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]