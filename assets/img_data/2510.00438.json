[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.00438/x1.png",
                "caption": "Figure 1:Examples of subject-to-video generation results of our proposed BindWeave, demonstrating its ability to produce high-fidelity, subject-consistent videos across a broad spectrum of scenarios from single-subject inputs to complex multi-subject compositions.",
                "position": 81
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.00438/x2.png",
                "caption": "Figure 2:Framework of our method. A multimodal large language model performs cross‑modal reasoning to ground entities and disentangle roles, attributes, and interactions from the prompt and optional reference images. The resulting subject‑aware signals condition a Diffusion Transformer through cross‑attention and lightweight adapters, guiding identity‑faithful, relation‑consistent, and temporally coherent video generation.",
                "position": 179
            },
            {
                "img": "https://arxiv.org/html/2510.00438/x3.png",
                "caption": "Figure 3:Illustration of our adaptive multi-reference conditioning strategy.",
                "position": 252
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.00438/x4.png",
                "caption": "Figure 4:Qualitative comparison on subject-to-video task, with four uniformly sampled frames shown in each case. Compared to other competing methods, our approach is superior in subject fidelity, naturalness, and semantic consistency with the caption.",
                "position": 348
            },
            {
                "img": "https://arxiv.org/html/2510.00438/x5.png",
                "caption": "Figure 5:Qualitative comparison on subject-to-video task, with four uniformly sampled frames shown in each case. Compared with other methods, our approach better avoids implausible phenomena and produces more natural videos while maintaining strong subject consistency.",
                "position": 351
            },
            {
                "img": "https://arxiv.org/html/2510.00438/x6.png",
                "caption": "Figure 6:Qualitative comparison of MLLM+T5 vs. T5-only. MLLM+T5 shows superior scale grounding, reliable action–object execution, and stronger temporal/textual coherence.",
                "position": 577
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.00438/x7.png",
                "caption": "Figure 7:Comparisons under a prompt–reference ambiguity (prompt: “a man”; reference: baby). Most baselines follow the prompt and generate an adult male, ignoring the features of the reference image or retaining only partial infant traits, whereas our method faithfully preserves the reference subject’s appearance.",
                "position": 1241
            },
            {
                "img": "https://arxiv.org/html/2510.00438/x8.png",
                "caption": "Figure 8:Comparisons in the subject-to-video setting illustrating the copy–paste issue under simple prompts. Many baselines directly copy the reference image into the video, causing the subject to remain static across frames, whereas our method preserves the subject’s temporal dynamics and natural motion.",
                "position": 1244
            },
            {
                "img": "https://arxiv.org/html/2510.00438/x9.png",
                "caption": "Figure 9:More generated results of BindWeave, demonstrating high fidelity and strong subject consistency.",
                "position": 1250
            },
            {
                "img": "https://arxiv.org/html/2510.00438/x10.png",
                "caption": "Figure 10:More generated results of BindWeave, demonstrating high fidelity and strong subject consistency across multi-references.",
                "position": 1256
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]