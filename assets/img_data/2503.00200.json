[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.00200/x1.png",
                "caption": "",
                "position": 78
            }
        ]
    },
    {
        "header": "IIntroduction",
        "images": []
    },
    {
        "header": "IIRelated Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.00200/x2.png",
                "caption": "Figure 2:Network Architecture.Given historical observations{Otâˆ’h+1,â€¦,Ot}subscriptğ‘‚ğ‘¡â„1â€¦subscriptğ‘‚ğ‘¡\\{O_{t-h+1},\\dots,O_{t}\\}{ italic_O start_POSTSUBSCRIPT italic_t - italic_h + 1 end_POSTSUBSCRIPT , â€¦ , italic_O start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT }and corresponding action chunks{Atâˆ’h,â€¦,Atâˆ’1}subscriptğ´ğ‘¡â„â€¦subscriptğ´ğ‘¡1\\{A_{t-h},\\dots,A_{t-1}\\}{ italic_A start_POSTSUBSCRIPT italic_t - italic_h end_POSTSUBSCRIPT , â€¦ , italic_A start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT }, the model predicts future observations{Ot+1,â€¦,Ot+h}subscriptğ‘‚ğ‘¡1â€¦subscriptğ‘‚ğ‘¡â„\\{O_{t+1},\\dots,O_{t+h}\\}{ italic_O start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT , â€¦ , italic_O start_POSTSUBSCRIPT italic_t + italic_h end_POSTSUBSCRIPT }and actions{At,â€¦,At+hâˆ’1}subscriptğ´ğ‘¡â€¦subscriptğ´ğ‘¡â„1\\{A_{t},\\dots,A_{t+h-1}\\}{ italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , â€¦ , italic_A start_POSTSUBSCRIPT italic_t + italic_h - 1 end_POSTSUBSCRIPT }. Each image observation is represented as a sequence ofNğ‘Nitalic_Ntokens, and each image corresponds to an action chunk withLğ¿Litalic_Lactions sampled at a higher frequency. During training, future observations are used with randomly masked tokens, while at inference time, the model starts from an empty image to predict the complete image. History observations, actions, and masked future observations are combined, passed through a Transformer, and decoded into actions and videos using diffusion heads.",
                "position": 163
            }
        ]
    },
    {
        "header": "IIIUnified Video Action Model",
        "images": []
    },
    {
        "header": "IVEvaluation",
        "images": []
    },
    {
        "header": "VUVAÂ as Policy",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.00200/x3.png",
                "caption": "TABLE I:Simulation Environments.We evaluate UVAÂ and baselines in both single-task and multi-task settings. In the multi-task scenario, the goal can be defined through the image observations (PushT-M) or language descriptions (Libero10).",
                "position": 347
            },
            {
                "img": "https://arxiv.org/html/2503.00200/x4.png",
                "caption": "Figure 3:Real-World Out-of-Distribution Evaluation.We use the training data provided by prior works[10,43]. The test scenario is significantly out-of-distribution with unseen environments, objects, robots, and even gripper colors. The numbers in the parentheses show the percentage of such category of test cases in evaluation. Please refer toour websitefor all evaluation rollouts.",
                "position": 431
            },
            {
                "img": "https://arxiv.org/html/2503.00200/x5.png",
                "caption": "Figure 4:Visual Disturbances on PushT.Tasks are performed under altered visual conditions, including changes in background color, distracting background objects, and goal color.",
                "position": 547
            },
            {
                "img": "https://arxiv.org/html/2503.00200/x6.png",
                "caption": "Figure 6:Robustness to History Length on PushT-M.Typical policy learning frameworks such as DP-C[9]often experience performance drops with increased history length due to overfitting, while our model maintains robust performance.",
                "position": 631
            }
        ]
    },
    {
        "header": "VIUVAÂ as a Video Generator",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.00200/x7.png",
                "caption": "Figure 7:Forward Dynamics Model on Block Pushing Task.During training, the robot pushes two blocks randomly to any target. During testing, the generated future image from UVAÂ is used to select the proper action that moves a specific object to a specific target.",
                "position": 688
            },
            {
                "img": "https://arxiv.org/html/2503.00200/x8.png",
                "caption": "Figure 9:Video Generation Results on Validation Set.UVA generates high-quality videos that closely match the ground truth, with 8 autoregressive steps further enhancing detail compared to a single autoregressive step. In contrast, UniPi occasionally produces blurry (Cup arrangement) or mismatched images (Towel fold and Libero10 left) and may fail to generate some objects (Libero10 right, the second moka pot). With the similar computational resources, UVA consistently outperforms UniPi.",
                "position": 742
            }
        ]
    },
    {
        "header": "VIIUVAÂ as a Forward Dynamics Model",
        "images": []
    },
    {
        "header": "VIIIUVAÂ as a Inverse Dynamic Model",
        "images": []
    },
    {
        "header": "IXDiscussion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.00200/x9.png",
                "caption": "Figure 10:Autoregressive Video Generation.Our method generates a video starting from an empty mask. Given a specified number of generation steps, the model produces a set of tokens at each autoregressive step, progressively constructing the video. If the generation step is set to 1, the entire video is generated in a single pass.",
                "position": 1516
            }
        ]
    },
    {
        "header": "XSupplementary Materials",
        "images": []
    }
]