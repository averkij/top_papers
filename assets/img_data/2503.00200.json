[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.00200/x1.png",
                "caption": "",
                "position": 78
            }
        ]
    },
    {
        "header": "IIntroduction",
        "images": []
    },
    {
        "header": "IIRelated Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.00200/x2.png",
                "caption": "Figure 2:Network Architecture.Given historical observations{Ot−h+1,…,Ot}subscript𝑂𝑡ℎ1…subscript𝑂𝑡\\{O_{t-h+1},\\dots,O_{t}\\}{ italic_O start_POSTSUBSCRIPT italic_t - italic_h + 1 end_POSTSUBSCRIPT , … , italic_O start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT }and corresponding action chunks{At−h,…,At−1}subscript𝐴𝑡ℎ…subscript𝐴𝑡1\\{A_{t-h},\\dots,A_{t-1}\\}{ italic_A start_POSTSUBSCRIPT italic_t - italic_h end_POSTSUBSCRIPT , … , italic_A start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT }, the model predicts future observations{Ot+1,…,Ot+h}subscript𝑂𝑡1…subscript𝑂𝑡ℎ\\{O_{t+1},\\dots,O_{t+h}\\}{ italic_O start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT , … , italic_O start_POSTSUBSCRIPT italic_t + italic_h end_POSTSUBSCRIPT }and actions{At,…,At+h−1}subscript𝐴𝑡…subscript𝐴𝑡ℎ1\\{A_{t},\\dots,A_{t+h-1}\\}{ italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , … , italic_A start_POSTSUBSCRIPT italic_t + italic_h - 1 end_POSTSUBSCRIPT }. Each image observation is represented as a sequence ofN𝑁Nitalic_Ntokens, and each image corresponds to an action chunk withL𝐿Litalic_Lactions sampled at a higher frequency. During training, future observations are used with randomly masked tokens, while at inference time, the model starts from an empty image to predict the complete image. History observations, actions, and masked future observations are combined, passed through a Transformer, and decoded into actions and videos using diffusion heads.",
                "position": 163
            }
        ]
    },
    {
        "header": "IIIUnified Video Action Model",
        "images": []
    },
    {
        "header": "IVEvaluation",
        "images": []
    },
    {
        "header": "VUVA as Policy",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.00200/x3.png",
                "caption": "TABLE I:Simulation Environments.We evaluate UVA and baselines in both single-task and multi-task settings. In the multi-task scenario, the goal can be defined through the image observations (PushT-M) or language descriptions (Libero10).",
                "position": 347
            },
            {
                "img": "https://arxiv.org/html/2503.00200/x4.png",
                "caption": "Figure 3:Real-World Out-of-Distribution Evaluation.We use the training data provided by prior works[10,43]. The test scenario is significantly out-of-distribution with unseen environments, objects, robots, and even gripper colors. The numbers in the parentheses show the percentage of such category of test cases in evaluation. Please refer toour websitefor all evaluation rollouts.",
                "position": 431
            },
            {
                "img": "https://arxiv.org/html/2503.00200/x5.png",
                "caption": "Figure 4:Visual Disturbances on PushT.Tasks are performed under altered visual conditions, including changes in background color, distracting background objects, and goal color.",
                "position": 547
            },
            {
                "img": "https://arxiv.org/html/2503.00200/x6.png",
                "caption": "Figure 6:Robustness to History Length on PushT-M.Typical policy learning frameworks such as DP-C[9]often experience performance drops with increased history length due to overfitting, while our model maintains robust performance.",
                "position": 631
            }
        ]
    },
    {
        "header": "VIUVA as a Video Generator",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.00200/x7.png",
                "caption": "Figure 7:Forward Dynamics Model on Block Pushing Task.During training, the robot pushes two blocks randomly to any target. During testing, the generated future image from UVA is used to select the proper action that moves a specific object to a specific target.",
                "position": 688
            },
            {
                "img": "https://arxiv.org/html/2503.00200/x8.png",
                "caption": "Figure 9:Video Generation Results on Validation Set.UVA generates high-quality videos that closely match the ground truth, with 8 autoregressive steps further enhancing detail compared to a single autoregressive step. In contrast, UniPi occasionally produces blurry (Cup arrangement) or mismatched images (Towel fold and Libero10 left) and may fail to generate some objects (Libero10 right, the second moka pot). With the similar computational resources, UVA consistently outperforms UniPi.",
                "position": 742
            }
        ]
    },
    {
        "header": "VIIUVA as a Forward Dynamics Model",
        "images": []
    },
    {
        "header": "VIIIUVA as a Inverse Dynamic Model",
        "images": []
    },
    {
        "header": "IXDiscussion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.00200/x9.png",
                "caption": "Figure 10:Autoregressive Video Generation.Our method generates a video starting from an empty mask. Given a specified number of generation steps, the model produces a set of tokens at each autoregressive step, progressively constructing the video. If the generation step is set to 1, the entire video is generated in a single pass.",
                "position": 1516
            }
        ]
    },
    {
        "header": "XSupplementary Materials",
        "images": []
    }
]