[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03558/x1.png",
                "caption": "",
                "position": 80
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03558/x2.png",
                "caption": "Figure 2:Comparison between our scene generation pipeline with multi-instance diffusion and existing compositional generation methods.",
                "position": 117
            },
            {
                "img": "https://arxiv.org/html/2412.03558/x3.png",
                "caption": "Figure 3:Method overview. Based on 3D object generation models, MIDI denoises the latent representations of multiple 3D instances simultaneously using a weight-shared DiT module. The multi-instance attention layers are introduced to learn cross-instance interaction and enable global awareness, while cross-attention layers integrate the information of object images and global scene context.",
                "position": 146
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary: 3D Object Generation Models",
        "images": []
    },
    {
        "header": "4MIDI: Multi-Instance 3D Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03558/x4.png",
                "caption": "Figure 4:Multi-instance attention. We extend the original object self-attention, where tokens of each object query only themselves, to multi-instance attention, where tokens of each instance query all tokens from all instances in the scene.",
                "position": 246
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03558/x5.png",
                "caption": "Figure 5:Qualitative comparisons on synthetic datasets, including 3D-Front[15]and BlendSwap[1].",
                "position": 491
            },
            {
                "img": "https://arxiv.org/html/2412.03558/x6.png",
                "caption": "Figure 6:Qualitative comparisons on real-world data, including Matterport3D[3]and ScanNet[8].",
                "position": 494
            },
            {
                "img": "https://arxiv.org/html/2412.03558/x7.png",
                "caption": "Figure 7:Qualitative ablation studies on the number of multi-instance attention layersKùêæKitalic_K, and the use of global scene image conditioning, and mixed training with single-object dataset.",
                "position": 558
            },
            {
                "img": "https://arxiv.org/html/2412.03558/x8.png",
                "caption": "Figure 8:Qualitative comparisons on stylized images that are generated by text-to-image diffusion models.",
                "position": 570
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03558/x9.png",
                "caption": "Figure 9:Detailed comparison between existing compositional generation methods and our multi-instance diffusion.",
                "position": 1702
            }
        ]
    },
    {
        "header": "7Background",
        "images": []
    },
    {
        "header": "8Implementation Details",
        "images": []
    },
    {
        "header": "9Additional Discussions",
        "images": []
    },
    {
        "header": "10Limitations",
        "images": []
    }
]