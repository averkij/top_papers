[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23605/x1.png",
                "caption": "Figure 1:State-of-the-art 4D generation model L4GM[73]and our solution.(a)L4GM performs video-to-4D generation. The side and back views of the generated 4D asset does not look alike the subject in the given source view.(b)Our proposed solution TIRE(Track,Inpaint,REsplat)adopts the progressive texture infilling paradigm to inpaint the 3D asset to achieve subject-driven 3D/4D generation, which preserves the identity of the generated assets when observing from the novel views.",
                "position": 111
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23605/x2.png",
                "caption": "Figure 2:Pipeline of TIRE. TIRE starts from a rough 3D asset created by existing models and its rendered multi-view observations. Afterwards, the three stagesTrack, Inpaint, Resplattarget at identifying the inpainting masks, infilling the occluded regions, and unprojecting back to 3D, respectively.",
                "position": 175
            },
            {
                "img": "https://arxiv.org/html/2510.23605/x3.png",
                "caption": "Figure 3:Comparison between forward tracking and backward tracking when identifying the inpainting mask. Forward tracking, which means that the tracking process starts from the given source view to the target views, though being more intuitive, leads to grainy inpainting results. In contrast, backward tracking produces more accurate masks in better shapes, which benefits the following inpainting process.",
                "position": 258
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23605/x4.png",
                "caption": "Figure 4:Qualitative comparison on image-to-3D generation with SV3D[90], Wonder3D[54], LGM[86], MeshFormer[50], TRELLIS[104], and Hunyuan3D-v2.5[38]. Compared against other method in the image-to-3D setting, our method better preserves the identity of the reference image, and also reaches superior quality on geometry. It is noticeable that even for the most recent advancements in image-to-3D like TRELLIS and Hunyuan3D-v2.5, the challenge of producing identity-preserving 3D assets is still not well solved.",
                "position": 315
            },
            {
                "img": "https://arxiv.org/html/2510.23605/x5.png",
                "caption": "Figure 5:Comparison between our method and the baselines Customize-It-3D[24](additional feed-forward operation from L4GM is applied after obtaining multi-view observations to allow it to generate dynamic 3D assets), STAG4D[123], and L4GM[73].",
                "position": 322
            },
            {
                "img": "https://arxiv.org/html/2510.23605/x6.png",
                "caption": "Figure 6:Comparison between our method and L4GM[73]. Although the main objective of our TIRE is to preserve the identity of the generated assets, the geometry of the generated assets also gets improved because the three stages in our pipeline collectively promote cross-view consistency.",
                "position": 325
            },
            {
                "img": "https://arxiv.org/html/2510.23605/x7.png",
                "caption": "Figure 7:Results of our user study. Our method scores the highest in overall qualitywithoutexplicitly informing the users that we are focusing on subject-driven generation.",
                "position": 467
            },
            {
                "img": "https://arxiv.org/html/2510.23605/x8.png",
                "caption": "Figure 8:Ablation study on the progressive learning strategy in TIRE. Without adopting progressive learning, the model tends to consistently infill the appearance of the given source viewregardless of the current pose, which results in wrongly infilling the textures on the side and back views.",
                "position": 482
            },
            {
                "img": "https://arxiv.org/html/2510.23605/x8.png",
                "caption": "Figure 8:Ablation study on the progressive learning strategy in TIRE. Without adopting progressive learning, the model tends to consistently infill the appearance of the given source viewregardless of the current pose, which results in wrongly infilling the textures on the side and back views.",
                "position": 485
            },
            {
                "img": "https://arxiv.org/html/2510.23605/x9.png",
                "caption": "Figure 9:Ablation study on different denoising schedules for inpainting. A smaller schedule may leave some regions unchanged, while a larger one may overly distort the textures.",
                "position": 490
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "NeurIPS Paper Checklist",
        "images": []
    },
    {
        "header": "Appendix AUser Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23605/x10.png",
                "caption": "Figure A:Screenshot of the instructions of our user study. From the instructions, wedo notexplicitly tell the participants that our project is targeting at subject-driven 4D generation (e.g.,in the title we only mention \"User Study on 4D Generation\"). Instead, the users are asked to rate the generated results based on the overall quality, which enhances the fairness and reduces the underlying bias of the potential subjective preference towards our method.",
                "position": 3310
            },
            {
                "img": "https://arxiv.org/html/2510.23605/x11.png",
                "caption": "Figure B:Screenshot of an example question, containing videos generated by three methods placed side by side in random order, the reference front view, and the questions to score the three generated results.",
                "position": 3313
            }
        ]
    },
    {
        "header": "Appendix BMore Details on Datasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23605/x12.png",
                "caption": "Figure C:Pipeline of constructing data for ourDreamBooth-Dynamicdataset. First, a DreamBooth-finetuned[75]customized text-to-image model is trained on several casually collected images of a subject. Afterwards, we use manually created text prompts to generate images with the specific subjects. Afterwards, a powerful image-to-video model[117]is applied to animate the static images into videos. Finally, we use SAM 2[71]to segment the foreground subjects to form the source view videos used in our evaluation process.",
                "position": 3323
            }
        ]
    },
    {
        "header": "Appendix CDiscussions and Limitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23605/x13.png",
                "caption": "Figure D:Side and back viewpoints generated by specific models. It is obvious to see each model suffers from a bias pattern for generating the appearance of the occluded viewpoints.",
                "position": 3330
            },
            {
                "img": "https://arxiv.org/html/2510.23605/x14.png",
                "caption": "Figure E:An example of DINO (ViT-S/16) similarity scores of a rendered viewpoint which is135∘135^{\\circ}away from the source view across different methods. The baseline methods Customize-It-3D and STAG4D are obviously incorrect regarding geometry but get higher scores for DINO similarity.",
                "position": 3345
            }
        ]
    },
    {
        "header": "Appendix DAdditional Implementation Details",
        "images": []
    },
    {
        "header": "Appendix EMore Ablation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23605/x15.png",
                "caption": "Figure F:Ablation study on the degree of progressiveness during theInpaintstage. Our current choice of degree of progressiveness (20∘20^{\\circ}) strikes a balance between inpainting quality and efficiency. Smaller degree of progressiveness (10∘10^{\\circ}) yields decent results but slows down the whole inpainting process, while larger degree of progressiveness (30∘30^{\\circ}) brings noticeable degradation in the inpainting quality.",
                "position": 3485
            },
            {
                "img": "https://arxiv.org/html/2510.23605/x16.png",
                "caption": "Figure G:Ablation study on the necessity of having the inpainting process. The color and texture of the assets without our inpainting process would be far from having satisfactory quality and decent identity preservation.",
                "position": 3488
            }
        ]
    },
    {
        "header": "Appendix FMore Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23605/x17.png",
                "caption": "Figure H:Qualitative comparisons on the in-the-wild videos which are displayed on the official website of L4GM[73]. The visualizations demonstrate that our results more faithfully reflect the identity of the given subjects, yielding better overall quality.",
                "position": 3640
            },
            {
                "img": "https://arxiv.org/html/2510.23605/x18.png",
                "caption": "Figure I:Qualitative results showing the improvement of our method when applying on the most advanced image-to-3D method Hunyuan3D-v2.5[38]. We can observe that our method produces superior texture of the generated assets which better match the identity of the reference image.",
                "position": 3643
            },
            {
                "img": "https://arxiv.org/html/2510.23605/x19.png",
                "caption": "Figure J:Qualitative comparison with more advanced methods including Hunyuan3D-v1.0[114], SPAR3D[27], Sudo AI (built upon One-2-3-45++[49]), and Neural4D (built upon Direct3D[102]). Even the recent advancements in 3D generation cannot satisfactorily handle the personalized 3D generation challenge.",
                "position": 3646
            },
            {
                "img": "https://arxiv.org/html/2510.23605/x20.png",
                "caption": "Figure K:Qualitative comparison with SV3D[90], Wonder3D[54], and LGM[86]. Compared against other method in the image-to-3D setting, our method achieves better preserves the identity of the reference image, and also reaches superior quality on geometry.",
                "position": 3649
            },
            {
                "img": "https://arxiv.org/html/2510.23605/x21.png",
                "caption": "Figure L:Additional qualitative results of the comparison between our method and the baselines Customize-It-3D[24], STAG4D[123], and L4GM[73]. Compared with other methods, our solution achieves superior subject fidelity along with improved geometry on the generated assets, due to our design in our three-stage framework that fosters cross-view consistency.",
                "position": 3652
            },
            {
                "img": "https://arxiv.org/html/2510.23605/x22.png",
                "caption": "Figure M:Qualitative comparison with SV4D[108]. Since only a certain number of selected views can be rendered with the official code of SV4D, we choose the close viewpoints but not the identical ones for comparison for the sake of simplicity. Nevertheless, it is still obvious that SV4D has inferior performance compared with our method in the perspectives of both appearance and geometry.",
                "position": 3655
            }
        ]
    },
    {
        "header": "Appendix GSocietal Impact",
        "images": []
    }
]