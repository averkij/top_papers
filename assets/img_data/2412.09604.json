[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09604/x1.png",
                "caption": "Figure 1:Comparison among exemplary unified MLLMs for synergizing image understanding and generation tasks.Compared with methods (a)∼similar-to\\sim∼(d) that incorporate complicated designs of model architectures, training methods, and the use of external pretrained diffusion models, (e) encoder-free unified MLLMs adopt a simple design that uses the simple next token prediction framework for both images understanding and generation tasks, allowing for broader data distribution and better scalability.",
                "position": 100
            },
            {
                "img": "https://arxiv.org/html/2412.09604/x2.png",
                "caption": "Figure 2:Comparision between SynerGen-VL and previous encoder-free unified MLLMs.SynerGen-VL adopts a token folding and unfolding mechanism and vision experts to build a strong and simple unified MLLM. With the same image context length, SynerGen-VL can support images of much higher resolutions, ensuring the performance of both high-resolution image understanding and generation.",
                "position": 120
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09604/x3.png",
                "caption": "Figure 3:Overview of the proposed SynerGen-VL.The image and text are represented as discrete tokens, and modeled with a single LLM and unified next-token prediction paradigm. Text and vision expert FFNs are introduced to incorporate visual capabilities into the pretrained LLM. To support processing high-resolution images, the input image token sequence is folded to reduce its length, and unfolded by a shallow autoregressive Transformer head to generate images.",
                "position": 154
            }
        ]
    },
    {
        "header": "3SynerGen-VL",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Ablation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09604/x4.png",
                "caption": "Figure 4:Cosine similarity of visual features between generation and understanding tasks across different layers. The representations of the image understanding and generation tasks are similar in shallow layers but disentagle in deeper layers.",
                "position": 1582
            },
            {
                "img": "https://arxiv.org/html/2412.09604/x5.png",
                "caption": "Figure 5:Attention map visualization of understanding and generation tasks.In the second and fourth rows, we visualize a query token(red)and its attended tokens(blue)in the input image. Each token corresponds to a horizontal rectangular area in the original image due to the2×4242\\times 42 × 4token folding. Darker blue indicates larger attention weights.",
                "position": 1592
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetailed Training Configurations",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09604/x6.png",
                "caption": "Figure 6:Qualitative results of image generation.The images are of size512×512512512512\\times 512512 × 512.",
                "position": 3123
            },
            {
                "img": "https://arxiv.org/html/2412.09604/extracted/6065241/figs/vis-understand-HK.jpg",
                "caption": "",
                "position": 3126
            },
            {
                "img": "https://arxiv.org/html/2412.09604/extracted/6065241/figs/vis-understand-cvpr.jpg",
                "caption": "",
                "position": 3155
            },
            {
                "img": "https://arxiv.org/html/2412.09604/extracted/6065241/figs/vis-understanding-pokemon.jpg",
                "caption": "",
                "position": 3166
            },
            {
                "img": "https://arxiv.org/html/2412.09604/extracted/6065241/figs/vis-understanding-ocr2.jpg",
                "caption": "",
                "position": 3192
            },
            {
                "img": "https://arxiv.org/html/2412.09604/extracted/6065241/figs/vis-understanding-harrypotter.jpg",
                "caption": "",
                "position": 3223
            },
            {
                "img": "https://arxiv.org/html/2412.09604/extracted/6065241/figs/vis-understanding-meme.jpg",
                "caption": "",
                "position": 3251
            },
            {
                "img": "https://arxiv.org/html/2412.09604/extracted/6065241/figs/vis-understanding-latex.png",
                "caption": "",
                "position": 3262
            },
            {
                "img": "https://arxiv.org/html/2412.09604/extracted/6065241/figs/vis-understanding-math1.png",
                "caption": "",
                "position": 3291
            },
            {
                "img": "https://arxiv.org/html/2412.09604/extracted/6065241/figs/vis-understanding-math2.png",
                "caption": "",
                "position": 3313
            }
        ]
    },
    {
        "header": "Appendix BVisualization",
        "images": []
    }
]