[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.17358/x1.png",
                "caption": "Figure 1:Existing methods rely on establishing correspondences between multiple frames to estimate inter-frame camera motion (a). This leads to failures during fast motion with lots of blur. We propose a method that can estimate intra-frame camera motion from a single image (b), making our method robust to aggressive motions.",
                "position": 85
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.17358/x2.png",
                "caption": "Figure 2:Overview of our method. Given a single motion blurred image, we pass it through the network to obtain a dense flow field and monocular depth prediction (Section4.1). These are then formulated in a linear system, where the optimal velocity parameters are solved for using linear least squares (Section4.2). Because the linear solver is fully differentiable, we can train the entire network end-to-end, supervised on the camera motion.",
                "position": 142
            }
        ]
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.17358/x3.png",
                "caption": "Figure 3:Overview for our synthetic dataset generation process. After preprocessing the dataset, we run selected frames through an interpolation network, which we use to synthesize our blurred image. We also take the first and last virtual frames to generateùíü^^ùíü\\mathcal{\\hat{D}}over^ start_ARG caligraphic_D end_ARG, which is subsequently used for computing‚Ñ±^^‚Ñ±\\mathcal{\\hat{F}}over^ start_ARG caligraphic_F end_ARG.",
                "position": 430
            }
        ]
    },
    {
        "header": "5Dataset Curation",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.17358/x4.png",
                "caption": "Figure 4:Visualization of the predicted velocities for the billiards sequence using our method, MASt3R and COLMAP (w/ DISK+LightGlue). The shaded area under the curve shows the error between the predicted velocity and GT velocity. Our translations and rotations are significantly better than MASt3R. While COLMAP with DISK + LightGlue feature matching does well on rotations, our method significantly outperforms it on translations.",
                "position": 727
            }
        ]
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.17358/x5.png",
                "caption": "Figure 5:Error CDFs for the billiards sequence, such that the left plot shows the distribution of translational error in the sequence and the right plot the rotational error. Curves closer to the top-left corner are better.",
                "position": 777
            },
            {
                "img": "https://arxiv.org/html/2503.17358/extracted/6300094/figures/robot_arm_cropped.png",
                "caption": "Figure 6:Real-world application example of our method using a camera attached to a fast-moving RoArm-M1 robot arm platform. (a) The robot arm used for recording. (b) The predicted and GT velocity time series for the camera attached to the end-effector.",
                "position": 780
            },
            {
                "img": "https://arxiv.org/html/2503.17358/extracted/6300094/figures/robot_arm_cropped.png",
                "caption": "",
                "position": 783
            },
            {
                "img": "https://arxiv.org/html/2503.17358/x6.png",
                "caption": "",
                "position": 787
            },
            {
                "img": "https://arxiv.org/html/2503.17358/x7.png",
                "caption": "Figure 7:Comparison of our method to using IMU integration. The IMU velocity estimate is accurate for a few seconds until it drifts. Our method provides accurate and drift-free estimates throughout the sequence.",
                "position": 808
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]