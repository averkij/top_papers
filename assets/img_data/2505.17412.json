[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17412/extracted/6469542/assets/teaserv6.png",
                "caption": "",
                "position": 85
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Sparse SDF VAE",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17412/x1.png",
                "caption": "Figure 2:The framework of our Direct3D-S2. We propose a fully end-to-end sparse SDF VAE (SS-VAE), which employs a symmetric encoder-decoder network to efficiently encode high-resolution sparse SDF volumes into sparse latent representationsùê≥ùê≥\\mathbf{z}bold_z. Then we train an image-conditioned diffusion transformer (SS-DiT) based onùê≥ùê≥\\mathbf{z}bold_z, and design a novel Spatial Sparse Attention (SSA) mechanism that significantly improves the training and inference efficiency of the DiT.",
                "position": 149
            }
        ]
    },
    {
        "header": "4Spatial Sparse Attention and DiT",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17412/x2.png",
                "caption": "Figure 3:The framework of our Spatial Sparse Attention (SSA). We partition the input tokens into blocks based on their 3D coordinates, and then construct key-value pairs through three distinct modules. For each query token, we utilize sparse 3D compression module to capture global information, while the spatial blockwise selection module selects important blocks based on compression attention scores to extract fine-grained features, and the sparse 3D window module injects local features. Ultimately, we aggregate the final output of SSA from the three modules using predicted gate scores.",
                "position": 222
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17412/x3.png",
                "caption": "Figure 4:Qualitative comparisons between other image-to-3D methods and our approach.",
                "position": 489
            },
            {
                "img": "https://arxiv.org/html/2505.17412/x4.png",
                "caption": "Figure 5:User Study for Image-to-3D Generation.",
                "position": 616
            }
        ]
    },
    {
        "header": "6Comparison of VAE",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17412/x5.png",
                "caption": "Figure 6:Qualitative comparisons of VAE reconstruction results. Note that we used a latent token length of 4096 during the inference of Dora[6].",
                "position": 627
            },
            {
                "img": "https://arxiv.org/html/2505.17412/x6.png",
                "caption": "Figure 7:Comparison of the forward and backward time of our SSA and FlashAttention-2.",
                "position": 633
            },
            {
                "img": "https://arxiv.org/html/2505.17412/x7.png",
                "caption": "Figure 8:The visualization results of our Direct3D-S2 for image-to-3D generation across four resolutions: {2563superscript2563256^{3}256 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT,3843superscript3843384^{3}384 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT,5123superscript5123512^{3}512 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT,10243superscript102431024^{3}1024 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT}.",
                "position": 636
            },
            {
                "img": "https://arxiv.org/html/2505.17412/x8.png",
                "caption": "Figure 9:Ablation studies for the three modules of SSA at resolution5123superscript5123512^{3}512 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT, wherewin,cmp, andslcdenote the sparse 3D window, sparse 3D compression, and spatial blockwise selection modules, respectively.",
                "position": 639
            },
            {
                "img": "https://arxiv.org/html/2505.17412/x9.png",
                "caption": "Figure 10:Ablation studies of our proposed SSA mechanism.",
                "position": 642
            },
            {
                "img": "https://arxiv.org/html/2505.17412/x10.png",
                "caption": "Figure 11:Ablation studies for sparse conditioning mechanism.",
                "position": 645
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Limitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17412/extracted/6469542/assets/mesh_comp.png",
                "caption": "Figure 12:More qualitative comparisons between otheropen-sourceimage-to-3D methods and our approach.Best viewed with zoom-in.",
                "position": 1359
            },
            {
                "img": "https://arxiv.org/html/2505.17412/extracted/6469542/assets/mesh_comp_c.png",
                "caption": "Figure 13:Qualitative comparisons betweenclosed-source commercialimage-to-3D models and our approach. Note that for each closed-source model we use the default setting of their web app.Best viewed with zoom-in.",
                "position": 1362
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]