[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10477/x1.png",
                "caption": "Figure 1:(a) Current works segregate physical entities. (b) Our SocioSeg identifies social entities (names, functions) via multi-modal data.(c) Existing reasoning methods employ a single-stage reasoning approach. (d) Our SocioReasoner employs a two-stage reasoning strategy with render-and-refine mechanism.",
                "position": 135
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3SocioSeg Dataset",
        "images": []
    },
    {
        "header": "4SocioReasoner Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10477/x2.png",
                "caption": "Figure 2:SocioReasoner Framework. Given a satellite image, a digital map, and a textual instruction, the VLM first generates bounding boxes to localize candidate regions. These boxes are fed into SAM to produce a coarse mask. The boxes and mask are then rendered onto the inputs for re-evaluation. The VLM emits boxes and points, which are again fed into SAM to yield the final mask.",
                "position": 278
            },
            {
                "img": "https://arxiv.org/html/2601.10477/x3.png",
                "caption": "Figure 3:Visualization of the SocioReasoner results. The top panel shows a comparison between the results of SocioReasoner (with both stages visualized) and competitive baselines. The bottom-left panel illustrates the reasoning process of SocioReasoner. The bottom-right panel displays the visualization results of SocioReasoner on the out-of-domain dataset.",
                "position": 321
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10477/",
                "caption": "Figure 4:Per-class accuracy comparison across Socio-classes. We select the top-20 most frequent classes in the test set for visualization. The full results are available in AppendixA.5.1.",
                "position": 342
            },
            {
                "img": "https://arxiv.org/html/2601.10477/x5.png",
                "caption": "Figure 5:(a) Sum reward during training. It shows the sum of rewards across training steps in the two-stage workflow.(b) Multi-stage gIoU during training. It shows the gIoU improvement across training steps in the two-stage workflow.\n(c) Different number of points. It visualizes the result of SocioReasoner in the refinement stage with different numbers of points.",
                "position": 552
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10477/x6.png",
                "caption": "Figure 6:The SocioSeg dataset overview. (a) Sample distribution across the three hierarchical tasks. (b) Socio-function class distribution. (c) Socio-name word cloud. (d) Socio-class distribution. (e) Sample examples from SocioSeg, including satellite images, digital maps, and socio-semantic mask labels.",
                "position": 1264
            },
            {
                "img": "https://arxiv.org/html/2601.10477/x7.png",
                "caption": "Figure 7:The SocioSeg OOD (New Region) dataset distribution.",
                "position": 1284
            },
            {
                "img": "https://arxiv.org/html/2601.10477/x8.png",
                "caption": "Figure 8:The two prompts above are the user prompt template for SocioReasoner, which adopts a two-stage reasoning process to mimic human annotation. The prompt below is the single-stage prompt used for the baseline without reflection and zero-shot GPT and Qwen models.",
                "position": 1561
            },
            {
                "img": "https://arxiv.org/html/2601.10477/x9.png",
                "caption": "Figure 9:The rewards visualization during the training process.",
                "position": 1954
            },
            {
                "img": "https://arxiv.org/html/2601.10477/x10.png",
                "caption": "Figure 10:All method Comparisons of SocioReasoner across the three hierarchical tasks.",
                "position": 1959
            },
            {
                "img": "https://arxiv.org/html/2601.10477/x11.png",
                "caption": "Figure 11:Failure cases of SocioReasoner.",
                "position": 1964
            },
            {
                "img": "https://arxiv.org/html/2601.10477/x12.png",
                "caption": "Figure 12:More inference examples of SocioReasoner.",
                "position": 1968
            },
            {
                "img": "https://arxiv.org/html/2601.10477/x13.png",
                "caption": "Figure 13:Per-class accuracy comparison across Socio-classes.",
                "position": 1973
            },
            {
                "img": "https://arxiv.org/html/2601.10477/x14.png",
                "caption": "Figure 14:Per-class accuracy comparison across Socio-functions.",
                "position": 1979
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]