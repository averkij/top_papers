[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19526/assets/QuantiPhy_logo_pure.png",
                "caption": "",
                "position": 140
            },
            {
                "img": "https://arxiv.org/html/2512.19526/x1.png",
                "caption": "",
                "position": 180
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19526/x2.png",
                "caption": "Figure 2:Sample examples fromQuantiPhy, illustrating the four core task combinations defined byDimensionality{2D, 3D} andPhysical Prior{Static, Dynamic}, as described in section3.1. Our collected data is diverse in nature, and each video is paired with multiple (prior, question, ground truth) triplets. Please see the supplementary materials for more data examples.",
                "position": 212
            }
        ]
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19526/sec/images/data_distribution2.png",
                "caption": "Figure 3:QuantiPhyStatistics.The collected data and curated QA pairs are among four main setups with further breakdowns.",
                "position": 289
            },
            {
                "img": "https://arxiv.org/html/2512.19526/x3.png",
                "caption": "Figure 4:The construction ofQuantiPhyproceeds in three sequential stages. First, we collect diverse raw videos from three different sources. Additionally, we segment these videos with solid plain background (described insubsection 3.2). Second, we obtain high-quality annotations, employing distinct labeling methods tailored to each data source to accurately capture the object’s physical properties. Finally, we formulate the benchmark tasks by associating each video with multiple (prior, question, ground truth) triplets. Each triplet is then categorized as either2Dor3D, depending on the object’s movement relative to the camera.",
                "position": 296
            }
        ]
    },
    {
        "header": "4Evaluation onQuantiPhy",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19526/sec/images/icons/medal1.png",
                "caption": "Table 1:Evaluation results onQuantiPhy. We report Mean Relative Accuracy (MRA %) on four kinematic categories (2S, 2D, 3S, 3D) and their average.Darkcell marks the best overall model andlightcell marks the best open-weight model.",
                "position": 416
            },
            {
                "img": "https://arxiv.org/html/2512.19526/sec/images/icons/medal1.png",
                "caption": "Table 1:Evaluation results onQuantiPhy. We report Mean Relative Accuracy (MRA %) on four kinematic categories (2S, 2D, 3S, 3D) and their average.Darkcell marks the best overall model andlightcell marks the best open-weight model.",
                "position": 419
            },
            {
                "img": "https://arxiv.org/html/2512.19526/sec/images/icons/medal2.png",
                "caption": "",
                "position": 480
            },
            {
                "img": "https://arxiv.org/html/2512.19526/sec/images/icons/medal3.png",
                "caption": "",
                "position": 515
            }
        ]
    },
    {
        "header": "5Dissecting Quantitative Reasoning in VLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19526/sec/images/mra_line_plot.png",
                "caption": "Figure 5:Effect of scene context.We plot the MRA (%) scores for all benchmark models on different categories, sorted in descending order according to their average MRA performance.",
                "position": 1788
            }
        ]
    },
    {
        "header": "6Discussion and Future Work",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Studies and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19526/x4.png",
                "caption": "Figure 6:Case 1: Faithful pixel–prior reasoning.",
                "position": 2849
            },
            {
                "img": "https://arxiv.org/html/2512.19526/x5.png",
                "caption": "Figure 7:Case 2: Counterfactual prior breaks faithfulness.",
                "position": 2862
            },
            {
                "img": "https://arxiv.org/html/2512.19526/x6.png",
                "caption": "Figure 8:Case 3: Video ablation reveals reliance on priors.",
                "position": 2873
            },
            {
                "img": "https://arxiv.org/html/2512.19526/x7.png",
                "caption": "Figure 9:Case 4: Strong gravitational prior overrides counterfactual physics.",
                "position": 2884
            },
            {
                "img": "https://arxiv.org/html/2512.19526/sec/images/mra_density_plot.png",
                "caption": "Figure 10:Distribution of MRA by model.One caveat to note is that the Avg. MRA inTable 2reflects the mean MRA across inference task categories for each model (i.e., the average MRA of2D-Static,2D-Dynamic,3D-Static, and3D-Dynamic). In contrast, the mean in this distribution plot represents the average MRA at the individual-question level for each model.",
                "position": 2982
            }
        ]
    },
    {
        "header": "Appendix BDataset Construction Guidelines",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19526/sec/images/SVA.png",
                "caption": "Figure 11:Examples of S/V/A scene.",
                "position": 3325
            },
            {
                "img": "https://arxiv.org/html/2512.19526/sec/images/2_3.png",
                "caption": "Figure 12:Examples of 2/3 scene.",
                "position": 3347
            },
            {
                "img": "https://arxiv.org/html/2512.19526/sec/images/s_m.png",
                "caption": "Figure 13:Examples of S/M scene.",
                "position": 3369
            },
            {
                "img": "https://arxiv.org/html/2512.19526/sec/images/XSC2.png",
                "caption": "Figure 14:Examples of X/S/C scene.",
                "position": 3397
            }
        ]
    },
    {
        "header": "Appendix CDetails of Data Collection",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19526/sec/images/collision.jpg",
                "caption": "Figure 15:Physics-driven example.A bowling ball collides with pins under Newtonian simulation.\nThe motion and resulting trajectories arise directly from rigid-body dynamics and elastic collisions, making this clip representative of our force-based motion category.",
                "position": 3862
            },
            {
                "img": "https://arxiv.org/html/2512.19526/sec/images/swimring.jpg",
                "caption": "Figure 16:Keyframed example.The floating swim ring follows a manually authored animation curve rather than buoyancy, drag, or wind forces.\nIts trajectory is visually plausible but not physically derived, representing our keyframed motion category.",
                "position": 3867
            },
            {
                "img": "https://arxiv.org/html/2512.19526/sec/images/curve.png",
                "caption": "Figure 17:Keyframed animation-curve example.The animation curve controlling the swim ring inFigure 16.\nThe ring moves forward along a manually authored trajectory, while small randomized perturbations in translation and rotation are added to imitate the visual appearance of floating motion.",
                "position": 3872
            },
            {
                "img": "https://arxiv.org/html/2512.19526/sec/images/rigidbody.png",
                "caption": "Figure 18:Physics-driven rigid-body example.Rigid-body simulation of objects such as the bowling pins reacting to the impact of an incoming ball inFigure 15.\nThe pins’ motion, tipping, and scattering are governed entirely by Newtonian dynamics and collision responses within the physics engine.",
                "position": 3877
            },
            {
                "img": "https://arxiv.org/html/2512.19526/sec/images/indoor_scene.png",
                "caption": "Figure 19:Examples of indoor scene.",
                "position": 3922
            },
            {
                "img": "https://arxiv.org/html/2512.19526/sec/images/outdoor_scene.png",
                "caption": "Figure 20:Examples of outdoor scene.",
                "position": 3925
            },
            {
                "img": "https://arxiv.org/html/2512.19526/sec/images/complex_red_blood_cell.png",
                "caption": "Figure 21:Examples of microscopic scene.",
                "position": 3931
            },
            {
                "img": "https://arxiv.org/html/2512.19526/sec/images/astronaut.png",
                "caption": "Figure 22:Examples of extraterrestrial scene.",
                "position": 3934
            },
            {
                "img": "https://arxiv.org/html/2512.19526/sec/images/simplify_to_x_bg.png",
                "caption": "Figure 23:Examples of building X background scene.",
                "position": 3943
            },
            {
                "img": "https://arxiv.org/html/2512.19526/sec/images/simplify_to_s_bg.png",
                "caption": "Figure 24:Examples of building S background scene.",
                "position": 3946
            },
            {
                "img": "https://arxiv.org/html/2512.19526/sec/images/scaled_scene.png",
                "caption": "Figure 25:Examples of scaled scene.",
                "position": 3955
            },
            {
                "img": "https://arxiv.org/html/2512.19526/sec/images/depth_annotator.png",
                "caption": "Figure 26:The users’ interface of the tool we have developed for obtaining depth value.",
                "position": 4349
            },
            {
                "img": "https://arxiv.org/html/2512.19526/x8.png",
                "caption": "Figure 27:Examples of videos from open-source platforms.",
                "position": 4361
            },
            {
                "img": "https://arxiv.org/html/2512.19526/x9.png",
                "caption": "Figure 28:Examples of self-recorded videos with identity removed.",
                "position": 4364
            },
            {
                "img": "https://arxiv.org/html/2512.19526/sec/images/segmentation_ui.png",
                "caption": "Figure 29:The users’ interface of the segmentation tool we have developed.",
                "position": 4405
            },
            {
                "img": "https://arxiv.org/html/2512.19526/sec/images/segmented_blender.png",
                "caption": "Figure 30:Examples of segmented blender data.After segmentation, we can replace the background image freely.",
                "position": 4444
            },
            {
                "img": "https://arxiv.org/html/2512.19526/sec/images/segmented_lab.png",
                "caption": "Figure 31:Examples of segmented lab data.After segmentation, we can replace the background image freely.",
                "position": 4447
            },
            {
                "img": "https://arxiv.org/html/2512.19526/sec/images/segmented_internet.png",
                "caption": "Figure 32:Examples of segmented internet data.After segmentation, we can replace the background image freely.",
                "position": 4450
            }
        ]
    },
    {
        "header": "Appendix DDetails of Data Annotation",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19526/sec/images/human_height_measure.png",
                "caption": "Figure 33:Examples of human height measuring.",
                "position": 4496
            },
            {
                "img": "https://arxiv.org/html/2512.19526/sec/images/bird_width.png",
                "caption": "Figure 34:Examples of flying animal measuring.",
                "position": 4502
            },
            {
                "img": "https://arxiv.org/html/2512.19526/x10.png",
                "caption": "Figure 35:Pixel-level measurement tool.",
                "position": 5410
            }
        ]
    },
    {
        "header": "Appendix EVision-Language Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19526/x11.png",
                "caption": "Figure 36:Selected examples of text inputs and answers (part 1).Question is the natural-language prompt presented to the VLM. GT Prior is the physical prior provided to the model. GT Depth Info is the depth annotation used for3Dreasoning tasks. GT Posterior is the numeric ground-truth answer to the kinematic inference question. SeeTable 3for detailed explanation. Raw Response shows the corresponding VLM output for each model, and Parsed Value shows the parsed value extracted from the Raw Response.",
                "position": 5514
            },
            {
                "img": "https://arxiv.org/html/2512.19526/x12.png",
                "caption": "Figure 37:Selected examples of text inputs and answers (part 2).Question is the natural-language prompt presented to the VLM. GT Prior is the physical prior provided to the model. GT Depth Info is the depth annotation used for3Dreasoning tasks. GT Posterior is the numeric ground-truth answer to the kinematic inference question. SeeTable 3for detailed explanation. Raw Response shows the corresponding VLM output for each model, and Parsed Value shows the parsed value extracted from the Raw Response.",
                "position": 5517
            }
        ]
    },
    {
        "header": "Appendix FPrompt Design",
        "images": []
    },
    {
        "header": "Appendix GAnswer Retrieval and Parsing",
        "images": []
    },
    {
        "header": "Appendix HHuman Study Details.",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19526/sec/images/2d_UI.jpg",
                "caption": "(a)2Dsurvey interface. Participants see the prior ground truth, question text, and a numeric answer box, and can replay or scrub the video.",
                "position": 5707
            },
            {
                "img": "https://arxiv.org/html/2512.19526/sec/images/2d_UI.jpg",
                "caption": "(a)2Dsurvey interface. Participants see the prior ground truth, question text, and a numeric answer box, and can replay or scrub the video.",
                "position": 5710
            },
            {
                "img": "https://arxiv.org/html/2512.19526/sec/images/3d_UI.jpg",
                "caption": "(b)3Dsurvey interface. The layout mirrors the2Dcondition, with additional depth prior ground truth.",
                "position": 5716
            },
            {
                "img": "https://arxiv.org/html/2512.19526/sec/images/human_mra_boxplots.png",
                "caption": "Figure 39:Distribution of human quantitative reasoning performance.Horizontal boxplots summarize participant-level mean MRA scores for the2D(top)\nand3D(bottom) survey conditions.",
                "position": 5779
            }
        ]
    },
    {
        "header": "Appendix ISketchfab Model Sources",
        "images": []
    }
]