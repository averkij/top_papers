[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/teaser.png",
                "caption": "Figure 1:Improving 3D correspondence understanding through finetuning on feature equivariance.Left:finetuning feature equivariance on one synthetic object can already enhance the vision transformer’s ability to generate better 3D feature correspondences on general objects.Right:This improvement further leads to superior performance across multiple 3D tasks, including pose estimation, video tracking, and semantic correspondence.",
                "position": 138
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Evaluation of Multiview Feature Equivariance",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/horse.png",
                "caption": "Figure 2:Feature visualizations of different models.The sample image is rendered from Objeverse. Colors are computed from the high-dimensional features using PCA. We can see that MAE struggles to distinguish different parts of the content (e.g.similar features between head and body). Both CLIP and DeiT produce inconsistent features for the chest region between View 1 and View 2. DINOv2 gives the best correspondence.",
                "position": 180
            },
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/correlation_pose_image.png",
                "caption": "Figure 3:Correlation between multiview feature equivariance and the task performances.Along the horizontal axis, lower APE indicates better feature equivariance, while the vertical axis reflects higher task performance across all four plots. The data points align roughly along the diagonal from the top left to the bottom right, suggesting a strong correlation between improved feature equivariance and better task performance.",
                "position": 189
            },
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/correlation_pose_video.png",
                "caption": "",
                "position": 192
            },
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/correlation_tracking.png",
                "caption": "",
                "position": 194
            },
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/correlation_corr.png",
                "caption": "",
                "position": 195
            },
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/correspondence_type.png",
                "caption": "Figure 4:Illustration of different types of correspondence tasks evaluated in our work.",
                "position": 250
            }
        ]
    },
    {
        "header": "3Feature Finetuning with Multiview Equivariance",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/finetune_sim2real_ape.png",
                "caption": "Figure 5:Generalization from synthetic images (Objaverse) to real images (MVImgNet).Left:Data points roughly around the diagonal from the bottom left to the upper right indicate the correlation between the APE tested on the two datasets. The * next to the model name means it is finetuned. All finetuning is done on Objaverse with only synthetic data.Right:Finetuned on Objaverse, the feature equivariance of the model (measured in PCDP) improves on MVImgNet.",
                "position": 284
            },
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/finetune_sim2real_pcdp.png",
                "caption": "",
                "position": 287
            },
            {
                "img": "https://arxiv.org/html/2411.19458/x1.png",
                "caption": "Figure 6:Feature visualization of DINOv2 before and after finetuning on MVImgNet objects (left two) and TAP-VID-DAVIS scenes (right one).For each example, we select three different views. The first column provides a reference color produced by PCA, while the second and third columns show the predicted feature correspondences. Our finetuned model demonstrates reduced noise and smoother feature boundaries, particularly noticeable in the reduction of jagged edges.",
                "position": 295
            },
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/finetune_pose_image.png",
                "caption": "Figure 7:One-shot pose estimation results before and after feature equivariance finetuning.",
                "position": 310
            },
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/finetune_pose_video.png",
                "caption": "",
                "position": 313
            },
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/finetune_tracking.png",
                "caption": "Figure 8:Video tracking results before and after feature equivariance finetuning.",
                "position": 323
            },
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/finetune_corr_diff_view.png",
                "caption": "Figure 9:Semantic correspondence results before and after feature equivariance finetuneing.",
                "position": 327
            },
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/finetune_corr_same_view.png",
                "caption": "",
                "position": 330
            },
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/ablation_obj_pose.png",
                "caption": "Figure 10:Finetuned performancesw.r.t.#training objects.We evaluate the performances of the DINOv2 model finetuned with 0, 1, 5, 10, 20, 50, 100 objects on the three tasks.",
                "position": 355
            },
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/ablation_obj_tracking.png",
                "caption": "",
                "position": 359
            },
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/ablation_obj_corr.png",
                "caption": "",
                "position": 361
            },
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/diff_objs.png",
                "caption": "Figure 11:Finetuning with different objects.All results are tested with finetuned DINOv2. Dashed lines indicate the performances of the original pretrained model. The feature finetuning method is effective with as few as one single object. It also shows insensitivity to the specific choice of the object, even if the object has limited textures or is uncommon in daily life.",
                "position": 370
            },
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/ablation_iter_pose.png",
                "caption": "Figure 12:Finetuned DINOv2 performancesw.r.t.#training iterations, trained withonly one objectover 0, 1, 5, 10, 20, 50, 100, 1000, 10000 training iterations.",
                "position": 485
            },
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/ablation_iter_tracking.png",
                "caption": "",
                "position": 489
            },
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/ablation_iter_corr.png",
                "caption": "",
                "position": 491
            }
        ]
    },
    {
        "header": "4Design Choices for Finetuning",
        "images": []
    },
    {
        "header": "5Related Works",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/fit-1.jpg",
                "caption": "Figure 13:FiT and DINOv2 semantic correspondence visualization. We find that FiT significantly disrupts the semantics of certain parts.",
                "position": 2261
            },
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/fit-2.jpg",
                "caption": "",
                "position": 2265
            },
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/lerf.jpg",
                "caption": "Figure 14:Visualization of LERF relevancy maps for the query “plate”.Our finetuned DINO features produce a more focused and accurate relevancy map compared to the original DINO features, with better localization of the plate region and reduced noise in irrelevant areas such as cookies.",
                "position": 2693
            },
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/conv_exp.jpg",
                "caption": "Figure 15:Comparison of feature visualizations with varying convolutional layers.Adding more than one convolutional layer introduces noise and reduces feature coherence, as shown by the highlighted regions.",
                "position": 2720
            },
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/hemisphere.jpg",
                "caption": "Figure 16:Feature visualization of an untextured hemisphere from different viewpoints.Top row:Input hemisphere rendered from four different angles.Middle row:Feature embeddings from DINOv2 visualized using RGB mapping, showing inconsistent features across views and edges (highlighted by white circles).Bottom row:Our fine-tuned DINOv2 produces more consistent features that better preserve correspondences across viewpoints, particularly at edges and inward outward views.",
                "position": 2734
            },
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/bg_inv.png",
                "caption": "Table 16:Quantitative results on the number of feature inliers that are background-invariant.",
                "position": 2743
            },
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/bg_inv.png",
                "caption": "Figure 17:Visualization of DINOv2’s feature correspondence before and after finetuning, using mutual nearest neighbor.After finetuning, we get more feature correspondences.",
                "position": 2810
            },
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/pose_estimation.jpg",
                "caption": "Figure 18:Pose estimation pipeline.During the onboarding phase, 2D dense features are extracted from the provided reference video and stored in a database. During inference, features are matched between a single query image and the database, followed by 3D-2D RANSAC-PnP to compute the final pose.",
                "position": 2824
            },
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/tracking.jpg",
                "caption": "Figure 19:Tracking pipeline.For each point in the source frame, its nearest neighbors are located in the feature space across other frames.",
                "position": 2827
            },
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/semantic_transfer.jpg",
                "caption": "Figure 20:Semantic transfer pipeline.For the given keypoints in the reference image, descriptors are extracted using the frozen ViT, and their nearest neighbors are identified in the query image’s feature space.",
                "position": 2830
            },
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/semantic_depth.jpg",
                "caption": "Figure 21:Semantic segmentation and depth estimation pipeline.Given an input image, a linear layer is fine-tuned on top of the frozen ViT to predict segmentation or depth.",
                "position": 2833
            },
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/sample_ycbv.png",
                "caption": "Figure 22:Qualitative results on YCB-Video pose estimation for different models, both before and after finetuning, are presented. Ground-truth poses are shown in green, while predictions are depicted in red. It can be observed that, in most cases, pose accuracy improves after finetuning, particularly for the MAE, CLIP, and DeiT models.",
                "position": 2843
            },
            {
                "img": "https://arxiv.org/html/2411.19458/x2.png",
                "caption": "Figure 23:Qualitative results on TAP-VID-DAVIS for different models, both before and after finetuning, are shown. Query points are marked in various colors in the first frame, with red lines indicating the trajectory of the points. Prior to finetuning, the trajectories are highly noisy and inconsistent. However, after finetuning, tracking becomes significantly more stable.",
                "position": 2846
            },
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/pascal_pf_1.png",
                "caption": "Figure 24:Qualitative results on PF-PASCAL (different views) for various models, both before and after finetuning, are presented. For each pair, the left image is the reference, and the right is the query. Ground-truth correspondences are shown in green, while predictions are depicted in red. It can be observed that, in most cases, finetuning improves accuracy by aligning the keypoints closer to their correct positions.",
                "position": 2849
            },
            {
                "img": "https://arxiv.org/html/2411.19458/extracted/6033091/figures/pascal_pf_2.png",
                "caption": "",
                "position": 2853
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]