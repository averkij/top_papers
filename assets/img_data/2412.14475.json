[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14475/x1.png",
                "caption": "Figure 1:Construction pipeline of multimodal triplets:(a)mining of image pairs,(b)generation of open-ended instructions. Multiple similarity models are used to introduce diversified correlations for the image pairs.",
                "position": 176
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14475/x2.png",
                "caption": "Figure 2:Performance scaling of MMRet-base on the MegaPairs as data size increases. The dashed lines indicate the performance of MagicLens-B (CLIP) trained on their dataset of 36.7M data pairs.",
                "position": 778
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ADetailed Prompt for Annotating Open-Ended Instructions",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14475/x3.png",
                "caption": "Figure 3:The specific prompts for MLLM. The value of WORD_NUM ranges from 60 to 100 in our practical data generation to enhance the diversity of the generated description.",
                "position": 1760
            },
            {
                "img": "https://arxiv.org/html/2412.14475/x4.png",
                "caption": "Figure 4:The specific prompts for LLM. The figure showcases two demonstrations, while in our practical data generation process, five demonstrations are randomly selected from a pool of 50 and fed into the LLM.",
                "position": 1763
            }
        ]
    },
    {
        "header": "Appendix BTraining Details of MMRet on MegaPairs",
        "images": []
    },
    {
        "header": "Appendix CDetailed Information and Evaluation Metrics of Zero-Shot CIR Benchmarks",
        "images": []
    },
    {
        "header": "Appendix DFull results on CIR Benchmarks",
        "images": []
    },
    {
        "header": "Appendix EFull Results on MMEB Benchmark",
        "images": []
    },
    {
        "header": "Appendix FVisualized Examples of MegaPairs",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14475/x5.png",
                "caption": "Figure 5:The visualized examples of MegaPairs. Each row represents a single example, with the query item highlighted in a blue rectangle and the target items enclosed within a dashed box.",
                "position": 3799
            },
            {
                "img": "https://arxiv.org/html/2412.14475/x6.png",
                "caption": "Figure 6:Top-5 retrieved images of MMRet and MagicLens on zero-shot CIR tasks, both using the CLIP-L backbone. Queries are shown with a blue background, and the most correct retrieved images are marked with green outlines.",
                "position": 3802
            }
        ]
    },
    {
        "header": "Appendix GQualitative Results of MMRet on Zero-shot CIR Tasks",
        "images": []
    }
]