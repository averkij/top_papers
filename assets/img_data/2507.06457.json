[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.06457/x1.png",
                "caption": "Figure 1:Three ‘generations’ of linear-attention state updates. Generation 1 (left): Gated Vector Recurrence keeps a single vectorht∈ℝdsubscriptℎ𝑡superscriptℝ𝑑h_{t}\\in\\mathbb{R}^{d}italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT. Each step first decays the previous state by an element-wise gateαtsubscript𝛼𝑡\\alpha_{t}italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT(grey), then adds the gated inputvtsubscript𝑣𝑡v_{t}italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT(pink). Update and memory are bothO⁢(d)𝑂𝑑O(d)italic_O ( italic_d ). Generation 2 (centre): Outer-Product State with Decay stores a full matrixSt∈ℝd×nsubscript𝑆𝑡superscriptℝ𝑑𝑛S_{t}\\in\\mathbb{R}^{d\\times n}italic_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_n end_POSTSUPERSCRIPT. A diagonal decay mask(𝟏⁢αt⊤)1superscriptsubscript𝛼𝑡top(\\mathbf{1}\\,\\alpha_{t}^{\\!\\top})( bold_1 italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT )scales every column, after which a rank-1 outer productvt⁢kt⊤subscript𝑣𝑡superscriptsubscript𝑘𝑡topv_{t}k_{t}^{\\!\\top}italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPTis written (pink/blue). Cost rises toO⁢(d2)𝑂superscript𝑑2O(d^{2})italic_O ( italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ). Generation 3 (right):ΔΔ\\Deltaroman_Δ-Rule Controlled Forgetting first erases the slice ofSt−1subscript𝑆𝑡1S_{t-1}italic_S start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPTaligned with keyktsubscript𝑘𝑡k_{t}italic_k start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTusing the projectorI−βt⁢kt⁢kt⊤𝐼subscript𝛽𝑡subscript𝑘𝑡superscriptsubscript𝑘𝑡topI-\\beta_{t}k_{t}k_{t}^{\\!\\top}italic_I - italic_β start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT(cells with1111), then writes the new outer productβt⁢vt⁢kt⊤subscript𝛽𝑡subscript𝑣𝑡superscriptsubscript𝑘𝑡top\\beta_{t}v_{t}k_{t}^{\\!\\top}italic_β start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT. This selective forget-then-write preserves capacity while matching theO⁢(d2)𝑂superscript𝑑2O(d^{2})italic_O ( italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )compute and memory of Generation 2. The arrow at the bottom emphasizes the historical progression from minimal to maximal recall capability.",
                "position": 345
            },
            {
                "img": "https://arxiv.org/html/2507.06457/x2.png",
                "caption": "Figure 2:Hybrid architecture: an embedding layer,N𝑁Nitalic_Nrepetitions of\nlinear-attention followed by full-attention, and a projection head.\nOnly the full-attention blocks grow the KV cache.",
                "position": 501
            }
        ]
    },
    {
        "header": "4Empirical Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.06457/extracted/6607318/Figures/trendgraph.png",
                "caption": "Figure 3:Language performance and recall performance tasks are averaged and compared over varying ratios",
                "position": 739
            },
            {
                "img": "https://arxiv.org/html/2507.06457/extracted/6607318/Figures/trend_graphs_2x2.png",
                "caption": "Figure 4:RULER sub task results based on ratio. RetNet and HGRN model families are omitted as their recall benchmark results were insignificant.",
                "position": 764
            },
            {
                "img": "https://arxiv.org/html/2507.06457/x3.png",
                "caption": "(a)Sequence lengthL=4096𝐿4096L=4096italic_L = 4096.",
                "position": 932
            },
            {
                "img": "https://arxiv.org/html/2507.06457/x3.png",
                "caption": "(a)Sequence lengthL=4096𝐿4096L=4096italic_L = 4096.",
                "position": 935
            },
            {
                "img": "https://arxiv.org/html/2507.06457/x4.png",
                "caption": "(b)Sequence lengthL=32768𝐿32768L=32768italic_L = 32768.",
                "position": 940
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.06457/x5.png",
                "caption": "Figure 6:Relationship between the sequence length and the number of FLOPs required by different token mixers. Note that the HGRN-2 and GLA overlap, see analysis in the text.",
                "position": 1795
            }
        ]
    },
    {
        "header": "Appendix AForward-Pass FLOP Analysis of Token Mixers",
        "images": []
    }
]