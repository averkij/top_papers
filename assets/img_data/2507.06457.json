[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.06457/x1.png",
                "caption": "Figure 1:Three â€˜generationsâ€™ of linear-attention state updates. Generation 1 (left): Gated Vector Recurrence keeps a single vectorhtâˆˆâ„dsubscriptâ„ğ‘¡superscriptâ„ğ‘‘h_{t}\\in\\mathbb{R}^{d}italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT. Each step first decays the previous state by an element-wise gateÎ±tsubscriptğ›¼ğ‘¡\\alpha_{t}italic_Î± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT(grey), then adds the gated inputvtsubscriptğ‘£ğ‘¡v_{t}italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT(pink). Update and memory are bothOâ¢(d)ğ‘‚ğ‘‘O(d)italic_O ( italic_d ). Generation 2 (centre): Outer-Product State with Decay stores a full matrixStâˆˆâ„dÃ—nsubscriptğ‘†ğ‘¡superscriptâ„ğ‘‘ğ‘›S_{t}\\in\\mathbb{R}^{d\\times n}italic_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_d Ã— italic_n end_POSTSUPERSCRIPT. A diagonal decay mask(ğŸâ¢Î±tâŠ¤)1superscriptsubscriptğ›¼ğ‘¡top(\\mathbf{1}\\,\\alpha_{t}^{\\!\\top})( bold_1 italic_Î± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT )scales every column, after which a rank-1 outer productvtâ¢ktâŠ¤subscriptğ‘£ğ‘¡superscriptsubscriptğ‘˜ğ‘¡topv_{t}k_{t}^{\\!\\top}italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPTis written (pink/blue). Cost rises toOâ¢(d2)ğ‘‚superscriptğ‘‘2O(d^{2})italic_O ( italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ). Generation 3 (right):Î”Î”\\Deltaroman_Î”-Rule Controlled Forgetting first erases the slice ofStâˆ’1subscriptğ‘†ğ‘¡1S_{t-1}italic_S start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPTaligned with keyktsubscriptğ‘˜ğ‘¡k_{t}italic_k start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTusing the projectorIâˆ’Î²tâ¢ktâ¢ktâŠ¤ğ¼subscriptğ›½ğ‘¡subscriptğ‘˜ğ‘¡superscriptsubscriptğ‘˜ğ‘¡topI-\\beta_{t}k_{t}k_{t}^{\\!\\top}italic_I - italic_Î² start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT(cells with1111), then writes the new outer productÎ²tâ¢vtâ¢ktâŠ¤subscriptğ›½ğ‘¡subscriptğ‘£ğ‘¡superscriptsubscriptğ‘˜ğ‘¡top\\beta_{t}v_{t}k_{t}^{\\!\\top}italic_Î² start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT. This selective forget-then-write preserves capacity while matching theOâ¢(d2)ğ‘‚superscriptğ‘‘2O(d^{2})italic_O ( italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )compute and memory of Generation 2. The arrow at the bottom emphasizes the historical progression from minimal to maximal recall capability.",
                "position": 345
            },
            {
                "img": "https://arxiv.org/html/2507.06457/x2.png",
                "caption": "Figure 2:Hybrid architecture: an embedding layer,Nğ‘Nitalic_Nrepetitions of\nlinear-attention followed by full-attention, and a projection head.\nOnly the full-attention blocks grow the KV cache.",
                "position": 501
            }
        ]
    },
    {
        "header": "4Empirical Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.06457/extracted/6607318/Figures/trendgraph.png",
                "caption": "Figure 3:Language performance and recall performance tasks are averaged and compared over varying ratios",
                "position": 739
            },
            {
                "img": "https://arxiv.org/html/2507.06457/extracted/6607318/Figures/trend_graphs_2x2.png",
                "caption": "Figure 4:RULER sub task results based on ratio. RetNet and HGRN model families are omitted as their recall benchmark results were insignificant.",
                "position": 764
            },
            {
                "img": "https://arxiv.org/html/2507.06457/x3.png",
                "caption": "(a)Sequence lengthL=4096ğ¿4096L=4096italic_L = 4096.",
                "position": 932
            },
            {
                "img": "https://arxiv.org/html/2507.06457/x3.png",
                "caption": "(a)Sequence lengthL=4096ğ¿4096L=4096italic_L = 4096.",
                "position": 935
            },
            {
                "img": "https://arxiv.org/html/2507.06457/x4.png",
                "caption": "(b)Sequence lengthL=32768ğ¿32768L=32768italic_L = 32768.",
                "position": 940
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.06457/x5.png",
                "caption": "Figure 6:Relationship between the sequence length and the number of FLOPs required by different token mixers. Note that the HGRN-2 and GLA overlap, see analysis in the text.",
                "position": 1795
            }
        ]
    },
    {
        "header": "Appendix AForward-Pass FLOP Analysis of Token Mixers",
        "images": []
    }
]