[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24200/figure/teaser.jpg",
                "caption": "Figure 1:We presentUniVid, an open-source unified video model for both understanding and generation tasks. Our model requires only a small amount of high-quality data for fine-tuning, achieveing competitive results across various tasks.",
                "position": 119
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24200/x1.png",
                "caption": "Figure 2:Overall architecture of our proposed UniVid for unified video understanding and generation. UniVid couples an autoregressive-based MLLM with a DiT-based diffusion decoder. The MLLM’s outputs are linked through a lightweight adapter to interface with the Wan(Wang et al.,2025)backbone, forming the generation branch, while simultaneously passing through the Pyramid Reflection module to connect with the LLM, thereby establishing the understanding branch.",
                "position": 161
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3The Proposed Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24200/x2.png",
                "caption": "Figure 3:Comparisons with State-of-the-Art Video Generation Models(Wang et al.,2025; MiniMax,2024; Kong et al.,2024; Fu et al.,2024).",
                "position": 955
            },
            {
                "img": "https://arxiv.org/html/2509.24200/x3.png",
                "caption": "Figure 4:Comparisons of State-of-the-Art Video Understanding Models(Lin et al.,2024; Xu et al.,2024).",
                "position": 980
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24200/x4.png",
                "caption": "Figure 5:The qualitative results of the video understanding. Blue for static questions, green for dynamic questions.",
                "position": 2114
            },
            {
                "img": "https://arxiv.org/html/2509.24200/x5.png",
                "caption": "Figure 6:The qualitative results of T2V and TI2V generation.",
                "position": 2117
            },
            {
                "img": "https://arxiv.org/html/2509.24200/x6.png",
                "caption": "Figure 7:The pipeline of the video understanding.",
                "position": 2125
            },
            {
                "img": "https://arxiv.org/html/2509.24200/figure/ablation_tma_01.jpg",
                "caption": "Figure 8:Ablation Study on Temperature Modality Alignment.",
                "position": 2518
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]