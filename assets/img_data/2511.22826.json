[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22826/x1.png",
                "caption": "",
                "position": 119
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Multi-Modal Alignment (MMA) Bench: Debugging Alignment Gaps in MLLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22826/x2.png",
                "caption": "Figure 2:Automated data curation pipeline for building MMA-Bench.Our two–stage pipeline converts raw AudioSet[gemmeke2017audio]into clean, semantically aligned audio–video samples.Stage 1simplifies the ontology by pruning action-less, ambiguous (e.g., audio event “hiss” could be associated with “stream” or “cat” ), and restricted classes (e.g.,“heart murmur”).Stage 2retains videos based on the simplified audio events. Here, only clips where the audible event is clearly produced by a visible object are retained yielding a high-quality subset which is further post-processed (Sec.3).",
                "position": 193
            },
            {
                "img": "https://arxiv.org/html/2511.22826/x3.png",
                "caption": "Figure 3:Verification of audio-visual semantic alignment. After automated pruning (Fig.2), each clip undergoes44simple yes/no consistency checks.\nA sample is kept only if it passes all checks.",
                "position": 201
            }
        ]
    },
    {
        "header": "4Are modern MLLMs Truly “Multi-modal”?",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22826/images/black_box/unimodal_qwen.png",
                "caption": "(a)Qwen2.5-Omni-7B.",
                "position": 427
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/black_box/unimodal_qwen.png",
                "caption": "(a)Qwen2.5-Omni-7B.",
                "position": 430
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/black_box/unimodal_vl2.png",
                "caption": "(b)VideoLLaMA2.",
                "position": 436
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/black_box/unimodal_pandagpt.png",
                "caption": "(c)PandaGPT.",
                "position": 442
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/black_box/unimodal_gemini.png",
                "caption": "(d)Gemini-2.0-Flash-Lite.",
                "position": 448
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/black_box/semantic_misalign_qwen.png",
                "caption": "(a)Qwen2.5-Omni-7B.",
                "position": 490
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/black_box/semantic_misalign_qwen.png",
                "caption": "(a)Qwen2.5-Omni-7B.",
                "position": 493
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/black_box/semantic_misalign_vl2.png",
                "caption": "(b)VideoLLaMA2.",
                "position": 499
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/black_box/semantic_misalign_pandagpt.png",
                "caption": "(c)PandaGPT.",
                "position": 505
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/black_box/semantic_misalign_gemini.png",
                "caption": "(d)Gemini-2.0-Flash-Lite.",
                "position": 511
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/black_box/text_misalign_qwen.png",
                "caption": "(a)Qwen2.5-Omni-7B.",
                "position": 547
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/black_box/text_misalign_qwen.png",
                "caption": "(a)Qwen2.5-Omni-7B.",
                "position": 550
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/black_box/text_misalign_vl2.png",
                "caption": "(b)VideoLLaMA2.",
                "position": 556
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/black_box/text_misalign_pandagpt.png",
                "caption": "(c)PandaGPT.",
                "position": 562
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/black_box/text_misalign_gemini.png",
                "caption": "(d)Gemini-2.0-Flash-Lite.",
                "position": 568
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/black_box/long_context_qwen.png",
                "caption": "(a)Qwen2.5-Omni-7B.",
                "position": 578
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/black_box/long_context_qwen.png",
                "caption": "(a)Qwen2.5-Omni-7B.",
                "position": 581
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/black_box/long_context_vl2.png",
                "caption": "(b)VideoLLaMA2.",
                "position": 587
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/black_box/long_context_pandagpt.png",
                "caption": "(c)PandaGPT.",
                "position": 593
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/black_box/long_context_gemini.png",
                "caption": "(d)Gemini-2.0-Flash-Lite.",
                "position": 599
            },
            {
                "img": "https://arxiv.org/html/2511.22826/x4.png",
                "caption": "(a)Visual tokens",
                "position": 721
            },
            {
                "img": "https://arxiv.org/html/2511.22826/x4.png",
                "caption": "(a)Visual tokens",
                "position": 731
            },
            {
                "img": "https://arxiv.org/html/2511.22826/x5.png",
                "caption": "(b)Audio tokens",
                "position": 737
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22826/x6.png",
                "caption": "Figure 10:Layer-wise Cohen’s-D before and after modality-aware tuning.Higher absolute values indicate stronger separation between attention distributions underD1D_{1}(visual prompt) andD2D_{2}(audio prompt).\nAfter tuning, the model reallocates attention more decisively toward the modality emphasized by the prompt, reflecting improved modality-selective reasoning.",
                "position": 996
            },
            {
                "img": "https://arxiv.org/html/2511.22826/x7.png",
                "caption": "",
                "position": 1004
            },
            {
                "img": "https://arxiv.org/html/2511.22826/x8.png",
                "caption": "",
                "position": 1007
            },
            {
                "img": "https://arxiv.org/html/2511.22826/x9.png",
                "caption": "",
                "position": 1010
            }
        ]
    },
    {
        "header": "6Limitations and Future Work",
        "images": []
    },
    {
        "header": "7Acknowledgements",
        "images": []
    },
    {
        "header": "Appendix Table of Contents",
        "images": []
    },
    {
        "header": "Appendix AAudioSet Ontology Details and Dataset Simplification",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22826/x10.png",
                "caption": "Figure 11:AudioSet Ontology tree of root “Natural Sounds”Nodes colored red are marked “restricted” by the original ontology. Nodes with gradient indicate that they have multiple parents from either same or different roots.",
                "position": 1192
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/class_wordcloud_filtered.png",
                "caption": "Figure 12:Distribution of4949audio events in the MMA-Bench",
                "position": 1241
            }
        ]
    },
    {
        "header": "Appendix BSample Curation and Quality Verification Protocol",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22826/images/resolution_stats.png",
                "caption": "(a)",
                "position": 1373
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/resolution_stats.png",
                "caption": "(a)",
                "position": 1376
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/temporal_length_stats.png",
                "caption": "(b)",
                "position": 1382
            }
        ]
    },
    {
        "header": "Appendix CVideo data preprocessing",
        "images": []
    },
    {
        "header": "Appendix DFinetuning Details",
        "images": []
    },
    {
        "header": "Appendix EWhite-box analysis: Additional Cohen’s-Dtrends",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22826/x11.png",
                "caption": "(a)Head-wise Cohen’s-D for Video tokens",
                "position": 1497
            },
            {
                "img": "https://arxiv.org/html/2511.22826/x11.png",
                "caption": "(a)Head-wise Cohen’s-D for Video tokens",
                "position": 1500
            },
            {
                "img": "https://arxiv.org/html/2511.22826/x12.png",
                "caption": "(b)Head-wise Cohen’s-D for Audio tokens in",
                "position": 1506
            },
            {
                "img": "https://arxiv.org/html/2511.22826/x13.png",
                "caption": "(a)Cohen’s-D of Video tokens",
                "position": 1520
            },
            {
                "img": "https://arxiv.org/html/2511.22826/x13.png",
                "caption": "(a)Cohen’s-D of Video tokens",
                "position": 1523
            },
            {
                "img": "https://arxiv.org/html/2511.22826/x14.png",
                "caption": "(b)Cohen’s-D of Audio tokens",
                "position": 1529
            }
        ]
    },
    {
        "header": "Appendix FAttention heatmaps",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22826/images/supp/qwen_layer27_all_steps_multimodal.png",
                "caption": "Figure 16:Attention heatmap of a few sampled input tokens against output tokens of Qwen2.5-Omni at layer 28. Notice that majority of attention mass is within textual tokens, indicating the textual prior of current MLLMs and their strong influence in model performance.",
                "position": 1561
            }
        ]
    },
    {
        "header": "Appendix GMisleading Text Prompt",
        "images": []
    },
    {
        "header": "Appendix HCross-Class Generalization and Compositionality",
        "images": []
    },
    {
        "header": "Appendix IBlack-Box Experiment with Other Baseline Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22826/images/supp/semantic_misalign_gemini-2.5-pro.png",
                "caption": "(a)Gemini-2.5-Pro",
                "position": 1672
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/supp/semantic_misalign_gemini-2.5-pro.png",
                "caption": "(a)Gemini-2.5-Pro",
                "position": 1675
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/supp/semantic_misalign_gemini-2.0-Flash.png",
                "caption": "(b)Gemini-2.0-Flash",
                "position": 1681
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/supp/semantic_misalign_Qwen3-Omni-30B-Instruct.png",
                "caption": "(c)Qwen3-Omni-30B",
                "position": 1687
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/supp/semantic_misalign_Chatbridge.png",
                "caption": "(d)ChatBridge",
                "position": 1693
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/supp/unimodal_gemini-2.5-pro.png",
                "caption": "(a)Gemini-2.5-Pro",
                "position": 1716
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/supp/unimodal_gemini-2.5-pro.png",
                "caption": "(a)Gemini-2.5-Pro",
                "position": 1719
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/supp/unimodal_gemini-2.0-Flash.png",
                "caption": "(b)Gemini-2.0-Flash",
                "position": 1725
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/supp/unimodal_Qwen3-Omni-30B-Instruct.png",
                "caption": "(c)Qwen3-Omni-30B",
                "position": 1731
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/supp/unimodal_Chatbridge.png",
                "caption": "(d)ChatBridge",
                "position": 1737
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/supp/text_misalign_gemini-2.5-pro.png",
                "caption": "(a)Gemini-2.5-Pro",
                "position": 1761
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/supp/text_misalign_gemini-2.5-pro.png",
                "caption": "(a)Gemini-2.5-Pro",
                "position": 1764
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/supp/text_misalign_gemini-2.0-Flash.png",
                "caption": "(b)Gemini-2.0-Flash",
                "position": 1770
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/supp/text_misalign_Qwen3-Omni-30B-Instruct.png",
                "caption": "(c)Qwen3-Omni-30B",
                "position": 1776
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/supp/text_misalign_Chatbridge.png",
                "caption": "(d)ChatBridge",
                "position": 1782
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/supp/long_context_gemini-2.5-pro.png",
                "caption": "(a)Gemini-2.5-Pro",
                "position": 1806
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/supp/long_context_gemini-2.5-pro.png",
                "caption": "(a)Gemini-2.5-Pro",
                "position": 1809
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/supp/long_context_gemini-2.0-Flash.png",
                "caption": "(b)Gemini-2.0-Flash",
                "position": 1815
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/supp/long_context_Qwen3-Omni-30B-Instruct.png",
                "caption": "(c)Qwen3-Omni-30B",
                "position": 1821
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/supp/long_context_Chatbridge.png",
                "caption": "(d)ChatBridge",
                "position": 1827
            }
        ]
    },
    {
        "header": "Appendix JUnimodal Abstention Evaluation: The “None of the Above” Experiment",
        "images": []
    },
    {
        "header": "Appendix KCan Reasoning Traces Fix Misalignment? (Chain-of-Thought Evaluation)",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22826/images/quality/190.png",
                "caption": "Figure 23:Qualitative Results Gallery.Red indicates incorrect baseline predictions, Green indicates our correct predictions. While the baseline consistently suffers from hallucinations driven by conflicting modalities, our model demonstrates robust grounding in the requested sensory input.",
                "position": 2049
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/quality/190.png",
                "caption": "",
                "position": 2052
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/quality/130.png",
                "caption": "",
                "position": 2056
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/quality/448.png",
                "caption": "",
                "position": 2061
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/quality/751.png",
                "caption": "",
                "position": 2065
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/quality/8.png",
                "caption": "Figure 24:Qualitative Results Gallery.Red indicates incorrect baseline predictions, Green indicates our correct predictions. While the baseline consistently suffers from hallucinations driven by conflicting modalities, our model demonstrates robust grounding in the requested sensory input.",
                "position": 2071
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/quality/8.png",
                "caption": "",
                "position": 2074
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/quality/560.png",
                "caption": "",
                "position": 2078
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/quality/293.png",
                "caption": "",
                "position": 2083
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/quality/350.png",
                "caption": "",
                "position": 2087
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/quality/65.png",
                "caption": "Figure 25:Qualitative Results Gallery.Red indicates incorrect baseline predictions, Green indicates our correct predictions. While the baseline consistently suffers from hallucinations driven by conflicting modalities, our model demonstrates robust grounding in the requested sensory input.",
                "position": 2093
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/quality/65.png",
                "caption": "",
                "position": 2096
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/quality/190.png",
                "caption": "",
                "position": 2100
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/quality/638.png",
                "caption": "",
                "position": 2105
            },
            {
                "img": "https://arxiv.org/html/2511.22826/images/quality/647.png",
                "caption": "",
                "position": 2109
            }
        ]
    },
    {
        "header": "Appendix LQualitative Analysis of Improved Modality Grounding",
        "images": []
    }
]