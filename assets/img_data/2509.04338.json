[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.04338/x1.png",
                "caption": "Figure 1:We presentFE2E, a DiT-based foundation model for monocular dense geometry prediction. Trained with limited supervision, FE2E achieves promising performance improvements in zero-shot depth and normal estimation. Bar length indicates the average ranking across all metrics from multiple datasets, where lower values are better. ‚òÖ represents the amount of training data used.",
                "position": 127
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.04338/x2.png",
                "caption": "Figure 2:FE2E Adaptation Pipeline.Thegrey backgroundshows the original editor‚Äôs workflow, while the other details FE2E: ‚ë† A pre-trained VAE encodes the logarithmically quantized depthùêù\\mathbf{d}, input imageùê±\\mathbf{x}, and normalsùêß\\mathbf{n}into latent space. ‚ë° The DiTfŒ∏f_{\\theta}learns a constant velocityùêØ\\mathbf{v}from a fixed originùê≥0y\\mathbf{z}^{y}_{0}to the target latentùê≥1y\\mathbf{z}^{y}_{1}, independent ofttor instructions. ‚ë¢ By repurposing the discarded output region, FE2E jointly predicts depth and normals without extra computation. Training loss is computed in the latent space, with final predictions decoded by VAE only at inference.",
                "position": 133
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.04338/x3.png",
                "caption": "Figure 3:Comparison between the Generative and Editing foundation models.We analyze the feature evolution at both the initial (Epoch 1) and final (Epoch 30) stages of fine-tuning, resulting in 4 groups. Each group presents: the DiT features at the input end (Block1), middle layers (Block20), output end (Block35), and the depth prediction‚Äôs AbsRel (Absolute Relative error). Visual implementation detailed in SecB.",
                "position": 198
            },
            {
                "img": "https://arxiv.org/html/2509.04338/x4.png",
                "caption": "Figure 4:Quantitative comparison of the training lossbetween Generative and Editing foundation models. The main plot details theconvergenceloss from epoch 5 to 30, while the inset displays the steepinitialloss reduction during the first 10 epochs, which occurs on a different scale.",
                "position": 206
            },
            {
                "img": "https://arxiv.org/html/2509.04338/x5.png",
                "caption": "Figure 5:Left: GT velocity field for network training. The gray dots represent different Gaussian noise (top) orzerostarting points (bottom), the red dots represent data samples.Right: Instantaneous velocityvvdetermines the tangent direction and creates errors in the cumulative path (top); The constant speed path is a straight line.",
                "position": 266
            },
            {
                "img": "https://arxiv.org/html/2509.04338/x6.png",
                "caption": "Table 1:Quantization errors at BF16 precision on Virtual KITTI dataset. Calculation details in SecC.",
                "position": 310
            },
            {
                "img": "https://arxiv.org/html/2509.04338/x6.png",
                "caption": "Figure 6:Illustration of BF16 quantization error. (b) and (d) show GT depth visualized with BF16 precision. For clarity, (a) and (c) use Maigold‚Äôs VAE regularization, mapping max/min values to 1/-1, respectively.",
                "position": 368
            },
            {
                "img": "https://arxiv.org/html/2509.04338/x7.png",
                "caption": "Figure 7:Quantitative comparison on zero-shot depth and normal estimation.The 1st row shows the input, the 2nd, 3rd rows are previous SoTA methods results, and the 4th row is ours prediction. White arrows highlight the regions we significantly improved. Zoom in for better view.",
                "position": 425
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.04338/x8.png",
                "caption": "Figure 8:Qualitative comparison on the Joint Estimation. The ‚Äòw/o Joint Estimation‚Äô shows 2 models‚Äô results.",
                "position": 890
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AExperiment Settings",
        "images": []
    },
    {
        "header": "Appendix BTraining Details of Finetune Analysis",
        "images": []
    },
    {
        "header": "Appendix CQuantization Error Calculation Details",
        "images": []
    },
    {
        "header": "Appendix DPreliminaries of Flow Matching",
        "images": []
    },
    {
        "header": "Appendix EReviews of Related Generative and Editing Models",
        "images": []
    },
    {
        "header": "Appendix FAddition Experiments Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.04338/x9.png",
                "caption": "Figure 9:Additional qualitative comparison on zero-shot affine-invariant depth estimation.FE2E achieves more accurate depth predictions, particularly in structurally complex regions. White arrows highlight these improvements.",
                "position": 2353
            },
            {
                "img": "https://arxiv.org/html/2509.04338/x10.png",
                "caption": "Figure 10:Additional qualitative comparison on zero-shot surface normal estimation.FE2E offers improved accuracy, particularly in detailed and complex regions.",
                "position": 2356
            }
        ]
    },
    {
        "header": "Appendix GLimitations and Future Work",
        "images": []
    }
]