[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11748/x1.png",
                "caption": "Figure 1:The difference between In-Context Exploration and Training Exploration.Our framework distinguishes between the exploration of the training process and in-context inference.\nIn the training phase, reinforcement learning incentivizes the model to explore and learn from diverse state distributions.\nIn contrast, during test-time inference, in-context exploration empowers the model to actively traverse and navigate states.",
                "position": 214
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Background",
        "images": []
    },
    {
        "header": "4In-Context Exploration",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11748/x2.png",
                "caption": "Figure 2:The Length Bottleneck of In-Context Exploration.(a)Capacity:Trajectory length dictates the maximum possible state coverage (Proposition4.1). (b)“Shallow Exploration Trap”:The probability of reaching deep states decays exponentially (Lemma4.2), preventing the model from utilizing this capacity.",
                "position": 504
            },
            {
                "img": "https://arxiv.org/html/2602.11748/x3.png",
                "caption": "Figure 3:The training dynamics ofCcontextC_{\\text{context}}andRContextR_{\\text{Context}}in GRPO and GSPO on Qwen3-4B-Base.\nTwo limitations are observed: (1)Shallow Exploration Trap: GRPO faces bottlenecks in extending trajectory length and performance, while GSPO shows slow length expansion.\n(2)Degrading Information Density: Both methods display degradation in ratio over time.",
                "position": 528
            }
        ]
    },
    {
        "header": "5Length-Incentivized Exploration",
        "images": []
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11748/x4.png",
                "caption": "Figure 4:CcontextC_{\\text{context}},RcontextR_{\\text{context}}, response length, and performance on the valid dataset comparing GSPO baseline and our recipe.",
                "position": 917
            },
            {
                "img": "https://arxiv.org/html/2602.11748/x5.png",
                "caption": "Figure 5:Test-time extrapolation performance. While standard baselines saturate or degrade when forced beyond their learned policy length, the Length-Incentivized Exploration recipe exhibits a superior scaling curve.",
                "position": 929
            },
            {
                "img": "https://arxiv.org/html/2602.11748/x6.png",
                "caption": "Figure 6:TrackingCcontextC_{\\text{context}},RcontextR_{\\text{context}}, response lengthLL, and valid performance during the training of Qwen3-4B.\nCompared to the GSPO baseline (Blue),\nthe LIE (Purple)\nsuccessfully breaks the “Shallow Exploration Trap” and achieves performance gains.",
                "position": 1014
            },
            {
                "img": "https://arxiv.org/html/2602.11748/x7.png",
                "caption": "Figure 7:Global Exploration Dynamics: Diversity and Entropy. Our recipe maintains significantly higher entropy and continuous growth in global state coverage compared with the baseline.",
                "position": 1034
            },
            {
                "img": "https://arxiv.org/html/2602.11748/x8.png",
                "caption": "Figure 8:Frequency analysis of cognitive behaviors.\nThe proposed recipe (+ LIE) significantly boosts all behaviors, with a notable enhancement in Backtracking.",
                "position": 1108
            },
            {
                "img": "https://arxiv.org/html/2602.11748/x9.png",
                "caption": "Figure 9:Impact of the Length-Incentivized Reward (LlenL_{\\text{len}}). Forcing the model to \"think longer\" yields performance gains. However, the accompanying drop in the distinct ratioRcontextR_{\\text{context}}reveals a tendency towards repetition.",
                "position": 1127
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Impact Statements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATheoretical Foundation",
        "images": []
    },
    {
        "header": "Appendix BExperimental Details",
        "images": []
    },
    {
        "header": "Appendix CAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11748/x10.png",
                "caption": "Figure 10:Training dynamics of maximizingCdistinctC_{\\text{distinct}}as a reward.",
                "position": 2043
            },
            {
                "img": "https://arxiv.org/html/2602.11748/x11.png",
                "caption": "Figure 11:The training dynamics of OctoThinker-3B-Base.",
                "position": 2055
            },
            {
                "img": "https://arxiv.org/html/2602.11748/x12.png",
                "caption": "Figure 12:Training Dynamics on Qwen3-4B-Base after SFT.",
                "position": 2251
            },
            {
                "img": "https://arxiv.org/html/2602.11748/x13.png",
                "caption": "Figure 13:CcontextC_{\\text{context}}andRcontextR_{\\text{context}}dynamics across 6, 10, 15-grams.",
                "position": 2263
            },
            {
                "img": "https://arxiv.org/html/2602.11748/x14.png",
                "caption": "Figure 14:Training dynamics of differentΔ​L\\Delta L",
                "position": 2307
            },
            {
                "img": "https://arxiv.org/html/2602.11748/x15.png",
                "caption": "Figure 15:Training dynamics of differentβ\\beta.",
                "position": 2398
            },
            {
                "img": "https://arxiv.org/html/2602.11748/x16.png",
                "caption": "Figure 16:Training dynamics of different reward components.",
                "position": 2433
            },
            {
                "img": "https://arxiv.org/html/2602.11748/sources/figures/reason_graph_gspo-step500-14_False.png",
                "caption": "(a)GSPO",
                "position": 2482
            },
            {
                "img": "https://arxiv.org/html/2602.11748/sources/figures/reason_graph_gspo-step500-14_False.png",
                "caption": "(a)GSPO",
                "position": 2485
            },
            {
                "img": "https://arxiv.org/html/2602.11748/sources/figures/reason_graph_gspo-skip-right-14_True.png",
                "caption": "(b)GSPO +LIE",
                "position": 2490
            }
        ]
    },
    {
        "header": "Appendix DCase Study",
        "images": []
    }
]