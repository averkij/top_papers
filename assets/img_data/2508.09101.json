[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.09101/logo/home.png",
                "caption": "",
                "position": 98
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.09101/figures/no.png",
                "caption": "Table 1:Comparison of Existing Code Generation Benchmarks and AutoCodeBench.MLing: MultiLingual;MLogi: MultiLogical, refers to programming problems that require the model to simultaneously implement multiple core functionalities.HFree: Human-Free;BDist: Balanced Distribution of multiple languages. TheDifficultyis rated based on the performance ofDeepSeek-V3-0324. The number ofCategoryis obtained using predefined labels fromDeepSeek-V3-0324. TheProblem Length is calculated usingQwen2.5-32B-Instructtokenizer.",
                "position": 116
            },
            {
                "img": "https://arxiv.org/html/2508.09101/figures/star.png",
                "caption": "",
                "position": 136
            },
            {
                "img": "https://arxiv.org/html/2508.09101/figures/yes.png",
                "caption": "",
                "position": 180
            }
        ]
    },
    {
        "header": "2AutoCodeBench: A Challenging, Practical, and Diverse Multilingual Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.09101/x1.png",
                "caption": "Figure 1:Tag and Language Distribution across our AutoCodeBench.",
                "position": 312
            },
            {
                "img": "https://arxiv.org/html/2508.09101/x1.png",
                "caption": "",
                "position": 315
            },
            {
                "img": "https://arxiv.org/html/2508.09101/x2.png",
                "caption": "",
                "position": 319
            },
            {
                "img": "https://arxiv.org/html/2508.09101/x3.png",
                "caption": "Figure 2:The overview of AutoCodeGen. It first generates code solution and the corresponding public/private test input functions based on multilingual code snippets (①). They are concatenated and executed in a sandbox to obtain test outputs, which are then combined by the LLM into complete test functions (②,③,④). Based on the code solution and test function, the LLM is prompted to generate accurate programming problems (⑤). Finally, a three-stage data filtering is applied: multiple sampling to remove too easy problems (⑥), LLM-as-Critic to discard low-quality ones (⑦), and diversity-based tagging to ensure distributional variety (⑧).",
                "position": 329
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.09101/x4.png",
                "caption": "Figure 3:The performance comparison of different models across two language sets.",
                "position": 2968
            },
            {
                "img": "https://arxiv.org/html/2508.09101/x5.png",
                "caption": "Figure 4:Performance drop of models on multi-logic problems (1,622) compared to full dataset.",
                "position": 2981
            },
            {
                "img": "https://arxiv.org/html/2508.09101/x6.png",
                "caption": "Figure 5:Scaling laws for different models.",
                "position": 2994
            },
            {
                "img": "https://arxiv.org/html/2508.09101/x6.png",
                "caption": "",
                "position": 2997
            },
            {
                "img": "https://arxiv.org/html/2508.09101/x7.png",
                "caption": "",
                "position": 3001
            },
            {
                "img": "https://arxiv.org/html/2508.09101/x8.png",
                "caption": "Figure 6:Performance improvement across multi-turn refinement with sandbox feedback.",
                "position": 3014
            },
            {
                "img": "https://arxiv.org/html/2508.09101/x9.png",
                "caption": "Figure 7:AutoCodeBench leaderboard showing Pass@1 performance of various LLMs.",
                "position": 3288
            }
        ]
    },
    {
        "header": "4Further Discussion",
        "images": []
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AData Category and Language Distribution Statistics",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.09101/x10.png",
                "caption": "(a)AutoCodeBench",
                "position": 4297
            },
            {
                "img": "https://arxiv.org/html/2508.09101/x10.png",
                "caption": "(a)AutoCodeBench",
                "position": 4300
            },
            {
                "img": "https://arxiv.org/html/2508.09101/x11.png",
                "caption": "(b)AutoCodeBench-Lite",
                "position": 4305
            },
            {
                "img": "https://arxiv.org/html/2508.09101/x12.png",
                "caption": "(c)FullStackBench",
                "position": 4310
            },
            {
                "img": "https://arxiv.org/html/2508.09101/x13.png",
                "caption": "(d)McEval",
                "position": 4315
            },
            {
                "img": "https://arxiv.org/html/2508.09101/x14.png",
                "caption": "(a)AutoCodeBench",
                "position": 4322
            },
            {
                "img": "https://arxiv.org/html/2508.09101/x14.png",
                "caption": "(a)AutoCodeBench",
                "position": 4325
            },
            {
                "img": "https://arxiv.org/html/2508.09101/x15.png",
                "caption": "(b)AutoCodeBench-Lite",
                "position": 4330
            },
            {
                "img": "https://arxiv.org/html/2508.09101/x16.png",
                "caption": "(c)FullStackBench",
                "position": 4335
            },
            {
                "img": "https://arxiv.org/html/2508.09101/x17.png",
                "caption": "(d)McEval",
                "position": 4340
            }
        ]
    },
    {
        "header": "Appendix BManual Verification",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.09101/x18.png",
                "caption": "Figure 10:The English prompt of annotation and critic.",
                "position": 4448
            },
            {
                "img": "https://arxiv.org/html/2508.09101/x19.png",
                "caption": "Figure 11:AutoCodeBench-Lite leaderboard showing Pass@1 performance of various LLMs.",
                "position": 4457
            }
        ]
    },
    {
        "header": "Appendix CMultilingual Code Sandbox Service",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.09101/x20.png",
                "caption": "Figure 12:The prompt of generating code solution. Due to the excessive length of the prompt, we have omitted the latter part.",
                "position": 4532
            },
            {
                "img": "https://arxiv.org/html/2508.09101/x21.png",
                "caption": "Figure 13:The prompt of generating test function. Due to the excessive length of the prompt, we have omitted the latter part.",
                "position": 4535
            },
            {
                "img": "https://arxiv.org/html/2508.09101/x22.png",
                "caption": "Figure 14:The prompt of generating programming problem.",
                "position": 4538
            },
            {
                "img": "https://arxiv.org/html/2508.09101/x23.png",
                "caption": "Figure 15:The prompt of translating languages.",
                "position": 4541
            }
        ]
    },
    {
        "header": "Appendix DPrompts for Automated Workflow",
        "images": []
    }
]