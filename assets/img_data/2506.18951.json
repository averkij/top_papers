[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18951/extracted/6562983/img/logo/bird_critic.png",
                "caption": "",
                "position": 174
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18951/x1.png",
                "caption": "Figure 1:Illustration of the SQL issue debugging process in BIRD-CRITIC. It should start with a user issue query (left) and issue SQL query (center-left), LLMs will produce a corrected SQL solution (right) based on reasoning and interaction with the environment.",
                "position": 187
            }
        ]
    },
    {
        "header": "2Problem Definition",
        "images": []
    },
    {
        "header": "3BIRD-CRITIC Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18951/x2.png",
                "caption": "Figure 2:Example task structure within the BIRD-CRITIC benchmark, demonstrating the transformation from a user-reported issue and error SQL to a revised SQL solution.",
                "position": 229
            },
            {
                "img": "https://arxiv.org/html/2506.18951/x3.png",
                "caption": "Figure 3:Distribution of issue categories in all BIRD-CRITIC, derived from an analysis of SQL usage in the real-world database applications. A detailed distribution is in AppendixE.",
                "position": 264
            },
            {
                "img": "https://arxiv.org/html/2506.18951/x3.png",
                "caption": "Figure 3:Distribution of issue categories in all BIRD-CRITIC, derived from an analysis of SQL usage in the real-world database applications. A detailed distribution is in AppendixE.",
                "position": 267
            }
        ]
    },
    {
        "header": "4Six-Gym: An Automated SQL Debugging Environment for LLMs",
        "images": []
    },
    {
        "header": "5Bird-Fixer: Elevating Open-Source LLMs to an SQL Issue Fixer",
        "images": []
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18951/x4.png",
                "caption": "Figure 4:LLM agent performance forBIRD-CRITIC-PG.TooActemploys constrained toolkit as actions, whileSQLActexecutes SQLs as actions.",
                "position": 638
            },
            {
                "img": "https://arxiv.org/html/2506.18951/x5.png",
                "caption": "Figure 5:Success Rate vs Query Diversity (by Issue Category). It shows a strong negative correlation (rùëüritalic_r= -0.89) between n-gram of tokens and model performance after normalization.",
                "position": 669
            },
            {
                "img": "https://arxiv.org/html/2506.18951/x6.png",
                "caption": "Figure 6:Ablation study of components inBird-Fixer.",
                "position": 776
            }
        ]
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AEnvironment Setup Details",
        "images": []
    },
    {
        "header": "Appendix BAnnotator Qualification",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18951/x7.png",
                "caption": "Figure 7:Examples of training materials by screenshots for BIRD-CRITIC annotators. Left: Docker setup instructions for creating the standardized annotation environment. Middle: Data annotation tutorials with detailed procedures for reproducing SQL issues. Right: Entry examination outline used to evaluate annotator proficiency across various SQL debugging challenges.",
                "position": 1640
            }
        ]
    },
    {
        "header": "Appendix CEvaluation Script Details",
        "images": []
    },
    {
        "header": "Appendix DEvaluation Metrics",
        "images": []
    },
    {
        "header": "Appendix EMore Statistics",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18951/extracted/6562983/img/no.png",
                "caption": "Table 5:Data statistics of features in BIRD-CRITIC compared to related benchmarks.\n[‚Ä†]: Results taken from public available Spider 2.0 Lite Gold SQL. EM refers to the Exact Match, EX refers to Execution Accuracy, and PCM-F1 refers Partial Component Match F1.",
                "position": 1795
            },
            {
                "img": "https://arxiv.org/html/2506.18951/extracted/6562983/img/yes.png",
                "caption": "",
                "position": 1835
            }
        ]
    },
    {
        "header": "Appendix FAlgorithm",
        "images": []
    },
    {
        "header": "Appendix GError Analysis Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18951/x8.png",
                "caption": "Figure 8:Detailed Error Analysis",
                "position": 2195
            }
        ]
    },
    {
        "header": "Appendix HExperiment Details",
        "images": []
    },
    {
        "header": "Appendix ILimitation And Future Work",
        "images": []
    },
    {
        "header": "Appendix JBroader Impact",
        "images": []
    },
    {
        "header": "Appendix KPrompt",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18951/x9.png",
                "caption": "Figure 9:An illustrative example of the BIRD-Fixer debugging workflow.",
                "position": 2595
            },
            {
                "img": "https://arxiv.org/html/2506.18951/x10.png",
                "caption": "",
                "position": 2599
            }
        ]
    },
    {
        "header": "Appendix LBIRD-Fixer Example",
        "images": []
    }
]