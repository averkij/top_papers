[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06776/x1.png",
                "caption": "Figure 1:Comparison of our model with baselines.After adaptation from an open-sourced AR LLM, our model has good long-sequence and reasoning capabilities and shows outstanding performance in various benchmarks.",
                "position": 78
            },
            {
                "img": "https://arxiv.org/html/2512.06776/x2.png",
                "caption": "Figure 2:The diffusion paradigm of ourNBDiff-7B-Instructmodel.We compare popular language generation paradigms. Diffusion LLMs adapted from AR adopt logit shift and attention mask growth; Block-Diffusion uses block-wise autoregressive and maintains an intra-block bidirectional mask; Our model adopts Block-Diffusion where bidirectional attention is used intra-block, but features a causal context.",
                "position": 109
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Rethinking DLM Adaptation from AR: to Where, and How?",
        "images": []
    },
    {
        "header": "4Designing Transition Paradigms",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06776/x3.png",
                "caption": "Figure 3:Our Parallel Training Diagram.The diagram shows the parallel training form of our Context-Causal setting (we usebâ€‹lâ€‹oâ€‹câ€‹kâ€‹sâ€‹iâ€‹zâ€‹e=4blocksize=4as an example; the actualbâ€‹lâ€‹oâ€‹câ€‹kâ€‹sâ€‹iâ€‹zâ€‹eblocksizeis 32). We concatenate a clean, unmasked token sequence to the noised sequence. The attention maskğŒall\\mathbf{M}_{\\mathrm{all}}is designed (shown in the right) such that strictly-causal attention is applied in the unmasked input; for the masked input, each token has bidirectional attention intra-block, but causal attention to past inter-block tokens that are unmasked. AR lossâ„’AR\\mathcal{L}_{\\mathrm{AR}}is introduced in addition to the canonical masked lossâ„’MDM\\mathcal{L}_{\\mathrm{MDM}}for faster adaptation.",
                "position": 366
            },
            {
                "img": "https://arxiv.org/html/2512.06776/x4.png",
                "caption": "Figure 4:Our Parallel Training Diagram.The diagram shows the parallel training form of our Context-Causal setting (we usebâ€‹lâ€‹oâ€‹câ€‹kâ€‹sâ€‹iâ€‹zâ€‹e=4blocksize=4as an example; the actualbâ€‹lâ€‹oâ€‹câ€‹kâ€‹sâ€‹iâ€‹zâ€‹eblocksizeis 32). We concatenate a clean, unmasked token sequence to the noised sequence. An attention mask is designed such that strictly-causal attention is applied in the unmasked input; for the masked input, each token has bidirectional attention intra-block, but causal attention to past inter-block tokens that are unmasked. AR lossâ„’AR\\mathcal{L}_{\\mathrm{AR}}is introduced in addition to the canonical masked lossâ„’MDM\\mathcal{L}_{\\mathrm{MDM}}for faster adaptation.",
                "position": 417
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Contributors",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    }
]