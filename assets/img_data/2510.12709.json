[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12709/x1.png",
                "caption": "Figure 1:SAIL-Embedding Capability Overview. We present SAIL-Embedding, an omni-modal embedding model adapted from vision-language-audio models. SAIL-Embedding is capable of following instructions and performing various omni-modal embedding tasks, such as Motivation/Tag-CLS (Classification), Search-q2i (Query-to-Item Retrieval), and Copair/Live-i2i (Item-to-Item Retrieval).",
                "position": 100
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12709/x2.png",
                "caption": "Figure 2:Embedding Architecture Comparison. (a) CLIP-like dual-tower embedding model architecture. (b) LLM/MLLM-based embedding model architecture. (c) SAIL-Embedding model architecture. In contrast, our model accommodates arbitrary modality inputs and can handle diverse downstream tasks. I, V, T, and O represent image, video, text, and omni-modality, respectively.",
                "position": 129
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12709/x3.png",
                "caption": "Figure 3:Illustration of Adaptive Multi-Source Data Balancing.For both training and validation datasets, we compute embeddings for different modalities and cluster them. We then construct a similarity matrix to represent the distance between training and validation clusters, which is further aggregated into a single similarity score. Based on these similarity values, each training dataset is assigned a weight using a Softmax function.",
                "position": 435
            },
            {
                "img": "https://arxiv.org/html/2510.12709/x4.png",
                "caption": "Figure 4:Overview of Our SAIL-Embedding Model.For each sample, we extract relevant textual information (e.g., titles, OCR text) and combine it with instructions to obtain textual tokens. Visual and audio information are encoded using dedicated encoders, and the resulting multimodal tokens are concatenated and passed through a fusion module to obtain a unified multimodal embedding. The system is trained by contrasting embeddings from query and target samples.",
                "position": 460
            },
            {
                "img": "https://arxiv.org/html/2510.12709/x5.png",
                "caption": "Figure 5:Illustration of instructions for different tasks.We design task-specific instructions by explicitly defining the task and its objective. Audio, image, and text tokens are then provided jointly to obtain the final results. For query and target tokens, modality-specific adaptations are applied to accommodate their respective modality combinations.",
                "position": 488
            },
            {
                "img": "https://arxiv.org/html/2510.12709/x6.png",
                "caption": "Figure 6:Illustration of Training Techniques. (a) Our progressive training framework gradually shifts from larger, diverse datasets to smaller, domain-specific datasets, balancing general-world knowledge with downstream specialization while maintaining training stability. (b) The conventional training method mixes heterogeneous data into a single batch. (c) Our stochastic specialization training randomly selects a dataset at each iteration to enhance robustness and specialization.",
                "position": 604
            },
            {
                "img": "https://arxiv.org/html/2510.12709/x7.png",
                "caption": "Figure 7:Recommendation Enhancement Training. We implement the (a) sequence-to-item distillation and (b) ID-to-item distillation to enhance the SAIL-Embeddingâ€™s collaboration-aware capabilities.",
                "position": 708
            }
        ]
    },
    {
        "header": "4Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12709/x8.png",
                "caption": "Figure 8:Illustration of the Results with Collaboration-aware Recommendation Enhancement (CRE).(a) Loss curve of the model fine-tuned with distilled ID embeddings. (b) Distribution of similarity scores for positive and negative samples in the original SAIL-Embedding. (c) Distribution of similarity scores after applying CRE.",
                "position": 1453
            },
            {
                "img": "https://arxiv.org/html/2510.12709/x9.png",
                "caption": "Figure 9:Ablation Studies on i2i Tasks. We systematically report on the effects of different modules and strategies on performance.",
                "position": 1661
            },
            {
                "img": "https://arxiv.org/html/2510.12709/x10.png",
                "caption": "Figure 10:Ablation Studies on q2i Tasks. We systematically report on the effects of different modules and strategies on performance.",
                "position": 1665
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Contributor List",
        "images": []
    },
    {
        "header": "7Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]