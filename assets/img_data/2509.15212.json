[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15212/x1.png",
                "caption": "Figure 1:Training data pipeline of RynnVLA-001.Our framework leverages three types of training data: (1) Ego-Centric Video Generative Pretraining uses millions of ego-centric human manipulation videos for future frame prediction. (2) Human-Centric Trajectory-Aware Video Modeling trains on videos with human keypoint annotations, enabling joint prediction of frames and trajectories. (3) Robot-Centric Vision-Language-Action Modeling employs robot datasets paired with language instructions to learn mappings from visual observations and language to robotic actions.",
                "position": 158
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15212/x2.png",
                "caption": "Figure 2:Model architecture and training stages of RynnVLA-001.The training consists of three stages: (1) Ego-Centric Video Generative Pretraining trains a transformer-based Image-to-Video (I2V) model for future frame prediction. (2) Human-Centric Trajectory-Aware Video Modeling extends the I2V model with action (trajectory) prediction heads, incorporating both visual and state embeddings (blue blocks). (3) Robot-Centric Vision-Language-Action Modeling transfers pretrained weights to robot data, where the model generates action embeddings decoded by ActionVAE into executable actions.",
                "position": 213
            }
        ]
    },
    {
        "header": "4Ego-Centric Video Data Curation Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15212/x3.png",
                "caption": "Figure 3:Illustration of Evaluation Tasks.We evaluate the performance of VLA models on three tasks: (1) pick up and place green blocks, (2) pick up and place strawberries, and (3) grab pen and put it into holder. Each task is evaluated under three settings: single-target manipulation, multi-target manipulation (first three images), and instruction-following with distractors (rightmost image).",
                "position": 346
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15212/x4.png",
                "caption": "Figure 4:Visualization of Video Generative Pretraining.Given an input image and a text prompt, an I2V model is trained to predict the next 7 frames. Our pretrained video generation model is capable of generating plausible motions while maintaining the consistency with the input image.",
                "position": 692
            },
            {
                "img": "https://arxiv.org/html/2509.15212/x5.png",
                "caption": "Figure 5:Analysis on the front camera’s function for coarse localization.(a) Under normal dual-camera settings, the robot successfully picks the strawberries. (b) The front camera is masked, leaving only the wrist camera functional. (c) The robot can still complete the task if the target is within the wrist camera’s initial field of view. However, task success rate drops from 80% (4/5) to 0% when the target is outside the wrist camera’s view (on the left side), demonstrating that the front camera is essential for guiding the robot to the target’s coarse location.",
                "position": 708
            },
            {
                "img": "https://arxiv.org/html/2509.15212/x6.png",
                "caption": "Figure 6:The front camera provides critical 3D projective information for precise manipulation.(a) With the standard camera configuration, the robot successfully inserts the pen into the holder. (b) When the front camera is elevated, the altered projective geometry of the scene causes the model to fail the task. This highlights the model’s reliance on the specific 3D perspective provided by the front camera for spatial reasoning.",
                "position": 711
            }
        ]
    },
    {
        "header": "6Discussion and Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]