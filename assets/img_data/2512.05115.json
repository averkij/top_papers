[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05115/x1.png",
                "caption": "",
                "position": 117
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05115/x2.png",
                "caption": "Figure 2:Overview ofLight-X.\nGiven an input videoùëΩs\\bm{V}^{s}, we first relight one frame with(iclight), conditioned on a lighting text prompt, to obtain a sparse relit videoùëΩ^s\\hat{\\bm{V}}^{s}.\nWe then estimate depths to construct a dynamic point cloudùí´\\mathcal{P}fromùëΩs\\bm{V}^{s}and a relit point cloudùí´^\\hat{\\mathcal{P}}fromùëΩ^s\\hat{\\bm{V}}^{s}.\nBoth point clouds are projected along a user-specified camera trajectory, producing geometry-aligned renders and masks(ùëΩp,ùëΩm)(\\bm{V}^{p},\\bm{V}^{m})and(ùëΩ^p,ùëΩ^m)(\\hat{\\bm{V}}^{p},\\hat{\\bm{V}}^{m}).\nThese six cues, together with illumination tokens extracted via a Q-Former, are fed into DiT blocks for conditional denoising.\nFinally, a VAE decoder reconstructs a high-fidelity videoùëΩt\\bm{V}^{t}faithful to the target trajectory and illumination.",
                "position": 190
            },
            {
                "img": "https://arxiv.org/html/2512.05115/x3.png",
                "caption": "Figure 3:Overview ofLight-Syn. From an in-the-wild videoVtV^{t}, we generate a degradedVsV^{s}and derive renders, masks(Vp,Vm)(V^{p},V^{m}), and relit counterparts(V^p,V^m)(\\hat{V}^{p},\\hat{V}^{m})via inverse transformations.",
                "position": 271
            },
            {
                "img": "https://arxiv.org/html/2512.05115/x4.png",
                "caption": "Figure 4:Left: Data curation pipelines for reference-image and HDR-map conditioned video generation.\nRight: Conditioning cues with soft masks used for model training.",
                "position": 320
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05115/x5.png",
                "caption": "Figure 5:Qualitative comparison for camera-illumination control with light prompts‚Äúneon light‚Äù(left) and‚Äúsunlight‚Äù(right). Our method outperforms baselines in relighting quality, temporal consistency, and novel-view content generation. Refer to the supplementary video for clearer comparisons.",
                "position": 387
            },
            {
                "img": "https://arxiv.org/html/2512.05115/x6.png",
                "caption": "Figure 6:Qualitative comparison for video relighting with light prompts‚Äúneon light‚Äù(left) and‚Äúsunlight‚Äù(right).\nOur method outperforms baseline methods in both relighting quality and temporal consistency.\nPlease refer to the supplementary video for clearer comparisons.",
                "position": 611
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix AMore Implementation Details",
        "images": []
    },
    {
        "header": "Appendix BDetailed Data Curation",
        "images": []
    },
    {
        "header": "Appendix CPreliminary: Video Diffusion Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05115/x7.png",
                "caption": "Figure A:Overview of background-conditioned video relighting.\nA foreground video is fused with a background video to form the source video,\nwhile IC-Light generates a sparse relit video. Both are fed into our model\nto produce the final relit video with consistent illumination and motion.",
                "position": 1109
            },
            {
                "img": "https://arxiv.org/html/2512.05115/x7.png",
                "caption": "Figure A:Overview of background-conditioned video relighting.\nA foreground video is fused with a background video to form the source video,\nwhile IC-Light generates a sparse relit video. Both are fed into our model\nto produce the final relit video with consistent illumination and motion.",
                "position": 1112
            },
            {
                "img": "https://arxiv.org/html/2512.05115/x8.png",
                "caption": "Figure B:The web interface of our user studies. Participants were shown the input video,\nthe relighting prompt (text or background image), and results of two methods (Method¬†1\nand Method¬†2) side by side. They evaluated each pair across four criteria by selecting\nthe better method.",
                "position": 1120
            },
            {
                "img": "https://arxiv.org/html/2512.05115/x9.png",
                "caption": "Figure C:Detailed Data curation pipelineLight-Syn. Given an original in-the-wild videoùëΩt\\bm{V}^{t}, we synthesize a degraded counterpartùëΩs\\bm{V}^{s}using different strategies for static, dynamic, and AI-generated scenes.\nFromùëΩs\\bm{V}^{s}, we obtain geometry-aligned renders and masks(ùëΩp,ùëΩm)(\\bm{V}^{p},\\bm{V}^{m})and relit counterparts(ùëΩ^p,ùëΩ^m)(\\hat{\\bm{V}}^{p},\\hat{\\bm{V}}^{m})via inverse geometric transformations.\nThe curated data provide paired videos for training, with Light-X taking the degraded video as input, the other signals as conditions, and the original video as ground truth.",
                "position": 1129
            },
            {
                "img": "https://arxiv.org/html/2512.05115/x10.png",
                "caption": "Figure D:Qualitative ablation results for the joint camera-illumination control task.",
                "position": 1135
            }
        ]
    },
    {
        "header": "Appendix DMore Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05115/x11.png",
                "caption": "Figure E:Qualitative comparison of background image-conditioned video relighting.\nOur method achieves superior both relighting quality and temporal consistency compared to baseline methods.",
                "position": 1298
            },
            {
                "img": "https://arxiv.org/html/2512.05115/x12.png",
                "caption": "Figure F:Visual results of background image-conditioned video relighting.\nOur method adapts the foreground subject to diverse background images,\nproducing natural illumination and consistent appearance across frames.",
                "position": 1304
            },
            {
                "img": "https://arxiv.org/html/2512.05115/x13.png",
                "caption": "Figure G:Results on non-Lambertian surfaces.",
                "position": 1453
            },
            {
                "img": "https://arxiv.org/html/2512.05115/x14.png",
                "caption": "Figure H:Qualitative results under large camera trajectories.",
                "position": 1465
            },
            {
                "img": "https://arxiv.org/html/2512.05115/x15.png",
                "caption": "Figure I:Qualitative results showing robustness under occluded reference frames. Light-X maintains coherent illumination propagation even when the reference frame contains partial occlusions.",
                "position": 1644
            },
            {
                "img": "https://arxiv.org/html/2512.05115/x16.png",
                "caption": "Figure J:FID variation as a function of temporal distance from the reference frame (the first frame).\nThe left part corresponds to the video relighting setting, while the right part corresponds to the joint camera-illumination control setting.",
                "position": 1649
            },
            {
                "img": "https://arxiv.org/html/2512.05115/x17.png",
                "caption": "Figure K:Qualitative results under different depth noise levels. Light-X maintains coherent illumination and motion consistency even when depth maps are perturbed with moderate Gaussian noise.",
                "position": 1678
            },
            {
                "img": "https://arxiv.org/html/2512.05115/x18.png",
                "caption": "Figure L:Examples of synthetic relighting data generated using graphics engines. These samples exhibit controlled illumination and viewpoint variations.",
                "position": 1904
            },
            {
                "img": "https://arxiv.org/html/2512.05115/x19.png",
                "caption": "Figure M:Qualitative ablation of theAI-generated data.",
                "position": 1914
            },
            {
                "img": "https://arxiv.org/html/2512.05115/x20.png",
                "caption": "Figure N:Qualitative ablation of theStatic data.",
                "position": 1919
            },
            {
                "img": "https://arxiv.org/html/2512.05115/x21.png",
                "caption": "Figure O:Qualitative ablation of theDynamic data.",
                "position": 1924
            },
            {
                "img": "https://arxiv.org/html/2512.05115/x22.png",
                "caption": "Figure P:Qualitative ablation of theglobal illumination control.",
                "position": 1929
            },
            {
                "img": "https://arxiv.org/html/2512.05115/x23.png",
                "caption": "Figure Q:Qualitative comparison of joint camera-illumination control.\nOur method achieves superior relighting quality, temporal consistency,\nand realistic novel-view content generation compared to baseline methods.",
                "position": 1950
            },
            {
                "img": "https://arxiv.org/html/2512.05115/x24.png",
                "caption": "Figure R:Qualitative comparison of text-conditioned video relighting.\nOur method achieves superior both relighting quality and temporal consistency compared to baseline methods.",
                "position": 1957
            },
            {
                "img": "https://arxiv.org/html/2512.05115/x25.png",
                "caption": "Figure S:Qualitative results of HDR map-conditioned video relighting.\nGiven an input video and an HDR environment map, our model generates a relit\nvideo.",
                "position": 1963
            },
            {
                "img": "https://arxiv.org/html/2512.05115/x26.png",
                "caption": "Figure T:Qualitative results of reference image-conditioned video relighting.\nHere, a reference image provides the target illumination style, which is transferred\nto the input video while preserving its content and motion.",
                "position": 1970
            },
            {
                "img": "https://arxiv.org/html/2512.05115/x27.png",
                "caption": "Figure U:Qualitative results of reference image-conditioned joint camera trajectory and illumination control. Here, a reference image provides the target illumination style, which is transferred\nto the input video while preserving its content and motion.",
                "position": 1977
            }
        ]
    },
    {
        "header": "Appendix EAdditional Ablation Analyses",
        "images": []
    }
]