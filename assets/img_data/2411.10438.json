[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.10438/x1.png",
                "caption": "(a)Training Loss",
                "position": 1053
            },
            {
                "img": "https://arxiv.org/html/2411.10438/x1.png",
                "caption": "(a)Training Loss",
                "position": 1056
            },
            {
                "img": "https://arxiv.org/html/2411.10438/x2.png",
                "caption": "(b)Validation Loss",
                "position": 1061
            },
            {
                "img": "https://arxiv.org/html/2411.10438/x3.png",
                "caption": "(c)Wall-clock time",
                "position": 1066
            },
            {
                "img": "https://arxiv.org/html/2411.10438/x4.png",
                "caption": "(a)Training Loss",
                "position": 1073
            },
            {
                "img": "https://arxiv.org/html/2411.10438/x4.png",
                "caption": "(a)Training Loss",
                "position": 1076
            },
            {
                "img": "https://arxiv.org/html/2411.10438/x5.png",
                "caption": "(b)Validation Loss",
                "position": 1081
            },
            {
                "img": "https://arxiv.org/html/2411.10438/x6.png",
                "caption": "(c)Wall-clock time",
                "position": 1086
            },
            {
                "img": "https://arxiv.org/html/2411.10438/x7.png",
                "caption": "(a)Training Loss",
                "position": 1093
            },
            {
                "img": "https://arxiv.org/html/2411.10438/x7.png",
                "caption": "(a)Training Loss",
                "position": 1096
            },
            {
                "img": "https://arxiv.org/html/2411.10438/x8.png",
                "caption": "(b)Validation Loss",
                "position": 1101
            },
            {
                "img": "https://arxiv.org/html/2411.10438/x9.png",
                "caption": "(c)Wall-clock time",
                "position": 1106
            },
            {
                "img": "https://arxiv.org/html/2411.10438/x10.png",
                "caption": "Figure 4:The accuracy for 0-shot evalution on Hellaswag(Zellers et al.,2019)dataset forAdamW,MuonandMARSon GPT-2 medium (355M) and large (770M) models.",
                "position": 1113
            },
            {
                "img": "https://arxiv.org/html/2411.10438/x11.png",
                "caption": "Figure 5:Validation loss curves forMARSandMARS-approxon GPT-2 small (125M, left) and medium (355M, right).",
                "position": 1140
            },
            {
                "img": "https://arxiv.org/html/2411.10438/x12.png",
                "caption": "",
                "position": 1143
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProofs of Lemma3.4",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.10438/x13.png",
                "caption": "Figure 6:Validation loss with respect to training tokens forAdamWwith learning rates6×10−46superscript1046\\times 10^{-4}6 × 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT,3×10−33superscript1033\\times 10^{-3}3 × 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPTandMARSwith learning rate3×10−33superscript1033\\times 10^{-3}3 × 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPTon GPT-2 small model (125M). The model trained withMARSachieves a validation loss of 2.852.",
                "position": 2243
            },
            {
                "img": "https://arxiv.org/html/2411.10438/x14.png",
                "caption": "(a)Training Loss",
                "position": 2254
            },
            {
                "img": "https://arxiv.org/html/2411.10438/x14.png",
                "caption": "(a)Training Loss",
                "position": 2257
            },
            {
                "img": "https://arxiv.org/html/2411.10438/x15.png",
                "caption": "(b)Validation Loss",
                "position": 2262
            }
        ]
    },
    {
        "header": "Appendix CHyper-parameter Choices",
        "images": []
    }
]