[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23764/x1.png",
                "caption": "Figure 1:The MMSI-Bench benchmark. Left and middle:Human experts carefully select sets of images that enable multi-image spatial reasoning to answer unambiguous and challenging spatial questions; human-annotated reasoning processes further facilitate automated error analysis.Right:Comparison of models and human performance on MMSI-Bench, highlighting the benchmarkâ€™s difficulty and the substantial gap between current MLLMs and human-level spatial intelligence.",
                "position": 99
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3MMSI-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23764/x2.png",
                "caption": "Figure 2:Representative MMSI-Bench samples from each category. Please zoom in to inspect image details. Questions and rationales are simplified for brevity; the complete versions appear in the supplementary material. Correct answers are highlighted in green.",
                "position": 142
            },
            {
                "img": "https://arxiv.org/html/2505.23764/x3.png",
                "caption": "Figure 3:Illustration of the MMSI-Bench construction pipeline: images are collected from diverse real-world datasets, relevant image sets are carefully selected, complex QA tasks and detailed reasoning processes are manually annotated, and all data undergo rigorous quality control.",
                "position": 225
            },
            {
                "img": "https://arxiv.org/html/2505.23764/x4.png",
                "caption": "Figure 4:Distribution of categories in MMSI-Bench.",
                "position": 228
            }
        ]
    },
    {
        "header": "4Evaluation on MMSI",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23764/extracted/6493893/images/prompt_effect.png",
                "caption": "Figure 5:Impact of linguistic and visual prompting on MMSI-Bench.",
                "position": 2472
            }
        ]
    },
    {
        "header": "5Error Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23764/x5.png",
                "caption": "Figure 6:Illustration of four error types identified in MLLM spatial reasoning on MMSI-Bench.",
                "position": 2489
            },
            {
                "img": "https://arxiv.org/html/2505.23764/extracted/6493893/images/error_three_pie.png",
                "caption": "Figure 7:Distribution of correct and error types across three representative MLLMs.",
                "position": 2514
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix AAppendix Overview and Organization",
        "images": []
    },
    {
        "header": "Appendix BLimitations, Social Impact, and License & Access",
        "images": []
    },
    {
        "header": "Appendix CComparison with Other Spatial Question-Answering Benchmarks",
        "images": []
    },
    {
        "header": "Appendix DAdditional Implementation Details and Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23764/extracted/6493893/images/0.jpg",
                "caption": "Figure 8:Illustration of the visual prompt utilized in our experiments.",
                "position": 2834
            },
            {
                "img": "https://arxiv.org/html/2505.23764/extracted/6493893/images/1.jpg",
                "caption": "Figure 9:Illustration of the visual prompt utilized in our experiments.",
                "position": 2837
            },
            {
                "img": "https://arxiv.org/html/2505.23764/extracted/6493893/images/10.jpg",
                "caption": "Figure 10:Illustration of the visual prompt utilized in our experiments.",
                "position": 2840
            },
            {
                "img": "https://arxiv.org/html/2505.23764/extracted/6493893/images/100.jpg",
                "caption": "Figure 11:Illustration of the visual prompt utilized in our experiments.",
                "position": 2843
            }
        ]
    },
    {
        "header": "Appendix EDetails of the Data Curation Process",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23764/extracted/6493893/images/anno_ui.jpg",
                "caption": "Figure 12:Screenshot of the data annotation interface, where annotators select images, design questions, and provide answers and reasoning.",
                "position": 3941
            },
            {
                "img": "https://arxiv.org/html/2505.23764/extracted/6493893/images/check_ui.jpg",
                "caption": "Figure 13:Screenshot of the quality control interface, where reviewers decide whether to retain or discard data and specify the reasons for data issues.",
                "position": 3944
            }
        ]
    },
    {
        "header": "Appendix FRepresentative MMSI-Bench Samples from Each Category (Full Version)",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23764/x6.png",
                "caption": "Figure 14:Full example for Position (Camera-Camera) category.",
                "position": 4148
            },
            {
                "img": "https://arxiv.org/html/2505.23764/x7.png",
                "caption": "Figure 15:Full example for Position (Camera-Object) category.",
                "position": 4151
            },
            {
                "img": "https://arxiv.org/html/2505.23764/x8.png",
                "caption": "Figure 16:Full example for Position (Camera-Region) category.",
                "position": 4154
            },
            {
                "img": "https://arxiv.org/html/2505.23764/x9.png",
                "caption": "Figure 17:Full example for Position (Object-Object) category.",
                "position": 4157
            },
            {
                "img": "https://arxiv.org/html/2505.23764/x10.png",
                "caption": "Figure 18:Full example for Position (Object-Region) category.",
                "position": 4160
            },
            {
                "img": "https://arxiv.org/html/2505.23764/x11.png",
                "caption": "Figure 19:Full example for Position (Region-Region) category.",
                "position": 4163
            },
            {
                "img": "https://arxiv.org/html/2505.23764/x12.png",
                "caption": "Figure 20:Full example for Motion (Camera) category.",
                "position": 4166
            },
            {
                "img": "https://arxiv.org/html/2505.23764/x13.png",
                "caption": "Figure 21:Full example for Motion (Object) category.",
                "position": 4169
            },
            {
                "img": "https://arxiv.org/html/2505.23764/x14.png",
                "caption": "Figure 22:Full example for Attribute (Measurement) category.",
                "position": 4172
            },
            {
                "img": "https://arxiv.org/html/2505.23764/x15.png",
                "caption": "Figure 23:Full example for Attribute (Appearance) category.",
                "position": 4175
            },
            {
                "img": "https://arxiv.org/html/2505.23764/x16.png",
                "caption": "Figure 24:Full example for Multi-Step Reasoning category.",
                "position": 4178
            }
        ]
    },
    {
        "header": "Appendix GAdditional Case Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23764/x17.png",
                "caption": "Figure 25:Illustration of situation-transformation reasoning errors (highlighted in orange).",
                "position": 4188
            },
            {
                "img": "https://arxiv.org/html/2505.23764/x18.png",
                "caption": "Figure 26:Illustration of overlap-matching and scene-reconstruction errors (highlighted in red), spatial-logic errors (highlighted in green).",
                "position": 4191
            },
            {
                "img": "https://arxiv.org/html/2505.23764/x19.png",
                "caption": "Figure 27:Illustration of grounding errors (highlighted in blue), spatial-logic errors (highlighted in green).",
                "position": 4194
            },
            {
                "img": "https://arxiv.org/html/2505.23764/x20.png",
                "caption": "Figure 28:Illustration of spatial-logic errors (highlighted in green).",
                "position": 4197
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]