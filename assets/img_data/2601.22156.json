[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22156/x1.png",
                "caption": "Figure 1:Left & center: the performance-efficiency tradeoff of our model,HypeNet, versus theQwen3series, measured with 128K context length and BFloat16 precision. Right: the time per output token of the 1.7B models across different context lengths. For 1M context length, the Qwen3 model runs out of GPU memory. HypeNet is converted from Qwen3 using our distillation procedure, HALO, and has better performance-efficiency tradeoff than Qwen3.",
                "position": 226
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22156/x2.png",
                "caption": "Figure 2:Various pipelines for converting Transformer models into hybrid models. The boxes with dotted lines represent training-free stages, while those with solid lines represent training stages. HALO is much more data-efficient than prior methods.",
                "position": 338
            }
        ]
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4HALO: An Efficient Pipeline to Distill Transformers into Hybrids",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22156/x3.png",
                "caption": "Figure 3:Illustration of HypeNet. The architectural modifications introduced during HALO are marked with ➊, ➋, ➌, and ➍.Red dotted linesindicate components that are removed during HALO, black dotted lines indicate components that are added.",
                "position": 485
            }
        ]
    },
    {
        "header": "5HypeNet: An Effective Attention-RNN Hybrid Architecture",
        "images": []
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22156/x4.png",
                "caption": "Figure 4:NIAH scores of HypeNet variants based on different position encodings, as a function of context length. The models are trained from scratch with 20B tokens and 500M parameters.",
                "position": 830
            },
            {
                "img": "https://arxiv.org/html/2601.22156/x5.png",
                "caption": "Figure 5:NIAH scores of HypeNet variants based on different RNN mixers, as a function of context length. The models are trained from scratch with 20B tokens and 500M parameters.",
                "position": 833
            },
            {
                "img": "https://arxiv.org/html/2601.22156/x6.png",
                "caption": "Figure 6:The prefilling time of HypeNet versus Qwen3-1.7B, across different context lengths.",
                "position": 1097
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AComplete Formulation of HypeNet",
        "images": []
    },
    {
        "header": "Appendix BHALO Training Configurations",
        "images": []
    },
    {
        "header": "Appendix CHypeNet Model Configurations",
        "images": []
    },
    {
        "header": "Appendix DAddition Notes on the Model Architecture",
        "images": []
    },
    {
        "header": "Appendix EComputational Cost of Each Stage",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22156/x7.png",
                "caption": "Figure 7:The inference prefilling time of various mixers as a function of context lengths, measured on one A800-80GB GPU using BFLoat16. The sliding window mixers are implemented with Flash-Attention-2, Mamba2 is implemented with its officialmamba_ssmlibrary, and all other RNN mixers are taken from the widely used Flash-Linear-Attention777https://www.github.com/fla-org/flash-linear-attention. Mamba2 ran out of CUDA Memory on 256K context length. The y-axis is on log scale.",
                "position": 2291
            }
        ]
    },
    {
        "header": "Appendix FMore Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22156/x8.png",
                "caption": "Figure 8:Results for validating attention logits scaling (see Eq.11). The plot shows the NIAH performance of HypeNet without attention logits scaling, HypeNet with constant scaling (which is common in RoPE-based length extrapolation methods), and HypeNet with the attention logits scaling defined in Eq.11.",
                "position": 2370
            }
        ]
    },
    {
        "header": "Appendix GWhich RNN Mixers are Compatible with HypeNet?",
        "images": []
    },
    {
        "header": "Appendix HTraining and Model Configurations for Training From Scratch Experiments",
        "images": []
    }
]