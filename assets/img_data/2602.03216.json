[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03216/x1.png",
                "caption": "Figure 1:Speedups with Token Sparse Attention.Attention acceleration ratios obtained by applying the proposed Token Sparse Attention (ours) to existing attention acceleration methods.τ\\taudenotes the sparsity level (i.e., higher the sparser).",
                "position": 82
            },
            {
                "img": "https://arxiv.org/html/2602.03216/x2.png",
                "caption": "Figure 2:Dynamics of Token Importance.(a)Layer-wise overlap of top-k important tokens, showing that token importance shifts significantly across layers.(b)Head-wise token importance rankings within the same layer, illustrating that different attention heads prioritize different subsets of tokens.",
                "position": 87
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03216/x3.png",
                "caption": "Figure 3:Overview of the proposed Token Sparse Attention.Stage 1 compressesQQ,KK, andVVby selecting a per-head token subsetSH=0S_{H=0}, yielding compactQ^\\hat{Q},K^\\hat{K}, andV^\\hat{V}that remain compatible with standard attention kernels. Stage 2 performs attention on the compressed tensors and scatters the resulting outputs back into the full sequence layout before adding the residual connection.",
                "position": 114
            },
            {
                "img": "https://arxiv.org/html/2602.03216/x4.png",
                "caption": "Figure 4:Sparse Layer Selection.(a)Layer-wise normalized drift measured across different tasks and context lengths. Task A and B correspond to a retrieval task and a summarization task, respectively.(b)Relationship between accuracy and drift under random 3-layer sparsification over 200 runs.",
                "position": 231
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03216/x5.png",
                "caption": "Figure 5:Accuracy-Speedup Trade-offs.(a)Accuracy-speedup Pareto frontier obtained by sweeping FlexPrefill hyperparameters, comparing with the Token Sparse Attention at token coverage ofτ\\tau=0.005.(b)Accuracy-speedup trade-off achieved by applying Token Sparse Attention with varying token coverage to FlashAttention (left) and FlexPrefill (right).",
                "position": 651
            },
            {
                "img": "https://arxiv.org/html/2602.03216/x6.png",
                "caption": "Figure 6:Attention Speedup Details.(a)Attention speedup across different context lengths.(b)Attention latency breakdown with Token Sparse Attention at 128K context, where the overhead includes token scoring and indexing, as well as QKV compression and attention output decompression.",
                "position": 674
            }
        ]
    },
    {
        "header": "4Related Works",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03216/images/appendix_needle.png",
                "caption": "Figure 7:Attention latency breakdown with Token Sparse Attention at 128K context, where the overhead includes token scoring and indexing, as well as QKV compression and attention output decompression.",
                "position": 1435
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]