[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04403/x1.png",
                "caption": "Figure 1:Predicting MMLU accuracy with our method.\nWe first use model size and data size to predict a ‚Äútask loss‚Äù on MMLU (step 1), and then use this task loss to predict task accuracy in ranked classification format (step 2).\nThe chained plot shows end-to-end prediction from (model size, data size) to task accuracy.\nThe functions in step 1 and 2 are fitted on data points collected fromladdermodels (markers colored in red, orange, green and cyan); 7B-4T and 13B-5T are the target models which we make predictions for.\nWe report relative prediction error in the plot next to the target model point.",
                "position": 167
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Setup",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04403/x2.png",
                "caption": "Figure 2:Predicting the task accuracy for the target models.\n‚óº = 1xC; ‚úö = 2xC;\\pentagofill\\pentagofill\\pentagofill= 5xC;‚òÖ‚òÖ\\bigstar‚òÖ= 10xC.\nWe report the average relative fitting error in parentheses following the task name, and prediction error in the plot next to the target model point.",
                "position": 651
            },
            {
                "img": "https://arxiv.org/html/2412.04403/x3.png",
                "caption": "Figure 3:Task loss vs training scale(N,D)ùëÅùê∑(N,D)( italic_N , italic_D ), with fitting on the power function inEquation¬†1.\n‚óº = 1xC; ‚úö = 2xC;\\pentagofill\\pentagofill\\pentagofill= 5xC;‚òÖ‚òÖ\\bigstar‚òÖ= 10xC.\nWe report the average relative fitting error in parentheses following the task name, and prediction error in the plot next to the target model point.\nIn general, the power function can be well fitted to data points from the ladder models, and the fitted function can make rather reliable predictions for the targer models.\nSeeTable¬†7for the parameters of the fitted functions.",
                "position": 790
            },
            {
                "img": "https://arxiv.org/html/2412.04403/x4.png",
                "caption": "Figure 4:Task RC accuracy vs task loss, with fitting on the sigmoid function inEquation¬†2.\nWe report the average relative fitting error in parentheses following the task name, and prediction error in the plot next to the target model point.\nFor each task, the data points collected from ladder models lie on a shared sigmoidal curve, and the fitted function can make rather reliable predictions for the target models. The shaded area represents the prediction intervals for the fitted function.\nSeeTable¬†7for the parameters of the fitted functions.",
                "position": 809
            }
        ]
    },
    {
        "header": "5Analyses",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04403/x5.png",
                "caption": "Figure 5:Intermediate checkpoints and relative standard deviation over the final 10 checkpoints (SD10) for the 1B-10xC ladder model on MMLU and OpenBookQA.\nThe intermediate checkpoints for OBQA appear noisier than MMLU, resulting in a higher SD10.\nWe find that tasks where ladder models exhibit high intermediate checkpoint noise indicate higher prediction error (Table¬†4).\nResults across all tasks are inFigure¬†9.",
                "position": 875
            },
            {
                "img": "https://arxiv.org/html/2412.04403/x6.png",
                "caption": "Figure 6:Prediction error on the 7B-4T target model as a function of the total compute-FLOPs used in the ladder models for prediction. The left-most point uses only the smallest compute model (190M-1xC) for prediction. The right-most point uses the full ladder (all 16 models). We observe that the prediction error generally reduces as the ladder FLOPs increase. ARC-C, ARC-E, and OBQA display higher variation (which can be attributed to the variation analysis in ¬ß5.1), but still have a downward trend.",
                "position": 1375
            },
            {
                "img": "https://arxiv.org/html/2412.04403/x7.png",
                "caption": "Figure 7:Prediction error on the 7B-4T target model when includingup tomodel size (NùëÅNitalic_N) in the ladder for prediction. Eg.NùëÅNitalic_Nup to 760M will include 190M, 370M, 760M models trained to 1xC, 2xC, 5xC, 10xC. For most tasks, we observe a downward trend in the prediction error asNùëÅNitalic_Nincreases.",
                "position": 1385
            },
            {
                "img": "https://arxiv.org/html/2412.04403/x8.png",
                "caption": "Figure 8:Prediction error on the 7B-4T target model when including models trainedup tochinchilla multiplier (x‚Å¢Cùë•ùê∂xCitalic_x italic_C). Eg.x‚Å¢Cùë•ùê∂xCitalic_x italic_Cup to 2xC will include 190M, 370M, 760M, 1B models trained to 1xC, 2xC. The downward trend of the prediction error is less significant than with varyingNùëÅNitalic_N.",
                "position": 1396
            }
        ]
    },
    {
        "header": "6Ablating Design Choices",
        "images": []
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AFitted Parameters",
        "images": []
    },
    {
        "header": "Appendix BAdditional Analyses",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04403/x9.png",
                "caption": "Figure 9:Intermediate checkpoints and standard deviation over the final 10 checkpoints (SD10) for the 1B-10xC ladder model.\nWe find some tasks exhibit high noise between adjacent training checkpoints, which is indicative of the inherent difficulty in predicting performance for such tasks using the model ladder.",
                "position": 2332
            },
            {
                "img": "https://arxiv.org/html/2412.04403/extracted/6046417/images/peteish_moreeval/mc_curves_mmlu.png",
                "caption": "Figure 10:Upper:MC accuracy curves during training of 7B-4T and 13B-5T.Lower left:Task MC accuracy vs task loss, with data points from all intermediate checkpoints of 7B-4T and 13B-5T. A sigmoidal function cannot fit the data points.Lower right:Task MC accuracy vs task loss, where the sigmoidal function (Equation¬†2) is fitted on data points from the intermediate checkpoints between 170k‚Äì450k steps of 7B-4T, and between 50k‚Äì150k steps of 13B-5T.",
                "position": 2370
            },
            {
                "img": "https://arxiv.org/html/2412.04403/extracted/6046417/images/peteish_moreeval/mc_curves_core.png",
                "caption": "",
                "position": 2374
            },
            {
                "img": "https://arxiv.org/html/2412.04403/x10.png",
                "caption": "",
                "position": 2376
            },
            {
                "img": "https://arxiv.org/html/2412.04403/x11.png",
                "caption": "",
                "position": 2378
            },
            {
                "img": "https://arxiv.org/html/2412.04403/x12.png",
                "caption": "Figure 11:Step 1 using compute-flopsCùê∂Citalic_Cinstead of(N,D)ùëÅùê∑(N,D)( italic_N , italic_D ).\n‚óº = 1xC; ‚úö = 2xC;\\pentagofill\\pentagofill\\pentagofill= 5xC;‚òÖ‚òÖ\\bigstar‚òÖ= 10xC.\nWe report the average relative fitting error in parentheses following the task name, and prediction error in the plot next to the target model point.",
                "position": 2399
            },
            {
                "img": "https://arxiv.org/html/2412.04403/x13.png",
                "caption": "Figure 12:Predicting finaltask cross-entropy(Equation¬†7) from model parameters and token budget in step 1.",
                "position": 2567
            },
            {
                "img": "https://arxiv.org/html/2412.04403/x14.png",
                "caption": "Figure 13:Predicting the task metric from thetask cross-entropy(Equation¬†7) in step 2.",
                "position": 2570
            },
            {
                "img": "https://arxiv.org/html/2412.04403/x15.png",
                "caption": "Figure 14:Chaining predictions from step 1 (Figure¬†12) and step 2 (Figure¬†13) withtask cross-entropyas the intermediate feature.",
                "position": 2573
            },
            {
                "img": "https://arxiv.org/html/2412.04403/x16.png",
                "caption": "Figure 15:Using language modeling loss on C4-en validation as the intermediate feature.From top to bottom: step 1, step 2, and chaining the two steps.",
                "position": 2618
            },
            {
                "img": "https://arxiv.org/html/2412.04403/x17.png",
                "caption": "",
                "position": 2622
            },
            {
                "img": "https://arxiv.org/html/2412.04403/x18.png",
                "caption": "",
                "position": 2624
            },
            {
                "img": "https://arxiv.org/html/2412.04403/x19.png",
                "caption": "Figure 16:Task RC accuracy vs training scale(N,D)ùëÅùê∑(N,D)( italic_N , italic_D ), with fitting on the single-step function inEquation¬†6.",
                "position": 2643
            }
        ]
    },
    {
        "header": "Appendix CAdditional Details on Design Choices",
        "images": []
    }
]