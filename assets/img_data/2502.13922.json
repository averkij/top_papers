[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13922/x1.png",
                "caption": "Figure 1:The comparison of long-context (InfiniteBench) and short-context (MMLU) performance among GPT-4-128K and smaller LLMs.",
                "position": 147
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3LongPO: Short-to-Long Preference Optimization",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13922/extracted/6217667/figures/longpo_refined.png",
                "caption": "Figure 2:The procedure of generating short-to-long preference data from step 1 to 7.",
                "position": 396
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Results and Analyses",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13922/x2.png",
                "caption": "Figure 3:The margins of the short-context performance of Mistral-7B-LongPO and baselines relative to corresponding base model. GLM and LWM refer to the margins of GLM-9B-1M and LWM-7B-1M over GLM-9B-128K and LWM-7B-128K, respectively. MT-Bench metrics (∈\\in∈[0, 10]) are linearly scaled to [0, 100] for better comparability across tasks. See numerical results inTable3.",
                "position": 891
            },
            {
                "img": "https://arxiv.org/html/2502.13922/x3.png",
                "caption": "Figure 4:Long- and short-context performance comparison among LongPO, SFT on chosen responses (SFT-Chosen), SFT on rejected responses (SFT-Rejected), DPO, and SFT on chosen responses with short-to-long constraint (SFT-Chosen-Constraint).",
                "position": 921
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion and Discussion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMathematical Derivations",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13922/extracted/6217667/figures/prompt.png",
                "caption": "Figure 5:The prompt for generating instruction pool.",
                "position": 1813
            },
            {
                "img": "https://arxiv.org/html/2502.13922/x4.png",
                "caption": "(a)The rewards for chosen response during training.",
                "position": 1816
            },
            {
                "img": "https://arxiv.org/html/2502.13922/x4.png",
                "caption": "(a)The rewards for chosen response during training.",
                "position": 1819
            },
            {
                "img": "https://arxiv.org/html/2502.13922/x5.png",
                "caption": "(b)The rewards for rejected response during training.",
                "position": 1824
            }
        ]
    },
    {
        "header": "Appendix BExperimental Details",
        "images": []
    }
]