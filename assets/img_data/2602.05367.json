[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05367/x1.png",
                "caption": "",
                "position": 137
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Motivation",
        "images": []
    },
    {
        "header": "4Method",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05367/x2.png",
                "caption": "(a)Inter-Path Correlation",
                "position": 1076
            },
            {
                "img": "https://arxiv.org/html/2602.05367/x2.png",
                "caption": "(a)Inter-Path Correlation",
                "position": 1079
            },
            {
                "img": "https://arxiv.org/html/2602.05367/x3.png",
                "caption": "(b)Training Loss",
                "position": 1085
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMathematical Analysis of Training Dynamics",
        "images": []
    },
    {
        "header": "Appendix BExtended Analysis: Inter-Path Adaptation under KL Divergence",
        "images": []
    },
    {
        "header": "Appendix CInitialization Analysis: Functionality vs. Approximation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05367/x4.png",
                "caption": "(a)ùêñFP\\mathbf{W}_{\\mathrm{FP}}",
                "position": 2075
            },
            {
                "img": "https://arxiv.org/html/2602.05367/x4.png",
                "caption": "(a)ùêñFP\\mathbf{W}_{\\mathrm{FP}}",
                "position": 2078
            },
            {
                "img": "https://arxiv.org/html/2602.05367/x5.png",
                "caption": "(b)Greedy SVID Init.",
                "position": 2083
            },
            {
                "img": "https://arxiv.org/html/2602.05367/x6.png",
                "caption": "(c)Iterative SVID Init.",
                "position": 2088
            },
            {
                "img": "https://arxiv.org/html/2602.05367/x7.png",
                "caption": "(d)Iter + I/O Scaling",
                "position": 2093
            },
            {
                "img": "https://arxiv.org/html/2602.05367/x8.png",
                "caption": "",
                "position": 2098
            },
            {
                "img": "https://arxiv.org/html/2602.05367/x9.png",
                "caption": "(e)Diff. (Greedy)",
                "position": 2104
            },
            {
                "img": "https://arxiv.org/html/2602.05367/x10.png",
                "caption": "(f)Diff. (Iterative)",
                "position": 2109
            },
            {
                "img": "https://arxiv.org/html/2602.05367/x11.png",
                "caption": "(g)Diff. (Iter + I/O)",
                "position": 2114
            }
        ]
    },
    {
        "header": "Appendix DExtended Results",
        "images": []
    },
    {
        "header": "Appendix EInference Performance Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05367/x12.png",
                "caption": "Figure 4:End-to-end decoding throughput (tokens/second) for Llama2-7B on an NVIDIA RTX 4090 across various generated token lengths.RaBiT‚Äôs parallel architecture consistently delivers superior performance over other 2-bit methods.",
                "position": 2425
            }
        ]
    },
    {
        "header": "Appendix FHyperparameters",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05367/x13.png",
                "caption": "Figure 5:Convergence analysis of Iterative Residual SVID on Llama2-7B. The metric is the Initial KL Divergence Loss (Lower is better). Convergence stabilizes around 20 iterations.",
                "position": 2655
            }
        ]
    },
    {
        "header": "Appendix GExtended Analysis of Inter-Path Adaptation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05367/x14.png",
                "caption": "Figure 6:Layer-wise MSE Decomposition in Llama2-7B‚Äôsdown_projlayers.The bars compare the total Mean Squared Error (MSE) for Standard QAT (blue) and RaBiT (orange). The top of each bar represents the base error term (C‚Ä≤C^{\\prime}), while the red-dashed component visualizes twice the covariance (2√óCov2\\times\\text{Cov}). RaBiT consistently generates a large negative covariance, which actively reduces the total MSE, demonstrating effective error cancellation. Notably, RaBiT also suppresses the extremely high MSE peak observed in the early layers of the Standard QAT baseline, indicating its robustness against layer sensitivity.",
                "position": 2663
            }
        ]
    },
    {
        "header": "Appendix HGenerated Samples",
        "images": []
    },
    {
        "header": "Appendix IAlgorithms",
        "images": []
    },
    {
        "header": "Appendix JCore Kernel Code",
        "images": []
    }
]