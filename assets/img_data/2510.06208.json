[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.06208/fig/teaser_v2.png",
                "caption": "Figure 1:ShapeGen4Dgenerates high-quality mesh sequences from input monocular videos.",
                "position": 71
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.06208/x1.png",
                "caption": "Figure 2:ShapeGen4D employs a flow-based latent diffusion transformer to generate a sequence of meshes from an input video. (a) A 3D VAE encodes shapes into latents by cross-attending subsampled query points with a dense point cloud. To encode a sequence of animated assets, query points are subsampled from the first-frame point cloud and then propagated through the animation to obtain query points for subsequent framesâ€”yielding temporally-aligned latents. The decoder maps these latents to signed distance fields, which are then converted into meshes via marching cubes. (b) The spatiotemporal diffusion transformer interleaves frozen dual/single-stream transformer blocks from the base 3D generative model, which process hidden states for each frame independently, with learnable spatiotemporal attention layers that capture cross-frame dependencies and enforce temporal consistency in the denoised latents.",
                "position": 107
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Dynamic Mesh Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.06208/x2.png",
                "caption": "Figure 3:Illustration of latents with and without aligning query points across frames in (a) and (b). In (c), we visualize the average normalizedL2L_{2}difference between latents at the closest 3D positions across neighboring frames. We observe that with alignment, theL2L_{2}difference is smaller, indicating that the latents are more consistent,i.e., less jittery compared to non-aligned latents.",
                "position": 160
            },
            {
                "img": "https://arxiv.org/html/2510.06208/x3.png",
                "caption": "Figure 4:Qualitative comparison of noise sharing. The base 3D model generates object shapes in arbitrary orientations agnostic to the input image viewpoint, often causing pose changes across a sequence (e.g. the hippo in the first row). We observe that sharing noise across frames reduces flickering and further improves shape quality in challenging cases such as the flag example.",
                "position": 185
            }
        ]
    },
    {
        "header": "4Mesh Registration and Texturization",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.06208/x4.png",
                "caption": "Figure 5:Qualitative comparison with baselines on the held-out Objaverse test set.",
                "position": 452
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AMesh Registration and Texturization Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.06208/x5.png",
                "caption": "Figure 6:Qualitative comparison on Consistent4D test set.",
                "position": 1418
            }
        ]
    },
    {
        "header": "Appendix BAdditional Results",
        "images": []
    }
]