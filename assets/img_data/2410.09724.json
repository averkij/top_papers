[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.09724/extracted/5922513/sections/calibration-framework.png",
                "caption": "Figure 1:(Top): Illustration of verbalized confidence generation. An LLM incorrectly answers a question with high confidence.\n(Bottom): Comparison between reward scores from a vanilla-trained reward modelLlama-3-8b-rm-mixtureand our calibrated reward modelLlama-3-8b-crm. The vanilla model shows bias towards high confidence though the answer is incorrect. Our calibrated reward model correctly assigns a higher reward to the low confidence for the incorrect answer.",
                "position": 251
            }
        ]
    },
    {
        "header": "2Exploring Systematic Biases and Overconfidence in RLHF-LLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.09724/x1.png",
                "caption": "Llama3-8B-SFTandLlama3-8B-PPO;Tulu-2-7BandTulu-2-DPO-7B",
                "position": 299
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x1.png",
                "caption": "",
                "position": 302
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x2.png",
                "caption": "",
                "position": 306
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x3.png",
                "caption": "",
                "position": 311
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x4.png",
                "caption": "",
                "position": 315
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x5.png",
                "caption": "Figure 3:Preference distributions forArmoRM-Llama3-8B-v0.1, a reward model for PPO training (left) andTulu-2-DPO-7B, a DPO model (right) on the modifiedRewardBenchdataset across four modes. From top to bottom:confidence_reversed,chosen_with_conf,rejected_with_conf,answer_only. Red bar indicates the preference for a rejected or high-confidence response, and blue bar indicates the preference for a chosen or low-confidence response.",
                "position": 357
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x5.png",
                "caption": "",
                "position": 360
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x6.png",
                "caption": "",
                "position": 364
            }
        ]
    },
    {
        "header": "3Calibrated Reward Modeling and Calculation",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.09724/extracted/5922513/sections/method-framework.png",
                "caption": "Figure 5:Framework for PPO-C.",
                "position": 475
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.09724/extracted/5922513/sections/eval_graphs/comparison_rejected_Llama-3-8b-rm-mixture.png",
                "caption": "(a)Performance comparison across various methods on six datasets. SFT: Supervised Fine-Tuned checkpoints, serving as the starting points for all methods. PPO††\\dagger†: an ablation of our PPO-M method which uses vanilla reward model in PPO training but on our modified dataset (with confidence-query system prompts).",
                "position": 572
            },
            {
                "img": "https://arxiv.org/html/2410.09724/extracted/5922513/sections/eval_graphs/comparison_rejected_Llama-3-8b-rm-mixture.png",
                "caption": "Figure 6:Preference distributions of our calibrated reward modelLlama-3-8b-crmand the pre-calibrated versionLlama-3-8b-rm-mixtureonrejected_with_conf.",
                "position": 1236
            }
        ]
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.09724/x7.png",
                "caption": "Table 1:Results on MT-Bench and Arena-Hard.",
                "position": 1268
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x7.png",
                "caption": "Table 2:Comparison of MT-Bench And Arena-Hard Performance forMistral-7B.",
                "position": 1401
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x7.png",
                "caption": "Table 3:Performance comparison of SFT, DPO, DPO††\\dagger†, and CDPO across six datasets usingMistral-7B.\nSFT and DPO denote the reference and trained DPO models, respectively. DPO††\\dagger†and CDPO initiate from the trained DPO checkpoint; DPO††\\dagger†applies standard DPO on the calibration dataset, focusing on chosen and rejected pairs to assess the impact of training with additional data.",
                "position": 1452
            }
        ]
    },
    {
        "header": "6Related Works",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelated Works",
        "images": []
    },
    {
        "header": "Appendix BLimitation and Broader Impact",
        "images": []
    },
    {
        "header": "Appendix CDatasets",
        "images": []
    },
    {
        "header": "Appendix DImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.09724/x7.png",
                "caption": "Llama3-8B-SFTandLlama3-8B-PPO;Tulu-2-7BandTulu-2-DPO-7B",
                "position": 3695
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x7.png",
                "caption": "",
                "position": 3698
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x8.png",
                "caption": "",
                "position": 3702
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x9.png",
                "caption": "",
                "position": 3707
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x10.png",
                "caption": "",
                "position": 3711
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x11.png",
                "caption": "Llama3-8B-SFTandLlama3-8B-PPO;Tulu-2-7BandTulu-2-DPO-7B",
                "position": 3718
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x11.png",
                "caption": "",
                "position": 3721
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x12.png",
                "caption": "",
                "position": 3725
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x13.png",
                "caption": "",
                "position": 3730
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x14.png",
                "caption": "",
                "position": 3734
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x15.png",
                "caption": "Llama3-8B-SFTandLlama3-8B-PPO;Tulu-2-7BandTulu-2-DPO-7B",
                "position": 3741
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x15.png",
                "caption": "",
                "position": 3744
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x16.png",
                "caption": "",
                "position": 3748
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x17.png",
                "caption": "",
                "position": 3753
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x18.png",
                "caption": "",
                "position": 3757
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x19.png",
                "caption": "Llama3-8B-SFTandLlama3-8B-PPO;Tulu-2-7BandTulu-2-DPO-7B",
                "position": 3764
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x19.png",
                "caption": "",
                "position": 3767
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x20.png",
                "caption": "",
                "position": 3771
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x21.png",
                "caption": "",
                "position": 3776
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x22.png",
                "caption": "",
                "position": 3780
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x23.png",
                "caption": "Llama3-8B-SFTandLlama3-8B-PPO;Tulu-2-7BandTulu-2-DPO-7B",
                "position": 3787
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x23.png",
                "caption": "",
                "position": 3790
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x24.png",
                "caption": "",
                "position": 3794
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x25.png",
                "caption": "",
                "position": 3799
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x26.png",
                "caption": "",
                "position": 3803
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x27.png",
                "caption": "(a)RLHFlow/ArmoRM-Llama3-8B-v0.1(Wang et al.,2024c)",
                "position": 3815
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x27.png",
                "caption": "(a)RLHFlow/ArmoRM-Llama3-8B-v0.1(Wang et al.,2024c)",
                "position": 3818
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x28.png",
                "caption": "",
                "position": 3821
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x29.png",
                "caption": "(b)CIR-AMS/BTRM_Qwen2_7b_0613",
                "position": 3828
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x30.png",
                "caption": "",
                "position": 3831
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x31.png",
                "caption": "(c)openbmb/Eurus-RM-7b(Yuan et al.,2024)",
                "position": 3838
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x32.png",
                "caption": "",
                "position": 3841
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x33.png",
                "caption": "(d)sfairXC/FsfairX-LLaMA3-RM-v0.1(Dong et al.,2023a)",
                "position": 3848
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x34.png",
                "caption": "",
                "position": 3851
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x35.png",
                "caption": "(e)OpenRLHF/Llama-3-8b-rm-700k(Hu et al.,2024)",
                "position": 3858
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x36.png",
                "caption": "",
                "position": 3861
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x37.png",
                "caption": "(a)Skywork/Skywork-Reward-Llama-3.1-8B(Liu & Zeng,2024)",
                "position": 3869
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x37.png",
                "caption": "(a)Skywork/Skywork-Reward-Llama-3.1-8B(Liu & Zeng,2024)",
                "position": 3872
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x38.png",
                "caption": "",
                "position": 3875
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x39.png",
                "caption": "(b)stabilityai/stablelm-2-12b-chat(Bellagente et al.,2024)",
                "position": 3882
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x40.png",
                "caption": "",
                "position": 3885
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x41.png",
                "caption": "(c)allenai/tulu-2-dpo-7b(Ivison et al.,2023)",
                "position": 3892
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x42.png",
                "caption": "",
                "position": 3895
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x43.png",
                "caption": "(d)openbmb/UltraRM-13b(Cui et al.,2023)",
                "position": 3902
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x44.png",
                "caption": "",
                "position": 3905
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x45.png",
                "caption": "(e)LxzGordon/URM-LLaMa-3.1-8B(Cui et al.,2023)",
                "position": 3912
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x46.png",
                "caption": "",
                "position": 3915
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x47.png",
                "caption": "(a)OpenRLHF/Llama-3-8b-rm-mixture(Hu et al.,2024)",
                "position": 3923
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x47.png",
                "caption": "(a)OpenRLHF/Llama-3-8b-rm-mixture(Hu et al.,2024)",
                "position": 3926
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x48.png",
                "caption": "",
                "position": 3929
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x49.png",
                "caption": "(b)NCSOFT/Llama-3-OffsetBias-RM-8B(Park et al.,2024)",
                "position": 3936
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x50.png",
                "caption": "",
                "position": 3939
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x51.png",
                "caption": "(c)OpenAssistant/oasst-rm-2.1-pythia-1.4b-epoch-2.5",
                "position": 3946
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x52.png",
                "caption": "",
                "position": 3949
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x53.png",
                "caption": "(d)IDEA-CCNL/Ziya-LLaMA-7B-Reward(Cui et al.,2023)",
                "position": 3956
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x54.png",
                "caption": "",
                "position": 3959
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x55.png",
                "caption": "(e)internlm/internlm2-20b-reward",
                "position": 3966
            },
            {
                "img": "https://arxiv.org/html/2410.09724/x56.png",
                "caption": "",
                "position": 3969
            },
            {
                "img": "https://arxiv.org/html/2410.09724/extracted/5922513/sections/eval_graphs/comparison_chosen_Llama-3-8b-rm-mixture.png",
                "caption": "(a)chosen_with_conf",
                "position": 3993
            },
            {
                "img": "https://arxiv.org/html/2410.09724/extracted/5922513/sections/eval_graphs/comparison_chosen_Llama-3-8b-rm-mixture.png",
                "caption": "(a)chosen_with_conf",
                "position": 3996
            },
            {
                "img": "https://arxiv.org/html/2410.09724/extracted/5922513/sections/eval_graphs/comparison_rejected_Llama-3-8b-rm-mixture.png",
                "caption": "(b)rejected_with_conf",
                "position": 4001
            },
            {
                "img": "https://arxiv.org/html/2410.09724/extracted/5922513/sections/eval_graphs/comparison_chosen_mistral-7b-hermes-rm-skywork.png",
                "caption": "(a)chosen_with_conf",
                "position": 4008
            },
            {
                "img": "https://arxiv.org/html/2410.09724/extracted/5922513/sections/eval_graphs/comparison_chosen_mistral-7b-hermes-rm-skywork.png",
                "caption": "(a)chosen_with_conf",
                "position": 4011
            },
            {
                "img": "https://arxiv.org/html/2410.09724/extracted/5922513/sections/eval_graphs/comparison_rejected_mistral-7b-hermes-rm-skywork.png",
                "caption": "(b)rejected_with_conf",
                "position": 4016
            }
        ]
    },
    {
        "header": "Appendix EMore Results and Analysis",
        "images": []
    }
]