[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09606/images/logo_v2.png",
                "caption": "",
                "position": 122
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x1.png",
                "caption": "Figure 1:Prior works of spatial reasoning have largely focused on indoor (11-3030m) scenes, while our SpaceVista model and dataset span scales fromm​mmm(1​e1e-33m) tok​mkm(1​e1e+33m).\nThis six-order-of-magnitude range introduces not only scale variation but also rich semantics and diverse tasks. SpaceVista enables all-scale spatial reasoning by integrating cues from micro-objects to macro-scenes.",
                "position": 152
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09606/x2.png",
                "caption": "Figure 2:(a) and (b) show model performance and dataset distribution across scales. Current models and datasets necessitate all-scale spatial reasoning.",
                "position": 163
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09606/x3.png",
                "caption": "Figure 3:Fig.(a) shows our automated data construction pipeline. The pie charts (b-c) depict the composition of scenes and sources. The bar charts (d–e) show object sizes rangingm​mmm-100​m100m, while object-to-camera distances typically span1010-600​m600m. Accordingly, we claim SpaceVista-1M basically covers them​mmm-k​mkmscale. The word clouds (f-g) provide a glimpse of the scene diversity.",
                "position": 224
            }
        ]
    },
    {
        "header": "3Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09606/x4.png",
                "caption": "Figure 4:The left part (a-d) shows that the undifferentiated mixture of cross-scale knowledge hinders, rather than facilitates, the model’s reasoning process. The horizontal axis represents the scale discrepancy, defined asa​n​s​w​e​rg​t\\frac{answer}{gt}(=11for the ideal situation), and the vertical axis denotes the proportion of answers. Fig.(e) is our SpaceVista model, where “<think>” is omitted for clarity.",
                "position": 406
            }
        ]
    },
    {
        "header": "4Method",
        "images": []
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09606/x5.png",
                "caption": "Figure 5:Visualization of scale-expert activations on salient tokens with an appropriate threshold. This shows the router selects experts based on the input.",
                "position": 722
            },
            {
                "img": "https://arxiv.org/html/2510.09606/images/medal_3.png",
                "caption": "Table 5:The SpaceVista-Bench leaderborad. We utilizegreen (1st),blue (2nd), andyellow (3rd)backgrounds to distinguish the top three results within each scene.\nWe employboldandunderlined textto denote the bests and second-best results across all open-source models.\nAll the baselines are instruction-tuned and are evaluated on the same resolution and fps.",
                "position": 826
            },
            {
                "img": "https://arxiv.org/html/2510.09606/images/medal_2.png",
                "caption": "",
                "position": 871
            },
            {
                "img": "https://arxiv.org/html/2510.09606/images/medal_1.png",
                "caption": "",
                "position": 1024
            }
        ]
    },
    {
        "header": "6Discussion And Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendices Contents",
        "images": []
    },
    {
        "header": "Appendix AImportant Information",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09606/x6.png",
                "caption": "Figure A6:Statistical chart of QA types. The spatial reasoning tasks for various scenes include abbreviations, for example, “Est.” for Estimation, “Dist.” for Distance, “Loc.” for Location, and “Com.” for Comparison.",
                "position": 2618
            },
            {
                "img": "https://arxiv.org/html/2510.09606/images/radar.png",
                "caption": "Figure A7:Comparison across popular spatial reasoning benchmarks. Our SpaceVista-7B model achieves certain performance boosts across all benchmarks.",
                "position": 2625
            }
        ]
    },
    {
        "header": "Appendix BData Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09606/x7.png",
                "caption": "Figure B8:Our self-collected data features various categories of objects, with tabletops and tiny tabletops ranging from 0.4m to 3mm, even including transparent and reflective objects.",
                "position": 2824
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x7.png",
                "caption": "Figure B8:Our self-collected data features various categories of objects, with tabletops and tiny tabletops ranging from 0.4m to 3mm, even including transparent and reflective objects.",
                "position": 2827
            },
            {
                "img": "https://arxiv.org/html/2510.09606/images/tiny_tabletop_collection.png",
                "caption": "Figure B9:A photo of the real scene for the collection of tiny tabletop.",
                "position": 2832
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x8.png",
                "caption": "Figure B10:Examples of identifying outdoor landmark objects from existing datasets and retrieving their scale-related ground truth data.",
                "position": 2845
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x9.png",
                "caption": "Figure B11:Automatic Processing Pipeline for Counting Task Scenes. Through data filtering, object tracking, and counting, the final counting video is obtained after data confirmation.",
                "position": 3120
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x10.png",
                "caption": "Figure B12:Visualization of robotic manipulation planning. Fig.(a) visualizes the option for moving the red box to the left of the upper box. Fig.(b) represents the key frame to carry out the manipulation.",
                "position": 3130
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x11.png",
                "caption": "Figure B13:The word cloud of the previous indoor spatial reasoning datasets.",
                "position": 3160
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x11.png",
                "caption": "Figure B13:The word cloud of the previous indoor spatial reasoning datasets.",
                "position": 3163
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x12.png",
                "caption": "Figure B14:The word cloud of our indoor subset.",
                "position": 3168
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x13.png",
                "caption": "Figure B15:The word cloud of our outdoor subset.",
                "position": 3174
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x14.png",
                "caption": "Figure B16:The word cloud of our tabletop subset.",
                "position": 3179
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x15.png",
                "caption": "Figure B17:The word cloud of our tiny tabletop subset.",
                "position": 3185
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x16.png",
                "caption": "Figure B18:The word cloud of the self-collected subset. Note: We use standard ISO 7046 to denote the models of the screw, which looks like“m4*10”.",
                "position": 3190
            },
            {
                "img": "https://arxiv.org/html/2510.09606/images/depth_max_min_boxplot.png",
                "caption": "Figure B19:The distribution of the maximum depth value of our dataset. The maximum distance denotes the farthest point observed.",
                "position": 3226
            },
            {
                "img": "https://arxiv.org/html/2510.09606/images/depth_max_min_boxplot.png",
                "caption": "Figure B19:The distribution of the maximum depth value of our dataset. The maximum distance denotes the farthest point observed.",
                "position": 3229
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x17.png",
                "caption": "Figure B20:The distribution of the specific sceneries. Note: this chart is just for basic knowledge. Due to the latter filtering policy, there might be some vague or inaccurate analysis.",
                "position": 3234
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x18.png",
                "caption": "Figure B21:The distribution of the size of the existing objects.",
                "position": 3261
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x18.png",
                "caption": "Figure B21:The distribution of the size of the existing objects.",
                "position": 3264
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x19.png",
                "caption": "Figure B22:The distribution of the distance between the target object and the camera.",
                "position": 3269
            }
        ]
    },
    {
        "header": "Appendix CModel Detail",
        "images": []
    },
    {
        "header": "Appendix DObservation Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09606/x20.png",
                "caption": "Figure D23:Visualization of GRPO updated and normalized correctness reward chart. This figure visualizes how the reward grows during the RL training stage.",
                "position": 4080
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x21.png",
                "caption": "Figure D24:Visualization of the normalized scale of each expert with different selected samples. It reflects the model’s capacity to allocate resources according to the inherent properties of each scene.",
                "position": 4091
            }
        ]
    },
    {
        "header": "Appendix EFAQ",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09606/images/D1_vsi_data.jpg",
                "caption": "Figure F25:Indoor data are rather simple and clean scenes inside a room. The overall scene is not as complex as the wild indoor scene.",
                "position": 7133
            },
            {
                "img": "https://arxiv.org/html/2510.09606/images/D2_DL3DV_indoor_data.jpg",
                "caption": "",
                "position": 7138
            },
            {
                "img": "https://arxiv.org/html/2510.09606/images/D3_DL3DV_outdoor_data.jpg",
                "caption": "Figure F27:Outdoor data is jointly collected from ground views, incorporating street, park, building and so on.",
                "position": 7142
            },
            {
                "img": "https://arxiv.org/html/2510.09606/images/D3_DL3DV_outdoor_data.jpg",
                "caption": "Figure F27:Outdoor data is jointly collected from ground views, incorporating street, park, building and so on.",
                "position": 7145
            },
            {
                "img": "https://arxiv.org/html/2510.09606/images/D4_DL3DV_drone_data.jpg",
                "caption": "Figure F28:Drone data captures ground objects from above at oblique angles, providing more complete structural coverage than traditional ground-based capture methods.",
                "position": 7151
            },
            {
                "img": "https://arxiv.org/html/2510.09606/images/D5_wildrgbd_data.jpg",
                "caption": "Figure F29:In this tabletop scene, videos capture tabletop objects exhibiting rich background variation and natural occlusions, delivering clearer structural coverage of the objects than traditional static indoor datasets.",
                "position": 7157
            },
            {
                "img": "https://arxiv.org/html/2510.09606/images/D5_wildrgbd_data.jpg",
                "caption": "Figure F29:In this tabletop scene, videos capture tabletop objects exhibiting rich background variation and natural occlusions, delivering clearer structural coverage of the objects than traditional static indoor datasets.",
                "position": 7160
            },
            {
                "img": "https://arxiv.org/html/2510.09606/images/D6_uco_3d_data.jpg",
                "caption": "Figure F30:Tiny tabletop objects captured with rich details for small objects, focusing on fine-scale scenes, unlike typical large or complex indoor or outdoor datasets.",
                "position": 7166
            },
            {
                "img": "https://arxiv.org/html/2510.09606/images/D7_our_collected.jpg",
                "caption": "Figure F31:These samples are collected by us. As small-scale, Tabletop, and Tiny Tabletop datasets offer rich details with accurate annotation.",
                "position": 7172
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x22.png",
                "caption": "Table F27:The spatial relation task QA preview.",
                "position": 7175
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x23.png",
                "caption": "",
                "position": 7298
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x24.png",
                "caption": "Table F28:The camera moving task QA preview.",
                "position": 7343
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x25.png",
                "caption": "",
                "position": 7536
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x26.png",
                "caption": "",
                "position": 7646
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x27.png",
                "caption": "Table F30:The size comparison task QA preview.",
                "position": 7695
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x28.png",
                "caption": "",
                "position": 7828
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x29.png",
                "caption": "",
                "position": 7897
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x30.png",
                "caption": "Table F32:The rotation estimation task QA preview.",
                "position": 8066
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x31.png",
                "caption": "",
                "position": 8186
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x32.png",
                "caption": "",
                "position": 8250
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x33.png",
                "caption": "",
                "position": 8352
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x34.png",
                "caption": "Table F34:The absolute distance task QA preview.",
                "position": 8401
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x35.png",
                "caption": "",
                "position": 8521
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x36.png",
                "caption": "",
                "position": 8585
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x37.png",
                "caption": "Table F36:The object counting task QA preview.",
                "position": 8737
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x38.png",
                "caption": "",
                "position": 8857
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x39.png",
                "caption": "",
                "position": 8921
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x40.png",
                "caption": "",
                "position": 9023
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x41.png",
                "caption": "Table F38:The route plan task QA preview for evaluation.",
                "position": 9072
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x42.png",
                "caption": "",
                "position": 9265
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x43.png",
                "caption": "Table F40:The depth estimation task QA preview.",
                "position": 9417
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x44.png",
                "caption": "",
                "position": 9537
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x45.png",
                "caption": "",
                "position": 9601
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x46.png",
                "caption": "",
                "position": 9703
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x47.png",
                "caption": "Table F42:The object matching task QA preview.",
                "position": 9752
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x48.png",
                "caption": "",
                "position": 9946
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x49.png",
                "caption": "Table F44:The manipulation planning task QA preview.",
                "position": 10106
            },
            {
                "img": "https://arxiv.org/html/2510.09606/x50.png",
                "caption": "",
                "position": 10291
            }
        ]
    },
    {
        "header": "Appendix FPreview",
        "images": []
    }
]