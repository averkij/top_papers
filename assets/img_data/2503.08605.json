[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08605/x1.png",
                "caption": "",
                "position": 106
            },
            {
                "img": "https://arxiv.org/html/2503.08605/x2.png",
                "caption": "Figure 2:Qualitative comparisonson CogVideoX-2B[55].\nAll examples are 5 times longer in duration compared to the underlying base model, generating a 30-second video.\nUnlike Gen-L-Video[50]and FIFO-Diffusion[21], which often struggle with overlapping artifacts and style drift, our method, SynCoS, ensures consistency in both content and style throughout the entire video.\nAdditionally, SynCoS generates long videos where each frame faithfully follows its designated prompt while ensuring seamless transitions between frames.",
                "position": 115
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08605/x3.png",
                "caption": "Figure 3:t-SNE visualizationof CLIP[37]features for the predicted video framesùê±^0|tsubscript^ùê±conditional0ùë°\\hat{\\mathbf{x}}_{0|t}over^ start_ARG bold_x end_ARG start_POSTSUBSCRIPT 0 | italic_t end_POSTSUBSCRIPT, at each timestep using different samplings.\nFaded colors indicate earlier timesteps (t‚âà1000ùë°1000t\\approx 1000italic_t ‚âà 1000), while vivid colors indicate later, small timesteps (t‚âà0ùë°0t\\approx 0italic_t ‚âà 0), illustrating feature trajectory evolution over time (top to bottom).",
                "position": 276
            },
            {
                "img": "https://arxiv.org/html/2503.08605/x3.png",
                "caption": "Figure 3:t-SNE visualizationof CLIP[37]features for the predicted video framesùê±^0|tsubscript^ùê±conditional0ùë°\\hat{\\mathbf{x}}_{0|t}over^ start_ARG bold_x end_ARG start_POSTSUBSCRIPT 0 | italic_t end_POSTSUBSCRIPT, at each timestep using different samplings.\nFaded colors indicate earlier timesteps (t‚âà1000ùë°1000t\\approx 1000italic_t ‚âà 1000), while vivid colors indicate later, small timesteps (t‚âà0ùë°0t\\approx 0italic_t ‚âà 0), illustrating feature trajectory evolution over time (top to bottom).",
                "position": 279
            },
            {
                "img": "https://arxiv.org/html/2503.08605/x4.png",
                "caption": "Figure 4:Qualitative comparison of sampling methods motivating SynCoS. Gen-L-Video[50]fails to maintain global coherence, resulting in abrupt appearance changes (e.g., a man morphing into a woman).\nCSD[22]retains a similar appearance of a man but shows poor adherence to local prompts, suffering from low frame quality with severe noise-like artifacts.\nIn contrast, our method achieves a balance, ensuring high-quality generation, strong prompt fidelity, and temporal coherence.",
                "position": 285
            }
        ]
    },
    {
        "header": "3Key observations",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08605/x5.png",
                "caption": "Figure 5:Overall illustration of our proposed method, Synchronized Coupled Sampling (SynCoS), a tuning-free inference framework for multi-event long video generation.\nSynCoS performs one-step denoising in three iterative stages, repeated fromt=1000ùë°1000t=1000italic_t = 1000tot=0ùë°0t=0italic_t = 0.\nIn the first stage, SynCoS performs temporal co-denoising with DDIM, dividing the long video into overlapping short chunks, denoising each chunk, and applying fusion for local smoothness.\nIn the second stage, SynCoS refines the locally fused output, enforcing global coherence by synchronizing information across both short- and long-distance chunks.\nFinally, in the third stage, it reverts the locally and globally refined output to the previous timestep.\nThrough these three synchronized stages of local and global denoising, SynCoS ensures smooth transitions, global semantic coherence, and high prompt fidelity, enabling multi-event long video generation.",
                "position": 381
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08605/x6.png",
                "caption": "Table 1:Quantitative comparison.Experimental results of SynCoS with baselines on multi-event long video generations.Boldindicates the best results.\nSynCoS consistently outperforms baselines across temporal consistency, per-frame quality, and prompt fidelity.",
                "position": 728
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08605/x7.png",
                "caption": "Table 2:Quantitative ablation study. *Abbreviations: subject consistency (SC), background consistency (BC), aesthetic quality (AQ), and prompt fidelity (PF).",
                "position": 933
            },
            {
                "img": "https://arxiv.org/html/2503.08605/x7.png",
                "caption": "Figure 7:Qualitative ablation study. Without a grounded timestep, structural inconsistencies arise (e.g., the volcano alternates between one and two peaks).\nWithout a fixed baseline noise, it fails to follow local prompts faithfully (e.g., missing eruptions or absent smoke).",
                "position": 1012
            }
        ]
    },
    {
        "header": "6Related work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AFoundational concepts and derivations",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08605/x8.png",
                "caption": "Figure 8:Instruction for generating structured prompt. This instruction follows the guidelines to create individual local prompts and a shared global prompt based on a scenario and the number of prompts the user gives.",
                "position": 1934
            },
            {
                "img": "https://arxiv.org/html/2503.08605/x9.png",
                "caption": "Table 3:Quantitative ablations study of the three coupled stages in SynCoS, omitting each stage during one-timestep denoising, demonstrates the importance of all three stages for coherent long video generation with multiple events.",
                "position": 2013
            }
        ]
    },
    {
        "header": "Appendix BExperimental details",
        "images": []
    },
    {
        "header": "Appendix CAdditional ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08605/x10.png",
                "caption": "Figure 10:Qualitative ablation study\non stride.Reducing the stride enhances content consistency, which is beneficial in scenarios like a knight running as the background transitions from grass to snow.",
                "position": 2229
            },
            {
                "img": "https://arxiv.org/html/2503.08605/x11.png",
                "caption": "(a) Long video generation on CogVideoX-2B, where a single video chunk consists of 26 frames.",
                "position": 2247
            },
            {
                "img": "https://arxiv.org/html/2503.08605/x11.png",
                "caption": "(a) Long video generation on CogVideoX-2B, where a single video chunk consists of 26 frames.",
                "position": 2250
            },
            {
                "img": "https://arxiv.org/html/2503.08605/x12.png",
                "caption": "(b) Long video generation on Open-Sora Plan (v1.3), where a single video chunk consists of 49 frames.",
                "position": 2256
            },
            {
                "img": "https://arxiv.org/html/2503.08605/x13.png",
                "caption": "",
                "position": 2264
            },
            {
                "img": "https://arxiv.org/html/2503.08605/x14.png",
                "caption": "",
                "position": 2267
            },
            {
                "img": "https://arxiv.org/html/2503.08605/x15.png",
                "caption": "",
                "position": 2269
            }
        ]
    },
    {
        "header": "Appendix DFurther discussions on previous work",
        "images": []
    },
    {
        "header": "Appendix EPseudo-code implementation of each stage.",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08605/x16.png",
                "caption": "(a) Cinematic effects",
                "position": 2922
            },
            {
                "img": "https://arxiv.org/html/2503.08605/x16.png",
                "caption": "(a) Cinematic effects",
                "position": 2925
            },
            {
                "img": "https://arxiv.org/html/2503.08605/x17.png",
                "caption": "",
                "position": 2929
            },
            {
                "img": "https://arxiv.org/html/2503.08605/x18.png",
                "caption": "",
                "position": 2931
            },
            {
                "img": "https://arxiv.org/html/2503.08605/x19.png",
                "caption": "(b) Storytelling",
                "position": 2938
            },
            {
                "img": "https://arxiv.org/html/2503.08605/x20.png",
                "caption": "(c) Background changes",
                "position": 2944
            },
            {
                "img": "https://arxiv.org/html/2503.08605/x21.png",
                "caption": "",
                "position": 2948
            },
            {
                "img": "https://arxiv.org/html/2503.08605/x22.png",
                "caption": "(d) Physical transformation",
                "position": 2957
            },
            {
                "img": "https://arxiv.org/html/2503.08605/x22.png",
                "caption": "(d) Physical transformation",
                "position": 2960
            },
            {
                "img": "https://arxiv.org/html/2503.08605/x23.png",
                "caption": "(e) Complex scene transitions",
                "position": 2966
            },
            {
                "img": "https://arxiv.org/html/2503.08605/x24.png",
                "caption": "",
                "position": 2970
            },
            {
                "img": "https://arxiv.org/html/2503.08605/x25.png",
                "caption": "(f) Compositional generation",
                "position": 2977
            },
            {
                "img": "https://arxiv.org/html/2503.08605/x26.png",
                "caption": "",
                "position": 2981
            }
        ]
    },
    {
        "header": "Appendix FAdditional qualitative results",
        "images": []
    }
]