[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Marco-Action RLHF",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02743/x1.png",
                "caption": "Figure 1:Illustration of the MA-RLHF framework. Standard RLHF makes decisions and evaluates value scores at the token level, while MA-RLHF operates at the macro action level, making decisions over sequences of tokens at a coarser temporal scale.",
                "position": 272
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02743/x2.png",
                "caption": "Figure 2:Test RM scores of Gemma-2B and Gemma-7B models on the TL;DR dataset.\nThe shaded regions represent the standard deviation on test RM scores across training runs.",
                "position": 396
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x2.png",
                "caption": "Figure 2:Test RM scores of Gemma-2B and Gemma-7B models on the TL;DR dataset.\nThe shaded regions represent the standard deviation on test RM scores across training runs.",
                "position": 399
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x3.png",
                "caption": "",
                "position": 403
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x4.png",
                "caption": "Figure 3:Distribution of the RM scores for vanilla PPO and MA-PPO on TL;DR test set.",
                "position": 410
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x5.png",
                "caption": "Figure 4:The win rates of MA-PPO against vanilla PPO, estimated by GPT-4 and Human. The left/middle/right figures are results forTL;DR,HH-RLHFandWebGPT Comparisonsrespectively.",
                "position": 417
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x6.png",
                "caption": "",
                "position": 420
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x7.png",
                "caption": "",
                "position": 421
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x8.png",
                "caption": "Figure 5:Performance of MA-PPO with various macro action termination strategies on the TL;DR dataset using Gemma-2B.Left:Test RM scores for different termination strategies.Right:GPT-4 evaluation across four dimensions‚Äìrelevance, coherence, consistency, and fluency‚Äìcomparing different MA termination methods.",
                "position": 550
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x9.png",
                "caption": "",
                "position": 553
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x10.png",
                "caption": "Figure 6:Test RM scores of differentnùëõnitalic_nvalues in MA-PPO evaluated by corresponding RM on the TL;DR (left) and HH-RLHF (right) dataset.",
                "position": 587
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x10.png",
                "caption": "Figure 6:Test RM scores of differentnùëõnitalic_nvalues in MA-PPO evaluated by corresponding RM on the TL;DR (left) and HH-RLHF (right) dataset.",
                "position": 590
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x11.png",
                "caption": "",
                "position": 594
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x12.png",
                "caption": "Figure 7:GPT-4 scores of vanilla PPO and MA-PPO with differentnùëõnitalic_nvalues on TL;DR.",
                "position": 600
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x13.png",
                "caption": "Figure 8:The effect of temperature on RM scores for varying sample sizes (Best-of-NùëÅNitalic_N) across models. (Left): RM score of theSFTmodel under different temperatures and sample sizes. (Mid): RM score ofvanilla PPOunder the same settings. (Right): RM score ofMA-PPO.",
                "position": 611
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x14.png",
                "caption": "",
                "position": 614
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x15.png",
                "caption": "",
                "position": 615
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x16.png",
                "caption": "Figure 9:Evaluation results for vanilla PPO and MA-PPO on Gemma-2-27B using the TL;DR dataset.Left: RM scores on validation set.Mid: Distribution of RM scores for vanilla PPO and MA-PPO.Right: Scaling trending on TL;DR dataset across 2B, 7B, and 27B model size, showing RM scores, GPT-4 evaluation, human evaluation results.",
                "position": 622
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x17.png",
                "caption": "",
                "position": 625
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x18.png",
                "caption": "",
                "position": 626
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x19.png",
                "caption": "Figure 10:RM score shifting pattern after RLHF training;Left: RM scores of best-of-NùëÅNitalic_N(N=8ùëÅ8N=8italic_N = 8) sampling compared to the SFT model.Mid Left: RM scores of vanilla PPO compared to the SFT model.Mid Right: RM scores of MA-PPO (n=5ùëõ5n=5italic_n = 5) compared to the SFT model.Right: RM scores of MA-PPO (n=‚àûùëõn=\\inftyitalic_n = ‚àû) compared to the SFT model.",
                "position": 633
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x20.png",
                "caption": "",
                "position": 636
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x21.png",
                "caption": "",
                "position": 637
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x22.png",
                "caption": "",
                "position": 638
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x23.png",
                "caption": "Figure 11:L2subscriptùêø2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTNorm of advantages and Q-values during training for MA-PPO and vanilla PPO.Left:L2subscriptùêø2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTnorm of advantages over training steps;Right:L2subscriptùêø2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTnorm of Q-values.",
                "position": 659
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x23.png",
                "caption": "Figure 11:L2subscriptùêø2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTNorm of advantages and Q-values during training for MA-PPO and vanilla PPO.Left:L2subscriptùêø2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTnorm of advantages over training steps;Right:L2subscriptùêø2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTnorm of Q-values.",
                "position": 662
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x24.png",
                "caption": "",
                "position": 666
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitations",
        "images": []
    },
    {
        "header": "Appendix BExperimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02743/x25.png",
                "caption": "Figure 13:Illustration of four termination rules for macro actions in the MA-RLHF framework. Each termination rule outputs a list of|œâœÑ|subscriptùúîùúè|\\omega_{\\tau}|| italic_œâ start_POSTSUBSCRIPT italic_œÑ end_POSTSUBSCRIPT |. In the parsing based termination, the macro action is determined when the token number of the current node is less thanC=4ùê∂4C=4italic_C = 4, which is represented as a number in the tree node.",
                "position": 2362
            }
        ]
    },
    {
        "header": "Appendix CAdditional Experiments Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02743/x26.png",
                "caption": "Figure 14:Test RM scores evaluated by corresponding reward model of Gemma-2B and Gemma-7B model on HH-RLHF dataset.",
                "position": 2435
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x26.png",
                "caption": "Figure 14:Test RM scores evaluated by corresponding reward model of Gemma-2B and Gemma-7B model on HH-RLHF dataset.",
                "position": 2438
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x27.png",
                "caption": "",
                "position": 2442
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x28.png",
                "caption": "Figure 15:Distribution of test RM scores for vanilla PPO and MA-PPO on Gemma-2B using the HH-RLHF dataset.",
                "position": 2448
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x29.png",
                "caption": "Figure 16:Test RM scores evaluated by corresponding reward model of Gemma-2B and Gemma-7B model on the WebGPT Comparisons dataset.",
                "position": 2461
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x29.png",
                "caption": "Figure 16:Test RM scores evaluated by corresponding reward model of Gemma-2B and Gemma-7B model on the WebGPT Comparisons dataset.",
                "position": 2464
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x30.png",
                "caption": "",
                "position": 2468
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x31.png",
                "caption": "Figure 17:Distribution of test RM scores for vanilla PPO and MA-PPO on WebGPT dataset.",
                "position": 2474
            }
        ]
    },
    {
        "header": "Appendix DFurther Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02743/x32.png",
                "caption": "Figure 18:Illustration of value function of macro actions in MA-RLHF framework. It takes the outputs from the value function of tokens as input, and returns the value of macro actions with differentœÉœÑsubscriptùúéùúè\\sigma_{\\tau}italic_œÉ start_POSTSUBSCRIPT italic_œÑ end_POSTSUBSCRIPTassignment.",
                "position": 2530
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x33.png",
                "caption": "Figure 19:Performance of MA-PPO with different value function estimations in MA-PPO on TL;DR dataset for Gemma-2B model.Lefttest RM scores.RightGPT-4 scores on 4 dimensions.",
                "position": 2533
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x34.png",
                "caption": "",
                "position": 2536
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x35.png",
                "caption": "Figure 20:RM score shifting pattern after RLHF training.Leftpresents the RM score of best of 8 sampling on vanilla PPO compared to the vanilla PPO.Rightpresents the RM score of best of 8 sampling on MA-PPO compared to the MA-PPO.",
                "position": 2544
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x36.png",
                "caption": "",
                "position": 2547
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x37.png",
                "caption": "Figure 21:Test reward scores evaluated by the corresponding reward model for summarizations generated with different sampling temperature on the TL;DR dataset.",
                "position": 2551
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x38.png",
                "caption": "",
                "position": 2554
            },
            {
                "img": "https://arxiv.org/html/2410.02743/x39.png",
                "caption": "Figure 22:Illustration of the macro action-RLHF (MA-RLHF) framework.",
                "position": 2561
            }
        ]
    },
    {
        "header": "Appendix EMA-RLHF Algorithms",
        "images": []
    },
    {
        "header": "Appendix FEvaluation Details",
        "images": []
    },
    {
        "header": "Appendix GGenerated Examples",
        "images": []
    }
]