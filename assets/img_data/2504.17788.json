[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.17788/x1.png",
                "caption": "",
                "position": 109
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.17788/x2.png",
                "caption": "Figure 2:Panda-Test dataset statistics.\nStatistics reflect human labels on held-out 1K video Panda-Test set, detailed in §4.1.\nOnly 9% are target dynamic camera pose estimation videos due to various issues,e.g.static scene, low-quality or non-real content, and ambiguous or blurry frame of reference.\nWe focus on moving cameras to facilitate downstream taskse.g.camera-controlled video generation and learned pose estimation.\nWe remove unsuitable videos using a combination of specialist models and a generalist VLM.",
                "position": 200
            }
        ]
    },
    {
        "header": "3DynPose-100K: Dataset Curation",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.17788/x3.png",
                "caption": "Figure 3:Pose estimation approach.\nWe apply the state-of-the-art point tracking method at a sliding window to produce dense, long-term correspondences.\nComplementary dynamic masks are used to remove non-static tracks.\nThe remaining static tracks are provided as input to global bundle adjustment.",
                "position": 342
            },
            {
                "img": "https://arxiv.org/html/2504.17788/x4.png",
                "caption": "",
                "position": 426
            },
            {
                "img": "https://arxiv.org/html/2504.17788/x5.png",
                "caption": "",
                "position": 546
            }
        ]
    },
    {
        "header": "4DynPose-100K: Dataset Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.17788/x6.png",
                "caption": "Figure 5:Dynamic video filtering on Panda-Test.We show PR curves for baselines and ablations. Our filtering surpasses all baselines and ablations by a considerable margin.\nTherepresents DynPose-100K’s operating thresholds.\nFor baselines, we show:■■\\blacksquare■Reconstructed points (CamCo[93]),■■\\blacksquare■Reprojection error,\n(solid■■\\blacksquare■) GPT-4o mini[60]: binary,\n(dashed■■\\blacksquare■) GPT-4o mini[60]: score,■■\\blacksquare■Hands23[16], and■■\\blacksquare■Ours.\nFor ablations, we begin from■■\\blacksquare■Hands23 and add components until we recover■■\\blacksquare■Ours. Specifically, we depict:■■\\blacksquare■Hands23,■■\\blacksquare■+Flow,■■\\blacksquare■+Tracking,■■\\blacksquare■+Masking,■■\\blacksquare■+Focal,■■\\blacksquare■+Distort,■■\\blacksquare■+VLM (Ours).",
                "position": 583
            },
            {
                "img": "https://arxiv.org/html/2504.17788/x7.png",
                "caption": "Figure 6:Left: Targeted video length.DynPose-100K videos are primarily 4-10s, ideal for dynamic pose: shorter videos contain little ego-motion, longer videos have less dense dynamics and ego-motion.Right: Diverse dynamic apparent size.Mean size in % across video. Large dynamic objects occlude static correspondences, making pose estimation challenging. Videos may average small size in the case of only a few dynamic frames.",
                "position": 644
            },
            {
                "img": "https://arxiv.org/html/2504.17788/x8.png",
                "caption": "",
                "position": 655
            },
            {
                "img": "https://arxiv.org/html/2504.17788/x9.png",
                "caption": "Figure 7:Predicted trajectories on Lightspeed. Pose sequence over time:ROYGBV.\nWe visualize photorealistic renderings of Lightspeed left.\nStatic methods DROID-SLAM, COLMAP and DUSt3R struggle in this dynamic setting, failing to register a consistent sequence or putting too much or too little curvature.\nTop: dynamic methods MonST3R, LEAP-VO and ParticleSfM do not produce smooth sequences.\nBottom: MonST3R, LEAP-VO and COLMAP+Mask add curvature.\nOurs produces smooth and accurate trajectories.",
                "position": 808
            }
        ]
    },
    {
        "header": "5Evaluation on Camera Pose Estimation",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.17788/x10.png",
                "caption": "Figure 8:Predicted trajectories on Panda-Test.\nPose sequence over time:ROYGBV.\nStatic methods like DROID-SLAM, DUSt3R and COLMAP struggle faced with dynamics.\nMonST3R, LEAP-VO, COLMAP+Mask and ParticleSfM can struggle with large dynamic regions (top) and tracking across varied appearance and lighting (bottom); while Ours handles these cases.",
                "position": 999
            },
            {
                "img": "https://arxiv.org/html/2504.17788/x11.png",
                "caption": "Table 4:Camera estimation on Panda-Test.DynPose-100K fine-tuning of DUSt3R has lower mean error and similar to or better than accuracy compared to synthetic data (MonST3R). We train with only 2K videos / 140K frames, smaller than the 1.3M frames used to train MonST3R; demonstrating efficient supervision.",
                "position": 1061
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADynPose-100K: Dataset Curation Additional Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.17788/x12.png",
                "caption": "Figure 9:Panda-Test breakdown. We add more examples for each category, including further breakdown statistics for non estimable.\nStats reflect human labels on the 1K Panda-Test set, detailed in §B.1.",
                "position": 2711
            },
            {
                "img": "https://arxiv.org/html/2504.17788/x13.png",
                "caption": "Figure 10:Masking composition. Each component of masking contributes to final masks.\nUnique contributions for semantics, interaction and motion are circled in red.\nSemantic segmentation handles common dynamic objects such as humans (all examples).\nObject interaction handles things humans are manipulating such as paper (top) or accessories (bottom).\nMotion handles things moving not by human hands such as swiveling chairs (second from top) or flying snowballs (third from top).\nSometimes objects are partially segmented by one component but complementary components can still give a more complete mask.E.g.in the third example, flying snowballs are segmented by semantics but motion helps complete the dynamic mask.\nAll components are combined and tracked smoothly by propagation (right).",
                "position": 2857
            }
        ]
    },
    {
        "header": "Appendix BDynPose-100K: Dataset Analysis Additional Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.17788/x14.png",
                "caption": "Table 5:Expanded detail on video filtering on Panda-Test.Left is Figure5showing PR curves for baselines and ablations. Right displays average precision for these curves; along with precision at the threshold of 0.40 recall, corresponding to the operating threshold of DynPose-100K.",
                "position": 2920
            }
        ]
    },
    {
        "header": "Appendix CExperimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.17788/x15.png",
                "caption": "Figure 12:Additional Lightspeed videos. High resolution(2560,1440)25601440(2560,1440)( 2560 , 1440 )sequences span indoor and outdoor; light and dark; close-up dynamic object and far-away dynamic object; forward and backward movement.",
                "position": 3059
            },
            {
                "img": "https://arxiv.org/html/2504.17788/x16.png",
                "caption": "Figure 13:Panda-Test correspondence annotation. Human matches (pink points) provide coarse accuracy while SuperPoint+LightGlue[18,47]correspondences (green points and line) provide precision.\nWe search for SP+LG matches within 10 pixels of human correspondences on 720p frames (pink circle).\nIf no match exists, we do not include the match in testing.",
                "position": 3111
            }
        ]
    },
    {
        "header": "Appendix DDynPose-100K: Dataset Analysis Additional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.17788/x17.png",
                "caption": "Figure 14:Panda-Test filtering score samples. High scoring (top), moderate scoring (middle) and low scoring (bottom) examples from Panda-Test.\nGround Truth 1 indicates suitable video, 0 is unsuitable.\nThe minimum average score in DynPose-100K is 0.91, meaning all filters must produce a relatively high score.Gindicates correct classification based on 0.91 threshold;Rare sample failure cases.\nReasons for exclusion: R1C6: zoom-in, R2C1: static camera, R2C2: shot change, R2C4: not real, R2C5: static camera, R2C6: distortion, R3C1: ambiguous frame of ref, R3C2-C3: not real, R3C4-C5: insufficient clear static region, R2C6: long focal.",
                "position": 3154
            },
            {
                "img": "https://arxiv.org/html/2504.17788/x18.png",
                "caption": "Figure 15:Sample videos on DynPose-100K.\nDynPose-100K collects a diverse set of videos with challenging trajectories and dynamics. It pairs these videos with high-quality pose annotations.\nThe dataset is best viewed via the Supplemental video.",
                "position": 3162
            },
            {
                "img": "https://arxiv.org/html/2504.17788/x19.png",
                "caption": "Figure 16:Additional comparison on Lightspeed.\nOurs has more accurate poses than baselines in challenging settings.\nTop: a dynamic object is static relative to the camera and of similar color to the background. Bottom: dynamic object quickly moves by at night while camera moves and turns.\nIn both cases, baselines are either incorrect or do not have continuous, smooth trajectories.\nTop: MonST3R curves upward towards the end, while COLMAP+Mask has a large turn.",
                "position": 3168
            },
            {
                "img": "https://arxiv.org/html/2504.17788/x20.png",
                "caption": "Table 7:Dataset statistics.Top left: distribution of final horizontal rotation. Top right: distribution of final vertical rotation.\nBottom left: distribution of sequential sum of absolute value of horizontal rotation. Bottom right: distribution of sequential sum of absolute value of vertical rotation.",
                "position": 3258
            },
            {
                "img": "https://arxiv.org/html/2504.17788/x21.png",
                "caption": "",
                "position": 3265
            },
            {
                "img": "https://arxiv.org/html/2504.17788/x22.png",
                "caption": "",
                "position": 3269
            },
            {
                "img": "https://arxiv.org/html/2504.17788/x23.png",
                "caption": "",
                "position": 3271
            },
            {
                "img": "https://arxiv.org/html/2504.17788/x24.png",
                "caption": "Figure 17:Additional poses on Panda-Test.\nOurs produces sensible poses on a variety of videos, further validating quantitative results.",
                "position": 3433
            },
            {
                "img": "https://arxiv.org/html/2504.17788/x25.png",
                "caption": "Figure 18:Additional comparison on Panda-Test.\nOurs best handles challenging lighting and large dynamic objects (top), and handling scale variation resulting from moving very close to objects (bottom).\nIn the top sequence, Our trajectory is accurate, while alternatives do not reflect the consistent, fast and mostly straight backward movement of the camera.\nCOLMAP and COLMAP+Mask register most of the bottom sequence, but miss the movement at the end of the trajectory (down and to the left).",
                "position": 3438
            }
        ]
    },
    {
        "header": "Appendix EAdditional Pose Results",
        "images": []
    }
]