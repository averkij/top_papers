[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.00072/x1.png",
                "caption": "Figure 1:Chapter-Llama:Our method generates automatic video chapters for hour-long videos by training a large language model (LLM) to predict chapter boundaries and titles.\nThe LLM processes transcribed speech (ASR) and descriptive captions of key frames,\nwhich are sampled based on ASR content.\nThistext-basedapproach,equipped with speech-based frame selection,enables efficient processing of long-form content.",
                "position": 206
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Chapter-Llama:Â LLM-based Video Chaptering",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.00072/x2.png",
                "caption": "Figure 2:Method overview:Our Chapter-Llama framework first selects video frames to process using speech information. Then we use a visual captioner to map the selected frames in the text space. We feed the resulting captions, along with speech transcripts, to the LLM which outputs the chapter boundaries and titles jointly as a single sequence of tokens.",
                "position": 390
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.00072/x3.png",
                "caption": "Figure 3:Qualitative results:We display two examples\nand compare our Chapter-Llama results against the ground truth (GT),\nas well as the zero-shot (ZS) and Vid2Seq (VS) baselines.For each example, we show the corresponding SODA (S) and CIDEr (C) scores.Our method overall shows the highest similarity with the GT,\nwhile Vid2Seq can suffer from repeated chapter titles, and\nzero-shot generations tend to over-segment.",
                "position": 834
            },
            {
                "img": "https://arxiv.org/html/2504.00072/extracted/6324773/figures/datasize.png",
                "caption": "Figure 4:Amount of training data:Our experiments show a substantial improvement when moving from zero-shot to training with 1k videos.Beyond 1k videos, performance continues to improve but at a much slower rate,motivating our choice of using only 10k training videos for our final LLM.",
                "position": 1309
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BData Analysis and Statistics",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.00072/extracted/6324773/figures/supmat/video_durations_vs_chapters.png",
                "caption": "Figure A.1:Video duration distribution:Distribution of video durations in our training set (bars, left axis) and average number of chapters per duration bin (gray line, right axis).\nMost videos are less than 15 minutes long, with progressively fewer videos at longer durations.\nThe average number of chapters increases with video duration but plateaus around 13 chapters for videos longer than one hour.",
                "position": 3577
            },
            {
                "img": "https://arxiv.org/html/2504.00072/x4.png",
                "caption": "Figure A.2:Video category distribution:We compare the distribution of video categories\nbetween the\ntraining set of the\nfull\nVidChapters-7M dataset and\nour 20k training subset.\nWe observe similar distributions given our uniform sampling from the original training set.",
                "position": 3597
            }
        ]
    },
    {
        "header": "Appendix CAdditional Quantitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.00072/extracted/6324773/figures/supmat/number_of_chapters.png",
                "caption": "Figure A.3:Accuracy of number of chapter predictions:The violin plot shows the distribution of differences between\nthe predicted and ground truth number of chapters for three video chaptering models:\nChapter-Llama, Zero-shot, and Vid2Seq.\nThe Chapter-Llama model exhibits the most concentrated distribution centered around 0,\nindicating accurate number of chapter prediction.\nThe Zero-shot model tends to slightly overpredict the number of chapters,\nwhile the Vid2Seq model often significantly overpredicts the number of chapters.\nThe median differences are 0, 1, and 2 for Chapter-Llama, Zero-shot, and Vid2Seq, respectively, with mean number of chapter differences of -0.2, 0.5, and 4.5 (not shown).",
                "position": 4777
            },
            {
                "img": "https://arxiv.org/html/2504.00072/x5.png",
                "caption": "Figure A.4:Segmentation metrics visualization:We illustrate with examples how tIoU and F1 scores are calculated for video chaptering.\nThe top example shows a high-quality prediction with good overlap,\nwhile the bottom example demonstrates a lower-quality prediction with more misalignments.\nWe additionally show the corresponding SODA (S) and CIDEr (C) scores.",
                "position": 4832
            },
            {
                "img": "https://arxiv.org/html/2504.00072/x6.png",
                "caption": "Figure A.5:Visualizing captions:We provide an example with chapter predictions using the speech-based frame selection model,\nthe corresponding captions sampled,\nand the refined predictions produced by Chapter-Llama.\nWe additionally show the corresponding SODA (S) and CIDEr (C) scores.\nWe see that the initially predicted chapter at timestamp 02:00 is suppressed by Chapter-Llama.",
                "position": 4853
            },
            {
                "img": "https://arxiv.org/html/2504.00072/x7.png",
                "caption": "Figure A.6:Additional qualitative examples:We show two more examples of our Chapter-Llama predictions compared to the ground truth (GT).\nOur method generates accurate temporal boundaries and relevant chapter titles that align well with the video content.\nFor each example, we display the corresponding SODA (S) and CIDEr (C) scores.",
                "position": 4884
            },
            {
                "img": "https://arxiv.org/html/2504.00072/x8.png",
                "caption": "",
                "position": 4888
            },
            {
                "img": "https://arxiv.org/html/2504.00072/x9.png",
                "caption": "Figure A.7:Additional qualitative examples without ASR:We show three examples of videos without speech,\ncomparing our Chapter-Llama predictions to ground truth (GT).\nDespite lacking ASR,\nour method still produces reasonable chapters\nby leveraging visual cues and on-screen text when available\n(top and bottom examples).\nFor each example, we display the corresponding SODA (S) and CIDEr (C) scores.",
                "position": 4896
            },
            {
                "img": "https://arxiv.org/html/2504.00072/x10.png",
                "caption": "",
                "position": 4900
            }
        ]
    },
    {
        "header": "Appendix DAdditional Qualitative Analyses",
        "images": []
    }
]