[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18135/x1.png",
                "caption": "Figure 1:We introduce the first open benchmark to evaluate world models by closed-loop task success, analyze the link between task success and visual quality, and investigate scaling laws.",
                "position": 185
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18135/x2.png",
                "caption": "Figure 2:Task success rate vs. generation quality.‚Ä†\\dagger: post-trained with extra data.\nWe defend that world models live and die by their closed-loop success, not flawless generated visuals.",
                "position": 200
            }
        ]
    },
    {
        "header": "2World-in-World: a Closed-Loop Interface for Visual World Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18135/x3.png",
                "caption": "Figure 3:Closed-loop online planning in World-in-World:\nAt time steptt, the agent receives the world state, represented by observationùê®t\\mathbf{o}_{t}, and invokes a proposal policyœÄproposal\\pi_{\\text{proposal}}(‚ù∂) to produce a total ofMMcandidate action plans. The unified action API (‚ù∑) transforms each plan into the control inputs required by the world model. The world model (‚ù∏) then predicts the corresponding future states as observationsùêé^t\\hat{\\mathbf{O}}_{t}. The revision policyœÄrevision\\pi_{\\text{revision}}(‚ùπ) evaluates all rollouts and commits to the best, yielding decisionùêÉt‚ãÜ\\mathbf{D}^{\\star}_{t}. This decision is applied in the environment, closing the interaction loop.",
                "position": 249
            },
            {
                "img": "https://arxiv.org/html/2510.18135/x4.png",
                "caption": "Figure 4:Top-left: Active Recognition (AR), the agent needs to identify a designated target under occlusions or extreme viewpoints while minimizing navigation cost.Top-right: Image-Goal Navigation (ImageNav), the agent reaches the viewpoint matching a goal image, emphasizing success rate and path efficiency.Bottom-left: Active Embodied Question Answering (A-EQA), the agent answers an open-ended question after active exploration.Bottom-right: Robotic Manipulation, the agent needs to control a robotic arm to complete tasks such as grasping and placement to specified targets.",
                "position": 341
            }
        ]
    },
    {
        "header": "3Evaluation Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18135/x5.png",
                "caption": "Figure 5:(a)SR vs. generation quality in AR; generation quality is scored as the average of an aesthetic predictor(Akio Kodaira,2024)and an image-quality predictor(Ke et al.,2021), both trained to match human preferences.(b)SR vs. controllability in AR; controllability is quantified as1‚àíLPIPS1-\\mathrm{LPIPS}between ground-truth and predicted observations.",
                "position": 987
            },
            {
                "img": "https://arxiv.org/html/2510.18135/x6.png",
                "caption": "Figure 6:SR vs. seen examples during post-training.\nSR increases consistently with more downstream data, revealing a clear data-scaling trend for adaptation.",
                "position": 995
            },
            {
                "img": "https://arxiv.org/html/2510.18135/x6.png",
                "caption": "Figure 6:SR vs. seen examples during post-training.\nSR increases consistently with more downstream data, revealing a clear data-scaling trend for adaptation.",
                "position": 998
            },
            {
                "img": "https://arxiv.org/html/2510.18135/x7.png",
                "caption": "Figure 7:SR vs. average number of world-model inferences per episode.\nIncreasing the inference-time computation allocated to each decision step leads to higher SR.",
                "position": 1006
            }
        ]
    },
    {
        "header": "4Discussion and Future Directions",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelated Work",
        "images": []
    },
    {
        "header": "Appendix BEmbodied Task Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18135/x8.png",
                "caption": "Figure 8:In AR, the world model supports both queries (perception and planning). In this example, the agent must identify a wooden door that is initially visible only from an extreme viewpoint. For each candidate action sequence, the world model predicts future observations; these forecasts augment the agent‚Äôs perception and inform the choice of the next action.",
                "position": 2578
            },
            {
                "img": "https://arxiv.org/html/2510.18135/x9.png",
                "caption": "Figure 9:Overview of our embodied closed-loop evaluation for A-EQA. For each question, the high-level planner proposes multiple candidate action plans and queries the world model to generate the corresponding future observations. The agent then evaluates each plan together with its predicted observations and selects the plan that maximizes the expected reward before executing it in the environment.",
                "position": 2643
            },
            {
                "img": "https://arxiv.org/html/2510.18135/x10.png",
                "caption": "Figure 10:Illustration of theSet-of-Marks(SoM) representation that encodes candidate navigable directions. The high-level planner chooses among these discrete landmarks when constructing candidate action plans.",
                "position": 2651
            },
            {
                "img": "https://arxiv.org/html/2510.18135/x11.png",
                "caption": "Figure 11:Illustration of the auxiliary information provided to the VLM policy. The objects are marked with indices, and their positions are given to the VLM to facilitate decision-making.",
                "position": 2688
            }
        ]
    },
    {
        "header": "Appendix CPost-Training Recipe for Embodied World Models",
        "images": []
    },
    {
        "header": "Appendix DPost-Training Dataset Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18135/x12.png",
                "caption": "Figure 12:Top-down visualization of sampled waypoints in a scene.\nRed (left) and yellow (right) dots are the final waypoints after radius-based pruning.\nThe proposed strategy places waypoints throughout peripheral regions while avoiding redundant interior points, yielding diverse and spatially balanced trajectories.",
                "position": 3426
            }
        ]
    },
    {
        "header": "Appendix EVisualizing World Model Predictions",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18135/x13.png",
                "caption": "Figure 13:Examples of good and bad predictions. The action sequence contains onlyForwardactions. Models that violate this requirement yield observations that can mislead the planner.",
                "position": 3442
            },
            {
                "img": "https://arxiv.org/html/2510.18135/x14.png",
                "caption": "Figure 14:Examples of good and bad predictions. The action sequence contains onlyForwardactions. Models that violate this requirement yield observations that can mislead the planner.",
                "position": 3445
            },
            {
                "img": "https://arxiv.org/html/2510.18135/x15.png",
                "caption": "Figure 15:Additional examples of good and bad predictions.",
                "position": 3448
            },
            {
                "img": "https://arxiv.org/html/2510.18135/x16.png",
                "caption": "Figure 16:Additional examples of good and bad predictions.",
                "position": 3451
            },
            {
                "img": "https://arxiv.org/html/2510.18135/x17.png",
                "caption": "Figure 17:Additional examples of good and bad predictions.",
                "position": 3454
            },
            {
                "img": "https://arxiv.org/html/2510.18135/x18.png",
                "caption": "Figure 18:Additional examples of good and bad predictions.",
                "position": 3457
            }
        ]
    },
    {
        "header": "Appendix FPrompt Templates used in World-in-World",
        "images": []
    }
]