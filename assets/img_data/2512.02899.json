[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02899/Figures/icon.png",
                "caption": "",
                "position": 125
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02899/x1.png",
                "caption": "Figure 1:Comparison of data usage and training time.Glanceachieves comparable generation quality with only1 training samples and within 1 GPU-hour, demonstrating extreme data and compute efficiency. Note that the x-axis is in logarithmic scale, and values equal to zero are therefore not representable.",
                "position": 159
            },
            {
                "img": "https://arxiv.org/html/2512.02899/x2.png",
                "caption": "Figure 2:Comparison of distill and accelerate strategies.\nPrior distillation pipelines rely on large training sets and costly retraining.Glancerequires only one training sample to obtain Slow-LoRA and Fast-LoRA, providing plug-and-play acceleration of the base generation model.",
                "position": 165
            },
            {
                "img": "https://arxiv.org/html/2512.02899/x3.png",
                "caption": "Figure 3:Visualization of Slow-Fast paradigm.\nIn the slow stage, we sample one timestep every two steps from the first 20 timesteps (i.e., 5 samples in total).\nIn the fast stage, an additional 5 timesteps are uniformly sampled from the remaining 40 steps. During inference, the slow-stage timesteps are executed prior to the fast-stage ones.",
                "position": 225
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02899/x4.png",
                "caption": "Figure 4:Comparison between Image Generation Benchmarks. Glance further shows performance trajectories that closely follow those of the corresponding base models.",
                "position": 797
            },
            {
                "img": "https://arxiv.org/html/2512.02899/x5.png",
                "caption": "Figure 5:Visual comparison of different Slow–Fast configurations.All images are generated from the same initial noise using the 50-step base model, our 8/10-step students, and other few-step models.\nSlow-Fast preserves semantic fidelity under strong acceleration, while additional steps progressively enhance fine details.",
                "position": 800
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02899/x6.png",
                "caption": "Figure 6:Qualitative results from the one-sample training setting.\nEven trained on a single image, the model generalizes well to unseen prompts, producing coherent and detailed results across diverse scenes.",
                "position": 1086
            },
            {
                "img": "https://arxiv.org/html/2512.02899/x7.png",
                "caption": "Figure 7:Text-render failure cases.Glancestruggles on extremely small text, producing blurred or distorted characters.",
                "position": 1154
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix AQwen-Image-Edit task",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02899/x8.png",
                "caption": "Figure 8:Training and inference examples for the one-shot Qwen-Image-Edit adaptation.",
                "position": 1274
            }
        ]
    },
    {
        "header": "Appendix BRemote Sensing Domain Speedup",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02899/x9.png",
                "caption": "Figure 9:Qualitative results from the one-sample training setting.After observing only a single remote sensing example,Glanceadapts effectively and begins generating images that exhibit correct aerial viewpoints and characteristics consistent with real remote sensing imagery.",
                "position": 1310
            }
        ]
    },
    {
        "header": "Appendix CMore Implementation details",
        "images": []
    },
    {
        "header": "Appendix DMore Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02899/x10.png",
                "caption": "Figure 10:An uncurated random batch from the OneIG-Bench prompt set.",
                "position": 1360
            },
            {
                "img": "https://arxiv.org/html/2512.02899/x11.png",
                "caption": "Figure 11:An uncurated random batch from the HPSv2 prompt set.",
                "position": 1365
            },
            {
                "img": "https://arxiv.org/html/2512.02899/x12.png",
                "caption": "Figure 12:Visual comparison of different Slow–Fast configurations.All images are generated from the same initial noise using the 50-step base model, our 8/10-step students, and other few-step models.\nSlow-Fast preserves semantic fidelity under strong acceleration, while additional steps progressively enhance fine details.",
                "position": 1377
            },
            {
                "img": "https://arxiv.org/html/2512.02899/x13.png",
                "caption": "Figure 13:An uncurated random batch from the OneIG-Bench prompt set.",
                "position": 1384
            },
            {
                "img": "https://arxiv.org/html/2512.02899/x14.png",
                "caption": "Figure 14:An uncurated random batch from the DPG prompt set.",
                "position": 1389
            }
        ]
    },
    {
        "header": "Appendix EFuture Work",
        "images": []
    }
]