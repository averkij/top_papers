[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21496/x1.png",
                "caption": "Table 1:UI-Genie dataset statistics. UI-Genie-RM-517k is the first dedicated GUI agent reward dataset, while UI-Genie-Agent-16k contains synthetic trajectories without manual annotation.",
                "position": 124
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21496/x2.png",
                "caption": "Figure 2:Overview of UI-Genie-RM model and reward training data construction.The model processes task instruction, historical context, current screenshot, and candidate action as inputs.\nOutputs are supervised by both action-level and task-level rewards.\nThe training data are constructed by rule-based verification, trajectory corruption, and hard negative mining processes.",
                "position": 276
            },
            {
                "img": "https://arxiv.org/html/2505.21496/x3.png",
                "caption": "Figure 3:Self-improvement of agent and reward models for UI-Genie.It expands training sets for both agent and reward models through reward-guided trajectory exploration and outcome verification, then finetunes both models.\nThis process repeats iteratively to improve capabilities on increasingly complex tasks.",
                "position": 339
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21496/x4.png",
                "caption": "Table 7:Ablation study of UI-Genie-RM components. \"Hist.\" indicates whether historical information is included, \"# Imgs\" shows the number of historical screenshots incorporated, and \"Unified\" indicates whether we use the unified representation for both action and task rewards.",
                "position": 2786
            }
        ]
    },
    {
        "header": "5Discussions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AUI-Genie Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21496/x5.png",
                "caption": "Figure 5:Step-level reward evaluation prompt used for comparative baseline models.",
                "position": 3536
            },
            {
                "img": "https://arxiv.org/html/2505.21496/x6.png",
                "caption": "Figure 6:Outcome-level reward evaluation prompt used for comparative baseline models.",
                "position": 3539
            }
        ]
    },
    {
        "header": "Appendix BPrompt for Evaluation",
        "images": []
    },
    {
        "header": "Appendix CExample of UI-Genie Training Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21496/x7.png",
                "caption": "Figure 7:Examples of action-level reward data used for UI-Genie-RM training. The visual prompts displayed here are for illustration purposes only and are not used during model training. For simplicity, we omit the history image displays, though they are included in the actual training process.",
                "position": 3595
            },
            {
                "img": "https://arxiv.org/html/2505.21496/x8.png",
                "caption": "Figure 8:Examples of trajectory-level reward data showing successful and failed task completions. We omit the history of actions in this visualization, but the complete action sequences are utilized during training.",
                "position": 3598
            },
            {
                "img": "https://arxiv.org/html/2505.21496/extracted/6485728/figures/contacts_14.png",
                "caption": "Figure 9:A successful trajectory example. Under the reward guidance of UI-Genie-RM, the agent successfully discovers and executes a sequence of actions that complete the assigned task.",
                "position": 3610
            },
            {
                "img": "https://arxiv.org/html/2505.21496/extracted/6485728/figures/settings_2.png",
                "caption": "Figure 10:A failure trajectory example. Despite receiving process reward guidance at each individual step, the sequence ultimately results in task failure as indicated by the negative outcome reward.",
                "position": 3613
            },
            {
                "img": "https://arxiv.org/html/2505.21496/extracted/6485728/figures/bluecoins_11.png",
                "caption": "Figure 11:Example of UI-Genie-Agent-72B executing an AndroidLab task. The task instruction: Adjust the expenditure on May 15, 2024, to 500 CNY.",
                "position": 3628
            },
            {
                "img": "https://arxiv.org/html/2505.21496/extracted/6485728/figures/calendar_5.png",
                "caption": "Figure 12:Example of UI-Genie-Agent-72B executing an AndroidLab task. The task instruction: Edit the event with title “work”, change the end time to be 7:00 PM.",
                "position": 3631
            },
            {
                "img": "https://arxiv.org/html/2505.21496/extracted/6485728/figures/task_138.png",
                "caption": "Figure 13:Example of UI-Genie-Agent-7B executing a task defined in Android Arena (A3). The task instruction: Search news about ‘panda’ in CNN.",
                "position": 3642
            },
            {
                "img": "https://arxiv.org/html/2505.21496/extracted/6485728/figures/task_149.png",
                "caption": "Figure 14:Example of UI-Genie-Agent-7B executing a task defined in Android Arena (A3). The task instruction: Open ‘settings’ in Coursera and switch to dark mode.",
                "position": 3645
            }
        ]
    },
    {
        "header": "Appendix DUI-Genie Trajectory Example on Online Evaluation",
        "images": []
    }
]