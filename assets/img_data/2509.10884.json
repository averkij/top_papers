[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.10884/figs/navr1_logo.png",
                "caption": "",
                "position": 125
            }
        ]
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.10884/x1.png",
                "caption": "Figure 2:Architecture of Nav-R1.Nav-R1 designs a Fast-in-Slow reasoning paradigm that processes egocentric RGB-D views, scene point cloud, and language instructions. The slow system performs long-horizon semantic reasoning, while the fast system executes real-time navigation, enabling coherent reasoning and low-latency control in embodied environments.",
                "position": 315
            }
        ]
    },
    {
        "header": "IIRelated Work",
        "images": []
    },
    {
        "header": "IIIDatasets",
        "images": []
    },
    {
        "header": "IVThe Proposed Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.10884/x2.png",
                "caption": "Figure 3:CoT Data Engine.We construct the Nav-CoT dataset by defining navigation instructions, integrating egocentric visual inputs, providing action options and specifying the output format. These components are fed into Gemini 2.5 Pro, which generates step-by-step reasoning and action decisions aligned with navigation goals.",
                "position": 412
            },
            {
                "img": "https://arxiv.org/html/2509.10884/x3.png",
                "caption": "Figure 4:The pipeline of RL Policy.The policy model generatesNNoutputs from text-image input. Then understanding reward (answer correctness and semantic alignment), navigation reward (path fidelity and endpoint accuracy), and format reward (structure adherence) are computed, grouped, and combined with a KL term to a frozen reference model to update the policy.",
                "position": 424
            }
        ]
    },
    {
        "header": "VExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.10884/x4.png",
                "caption": "Figure 5:Real-world robot setup and deployment pipeline.(a) Hardware platform: the WHEELTEC R550 robot equipped with Jetson Orin Nano (on-board PC), M10P LiDAR for mapping, Astra Pro RGB-D camera for perception, and STM32 microcontroller for motor control.\n(b) Deployment process: egocentric visual inputs are transmitted to the embodied foundation model Nav-R1, which performs reasoning and navigation. The decisions are then sent back to the on-board PC and converted into low-level motor commands by the STM32 controller.",
                "position": 1569
            },
            {
                "img": "https://arxiv.org/html/2509.10884/x5.png",
                "caption": "Figure 6:Qualitative results from the real-world deployment of Nav-R1.We evaluate the agent in three indoor scenarios: meeting room, lounge, and corridor. Each scene illustrates the BEV trajectory and ego-centric video frames, showing the modelâ€™s ability to generalize to diverse layouts and object configurations in real-world environments.",
                "position": 1575
            }
        ]
    },
    {
        "header": "VITest-Time Efficiency",
        "images": []
    },
    {
        "header": "VIIConclusion",
        "images": []
    },
    {
        "header": "APPENDIX",
        "images": []
    },
    {
        "header": "VIIIAblation Study",
        "images": []
    },
    {
        "header": "IXLimitation and Future Work",
        "images": []
    },
    {
        "header": "XVisualization",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.10884/x6.png",
                "caption": "Figure 7:Nav-CoT-110K CoT data example.",
                "position": 1928
            },
            {
                "img": "https://arxiv.org/html/2509.10884/x7.png",
                "caption": "Figure 8:Real-world qualitative results of Nav-R1 on VLN and ObjectNav tasks in meeting room.",
                "position": 1932
            },
            {
                "img": "https://arxiv.org/html/2509.10884/x8.png",
                "caption": "Figure 9:Real-world qualitative results of Nav-R1 on VLN and ObjectNav tasks in lounge.",
                "position": 1936
            },
            {
                "img": "https://arxiv.org/html/2509.10884/x9.png",
                "caption": "Figure 10:Real-world qualitative results of Nav-R1 on VLN and ObjectNav tasks in corridor.",
                "position": 1940
            },
            {
                "img": "https://arxiv.org/html/2509.10884/x10.png",
                "caption": "Figure 11:Real-world qualitative results of Nav-R1 on embodied dialogue task.",
                "position": 1944
            },
            {
                "img": "https://arxiv.org/html/2509.10884/x11.png",
                "caption": "Figure 12:Real-world qualitative results of Nav-R1 on embodied reasoning task.",
                "position": 1948
            },
            {
                "img": "https://arxiv.org/html/2509.10884/x12.png",
                "caption": "Figure 13:Real-world qualitative results of Nav-R1 on embodied planning task.",
                "position": 1952
            },
            {
                "img": "https://arxiv.org/html/2509.10884/x13.png",
                "caption": "Figure 14:Visual results of VLN on VLN-CE R2R.",
                "position": 1956
            },
            {
                "img": "https://arxiv.org/html/2509.10884/x14.png",
                "caption": "Figure 15:Visual results of VLN on VLN-CE R2R.",
                "position": 1960
            },
            {
                "img": "https://arxiv.org/html/2509.10884/x15.png",
                "caption": "Figure 16:Visual results of ObjectNav on HM3D.",
                "position": 1964
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]