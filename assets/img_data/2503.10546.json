[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10546/x1.png",
                "caption": "",
                "position": 67
            }
        ]
    },
    {
        "header": "IIntroduction",
        "images": []
    },
    {
        "header": "IIRelated Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10546/x2.png",
                "caption": "Figure 2:Overview of KUDA. Taking the RGBD observations and a language instruction as inputs, we first utilize the large vision model to obtain the keypoints and label them on the RGB image to obtain the visual prompt (green dot C marks the center reference point). Next, the vision-language model generates code for target specifications, which are projected into 3D space to construct the 3D objectives. Lastly, we utilize the pre-trained dynamics model for model-based planning. After a certain number of actions, the VLM is re-queried with the current observation, enabling high-level closed-loop planning to correct VLM and execution errors.",
                "position": 158
            }
        ]
    },
    {
        "header": "IIIMethod",
        "images": []
    },
    {
        "header": "IVExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10546/x3.png",
                "caption": "Figure 3:Qualitative Results of the Rollouts.We show the target specification and robot executions of various tasks on different objects, highlight the effectiveness of our framework. We show the initial state and the target specification visualization of our system, along with the robot executions, to demonstrate the performance of our framework on various manipulation tasks. Note that we show the granular collection task to exhibit how our VLM-level closed-loop control works in our two VLM-level loops.",
                "position": 264
            },
            {
                "img": "https://arxiv.org/html/2503.10546/x4.png",
                "caption": "Figure 4:Visualizations of Error Breakdown.We provide a detailed breakdown of each failure mode, marked in red. While we achieved an80%percent8080\\%80 %success rate across 60 trials for various tasks, the primary cause of failure was perception errors, accounting for10%percent1010\\%10 %of all trials and50%percent5050\\%50 %of the failure cases.",
                "position": 293
            }
        ]
    },
    {
        "header": "VConclusion & Limitations",
        "images": []
    },
    {
        "header": "VIAcknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]