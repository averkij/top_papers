[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.11456/x1.png",
                "caption": "(a)Difficulty Levels",
                "position": 178
            },
            {
                "img": "https://arxiv.org/html/2504.11456/x1.png",
                "caption": "(a)Difficulty Levels",
                "position": 181
            },
            {
                "img": "https://arxiv.org/html/2504.11456/x2.png",
                "caption": "(b)Improvement of SFT and RL using DeepMath-103K",
                "position": 186
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Overview of DeepMath-103K",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.11456/x3.png",
                "caption": "Figure 2:An example data sample from the DeepMath-103K dataset, illustrating its components.",
                "position": 240
            },
            {
                "img": "https://arxiv.org/html/2504.11456/x4.png",
                "caption": "Figure 3:Hierarchical breakdown of covered mathematical topics in DeepMath-103K.",
                "position": 286
            },
            {
                "img": "https://arxiv.org/html/2504.11456/x5.png",
                "caption": "Figure 4:Contamination rates of common mathematical and STEM benchmarks detected in the raw data sources before decontamination.",
                "position": 295
            }
        ]
    },
    {
        "header": "3Construction of DeepMath-103K",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.11456/x6.png",
                "caption": "Figure 5:The data curation pipeline for DeepMath-103K. Starting with an initial pool of 2,869K raw questions, successive stages of data decontamination, difficulty filtering (retaining levels≥\\geq≥5), and answer verifiability filtering yield 95K problems. These are then combined with 8K problems from SimpleRL(Zeng et al.,2025b)to form the final DeepMath-103K dataset.",
                "position": 343
            },
            {
                "img": "https://arxiv.org/html/2504.11456/x7.png",
                "caption": "Figure 6:Difficulty distributions of various open mathematical reasoning datasets considered as potential sources.",
                "position": 376
            }
        ]
    },
    {
        "header": "4Effectiveness of DeepMath-103K",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.11456/x8.png",
                "caption": "Table 3:Results of RL and SFT using different training datasets. “DeepMath” denotes models trained using DeepMath-103K. Performance is evaluated using pass@1 (n=16) accuracy. We also list the results of Qwen2.5-Math-7B-Instruct for reference.",
                "position": 627
            },
            {
                "img": "https://arxiv.org/html/2504.11456/x8.png",
                "caption": "Figure 7:Average reasoning length (number of tokens) on different benchmarks for models trained with RL-Zero using various datasets, starting from Qwen-2.5-7B-Base.",
                "position": 801
            },
            {
                "img": "https://arxiv.org/html/2504.11456/x9.png",
                "caption": "Figure 8:Variance (change from initial state) of beneficial cognitive behaviors observed during RL-Zero training on DeepMath-103K.",
                "position": 809
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]