[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19324/x1.png",
                "caption": "Figure 1:Trajectory attentioninjects partial motion information by making content along trajectories consistent. It facilitates various tasks such as camera motion control on images and videos, and first-frame-guided video editing. Yellow boxes indicate reference contents. Greenboxes indicate input frames.Blueboxes indicate output frames.",
                "position": 130
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19324/x2.png",
                "caption": "Figure 2:Attention map visualization of temporal attention and trajectory attention.(a) Temporal attention tends to concentrate its weight on a narrow, adjacent frame window. (b) In contrast, trajectory attention exhibits a broader attention window, highlighting its capacity to produce more consistent and controllable results. Here, the attention map is structured with the frame number as the side length. The attention weights are normalized within the range of 0 to 1, where higher values (indicated by light yellow) represent stronger attention.",
                "position": 190
            },
            {
                "img": "https://arxiv.org/html/2411.19324/x3.png",
                "caption": "Figure 3:Overview of the proposed motion control pipeline.Our method allows for conditioning on trajectories from various sources – such as camera motion derived from a single image, as shown in this figure. We inject these conditions into the model through trajectory attention, enabling explicit and fine-grained control over the motion in the generated video.",
                "position": 193
            },
            {
                "img": "https://arxiv.org/html/2411.19324/x4.png",
                "caption": "Figure 4:Visualization of vanilla temporal attention and trajectory attention.",
                "position": 209
            },
            {
                "img": "https://arxiv.org/html/2411.19324/x5.png",
                "caption": "Figure 5:Training strategy for trajectory attention.To leverage the motion modeling capability learned from large-scale data, we initialize the weights of the QKV projectors with those from temporal attention layers. Additionally, the output projector is initialized with zero weights to ensure a smooth and gradual training process.",
                "position": 367
            }
        ]
    },
    {
        "header": "4Fine-grained Control of Video Generation",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19324/x6.png",
                "caption": "Figure 6:Qualitative comparisons for camera motion control on images. While other methods often exhibit significant quality degradation or inconsistencies in camera motion, our approach consistently delivers high-quality results with precise, fine-grained control over camera movements. Regions are highlighted in yellow boxes to reveal camera motion. For a more comprehensive understanding, we highly recommend viewing the accompanying videos in the supplementary materials.",
                "position": 558
            },
            {
                "img": "https://arxiv.org/html/2411.19324/x7.png",
                "caption": "Figure 7:Qualitative comparisons for camera motion control on videos.In the second row, we provide video frames after view warping as a reference. Methods like NVS Solver(You et al.,2024)use frame-wise information injection but overlook temporal continuity, leading to inconsistent motion control, especially in frames farther from the first one. In contrast, our approach explicitly models attention across frames, which significantly benefits control precision. We highlight the control precision with yellow boxes, where our method aligns better with the reference. *: we integrate NVS Solver’s capability to inject frame-wise information, achieving better video alignment with the original videos.",
                "position": 596
            },
            {
                "img": "https://arxiv.org/html/2411.19324/x8.png",
                "caption": "Figure 8:Results on first-frame guided video editing.We compare our method with those fromOuyang et al. (2024); Ku et al. (2024). The results show that other methods struggle to maintain consistency after editing. In contrast, our method successfully preserves the edited features across frames, thanks to its ability to model trajectory consistency throughout the video.",
                "position": 599
            },
            {
                "img": "https://arxiv.org/html/2411.19324/x9.png",
                "caption": "Figure 9:Qualitative results on Open-Sora-Plan.(Lab & etc.,2024)By incorporating trajectory attention into the 3D attention module, we successfully enable camera motion control.",
                "position": 697
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19324/x10.png",
                "caption": "Figure 10:Pipe for video diffusion models with 3D Attention.The key distinction with the pipeline in the main paper lies in applying trajectory attention to the 3D attention module, rather than to the temporal attention mechanism.",
                "position": 1541
            },
            {
                "img": "https://arxiv.org/html/2411.19324/extracted/6032239/figures/depthmap.png",
                "caption": "Figure 11:Depth estimation results.",
                "position": 1607
            },
            {
                "img": "https://arxiv.org/html/2411.19324/x11.png",
                "caption": "Figure 12:Point trajectory estimation results.",
                "position": 1707
            },
            {
                "img": "https://arxiv.org/html/2411.19324/x12.png",
                "caption": "Figure 13:Input process visualization. For all tasks, the inputs to the network are the first frame and the extracted trajectories. The usage of the first frame and the trajectories are identical to Fig. 3 in the main paper. The wrapped frames and the reference frames will not be used as inputs to the generation network.",
                "position": 1770
            },
            {
                "img": "https://arxiv.org/html/2411.19324/x13.png",
                "caption": "Figure 14:Using synthetic optical flow as guidance. Our method supports directly using optical flow to guide generation. Blue boxes indicate the optical flow. Yellow boxes indicate the reference image.",
                "position": 1780
            },
            {
                "img": "https://arxiv.org/html/2411.19324/x14.png",
                "caption": "Figure 15:Examples of challenging situations.Our method effectively addresses complex scenarios, including (a) video editing involving multiple objects, (b) video editing in the presence of occlusions, (c) diverse and rapid camera movements, such as zooming in and out, as well as clockwise rotations, and (d) video editing with distinct object categories. Please note that yellow boxes indicate reference videos, green boxes indicate input frames and blue boxes indicate output results.",
                "position": 1790
            },
            {
                "img": "https://arxiv.org/html/2411.19324/x15.png",
                "caption": "Figure 16:Visualization of camera trajectories.The first row displays the estimated trajectories from our generation alongside the ground truth trajectories. The second row presents the estimated trajectories from CameraCtrl (denoted as “CC”) compared to the ground truth. The results indicate that our method aligns significantly better with the ground truth camera motion trajectories.",
                "position": 1810
            },
            {
                "img": "https://arxiv.org/html/2411.19324/x16.png",
                "caption": "Figure 17:Results on sparse trajectories. In (a), we show that trajectory attention remains robust even with relatively sparse trajectories. Even when the trajectory density is reduced to 1/16 of the original video resolution, it still performs well in motion control. In (b), we apply the trajectory mask to selectively use only a portion of the trajectories, keeping the regions outside the mask static. The model accurately follows the motion within the small masked area.(c) If we apply sparse trajectories control to a specific region (i.e., the dog region), the output results are more dynamic.Best viewed on the attached HTML file.",
                "position": 1820
            },
            {
                "img": "https://arxiv.org/html/2411.19324/x17.png",
                "caption": "Figure 18:Applications on drag signals.Trajectory attention supports hand-crafted dragging trajectory. Row 1: origin videos. Row 2: dragged results.",
                "position": 1823
            },
            {
                "img": "https://arxiv.org/html/2411.19324/x18.png",
                "caption": "Figure 19:Visualization on the training data.The training data includes origin frames, predicted optical flow, and occlusion masks.",
                "position": 1826
            },
            {
                "img": "https://arxiv.org/html/2411.19324/x19.png",
                "caption": "Figure 20:Visualization on failure cases.Our method encounters challenges when dealing with extremely fast motions as well as complex and difficult-to-estimate motion patterns.",
                "position": 1836
            },
            {
                "img": "https://arxiv.org/html/2411.19324/x20.png",
                "caption": "Figure 21:Visualization on different intrinsic parameters.",
                "position": 1839
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]