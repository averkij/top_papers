[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.00615/x1.png",
                "caption": "Figure 1:Accuracy–Peak tokens trade-offon AppWorld(Trivedi et al.,2024). We compare average accuracy versus peak input tokens in history compression.Acon(ours) reduces cost while preserving accuracy for the large model (gpt-4.1) relative to a naive prompting baseline, and even improves accuracy on smaller models (gpt-4.1-mini and Qwen-14B). More results are inSection 4.",
                "position": 167
            },
            {
                "img": "https://arxiv.org/html/2510.00615/x2.png",
                "caption": "Figure 2:Motivation: Unbounded context in LLM agents.As LLM agents interact with environments, actions and observations continuously accumulate, leading to ever-growing contexts that incur high memory usage as in the red line on the right plot. This motivatesAgent Context Optimization(Acon), which optimally compresses histories and observations into concise summaries, reducing peak tokens and memory as in the blue line on the right plot.",
                "position": 173
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Agent Context Optimization (Acon)",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.00615/x3.png",
                "caption": "Figure 3:Compression Guideline Optimization.Feedback is generated by contrasting successful trajectories (no compression) with failed ones (with compression). The collected feedback is then used by LLM to refine the compression guidelines.",
                "position": 386
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.00615/x4.png",
                "caption": "Figure 4:Results of distilled compressors on history compressionwith gpt-4.1 as the agent. Student models (Qwen3-14B, Qwen3-8B, Phi-4) are distilled from gpt-4.1 compressor using the optimized compression guideline afterut¯¯\\overline{\\underline{\\textsc{ut}}}step, and evaluated across all benchmarks. We also include result with gpt-4.1-mini without distillation for comparison.",
                "position": 1016
            },
            {
                "img": "https://arxiv.org/html/2510.00615/x5.png",
                "caption": "Figure 5:Performance-efficiency trade-off of the Qwen3-14B agentdistilled from gpt-4.1 trajectories. For distilled compressors, we use the same distillation setting as inFigure 4. Compared to the baseline without compression, our frameworkAconprovides compressed trajectories combined with a distilled compressor, enabling the distilled agent to achieve consistently higher accuracy while requiring substantially fewer peak input tokens across all benchmarks.",
                "position": 1029
            },
            {
                "img": "https://arxiv.org/html/2510.00615/x6.png",
                "caption": "Figure 6:Ablation studies on thresholds for compressionon AppWorld with gpt-4.1. (1) the number of compressions (compression frequency) for each length of task trajectories (task steps). (2) the performance comparison for each threshold setting.",
                "position": 1041
            },
            {
                "img": "https://arxiv.org/html/2510.00615/x6.png",
                "caption": "",
                "position": 1044
            },
            {
                "img": "https://arxiv.org/html/2510.00615/x7.png",
                "caption": "",
                "position": 1048
            },
            {
                "img": "https://arxiv.org/html/2510.00615/x8.png",
                "caption": "",
                "position": 1052
            },
            {
                "img": "https://arxiv.org/html/2510.00615/x9.png",
                "caption": "Table 3:Ablation studies on the prompt optimizerin AppWorld, gpt-4.1 agent and history compressor. Default is o3 optimizer with task contrastive feedback.",
                "position": 1058
            },
            {
                "img": "https://arxiv.org/html/2510.00615/x9.png",
                "caption": "Figure 7:API cost comparison.H denotes history and O denotes observation compression.",
                "position": 1099
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Reproducible Statements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitations & Future Work",
        "images": []
    },
    {
        "header": "Appendix BExperimental Setup Details",
        "images": []
    },
    {
        "header": "Appendix CAdditional Results",
        "images": []
    },
    {
        "header": "Appendix DQualitative Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.00615/x10.png",
                "caption": "Figure 8:Results of distilled compressors on observation compressionwith gpt-4.1 as the agent. Student models (Qwen3-14B, Qwen3-8B, Phi-4) are distilled from gpt-4.1 compressor using the optimized compression guideline afterut¯¯\\overline{\\underline{\\textsc{ut}}}step, and evaluated across all benchmarks. We also include result with gpt-4.1-mini without distillation for comparison.",
                "position": 3947
            }
        ]
    },
    {
        "header": "Appendix ELLM Usage",
        "images": []
    }
]