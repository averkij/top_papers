[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13198/x1.png",
                "caption": "Figure 1:Comparison of traditional GEC and DARAG. We augment the training dataset with synthetic data generated using our algorithm and named entities retrieved from a datastore to improve in-domain and out-of-domain ASR.",
                "position": 136
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13198/x2.png",
                "caption": "Figure 2:Illustration of DARAG. \\raisebox{-0.9pt}{1}⃝ We generate synthetic data with LLMs and TTS models that are then used to generate hypotheses with diverse errors consistent with the types the ASR model generates on the test set. \\raisebox{-0.9pt}{2}⃝ We extract the NEs and store them in a datastore. During training, for every instance, we retrieve the top-kmost similar NEs to the best hypothesis and use it to construct an instruction-response pair.Note that in OOD settings we only assume the availability of only a few unsupervised speech samples in the original train set and pseudo-transcripts for prompting are generated using the in-domain ASR model.",
                "position": 390
            }
        ]
    },
    {
        "header": "4Methodology",
        "images": []
    },
    {
        "header": "5Experimental Setup",
        "images": []
    },
    {
        "header": "6Results and Analysis",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    },
    {
        "header": "Appendix BPrompts",
        "images": []
    },
    {
        "header": "Appendix CAre LLMs Just Replicating the Original Training Data?",
        "images": []
    },
    {
        "header": "Appendix DDARAG w/o Voice Cloning",
        "images": []
    },
    {
        "header": "Appendix EIn-Domain Performance in Out-of-Domain Settings",
        "images": []
    },
    {
        "header": "Appendix FHyper-parameter Tuning",
        "images": []
    },
    {
        "header": "Appendix GExamples of Generated Transcripts",
        "images": []
    },
    {
        "header": "Appendix HExamples of DARAG Corrections",
        "images": []
    },
    {
        "header": "Appendix IAdditional Details",
        "images": []
    }
]