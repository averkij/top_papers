[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IINTRODUCTION",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.17502/figures/SocialMAE2.png",
                "caption": "Figure 1:Social-MAE model for voice and face analysis in videos. The model is pre-trained to reconstruct audio and visual modalities from masked portions of their corresponding input, narrowing the difference between each modality representation.",
                "position": 102
            }
        ]
    },
    {
        "header": "IIMETHOD",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.17502/figures/Reconstruction_CR.png",
                "caption": "((a))Reconstruction on CREMA-D.",
                "position": 160
            },
            {
                "img": "https://arxiv.org/html/2508.17502/figures/Reconstruction_CR.png",
                "caption": "((a))Reconstruction on CREMA-D.",
                "position": 163
            },
            {
                "img": "https://arxiv.org/html/2508.17502/figures/Reconstruction_FI.png",
                "caption": "((b))Reconstruction on ChaLearn First Impressions.",
                "position": 168
            }
        ]
    },
    {
        "header": "IIIEXPERIMENTS AND RESULTS",
        "images": []
    },
    {
        "header": "IVCONCLUSIONS",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]