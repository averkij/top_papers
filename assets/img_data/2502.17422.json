[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17422/extracted/6218004/figures/usc_logo.png",
                "caption": "",
                "position": 45
            },
            {
                "img": "https://arxiv.org/html/2502.17422/extracted/6218004/figures/vu_logo.png",
                "caption": "",
                "position": 60
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17422/x1.png",
                "caption": "Figure 1:The effect of visual cropping on the probability of answers predicted by BLIP-2 FlanT5XLzero-shot VQA model. The x-axis labels are indices to the respective cropped images displayed under each plot that the model sees at each step. The model gradually finds the correct answer.",
                "position": 98
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3MLLMs’ Sensitivity to the Size of Visual Concepts",
        "images": []
    },
    {
        "header": "4Do MLLMs Know Where to Look?",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17422/x2.png",
                "caption": "Figure 2:Examples of MLLMs knowing where to look despite answering incorrectly. The right panel in each example displays relative attention to the image (defined inSec.4) of one layer in the MLLM.",
                "position": 242
            },
            {
                "img": "https://arxiv.org/html/2502.17422/x3.png",
                "caption": "Figure 3:MLLMs’ attention ratio across all layers (average with95%percent9595\\%95 %CI over TextVQA). The attention ratio measures how significantly the MLLM is attending to the ground-truth bounding box (defined inSec.4). We observe that it is greater than 1 in most layers, showing that the MLLMs know where to look in the image even when they fail to answer correctly.",
                "position": 245
            }
        ]
    },
    {
        "header": "5Automatic Visual Cropping (ViCrop)",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17422/x4.png",
                "caption": "Figure 4:Illustration of the proposed visual cropping approach applied to two MLLMs.",
                "position": 261
            }
        ]
    },
    {
        "header": "6ViCrop Method Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17422/x5.png",
                "caption": "Figure 5:Examples ofrel-atthelping MLLMs correct their mistakes (cyan-colored bounding box shows cropped region byrel-att; zoom-in insets are displayed for better readability).",
                "position": 299
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BDataset Statistics",
        "images": []
    },
    {
        "header": "Appendix CPrompt Format for Zero-shot Inference",
        "images": []
    },
    {
        "header": "Appendix DOrthogonal Benefits to LLaVA-NeXT",
        "images": []
    },
    {
        "header": "Appendix EComparison with the V* method (SEAL)",
        "images": []
    },
    {
        "header": "Appendix FExternal Tools ViCrop",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17422/x6.png",
                "caption": "Figure 6:Success (first 3) and failure (last) examples of LLaVA-1.5 (rel-att) on the V∗benchmark (cyan-colored bounding box shows cropped region byrel-att; zoom-in insets are displayed for better readability).",
                "position": 1659
            },
            {
                "img": "https://arxiv.org/html/2502.17422/x7.png",
                "caption": "Figure 7:Success (first 9) and failure (last 6) examples of LLaVA-1.5 (rel-att) on the TextVQA benchmark (cyan-colored bounding box shows cropped region byrel-att).",
                "position": 1662
            },
            {
                "img": "https://arxiv.org/html/2502.17422/x8.png",
                "caption": "Figure 8:Success (first 9) and failure (last 6) examples of InstructBLIP (rel-att) on the TextVQA benchmark (cyan-colored bounding box shows cropped region byrel-att).",
                "position": 1665
            }
        ]
    },
    {
        "header": "Appendix GAdditional Examples on Model’s Predictions",
        "images": []
    }
]