[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background and Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.08665/extracted/6435937/images/SkillFormer.jpg",
                "caption": "Figure 1:Overview of the SkillFormer architecture. Multi-view video inputs (one egocentric and up to four exocentric) are processed through a shared TimeSformer backbone fine-tuned with LoRA. Features are fused using the CrossViewFusion module and passed to a classification head.",
                "position": 121
            }
        ]
    },
    {
        "header": "3Proposed Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.08665/extracted/6435937/images/CrossViewFusion.jpg",
                "caption": "Figure 2:Architecture of the CrossViewFusion module. Input features from multiple views(B,V,input_dim)ùêµùëâinput_dim(B,V,\\text{input\\_dim})( italic_B , italic_V , input_dim )are first normalized and integrated using multi-head attention. The fused representation passes through a feed-forward network with residual connections and a learnable gating mechanism. Final projections and normalization are followed by adaptive self-calibration using learned mean and variance parameters, producing output features of shape(B,out_dim)ùêµout_dim(B,\\text{out\\_dim})( italic_B , out_dim ).",
                "position": 150
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Results",
        "images": []
    },
    {
        "header": "6Limitations and Future Work",
        "images": []
    },
    {
        "header": "7Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]