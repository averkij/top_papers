[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02358/x1.png",
                "caption": "",
                "position": 150
            },
            {
                "img": "https://arxiv.org/html/2601.02358/figures/rocket.png",
                "caption": "",
                "position": 161
            },
            {
                "img": "https://arxiv.org/html/2601.02358/figures/github-logo.png",
                "caption": "",
                "position": 163
            },
            {
                "img": "https://arxiv.org/html/2601.02358/x2.png",
                "caption": "Figure 1:Showcase of VINO in image generation and image editing.",
                "position": 167
            },
            {
                "img": "https://arxiv.org/html/2601.02358/x3.png",
                "caption": "Figure 2:Showcase of VINO in video generation and video editing.",
                "position": 170
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02358/x4.png",
                "caption": "Figure 3:Overview of the VINO pipeline.Our unified framework conditions generation on an interleaved omnimodal context that jointly encodes system prompts, prompts/instructions, reference images/videos, and learnable tokens.\nA frozen VLM processes textual instructions together with visual references, producing multimodal embeddings that are augmented with learnable tokens (purple) and separated by special tokens (vision start tokenand vision end token).\nThese interleaved multimodal representations are fed into the MMDiT blocks, which also receive VAE latents from the reference images or video.\nThe MMDiT model performs denoising conditioned on the full multimodal context, enabling VINO to execute image and video generation as well as instruction-based editing within a single unified architecture.",
                "position": 251
            },
            {
                "img": "https://arxiv.org/html/2601.02358/figures/st.png",
                "caption": "",
                "position": 254
            },
            {
                "img": "https://arxiv.org/html/2601.02358/figures/ed.png",
                "caption": "",
                "position": 254
            }
        ]
    },
    {
        "header": "2Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02358/x5.png",
                "caption": "Figure 5:3D RoPE strategy for the VAE branch in VINO.We apply a unified 3D RoPE schedule along the temporal axis to interleave different visual modalities in the MMDiT VAE branch.\nEach modality—single reference images, multi-frame reference videos, and noisy target latents—is placed on a shared RoPE timeline, separated by special tokens, which are projected from the VLM output.\nThis structured RoPE layout enables the model to distinguish heterogeneous visual sources.",
                "position": 319
            },
            {
                "img": "https://arxiv.org/html/2601.02358/x6.png",
                "caption": "Figure 6:Training data distribution across the stages.This progressive strategy gradually transforms the base model from a pure text-to-video generator into a capable multi-task visual generator.",
                "position": 341
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02358/x7.png",
                "caption": "Figure 7:Qualitative comparison of video editing results between VINO and VACE-Ditto[vace,ditto].Given the same input video and editing instructions, VINO delivers markedly stronger instruction following and better visual quality.",
                "position": 1059
            },
            {
                "img": "https://arxiv.org/html/2601.02358/x8.png",
                "caption": "Figure 8:Ablation on learnable tokens.Training curves comparing models with (purple) and without (blue) learnable tokens across tasks (T2V, T2I, I2V, and image reconstruction).\nIntroducing learnable tokens substantially stabilizes optimization, yielding lower gradient variance, reduced gradient norms, and smoother, healthier loss trajectories.\nThis demonstrates that learnable tokens provide more reliable multimodal conditioning and facilitate more stable convergence during unified visual generation training.",
                "position": 1527
            },
            {
                "img": "https://arxiv.org/html/2601.02358/x9.png",
                "caption": "Figure 9:Ablation on learnable tokens for image editing.Visual comparison of editing results with and without learnable tokens under multimodal conditioning.\nWhen provided with both the input image and an instruction, the model equipped with learnable tokens produces edits that more faithfully follow the instruction while preserving scene structure and appearance.\nIn contrast, the model without learnable tokens often misinterprets the editing intent or generates semantically inconsistent content.",
                "position": 1534
            },
            {
                "img": "https://arxiv.org/html/2601.02358/x10.png",
                "caption": "Figure 10:Qualitative comparison under different image-CFG scales.Increasing the image-CFG weight strengthens adherence to the visual identity provided by the reference images, yielding video frames that more closely match the subject’s appearance and attributes.\nHowever, excessively large image-CFG values over-constrain the generative process, suppressing motion diversity and leading to noticeably reduced temporal dynamics.",
                "position": 1541
            },
            {
                "img": "https://arxiv.org/html/2601.02358/x11.png",
                "caption": "Figure 11:Ablation on the special token for separating VAE latents.\nWithout special token (bottom row), the model incorrectly entangles the temporal structure of the input video with static image latents, causing noticeable artifacts—most prominently distorted structures in the first generated frame.",
                "position": 1547
            }
        ]
    },
    {
        "header": "4Related Works",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ABase Models",
        "images": []
    },
    {
        "header": "Appendix BMore Details",
        "images": []
    },
    {
        "header": "Appendix CQuantitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02358/x12.png",
                "caption": "Figure 12:Training progression from Stage 3, demonstrated through image editing tasks.The model benefits from the strong initialization achieved in earlier stages, gradually improving its ability to align visuals with instructions. As training progresses, the model refines its understanding, capturing finer details and showing clear progress in handling different tasks.",
                "position": 2332
            },
            {
                "img": "https://arxiv.org/html/2601.02358/x13.png",
                "caption": "Figure 13:The effect of learnable tokens in the final model. Use the same initial gaussian noise.",
                "position": 2336
            }
        ]
    },
    {
        "header": "Appendix DQualitative results",
        "images": []
    }
]