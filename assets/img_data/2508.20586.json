[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20586/Figures/teaser.pdf",
                "caption": "",
                "position": 123
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20586/Figures/compare.pdf",
                "caption": "Figure 2:Architectural comparison of multi-reference try-on methods. Our cacheable U-Net (c) avoids the parameter overhead of ReferenceNet (a) and the computational redundancy of In-Context Learning (b).",
                "position": 131
            },
            {
                "img": "https://arxiv.org/html/2508.20586/Figures/dataset.pdf",
                "caption": "Figure 3:Illustrative examples from our proposed DressCode-MR dataset. Each sample provides a pairing of a full-body person image with a set of corresponding canonical images for each item.",
                "position": 177
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20586/Figures/framework.pdf",
                "caption": "Figure 4:Overview of FastFit. Our model accelerates multi-reference virtual try-on through a novel cacheable UNet architecture. It enables lossless KV caching for reference features by conditioning them on class embeds instead of the timestep and using Semi-Attention to interact with the denoising features, which eliminates redundant computations.",
                "position": 204
            },
            {
                "img": "https://arxiv.org/html/2508.20586/Figures/attention_mask.pdf",
                "caption": "Figure 5:Visualization of the Semi-Attention Mask. Denosing featureXXattend to all features, while each reference featureRiR_{i}is restricted to its own.",
                "position": 282
            },
            {
                "img": "https://arxiv.org/html/2508.20586/Figures/mr_compare.pdf",
                "caption": "Figure 6:Qualitative comparison on the DressCode-MR dataset. SeeSectionA.2.2for more results. Please zoom in for more details.",
                "position": 345
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20586/Figures/sr_compare.pdf",
                "caption": "Figure 7:Qualitative comparison on the VITON-HD(Choi et al.,2021)and DressCode(Morelli et al.,2022)dataset. SeeSectionA.2.1for more results. Please zoom in for more details.",
                "position": 362
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20586/Figures/sr_compare_appendix.pdf",
                "caption": "Figure 8:More visual comparisons on the VITON-HD(Choi et al.,2021)and DressCode(Morelli et al.,2022)dataset with baseline methods. Please zoom in for more details.",
                "position": 1717
            },
            {
                "img": "https://arxiv.org/html/2508.20586/Figures/mr_compare_appendix.pdf",
                "caption": "Figure 9:More visual comparisons on the DressCode-MR dataset with baseline methods. Please zoom in for more details.",
                "position": 1720
            },
            {
                "img": "https://arxiv.org/html/2508.20586/Figures/ablation.pdf",
                "caption": "Figure 10:Demonstration of the visual effect of class embeddings. By providing different class embeddings (e.g., ’Upper’, ’Lower’, ’Dresses’, ’Shoes’, ’Bag’) for the same reference image, our model can be directed to selectively transfer the corresponding item to the source person.",
                "position": 1723
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]