[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Implementation Details",
        "images": []
    },
    {
        "header": "5Data",
        "images": []
    },
    {
        "header": "6Scaling Laws for DLCM",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.24617/x1.png",
                "caption": "Figure 3:Hyperparameter tuning and transfer underμ\\muP.Left:We sweepηconceptbase\\eta^{\\text{base}}_{\\text{concept}}while fixingηtokenbase\\eta^{\\text{base}}_{\\text{token}}at itsμ\\muP-predicted value on the 87M proxy model. The loss curve shows a well-defined minimum near100%100\\%.Right:We jointly scale both base learning rates by the same factor and observe consistent minima across model sizes (87M–834M), validating the zero-shot transferability ofμ\\muP-derived hyperparameters.",
                "position": 789
            },
            {
                "img": "https://arxiv.org/html/2512.24617/x2.png",
                "caption": "",
                "position": 792
            },
            {
                "img": "https://arxiv.org/html/2512.24617/full_training_linear.png",
                "caption": "Figure 4:Full training trajectory fit.Comparison between predicted loss (Equation22) and empirical loss across model sizes (274M–833M), compression factorsR∈{2,4,8}R\\in\\{2,4,8\\}, and training budgets. The joint fit achievesR2>0.98R^{2}>0.98.",
                "position": 864
            },
            {
                "img": "https://arxiv.org/html/2512.24617/robust_decay_fit.png",
                "caption": "Figure 5:Decay-phase fit.Simplified fit on the final portion of training tokens, validating that our WSD scaling law accurately captures late-stage behaviour withR2=0.93R^{2}=0.93.",
                "position": 867
            },
            {
                "img": "https://arxiv.org/html/2512.24617/figure_efficiency.png",
                "caption": "Figure 6:Efficiency analysis of the DLCM architecture.\n(a) Architectural efficiency (Loss/FLOPs) across backbone proportionsPPfor different compression ratiosRR. (b) FLOPs savings compared to baseline\nmodels of varying sizes, with DLCM configured atP=60%,R=4P=60\\%,R=4.",
                "position": 884
            },
            {
                "img": "https://arxiv.org/html/2512.24617/figure_flops_savings.png",
                "caption": "",
                "position": 894
            }
        ]
    },
    {
        "header": "7Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.24617/budgets.png",
                "caption": "Figure 7:Top: Average loss comparison between concept model (blue) and baseline model (orange) across relative positions within concepts. Bottom: Loss difference (Concept - Baseline), wheregreen indicates improvement(lower loss) and red indicates degradation.",
                "position": 1200
            }
        ]
    },
    {
        "header": "8Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.24617/red.png",
                "caption": "Figure 8:Average compressed sequence length over training steps.Red:Learned Boundary Predictor.Purple:Rule-Based Predictor. The x-axis represents training steps, and the y-axis represents the average number of tokens post-compression.",
                "position": 1255
            }
        ]
    },
    {
        "header": "9Conclusion",
        "images": []
    },
    {
        "header": "Contributions",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.24617/speed_comp.png",
                "caption": "Figure 9:Flash Attention Varlen speedup (Tflex/TflashT_{\\text{flex}}/T_{\\text{flash}}) vs. Sequence Length. The plot visualizes the data from Table6, highlighting the performance trend across different scales and hidden sizes.",
                "position": 1859
            }
        ]
    },
    {
        "header": "10Appendix: Segmentation Examples at Different Compression Ratios",
        "images": []
    }
]