[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06016/extracted/6058089/figures/teaser_cow.jpeg",
                "caption": "Figure 1:Motivation.Videos generated by Stable Video Diffusion[5]suffer from appearance drift, while those from our method, Track4Gen, are free from such appearance inconsistency issues.",
                "position": 119
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06016/extracted/6058089/figures/overview.jpeg",
                "caption": "Figure 2:Track4Gen overview.Red-colored blocks represent layers optimized by the diffusion loss‚Ñídiffsubscript‚Ñídiff\\mathcal{L}_{\\text{diff}}caligraphic_L start_POSTSUBSCRIPT diff end_POSTSUBSCRIPT, whilegreenblocks are optimized by the correspondence loss‚Ñícorrsubscript‚Ñícorr\\mathcal{L}_{\\text{corr}}caligraphic_L start_POSTSUBSCRIPT corr end_POSTSUBSCRIPT. Blocks colored bothredandgreenare influenced by the joint loss,‚Ñídiff+Œª‚Å¢‚Ñícorrsubscript‚ÑídiffùúÜsubscript‚Ñícorr{\\color[rgb]{0.8,0.0,0.0}\\definecolor[named]{pgfstrokecolor}{rgb}{0.8,0.0,0.0}%\n\\mathcal{L}_{\\text{diff}}}+\\lambda{\\color[rgb]{0.0,0.42,0.24}\\definecolor[%\nnamed]{pgfstrokecolor}{rgb}{0.0,0.42,0.24}\\mathcal{L}_{\\text{corr}}}caligraphic_L start_POSTSUBSCRIPT diff end_POSTSUBSCRIPT + italic_Œª caligraphic_L start_POSTSUBSCRIPT corr end_POSTSUBSCRIPT.\nSee text for details.",
                "position": 164
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06016/extracted/6058089/figures/observation-1.jpeg",
                "caption": "Figure 3:Real-world video tracking using different video diffusion features. Given color-coded query points on the first frame (Leftmost column), we display tracked points on target frames using features from different blocks (right columns).\nThe 13th frame (first row) and 8th frame (second row) are shown as target frames.\nFull results are available onour page.",
                "position": 240
            },
            {
                "img": "https://arxiv.org/html/2412.06016/extracted/6058089/figures/observation-2.jpeg",
                "caption": "Figure 4:Generated video tracking using video diffusion features.Tracks based on diffusion features are annotated on the generated videos. Track4Gen generates more consistent results.",
                "position": 245
            },
            {
                "img": "https://arxiv.org/html/2412.06016/extracted/6058089/figures/vid-gen-comparison.jpeg",
                "caption": "Figure 5:Image-to-video generation results of the original SVD and Track4Gen.Please visitour projectfor full video view.",
                "position": 265
            },
            {
                "img": "https://arxiv.org/html/2412.06016/extracted/6058089/figures/vid-gen-ablation.jpeg",
                "caption": "Figure 6:Qualitative ablation on video generation.Track4Gen is compared withfinetunedSVD (SVD finetuned on the same training videos without any correspondence supervision) and Track4Gen trained without the Refiner module.",
                "position": 349
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06016/extracted/6058089/figures/video-tracking-raw-features.jpeg",
                "caption": "Figure 7:Qualitative comparison of Track4Gen and baselines for real-world video tracking.The leftmost column displays query points in the first frame, while the following three columns show tracking results using features from each model.",
                "position": 482
            },
            {
                "img": "https://arxiv.org/html/2412.06016/x1.jpeg",
                "caption": "(a)Identity preservation",
                "position": 503
            },
            {
                "img": "https://arxiv.org/html/2412.06016/x1.jpeg",
                "caption": "(a)Identity preservation",
                "position": 506
            },
            {
                "img": "https://arxiv.org/html/2412.06016/x2.jpeg",
                "caption": "(b)Motion naturalness",
                "position": 511
            },
            {
                "img": "https://arxiv.org/html/2412.06016/extracted/6058089/figures/ours_with_dino_tracker.jpeg",
                "caption": "Figure 9:Extending Track4Gen with test-time adaptation[67].",
                "position": 638
            }
        ]
    },
    {
        "header": "5Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06016/extracted/6058089/figures/supple-without-refiner.jpeg",
                "caption": "Figure 10:Comparison of Track4Gen with and without Refiner.Top:Correspondence loss‚Ñícorrsubscript‚Ñícorr\\mathcal{L}_{\\text{corr}}caligraphic_L start_POSTSUBSCRIPT corr end_POSTSUBSCRIPTis computed using the refined featuresùíâ~1:Nsuperscript~ùíâ:1ùëÅ\\tilde{{\\boldsymbol{h}}}^{1:N}over~ start_ARG bold_italic_h end_ARG start_POSTSUPERSCRIPT 1 : italic_N end_POSTSUPERSCRIPT.Bottom:Correspondence loss‚Ñícorrsubscript‚Ñícorr\\mathcal{L}_{\\text{corr}}caligraphic_L start_POSTSUBSCRIPT corr end_POSTSUBSCRIPTis computed using the raw diffusion featuresùíâ1:Nsuperscriptùíâ:1ùëÅ{\\boldsymbol{h}}^{1:N}bold_italic_h start_POSTSUPERSCRIPT 1 : italic_N end_POSTSUPERSCRIPT.",
                "position": 2071
            },
            {
                "img": "https://arxiv.org/html/2412.06016/extracted/6058089/figures/supple_user_study.jpeg",
                "caption": "Figure 11:Example user evaluation page.The order of Track4Gen and the baseline is randomly shuffled to ensure a fair comparison.",
                "position": 2106
            }
        ]
    },
    {
        "header": "Appendix BAdditional Metrics",
        "images": []
    },
    {
        "header": "Appendix CAdditional Video Generation Results",
        "images": []
    },
    {
        "header": "Appendix DAdditional Video Tracking Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06016/extracted/6058089/figures/supple-generated-tracking.jpeg",
                "caption": "Figure 12:Generated Videos with Embedded Tracks.Predicted point tracks are annotated on the videos generated by Track4Gen.",
                "position": 2201
            },
            {
                "img": "https://arxiv.org/html/2412.06016/extracted/6058089/figures/supple-full-123.jpeg",
                "caption": "Figure 13:Qualitative video generation results:Track4Gen compared against all three baselines.",
                "position": 2225
            },
            {
                "img": "https://arxiv.org/html/2412.06016/extracted/6058089/figures/supple-full-456.jpeg",
                "caption": "Figure 14:Qualitative video generation results:Track4Gen compared against all three baselines.",
                "position": 2232
            },
            {
                "img": "https://arxiv.org/html/2412.06016/extracted/6058089/figures/supple_raw_feature_track.jpeg",
                "caption": "Figure 15:Additional feature comparison on real-world video tracking:Track4Gen vs DINOv2 vs Stable Video Diffusion vs ZeroScope",
                "position": 2239
            },
            {
                "img": "https://arxiv.org/html/2412.06016/extracted/6058089/figures/supple-vs-dino.jpeg",
                "caption": "Figure 16:Additional feature comparison on real-world video tracking:Track4Gen vs DINOv2",
                "position": 2246
            },
            {
                "img": "https://arxiv.org/html/2412.06016/extracted/6058089/figures/supple_with_dino.jpeg",
                "caption": "Figure 17:Extending Track4Gen features with test-time adaptation[67].",
                "position": 2252
            },
            {
                "img": "https://arxiv.org/html/2412.06016/extracted/6058089/figures/supple_progress.jpeg",
                "caption": "Figure 18:Optimization progress visualization.The first rows show tracking results using zero-shot Track4Gen features, while the third rows display results after 5,000 optimization steps.",
                "position": 2258
            },
            {
                "img": "https://arxiv.org/html/2412.06016/extracted/6058089/figures/supple_failure_track.jpeg",
                "caption": "Figure 19:Video tracking failure cases.Track4Gen features struggle to capture point correspondences in videos with fast-moving objects or multiple semantically similar objects.",
                "position": 2264
            },
            {
                "img": "https://arxiv.org/html/2412.06016/extracted/6058089/figures/supple_limit.jpeg",
                "caption": "Figure 20:Limitation.Generated videos of Track4Gen may exhibit reduced camera motion.",
                "position": 2271
            },
            {
                "img": "https://arxiv.org/html/2412.06016/extracted/6058089/figures/supple_failure_gen.jpeg",
                "caption": "Figure 21:Video generation failure cases.Track4Gen may generate videos with physically unrealistic motion and artifacts on human faces.\nFor instance, the red bus (row 1) drives backward, the frog (row 2) jumps mid-air, and the faces (row 3,4) display artifacts.",
                "position": 2277
            }
        ]
    },
    {
        "header": "Appendix EDiscussion on Limitation and Failure",
        "images": []
    }
]