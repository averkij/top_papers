[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.01502/x1.png",
                "caption": "Figure 1:Tangential and radial translations result in geometrically regular but distinct depth-dependent flows. Specifically, the rigid flow induced by tangential translation varies inversely with depth, while the one resulting from radial translation is not only depth-dependent but also subject to perspective scaling. In contrast, rotation results in irregular, depth-independent flows.",
                "position": 98
            }
        ]
    },
    {
        "header": "IIRelated Work",
        "images": []
    },
    {
        "header": "IIIPreliminaries",
        "images": []
    },
    {
        "header": "IVDiscriminative Treatment of Motion Components",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.01502/x2.png",
                "caption": "Figure 2:The discriminative treatment of motion components and the resulting flow decomposition processes.",
                "position": 308
            },
            {
                "img": "https://arxiv.org/html/2511.01502/x3.png",
                "caption": "Figure 3:Ego-motion transformation decomposition: (a) Ideally, ego-motion can be decomposed into pure tangential and radial translation components; (b) In practice, errors in the PoseNet predictions introduce undesired deviations to the decomposed components.",
                "position": 440
            }
        ]
    },
    {
        "header": "VDiMoDE",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.01502/x4.png",
                "caption": "Figure 4:An overview of the DiMoDE framework. Centered on the core idea of discriminative motion component treatment, depth (from DepthNet) and ego-motion (from PoseNet) are utilized to perform optical axis and imaging plane alignments. FlowNet generates dense correspondences, which are transformed during the alignment processes. The transformed flows are ultimately leveraged to incorporate two sets of geometric constraints into the unsupervised joint learning framework, thereby simultaneously improving both depth and pose estimation performance.",
                "position": 496
            }
        ]
    },
    {
        "header": "VIExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.01502/x5.png",
                "caption": "Figure 5:An illustration of our designed handheld setup equipped with calibrated and synchronized sensors for accurate real-world data collection.",
                "position": 680
            },
            {
                "img": "https://arxiv.org/html/2511.01502/x6.png",
                "caption": "Figure 6:Comparisons of trajectories on the test sets of the KITTI Odometry dataset[1]. All predicted trajectories are aligned with the ground truth using a 7-DoF similarity transformation.",
                "position": 1141
            },
            {
                "img": "https://arxiv.org/html/2511.01502/x7.png",
                "caption": "Figure 7:Comparison of storage overhead and inference speed among representative learning-based and hybrid visual odometry methods.",
                "position": 1144
            },
            {
                "img": "https://arxiv.org/html/2511.01502/x8.png",
                "caption": "Figure 8:Comparisons of trajectories on several long-term sequences in the train set of the KITTI Odometry dataset[1]. All predicted trajectories are aligned with the ground truth using a 7-DoF similarity transformation.",
                "position": 1367
            },
            {
                "img": "https://arxiv.org/html/2511.01502/x9.png",
                "caption": "Figure 9:Comparisons of trajectories produced by models finetuned on the additional sequences of the KITTI Odometry dataset (‘ft’ denotes finetuned models). All predicted trajectories are aligned with the ground truth using a 7-DoF similarity transformation.",
                "position": 1498
            },
            {
                "img": "https://arxiv.org/html/2511.01502/x10.png",
                "caption": "Figure 10:Comparisons of trajectories produced by models finetuned on the newly created MIAS-Odom dataset. All predicted trajectories are aligned with the ground truth using a 7-DoF similarity transformation.",
                "position": 1604
            },
            {
                "img": "https://arxiv.org/html/2511.01502/x11.png",
                "caption": "Figure 11:Qualitative results on the DDAD[56]dataset.",
                "position": 2182
            },
            {
                "img": "https://arxiv.org/html/2511.01502/",
                "caption": "Figure 12:Qualitative results on the nuScenes[50]dataset.",
                "position": 2185
            },
            {
                "img": "https://arxiv.org/html/2511.01502/x13.png",
                "caption": "Figure 13:Qualitative zero-shot monocular depth estimation results on the Make3D[57]and DIML[58]datasets.",
                "position": 2426
            },
            {
                "img": "https://arxiv.org/html/2511.01502/x14.png",
                "caption": "Figure 14:Quantitative results demonstrating DiMoDE’s convergence behavior and robustness to hyperparameter variations: (a) comparison of validation curves for the baseline model, DiMoDE, and its ablation variants; (b) analysis of the impact of varying hyperparametersλ1\\lambda_{1}andλ2\\lambda_{2}on the performance of both tasks.",
                "position": 2618
            }
        ]
    },
    {
        "header": "VIIDiscussion",
        "images": []
    },
    {
        "header": "VIIIConclusion and Future Works",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]