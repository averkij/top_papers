[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02613/x1.png",
                "caption": "Figure 1:NL-Eyeevaluates the abductive reasoning capabilities of VLMs. The main setup involves a premise image and two hypothesis images, where the model is tasked with inferring which hypothesis is more plausible, and to provide an explanation for its choice.",
                "position": 228
            }
        ]
    },
    {
        "header": "2TheNL-EyeBenchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02613/x2.png",
                "caption": "Figure 2:Fully annotated examples fromNL-Eye. Each example includes the three images, the textual descriptions (prompts) used to generate them, the gold label, an explanation for why the gold is more plausible, and indications of the reasoning category and temporal direction and duration.",
                "position": 297
            },
            {
                "img": "https://arxiv.org/html/2410.02613/x3.png",
                "caption": "Figure 3:Real examples from each reasoning category inNL-Eye. The more plausible hypotheses are framed in green. The gold explanations are included below each sample.",
                "position": 323
            }
        ]
    },
    {
        "header": "3Data Curation",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02613/x4.png",
                "caption": "Figure 4:NL-Eyedata curation workflow scheme. The process includes three steps: (1) writing textual descriptions, (2) generating images, and (3) generating explanation and categorization. Yellow denotes steps that require human involvement while turquoise denotes model-based generations.",
                "position": 332
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02613/x5.png",
                "caption": "Table 1:Models and baselines by their input strategy and reasoning approach.",
                "position": 382
            },
            {
                "img": "https://arxiv.org/html/2410.02613/x6.png",
                "caption": "Table 2:Main results:Scores for vision-based experiments. Automatic evaluation scores are not presented for Humans since their explanations serve as references. Regardless of the input strategy, VLMs are greatly outperformed by humans and mostly perform on par or even below the baselines.",
                "position": 544
            }
        ]
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02613/x6.png",
                "caption": "Table 3:Text-Based:Performance for prediction in the triplet setup. Predictor models perform well and similarly to (vision-based reasoning of) humans when using the gold description. However, VLM describers generate useless captions which do not help solve the task.",
                "position": 608
            }
        ]
    },
    {
        "header": "6VLM Failure Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02613/x7.png",
                "caption": "Figure 7:Failure factors of model explanation for incorrect plausibility prediction.",
                "position": 706
            }
        ]
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Reproducibility",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02613/x8.png",
                "caption": "Table 4:API and version of closed-source models used for inference onNL-Eyetasks.",
                "position": 1649
            },
            {
                "img": "https://arxiv.org/html/2410.02613/x8.png",
                "caption": "Table 5:Textual prompts for task descriptions in different input strategies and setups.",
                "position": 1665
            }
        ]
    },
    {
        "header": "Appendix BComplementary Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02613/x8.png",
                "caption": "Table 6:Plausibility predictionresults of the triplet setup. Order refers to the position of the correct hypothesis image in the input, whether it was presented first (order 1) or second (order 2).",
                "position": 1750
            },
            {
                "img": "https://arxiv.org/html/2410.02613/x8.png",
                "caption": "Table 7:Pairs-setup performance with additional rank information regarding rank differences and absolute values.",
                "position": 1779
            },
            {
                "img": "https://arxiv.org/html/2410.02613/x8.png",
                "caption": "Table 8:Human votes of candidate explanations. The percentage of votes reflects annotators’ agreement with the candidate explanations provided by the models. 0-votes notes no-selection, 3-votes notes selected unanimously.",
                "position": 1803
            },
            {
                "img": "https://arxiv.org/html/2410.02613/x8.png",
                "caption": "Table 9:Number of evaluated explanations. The explanations are associated with correctplausibility prediction. Human explanations include the correct explanations of 3 crowd-workers. The explanations are evaluated by human and automatically.",
                "position": 1830
            },
            {
                "img": "https://arxiv.org/html/2410.02613/x8.png",
                "caption": "Table 10:Plausibility predictionanalysis: Model vs. Human Comparison. ✓notes correct prediction while × notes incorrect one.",
                "position": 1854
            },
            {
                "img": "https://arxiv.org/html/2410.02613/x8.png",
                "caption": "Figure 8:Image-to-text descriptions example. Detailed descriptions by Claude 3.5 and the gold textual descriptions. In bold style are the key necessary details for succeeding in theplausibility predictiontask.",
                "position": 1869
            },
            {
                "img": "https://arxiv.org/html/2410.02613/x9.png",
                "caption": "Table 11:Failure factors with examples, as illustrated by the following scene - a premise ofa man in a hospital bed with a broken legand two hypotheses:a wet floor with (less plausible) and without (plausible) a warning sign(Figure1).",
                "position": 1887
            },
            {
                "img": "https://arxiv.org/html/2410.02613/x9.png",
                "caption": "Figure 9:VLM failure analysis: Explanations examples. Based on five main factors: (A) Style & consistency, (B) Time, (C) Ignored key details, (D) Missing knowledge and (E) Failed comparison.",
                "position": 1903
            },
            {
                "img": "https://arxiv.org/html/2410.02613/x10.png",
                "caption": "Figure 10:VLM failure analysis: When the model’s plausibility prediction is correct - the explanation can be either valid (B) or not (A).",
                "position": 1909
            },
            {
                "img": "https://arxiv.org/html/2410.02613/x11.png",
                "caption": "Figure 11:VLM failure analysis: When humans plausibility prediction is incorrect, and model’s explanation is correct - the explanation can be either valid (B) or not (A).",
                "position": 1915
            }
        ]
    },
    {
        "header": "Appendix CNL-EyeDataset Creation - Complementary Information",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02613/x12.png",
                "caption": "Table 12:NL-Eyetextual descriptions examples. One example for each reasoning category. Every example consists of a premise phrase, a plausible hypothesis phrase, and an implausible hypothesis phrase.",
                "position": 1965
            },
            {
                "img": "https://arxiv.org/html/2410.02613/x12.png",
                "caption": "Table 13:Examples of suggested textual descriptions (scenes) filtered by specific criteria.",
                "position": 1989
            },
            {
                "img": "https://arxiv.org/html/2410.02613/x12.png",
                "caption": "Figure 12:Zoom into the image generation step inNL-Eyecuration, as seen in Figure4. Yellow color notes a hand-curated stage, while turquoise notes a model-generated stage. All stages require human involvement for fixing, editing and validating.",
                "position": 2069
            },
            {
                "img": "https://arxiv.org/html/2410.02613/x13.png",
                "caption": "Figure 13:Dataset Analysis. A histogram of theNL-Eyeexamples. The benchmark is also annotated with diverse domains (left):administration, business & work, daily life & hospitality, education, healthcare, religion, science & technology, sports & arts and transportation, and representation of time duration and direction (right) in every reasoning category. Parallel in noted by ”parallel-short”.",
                "position": 2072
            },
            {
                "img": "https://arxiv.org/html/2410.02613/x14.png",
                "caption": "Figure 14:Guidelines for the crowd-workers. Guidelines for the human baseline on the plausibility prediction and plausibility explanation tasks (left), and for human evaluation of explanations (right).",
                "position": 2079
            },
            {
                "img": "https://arxiv.org/html/2410.02613/x15.png",
                "caption": "Figure 15:AMT questionnaire (human baseline) screen of the plausibility prediction and plausibility explanation tasks.",
                "position": 2093
            },
            {
                "img": "https://arxiv.org/html/2410.02613/x16.png",
                "caption": "Figure 16:Human evaluation of explanation screen, including (a) instructions provided to participants, and (b) a screenshot of the AMT questionnaire.",
                "position": 2096
            },
            {
                "img": "https://arxiv.org/html/2410.02613/extracted/5899175/figures/human_expkanation_screen.png",
                "caption": "",
                "position": 2106
            },
            {
                "img": "https://arxiv.org/html/2410.02613/extracted/5899175/figures/images/appendix_examples-1.png",
                "caption": "Figure 17:NL-Eyeexamples from each reasoning category (3 triplets per category). Each example consists of 3 images: a premise (left column), a plausible hypothesis (green frame), and a less plausible hypothesis (red frame). While the gold explanations are included in the benchmark, we invite the reader to attempt to create valid explanations on their own.",
                "position": 2115
            }
        ]
    },
    {
        "header": "Appendix DNL-EyeTriplet Examples",
        "images": []
    }
]