[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.09045/x1.png",
                "caption": "Figure 1:Multimodal fake news with hyperrealistic generated images from Midjourney poses a significant challenge for both state-of-the-art MLLMs (< 24% F-1) and humans (60% F-1). Our detectors achieve over 98% F-1 on in-domain (ID) data and can generalize on out-of-domain (OOD) data from unseen news publishers and image generators (85% F-1)",
                "position": 120
            }
        ]
    },
    {
        "header": "2MiRAGeNews Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.09045/extracted/5917426/figures/datagen.png",
                "caption": "Figure 2:Example of MiRAGeNews dataset generation. We use GPT-4 to generate a misleading caption, which is then used by Midjourney to generate the image.",
                "position": 153
            },
            {
                "img": "https://arxiv.org/html/2410.09045/x2.png",
                "caption": "Figure 3:Overview of ourMiRAGedetector for multimodal AI-generated news detection, which combinesMiRAGe-ImgwithMiRAGe-Txt.MiRAGe-Imgtrains a linear layer on the outputs from the image linear model and Object-Class Concept Bottleneck Model (CBM), whileMiRAGe-Txttrains a linear layer on the outputs from the text linear model and Text Bottleneck Model (TBM). Outputs from two models can be either early fused or late fused to make the final prediction on the image-caption pair.",
                "position": 167
            }
        ]
    },
    {
        "header": "3Detectors",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.09045/extracted/5917426/figures/fus.png",
                "caption": "Figure 4:(a) Early Fusiondetector uses both image and text features together for classification while(b) Late Fusiondetector uses outputs from previously trained unimodal detectors.",
                "position": 202
            }
        ]
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.09045/x3.png",
                "caption": "Figure 5:We see that MiRAGe-Img outperforms existing image-only detectors in both in-domain (ID) and out-of-domain (OOD). ZS and FT are short for Zero-Shot and Fine-Tuned, respectively",
                "position": 220
            },
            {
                "img": "https://arxiv.org/html/2410.09045/x4.png",
                "caption": "Figure 6:We see that MiRAGe outperforms existing image-text detectors in out-of-domain settings. ZS and FT are short for Zero-Shot and Fine-Tuned, respectively.",
                "position": 244
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "8Ethics Statement",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AModel Implementation Details",
        "images": []
    },
    {
        "header": "Appendix BAblation Study",
        "images": []
    },
    {
        "header": "Appendix CHuman Study Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.09045/x5.png",
                "caption": "Figure 7:Annotated generated image anomalies from human study",
                "position": 2552
            },
            {
                "img": "https://arxiv.org/html/2410.09045/x5.png",
                "caption": "Figure 7:Annotated generated image anomalies from human study",
                "position": 2555
            },
            {
                "img": "https://arxiv.org/html/2410.09045/x6.png",
                "caption": "Figure 8:Annotated generated caption anomalies from human study",
                "position": 2561
            },
            {
                "img": "https://arxiv.org/html/2410.09045/x7.png",
                "caption": "Figure 9:The user interface of the human study where each participant is given a pair of news images and caption and asked to determine whether they are real or generated",
                "position": 2567
            },
            {
                "img": "https://arxiv.org/html/2410.09045/x8.png",
                "caption": "Figure 10:Comparison of different image generators across examples from the MiRAGeNews dataset. We see that modern generators such as Midjourney produce high-quality images that are difficult to distinguish from real images.",
                "position": 2577
            }
        ]
    },
    {
        "header": "Appendix DExamples",
        "images": []
    }
]