[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.12670/x1.png",
                "caption": "Figure 1:Agent architecture stack and resolution rates across 7 agent-model configurations on 84 tasks. Curated Skills (beige) improve performance by +16.2pp on average; self-generated Skills (amber) provide negligible or negative benefit.",
                "position": 394
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2SkillsBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.12670/x2.png",
                "caption": "Figure 2:SkillsBenchpipeline overview.Phase 1 (Benchmark Construction):We aggregate Skills from three sources—open-source repositories (12,847), the Claude Code ecosystem (28,412), and corporate partners (5,891)—yielding 47,150 unique Skills after deduplication. In parallel, 322 contributors submit 105 candidate tasks.Phase 2 (Quality Filtering):Each task undergoes automated checks (structural validity, AI detection, leakage audit) and human review (data validity, task realism, oracle quality, Skill quality, anti-cheating), producing 84 tasks spanning 11 domains.Phase 3 (Evaluation):Tasks are executed under three conditions (no Skills, with curated Skills, self-generated Skills) across three commercial agent harnesses (Claude Code, Gemini CLI, Codex CLI). Deterministicpytestverifiers produce pass/fail outcomes; 7 agent-model configurations yield 7,308 trajectories, with curated Skills providing +12.66pp average improvement.",
                "position": 641
            },
            {
                "img": "https://arxiv.org/html/2602.12670/x3.png",
                "caption": "Figure 3:SkillsBenchconsists of tasks spanning 11 domains.",
                "position": 699
            }
        ]
    },
    {
        "header": "3Experimental Setup",
        "images": []
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.12670/x4.png",
                "caption": "Figure 4:Pareto frontier of pass rate vs. cost across model-harness configurations. Filled markers indicate with-Skills conditions; hollow markers indicate without-Skills. Skills shift the Pareto frontier upward, with Gemini 3 Flash and Claude Opus dominating the with-Skills frontier. Cost positions in this figure reflect the evaluation infrastructure’s pricing model. Trajectory analysis reveals that Flash consumes 2.3×\\timesmore input tokens per task than Pro (1.08M vs. 0.47M), a compensatory strategy where the smaller model substitutes iterative exploration for reasoning depth. At official API pricing ($0.50 vs. $2.00 per 1M input tokens), Flash’s 4×\\timeslower per-token cost more than offsets this higher volume, making Flash 44% cheaper per task ($0.55 vs. $0.98).",
                "position": 958
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASkill Ecosystem Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.12670/x5.png",
                "caption": "Figure 5:Temporal dynamics of Skill creation over 136 days. Daily additions (bars, left axis) remained modest through late 2025, then surged to a peak of 18,904 in January 2026. The cumulative curve (line, right axis) reflects exponential-like growth, reaching 84,192 total Skills.",
                "position": 1830
            },
            {
                "img": "https://arxiv.org/html/2602.12670/x6.png",
                "caption": "Figure 6:Token distribution of SKILL.md files (n=36,338, 99.5th percentile shown). Most Skills are lightweight with median∼\\sim1.5k tokens.",
                "position": 1845
            },
            {
                "img": "https://arxiv.org/html/2602.12670/x7.png",
                "caption": "Figure 7:Total Skill size distribution (n=37,078, 99.5th percentile shown, excluding metadata.json). Median total size remains under 2.5k tokens, with distribution highly skewed toward concise artifacts.",
                "position": 1849
            },
            {
                "img": "https://arxiv.org/html/2602.12670/x8.png",
                "caption": "Figure 8:Distribution of Skill categories. The top 10 categories account for 79.6% of all Skills, with Documentation (11.9%), Git/Version Control (11.8%), and Code Quality (9.0%) leading. No single category dominates, reflecting diverse developer needs across documentation, infrastructure, testing, and frontend tasks.",
                "position": 1891
            },
            {
                "img": "https://arxiv.org/html/2602.12670/x9.png",
                "caption": "Figure 9:File count distribution per Skill. Most Skills contain 1–5 files.",
                "position": 1902
            },
            {
                "img": "https://arxiv.org/html/2602.12670/x10.png",
                "caption": "Figure 10:File extension distribution. Markdown files dominate, indicating Skills prioritize natural-language instructions over executable implementations.",
                "position": 1906
            }
        ]
    },
    {
        "header": "Appendix BTask Specification and Review Process",
        "images": []
    },
    {
        "header": "Appendix CExperimental Setup Details",
        "images": []
    },
    {
        "header": "Appendix DTask-Level Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.12670/x11.png",
                "caption": "Figure 11:Task pass rate per model with curated Skills. The grid reveals a common set of easy tasks (top, uniformly blue) solved by all models, and hard tasks (bottom, uniformly red) unsolved even with Skills. Tasks along the diagonal transition from solvable to unsolvable as model capability decreases.",
                "position": 2865
            },
            {
                "img": "https://arxiv.org/html/2602.12670/x12.png",
                "caption": "Figure 12:Task pass rate per model without Skills (baseline). Compared toFigure 11, the blue region contracts substantially, confirming that Skills shift many tasks from unsolved to solved.",
                "position": 2868
            },
            {
                "img": "https://arxiv.org/html/2602.12670/x13.png",
                "caption": "Figure 13:Skills uplift per task (with Skills−-without Skills). Blue cells indicate positive uplift; red cells indicate tasks where Skills hurt performance. The majority of cells are blue, confirming broad Skill benefit. A small number of tasks show negative delta for specific models.",
                "position": 2871
            }
        ]
    },
    {
        "header": "Appendix EAdditional Experimental Details",
        "images": []
    },
    {
        "header": "Appendix FComplete Task List",
        "images": []
    },
    {
        "header": "Appendix GComprehensive Results Summary",
        "images": []
    },
    {
        "header": "Appendix HToken Usage and Cost Efficiency",
        "images": []
    },
    {
        "header": "Appendix IFailure Analysis",
        "images": []
    }
]