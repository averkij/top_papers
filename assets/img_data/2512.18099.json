[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.18099/x1.png",
                "caption": "Figure 1:Overview ofSAM Audio. Given an audio mixture,SAM Audioseparates it into target and residual stems, conditioned on any combination of text descriptions (text prompts), visual masks (visual prompts), and temporal intervals (span prompts).",
                "position": 351
            }
        ]
    },
    {
        "header": "4Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.18099/x2.png",
                "caption": "Figure 2:Illustration of our pseudo-labeling data synthesis pipeline. PLM-Audio generates text prompts from mixtures, which guide SAM Audio to produce target/residual stems. A filter stage retains only high-quality pseudo-labeled stems.",
                "position": 746
            },
            {
                "img": "https://arxiv.org/html/2512.18099/x3.png",
                "caption": "Figure 3:Illustration of pseudo-labeled visual data. The pseduo-labeling pipeline produces a text caption of the target audio, which is used to prompt SAM3 to obtain the visual mask.",
                "position": 949
            },
            {
                "img": "https://arxiv.org/html/2512.18099/x4.png",
                "caption": "Figure 4:Illustration of span generation. RMS energy (top) and Mel-spectrograms (bottom) for the targetxtgtx_{\\mathrm{tgt}}, residualxresx_{\\mathrm{res}},\nand mixturexmixx_{\\mathrm{mix}}.\nYellow intervals denote detected spans corresponding to active sound events.",
                "position": 976
            }
        ]
    },
    {
        "header": "5Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.18099/x5.png",
                "caption": "Figure 5:Summary of task, modality, and dataset coverage inSAM Audio-Bench. The modality abbreviations are as follows: “T” indicates the item can be used with a text-only prompt (e.g. for speaker separation this implies that the text description can be unambiguously associated with a single speaker), “V” indicates that the target sound is on-screen and that we have a SAM masklet provided and “S” denotes that there are event boundaries for the target sound.",
                "position": 1440
            },
            {
                "img": "https://arxiv.org/html/2512.18099/x6.png",
                "caption": "Figure 6:Diagram of SAM Audio Judge Model",
                "position": 1636
            }
        ]
    },
    {
        "header": "6Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.18099/x7.png",
                "caption": "Figure 7:Net Win Rate (%) ofSAM Audioagainst SoTA separation models in text-prompted tasks.†\\dagger: proprietary models",
                "position": 2347
            },
            {
                "img": "https://arxiv.org/html/2512.18099/x8.png",
                "caption": "Figure 8:Net Win Rate (%) ofSAM Audioagainst SoTA separation models in visual-prompted tasks",
                "position": 2447
            },
            {
                "img": "https://arxiv.org/html/2512.18099/assets/plots/video_samples/sample_67_three_panel.png",
                "caption": "Figure 9:Visual-prompted samples where text descriptions are ambiguous.\nEach example shows (top) masked video, (middle) input mixture spectrogram,\nand (bottom) separated target spectrogram. The target speaker is highlighted in each video.",
                "position": 2453
            },
            {
                "img": "https://arxiv.org/html/2512.18099/assets/plots/video_samples/sample_39_three_panel.png",
                "caption": "",
                "position": 2457
            },
            {
                "img": "https://arxiv.org/html/2512.18099/x9.png",
                "caption": "Figure 10:Net Win Rate (%) ofSAM Audiowith text & span / span as input against a text-only model",
                "position": 2567
            },
            {
                "img": "https://arxiv.org/html/2512.18099/x10.png",
                "caption": "Table 13:Using vs. not using predicted span for text-prompting.\nOVR: overall subjective score. For all the metrics below, higher is better.",
                "position": 2593
            },
            {
                "img": "https://arxiv.org/html/2512.18099/x10.png",
                "caption": "Figure 11:Net Win Rate (%) ofSAM Audiousing predicted span against not using predicted span for text prompting.",
                "position": 2687
            },
            {
                "img": "https://arxiv.org/html/2512.18099/x11.png",
                "caption": "Table 14:Comparison against baselines for music removal",
                "position": 2704
            },
            {
                "img": "https://arxiv.org/html/2512.18099/x11.png",
                "caption": "Figure 12:Net Win Rate (%) ofSAM Audioover baselines in music removal",
                "position": 2730
            },
            {
                "img": "https://arxiv.org/html/2512.18099/x12.png",
                "caption": "Figure 13:Effect of varying ODE steps under the midpoint solver. Fewer steps reduce computation at a modest cost in quality.",
                "position": 2740
            }
        ]
    },
    {
        "header": "7SAM AudioResults",
        "images": []
    }
]