[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20848/x1.png",
                "caption": "Figure 1:Accuracy and throughput comparisons of Nemotron 3 Nano with Qwen3-30B-A3B-Thinking-2507 and GPT-OSS-20B. Nemotron 3 Nano achieves on-par or better accuracies across multiple benchmarks. RULER scores for 1M context length are available only for Nemotron 3 Nano and Qwen3 since GPT-OSS-20B has a context length of 128K tokens. Further, on 8K input / 16K output setting, Nemotron 3 Nano provides inference throughput that is 3.3×\\timeshigher than Qwen3-30B-A3B-Thinking-2507 and 2.2×\\timeshigher than GPT-OSS-20B. We measured throughput on a single H200 GPU with vLLM and TRT-LLM and used the best out of the two for each model. We used the OpenHands harness to evaluate SWE-Bench.",
                "position": 122
            },
            {
                "img": "https://arxiv.org/html/2512.20848/assets/huggingface-color.png",
                "caption": "",
                "position": 143
            }
        ]
    },
    {
        "header": "2Pretraining",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20848/x2.png",
                "caption": "Figure 2:Nemotron 3 Nano layer pattern. We use a hybrid Mamba-Transformer architecture similar to the previous generation of Nemotron models. In addition, we scale the model sparsely by using MoE layers instead of standard FFN layers.",
                "position": 220
            },
            {
                "img": "https://arxiv.org/html/2512.20848/x3.png",
                "caption": "(a)Data mixture of Phase 1.",
                "position": 497
            },
            {
                "img": "https://arxiv.org/html/2512.20848/x3.png",
                "caption": "(a)Data mixture of Phase 1.",
                "position": 500
            },
            {
                "img": "https://arxiv.org/html/2512.20848/x4.png",
                "caption": "(b)Data mixture of Phase 2.",
                "position": 505
            }
        ]
    },
    {
        "header": "3Post-Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20848/x5.png",
                "caption": "Figure 4:Example prompt materialization using the Nemotron 3 Nano chat template for a 2-turn conversation. For a given generation, only reasoning content from the current turn is materialized into the prompt.",
                "position": 719
            },
            {
                "img": "https://arxiv.org/html/2512.20848/x6.png",
                "caption": "Figure 5:SFT data blend for Nemotron 3 Nano.",
                "position": 801
            },
            {
                "img": "https://arxiv.org/html/2512.20848/x7.png",
                "caption": "Figure 6:Batch-wise pass rates across the RL curriculum.",
                "position": 873
            },
            {
                "img": "https://arxiv.org/html/2512.20848/x8.png",
                "caption": "Figure 7:Comparison between curriculum sampling and random sampling.",
                "position": 879
            },
            {
                "img": "https://arxiv.org/html/2512.20848/x9.png",
                "caption": "Figure 8:RLVR surpasses or matches heavily fine-tuned SFT model across all evaluated domains.",
                "position": 904
            },
            {
                "img": "https://arxiv.org/html/2512.20848/x10.png",
                "caption": "Figure 9:Benchmark performance throughout RL training.",
                "position": 937
            },
            {
                "img": "https://arxiv.org/html/2512.20848/x11.png",
                "caption": "Figure 10:GenRM performance improves across benchmarks as we scale up RL training.",
                "position": 967
            }
        ]
    },
    {
        "header": "4Quantization",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20848/x12.png",
                "caption": "Figure 11:Ablation study of different quantization configurations for accuracy–throughput trade-offs. Accuracy recovery and throughput improvements are computed relative to the Nemotron 3 Nano BF16 checkpoint, with values normalized such that the BF16 baseline is 100%. Accuracy recovery is defined as the median of the recovery rates across all benchmarks.\nThe benchmark was conducted on a single H100 with ISL/OSL=8K/16K. Given that more aggressively quantized models can accommodate larger batch sizes due to lower memory footprint, we used the maximum batch size for each quantization configuration for fair comparisons under the same hardware constraints.",
                "position": 1374
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Contributors",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ABase model evaluations",
        "images": []
    },
    {
        "header": "Appendix BMMLU-redux evaluation",
        "images": []
    },
    {
        "header": "Appendix CDPO for Reducing Tool Hallucination",
        "images": []
    },
    {
        "header": "Appendix DSafety Preference Data",
        "images": []
    },
    {
        "header": "Appendix EPrompt Sensitivity Analysis",
        "images": []
    }
]