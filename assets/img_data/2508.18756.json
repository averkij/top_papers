[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.18756/x1.png",
                "caption": "Figure 1:Overall structure of 3 sparse layers. (a) MoE layer; (b) Product Key Memory (PKM) layer; (c) UltraMemV2 layer.",
                "position": 267
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.18756/x2.png",
                "caption": "Figure 2:PEER ablation study on UltraMemV2-430M/5B. Training loss (left) and Open-Benchmark accuracy (right) comparing PEER with Baseline using value embedding.",
                "position": 1021
            },
            {
                "img": "https://arxiv.org/html/2508.18756/x3.png",
                "caption": "Figure 3:Single projector and multi-query modifications on UltraMemV2-430M/5B. Training loss difference (left) and Open-Benchmark accuracy (right) comparing baseline with the proposed approach. The modifications achieve 0.0026 loss reduction and 0.7-point accuracy improvement after 1.5T tokens.",
                "position": 1029
            },
            {
                "img": "https://arxiv.org/html/2508.18756/x4.png",
                "caption": "Figure 4:Ablation study on the number of heads in UltraMemV2-430M/5B. Training loss difference (left) and Open-Benchmark accuracy (right) comparing single head vs. two heads. Using a single head achieves 8e-4 loss reduction and 0.2-point accuracy improvement after 1T tokens.",
                "position": 1040
            },
            {
                "img": "https://arxiv.org/html/2508.18756/x5.png",
                "caption": "Figure 5:Training loss difference under different memory computational proportion.",
                "position": 1169
            },
            {
                "img": "https://arxiv.org/html/2508.18756/x6.png",
                "caption": "Figure 6:Left: ablation on the sum ofDvD_{v}andDpD_{p}. Right: ablation on the proportion ofDvD_{v}andDpD_{p}.",
                "position": 1183
            },
            {
                "img": "https://arxiv.org/html/2508.18756/x7.png",
                "caption": "Figure 7:Effect of UltraMem layer number on training dynamics and performance. Validation loss (left) and open-benchmark accuracy (right) for UltraMemV2-430M/5B with varying numbers of UltraMemV2 layers (2, 5, 10, 20) under fixed computational budget. Although there is no obvious gain in the validation set loss after adding a certain number of layers, the model with more UltraMemV2 layers performs better in downstream tasks.",
                "position": 1265
            },
            {
                "img": "https://arxiv.org/html/2508.18756/x8.png",
                "caption": "Figure 8:Tucker core penalty loss ablation study. Left: Open-Benchmark accuracy during training with and without Tucker core penalty loss. Right: Evolution of Tucker core eigenvalue magnitudes, showing the dominant eigenvalueλ1\\lambda_{1}(rank0) and second-largest eigenvalueλ2\\lambda_{2}(rank1). The penalty loss maintains eigenvalue separation without compromising downstream performance, validating that explicit eigenvalue constraints are unnecessary when the natural eigenvalue gap is sufficient.",
                "position": 1373
            },
            {
                "img": "https://arxiv.org/html/2508.18756/x9.png",
                "caption": "Figure 9:Effect of balance loss on training dynamics with different numbers of activated values. Training loss (left) and downstream accuracy on Open-Benchmark (right) for UltraMemV2-430M/5B with and without balance loss (β=0.001\\beta=0.001). Balance loss improves performance when fewer values are activated (TopM=47\\texttt{TopM}=47) but degrades performance with more activated values (TopM=94\\texttt{TopM}=94), indicating that the regularization benefit depends on the sparsity level.",
                "position": 1383
            },
            {
                "img": "https://arxiv.org/html/2508.18756/x10.png",
                "caption": "Figure 10:Comparison of Ring vs. NoRing topologies for shared memory configurations. Left: Training loss difference (smoothed) showing NoRing topology achieves lower training loss. Right: Open benchmark accuracy demonstrating the advantages of Ring, which achieving better final accuracy (Diff=0.014).",
                "position": 1461
            },
            {
                "img": "https://arxiv.org/html/2508.18756/x11.png",
                "caption": "Figure 11:Impact of the number of shared layers in Ring topology configurations. Left: Training loss difference (smoothed) relative to baseline showing S4-Ring, S9-Ring, and S16-Ring all achieve lower training loss, with benefits saturating around 9 shared layers. Right: Open benchmark accuracy demonstrating S9-Ring achieves the best downstream performance with 1-point improvement over baseline, while S16-Ring shows no improvement over S9-Ring.",
                "position": 1467
            },
            {
                "img": "https://arxiv.org/html/2508.18756/x12.png",
                "caption": "Figure 12:Comparison of block-wise sharing topology (S6-Block) against ring-based topology. Left: Training loss difference (smoothed) showing S6-Block achieves comparable training loss improvements to S9-Ring while being marginally inferior. Right: Open benchmark accuracy demonstrating S6-Block offers substantial improvements over baseline with performance Slightly worse than S9-Ring, validating block-wise sharing as a practical alternative for large-scale model training.",
                "position": 1473
            },
            {
                "img": "https://arxiv.org/html/2508.18756/x13.png",
                "caption": "Figure 13:Impact of value learning rate schedules on training dynamics and downstream performance. Left: Training loss difference (smoothed) showing the baseline (4x→1x decay) initially achieves lowest loss but converges with constant rate settings by 1.4T tokens. Right: Open-domain benchmark accuracy demonstrating constant 1x learning rate achieves best final performance (0.4 point improvement), suggesting decaying schedules provide early training benefits that diminish over extended training horizons.",
                "position": 1509
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Optimized Initialization",
        "images": []
    },
    {
        "header": "7Evaluation Benchmark",
        "images": []
    },
    {
        "header": "8Open-source model hyperparameters",
        "images": []
    }
]