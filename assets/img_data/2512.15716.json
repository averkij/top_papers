[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.15716/x1.png",
                "caption": "",
                "position": 64
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.15716/x2.png",
                "caption": "Figure 2:Overview of the training stage of Spatia. Each training video is divided into a target clip, a preceding clip, and a candidate-frame set. Text tokens are omitted for simplicity. (a) A frame is randomly selected from the candidate-frame set to estimate a 3D scene point cloudùíÆ\\mathcal{S}. Using the estimated camera poses together withùíÆ\\mathcal{S}, we then generate view-specific scene point cloud sequences for both the target and preceding clips. (b) The most spatially relevant frames are then retrieved from the candidate-frame set as reference frames. (c) The spatial conditions obtained from (a) and (b) guide the video generation process. The detailed network architecture is provided in Figure3.",
                "position": 164
            },
            {
                "img": "https://arxiv.org/html/2512.15716/x3.png",
                "caption": "Figure 3:Illustration of a single network block composed of one ControlNet[zhang2023adding]block operating in parallel with four main blocks. Detailed definitions of all token types are provided in Figure2.",
                "position": 232
            },
            {
                "img": "https://arxiv.org/html/2512.15716/x4.png",
                "caption": "Figure 4:Illustration of the Spatia inference process. At the first iteration, the user provides an initial image, from which Spatia estimates the initial 3D scene point cloud. The user then specifies a text instruction and a camera path based on the estimated scene, producing a projection video along the desired trajectory that conditions the generation of clip-1.\nIn subsequent iterations, two steps are performed: (1) Spatia updates the spatial memory (3D scene point cloud) using all previously generated frames via MapAnything[keetha2025mapanything]; and (2) the user specifies a new text instruction and camera path based on the updated scene. Spatia then takes the reference frames (generated as described in Section3.1.2), the previously generated clip, and the new projection video as input to produce the next video clip.Text instructions are omitted.",
                "position": 252
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6More Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.15716/x5.png",
                "caption": "Figure 5:Qualitative comparison of three variants for long-horizon video generation: (1) our default model Spatia, (2) a variant using only scene videos without reference frames, and (3) a variant using reference frames but no scene videos. The spatial memories shown in the figure are generated by Spatia.",
                "position": 1188
            },
            {
                "img": "https://arxiv.org/html/2512.15716/x6.png",
                "caption": "Figure 6:Visualization of closed-loop video generation. The camera follows a trajectory that returns to its initial viewpoint, enabling direct comparison between the first and final frames to evaluate the effectiveness of the spatial memory mechanism.",
                "position": 1191
            },
            {
                "img": "https://arxiv.org/html/2512.15716/x7.png",
                "caption": "Figure 7:Visualizations of dynamic‚Äìstatic disentanglement. Our model maintains a spatial memory containing only the static scene point cloud while generating videos that include dynamic entities acting within the same scenes.",
                "position": 1194
            },
            {
                "img": "https://arxiv.org/html/2512.15716/x8.png",
                "caption": "Figure 8:Demonstration of 3D-aware interactive editing. By directly modifying the spatial memory (i.e., the scene point cloud), users can achieve geometrically precise edits in the generated videos, such as removing an object (2nd row), adding a new object (3rd row), or altering object attributes (4th row).",
                "position": 1197
            }
        ]
    },
    {
        "header": "7Visualization",
        "images": []
    }
]