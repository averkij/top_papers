[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08584/images/header.jpeg",
                "caption": "",
                "position": 86
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08584/x1.png",
                "caption": "Figure 1:Overview of Ministral 3 training recipe.Pretraining: We start from pruning the parent model, Mistral Small 3.1, into the largest child model (14B Init.).\nNext, we continue pretraining the child model with logit distillation from the parent model as the teacher to obtain the up-trained short context child model (14B Short Ctx.).\nFrom 14B Short Ctx., we perform another round of distillation with longer context window (see §3.1for details) to obtain the final Ministral 3 14B Base model.\nIn parallel, 14B Short Ctx. is pruned to initialize the next child model (8B Init.), from which we repeat the process to derive Ministral 3 8B Base model.\nWe repeat the same process for the 3B version.Post-training:\nEach Base model is then post-trained into the instruction-following and reasoning variants.\nFor instruction-following, our post-training recipe includes supervised fine-tuning (SFT) and Online Direct Preference Optimization (ODPO).\nFor reasoning, the process involved supervised fine-tuning with chain-of-thought data (SFT w/ CoT), Group Relative Policy Optimization (GRPO;Shaoet al.[2024]), and ODPO.",
                "position": 104
            }
        ]
    },
    {
        "header": "2Model Architecture",
        "images": []
    },
    {
        "header": "3Training Recipe",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08584/images/plot_ce_loss_AVG.png",
                "caption": "Algorithm 1Cascade Distillation.",
                "position": 217
            },
            {
                "img": "https://arxiv.org/html/2601.08584/images/plot_ce_loss_AVG.png",
                "caption": "Figure 2:Illustration of Cascade Distillation.",
                "position": 286
            }
        ]
    },
    {
        "header": "4Results",
        "images": []
    },
    {
        "header": "5Discussions",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08584/images/MM3_vs_MS3_Pretraining.png",
                "caption": "Figure 3:Ministral 3 14B pretraining ablations comparing distillation from Mistral Small 3.1 and Mistral Medium 3 teachers. Despite Mistral Medium 3 being larger and more capable, distillation from Mistral Small 3.1 consistently yields stronger downstream performance across different benchmarks.",
                "position": 1044
            },
            {
                "img": "https://arxiv.org/html/2601.08584/images/MS3_Base_vs_Instruct_Teacher.png",
                "caption": "Figure 4:Ministral 3 3B pretraining ablations comparing distillation from base and post-trained (instruct/reasoning) variants of Mistral Small 3.1. The instruct teacher yields stronger performance on STEM benchmarks, while achieving comparable results on knowledge and multimodal evaluations..",
                "position": 1056
            },
            {
                "img": "https://arxiv.org/html/2601.08584/images/blog_triangle_plot.png",
                "caption": "Figure 5:Verbosity (in terms of number of output tokens) v.s. accuracy on GPQA Diamond with Ministral 3 instruction-following and reasoning.",
                "position": 1077
            },
            {
                "img": "https://arxiv.org/html/2601.08584/images/reasoning_odpo.png",
                "caption": "Figure 6:Impact of ODPO on chat benchmarks for Ministral 3 reasoning models, applied on top of GRPO-trained checkpoints. ODPO delivers substantial gains across all benchmarks for the 14B and 8B variants.",
                "position": 1115
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]