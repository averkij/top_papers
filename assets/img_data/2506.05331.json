[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05331/x1.png",
                "caption": "Figure 1:Comparison of three CoT reasoning methods: text-only CoT reasoning, box-shaped visual CoT reasoning and our visual interleaved CoT reasoning methods. (1) Text-only CoT lacks visual information, causing perception errors in mathematical reasoning. (2) Box-level cues are too coarse to capture complex visual structures in mathematical images. (3) Token-level interleaved CoT accurately identifies fine-grained visual regions to support reasoning.",
                "position": 139
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05331/x2.png",
                "caption": "Figure 2:Overview of the MINT-CoT framework.During CoT reasoning, MINT-CoT generates an Interleave Token before each reasoning step and computes the similarity scores between embeddings projected by the decoder-side visual projector and the interleave projector. Based on these similarity scores, relevant visual tokens are selected, and the model inferences with these selected visual tokens.",
                "position": 231
            },
            {
                "img": "https://arxiv.org/html/2506.05331/x3.png",
                "caption": "Figure 3:Data generation pipline.Step 1: Grid Images.We divide each image into grid cells and assign index values to each cell.Step 2: Apply OCR.We use PaddleOCR to recognize textual elements and associate them with corresponding grid indices.Step 3:Extract Key Words. We employ GPT-4o to extract key words from each reasoning step.Step 4: Align and Annotate Key Words.We use GPT-4o to annotate each key word with the grid indices, and get the final visual interleaved CoT reasoning steps.",
                "position": 303
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05331/x4.png",
                "caption": "Table 5:Ablation study of different interleaving methods on GeoQA and MathVista-Math.Our Interleaved CoT SFT achieves the highest improvement on both benchmarks, demonstrating the effectiveness of our interleaved token selection method.",
                "position": 811
            },
            {
                "img": "https://arxiv.org/html/2506.05331/x4.png",
                "caption": "Figure 4:F1 score plot of visual token selection during Interleaved CoT SFT.",
                "position": 884
            },
            {
                "img": "https://arxiv.org/html/2506.05331/x5.png",
                "caption": "Figure 5:Qualitative results of Qwen2-VL-7B-Instruct and MINT-CoT-7B. MINT-CoT-7B demonstrates improved CoT reasoning capability by interleaving fine-grained visual tokens. There is also a visualization of the similarity scores for the Interleaved Token generated during Step 4.",
                "position": 926
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05331/x6.png",
                "caption": "Figure 6:An example from MINT-CoT dataset.",
                "position": 2298
            },
            {
                "img": "https://arxiv.org/html/2506.05331/x7.png",
                "caption": "Figure 7:An example from MINT-CoT dataset.",
                "position": 2301
            },
            {
                "img": "https://arxiv.org/html/2506.05331/x8.png",
                "caption": "Figure 8:An example from MINT-CoT dataset.",
                "position": 2304
            },
            {
                "img": "https://arxiv.org/html/2506.05331/x9.png",
                "caption": "Figure 9:Comparison between Qwen2-VL-7B-Instruct and MINT-CoT-7B.",
                "position": 2307
            },
            {
                "img": "https://arxiv.org/html/2506.05331/x10.png",
                "caption": "Figure 10:Comparison between Qwen2-VL-7B-Instruct and MINT-CoT-7B.",
                "position": 2310
            },
            {
                "img": "https://arxiv.org/html/2506.05331/x11.png",
                "caption": "Figure 11:Comparison between Qwen2-VL-7B-Instruct and MINT-CoT-7B.",
                "position": 2313
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]