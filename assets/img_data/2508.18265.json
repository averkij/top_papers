[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.18265/x1.png",
                "caption": "Figure 1:Comparison between InternVL3.5 and leading MLLMs in general capabilities.Hatched bars represent closed-source commercial models.\nWe report average scores on a set of multimodal general, reasoning, text, and agentic benchmarks:\nMMBench v1.1 (en)liu2023mmbench, MMStarchen2024mmstar, BLINKfu2024blink, HallusionBenchguan2023hallusionbench, AI2Dkembhavi2016ai2d, OCRBenchliu2023ocrbench, MMVetyu2023mmvet, MME-RealWorld (en)zhang2024mme, MVBenchli2024mvbench, VideoMMEfu2024video, MMMUyue2023mmmu, MathVistalu2023mathvista, MathVisionwang2024mathvision, MathVersezhang2024mathverse, DynaMathzou2024dynamath, WeMathqiao2024wemath, LogicVistaxiao2024logicvista, MATH500DBLP:conf/nips/HendrycksBKABTS21, AIME24aime2024, AIME25aime2025, GPQArein2024gpqa, MMLU-Prowang2024mmlu, GAOKAOZhang2023gaokao, IFEvalzhou2023instruction,\nSGP-Benchqiu2024can, VSI-Benchyang2024think, ERQAerqa, SpaCE-10gong2025space10, and OmniSpatialjia2025omnispatial.",
                "position": 166
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2InternVL3.5",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.18265/x2.png",
                "caption": "Figure 2:Overall architecture.InternVL3.5 adopts the “ViT–MLP–LLM” paradigm as in previous versions. Building upon InternVL3.5, we further introduce InternVL3.5-Flash, which is extended with an additional visual resolution router (ViR) to dynamically select the appropriate compression rate (e.g.,14\\frac{1}{4}or116\\frac{1}{16}) for each image patch. Unlike Dynamic High Resolution which only splits image patches from the perspective of image width and height, our proposed ViR further introduces adaptivity from the perspective of semantic content.",
                "position": 218
            },
            {
                "img": "https://arxiv.org/html/2508.18265/x3.png",
                "caption": "Figure 3:Training recipes of InternVL3.5.InternVL3.5 consists of three training stages: (1) native pre-training for vision-language alignment, (2) supervised fine-tuning for adaptation to downstream tasks, (3) Cascade RL for improvement on reasoning capabilities. InternVL3.5-Flash is an efficient version of InternVL3.5, which further integrates a visual resolution router (ViR) through consistency training and router training.",
                "position": 371
            },
            {
                "img": "https://arxiv.org/html/2508.18265/x4.png",
                "caption": "Figure 4:Overview of Decoupled Vision-Language Deployment.DvD decouples the vision and language models and deploys them on separate servers. The right side shows a time-consumption trace of the pipeline.(a)In the original deployment, the ViT, MLP, and LLM are executed sequentially. Given their substantial differences in size and computation patterns, this serial design significantly slows down inference.(b)With DvD, the inference of the ViT and the LLM is performed in parallel and asynchronously. Thus, ViT’s computations can overlap with the LLM’s prefilling and decoding, reducing resource conflicts and improving inference speed.",
                "position": 541
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.18265/x5.png",
                "caption": "Figure 5:Ablation on Cascade RL.We report average scores on the same set of multimodal reasoning and mathematical benchmarks as in Table3. Full results are provided in Table15.",
                "position": 8956
            }
        ]
    },
    {
        "header": "4Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]