[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16969/x1.png",
                "caption": "",
                "position": 254
            },
            {
                "img": "https://arxiv.org/html/2512.16969/x2.png",
                "caption": "",
                "position": 255
            },
            {
                "img": "https://arxiv.org/html/2512.16969/x3.png",
                "caption": "",
                "position": 256
            },
            {
                "img": "https://arxiv.org/html/2512.16969/x4.png",
                "caption": "",
                "position": 266
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Scientific General Intelligence: Concept and Operational Definition",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16969/x5.png",
                "caption": "Figure 2:SGI-Bench Workflow Pipeline: The end-to-end four-stage framework (Deliberation, Conception, Action, Perception) that operationalizes scientific discovery, mapping tasks to capabilities and aligning evaluation with scientist practice.",
                "position": 437
            },
            {
                "img": "https://arxiv.org/html/2512.16969/x6.png",
                "caption": "Figure 8:Benchmark Subjects: Overview of 10 scientific domains covered by SGI-Bench.",
                "position": 1448
            },
            {
                "img": "https://arxiv.org/html/2512.16969/x7.png",
                "caption": "Figure 9:Benchmark Data Distribution: (a) Overall discipline distribution; (b) Dry experiment discipline distribution; (c) Wet experiment discipline distribution; (d) Scientific Deep Research question types; (e) Dry Experiment function types; (f) Experimental Reasoning image modalities; (g) Experimental Reasoning reasoning paradigms.",
                "position": 1456
            }
        ]
    },
    {
        "header": "3SGIEvalAgent: Agentic Evaluation Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16969/x8.png",
                "caption": "Figure 10:Evaluation Framework.",
                "position": 1515
            }
        ]
    },
    {
        "header": "4Evaluation Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16969/x9.png",
                "caption": "Table 5:Overview Results Across SGI-Bench Tasks: Aggregated performance across Deep Research, Idea Generation, Dry/Wet Experiment, and Experimental Reasoning. The scores for Deep Research are based on the exact match metric (the strictest metric). Idea Generation scores are the average of four metrics evaluating ideas. Dry Experiment scores are based on PassAll@5 (the strictest metric). Wet Experiment scores are the average of action sequence similarity and parameter accuracy. Experimental Reasoning scores are based on the multi-choice accuracy metric (the strictest metric). The SGI-Score is the average across these tasks, reflecting the overall capability of an AI model in various scientific research scenarios. An asterisk∗indicates results from different versions of the same series of multimodal models.",
                "position": 1703
            },
            {
                "img": "https://arxiv.org/html/2512.16969/x10.png",
                "caption": "",
                "position": 1907
            },
            {
                "img": "https://arxiv.org/html/2512.16969/x11.png",
                "caption": "",
                "position": 1925
            },
            {
                "img": "https://arxiv.org/html/2512.16969/x12.png",
                "caption": "Figure 11:Scientific Deep Research Case: Example multi-hop workflow illustrating data retrieval, evidence synthesis, and quantitative analysis.",
                "position": 1963
            },
            {
                "img": "https://arxiv.org/html/2512.16969/x13.png",
                "caption": "Figure 12:Scientific Deep Research Evaluation of LLMs: Exact Match (EM) and Step-Level Accuracy (SLA) across models using scientist-aligned metrics.",
                "position": 1978
            },
            {
                "img": "https://arxiv.org/html/2512.16969/x14.png",
                "caption": "Figure 13:Scientific Deep Research Evaluation of Multi-Agent Systems: EM and SLA for tool-augmented agent systems.",
                "position": 1982
            },
            {
                "img": "https://arxiv.org/html/2512.16969/x15.png",
                "caption": "Figure 14:Scientific Deep Research Performance by Type: Comparison across Data, Properties, Micro-Experiments, and Macro-Experiments categories.",
                "position": 1987
            },
            {
                "img": "https://arxiv.org/html/2512.16969/x16.png",
                "caption": "Figure 15:Idea Generation Case: Input information such as related work and objective, and output a structured idea, including a graph consisting of specific implementation steps.",
                "position": 2001
            },
            {
                "img": "https://arxiv.org/html/2512.16969/x17.png",
                "caption": "Table 6:Idea Generation Results: The ideas generated by the model outperformed the average proportion of the original papers in the four dimensions of Effectiveness, Novelty, Detailedness, and Feasibility.",
                "position": 2005
            },
            {
                "img": "https://arxiv.org/html/2512.16969/x17.png",
                "caption": "Figure 16:Dry Experiment Code Examples: Masked-function completion setup with I/O formats, and functional descriptions.",
                "position": 2265
            },
            {
                "img": "https://arxiv.org/html/2512.16969/x18.png",
                "caption": "Table 7:Dry Experiment Metrics Across Models: PassAll@k, Average Execution Time (AET), and Smooth Execution Rate (SER) under five unit tests per problem.",
                "position": 2272
            },
            {
                "img": "https://arxiv.org/html/2512.16969/x18.png",
                "caption": "Figure 17:PassAll@5 by Function Category: Completion accuracy across numerical calculation, statistical analysis, simulation, metric calculation, data processing, and predictive modeling.",
                "position": 2508
            },
            {
                "img": "https://arxiv.org/html/2512.16969/x19.png",
                "caption": "Figure 18:Dry Experiment Case Study: Gravitational-wave computation highlighting the impact of numerical integration strategy on scientific outcomes.",
                "position": 2520
            },
            {
                "img": "https://arxiv.org/html/2512.16969/x20.png",
                "caption": "Figure 19:Wet Experiment Workflow: Action-pool based protocol construction with typical errors in step sequencing and parameter specification.",
                "position": 2567
            },
            {
                "img": "https://arxiv.org/html/2512.16969/x21.png",
                "caption": "Figure 20:Wet Experiment Evaluation: Sequence Similarity (SS) and Parameter Accuracy (PA) across models for laboratory protocol planning.",
                "position": 2574
            },
            {
                "img": "https://arxiv.org/html/2512.16969/x22.png",
                "caption": "Figure 21:Wet Experiment Case Study: NSCLC anti–PD-1 immunotherapy workflow—ground-truth protocol versus model-generated design.",
                "position": 2582
            },
            {
                "img": "https://arxiv.org/html/2512.16969/x23.png",
                "caption": "Figure 22:Experimental Reasoning Modalities: Examples of process, visualization, observation, simulation, and experiment images used as multi-modal evidence.",
                "position": 2599
            },
            {
                "img": "https://arxiv.org/html/2512.16969/x24.png",
                "caption": "Figure 23:Experimental Reasoning Case: Multi-image question requiring cross-modal synthesis and step-wise reasoning.",
                "position": 2606
            },
            {
                "img": "https://arxiv.org/html/2512.16969/x25.png",
                "caption": "Figure 24:Experimental Reasoning Evaluation: Multi-Choice Accuracy (MCA) and Reasoning Validity (RV) across models on multimodal tasks.",
                "position": 2613
            },
            {
                "img": "https://arxiv.org/html/2512.16969/x26.png",
                "caption": "Figure 25:Experimental Reasoning Performance by Type and Discipline: Breakdown across reasoning paradigms (signal, attribute, comparative, causal) and 10 scientific domains.",
                "position": 2621
            }
        ]
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16969/x27.png",
                "caption": "Figure 26:TTRL Training Framework: The model generates candidate ideas evaluated against online retrieved related works to calculate novelty rewards, guiding GRPO updates.",
                "position": 2652
            },
            {
                "img": "https://arxiv.org/html/2512.16969/x28.png",
                "caption": "Table 8:TTRL Hyperparameters: Key training configuration for GRPO-based test-time reinforcement learning.",
                "position": 2721
            },
            {
                "img": "https://arxiv.org/html/2512.16969/x28.png",
                "caption": "Figure 27:TTRL Training Dynamics: Format reward saturates quickly, followed by steady growth in idea novelty.",
                "position": 2778
            },
            {
                "img": "https://arxiv.org/html/2512.16969/x29.png",
                "caption": "Figure 28:TTRL Case Study: Comparison of generated research ideas before and after TTRL, highlighting structural innovation (dual-branch transformer, differentiable physics engine) versus generic pre-training assembly.",
                "position": 2799
            },
            {
                "img": "https://arxiv.org/html/2512.16969/x30.png",
                "caption": "Figure 29:Agent Tool Calls: Frequency (left) and efficiency (right) across leading models.",
                "position": 2840
            },
            {
                "img": "https://arxiv.org/html/2512.16969/x31.png",
                "caption": "Figure 30:SGIEvalAgent Case: Model Users describe their evaluation needs, SGIEvalAgent customizes the evaluation plan and metrics based on these needs, and finally provides an evaluation report..",
                "position": 2849
            }
        ]
    },
    {
        "header": "6Challenges and Future Directions",
        "images": []
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]