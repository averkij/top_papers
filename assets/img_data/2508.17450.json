[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.17450/x1.png",
                "caption": "Figure 1:Overview ofDuET-PD(DualEvaluation forTrust inPersuasiveDialogues). After evaluating initial stances, LLMs engage in multi-turn dialogues featuring either positive (corrective) or negative (misleading) persuasion, using established appeal techniques (ยง3.3). Stance checks occur after each turn.",
                "position": 140
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Persuasion Dataset Construction",
        "images": []
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.17450/x2.png",
                "caption": "Figure 2:An overview of the construction of ourResistandHolisticDPO datasets. The Holistic DPO dataset comprises 3 types of samples: Baseline (which rewards correct answers at turn 0, Resist (which rewards refutations and correct answers under negative persuasions), and Relent (which rewards affirmations and correct answers under positive persuasions).",
                "position": 436
            }
        ]
    },
    {
        "header": "5Results & Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.17450/x3.png",
                "caption": "Figure 3:Accuracy evolution over 3 turns of POS/NEG persuasion on MMLU-Pro and SALAD-Bench for GPT-4o, GPT-4o-mini, and Llama-3.1-8B-Instruct, averaged across all seven persuasive techniques.",
                "position": 474
            },
            {
                "img": "https://arxiv.org/html/2508.17450/x4.png",
                "caption": "(a)MMLU-Pro",
                "position": 477
            },
            {
                "img": "https://arxiv.org/html/2508.17450/x4.png",
                "caption": "(a)MMLU-Pro",
                "position": 480
            },
            {
                "img": "https://arxiv.org/html/2508.17450/x5.png",
                "caption": "(b)SALAD-Bench",
                "position": 485
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADataset Details",
        "images": []
    },
    {
        "header": "Appendix BTraining Details",
        "images": []
    },
    {
        "header": "Appendix CEffect of Training Data Size on DPO Performance",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.17450/x6.png",
                "caption": "Figure 5:Evolution of Accuracy across 3 turns of POS and NEG persuasions for MMLU-Pro and SALAD-Bench, comparing Llama-3.1-8B-Instruct models fine-tuned with varying DPO dataset sizes (20% to 100%) for Resist and Holistic strategies.",
                "position": 2000
            }
        ]
    },
    {
        "header": "Appendix DPersuasion Appeal Generation and Validation",
        "images": []
    },
    {
        "header": "Appendix EPrompts",
        "images": []
    },
    {
        "header": "Appendix FPerformance Breakdown by Category (Averaged Across Models)",
        "images": []
    },
    {
        "header": "Appendix GFormal Metric and Confidence Definitions",
        "images": []
    },
    {
        "header": "Appendix HExamples of Generated Appeals",
        "images": []
    },
    {
        "header": "Appendix IConversation Samples",
        "images": []
    }
]