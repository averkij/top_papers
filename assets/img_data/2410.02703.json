[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02703/x1.png",
                "caption": "Figure 1:A visualization of the soft-masking by selective attention (red strike-through) and attention strength (averaged across heads, blue highlight) for different tasks (see Section2).",
                "position": 120
            }
        ]
    },
    {
        "header": "2Motivating Examples",
        "images": []
    },
    {
        "header": "3Selective Attention",
        "images": []
    },
    {
        "header": "4Context Pruning",
        "images": []
    },
    {
        "header": "5Experimental Setup",
        "images": []
    },
    {
        "header": "6Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02703/extracted/5899480/images/comb.png",
                "caption": "Figure 3:(Left)The validation perplexity of ad=12𝑑12d=12italic_d = 12transformer, with (blue) and without (orange) selective attention, for varying context sizes.(Right)The validation perplexity of transformers of various sizes, with (blue) and without (orange) selective attention, for a context size of 512.",
                "position": 357
            },
            {
                "img": "https://arxiv.org/html/2410.02703/extracted/5899480/images/extra_heads.png",
                "caption": "Figure 4:Perplexity on the C4 validation set after 524,288 training steps of transformers with various sizes (parameterized byd𝑑ditalic_das per Section5) with and without selective attention. For the cases without selective attention we add additional attention heads with their respective parameters (i.e. we increase the sizes of all projection matrices). Transformers with selective attention perform equivalently to those with standard attention modules with∼similar-to\\sim∼2X as many heads and parameters.",
                "position": 364
            }
        ]
    },
    {
        "header": "7Selection Patterns",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02703/extracted/5899480/images/interp_triangles_thresh2.png",
                "caption": "Figure 5:Visualization of theF𝐹Fitalic_Fmatrix (greener is lower, i.e. less masking) for ad=12𝑑12d=12italic_d = 12transformer for the text in AppendixA.6.",
                "position": 463
            }
        ]
    },
    {
        "header": "8Related Works",
        "images": []
    },
    {
        "header": "9Discussion",
        "images": []
    },
    {
        "header": "10Improving Neural Architectures",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02703/extracted/5899480/images/tradeoffs.png",
                "caption": "Figure 6:The trade-off between perplexity and KV-cache size ford=12𝑑12d=12italic_d = 12transformers with context sizes of 512, 1,024, and 2,048. Note that in all cases the perplexity with selective attention is better or equal to that of the baseline without selective attention (the dotted lines). Selective attention transformers trained with theℒm⁢e⁢msubscriptℒ𝑚𝑒𝑚\\mathcal{L}_{mem}caligraphic_L start_POSTSUBSCRIPT italic_m italic_e italic_m end_POSTSUBSCRIPTloss andϵ=0.1italic-ϵ0.1\\epsilon=0.1italic_ϵ = 0.1(Equation2) match the perplexity of the baseline with 16X, 25X, and 47X less memory, while those with the standard loss match the perplexity of the baseline with 5X, 7X, and 8X less memory, respectively.",
                "position": 1382
            },
            {
                "img": "https://arxiv.org/html/2410.02703/extracted/5899480/images/interp_smooth.png",
                "caption": "Figure 7:Visualization of theF𝐹Fitalic_Fmatrix (greener is lower, i.e. less masking) for ad=12𝑑12d=12italic_d = 12transformer averaged across 1,000 examples.",
                "position": 1504
            },
            {
                "img": "https://arxiv.org/html/2410.02703/x2.png",
                "caption": "Figure 8:Visualization of theF𝐹Fitalic_Fmatrix (greener/bluer is lower, i.e. less masking) for ad=12𝑑12d=12italic_d = 12transformer averaged across 1,000 examples for two training runs (different random initialization, and different shuffle of the training data). While we only have anecdotal evidence, it is interesting that we sometimes observe these stable sparsity patterns across training runs.",
                "position": 1507
            },
            {
                "img": "https://arxiv.org/html/2410.02703/extracted/5899480/images/interp_triangles_2.png",
                "caption": "Figure 9:Visualization of the persisted elements for ad=12𝑑12d=12italic_d = 12transformer for the text in AppendixA.6. The white pixels denote tokens removed from the context buffer as per Section4.",
                "position": 1510
            },
            {
                "img": "https://arxiv.org/html/2410.02703/extracted/5899480/images/ashkara_detailed_2.png",
                "caption": "Figure 10:A visualizing of the elements that are masked for the last (512th) token, for ad=12𝑑12d=12italic_d = 12transformer for the text in AppendixA.6. We observe some interesting patterns, for example, layer 4 persists the end-of-sentence periods (“.”).",
                "position": 1513
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]