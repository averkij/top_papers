[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21468/figures/teaser.png",
                "caption": "Figure 1:Comparison of memory paradigms.(a)Raw History Memoryfetches relevant history passages but suffers from noise and redundancy.\n(b)Textual Summary Memoryallows the agent to summarize the history but suffers from uniform information density, where auxiliary details (gray) consume as much token space as crucial information (green).\n(c)Visual Memory (Ours)allocates memory budget via visual layout to achieve adaptive information density.",
                "position": 262
            },
            {
                "img": "https://arxiv.org/html/2601.21468/figures/framework.png",
                "caption": "Figure 2:Framework of MemOCR.(a)Memory Drafting (Text Domain):The LLM agent incrementally updates a rich-text memory based on new incoming chunks, assigning visual priority via formatting and structure.\n(b)Memory Reading (Vision Domain):The rich text is rendered into a 2D memory image, which serves as the agent’s sole working context for answering queries.\n(c)Budget-Aware Training Objectives:We train the agent under varying degrees of memory compression. The drafting ability is updated via aggregated advantages, while the reading ability is updated via separate advantages.",
                "position": 292
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21468/figures/data_augmentation.png",
                "caption": "Figure 3:Design of the budget-aware training objectives.(1)Standard QAuses the unmodified question and memory for global correctness.\n(2)QA w/ Augmented Memoryrequires the visibility of crucial evidence even when the visual memory is heavily compressed.\n(3)QA w/ Augmented Questionensures detailed information is clearly identified with sufficient tokens.\nThe low-budget, high-detail setting (gray area) is excluded as identifying detailed features under severe compression is infeasible.",
                "position": 380
            }
        ]
    },
    {
        "header": "3Method: MemOCR",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21468/x1.png",
                "caption": "Figure 4:Comparison of accuracy and relative performance drop across varying memory budgets (RQ2).MemOCR degrades more gracefully than textual baselines as budgets tighten.\nWithout visual layout, MemOCR’s low-budget robustness drops significantly, which suggests that adaptive information density facilitates more efficient memory budget utilization.",
                "position": 1354
            },
            {
                "img": "https://arxiv.org/html/2601.21468/x2.png",
                "caption": "Figure 5:Oracle analysis of layout regions (RQ3).We compare MemOCR with oracle variants that inject ground-truth evidence into either theCrucialorDetailedregion of the rendered memory.\nWhile both injections improve accuracy, injecting into the crucial region yields larger gains, especially under tight memory budgets.",
                "position": 1361
            },
            {
                "img": "https://arxiv.org/html/2601.21468/figures/rq3_density_length.png",
                "caption": "Figure 6:RL induces adaptive information density (RQ3).(a) With training, ground-truth evidence becomes more concentrated in the crucial region while decreasing in the detailed region.\n(b) The crucial region remains much shorter than the detail part.",
                "position": 1368
            },
            {
                "img": "https://arxiv.org/html/2601.21468/figures/case_study.png",
                "caption": "Figure 7:Case study at an extreme memory budget (16 tokens).(Left) The textual baseline fails due to hard truncation of the context.\n(Middle) MemOCR without layout control fails because uniform text becomes unreadable after down-sampling.\n(Right) MemOCR preserves the crucial evidence “Gene MacLellan” through adaptive layout, enabling correct reasoning even at low resolution.",
                "position": 1397
            }
        ]
    },
    {
        "header": "5Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelated Work",
        "images": []
    },
    {
        "header": "Appendix BLimitations",
        "images": []
    },
    {
        "header": "Appendix CImplementation Details",
        "images": []
    },
    {
        "header": "Appendix DSupplementary Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21468/figures/bad_cases.png",
                "caption": "Figure 8:Failure Mode Analysis under Resource Constraints (16-token budget).(Top) In comparative reasoning (i.e.,to choose among two candidates), while the layout successfully highlights entity headers, the body text containing crucial attributes is compressed into unreadable noise during downsampling.\n(Bottom) When the rich-text memory length exceeds the visual canvas capacity, the forced font scaling drops below the visual encoder’s resolution threshold, resulting in information loss.",
                "position": 3637
            }
        ]
    },
    {
        "header": "Appendix EBad Case Analysis",
        "images": []
    }
]