[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22921/x1.png",
                "caption": "Figure 1:Example illustrating that checking the final answer alone is insufficient for evaluating reasoning. GRPO (right) makes mistakes and reaches a wrong answer (4.76) but takes an extra rounding step to the correct one (5), likely as a learned strategy through training.",
                "position": 122
            },
            {
                "img": "https://arxiv.org/html/2509.22921/x1.png",
                "caption": "Figure 2:Comparison of our method against baselines across with Qwen2.5-1.5B-Math (top) and Llama-3.2-3B (bottom) averaged across three evaluation datasets. To ensure bigger surface means better results, the reasoning loss rate and the KL divergence were inverted.",
                "position": 233
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22921/x2.png",
                "caption": "Figure 3:Pareto frontier analysis showing the trade-off between final answer correctness and constraint satisfaction across different methods and hyperparameter settings for Qwen2.5-1.5B-Math. Each point represents a different method configuration. The points in red belong to the Pareto front.",
                "position": 588
            },
            {
                "img": "https://arxiv.org/html/2509.22921/x3.png",
                "caption": "Figure 4:Pairwise comparison heatmap showing the relative performance of our method against baselines, averaged across all three evaluation datasets (Apple/GSM-Symbolic, GSM8K, and MATH) with Qwen2.5-3B-Math. Darker colors indicate superior performance in row-to-column comparisons. Averaging over columns gives the reasoning win rate (RWR) and over rows the reasoning loss rate (RLR).",
                "position": 610
            }
        ]
    },
    {
        "header": "5Related Works",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADerivation of Policy Gradient",
        "images": []
    },
    {
        "header": "Appendix BProofs of Constraint Satisfaction Guarantee",
        "images": []
    },
    {
        "header": "Appendix CA perspective of LLM distillation as Contextual MDPs",
        "images": []
    },
    {
        "header": "Appendix DAlgorithm and Implementation",
        "images": []
    },
    {
        "header": "Appendix EMore experiments results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22921/x4.png",
                "caption": "Figure 5:Pairwise comparison heatmap showing the relative performance of our method against baselines, averaged across all three evaluation datasets (Apple/GSM-Symbolic, GSM8K, and MATH) on Llama3.2-3B. Averaging over columns gives the reasoning win rate (RWR) and over rows the reasoning loss rate (RLR).",
                "position": 2298
            }
        ]
    },
    {
        "header": "Appendix FGenerated Answers",
        "images": []
    }
]