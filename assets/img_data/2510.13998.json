[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13998/x1.png",
                "caption": "Figure 1:Performance on downstream tasks across model size, with inference speed and memory efficiency comparison. We observed that directly finetuning full-precision LLMs into 1.58-bit LLMs (denoted as 1.58-bit BitNet-SFT) leads to a notable performance gap compared to the FP16 baseline, and this gap remains or even widens as the model size increases. In contrast,BitDistillpreserves scalability, resulting in performance comparable to full-precision counterparts across all model size, while reducing10×10\\timesmemory usage and2.65×2.65\\timesfaster inference on CPUs.",
                "position": 61
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3BitDistill: Finetuning LLMs into 1.58-bit BitNet For Downstream Tasks",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13998/sections/pics/step100k_16_all_projections_comparison.png",
                "caption": "Figure 2:Visualization of model weights. The top two rows show the quantized weights of BitNet trained from scratch along with their corresponding FP16 distributions. The bottom two rows show the quantized weights of BitNet after loading weights from LLMs and performing continued training (stage-2 in §3.2), together with their corresponding FP16 distributions.",
                "position": 614
            },
            {
                "img": "https://arxiv.org/html/2510.13998/sections/pics/SubLN.png",
                "caption": "Figure 3:Analysis of SubLN, layer selection for Eq.12and teacher selection over training steps. (a) Fine-tuning existing LLMs into 1.58-bit BitNet with SubLN yields better performance and faster convergence. (b) Comparison of MNLI accuracies obtained by distilling different layers on Qwen3-0.6B. (c) Comparison of MNLI accuracies obtained by distilling Qwen3-0.6B with different size of FP16 teachers.",
                "position": 636
            },
            {
                "img": "https://arxiv.org/html/2510.13998/sections/pics/distill.png",
                "caption": "",
                "position": 639
            },
            {
                "img": "https://arxiv.org/html/2510.13998/sections/pics/teacher_bar.png",
                "caption": "",
                "position": 640
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]