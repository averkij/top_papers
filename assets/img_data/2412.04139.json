[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04139/x1.png",
                "caption": "Figure 1:Architectural comparison of expert scaling approaches in large language models. (1)PEERstoresNùëÅNitalic_Nstandalone experts accessed via product key retrieval, resulting in memory usage that grows linearly with the number of experts,O‚Å¢(N)ùëÇùëÅO(N)italic_O ( italic_N ). (2)¬†Our proposedMonet-HD(Horizontal Decomposition) partitions experts into bottom and top layers, dynamically composing experts. This reduces space complexity toO‚Å¢(N)ùëÇùëÅO(\\sqrt{N})italic_O ( square-root start_ARG italic_N end_ARG ). (3)Monet-VD(Vertical Decomposition) orthogonally partitions layers with left and right segments,\nwhile maintaining the same space complexity.",
                "position": 330
            },
            {
                "img": "https://arxiv.org/html/2412.04139/x2.png",
                "caption": "Figure 3:Knowledge unlearning and accuracy perturbation across 14 MMLU domains. Rows represent the domains where knowledge unlearning was applied, while columns display theresultingperformance of the LLM in each domain. In(a)Monet(Ours), expertsthat show skewed routing scoresfor the target domain were removed. In(b) Gemma Scope, sparse SAE features for the target domain were suppressed.In(c) OLMoE, the most activated expert per domainwasremoved. In(d)LLaMA, domain-specific MLP neurons were suppressed based on first-layer activations.Bright pixels indicateminimal accuracy loss, whiledarker pixelsrepresent agreaterdrop.",
                "position": 1417
            },
            {
                "img": "https://arxiv.org/html/2412.04139/x3.png",
                "caption": "",
                "position": 1427
            },
            {
                "img": "https://arxiv.org/html/2412.04139/x4.png",
                "caption": "",
                "position": 1434
            },
            {
                "img": "https://arxiv.org/html/2412.04139/x5.png",
                "caption": "",
                "position": 1440
            },
            {
                "img": "https://arxiv.org/html/2412.04139/x6.png",
                "caption": "Figure 5:Detection of toxic experts through token activations and toxicity scores. The top row lists example tokens that highly activate each expert. The bottom row displays scatter plots corresponding to these experts, where each blue point represents a token activation from the RealToxicityPrompts dataset. In the scatter plots, the x-axis indicates the toxicity score of the token, and the y-axis shows the routing score assigned to the expert for that token. The correlation coefficient between toxicity scores and expert routing scores is noted above each plot. High correlation coefficients enabled us to identify experts associated with toxic knowledge within the model.",
                "position": 4080
            },
            {
                "img": "https://arxiv.org/html/2412.04139/x7.png",
                "caption": "",
                "position": 4268
            },
            {
                "img": "https://arxiv.org/html/2412.04139/x8.png",
                "caption": "",
                "position": 4449
            },
            {
                "img": "https://arxiv.org/html/2412.04139/x9.png",
                "caption": "",
                "position": 4454
            },
            {
                "img": "https://arxiv.org/html/2412.04139/extracted/6047748/Examples/Appendix-Llava/Images/visionmonet-1b4-4-189891.jpeg",
                "caption": "Figure 9:List of image and text activation examples of vision-language modelVisionMonet‚Äôs experts. Image examples were sampled from the CC3M(Sharma et¬†al.,2018)dataset, based on the routing score of a multimodal expert.",
                "position": 7950
            },
            {
                "img": "https://arxiv.org/html/2412.04139/extracted/6047748/Examples/Appendix-Llava/Images/visionmonet-1b4-4-184117.jpeg",
                "caption": "",
                "position": 8093
            },
            {
                "img": "https://arxiv.org/html/2412.04139/extracted/6047748/Examples/Appendix-Llava/Images/visionmonet-1b4-4-57497.jpeg",
                "caption": "",
                "position": 8166
            },
            {
                "img": "https://arxiv.org/html/2412.04139/extracted/6047748/Examples/Appendix-Llava/Images/visionmonet-1b4-4-133620.jpeg",
                "caption": "",
                "position": 8242
            },
            {
                "img": "https://arxiv.org/html/2412.04139/extracted/6047748/Examples/Appendix-Llava/Images/visionmonet-1b4-4-250250.jpeg",
                "caption": "",
                "position": 8311
            },
            {
                "img": "https://arxiv.org/html/2412.04139/extracted/6047748/Examples/Appendix-Llava/Images/visionmonet-1b4-5-49776.jpeg",
                "caption": "",
                "position": 8379
            },
            {
                "img": "https://arxiv.org/html/2412.04139/extracted/6047748/Examples/Appendix-Llava/Images/visionmonet-1b4-5-100768.jpeg",
                "caption": "Figure 10:List of image and text activation examples of vision-language modelVisionMonet‚Äôs experts. Image examples were sampled from the CC3M(Sharma et¬†al.,2018)dataset, based the routing score of a multimodal expert.",
                "position": 8386
            },
            {
                "img": "https://arxiv.org/html/2412.04139/extracted/6047748/Examples/Appendix-Llava/Images/visionmonet-1b4-2-50634.jpeg",
                "caption": "",
                "position": 8529
            },
            {
                "img": "https://arxiv.org/html/2412.04139/extracted/6047748/Examples/Appendix-Llava/Images/visionmonet-1b4-4-176960.jpeg",
                "caption": "",
                "position": 8594
            },
            {
                "img": "https://arxiv.org/html/2412.04139/extracted/6047748/Examples/Appendix-Llava/Images/visionmonet-1b4-4-117738.jpeg",
                "caption": "",
                "position": 8654
            },
            {
                "img": "https://arxiv.org/html/2412.04139/extracted/6047748/Examples/Appendix-Llava/Images/visionmonet-1b4-1-214604.jpg",
                "caption": "",
                "position": 8723
            },
            {
                "img": "https://arxiv.org/html/2412.04139/extracted/6047748/Examples/Appendix-Llava/Images/visionmonet-1b4-1-143910.jpg",
                "caption": "",
                "position": 8795
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]