[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.09781/extracted/6136615/figures/logo_v2.png",
                "caption": "",
                "position": 65
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.09781/extracted/6136615/figures/figure1_showv7.png",
                "caption": "Figure 1:VideoWorld explores learning knowledge from unlabeled videos, ranging from task-specific rules to high-level reasoning and planning capabilities. Compared to other learning methods: reinforcement learning (RL), supervised learning (SL) and text-based learning, it offers three advantages: 1) better generalization with unified visual representation for various tasks and interfaces, 2) lower manual annotation burden, and 3) learning richer real-world information than text description.",
                "position": 93
            },
            {
                "img": "https://arxiv.org/html/2501.09781/extracted/6136615/figures/rep_space_v3.png",
                "caption": "Figure 2:Comparison of prediction targets.‚ÄúState‚Äù, ‚ÄúVideo‚Äù and ‚ÄúVideo w/ LDM‚Äù refer to three different prediction targets: a state sequence (e.g., labeled positions of moves in Go), a raw video sequence, and a video sequence augmented with latent codes representing future visual changes (this approach is adopted by VideoWorld). ‚ÄúAction-Value‚Äù denotes the score for each move in the game, with details provided in Sec.4.2. By combining rich video information with a compact representation of visual changes, VideoWorld enables more effective learning.",
                "position": 123
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.09781/extracted/6136615/figures/overview_v6.png",
                "caption": "Figure 3:Overview of the proposed VideoWorld model architecture.(Left) Overall architecture. (Right) The proposed latent dynamics model (LDM). First, LDM compresses the visual changes from each frame to its subsequentHùêªHitalic_Hframes into a set of latent codes. Then, an auto-regressive transformer seamlessly integrates the output of LDM with the next token prediction paradigm.",
                "position": 201
            }
        ]
    },
    {
        "header": "3VideoWorld",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.09781/extracted/6136615/figures/umap_v4.png",
                "caption": "Figure 4:UMAP projection[34]of the learned latent codeon the Go (Left) and CALVIN (right) training set. Each point represents\nthe continuous (pre-quantization) latent code generated by the LDM. In Go examples, odd steps represent white‚Äôs moves, and even steps represent black‚Äôs moves. We visualize the latent codes of black moves in steps 2/4/6. The legend shows examples of common patterns learned for new black moves. For clarity, these moves are highlighted on the board with added colors and lines to indicate new patterns. On the right, we visualize the latent codes of the robotic arm‚Äôs movement along the X/Y/Z axes at intervals of 1, 5, and 10 frames. Points are color-coded by displacement range, with purple and red indicating the maximum displacement in opposite directions along each axis.",
                "position": 233
            }
        ]
    },
    {
        "header": "4Video-GoBench",
        "images": []
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.09781/extracted/6136615/figures/umap_test_v5.png",
                "caption": "Figure 5:Illustration of playing against KataGOand UMAP projection[34]of the predicted latent code. Our model plays as black. The generated latent code is visualized through the LDM decoder and new stones in the visualization are marked with colors to match the legend. The visualization serves as a probe, indicating that the model shows signs of forward planning.",
                "position": 328
            },
            {
                "img": "https://arxiv.org/html/2501.09781/extracted/6136615/figures/umap_calvin.png",
                "caption": "Figure 6:Illustration of robotic manipulationand UMAP projection of the predicted latent code during inference. Latent codes are visualized through the LDM decoder. The UMAP projection illustrates the 9 predicted latent codes (i.e.H=9ùêª9H=9italic_H = 9) across different tasks, with each point color-coded by task type. Visualizations with a yellow background show the model‚Äôs actual robotic arm control during inference, while those with a green background represent the model‚Äôs next-frame predictions during training.",
                "position": 968
            },
            {
                "img": "https://arxiv.org/html/2501.09781/extracted/6136615/figures/supp_vis_cap.png",
                "caption": "(a)",
                "position": 987
            },
            {
                "img": "https://arxiv.org/html/2501.09781/extracted/6136615/figures/supp_vis_cap.png",
                "caption": "(a)",
                "position": 990
            },
            {
                "img": "https://arxiv.org/html/2501.09781/extracted/6136615/figures/supp_vis_sac.png",
                "caption": "(b)",
                "position": 996
            },
            {
                "img": "https://arxiv.org/html/2501.09781/extracted/6136615/figures/supp_vis_calvin_3.png",
                "caption": "Figure 8:Visualizations of performing CALVIN tasks.",
                "position": 1003
            },
            {
                "img": "https://arxiv.org/html/2501.09781/extracted/6136615/figures/supp_vis_rlbench.png",
                "caption": "Figure 9:Visualizations of performing RLBench tasks.",
                "position": 1006
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.09781/extracted/6136615/figures/state_count.png",
                "caption": "(a)",
                "position": 2498
            },
            {
                "img": "https://arxiv.org/html/2501.09781/extracted/6136615/figures/state_count.png",
                "caption": "(a)",
                "position": 2501
            },
            {
                "img": "https://arxiv.org/html/2501.09781/extracted/6136615/figures/rep.png",
                "caption": "(b)",
                "position": 2507
            }
        ]
    },
    {
        "header": "Appendix BDetails on Video-GoBench",
        "images": []
    }
]