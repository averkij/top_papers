[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23588/x1.png",
                "caption": "(a)Pixel Autoregressive models",
                "position": 127
            },
            {
                "img": "https://arxiv.org/html/2510.23588/x1.png",
                "caption": "(a)Pixel Autoregressive models",
                "position": 130
            },
            {
                "img": "https://arxiv.org/html/2510.23588/x2.png",
                "caption": "(b)Normalizing Flow",
                "position": 135
            },
            {
                "img": "https://arxiv.org/html/2510.23588/x3.png",
                "caption": "(c)Flow Autoregressive Transformer (FARMER)",
                "position": 140
            }
        ]
    },
    {
        "header": "2Preliminary",
        "images": []
    },
    {
        "header": "3Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23588/x4.png",
                "caption": "Figure 2:Overview of FARMER.Left, FARMER consists an autoregressive flow (AF) and an autoregressive (AR) model. The AF maps image patches to latent sequences, while the AR predicts Gaussian Mixture Models (GMMs) conditioned on these latents, optimizing their likelihood end-to-end.Middle, Each AF block performs an invertible next-token transformation of the input sequence to obtain a new sequence.Right, AR splits latent channels into informative and redundant groups, modeling each informative tokenâ€™s likelihood via a GMM conditioned on its previous tokens, and redundant tokens jointly via a shared GMM conditioned on all informative tokens. This separation enables disentangling structural and detailed information.",
                "position": 318
            },
            {
                "img": "https://arxiv.org/html/2510.23588/x5.png",
                "caption": "(a)Autoregressive Flow Reverse Process",
                "position": 531
            },
            {
                "img": "https://arxiv.org/html/2510.23588/x5.png",
                "caption": "(a)Autoregressive Flow Reverse Process",
                "position": 534
            },
            {
                "img": "https://arxiv.org/html/2510.23588/x6.png",
                "caption": "(b)One-Step Distillation Process",
                "position": 539
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23588/x7.png",
                "caption": "Figure 4:Qualitative Results. Images generated by FARMER on ImageNet 256x256.",
                "position": 1171
            },
            {
                "img": "https://arxiv.org/html/2510.23588/x8.png",
                "caption": "Figure 5:Qualitative Comparison. Images of class 0 in ImageNet generated by FARMER, MAR, and DiT.",
                "position": 1174
            },
            {
                "img": "https://arxiv.org/html/2510.23588/Figs/k_ablation_plot_final.png",
                "caption": "(a)Impact of GMM mixture component number",
                "position": 1301
            },
            {
                "img": "https://arxiv.org/html/2510.23588/Figs/k_ablation_plot_final.png",
                "caption": "(a)Impact of GMM mixture component number",
                "position": 1304
            },
            {
                "img": "https://arxiv.org/html/2510.23588/Figs/fid_is_plot_final.png",
                "caption": "(b)Impact of informative dimension",
                "position": 1309
            },
            {
                "img": "https://arxiv.org/html/2510.23588/x9.png",
                "caption": "Figure 7:The impact of redundant channels. The numbers above indicate scaling factors applied to the variance of the shared GMM distribution for redundant channels. Adjusting this variance controls sampling diversity: larger variance yields more diverse, potentially out-of-distribution samples, while smaller variance limits diversity. Visualization results demonstrate that the self-supervised dimension reduction effectively separates structural information from color details.",
                "position": 1324
            },
            {
                "img": "https://arxiv.org/html/2510.23588/x10.png",
                "caption": "Figure 8:The sample images with abnormal log-determinant values. High logdet values cause strong compression in parts of the data space, leading to blurred textures and missing fine-scale details in the generated images.",
                "position": 1387
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Discussions",
        "images": []
    }
]