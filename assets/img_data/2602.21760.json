[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21760/x1.png",
                "caption": "",
                "position": 72
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21760/x2.png",
                "caption": "Figure 2:Comparison of parallel strategies for diffusion inference.(a) Patch-based data parallel frameworks suffer from bottlenecks caused by all-gather operations and artifacts at patch boundaries, leading to limited acceleration and quality degradation. (b) Pipeline parallel frameworks incur excessive asynchronous communication overhead and accumulate estimate errors. (c) Ourhybrid parallelism, which incorporates condition-based data parallelism, adaptively combines both paradigms to achieve high fidelity and fast generation.",
                "position": 82
            },
            {
                "img": "https://arxiv.org/html/2602.21760/x3.png",
                "caption": "Figure 3:Overview of the proposed diffusion inference hybrid parallel framework.Our method adaptively switches parallelism modes atœÑ1\\tau_{1}andœÑ2\\tau_{2}, optimizing the trade-off between computational efficiency and consistency of conditional guidance, and demonstrates superior inference acceleration performance while preserving high generation quality.",
                "position": 151
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21760/x4.png",
                "caption": "Figure 4:Illustration of therel-MAEt‚Äã(œµc,œµu)\\boldsymbol{\\text{rel-MAE}_{t}(\\epsilon_{c},\\epsilon_{u})}curve.Therel-MAEt‚Äã(œµc,œµu)\\text{rel-MAE}_{t}(\\epsilon_{c},\\epsilon_{u})value is relatively large beforeœÑ1\\tau_{1}and afterœÑ2\\tau_{2}, while it converges near zero between them, indicating stable alignment between conditional and unconditional branches during the parallelism phase.",
                "position": 269
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21760/x5.png",
                "caption": "Figure 5:Qualitative results of the main experiments.We compare 1024√ó\\times1024 image generations from the SDXL model. Our method achieves the best acceleration and FID performance, while producing visuals most similar to the original.",
                "position": 649
            },
            {
                "img": "https://arxiv.org/html/2602.21760/x6.png",
                "caption": "Figure 6:Visualization of speed‚Äìquality trade-off across different parallelism intervalsk\\boldsymbol{k}.Smallerkkvalues preserve higher fidelity, whereas largerkkachieve greater acceleration. Our method consistently dominates prior works across the trade-off frontier. All experiments were conducted on 2 GPUs.",
                "position": 652
            },
            {
                "img": "https://arxiv.org/html/2602.21760/x7.png",
                "caption": "Figure 7:Comparison of high-resolutions tasks.We compare different parallel inference methods on the SDXL model using NVIDIA H200 GPUs across 1024√ó\\times1024, 2048√ó\\times2048, and 2560√ó\\times2560 high-resolutions.",
                "position": 724
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AEvaluation of Hybrid Parallelism",
        "images": []
    },
    {
        "header": "Appendix BEmpirical Visualization of DenoisingDiscrepancy",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21760/x8.png",
                "caption": "Figure 8:Empirical visualization of denoising discrepancy curve.",
                "position": 1398
            }
        ]
    },
    {
        "header": "Appendix CAdaptive Parallelism Switching Algorithm",
        "images": []
    },
    {
        "header": "Appendix DDerivation of Score-Based Interpretation of Denoising Discrepancy",
        "images": []
    },
    {
        "header": "Appendix ERobustness of Determineùùâùüè\\boldsymbol{\\tau_{1}}underStochastic Denoising Noise",
        "images": []
    },
    {
        "header": "Appendix FExtensibility to Many GPU Configurations Structures",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21760/x9.png",
                "caption": "(a)Batch-level extension underùëµ\\boldsymbol{N}GPUs configuration.",
                "position": 1583
            },
            {
                "img": "https://arxiv.org/html/2602.21760/x9.png",
                "caption": "(a)Batch-level extension underùëµ\\boldsymbol{N}GPUs configuration.",
                "position": 1586
            },
            {
                "img": "https://arxiv.org/html/2602.21760/x10.png",
                "caption": "(b)Layer-wise pipeline extension on a 4‚ÄâGPUs configuration.",
                "position": 1592
            }
        ]
    },
    {
        "header": "Appendix GImplementation Details",
        "images": []
    },
    {
        "header": "Appendix HQuantitative Results on the ParallelismIntervalùíå\\boldsymbol{k}",
        "images": []
    },
    {
        "header": "Appendix IAdditional Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21760/x11.png",
                "caption": "Figure 10:Additional qualitative results of the main experiments.We compare 1024√ó\\times1024 image generations from the SDXL model. Our method achieves the best acceleration and FID performance, while producing visuals most similar to the original.",
                "position": 1676
            },
            {
                "img": "https://arxiv.org/html/2602.21760/x12.png",
                "caption": "Figure 11:Additional qualitative comparisons across differentkkvalues.We compare 1024√ó\\times1024 image generations from the SDXL model across various parallelism intervals. Smallerkkvalues preserve higher visual fidelity, whereas largerkkgradually reduce local detail due to the extended parallelism window. Although the overall appearance remains similar, fine-grained conditional attributes become subtly blurred askkincreases.",
                "position": 1685
            }
        ]
    },
    {
        "header": "Appendix JQualitative Comparion Results via Differentùíå\\boldsymbol{k}",
        "images": []
    }
]