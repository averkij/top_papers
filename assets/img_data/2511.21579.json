[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21579/x1.png",
                "caption": "Figure 1:Harmonyemploys a cross-task synergy training strategy to achieve robust audio-visual synchronization. This versatile framework supports multiple generation paradigms, including joint audio-video synthesis as well as audio-driven and video-driven generation, while also demonstrating strong generalizability to diverse audio types (e.g., music) and visual styles.",
                "position": 108
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21579/x2.png",
                "caption": "Figure 2:(a)Mitigating Correspondence Drift with Cross-Task Synergy.Our training paradigm leverages a supervised audio- and video-driven task to provide a strong alignment signal. This instills robust synchronization features in the model, stabilizing the otherwise stochastic joint generation process. (b)Overview of the Harmony Model.The architecture features parallel branches for multimodal inputs. The video stream is conditioned on a reference image and a descriptive prompt. The audio stream is conditioned on a reference audio, an ambient sound description, and a speech transcript. The model then generates a single, synchronized audio-visual result.",
                "position": 298
            },
            {
                "img": "https://arxiv.org/html/2511.21579/x3.png",
                "caption": "Figure 3:Comparison of the audio-video alignment score among different training strategies.",
                "position": 331
            },
            {
                "img": "https://arxiv.org/html/2511.21579/x4.png",
                "caption": "Figure 4:SyncCFGemploys the mute audio and static video as the negative anchors to capture the synchronization feature, which can effectively enhance the audio-video alignment.",
                "position": 497
            },
            {
                "img": "https://arxiv.org/html/2511.21579/x5.png",
                "caption": "Figure 5:Qualitative Comparisonbetween Harmony and the state-of-the-art methods, including Universe-1[wang2025universe]and Ovi[low2025ovi].",
                "position": 557
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21579/x6.png",
                "caption": "Figure 6:Visualization of the audio-to-video frame-wise cross-attention map, where the audio can accurately capture the sound source from the videos.",
                "position": 800
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Appendix AOverview",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21579/x7.png",
                "caption": "Figure 7:More comparison on human-speech video generation.",
                "position": 1539
            },
            {
                "img": "https://arxiv.org/html/2511.21579/x8.png",
                "caption": "Figure 8:More comparison on environment-sound video generation.",
                "position": 1542
            }
        ]
    },
    {
        "header": "Appendix CBenchmark Settings",
        "images": []
    },
    {
        "header": "Appendix DMore Quantitative Comparisons",
        "images": []
    },
    {
        "header": "Appendix EMore Qualitative Comparisons",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21579/x9.png",
                "caption": "Figure 9:Visualization of the voice-clone results of our model.",
                "position": 1682
            }
        ]
    },
    {
        "header": "Appendix FDetails about Voice Clone",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21579/x10.png",
                "caption": "Figure 10:Visualization of the audio-driven results of our model.",
                "position": 1695
            }
        ]
    },
    {
        "header": "Appendix GAudio-Driven Performance",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21579/x11.png",
                "caption": "Figure 11:More results on human-speech video generation.",
                "position": 1731
            },
            {
                "img": "https://arxiv.org/html/2511.21579/x12.png",
                "caption": "Figure 12:Visualization of speech-video generation in diverse style.",
                "position": 1734
            },
            {
                "img": "https://arxiv.org/html/2511.21579/x13.png",
                "caption": "Figure 13:More results on ambient-sound video generation.",
                "position": 1737
            }
        ]
    },
    {
        "header": "Appendix HMore qualitative results",
        "images": []
    }
]