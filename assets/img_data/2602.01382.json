[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01382/x1.png",
                "caption": "Figure 1:Overview of the PromptRL framework.PromptRL jointly trains a language model and a flow-matching image generator within a unified RL loop. Given an original prompt (and optionally a reference image), the LM produces semantically grounded prompt variants that expand the exploration space beyond fixed-prompt training. These prompts are paired with independent noise samples and passed to the flow-matching model to generate diverse images. A mixture of reward functions evaluates each image and guides the evolution of the LM (for improved prompt rewriting) and the FM (for improved visual generation).",
                "position": 82
            },
            {
                "img": "https://arxiv.org/html/2602.01382/figures/sd15_grid_4x4_bee.jpg",
                "caption": "Figure 2:The quality-diversity dilemma in flow-based T2I models and its mitigation through prompt refinement.As models advance from Stable Diffusion v1-5 (a) to FLUX.1-dev (b), they achieve higher text-image alignment (TI-Sim) and aesthetic quality (P.S.) but suffer from dramatically reduced output diversity (II-Sim), creating an exploration bottleneck for RL optimization. LM-based prompt refinement (c) partially restores diversity while maintaining quality, demonstrating that linguistic variations can expand the exploration space. All images in each row share identical random seeds to isolate the effect of prompt conditioning.",
                "position": 101
            },
            {
                "img": "https://arxiv.org/html/2602.01382/figures/flux_grid_4x4_bee.jpg",
                "caption": "",
                "position": 139
            },
            {
                "img": "https://arxiv.org/html/2602.01382/figures/cot_grid_4x4_bee.jpg",
                "caption": "",
                "position": 143
            },
            {
                "img": "https://arxiv.org/html/2602.01382/figures/sd15_grid_4x4_koala.jpg",
                "caption": "",
                "position": 154
            },
            {
                "img": "https://arxiv.org/html/2602.01382/figures/flux_grid_4x4_koala.jpg",
                "caption": "",
                "position": 158
            },
            {
                "img": "https://arxiv.org/html/2602.01382/figures/cot_grid_4x4_koala.jpg",
                "caption": "",
                "position": 162
            }
        ]
    },
    {
        "header": "2Related works",
        "images": []
    },
    {
        "header": "3Understanding flow RL inefficiencies",
        "images": []
    },
    {
        "header": "4PromptRL",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01382/x2.png",
                "caption": "Figure 3:Qualitative comparison on instructional image editing tasks.\nOur method enables the LM to leverage the original image’s visual signals to transform vague editing instructions into more explicit and image-specific prompts, ultimately improving editing performance.",
                "position": 298
            },
            {
                "img": "https://arxiv.org/html/2602.01382/x3.png",
                "caption": "Figure 4:Qualitative comparison on text-to-image generation. The first two prompts are from GenEval. The third prompt is from OCR-1k. And the last two prompts are from Drawbench.",
                "position": 325
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01382/x4.png",
                "caption": "(a)Training curve comparison on GenEval reward.",
                "position": 938
            },
            {
                "img": "https://arxiv.org/html/2602.01382/x4.png",
                "caption": "(a)Training curve comparison on GenEval reward.",
                "position": 941
            },
            {
                "img": "https://arxiv.org/html/2602.01382/x5.png",
                "caption": "(b)Training curve comparison on OCR reward.",
                "position": 947
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Impact statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining details",
        "images": []
    },
    {
        "header": "Appendix BDiscussion",
        "images": []
    }
]