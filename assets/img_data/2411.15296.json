[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15296/extracted/6018486/figs/timeline.jpg",
                "caption": "Figure 1:Time line of existing MLLM benchmarks. The center shows the number of benchmarks born at each time.",
                "position": 183
            }
        ]
    },
    {
        "header": "2Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15296/extracted/6018486/figs/arch.jpg",
                "caption": "Figure 2:Typical MLLM architecture. Tokenizer and De-Tokenizer are used for the processing of text, as the standard flow of LLM. With respect to other modalities, specialized encoders and connectors are often required to convert them into tokens, as well pre-trained generators[10,11]to enable multimodal generation capabilities. There are also methods that employ purely discrete modeling to achieve both understanding and generation[12].",
                "position": 269
            },
            {
                "img": "https://arxiv.org/html/2411.15296/extracted/6018486/figs/train.jpg",
                "caption": "Figure 3:Illustration of three training stages of MLLMs. In the first stage, image-caption pairs are usually used for the modality alignment. In the second stage, the model is tuned on various QA pairs to make it capable of following instrctions. The third stage is responsible for making the model conform to human preferences.",
                "position": 284
            }
        ]
    },
    {
        "header": "3Benchmark Categories",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15296/extracted/6018486/figs/cases.jpg",
                "caption": "Figure 5:Examples of different MLLM evaluation tasks. The answer can be Open-Ended, Yes-or-No, or Multi-Choice.",
                "position": 595
            }
        ]
    },
    {
        "header": "4Benchmark Construction",
        "images": []
    },
    {
        "header": "5Evaluation Judge",
        "images": []
    },
    {
        "header": "6Evaluation Metric",
        "images": []
    },
    {
        "header": "7Evaluation Toolkit",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15296/x1.png",
                "caption": "Figure 7:Major components and evaluation pipeline of the toolkit. By integrating various types of datasets and models, the evaluation toolkit facilitates the efficient acquisition and timely updating of assessment results, enabling comprehensive performance comparisons across models.",
                "position": 2107
            }
        ]
    },
    {
        "header": "8Challenges and Future Directions",
        "images": []
    },
    {
        "header": "9Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]