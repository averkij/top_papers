[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21675/Figures/logos/globe_16254565.png",
                "caption": "",
                "position": 128
            },
            {
                "img": "https://arxiv.org/html/2512.21675/Figures/logos/GitHub_Invertocat_Dark.png",
                "caption": "",
                "position": 131
            },
            {
                "img": "https://arxiv.org/html/2512.21675/Figures/logos/hf-logo.png",
                "caption": "",
                "position": 134
            },
            {
                "img": "https://arxiv.org/html/2512.21675/x1.png",
                "caption": "",
                "position": 138
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21675/x2.png",
                "caption": "Figure 2:Semantic-levelvs.Perceptual-levelunderstanding.",
                "position": 174
            },
            {
                "img": "https://arxiv.org/html/2512.21675/x3.png",
                "caption": "Figure 3:Representative QA examples in UniPercept-Bench.Questions follow a three-level hierarchy ofDomain–Category–Criterion, defining perceptual scope, specific visual aspects, and fine-grained criteria for constructing diverse, perception-oriented VQA tasks.",
                "position": 180
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21675/x4.png",
                "caption": "Figure 4:Constuction pipeline of UniPercept-Bench.A three-stage process initial QA generation, reject sampling, and human refinement to produce high-quality perceptual-level QA pairs across aesthetics, quality, structure, and texture.",
                "position": 226
            },
            {
                "img": "https://arxiv.org/html/2512.21675/unipercept/Figures/benchmark/table_v6.png",
                "caption": "Figure 5:Distribution of UniPercept-Benchacross (a) Domain, (b) Category, and (c) Criterion. Zoom in for best view.",
                "position": 229
            }
        ]
    },
    {
        "header": "3UniPercept-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21675/x5.png",
                "caption": "Figure 6:Training pipeline of UniPercept.A two-stage framework combining domain-adaptive pre-training for perceptual understanding and task-aligned RL to jointly optimize Visual Rating and Visual Question Answering.",
                "position": 262
            }
        ]
    },
    {
        "header": "4UniPercept",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21675/x6.png",
                "caption": "Table 3:Performance comparison of different models on UniPercept-Bench-VQA (IQA).",
                "position": 1084
            },
            {
                "img": "https://arxiv.org/html/2512.21675/x6.png",
                "caption": "Figure 7:Results on UniPercept-Bench.VQA and VR are evaluated by Acc. and(S​R​C​C+P​L​C​C)/2(SRCC+PLCC)/2.",
                "position": 1337
            },
            {
                "img": "https://arxiv.org/html/2512.21675/x7.png",
                "caption": "Figure 8:More results of FLUX.1-dev w/ UniPercept Reward.Different reward signals emphasize distinct perceptual attributes, whileUniPercept Reward (All)achieves the best overall performance by integrating complementary perceptual cues.",
                "position": 1749
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Unifying Perceptual-Level Image Understanding",
        "images": []
    },
    {
        "header": "8Details of ISTA",
        "images": []
    },
    {
        "header": "9Details of UniPercept-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21675/x8.png",
                "caption": "Figure 10:Relationship between different perceptual domains. Ratings are provided by UniPercept.",
                "position": 5233
            }
        ]
    },
    {
        "header": "10Further Discussion on UniPercept",
        "images": []
    },
    {
        "header": "11More Examples of UniPercept-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21675/x9.png",
                "caption": "Figure 11:More Examples in UniPercept-Bench.",
                "position": 5688
            },
            {
                "img": "https://arxiv.org/html/2512.21675/x10.png",
                "caption": "Figure 12:Example of UniPercept-Constructed Image Profiles.",
                "position": 5691
            },
            {
                "img": "https://arxiv.org/html/2512.21675/x11.png",
                "caption": "Figure 13:Example of UniPercept-Constructed Image Profiles.",
                "position": 5694
            },
            {
                "img": "https://arxiv.org/html/2512.21675/x12.png",
                "caption": "Figure 14:Example of UniPercept-Constructed Image Profiles.",
                "position": 5697
            }
        ]
    },
    {
        "header": "12UniPercept-Constructed Image Profiles",
        "images": []
    }
]