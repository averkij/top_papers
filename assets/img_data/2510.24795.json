[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24795/x1.png",
                "caption": "Figure 1:Necessity of Efficient VLAs.This figure highlights the disparity between powerful but resource-intensive foundation VLAs and the practical deployment requirements of diverse edge robotic platforms. Bridging this gap by developing more compact, economical, and applicable solutions is the primary motivation for pursuing efficient VLAs.",
                "position": 162
            },
            {
                "img": "https://arxiv.org/html/2510.24795/x2.png",
                "caption": "Figure 2:The Organization of Our Survey.We systematically categorize efficient VLAs into three core pillars: (1)Efficient Model Design, encompassing efficient architectures and model compression techniques; (2)Efficient Training, covering efficient pre-training and post-training strategies; and (3)Efficient Data Collection, including efficient data collection and augmentation methods. The framework also reviews VLA foundations, key applications, challenges, and future directions, establishing the groundwork for advancing scalable embodied intelligence.",
                "position": 168
            }
        ]
    },
    {
        "header": "2Vision-Language-Action Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24795/x3.png",
                "caption": "Figure 3:An Overview Of VLAs.VLAs integrate vision encoders to extract visual features, LLM backbones to fuse multimodal inputs, and action decoders (MLP-based, autoregressive, or generative) to produce robotic control signals, enabling end-to-end vision-language-action reasoning for embodied manipulation tasks.",
                "position": 222
            },
            {
                "img": "https://arxiv.org/html/2510.24795/x4.png",
                "caption": "Figure 4:Timeline of Foundational VLA Models and Efficient VLAs.The timeline illustrates the progression of foundational VLA models and efficient VLAs from 2023 to 2025, highlighting the explosive growth in enhancing the efficiency of VLA to bridge computational demands with real-world robotic deployment.",
                "position": 291
            }
        ]
    },
    {
        "header": "3Efficient Model Design",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24795/x5.png",
                "caption": "Figure 5:Key strategies forEfficient Architectures(section˜3.1) in VLAs. We illustrate six primary approaches: (a) Efficient Attention (section˜3.1.1), mitigating theO​(n2)O(n^{2})complexity of standard self-attention; (b) Transformer Alternatives (section˜3.1.2), such as Mamba; (c) Efficient Action Decoding (section˜3.1.3), advancing from autoregressive generation to parallel and generative methods; (d) Lightweight Components (section˜3.1.4), adopting smaller model backbones; (e) Mixture-of-Experts (section˜3.1.5), employing sparse activation via input routing; and (f) Hierarchical Systems (section˜3.1.6), which decouple high-level VLM planning from low-level VLA execution.",
                "position": 542
            },
            {
                "img": "https://arxiv.org/html/2510.24795/x6.png",
                "caption": "Figure 6:Key strategies forModel Compression(section˜3.2) in VLAs. We illustrate three primary approaches: (a) Layer Pruning (section˜3.2.1), which removes redundant layers to reduce model depth and computational cost; (b) Quantization (section˜3.2.2), which reduces the numerical precision of model parameters to decrease memory footprint and accelerate inference; and (c) Token Optimization (section˜3.2.3), which minimizes the number of processed tokens via token compression (merging tokens), token pruning (dropping non-essential tokens), and token caching (reusing static tokens).",
                "position": 1315
            }
        ]
    },
    {
        "header": "4Efficient Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24795/x7.png",
                "caption": "Figure 7:Key strategies forEfficient Training(section˜4) in VLAs, divided into two main stages. (a) Efficient Pre-Training (section˜4.1) migrates general-purpose VLMs into the embodied domain to create an initial, action-aware policy, encompassing Data-Efficient Pre-training (section˜4.1.1), Efficient Action Representation (section˜4.1.2) , and Other Pre-training Strategies (section˜4.1.3). (b) Efficient Post-Training (section˜4.2) subsequently specializes this policy for specific tasks, leveraging Supervised Fine-tuning (section˜4.2.1) and RL-Based Methods (section˜4.2.2).",
                "position": 2045
            }
        ]
    },
    {
        "header": "5Efficient Data Collection",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24795/x8.png",
                "caption": "Figure 8:Taxonomy of Efficient Data Collection Strategies in VLAs.This figure illustrates the primary approaches undersection˜5, encompassing human-in-the-loop, simulated, reusability-oriented, self-driven, and augmentative techniques for scalable acquisition of high-quality robotic datasets while minimizing resource overhead.",
                "position": 3570
            }
        ]
    },
    {
        "header": "6Applications",
        "images": []
    },
    {
        "header": "7Challenges and Future Works",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]