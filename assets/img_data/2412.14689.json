[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14689/x1.png",
                "caption": "Figure 1:Model collapse of synthetic data.‚ë†The model continuously trains on its previously generated data, leading to a gradual decline in model performance, i.e., model collapse. Starting from real data(xo,yo)subscriptùë•ùëúsubscriptùë¶ùëú(x_{o},y_{o})( italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ), the test errorEt‚Å¢e‚Å¢s‚Å¢tsubscriptùê∏ùë°ùëíùë†ùë°E_{test}italic_E start_POSTSUBSCRIPT italic_t italic_e italic_s italic_t end_POSTSUBSCRIPTincreases asf0subscriptùëì0f_{0}italic_f start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTundergoes iterative training on synthetic data(y1,y2,‚Ä¶,yn)subscriptùë¶1subscriptùë¶2‚Ä¶subscriptùë¶ùëõ(y_{1},y_{2},\\dots,y_{n})( italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚Ä¶ , italic_y start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ).‚ë°ToEdit¬†(ours), we use a trained model for token-level editing rather than purely synthesizing data.\nLeveragingf0subscriptùëì0f_{0}italic_f start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTand an operation matrixmisubscriptùëöùëñm_{i}italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTto edit the data, the test error is constrained within a fixed upper bound. Therefore, we can preserve the distribution coverage to avoid model collapse.",
                "position": 153
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14689/x2.png",
                "caption": "Figure 2:Non-iterative model collapse. Training language models from scratch on AI-synthesized data or a mixture of human and synthetic data leads to performance degradation. This degradation is negatively correlated with the proportion of synthetic data used in training.A.We pre-train GPT-2 Small (124M) on human (Dolma(Soldaini et¬†al.,2024)) and synthetic (Cosmopedia(Ben¬†Allal et¬†al.,2024)) data. As the proportion of synthetic data increases, the model‚Äôs loss decreases.B.As the proportion of synthetic data increases, the PPL also rises. This trend remains consistent across different validation sets. More results on downstream tasks are presented in10and11.",
                "position": 407
            }
        ]
    },
    {
        "header": "2Non-iterative Model Collapse",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14689/x3.png",
                "caption": "Figure 3:PPL distribution of human and synthetic data estimated by Llama-3-8B. The synthetic data lacks the long tail of the human-produced data and is also concentrated within the first25%percent2525\\%25 %of the human-produced data distribution.A.Distribution of human-produced data is sharp with a long tail, spanning a wide range from 0 to over 100.B.The values are concentrated within a much narrower range, mostly between 0 and 12.\nThe experiment uses Dolma v6 and Cosmopedia as human and synthetic data, each with sampled 6B tokens. More results in Figure9.",
                "position": 436
            },
            {
                "img": "https://arxiv.org/html/2412.14689/x4.png",
                "caption": "Figure 4:A.Embedding visualization using t-SNE and sentence-transformers.B.pre-training results for selected synthetic data and other data mixtures.",
                "position": 456
            },
            {
                "img": "https://arxiv.org/html/2412.14689/x5.png",
                "caption": "Figure 5:Uni/Bi-gram feature distribution across 10,000 hash buckets.",
                "position": 459
            },
            {
                "img": "https://arxiv.org/html/2412.14689/x6.png",
                "caption": "Figure 6:U-shape token probability distribution of Dolma-sampled V6 estimated by Qwen-0.5B-Instruct(qwe,2024).",
                "position": 466
            }
        ]
    },
    {
        "header": "3Token-Level Editing",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProof",
        "images": []
    },
    {
        "header": "Appendix BMore Results of Human and Synthetic Data Mixture Training",
        "images": []
    },
    {
        "header": "Appendix CExperiment Settings",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14689/x7.png",
                "caption": "Figure 7:OLMo-237M pretraining with mixed human and synthetic data proportions. We pretrain the OLMo-237M model using a mixture of human data (Dolma(Soldaini et¬†al.,2024)) and synthetic data (Cosmopedia(Ben¬†Allal et¬†al.,2024)).",
                "position": 2818
            },
            {
                "img": "https://arxiv.org/html/2412.14689/x7.png",
                "caption": "Figure 7:OLMo-237M pretraining with mixed human and synthetic data proportions. We pretrain the OLMo-237M model using a mixture of human data (Dolma(Soldaini et¬†al.,2024)) and synthetic data (Cosmopedia(Ben¬†Allal et¬†al.,2024)).",
                "position": 2821
            },
            {
                "img": "https://arxiv.org/html/2412.14689/x8.png",
                "caption": "Figure 8:GPT-2 perplexity (PPL) on validation sets, trained from scratch.",
                "position": 2826
            },
            {
                "img": "https://arxiv.org/html/2412.14689/x9.png",
                "caption": "Figure 9:PPL distribution of human and synthetic data estimated by StabLM-Zephyr-3B. This indicates that different prior distributions yielded the same result, which is consistent with Figure3. The synthetic data lacks a long tail and is concentrated within a narrow portion of the distribution.",
                "position": 3213
            }
        ]
    },
    {
        "header": "Appendix DDiscussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14689/x10.png",
                "caption": "Figure 10:The top 40 bi-grams from separately sampled 1M subsets of Dolma, Cosmopedia, and DSIR-selected datasets.",
                "position": 3476
            },
            {
                "img": "https://arxiv.org/html/2412.14689/x11.png",
                "caption": "Figure 11:The top 64 bi-grams from separately sampled 1M subsets of Dolma, Cosmopedia, and DSIR-selected datasets.",
                "position": 3479
            },
            {
                "img": "https://arxiv.org/html/2412.14689/x12.png",
                "caption": "Figure 12:Density sampling response values. This result further confirms the issue of feature collapse in synthetic data.",
                "position": 3482
            }
        ]
    },
    {
        "header": "Appendix EPotential Applications and Future Work",
        "images": []
    }
]