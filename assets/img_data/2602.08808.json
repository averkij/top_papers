[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08808/x1.png",
                "caption": "",
                "position": 131
            },
            {
                "img": "https://arxiv.org/html/2602.08808/x2.png",
                "caption": "",
                "position": 134
            },
            {
                "img": "https://arxiv.org/html/2602.08808/x3.png",
                "caption": "",
                "position": 137
            },
            {
                "img": "https://arxiv.org/html/2602.08808/x4.png",
                "caption": "",
                "position": 139
            },
            {
                "img": "https://arxiv.org/html/2602.08808/figures/figure-1-a-v2.png",
                "caption": "1aHow2Minemines and refines how-to procedures at web scale across 14 topics. Running this pipeline on about 1M documents yields 351K procedures.",
                "position": 141
            },
            {
                "img": "https://arxiv.org/html/2602.08808/figures/figure-1-a-v2.png",
                "caption": "1aHow2Minemines and refines how-to procedures at web scale across 14 topics. Running this pipeline on about 1M documents yields 351K procedures.",
                "position": 148
            },
            {
                "img": "https://arxiv.org/html/2602.08808/figures/fig-1b.png",
                "caption": "1bHow2Bench+How2Score+How2Judgesupport scalable evaluation with an open 8B judge, showing clear scaling trends across model sizes and training stages.",
                "position": 154
            },
            {
                "img": "https://arxiv.org/html/2602.08808/figures/fig-1c.png",
                "caption": "1cRL onHow2TrainwithHow2Scoreas the reward liftHow2Benchperformance while maintaining or improving scores on 12 standard post-training evaluations, indicating broad downstream utility.",
                "position": 160
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Problem Setting and Related Work",
        "images": []
    },
    {
        "header": "3How2Mine: Extracting Realistic Step-by-Step Procedures from the Web",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08808/figures/funnel.png",
                "caption": "Figure 2:Given a sample of 980K topic-stratified web documents,How2Mineyields 351Kprocedure instances(goal + resources list + reference steps), and can be easily scaled to larger corpora.",
                "position": 328
            }
        ]
    },
    {
        "header": "4How2Score: Measuring Procedural Validity by Detecting Critical Failures",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08808/figures/judge-human-agreement.png",
                "caption": "Figure 3:Agreement between LLM judges and the human majority label on critical-failure detection (N=200), reported overall and stratified by the human-majority class (has_failure/no_failure). §LABEL:sec:appendix_prompts_judgingfor the judge prompt; §LABEL:sec:appendix_human_annotationfor annotation details.",
                "position": 465
            }
        ]
    },
    {
        "header": "5How2Bench: Evaluating Performance on Step-by-Step Procedure Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08808/figures/bench-results.png",
                "caption": "Figure 4:How2Benchresults on selected models. We reportHow2Scorecomputed withHow2Judgealong with the average generated tokens for each model. The average reference length is97.44 tokens. For open models,Basedenotes the final non-post-trained checkpoint, andInstructdenotes the post-trained checkpoint.",
                "position": 492
            },
            {
                "img": "https://arxiv.org/html/2602.08808/figures/cross-judge.png",
                "caption": "Figure 5:Cross-judge robustness check for self-preference bias on closed models spanning the GPT, Gemini, and Claude families: we rescore the same generations with four judges (How2Judge, GPT 5, Gemini 2.5 Pro, Claude 4.5 Opus) and find the ranking is unchanged.",
                "position": 520
            }
        ]
    },
    {
        "header": "6Improving Step-by-Step Procedure Generation with RL",
        "images": []
    },
    {
        "header": "7Robustness to Format and Memorization Confounds",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08808/figures/posttrain-from-pretrain.png",
                "caption": "Figure 6:Post-training from different Olmo 3 7B pretraining checkpoints (x-axis). RL gains grow substantially at later checkpoints, while SFT yields modest improvements.",
                "position": 803
            }
        ]
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMotivational analysis of query type distribution",
        "images": []
    },
    {
        "header": "Appendix BDetails on data pipeline",
        "images": []
    }
]