[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07885/figures/main_figure.png",
                "caption": "Figure 1:Intelligence per Watt: A Study of Local Intelligence Efficiency.We present thefirst systematic study of local AI inference efficiencyacross models, hardware, and real-world workloads.(Left)Intelligence efficiency is defined as task accuracy per unit of power, capturing both capabilities delivered and energy consumed.(Left-Middle)We conductcomprehensive performance profilingacross 20+ state-of-the-art local LMs (≤20​B\\leq 20Bactive parameters), diverse hardware accelerators (Apple,NVIDIA,AMD), multiple performance metrics, and 1M+ real-world queries spanning chat and reasoning tasks.(Right-Middle)Local LM capabilities are improving rapidly: win/tie rate versus frontier models increases from23.2%23.2\\%(2023) to71.3%71.3\\%(2025)—a3.1×3.1\\timesimprovement in accuracy, demonstrating that local models can accurately handle significant portions of single-turn chat and reasoning queries.(Right)Intelligence per watt improves5.3×5.3\\timesfrom 2023–2025, driven by advances in both model architectures and hardware accelerators, with local accelerators showing1.5×1.5\\timesefficiency headroom compared to enterprise-grade systems.",
                "position": 169
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Dataset and Profiling Harness",
        "images": []
    },
    {
        "header": "5Intelligence Efficiency Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07885/x1.png",
                "caption": "Figure 2:Local Models Rival Cloud Models Across Diverse Benchmarks:Individual model performance scales with size, ranging from 31.5–69.4% forIBM Granite4-H-Small, 30.0–83.6% forGemma3-12B, 51.5–80.4% forGPT-OSS-120B, and 66.5–89.5% forGemini 2.5 Pro.\nLocal routing (best local LM per query) achieves 97.8%, 88.3%, 77.0%, and 92.4% onWildchat,NaturalReasoning,SuperGPQA, andMMLU Prorespectively, surpassing cloud routing (100%, 82.9%, 66.5%, 87.4%) on three of four benchmarks.",
                "position": 435
            },
            {
                "img": "https://arxiv.org/html/2511.07885/x2.png",
                "caption": "Figure 3:Rapid Improvement of Local LMs across Chat and Reasoning Queries: We evaluate the performance of SOTA local models released between April 2024 and August 2025 onWildchatandNaturalReasoning.\nOnWildchat(left), local models show a win/tie rate of 78.2% againstQwen3-235Bas of August 2025, compared to just 28.0% in April 2024—a 2.8×\\timesimprovement in 16 months.\nOnNaturalReasoning(right), local models achieve 80.9% accuracy by August 2025, up from 48.7% in April 2024—a 66% relative improvement.",
                "position": 473
            },
            {
                "img": "https://arxiv.org/html/2511.07885/x3.png",
                "caption": "Figure 4:Increasing GPU Memory of Consumer Accelerators: Memory capacity (GB) for local accelerators. Over the past decade, local hardware has significantly closed the memory gap with cloud-grade accelerators, particularly since 2020, driven by advances in high bandwidth memory (HBM) components and unified memory architectures.",
                "position": 486
            },
            {
                "img": "https://arxiv.org/html/2511.07885/x4.png",
                "caption": "Figure 5:Increase in Intelligence per Joule for Local LMs and Accelerators: Efficiency improved18.0×18.0\\timesover 16 months, decomposed into3.1×3.1\\timesfrom better local LMs and5.9×5.9\\timesfrom better local accelerators.",
                "position": 644
            },
            {
                "img": "https://arxiv.org/html/2511.07885/figures/burst_gpt_plot_v3.1.png",
                "caption": "Figure 6:Energy, Compute, and Capital Gains from Model Routing.\nCumulative resource consumption over 24 hours and 80.2M LLM queries(Wang et al.,2025).\nUsing our local-cloud router between44small LMs on Apple M4 Max andQwen3-235Bon an H200 yields substantial savings at various routing accuracies. For the 80% accurate router, we observe: 64.3% in energy savings, 61.8% in compute, and 59.0% in cost compared to naively routing every query toQwen3-235B, capturing a majority of the gains achievable by the theoretical best-case (Oracle Router).",
                "position": 955
            }
        ]
    },
    {
        "header": "6Extended Experiments",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExtended Related Works",
        "images": []
    },
    {
        "header": "Appendix BDataset and Profiling Harness",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07885/figures/domain_routing_stacked_bar.png",
                "caption": "Figure 7:Local Win/Tie-Rate vs. Cloud LMs by Domain. Stacked bars show the fraction of single-turn chat and reasoning queries handled by local LMs (<20​B<20Bactive parameters; blue) versus those routed to frontier models in the cloud (red), computed per economic index domain(Appel et al.,2025)",
                "position": 2840
            },
            {
                "img": "https://arxiv.org/html/2511.07885/figures/wildchat_difficulty_analysis.png",
                "caption": "Figure 8:Chat Task Performance by Difficulty Level and Year. Model success rates across four difficulty levels and three model generations (2023, 2024, 2025). The data reveals dramatic progress across all difficulty levels, with 2023 models achieving 28.79% overall success rising to 98.12% by 2025. Notably, Levels 1-3 approach near-perfect performance (98-99%), while Level 4 shows the largest relative improvement (+210.4% per year) despite starting from the lowest baseline (17.77%).",
                "position": 4194
            },
            {
                "img": "https://arxiv.org/html/2511.07885/figures/nr_difficulty_analysis.png",
                "caption": "Figure 9:Reasoning Task Performance by Difficulty Level and Year. Model success rates on across five difficulty levels and three model generations. The benchmark shows a three-tier saturation pattern: near-complete (98-99% on Levels 1-2), approaching saturation (85-92% on Levels 3-4), and wide-open frontier (51% on Level 5).",
                "position": 4197
            },
            {
                "img": "https://arxiv.org/html/2511.07885/x5.png",
                "caption": "Figure 10:Perplexity and Accuracy per Joule Trends acrossWildchatandNaturalReasoning.",
                "position": 4216
            },
            {
                "img": "https://arxiv.org/html/2511.07885/x6.png",
                "caption": "Figure 11:Minimal Accuracy Degradation Shifting from FP16 to FP4 for Open-Source Local Models: Evaluation across three reasoning datasets (N=10,000N=10,000each) shows2−3%2-3\\%accuracy loss per precision step, demonstrating thatF​P​8FP8/F​P​4FP4quantization enables efficient deployment with acceptable performance tradeoffs.",
                "position": 4289
            },
            {
                "img": "https://arxiv.org/html/2511.07885/x7.png",
                "caption": "Figure 12:Open-Source Local LMs Performance vs. U.S. GDP - WildChat and Natural Reasoning: Model accuracy on WildChat and Natural Reasoning benchmarks plotted against relevant GDP in trillions of dollars.\nBoth benchmarks show continued performance improvements as training compute scales across models from Qwen3B-4B to Qwen3B-A22B-235B.\nFor our calculations, we compute the weighted sum of an LM’s accuracy on each U.S. Labor category vs. the U.S. GDP associated with that category.",
                "position": 4497
            },
            {
                "img": "https://arxiv.org/html/2511.07885/x8.png",
                "caption": "Figure 13:Open-Source Local LMs Performance vs. U.S. GDP - SuperGPQA and MMLU Pro: Model accuracy on SuperGPQA and MMLU Pro benchmarks plotted against relevant GDP in trillions of dollars.\nBoth benchmarks show continued performance improvements as training compute scales across models from Qwen3B-4B to Qwen3B-A22B-235B.\nFor our calculations, we compute the weighted sum of an LM’s accuracy on each U.S. Labor category vs. the U.S. GDP associated with that category.",
                "position": 4503
            }
        ]
    },
    {
        "header": "Appendix CLocal-Cloud Experiments",
        "images": []
    }
]