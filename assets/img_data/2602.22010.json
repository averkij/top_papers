[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.22010/x1.png",
                "caption": "Figure 1:WoG first incorporates future observations into the action inference pipeline, projecting them into the condition space for action generation. Subsequently, it decouples future observations from the pipeline and simultaneously predicts these future conditions alongside actions, thereby transferring the knowledge of future conditions into the VLA model.",
                "position": 172
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.22010/x2.png",
                "caption": "Figure 2:Overview of WoG. WoG is trained in two stages.\nIn the first stage, future observations encoded by frozen vision foundation models are queried and compressed by a trainable Q-former-based Future Encoder to form condition representations, which, together with VLM-encoded current observations and instructions, are used for action prediction.\nIn the second stage, the encoder and vision models are frozen, and the VLM backbone is trained to align with the conditions while predicting actions.",
                "position": 262
            }
        ]
    },
    {
        "header": "4Simulation Experiments",
        "images": []
    },
    {
        "header": "5Real-World Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.22010/x3.png",
                "caption": "Figure 3:Overview of our real-world experiment setup. The figure shows our robotic platform and sensors (left), the execution of the three tasks under the in-distribution setup (middle), and the modifications applied for the out-of-distribution setup (right).",
                "position": 1009
            },
            {
                "img": "https://arxiv.org/html/2602.22010/x4.png",
                "caption": "Figure 4:Performance after training with UMI data.Compared to training solely with robot data, the WoG showed a 42% improvement in performance on the P&P task and a 33% on the Fold task.",
                "position": 1322
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Detailed Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.22010/x5.png",
                "caption": "Figure 5:Detailed illustration of the query mechanisms within WoG.Theleft paneldepicts the future encoder, which maintains a set of learnable queries to extract low-dimensional, action-relevant conditions from the features of pretrained vision models.\nTheright panelillustrates the query mechanism for condition prediction during the second training stage.\nHere, a set of learnable query embeddings performs cross-attention with the last hidden states of the VLM, producing predictive representations that are supervised to align with the target conditions generated by the now-frozen future encoder.",
                "position": 1362
            }
        ]
    },
    {
        "header": "8Simulation Experiments",
        "images": []
    },
    {
        "header": "9Real-World Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.22010/x6.png",
                "caption": "Figure 6:Process of human manipulation process.We keep the same human manipulation data collection strategy as in[gr3,gr-dexter]. Human trajectories can be efficiently collected with\nVR devices at a rate of approximately 450 trajectories per hour, substantially outpacing the teleoperated\nrobot trajectory collection. Nevertheless, as discussed earlier, we use action annotations for only 11% of the data, while the remaining 89% are leveraged solely as unlabeled videos.\nWe argue that this setting more faithfully reflects real-world conditions, where action-labeled human manipulation data are scarce but large-scale unlabeled videos are abundant, and is therefore essential for evaluating the scalability of WoG.",
                "position": 1519
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]