[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Background",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14978/x1.png",
                "caption": "Figure 1:Method.We fine-tune a pretrained text-to-image model into a few-step image-editing model using differentiable VLM-feedback regarding edit success. In addition, we use distribution matching loss (DMD(Yin et al.,2024a)) to ensure output images remain in the natural image manifold.",
                "position": 232
            },
            {
                "img": "https://arxiv.org/html/2510.14978/x2.png",
                "caption": "Figure 2:Qualitative comparison on GEdit-Benchunder the few-step sampling setting. For an upper-bound comparison, in the1st1^{\\text{st}}column we show results of the best multi-step sampling method (as measured by the quantitative metrics in Table1). Our method performs on par or better than baseline methods across different edit types in the few-step setting. We show more samples in the Appendix Figure10",
                "position": 431
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14978/x3.png",
                "caption": "Figure 3:Qualitative comparison on Customization task.Our method can generate the object in new contexts while having better fidelity under few-step sampling. We show more samples in the Appendix Figure12.",
                "position": 706
            },
            {
                "img": "https://arxiv.org/html/2510.14978/x4.png",
                "caption": "Table 4:Dataset and VLM scale and comparison with Reinforcement Learningon the GEdit-Bench. Increasing dataset scale and using stronger VLMs leads to increased performance. Our method also performs better than post-training an SFT model with RL(Liu et al.,2025a).",
                "position": 779
            },
            {
                "img": "https://arxiv.org/html/2510.14978/x4.png",
                "caption": "Figure 4:Qualitative analysis of ablation experiments.Our method maintains better input and edited image alignment compared to only training with DMD loss, which also fails on tasks like removal. Compared to fine-tuning an SFT model with RL, our method results in better fidelity while following the edit instruction. Please zoom in for details.",
                "position": 859
            }
        ]
    },
    {
        "header": "6Discussion and Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Comparison with Baseline Methods",
        "images": []
    },
    {
        "header": "Appendix BAblation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14978/x5.png",
                "caption": "Figure 5:Performance for each sub edit-type.Training with only DMD loss fails to achieve certain tasks like removal and style changes. In addition, using binary cross-entropy loss and VLM identity-based questions helps improve the overall performance.",
                "position": 2462
            },
            {
                "img": "https://arxiv.org/html/2510.14978/x5.png",
                "caption": "Figure 5:Performance for each sub edit-type.Training with only DMD loss fails to achieve certain tasks like removal and style changes. In addition, using binary cross-entropy loss and VLM identity-based questions helps improve the overall performance.",
                "position": 2464
            },
            {
                "img": "https://arxiv.org/html/2510.14978/x6.png",
                "caption": "Figure 6:Training with only VLM-editing lossleads to lower fidelity samples with the model only maximizing the edit success probability. Current general-purpose VLMs are often not good at subjective tasks like evaluating image fidelity, highlighting the requirement of distribution matching loss in our framework.",
                "position": 2468
            },
            {
                "img": "https://arxiv.org/html/2510.14978/x7.png",
                "caption": "Figure 7:Unreliable VLM response on intermediate outputs of a multi-step diffusion model.Here we show a 28-step diffusion process, denoising predictions from early steps (e.g.,t=4t=4), which correspond to high noise levels, are blurry and semantically ambiguous. This can lead to unreliable responses from the VLM, as shown here. Therefore, we adopt a few-step diffusion model that always generates sharp images.",
                "position": 2474
            },
            {
                "img": "https://arxiv.org/html/2510.14978/x7.png",
                "caption": "Figure 7:Unreliable VLM response on intermediate outputs of a multi-step diffusion model.Here we show a 28-step diffusion process, denoising predictions from early steps (e.g.,t=4t=4), which correspond to high noise levels, are blurry and semantically ambiguous. This can lead to unreliable responses from the VLM, as shown here. Therefore, we adopt a few-step diffusion model that always generates sharp images.",
                "position": 2476
            },
            {
                "img": "https://arxiv.org/html/2510.14978/x8.png",
                "caption": "Figure 8:Our (4-step) vs single-step editing model.We compare our final 4-step model with a single-step model, both trained via our approach. Editing an image in a single step is still challenging and leads to lower-fidelity outputs.",
                "position": 2480
            },
            {
                "img": "https://arxiv.org/html/2510.14978/x9.png",
                "caption": "Figure 9:Limitation.Our method can struggle to maintain exact pixel consistency between the input and edited image. Having LPIPS(Zhang et al.,2018)loss between the input and output edited image can resolve it to an extent (top row) but at the cost of reduced editing success (bottom row).",
                "position": 2486
            }
        ]
    },
    {
        "header": "Appendix CLimitation",
        "images": []
    },
    {
        "header": "Appendix DDataset Construction Details",
        "images": []
    },
    {
        "header": "Appendix ETraining Implementation Details",
        "images": []
    },
    {
        "header": "Appendix FOther Baseline Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14978/x10.png",
                "caption": "Figure 10:Qualitative comparison on GEdit-Bench.We show results of our and baseline image-editing methods under the few-step sampling setting. For comparison, we also show the results of the best method with multi-step sampling, as measured by the quantitative metrics (Table1), in thest1{}^{1}{\\text{st}}column. Our method performs on par or better than baseline methods across different edit types in the few-step setting.",
                "position": 2767
            },
            {
                "img": "https://arxiv.org/html/2510.14978/x11.png",
                "caption": "Figure 11:Qualitative comparison on ImgEdit-Bench.We show results of our and baseline image-editing methods under the few-step sampling setting. For comparison, we also show the results of the best method with multi-step sampling, as measured by the quantitative metrics (Table6), in the1st1^{\\text{st}}column. Our method performs on par or better than baseline methods across different edit types in the few-step setting.",
                "position": 2771
            },
            {
                "img": "https://arxiv.org/html/2510.14978/x12.png",
                "caption": "Figure 12:Qualitative comparison on DreamBooth.We show results of our and baseline methods under the few-step sampling setting. For comparison, we also show the results of the best method with multi-step sampling, as measured by the quantitative metrics in the first column. Our method performs comparably with baseline methods on identity alignment while having better image fidelity across different concepts in the few-step setting.",
                "position": 2775
            }
        ]
    },
    {
        "header": "Appendix GSocietal Impact",
        "images": []
    }
]