[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2RLHF Preliminaries",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Empirical Results",
        "images": []
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Related Work on Game-theoretic RLHF",
        "images": []
    },
    {
        "header": "Appendix BPseudo-Algorithm of MNPO",
        "images": []
    },
    {
        "header": "Appendix CExperimental Details",
        "images": []
    },
    {
        "header": "Appendix DFormulations of Preference Optimization Objectives",
        "images": []
    },
    {
        "header": "Appendix EMathematical Analysis",
        "images": []
    },
    {
        "header": "Appendix FLimitations and Future Work",
        "images": []
    },
    {
        "header": "Appendix GThe Use of Large Language Models (LLMs)",
        "images": []
    }
]