[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16508/extracted/6016435/Diagrams/IntroFigALM.png",
                "caption": "",
                "position": 154
            },
            {
                "img": "https://arxiv.org/html/2411.16508/x1.png",
                "caption": "",
                "position": 165
            },
            {
                "img": "https://arxiv.org/html/2411.16508/x2.png",
                "caption": "",
                "position": 171
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16508/x3.png",
                "caption": "Figure 3:Data collection and verification pipeline. Our benchmark features both cultural specific content sourced from the web (left) and generic image understanding collection sourced from existing LMM benchmark. The cultural part is carefully filtered to remove noisy samples and private information. We use GPT4o for translations which are manually verified and corrected with over 800 hours of human annotators (native speakers). OurALM-benchhas diverse question types and features approximately 23K QA pairs in total in 100 languages.",
                "position": 571
            }
        ]
    },
    {
        "header": "3ALM Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16508/x4.png",
                "caption": "Figure 4:Data statistics of ourALM-benchshowing the diversity of the scripts, global coverage, comprehensive categories, and various question types. Our dataset contains 22.7K high-quality question-answers in total, covering 100 languages and 24 scripts. All the samples are manually verified by native speakers.",
                "position": 604
            },
            {
                "img": "https://arxiv.org/html/2411.16508/x5.png",
                "caption": "Figure 5:Analysis of the type of mistakes GPT-4o made while language translations via human-feedback. GPT-4o encounters more issues with respect to semantic and grammatical errors.",
                "position": 663
            },
            {
                "img": "https://arxiv.org/html/2411.16508/x6.png",
                "caption": "Figure 6:ALM-benchPerformance comparison of different open and closed-sourced models (y-axis) on the 100 languages (x-axis) of ourALM-bench. The performance is represented as an average accuracy across all questions in a language. The actual performance of a model on a language is shown in each respective box, where the higher accuracy is highlighted with a high color intensity. The closed-source propriety models generally perform better across languages compared to their open-sourced counterparts. The performance on several high resource languages (e.g., English, French, Chinese and Spanish) is generally higher throughout different models, whereas all open-source models struggle on low resource languages (e.g., Amharic, Kinyarwanda, Burmese)\nOverall, GPT-4o and GLM-4V-9B performs better in terms of closed-source and open-source models, respectively. Best viewed zoomed in.",
                "position": 666
            }
        ]
    },
    {
        "header": "4Benchmarking LMMs onALM-bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16508/x7.png",
                "caption": "Figure 7:Performance comparison of GPT-4o and Qwen2-VL on different language scripts. The count above each bar indicates the number of languages present in that script.",
                "position": 677
            },
            {
                "img": "https://arxiv.org/html/2411.16508/x8.png",
                "caption": "Figure 8:Error analysis across 4 diverse language scripts, including Bengali, Sinhalese, Latin and Cyrillic on GPT-4o results, demonstrates significant challenges for even the top-performing closed-source models, particularly in cultural and reasoning comprehension. TheALM-benchhighlights these gaps, especially in languages with complex dialectal variations.",
                "position": 712
            },
            {
                "img": "https://arxiv.org/html/2411.16508/x9.png",
                "caption": "Figure 9:Performance comparison of GPT-4o and Qwen2-VL on different language families. The count above each bar indicates the number of languages present in that script.",
                "position": 724
            },
            {
                "img": "https://arxiv.org/html/2411.16508/x10.png",
                "caption": "Figure 10:We present the qualitative examples of the success cases in the first row and failure cases of GPT-4o in the second row on different languages & domains inALM-bench. For the failure cases, we specify different error types.\nFor instance, the Urdu language question asks about the festival depicted in the image. The image specifically refers toMela Chiraghan(Festival of Lights), a celebration held in honor of the Sufi saint Shah Jamal’s shrine. Since the decoration in the image closely resembles that of Eid Milad un Nabi — another religious festival—the model erroneously associates it with this wrong event. This constitutes a lack of cultural understanding since the model fails to distinguish between the theme behind the decorations. Eid Milad un Nabi typically features more modest, reverential lighting with green lights, whereas the lighting in Mela Chiraghan is brighter and more colorful. Additionally, people typically dress for the Eid Milad un Nabi event in a traditional outfit which is absent in the image. These examples highlight the model’s gap in cultural knowledge and its limitations in terms of accurately interpreting the cultural context of the given sample. Additional results are in the suppl. material.",
                "position": 782
            },
            {
                "img": "https://arxiv.org/html/2411.16508/x11.png",
                "caption": "Figure 11:Performance of top-performing VLMs onALM-benchaccording to different question types. Compared to open-ended questions, VLMs perform better on MCQs and T/F questions.",
                "position": 796
            }
        ]
    },
    {
        "header": "5Ethical Consideration",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AALM-Bench Categories",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16508/extracted/6016435/Diagrams/CategoryWisePieChart.png",
                "caption": "Figure A.1:A breakdown of the cultural categories from ourALM-benchis depicted. We ensure consistent samples across all subsets, except for theEconomycategory, where culturally unique images were challenging to find. These visual and question-answer samples are verified and filtered by the native speakers, removing any culturally irrelevant and redundant information.",
                "position": 1566
            }
        ]
    },
    {
        "header": "Appendix BAnnotators Demographics",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16508/x12.png",
                "caption": "Figure A.2:Qualitative examples of various mistakes in GPT-4o translations includingsemantic, cultural, language and grammatical errors. We employ expert human-feedback to rewrite the correct translations for all samples in ourALM-benchdataset.",
                "position": 1582
            },
            {
                "img": "https://arxiv.org/html/2411.16508/x13.png",
                "caption": "Figure A.3:top-left.depicts theAgeandGenderdistribution of our volunteers. We have one-fourth of the Female speakers and it is also shown that almost 48.3% of our volunteers fall in the age-bracket of 18-25.top-right.depicts the volunteer’s native language proficiency in years.bottom-left.depicts the volunteer’s Cultural Literacy proficiency in years.bottom-right.depicts the volunteer’s native language proficiency in years.",
                "position": 1585
            }
        ]
    },
    {
        "header": "Appendix CGuidelines for Data Verification",
        "images": []
    },
    {
        "header": "Appendix DVerification Platform and UI Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16508/x14.png",
                "caption": "Figure A.4:User interface for translation verification hosted on Gradio, allowing contributors to classify incorrect entries and provide accurate translations.",
                "position": 1634
            }
        ]
    },
    {
        "header": "Appendix EImage Web Searching",
        "images": []
    },
    {
        "header": "Appendix FBlurring the PIDs",
        "images": []
    },
    {
        "header": "Appendix GInstruction Prompt for QA Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16508/x15.png",
                "caption": "Figure A.5:Prompts used for generation of cultural QA pairs.search_queryrefers to the query used to search the image online. It includes language, country and cultural category details.captionis a manually added textual description specific to the image.cultural categoryindicates the domain to which the image belongs, selected from the 13 cultural domains in ourALM-benchdataset.",
                "position": 1720
            }
        ]
    },
    {
        "header": "Appendix HInstruction Prompt for QA Translations",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16508/x16.png",
                "caption": "Figure A.6:Prompts used for translating cultural QAs using GPT-4o.English_inputrefers to either the English question or answer to be translated into thetarget_lang, the desired target language.",
                "position": 1730
            }
        ]
    },
    {
        "header": "Appendix IInstruction Prompt for LMM Answer Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16508/x17.png",
                "caption": "Figure A.7:Prompts used to generate answers for cultural questions using multiple Large Multimodal Models evaluated in our study. Different prompts are designed for different question types.questionrefers to the cultural question associated with the given image, previously generated using GPT-4o.choicesrepresents the four options provided in multiple choice question type, andtarget_languageis the desired local language for the response.",
                "position": 1740
            }
        ]
    },
    {
        "header": "Appendix JPrompts for GPT-Scoring",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16508/x18.png",
                "caption": "Figure A.8:Prompts used to generate a score between 0 and 10, with GPT-4o acting as the evaluator to compare an LMM’s predicted answers against ground truth answers. The termsquestion,ground_truth, andpredicted_answerrefer to the cultural question, the ground truth answer generated by GPT-4o and verified by experts, and the model’s predicted answer, respectively.",
                "position": 1747
            }
        ]
    },
    {
        "header": "Appendix KGuidelines for Error Analysis on GPT-4o Output",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16508/x19.png",
                "caption": "Figure A.9:qualitative examples of various question types in Urdu and Chinese Simplified Language.",
                "position": 1834
            },
            {
                "img": "https://arxiv.org/html/2411.16508/x20.png",
                "caption": "Figure A.10:qualitative examples of various question types in Hindi and Sinhala Language.",
                "position": 1837
            },
            {
                "img": "https://arxiv.org/html/2411.16508/x21.png",
                "caption": "Figure A.11:qualitative examples of various question types in Italian and German Language.",
                "position": 1840
            },
            {
                "img": "https://arxiv.org/html/2411.16508/x22.png",
                "caption": "Figure A.12:qualitative examples of various question types in Spanish and Emirati Arabic Language.",
                "position": 1843
            },
            {
                "img": "https://arxiv.org/html/2411.16508/x23.png",
                "caption": "Figure A.13:qualitative examples of various question types in Saudi Arabic and Afrikaans Language.",
                "position": 1846
            },
            {
                "img": "https://arxiv.org/html/2411.16508/x24.png",
                "caption": "Figure A.14:Some more qualitative examples of various question types from our benchmark.",
                "position": 1849
            },
            {
                "img": "https://arxiv.org/html/2411.16508/x25.png",
                "caption": "Figure A.15:Word clouds depicting prominent concepts from 19 categories in ourALM-bench. For intriguing results, we plot and demonstrate the results on English samples of the plot representing both the cultural and generic elements for the entire 100 languages.",
                "position": 1852
            }
        ]
    },
    {
        "header": "Appendix LQualitative examples with different question types",
        "images": []
    }
]