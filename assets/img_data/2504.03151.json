[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03151/extracted/6335150/fig/count.png",
                "caption": "Figure 1:Papers on visual reasoning per quarter over the last three years, with state computed using referenced papers. (Data current to mid-March 2025.)",
                "position": 96
            }
        ]
    },
    {
        "header": "2Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03151/extracted/6335150/fig/training_inference.png",
                "caption": "Figure 2:Framework illustrating training and inference for reasoning optimization. A virtuous cycle emerges as better policies generate improved trajectories, which in turn enhance the model through stronger supervision.",
                "position": 183
            }
        ]
    },
    {
        "header": "3Post-training improvements",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03151/extracted/6335150/fig/search_framework.png",
                "caption": "Figure 3:Search framework where language models explore and refine reasoning paths. Trajectories are scored using reward models, based on expected utility or final output quality, and guided by feedback, world models, and evaluators to select the most promising steps.",
                "position": 289
            }
        ]
    },
    {
        "header": "4Test-Time compute",
        "images": []
    },
    {
        "header": "5Datasets and Benchmarks",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplications from the Learning Science Perspective",
        "images": []
    }
]