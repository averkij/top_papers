[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17215/x1.png",
                "caption": "(a)Computation Scaling (1.8B‚Üí‚Üí\\rightarrow‚Üí500M)",
                "position": 158
            },
            {
                "img": "https://arxiv.org/html/2410.17215/x1.png",
                "caption": "(a)Computation Scaling (1.8B‚Üí‚Üí\\rightarrow‚Üí500M)",
                "position": 161
            },
            {
                "img": "https://arxiv.org/html/2410.17215/x2.png",
                "caption": "(b)Model Size Scaling",
                "position": 169
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17215/x3.png",
                "caption": "Figure 2:Results of applying KD methods in fine-tuning to pre-train a 200M student LM, using a 1.8B teacher LM. See Section3.1for method and evaluation details. When the training FLOPs are controlled, all KD methods perform similar or worse than Pre-Train w/o KD.",
                "position": 186
            }
        ]
    },
    {
        "header": "2MiniPLM: KD for Pre-training LMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17215/x4.png",
                "caption": "(a)MiniPLMTraining Framework",
                "position": 230
            },
            {
                "img": "https://arxiv.org/html/2410.17215/x4.png",
                "caption": "(a)MiniPLMTraining Framework",
                "position": 233
            },
            {
                "img": "https://arxiv.org/html/2410.17215/x5.png",
                "caption": "(b)Effect ofDifference Sampling",
                "position": 241
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17215/x6.png",
                "caption": "(a)1.8B Teacher‚Üí‚Üí\\rightarrow‚Üí200M Student",
                "position": 734
            },
            {
                "img": "https://arxiv.org/html/2410.17215/x6.png",
                "caption": "(a)1.8B Teacher‚Üí‚Üí\\rightarrow‚Üí200M Student",
                "position": 737
            },
            {
                "img": "https://arxiv.org/html/2410.17215/x7.png",
                "caption": "(b)1.8B Teacher‚Üí‚Üí\\rightarrow‚Üí500M Student",
                "position": 745
            },
            {
                "img": "https://arxiv.org/html/2410.17215/x8.png",
                "caption": "(c)1.8B Teacher‚Üí‚Üí\\rightarrow‚Üí1.2B Student",
                "position": 753
            },
            {
                "img": "https://arxiv.org/html/2410.17215/x9.png",
                "caption": "Table 3:Results of KD across model families. We use the teacher and reference LM from the Qwen family to distill the Llama3.1 and Mamba models. The average zero-shot accuracies on the downstream tasks and the losses on the DCLM corpus are reported. Note that Vanilla KD and MiniLLM cannot be applied when the teacher and student LMs use different tokenizations.",
                "position": 844
            },
            {
                "img": "https://arxiv.org/html/2410.17215/x9.png",
                "caption": "Figure 5:MiniPLMin the data-constrained setting. We fixùíüùíü\\mathcal{D}caligraphic_Dto contain 50B tokens and alter the sampling ratioŒ±ùõº\\alphaitalic_Œ±to obtainùíü‚Ä≤superscriptùíü‚Ä≤\\mathcal{D}^{\\prime}caligraphic_D start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPTwithDifference Sampling, which will be trained on for multiple epochs to achieve the constant total trained tokens. The y-axis represents the test loss on the DCLM corpus.",
                "position": 887
            },
            {
                "img": "https://arxiv.org/html/2410.17215/x10.png",
                "caption": "Figure 6:Impact of the teacher LM‚Äôs sizes on Vanilla KD andMiniPLM, with the pre-training FLOPs aligned. The y-axis represents the average zero-shot accuracy on the downstream tasks.",
                "position": 939
            },
            {
                "img": "https://arxiv.org/html/2410.17215/x10.png",
                "caption": "Figure 6:Impact of the teacher LM‚Äôs sizes on Vanilla KD andMiniPLM, with the pre-training FLOPs aligned. The y-axis represents the average zero-shot accuracy on the downstream tasks.",
                "position": 942
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProof of Proposition3",
        "images": []
    },
    {
        "header": "Appendix BMore Experimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17215/x11.png",
                "caption": "(a)1.8B‚Üí‚Üí\\rightarrow‚Üí200M",
                "position": 2909
            },
            {
                "img": "https://arxiv.org/html/2410.17215/x11.png",
                "caption": "(a)1.8B‚Üí‚Üí\\rightarrow‚Üí200M",
                "position": 2912
            },
            {
                "img": "https://arxiv.org/html/2410.17215/x12.png",
                "caption": "(b)1.8B‚Üí‚Üí\\rightarrow‚Üí500M",
                "position": 2920
            },
            {
                "img": "https://arxiv.org/html/2410.17215/x13.png",
                "caption": "(c)1.8B‚Üí‚Üí\\rightarrow‚Üí1.2B",
                "position": 2928
            },
            {
                "img": "https://arxiv.org/html/2410.17215/x14.png",
                "caption": "Figure 8:Impact of the reference model size. We use the 1.8B LM as the teacher and the 200M LM as the student. We report the average zero-shot accuracy on the downstream tasks of the LMs trained withMiniPLMand compare it with Vanilla KD.",
                "position": 3147
            },
            {
                "img": "https://arxiv.org/html/2410.17215/x14.png",
                "caption": "Figure 8:Impact of the reference model size. We use the 1.8B LM as the teacher and the 200M LM as the student. We report the average zero-shot accuracy on the downstream tasks of the LMs trained withMiniPLMand compare it with Vanilla KD.",
                "position": 3150
            },
            {
                "img": "https://arxiv.org/html/2410.17215/x15.png",
                "caption": "Figure 9:Impact of the difference sampling ratioŒ±ùõº\\alphaitalic_Œ±. We report the average zero-shot accuracy on the downstream tasks of the LMs trained withMiniPLM, usingŒ±‚àà[0.3,0.4,0.5,0.6,0.7,0.9]ùõº0.30.40.50.60.70.9\\alpha\\in[0.3,0.4,0.5,0.6,0.7,0.9]italic_Œ± ‚àà [ 0.3 , 0.4 , 0.5 , 0.6 , 0.7 , 0.9 ]and compare it with Vanilla KD.",
                "position": 3155
            }
        ]
    },
    {
        "header": "Appendix CMore Results",
        "images": []
    }
]