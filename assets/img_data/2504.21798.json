[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.21798/x1.png",
                "caption": "Figure 1:Scaling task instances(left) andperformance(right) for SWE-agent‚Äôs with\\bugs.\nUsing\\bugs, we can create100100100100s to1000100010001000s of instances for any Python codebase, enabling us to trainSWE-agent-LM-32Bwhich achieves40.240.240.240.2% on SWE-bench Verified.\nThe dotted lines for the left graph project the instances created per strategy for up to250250250250repos.",
                "position": 120
            },
            {
                "img": "https://arxiv.org/html/2504.21798/x2.png",
                "caption": "",
                "position": 129
            },
            {
                "img": "https://arxiv.org/html/2504.21798/x3.png",
                "caption": "Figure 2:\\bugscreates training data for software engineering agents by crafting bugs into real codebases.\nGiven a codebase, we employ several strategies to create task instances that break existing tests.\nUsing\\bugs, we create50505050k+ task instances with execution environments from128128128128real world repositories.",
                "position": 167
            }
        ]
    },
    {
        "header": "2\\bugs: Software Task Generation at Scale",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.21798/x4.png",
                "caption": "Table 1:Summary of\\bugsstatistics.\n‚ÄúYield %‚Äù is the % of candidates generated by a strategy that break1+limit-from11+1 +tests.\n‚ÄúCost‚Äù is the average cost to generate one candidate.\n‚ÄúF2P‚Äù (Fail to Pass tests), ‚ÄúLines [Edited]‚Äù are median values.",
                "position": 288
            },
            {
                "img": "https://arxiv.org/html/2504.21798/x5.png",
                "caption": "Figure 3:Distribution of task instance difficulty (easy/medium/hard) for existing SWE-bench style datasets (left5555bars) and\\bugs(right5555bars), assessed by our difficulty rating model.\nThe average difficulty score for each dataset is listed above each bar.\nFor\\bugs, per bug strategy, we sample1000100010001000task instances with LM generated issue text.",
                "position": 384
            },
            {
                "img": "https://arxiv.org/html/2504.21798/extracted/6399622/figures/check-mark.png",
                "caption": "Table 2:Comparison of open source training datasets for software engineering tasks.\nRelative to existing datasets,\\bugshas multiple times the number of task instances, repositories, and environments at a fraction of prior storage costs.\nSWE-fixer and SWE-bench-train task instances do not have execution environments, so ‚ÄúEnv. Size‚Äù is blank.",
                "position": 415
            },
            {
                "img": "https://arxiv.org/html/2504.21798/extracted/6399622/figures/cross-mark.png",
                "caption": "",
                "position": 456
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": []
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.21798/x6.png",
                "caption": "Figure 4:We fine-tune a7777B base model andSWE-agent-LM-32Bon700700700700trajectories for SymPy.\nSpecialization greatly improves SymPy performance, with a slight drop in generalization.",
                "position": 865
            },
            {
                "img": "https://arxiv.org/html/2504.21798/x6.png",
                "caption": "Figure 4:We fine-tune a7777B base model andSWE-agent-LM-32Bon700700700700trajectories for SymPy.\nSpecialization greatly improves SymPy performance, with a slight drop in generalization.",
                "position": 868
            },
            {
                "img": "https://arxiv.org/html/2504.21798/x7.png",
                "caption": "Figure 5:Keeping the number of training samples fixed at700700700700, we observe performance increases logarithmically with the number of repositories.",
                "position": 876
            },
            {
                "img": "https://arxiv.org/html/2504.21798/x8.png",
                "caption": "Figure 6:For instances solved by both models,SWE-agent-LM-32Bis faster than Claude 3.7 Sonnet (solved in fewer steps).",
                "position": 918
            },
            {
                "img": "https://arxiv.org/html/2504.21798/x8.png",
                "caption": "Figure 6:For instances solved by both models,SWE-agent-LM-32Bis faster than Claude 3.7 Sonnet (solved in fewer steps).",
                "position": 921
            },
            {
                "img": "https://arxiv.org/html/2504.21798/x9.png",
                "caption": "Figure 7:For unsuccessfully resolved task instances, a frequent failure mode is thatSWE-agent-LM-32Bwill repeat actions.",
                "position": 928
            },
            {
                "img": "https://arxiv.org/html/2504.21798/x10.png",
                "caption": "Figure 8:More than half of the unresolved instances ofSWE-agent-LM-32Bcorrespond to runs terminated by cost/step limits, and these limits are frequently reached before source code has been modified. See ¬ßF.3for details on how failures are attributed.",
                "position": 952
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.21798/x11.png",
                "caption": "Figure 9:An overview of pipelines in\\bugs.\nScripts/functions and manual steps are highlighted inblue.\nArtifacts that are also the inputs and outputs of these scripts are inorange.\\bugsfits in seamlessly with the SWE-bench and SWE-agent ecosystem.\nUse\\bugsto construct execution environments and generate task instances.\nUse SWE-agent to generate expert trajectories on\\bugstask instances and run inference with models trained on these trajectories.\nUse SWE-bench to evaluate how good your models are at resolving GitHub issues and performing software engineering tasks.",
                "position": 1823
            }
        ]
    },
    {
        "header": "Appendix AInfrastructure",
        "images": []
    },
    {
        "header": "Appendix BBug Generation Strategies",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.21798/x12.png",
                "caption": "Figure 10:Workflow to generate bugs for a function or class with an LM.\nWe first extract all functions or classes from a codebase, then enumerate across all candidates and prompt the LM to generate either a bug-laced rewrite or a re-implementation.",
                "position": 2234
            },
            {
                "img": "https://arxiv.org/html/2504.21798/x13.png",
                "caption": "Figure 11:Workflow to generate bugs via procedural modifications.\nPer function/class, the source code is first convert into anast.\nThe modification then mutates theast(e.g. removes an assignment statement).\nThe modifiedastis then converted back into source code with a bug of the modification type introduced.",
                "position": 2307
            },
            {
                "img": "https://arxiv.org/html/2504.21798/x14.png",
                "caption": "Figure 12:Workflow to generate bugs by combining bug patches.\nWe takenùëõnitalic_npatches (generated using an LM or procedural modification), then sequentially apply each bug patch to the codebase.\nIf all individual patches apply successfully, we save the resulting single patch which now represents allnùëõnitalic_nbugs combined.",
                "position": 2509
            },
            {
                "img": "https://arxiv.org/html/2504.21798/x15.png",
                "caption": "Figure 13:Workflow to generate bugs by reverting changes made in the diff patch correspond to a real GitHub pull request (PR).\nGiven the patch and the files modified by the patch, we prompt the LM to generate a complete rewrite of each file thatreversesthe changes made in the PR.\nThe changes are applied to the codebase, and we extract the patch, which now captures the reversal of the PR changes.",
                "position": 2683
            }
        ]
    },
    {
        "header": "Appendix CDataset Statistics",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.21798/x16.png",
                "caption": "Figure 14:Comparison of cumulative distributions for Fail-to-Pass tests along with the lines and files edited by the gold patch across\\bugsand four SWE-bench style datasets.",
                "position": 4640
            }
        ]
    },
    {
        "header": "Appendix DIssue Generation",
        "images": []
    },
    {
        "header": "Appendix EDifficulty Rating",
        "images": []
    },
    {
        "header": "Appendix FExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.21798/x17.png",
                "caption": "Figure 15:Distribution of number of turns for trajectories represented in the final dataset.",
                "position": 5274
            },
            {
                "img": "https://arxiv.org/html/2504.21798/x18.png",
                "caption": "Figure 16:The average step count depends strongly on the prescribed step limit.",
                "position": 5293
            },
            {
                "img": "https://arxiv.org/html/2504.21798/x18.png",
                "caption": "Figure 16:The average step count depends strongly on the prescribed step limit.",
                "position": 5296
            },
            {
                "img": "https://arxiv.org/html/2504.21798/x19.png",
                "caption": "Figure 17:Number of successful instances submitted before a given step limit.",
                "position": 5301
            },
            {
                "img": "https://arxiv.org/html/2504.21798/x20.png",
                "caption": "Figure 18:Categorizing failure modes",
                "position": 5384
            }
        ]
    },
    {
        "header": "Appendix GMiscellaneous",
        "images": []
    }
]