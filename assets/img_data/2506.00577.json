[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00577/extracted/6499919/images/ai_economist.png",
                "caption": "",
                "position": 138
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00577/x1.png",
                "caption": "Figure 1:Overview of the Recon Pipeline.Step 1: We curate a high-quality economic dataset (Recon Dataset) from benchmarks such as STEER, and distill reasoning traces from teacher models to construct the Recon-CoT Dataset.Step 2: A base model is post-trained via supervised fine-tuning (Recon-SFT) on Recon-CoT and reinforcement learning (Recon-RL) on the Recon Dataset.Step 3: The resulting models are evaluated on reasoning benchmarks (Recon-Eval Dataset), self-play, and multi-agent games against opponent agents.",
                "position": 184
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Dataset Curation",
        "images": []
    },
    {
        "header": "5Evaluation Results",
        "images": []
    },
    {
        "header": "6Insights and Future Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00577/x2.png",
                "caption": "Figure 2:Training dynamics for SFT (a) and RL (b).",
                "position": 2634
            },
            {
                "img": "https://arxiv.org/html/2506.00577/x3.png",
                "caption": "",
                "position": 2644
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]