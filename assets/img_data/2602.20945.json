[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.20945/x1.png",
                "caption": "Figure 1:General pipeline for efficient reasoning via RL.\nThe key is to promote short and accurate thinking trajectories via reward design.\nIn this paper, we provide systematic insights () considering data, reward, and optimization.",
                "position": 162
            },
            {
                "img": "https://arxiv.org/html/2602.20945/figures/len.png",
                "caption": "",
                "position": 166
            }
        ]
    },
    {
        "header": "2Preliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.20945/x2.png",
                "caption": "Figure 2:Training dynamics of various reward shaping methods onDeepSeek-R1-Distill-Qwen-1.5B.\nAll of them follow the two-stage paradigm.\nThe behaviors are distinct when evaluated under different token budgets.",
                "position": 251
            },
            {
                "img": "https://arxiv.org/html/2602.20945/x3.png",
                "caption": "Figure 3:Performance training on all prompts and easy/hard counterparts (rolloutLR=16​kL_{R}=16k, targetLT=4​kL_{T}=4k).",
                "position": 330
            }
        ]
    },
    {
        "header": "3Experiments and Guidelines",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.20945/x4.png",
                "caption": "Figure 4:Performance with various rolloutsNNusing DeepScaleR-Easy.",
                "position": 379
            },
            {
                "img": "https://arxiv.org/html/2602.20945/x5.png",
                "caption": "Figure 5:Performance for various reward strategies on negative rollouts (rolloutLR=16​kL_{R}=16k, targetLT=4​kL_{T}=4k,N=24N=24).\nWe also visualizeLR=4​kL_{R}=4k,LT=4​kL_{T}=4kfor comparison.",
                "position": 477
            },
            {
                "img": "https://arxiv.org/html/2602.20945/x6.png",
                "caption": "Figure 6:Performance for off-policy strategy with various staleness (i.e., 2,4,8,16).",
                "position": 530
            }
        ]
    },
    {
        "header": "4Extensive Analysis",
        "images": []
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitation and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ADetailed Results on Reward Engineering",
        "images": []
    },
    {
        "header": "Appendix BDetailed Results for Data Selection",
        "images": []
    },
    {
        "header": "Appendix CDetailed Results for More Rollouts",
        "images": []
    },
    {
        "header": "Appendix DCase Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.20945/x7.png",
                "caption": "Figure 7:Performance onQwen3-0.6Bwhen adaptively setting target lengthLTL_{T}as 90-th qualities.",
                "position": 1444
            },
            {
                "img": "https://arxiv.org/html/2602.20945/x8.png",
                "caption": "Figure 8:Training dynamics of various reward shaping methods on AIME’25, MATH-500, and AMC.",
                "position": 1457
            },
            {
                "img": "https://arxiv.org/html/2602.20945/x9.png",
                "caption": "Figure 9:Training dynamics of various reward shaping methods on Minerva Math, Olympiad Bench, and LiveCodeBench.",
                "position": 1462
            },
            {
                "img": "https://arxiv.org/html/2602.20945/x10.png",
                "caption": "Figure 10:Performance training on various training prompts (rolloutLR=16​kL_{R}=16k, targetLT=8​kL_{T}=8k).",
                "position": 1467
            },
            {
                "img": "https://arxiv.org/html/2602.20945/x11.png",
                "caption": "Figure 11:Performance training on various training prompts (rolloutLR=4​kL_{R}=4k, targetLT=4​kL_{T}=4k).",
                "position": 1472
            },
            {
                "img": "https://arxiv.org/html/2602.20945/x12.png",
                "caption": "Figure 12:Performance training on various rolloutsNN(LR=16​kL_{R}=16k,LT=4​kL_{T}=4k).",
                "position": 1477
            },
            {
                "img": "https://arxiv.org/html/2602.20945/x13.png",
                "caption": "Figure 13:Performance training on various rolloutsNN(LR=4​kL_{R}=4k,LT=4​kL_{T}=4k).",
                "position": 1482
            }
        ]
    },
    {
        "header": "Appendix EAdaptive Target LengthLTL_{T}",
        "images": []
    }
]