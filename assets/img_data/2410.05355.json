[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Model Architecture",
        "images": []
    },
    {
        "header": "3Pre-training",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05355/x1.png",
                "caption": "Figure 1:Data mixtures across training stages",
                "position": 203
            }
        ]
    },
    {
        "header": "4Evaluation and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05355/extracted/5907421/plots/latency_throughput/max_length_comp.png",
                "caption": "Figure 2:We vary the context length of the prompt to determine the maximum sequence length that could be processed without encountering an out-of-memory (OOM) error. To ensure a fair comparison, all models were configured with a rescaled vocabulary size.",
                "position": 582
            },
            {
                "img": "https://arxiv.org/html/2410.05355/extracted/5907421/plots/latency_throughput/throughput_memory.png",
                "caption": "Figure 3:With a fixed batch size and context length of 1, we vary the generated tokens up to 130k for Faclon-Mamba-7B, and Mistral-7B with a resized vocabulary for fair comparisons.",
                "position": 588
            }
        ]
    },
    {
        "header": "5Model Integration and Availability",
        "images": []
    },
    {
        "header": "6Discussions and conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]