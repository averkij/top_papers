[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11652/fig/acadreason.png",
                "caption": "",
                "position": 85
            },
            {
                "img": "https://arxiv.org/html/2510.11652/fig/huggingface.png",
                "caption": "",
                "position": 98
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11652/x1.png",
                "caption": "Figure 1:Overview of theAcadReasonbenchmark construction and evaluation pipeline. It consists of three stages:(1) High-Quality Academic Papers Collection– experts filter 430 papers across 5 domains into 50 top-tier theoretical works;(2) High-Reasoning Research Question Extraction– research questions are refined into formal queries with golden answers containing sufficient reasoning;(3) Checklists and Hints Extraction– background, definition, and methodology hints are provided together with verifiable, independent checklists. For evaluation, candidate responses are compared against golden answers and checklists, and GPT-5 mini assigns final scores.",
                "position": 113
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3AcadReasonBenchmark",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11652/x2.png",
                "caption": "Figure 2:General performance on different domains in Checklist Score",
                "position": 604
            },
            {
                "img": "https://arxiv.org/html/2510.11652/x3.png",
                "caption": "(a)The performance gain of various models across different hint types.",
                "position": 740
            },
            {
                "img": "https://arxiv.org/html/2510.11652/x3.png",
                "caption": "(a)The performance gain of various models across different hint types.",
                "position": 743
            },
            {
                "img": "https://arxiv.org/html/2510.11652/x4.png",
                "caption": "(b)The average performance gain across different hint types and disciplinary categories.",
                "position": 748
            }
        ]
    },
    {
        "header": "5Case study",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11652/x5.png",
                "caption": "Figure 4:Side-by-side comparison of OAgents and GPT-5 on the legal reasoning task.",
                "position": 770
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Contributions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "8Data Statistics",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11652/fig/fig2.png",
                "caption": "Figure 5:Category Distribution",
                "position": 1442
            }
        ]
    },
    {
        "header": "9Prompt for Infer and Evaluation",
        "images": []
    },
    {
        "header": "10LLM Usage",
        "images": []
    },
    {
        "header": "11Annotation and validation guideline",
        "images": []
    },
    {
        "header": "12More Experiment Result",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11652/x6.png",
                "caption": "Figure 11:General performance on different domains with Pass Rate.",
                "position": 2405
            }
        ]
    },
    {
        "header": "13Specific Case of Acadreason benchmark",
        "images": []
    }
]