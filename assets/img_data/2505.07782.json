[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.07782/x1.png",
                "caption": "Figure 1:Benchmark evaluations of eight frontiers LLMs across 50 evaluation tasks inMLE-Dojo.",
                "position": 142
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Benchmark Tasks and Dataset Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.07782/x2.png",
                "caption": "Figure 2:Overview of task diversity inMLE-Dojo, highlighting representative examples from four major domains: time series, computer vision, tabular data, and natural language processing.",
                "position": 399
            },
            {
                "img": "https://arxiv.org/html/2505.07782/x3.png",
                "caption": "Figure 3:Overview of data structure inMLE-Dojo.",
                "position": 425
            }
        ]
    },
    {
        "header": "4MLE-Dojo",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.07782/x4.png",
                "caption": "Figure 4:Overview ofMLE-Dojo. The framework bridges MLE-Agents with MLE task environments through standardized interfaces for observation and action spaces.",
                "position": 460
            },
            {
                "img": "https://arxiv.org/html/2505.07782/x5.png",
                "caption": "Figure 5:Interaction loop inMLE-Dojowith theoretical model (left) and concrete Python API (right).",
                "position": 465
            },
            {
                "img": "https://arxiv.org/html/2505.07782/x6.png",
                "caption": "Figure 6:Elo Rankings of eight frontiers LLMs on our proposedMLE-Dojoacross four main MLE domains inMLE-Dojo: tabular analysis, computer vision (CV), natural language processing (NLP), and MLE-Lite.",
                "position": 569
            }
        ]
    },
    {
        "header": "5LLMs as MLE Agents inMLE-Dojo",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.07782/x7.png",
                "caption": "Figure 7:Task difficulty sorted by average HumanRank score.",
                "position": 781
            },
            {
                "img": "https://arxiv.org/html/2505.07782/x8.png",
                "caption": "Figure 8:Performance profiles and corresponding AUP curves for evaluating the robustness of LLMs across four ML tasks. The x-axis represents the performance ratio thresholdœÑùúè\\tauitalic_œÑ, while the y-axis indicates the fraction of tasks for which a model achieves performance within a factorœÑùúè\\tauitalic_œÑof the best-performing model.",
                "position": 789
            },
            {
                "img": "https://arxiv.org/html/2505.07782/x9.png",
                "caption": "Figure 9:Relationship between average computational cost and performance across evaluated LLMs and task categories. Each point represents the average cost per task, with specific attention given to reasoning vs. non-reasoning model cost dynamics. Note thatGemini-2.0-ProandGemini-2.5-Proare excluded from cost analysis (only for performance reference) due to current free usage.",
                "position": 803
            },
            {
                "img": "https://arxiv.org/html/2505.07782/x10.png",
                "caption": "Figure 10:Step-wise HumanRank performance comparisons between reasoning and non-reasoning models, considering only code execution and validation steps. Information-requesting steps are excluded.",
                "position": 806
            },
            {
                "img": "https://arxiv.org/html/2505.07782/x11.png",
                "caption": "Figure 11:Proportion of code execution actions relative to combined execution and validation actions.",
                "position": 827
            },
            {
                "img": "https://arxiv.org/html/2505.07782/x12.png",
                "caption": "Figure 12:Average failure rates across tasks.",
                "position": 830
            },
            {
                "img": "https://arxiv.org/html/2505.07782/x13.png",
                "caption": "Figure 13:Execution error types.",
                "position": 840
            },
            {
                "img": "https://arxiv.org/html/2505.07782/x14.png",
                "caption": "Figure 14:Total chat history length (left) and best solution length (right).",
                "position": 850
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitations and Broader Impacts",
        "images": []
    },
    {
        "header": "Appendix BDisclaimer",
        "images": []
    },
    {
        "header": "Appendix CUnified Data Structure",
        "images": []
    },
    {
        "header": "Appendix DData Details",
        "images": []
    },
    {
        "header": "Appendix EImplementation Details",
        "images": []
    },
    {
        "header": "Appendix FAdditional Experimental Results",
        "images": []
    },
    {
        "header": "Appendix GPrompt Details",
        "images": []
    },
    {
        "header": "Appendix HCode Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.07782/x15.png",
                "caption": "Figure 16:Statistics of method implementation across 50 evaluation tasks of eight frontiers LLM.",
                "position": 6804
            }
        ]
    },
    {
        "header": "Appendix IAgent Scaffolds",
        "images": []
    }
]