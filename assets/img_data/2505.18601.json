[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18601/x1.png",
                "caption": "Figure 1:Conceptual overview ofFlex-Judge. We train a multimodal judge model using a small amount of text-only reasoning data. Unlike previous approaches that require modality-specific supervision,Flex-Judge leverages structured text-only rationale behind judgments to enable generalization across modalities.Once trained,Flex-Judge can be applied to various evaluation tasks, including vision-language tasks, audio quality scoring, and molecular structure, without the need for additional task-specific or modality-specific annotations.",
                "position": 173
            }
        ]
    },
    {
        "header": "2Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18601/x2.png",
                "caption": "(a)Quality & Length",
                "position": 247
            },
            {
                "img": "https://arxiv.org/html/2505.18601/x2.png",
                "caption": "(a)Quality & Length",
                "position": 250
            },
            {
                "img": "https://arxiv.org/html/2505.18601/x3.png",
                "caption": "(b)Number of Samples",
                "position": 255
            },
            {
                "img": "https://arxiv.org/html/2505.18601/x4.png",
                "caption": "(c)Sampling Temperature",
                "position": 260
            },
            {
                "img": "https://arxiv.org/html/2505.18601/x5.png",
                "caption": "Figure 3:Reasoning process ofFlex-Judge on the text-image alignment task (GenAI-Bench[35]). Additional qualitative examples are found in AppendixD.",
                "position": 302
            }
        ]
    },
    {
        "header": "3Experimental Evaluation ofFlex-Judge",
        "images": []
    },
    {
        "header": "4Broader Impact: Case Study on Molecule Evaluator",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18601/x6.png",
                "caption": "Figure 4:(Left) Accuracy‚Äâ(%) trends on the parallel artificial membrane permeability assay (PAMPA;[61]) task with different judgment scores. (Middle) Accuracy trends on the number of sampled responses in best-of-NùëÅNitalic_Nsampling. (Right) Performance comparison with reward-guided Mol-LLaMA. We report accuracy with prompt styles of default, CoT, and task information, as described in[27].",
                "position": 1490
            },
            {
                "img": "https://arxiv.org/html/2505.18601/x6.png",
                "caption": "",
                "position": 1493
            },
            {
                "img": "https://arxiv.org/html/2505.18601/x7.png",
                "caption": "",
                "position": 1497
            }
        ]
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18601/x8.png",
                "caption": "Figure 5:(Left) Performance comparison ofFlex-Judge with and without reasoning. (Right) Relationship between the average reasoning length ofFlex-VL-7B and the accuracy gain from reasoning over non-reasoning evaluation across subcategories in MLLM-as-a-Judge (Pair w. Tie).",
                "position": 1601
            },
            {
                "img": "https://arxiv.org/html/2505.18601/x8.png",
                "caption": "",
                "position": 1664
            },
            {
                "img": "https://arxiv.org/html/2505.18601/x9.png",
                "caption": "(a)Majority Voting (Pair)",
                "position": 1676
            },
            {
                "img": "https://arxiv.org/html/2505.18601/x9.png",
                "caption": "(a)Majority Voting (Pair)",
                "position": 1679
            },
            {
                "img": "https://arxiv.org/html/2505.18601/x10.png",
                "caption": "(b)Budget Forcing (Pair)",
                "position": 1684
            },
            {
                "img": "https://arxiv.org/html/2505.18601/x11.png",
                "caption": "(c)Majority Voting (Score)",
                "position": 1689
            },
            {
                "img": "https://arxiv.org/html/2505.18601/x12.png",
                "caption": "(d)Budget Forcing (Score)",
                "position": 1694
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Related Work",
        "images": []
    },
    {
        "header": "Appendix BExperimental Details",
        "images": []
    },
    {
        "header": "Appendix CAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18601/x13.png",
                "caption": "(a)Flex-Omni-7B vs. Human",
                "position": 3437
            },
            {
                "img": "https://arxiv.org/html/2505.18601/x13.png",
                "caption": "(a)Flex-Omni-7B vs. Human",
                "position": 3440
            },
            {
                "img": "https://arxiv.org/html/2505.18601/x14.png",
                "caption": "(b)Flex-VL-7B vs. Human",
                "position": 3445
            },
            {
                "img": "https://arxiv.org/html/2505.18601/x15.png",
                "caption": "(a)Image Understanding",
                "position": 3460
            },
            {
                "img": "https://arxiv.org/html/2505.18601/x15.png",
                "caption": "(a)Image Understanding",
                "position": 3463
            },
            {
                "img": "https://arxiv.org/html/2505.18601/x16.png",
                "caption": "(b)Image Edition",
                "position": 3468
            },
            {
                "img": "https://arxiv.org/html/2505.18601/x17.png",
                "caption": "(c)Video Generation",
                "position": 3473
            },
            {
                "img": "https://arxiv.org/html/2505.18601/x18.png",
                "caption": "Figure 13:Reasoning process ofFlex-Judge on the OCR task (VL-RewardBench).",
                "position": 3635
            },
            {
                "img": "https://arxiv.org/html/2505.18601/x19.png",
                "caption": "Figure 14:Reasoning process ofFlex-Judge on the image quality assessment task (MJ-Bench).",
                "position": 3638
            }
        ]
    },
    {
        "header": "Appendix DQualitative Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18601/x20.png",
                "caption": "Figure 15:Reasoning process ofFlex-Judge on the image editing task (GenAI-Bench).",
                "position": 3674
            },
            {
                "img": "https://arxiv.org/html/2505.18601/x21.png",
                "caption": "Figure 16:Reasoning process ofFlex-Mol-LLaMA judge on the PAMPA task.",
                "position": 3684
            },
            {
                "img": "https://arxiv.org/html/2505.18601/x22.png",
                "caption": "",
                "position": 3688
            },
            {
                "img": "https://arxiv.org/html/2505.18601/x23.png",
                "caption": "Figure 17:Reasoning process ofFlex-Mol-LLaMA, judging the preference of two Mol-LLaMA responses on biological attributes prediction data. Here, the response 1 is preferred (score 9) against the response 2 (score 4).",
                "position": 3692
            }
        ]
    },
    {
        "header": "Appendix ELimitations",
        "images": []
    }
]