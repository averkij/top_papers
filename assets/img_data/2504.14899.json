[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.14899/x1.png",
                "caption": "Figure 1:Given a single-view image across various domains (e.g., real-world, text-to-image, animation), we first extract the monocular depth and focal length of it via Depth-Pro[6]and then achieve relative point clouds. Then, the proposed Uni3C can generate impressive videos under arbitrary camera trajectories (a), human motion characters (SMPL-X[36]), or both of these conditions (b).\n(c) Uni3C further supports the camera-controlled motion transfer.",
                "position": 157
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary: Video Diffusion Models",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.14899/x2.png",
                "caption": "Figure 2:The overview pipeline of PCDController.PCDController is built as a lightweight DiT trained from scratch. We first obtain the point clouds via the monocular depth extracted from the first view. Then, the point clouds are warped and rendered into the videoVpâ¢câ¢dsubscriptğ‘‰ğ‘ğ‘ğ‘‘V_{pcd}italic_V start_POSTSUBSCRIPT italic_p italic_c italic_d end_POSTSUBSCRIPT. The input conditions for PCDController comprise renderedVpâ¢câ¢dsubscriptğ‘‰ğ‘ğ‘ğ‘‘V_{pcd}italic_V start_POSTSUBSCRIPT italic_p italic_c italic_d end_POSTSUBSCRIPT, PlÃ¼cker rayğğ\\mathbf{P}bold_P, and the noisy latentztsubscriptğ‘§ğ‘¡z_{t}italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT.\nNote that only the PCDController and camera encoder are trainable in our framework.",
                "position": 291
            },
            {
                "img": "https://arxiv.org/html/2504.14899/x3.png",
                "caption": "Figure 3:Results of PCDController with imperfect point clouds.Benefiting from the well-preserved capacity from VDM, PCDController enjoys robust generation with inferior point clouds.",
                "position": 343
            },
            {
                "img": "https://arxiv.org/html/2504.14899/x4.png",
                "caption": "Figure 4:The assignment of multi-modal conditions for Uni3C.Camera, point clouds, SMPL-X[36]and Hamer[37]are all presented as 3D conditions.",
                "position": 375
            },
            {
                "img": "https://arxiv.org/html/2504.14899/x5.png",
                "caption": "Figure 5:The overview of global 3D world guidance.(a) We first align the SMPL-X characters from the human world spaceWhâ¢uâ¢msubscriptğ‘Šâ„ğ‘¢ğ‘šW_{hum}italic_W start_POSTSUBSCRIPT italic_h italic_u italic_m end_POSTSUBSCRIPTto the environment world spaceWeâ¢nâ¢vsubscriptğ‘Šğ‘’ğ‘›ğ‘£W_{env}italic_W start_POSTSUBSCRIPT italic_e italic_n italic_v end_POSTSUBSCRIPTcomprising dense point clouds. (b) GeoCalib[53]is used to calibrate the gravity direction of SMPL-X. (c) The rigid transformation coefficientss~,R~,t~~ğ‘ ~ğ‘…~ğ‘¡\\tilde{s},\\tilde{R},\\tilde{t}over~ start_ARG italic_s end_ARG , over~ start_ARG italic_R end_ARG , over~ start_ARG italic_t end_ARGare employed to align the whole SMPL-X sequence. We re-render all aligned conditions under specific camera trajectories as the global 3D world guidance.",
                "position": 379
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.14899/x6.png",
                "caption": "Figure 6:Ablation results of PlÃ¼cker ray and point clouds during the training phase.Point clouds enjoy highly accurate camera control against PlÃ¼cker ray.",
                "position": 805
            },
            {
                "img": "https://arxiv.org/html/2504.14899/x7.png",
                "caption": "Figure 7:Rendering results with and without gravity calibration by GeoCalib[53].",
                "position": 823
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.14899/x8.png",
                "caption": "Figure 8:Our out-of-distribution benchmark for camera control.The validation set includes generative, human, scene-level, and object-level images with diverse aspect ratios.",
                "position": 863
            },
            {
                "img": "https://arxiv.org/html/2504.14899/x9.png",
                "caption": "Figure 9:Qualitative results of camera control on our benchmark.We compare the proposed PCDController to ViewCrafter[66], SEVA[71], and our model without point cloud guidance. The leftmost image is the reference condition. â€œfullâ€ indicates using both PlÃ¼cker ray and point clouds as conditions.",
                "position": 867
            },
            {
                "img": "https://arxiv.org/html/2504.14899/x10.png",
                "caption": "Figure 10:Results of the challenging orbital 360âˆ˜rotations from PCDController.The leftmost images are the reference views.",
                "position": 872
            },
            {
                "img": "https://arxiv.org/html/2504.14899/x11.png",
                "caption": "Figure 11:Results of unified camera and human motion controls.The leftmost images are the reference views, while the first row indicates the aligned 3D world guidance.",
                "position": 876
            },
            {
                "img": "https://arxiv.org/html/2504.14899/x12.png",
                "caption": "Figure 12:Results of unified camera, human motion, and Hamer controls.The leftmost images are the reference views, while the first and second rows indicate the aligned 3D world guidance and Hamer rendering, respectively.",
                "position": 880
            },
            {
                "img": "https://arxiv.org/html/2504.14899/x13.png",
                "caption": "Figure 13:Results of motion transfer.The first row indicates the reference video, while others show our generated videos transferring motions from the reference sequence.",
                "position": 884
            },
            {
                "img": "https://arxiv.org/html/2504.14899/x14.png",
                "caption": "Figure 14:Results transferred from random integrated motion clips of BABEL[41].The motion sequences are listed on the left, which are executed from light to dark colors.",
                "position": 888
            },
            {
                "img": "https://arxiv.org/html/2504.14899/x15.png",
                "caption": "Figure 15:Failed cases generated by Uni3C.These results are primarily limited by the conflict between human motions and environments.",
                "position": 892
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]