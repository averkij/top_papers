[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.01079/x1.png",
                "caption": "",
                "position": 141
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.01079/x2.png",
                "caption": "Figure 2:Overview.(a) The left denotes an illustration of how Multi Query Disentanglement is performed in the cross-attention layer. (b) The upper right figure shows Background Consistency Guidance with recalled latents, conducting latent blending with the saved latents. (c) The right below shows the layer-wise memory, saving the previous editing steps‚Äô latents, masks, and prompt embeddings.",
                "position": 230
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.01079/x3.png",
                "caption": "Figure 3:Overview of our proposed Multi-Edit Benchmark for evaluation of iterative editing scenario.(a) explains the dataset generation pipeline through GPT-4 API, and (b) explains the evaluation methodology in visual alignment using CLIP and semantic alignment using LLaVa for single-image and in a layer-wise manner.",
                "position": 392
            },
            {
                "img": "https://arxiv.org/html/2505.01079/x4.png",
                "caption": "Table 2:Quantitative results on our proposed benchmark.Our method shows higher semantic and visual alignment than other baselines, including layout-to-image synthesis and image editing frameworks. Ordering‚Ä†‚Ä†\\dagger‚Ä†denotes the adaptation of the framework adequate for iterative image generation.",
                "position": 491
            }
        ]
    },
    {
        "header": "4Multi-Edit Benchmark",
        "images": []
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.01079/x4.png",
                "caption": "Figure 4:Qualitative comparison on the effect of Query Disentanglement (QD).",
                "position": 663
            },
            {
                "img": "https://arxiv.org/html/2505.01079/x5.png",
                "caption": "Figure 5:Comparison in image editing capability with latest image editing models.[32,3]Note that the initial image is generated by our framework, which is equivalent to PixArt-Œ±ùõº\\alphaitalic_Œ±[13]with no mask input.",
                "position": 725
            },
            {
                "img": "https://arxiv.org/html/2505.01079/x6.png",
                "caption": "Figure 6:Improved editability of image through Background Consistency Guidance and Multi-Query Disentangled cross attention.Through recycling the previous step‚Äôs latents, we can remove the object that is behind the foreground object, enabling enhanced editability of the image.",
                "position": 742
            },
            {
                "img": "https://arxiv.org/html/2505.01079/x7.png",
                "caption": "Figure 7:Comparison in interactive scenarios with existing T2I generative models.Stable Diffusion XL[38]and PixArt-Œ±ùõº\\alphaitalic_Œ±[13]use text input only.",
                "position": 757
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.01079/x8.png",
                "caption": "",
                "position": 1615
            }
        ]
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.01079/x9.png",
                "caption": "Figure 9:Analysis on computational resources for iterative editing.",
                "position": 1790
            },
            {
                "img": "https://arxiv.org/html/2505.01079/x10.png",
                "caption": "Figure 10:Extensive multi-editing scenario.Our framework enables sequential editing of multiple edits, more than just two or three times editing, meeting the user‚Äôs need to edit extensively on generated images.",
                "position": 1793
            }
        ]
    },
    {
        "header": "Appendix BAnalysis on Computational Overhead",
        "images": []
    },
    {
        "header": "Appendix CPerceptual Study",
        "images": []
    },
    {
        "header": "Appendix DDataset and Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.01079/x11.png",
                "caption": "Figure 11:Comparison of BLEU, METOER, and CLIP score on each step.",
                "position": 2016
            },
            {
                "img": "https://arxiv.org/html/2505.01079/x12.png",
                "caption": "Figure 12:Qualitative comparison on LooseControl.",
                "position": 2059
            },
            {
                "img": "https://arxiv.org/html/2505.01079/x13.png",
                "caption": "Figure 13:Comparison with other latest editing approaches[2,32]with Multi-Edit Bench Dataset.The approaches in the first two rows show results with baseline editing approaches. The background image is generated by our framework.",
                "position": 2160
            },
            {
                "img": "https://arxiv.org/html/2505.01079/x14.png",
                "caption": "Figure 14:Comparison with other latest editing approaches.The approaches in the first two rows for each example show results with baseline editing approaches. The background image is generated by our framework.",
                "position": 2246
            },
            {
                "img": "https://arxiv.org/html/2505.01079/x15.png",
                "caption": "Figure 15:Comparison with other latest editing approaches.The approaches in the first two rows for each example show results with baseline editing approaches. The background image is generated by our framework.",
                "position": 2249
            },
            {
                "img": "https://arxiv.org/html/2505.01079/x16.png",
                "caption": "Figure 16:Comparison under improved editing scenario.Ours maintain the background well compared to other commercial products[1,37]or baselines[3,32].",
                "position": 2252
            },
            {
                "img": "https://arxiv.org/html/2505.01079/x17.png",
                "caption": "Figure 17:Comparison with depth-aware text-to-image approaches.The approaches in the first three rows utilize a depth map, exemplar image, and text prompt. The approaches in the next three rows get a depth map and text prompt. Our approach rivals the baseline approaches without using depth maps or exemplar images.",
                "position": 2255
            }
        ]
    },
    {
        "header": "Appendix EQualitative Results",
        "images": []
    }
]