[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.09925/x1.png",
                "caption": "Figure 1:Performance comparison of FUSION with leading MLLM models across 18 benchmark dimensions. With only 630 vision tokens, our model (FUSION-X) significantly outperforms Cambrian-1 and Florence-VL, achieving overall parity with LLaVA-OneVision, while maintaining a minimal performance gap with top-tier models such as InternVL2 and Qwen2VL. Furthermore, even when the number of vision tokens is reduced to 300, our model (FUSION-L) preserves 95% of its original performance, remaining on par with Florence-VL.",
                "position": 126
            },
            {
                "img": "https://arxiv.org/html/2504.09925/x2.png",
                "caption": "Figure 2:Visualization of modality alignment and integration. At pixel-level, we compute attention maps between image regions and question-relevant keywords within the vision encoder. At space-level, we measure the cosine similarity between vision tokens projected into the LLM embedding space and corresponding keywords. At question-level, we visualize attention maps from question keywords to vision tokens during LLM decoding. The results indicate that our model achieves consistent and progressively enhanced cross-modal alignment throughout the processing pipeline.",
                "position": 133
            },
            {
                "img": "https://arxiv.org/html/2504.09925/x3.png",
                "caption": "Figure 3:Illustration of our Text-Guided Unified Vision Encoding and Dual-Supervised Semantic Mapping Loss. Given an input image, the corresponding question is first projected into the vision feature space and processed jointly with the image. The extracted visual features are then mapped into the text space and fed into LLM. To ensure the reliability of the mapping MLP, we reconstruct the text and image tokens by reusing the encoded tokens and projecting them back into their original feature spaces, then compute the similarity between the reconstructed and raw tokens to encourage structural alignment between the two spaces.",
                "position": 146
            }
        ]
    },
    {
        "header": "2Preliminaries and Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.09925/x4.png",
                "caption": "Figure 4:Illustration of our Context-Aware Recursive Alignment Decoding. For each set of question tokens (highlighted in yellow), we prepend a set of context-aware latent tokens (highlighted in green). Additional interaction layers are introduced between decoding layers, where vision tokens interact with both latent tokens and question tokens at a question-level granularity (e.g., Group 1, Group 2, …).",
                "position": 195
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.09925/x5.png",
                "caption": "Figure 5:Overview of our Text-Centered QA Dataset framework. Our approach shifts the focus from visual content to textual richness by leveraging high-quality captions, enriching them with LLMs, and using them as the foundation for both image generation via diffusion models and diverse QA pair construction.",
                "position": 399
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Ablation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.09925/x6.png",
                "caption": "(a)Model performance under varying numbers of latent vision tokens.",
                "position": 1790
            },
            {
                "img": "https://arxiv.org/html/2504.09925/x6.png",
                "caption": "(a)Model performance under varying numbers of latent vision tokens.",
                "position": 1793
            },
            {
                "img": "https://arxiv.org/html/2504.09925/x7.png",
                "caption": "(b)Model performance under varying numbers of global vision tokens.",
                "position": 1798
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.09925/x8.png",
                "caption": "",
                "position": 2889
            },
            {
                "img": "https://arxiv.org/html/2504.09925/x9.png",
                "caption": "",
                "position": 2968
            },
            {
                "img": "https://arxiv.org/html/2504.09925/x10.png",
                "caption": "",
                "position": 3114
            }
        ]
    },
    {
        "header": "Appendix AData Collection",
        "images": []
    },
    {
        "header": "Appendix BAdditional Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.09925/x11.png",
                "caption": "Figure 8:Comparison of Modality Alignment with traditional MLLMs. We conduct a comparative analysis of modality alignment across three different levels using FUSION, LLaVA, and LLaVA-NeXT. To ensure a fair evaluation, we adopt consistent visualization and augmentation strategies across all models.",
                "position": 6642
            }
        ]
    },
    {
        "header": "Appendix CComparison and Deeper Exploration of Model Architectures",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.09925/x12.png",
                "caption": "Figure 9:Illustration of our model’s multiturn dialogue",
                "position": 6992
            },
            {
                "img": "https://arxiv.org/html/2504.09925/x12.png",
                "caption": "",
                "position": 6995
            },
            {
                "img": "https://arxiv.org/html/2504.09925/x13.png",
                "caption": "",
                "position": 7000
            },
            {
                "img": "https://arxiv.org/html/2504.09925/x14.png",
                "caption": "Figure 10:Illustration of our SynthConvShort",
                "position": 8503
            },
            {
                "img": "https://arxiv.org/html/2504.09925/x15.png",
                "caption": "Figure 11:Illustration of our SynthConvLong",
                "position": 8506
            },
            {
                "img": "https://arxiv.org/html/2504.09925/x16.png",
                "caption": "Figure 12:Illustration of our SynthContrastShort",
                "position": 8509
            },
            {
                "img": "https://arxiv.org/html/2504.09925/x17.png",
                "caption": "Figure 13:Illustration of our SynthContrastLong",
                "position": 8512
            },
            {
                "img": "https://arxiv.org/html/2504.09925/x18.png",
                "caption": "Figure 14:Illustration of our SynthMultiChoice",
                "position": 8515
            },
            {
                "img": "https://arxiv.org/html/2504.09925/x19.png",
                "caption": "Figure 15:Illustration of our SynthColor",
                "position": 8518
            },
            {
                "img": "https://arxiv.org/html/2504.09925/x20.png",
                "caption": "Figure 16:Illustration of our SynthCount",
                "position": 8521
            },
            {
                "img": "https://arxiv.org/html/2504.09925/x21.png",
                "caption": "Figure 17:Illustration of our SynthScene",
                "position": 8524
            },
            {
                "img": "https://arxiv.org/html/2504.09925/x22.png",
                "caption": "Figure 18:Illustration of our SynthSpatial",
                "position": 8527
            },
            {
                "img": "https://arxiv.org/html/2504.09925/x23.png",
                "caption": "Figure 19:Illustration of our SynthText",
                "position": 8530
            }
        ]
    },
    {
        "header": "Appendix DSynthesized Language-Driven Framework Process",
        "images": []
    }
]