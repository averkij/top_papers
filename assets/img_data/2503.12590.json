[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12590/x1.png",
                "caption": "",
                "position": 83
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12590/x2.png",
                "caption": "Figure 2:Simple token replacement in DiT (right) achieves high-fidelity subject reconstruction through its position-disentangled representation, while U-Net‚Äôs convolutional entanglement (left) induces blurred edges and artifacts.",
                "position": 124
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12590/x3.png",
                "caption": "Figure 3:Attention sharing[57,5]fails in DiT due to the explicit positional encoding mechanism. When keeping the original positions(i,j)‚àà[0,w)√ó[0,h)ùëñùëó0ùë§0‚Ñé(i,j)\\in[0,w)\\times[0,h)( italic_i , italic_j ) ‚àà [ 0 , italic_w ) √ó [ 0 , italic_h )in reference tokens, denoising tokens over-attend to reference ones with the same positions (shown in attention maps of (a)), resulting in ghosting artifacts in the generated image. Modified strategies, (b) removing positions and (c) shifting to non-overlapping regions, avoid collisions but loses identity alignment, as attention is almost absent on reference tokens.",
                "position": 257
            },
            {
                "img": "https://arxiv.org/html/2503.12590/x4.png",
                "caption": "Figure 4:Method overview. Our framework anchors subject identity in early denoising through mask-guided token replacement with preserved positional encoding, and transitions to multi-modal attention for semantic fusion with text in later steps. During token replacement, we inject variations via patch perturbations. This timestep-adaptive strategy balances identity preservation and generative flexibility.",
                "position": 260
            },
            {
                "img": "https://arxiv.org/html/2503.12590/x5.png",
                "caption": "Figure 5:Seamless extensions. Our framework enables: (a) layout-guided generation by translating token-injected regions, (b) multi-subject composition through sequential token injection, and (c) inpainting and outpainting via specifying masks and increased replacement.",
                "position": 263
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12590/x6.png",
                "caption": "Figure 6:Qualitative comparisons on single-subject personalization. More results can be found in the supplementary materials.",
                "position": 342
            },
            {
                "img": "https://arxiv.org/html/2503.12590/x7.png",
                "caption": "Figure 7:Qualitative comparisons on multi-subject personalization.",
                "position": 385
            },
            {
                "img": "https://arxiv.org/html/2503.12590/x8.png",
                "caption": "Figure 8:Qualitative results on subject-scene composition.",
                "position": 388
            },
            {
                "img": "https://arxiv.org/html/2503.12590/x9.png",
                "caption": "Figure 9:Qualitative ablation studies on token replacement thresholdœÑùúè\\tauitalic_œÑand patch perturbation.",
                "position": 690
            },
            {
                "img": "https://arxiv.org/html/2503.12590/x10.png",
                "caption": "Figure 10:Visualization results ofPersonalize Anythinginlayout-guided generation(top),inpainting(middle), andoutpainting(bottom).",
                "position": 693
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Analysis of Position Sensitivity in DiT",
        "images": []
    },
    {
        "header": "7User Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12590/x11.png",
                "caption": "Figure 11:User study results on single-subject personalization.",
                "position": 1647
            },
            {
                "img": "https://arxiv.org/html/2503.12590/x12.png",
                "caption": "Figure 12:User study results on multi-subject personalization.",
                "position": 1650
            },
            {
                "img": "https://arxiv.org/html/2503.12590/x13.png",
                "caption": "Figure 13:User study results on subject-scene composition.",
                "position": 1653
            },
            {
                "img": "https://arxiv.org/html/2503.12590/x14.png",
                "caption": "Figure 14:Full qualitative comparisons on single-subject personalization.",
                "position": 1665
            }
        ]
    },
    {
        "header": "8More Results",
        "images": []
    }
]