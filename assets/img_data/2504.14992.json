[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.14992/x1.png",
                "caption": "(a)Training loss.",
                "position": 180
            },
            {
                "img": "https://arxiv.org/html/2504.14992/x1.png",
                "caption": "(a)Training loss.",
                "position": 183
            },
            {
                "img": "https://arxiv.org/html/2504.14992/x2.png",
                "caption": "(b)HellaSwag accuracy.",
                "position": 188
            },
            {
                "img": "https://arxiv.org/html/2504.14992/x3.png",
                "caption": "(c)Prefilling time.",
                "position": 193
            },
            {
                "img": "https://arxiv.org/html/2504.14992/x4.png",
                "caption": "(d)Decoding latency.",
                "position": 198
            }
        ]
    },
    {
        "header": "2Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.14992/x5.png",
                "caption": "Figure 2:Overview of the transformer block inPHD. Specifically, the input tokens are repeated multiple times fed into the transformer block simultaneously. Theoriginal tokensgenerate KV cache that can be attended to by all the following tokens, while thehidden decoding tokensonly generate KV cache that can be attended to within the current tokens (Token 3in the Figure).",
                "position": 251
            },
            {
                "img": "https://arxiv.org/html/2504.14992/extracted/6375030/figures/chunkwise_swa_rearrange.png",
                "caption": "Figure 3:The attention matrix inPHD. The interleaving oforiginal tokensandhidden decoding tokensintroduce very sparse attention matrix that is not device friendly. We rearrange the input sequence and split the original tokens and hidden decoding tokens into two groups. In this way, we group the un-attended attention positions in a continuous block, which is efficient for optimization.",
                "position": 324
            },
            {
                "img": "https://arxiv.org/html/2504.14992/extracted/6375030/figures/attention_matrix_compare.png",
                "caption": "Figure 4:Comparison of the attention matrices inPHD,PHD-SWAandPHD-CSWA. In this figure, we set the repeating timesKùêæKitalic_Kto 3, which means there are 2 hidden decoding tokens in each attention matrix, and set the window sizeWùëäWitalic_Wto 4 and chunk sizeCùê∂Citalic_Cto 4.",
                "position": 350
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.14992/x6.png",
                "caption": "Figure 5:Training curves ofPHD-CSWAvariants and baseline model on OLMo2-1.2B. We smooth these metrics via exponential moving average with weight 0.99 for loss and 0.7 for downstream tasks.",
                "position": 372
            },
            {
                "img": "https://arxiv.org/html/2504.14992/x7.png",
                "caption": "(a)",
                "position": 505
            },
            {
                "img": "https://arxiv.org/html/2504.14992/x7.png",
                "caption": "(a)",
                "position": 508
            },
            {
                "img": "https://arxiv.org/html/2504.14992/x7.png",
                "caption": "(a)",
                "position": 511
            },
            {
                "img": "https://arxiv.org/html/2504.14992/x8.png",
                "caption": "(b)",
                "position": 517
            },
            {
                "img": "https://arxiv.org/html/2504.14992/x9.png",
                "caption": "(c)",
                "position": 526
            },
            {
                "img": "https://arxiv.org/html/2504.14992/x9.png",
                "caption": "(c)",
                "position": 529
            },
            {
                "img": "https://arxiv.org/html/2504.14992/x10.png",
                "caption": "(d)",
                "position": 535
            },
            {
                "img": "https://arxiv.org/html/2504.14992/x11.png",
                "caption": "(a)Training loss.",
                "position": 574
            },
            {
                "img": "https://arxiv.org/html/2504.14992/x11.png",
                "caption": "(a)Training loss.",
                "position": 577
            },
            {
                "img": "https://arxiv.org/html/2504.14992/x12.png",
                "caption": "(b)Validation loss.",
                "position": 582
            },
            {
                "img": "https://arxiv.org/html/2504.14992/x13.png",
                "caption": "(c)Hellaswag accuracy.",
                "position": 587
            },
            {
                "img": "https://arxiv.org/html/2504.14992/x14.png",
                "caption": "(d)MMLU-V accuracy.",
                "position": 592
            },
            {
                "img": "https://arxiv.org/html/2504.14992/x15.png",
                "caption": "(a)",
                "position": 714
            },
            {
                "img": "https://arxiv.org/html/2504.14992/x15.png",
                "caption": "(a)",
                "position": 717
            },
            {
                "img": "https://arxiv.org/html/2504.14992/x16.png",
                "caption": "(b)",
                "position": 722
            }
        ]
    },
    {
        "header": "4Related Works",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]