[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16040/x1.png",
                "caption": "Figure 1:Performance overview. (Left) Comparison betweenTime-MoEmodels and state-of-the-art time series foundation models, reporting the average zero-shot performance across six benchmark datasets. (Right) Comparison of few- and zero-shot performance betweenTime-MoEand dense variants, with similar effective FLOPs per time series token, across the same six benchmarks.",
                "position": 122
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16040/x2.png",
                "caption": "Figure 2:The architecture ofTime-MoE, which is a decoder-only model. Given an input time series of arbitrary length,1we first tokenize it into a sequence of data points,2which are then encoded. These tokens are processed throughNùëÅNitalic_N-stacked backbone layers, primarily consisting of causal multi-head self-attention and3sparse temporal mixture-of-expert layers. During training,4we optimize forecasting heads at multiple resolutions. For model inference,Time-MoEprovides forecasts of flexible length by5dynamically scheduling these heads.",
                "position": 199
            }
        ]
    },
    {
        "header": "4Main Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16040/extracted/5895268/figures/timemoe-logo.png",
                "caption": "Table 3:Full results of zero-shot forecasting experiments. A lower MSE or MAE indicates a better prediction. TimesFM, due to its use of Weather datasets in pretraining, is not evaluated on these two datasets and is denoted by a dash (‚àí--).Red: the best,Blue: the 2nd best.",
                "position": 530
            },
            {
                "img": "https://arxiv.org/html/2409.16040/extracted/5895268/figures/zero-shot.png",
                "caption": "",
                "position": 544
            },
            {
                "img": "https://arxiv.org/html/2409.16040/extracted/5895268/figures/timemoe-logo.png",
                "caption": "Table 4:Full results of in-domain forecasting experiments. A lower MSE or MAE indicates a better prediction.\nFull-shot results besides Global Temp are obtained from(Liu et¬†al.,2024a).Red: the best,Blue: the 2nd best.",
                "position": 1415
            },
            {
                "img": "https://arxiv.org/html/2409.16040/extracted/5895268/figures/full-shot.png",
                "caption": "",
                "position": 1431
            },
            {
                "img": "https://arxiv.org/html/2409.16040/x3.png",
                "caption": "Figure 3:Scalability analysis.(Left)Comparison of dense and sparse models in terms of training and inference costs.(Right)Average MSE for 96-horizon forecasting across six benchmarks, comparingTime-MoEand dense models, both trained from scratch with varying data sizes.",
                "position": 2412
            },
            {
                "img": "https://arxiv.org/html/2409.16040/x4.png",
                "caption": "Figure 4:Gating scores for experts across different layers in the six benchmarks.",
                "position": 2472
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AFurther Related Work",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    },
    {
        "header": "Appendix CProcessed Data Archive",
        "images": []
    },
    {
        "header": "Appendix DAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16040/x5.png",
                "caption": "Figure 5:Zero-shot forecasting cases from ETTh1 by different models, with forecast horizon 96.Bluelines are the ground truths andreadlines are the model predictions.",
                "position": 5569
            },
            {
                "img": "https://arxiv.org/html/2409.16040/x6.png",
                "caption": "Figure 6:Zero-shot forecasting cases from ETTh2 by different models, with forecast horizon 96.",
                "position": 5572
            },
            {
                "img": "https://arxiv.org/html/2409.16040/x7.png",
                "caption": "Figure 7:Zero-shot forecasting cases from ETTm1 by different models, with forecast horizon 96.",
                "position": 5575
            },
            {
                "img": "https://arxiv.org/html/2409.16040/x8.png",
                "caption": "Figure 8:Zero-shot forecasting cases from ETTm2 by different models, with forecast horizon 96.",
                "position": 5578
            },
            {
                "img": "https://arxiv.org/html/2409.16040/x9.png",
                "caption": "Figure 9:Zero-shot forecasting cases from Weather by different models, with forecast horizon 96.",
                "position": 5581
            },
            {
                "img": "https://arxiv.org/html/2409.16040/x10.png",
                "caption": "Figure 10:Zero-shot forecasting cases from Global Temp by different models, with forecast horizon 96.",
                "position": 5584
            }
        ]
    },
    {
        "header": "Appendix EForecast Showcases",
        "images": []
    }
]