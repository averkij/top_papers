[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24320/x1.png",
                "caption": "Figure 1:Left:Critique-RL achieves better performance and discrimination on MATH.Right:Inference compute scaling for Critique-RL, with @2k and @3k indicating sampling amounts that are 2 times and 3 times the x-axis value, respectively. Critique-RL improves the performance ceiling and is more compute-efficient.",
                "position": 162
            },
            {
                "img": "https://arxiv.org/html/2510.24320/x2.png",
                "caption": "Figure 2:Left:A case illustrating the two-player actor-critic interaction, including the original response from the actor, the critique from the critic, and the refinement from the Actor.Right:Overview of our method and its comparison with baseline RL. The snowflake iconon the Actor indicates that it is fixed, while the fire iconon the Critic indicates that it will be updated. Our method employs a two-stage RL process. It optimize discriminability of critique models in Stage I, and optimize helpfulness while maintaining discriminability in Stage II.",
                "position": 167
            },
            {
                "img": "https://arxiv.org/html/2510.24320/x5.png",
                "caption": "",
                "position": 169
            },
            {
                "img": "https://arxiv.org/html/2510.24320/x6.png",
                "caption": "",
                "position": 169
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24320/x7.png",
                "caption": "Figure 3:Training dynamics of preliminary experiments. ‚ÄúAcc@Dis Originally Correct‚Äù and ‚ÄúAcc@Dis Originally Incorrect‚Äù refer to the discrimination accuracy of originally correct and incorrect responses, respectively. Baselines using indirect reward signals to optimize helpfulness tend to exhibit overly conservative or aggressive behavior as the discriminability is not well optimized. In contrast, our Critique-RL optimizes discriminability in Stage I, and optimizes helpfulness while maintaining discriminability in Stage II, achieving better inAcc@Refine,ùö´ùíÑ‚Üíùíä\\bm{\\Delta^{c\\to i}}andùö´ùíä‚ÜíùíÑ\\bm{\\Delta^{i\\to c}}.",
                "position": 350
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24320/x8.png",
                "caption": "Figure 4:Results of critique-refinement of Critique-RL using Qwen2.5-3B.",
                "position": 794
            }
        ]
    },
    {
        "header": "6Discussion and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24320/x9.png",
                "caption": "Figure 5:Performance with and without the oracle verifier. When the oracle verifier is available, the model no longer needs to make discrimination and just needs to provides useful feedback. This allows us to evaluate the model‚Äôs helpfulness more accurately.",
                "position": 943
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix APerformance on Varying Base Models",
        "images": []
    },
    {
        "header": "Appendix BPerformance on Varying Model Series",
        "images": []
    },
    {
        "header": "Appendix CComparison with Other Important Refinement Methods",
        "images": []
    },
    {
        "header": "Appendix DMore Test-time Scaling Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24320/",
                "caption": "Figure 6:Inference compute scaling for Critique-RL, with @2k and @3k indicating sampling amounts that are 2 times and 3 times the x-axis value, respectively. Critique-RL improves the performance ceiling and is more compute-efficient.",
                "position": 2027
            },
            {
                "img": "https://arxiv.org/html/2510.24320/x11.png",
                "caption": "Figure 7:Refine compute scaling for Critique-RL and SFT critic with Qwen2.5-3B and Qwen2.5-7B.",
                "position": 2032
            }
        ]
    },
    {
        "header": "Appendix EPerformance on Summarization Task",
        "images": []
    },
    {
        "header": "Appendix FValidating the Effectiveness of Critique Model",
        "images": []
    },
    {
        "header": "Appendix GSensitivity Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24320/x12.png",
                "caption": "Figure 8:Example 1 of qualitative analysis. The actor‚Äôs original response is incorrect. The model after SFT is unable to detect errors in the response, leading the actor‚Äôs refinement response to retain the same errors. However, the model trained after Critique-RL identifies the errors in the original response and provides detailed, constructive suggestions for modification, leading to the correct refinement response.",
                "position": 2353
            },
            {
                "img": "https://arxiv.org/html/2510.24320/x13.png",
                "caption": "Figure 9:Example 2 of qualitative analysis. The actor‚Äôs original response is incorrect. The model trained after Critique-RL Stage I is able to detect this error, demonstrating its discriminability. However, the model provides the actor with low-quality suggestion, causing the actor‚Äôs refinement response to be incorrect. In contrast, for the same erroneous original response, model trained after Critique-RL Stage II not only detects the error but also offers a constructive suggestion, ultimately leading to the correct refinement response, demonstrating the advantage of two-stage RL process.",
                "position": 2356
            }
        ]
    },
    {
        "header": "Appendix HQualitative Analysis",
        "images": []
    }
]