[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17897/x1.png",
                "caption": "Figure 1:(Left) Illustration of the relative training loss (loss of target model - loss of vanilla Transformer) curve between different Transformer variants; model size is fixed to be 82M. (Middle) The average entropy of token importance across layers in ResFormervs.the vanilla Transformer, where token importance is derived from the attention matrix. Lower entropy indicates more focused attention on specific tokens. More details can be found in Eqn.11. (Right) The average entropy of token importance across layers in Llama (8B)(Dubey et¬†al.,2024)and Mistral (7B)(Jiang et¬†al.,2023).",
                "position": 121
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x2.png",
                "caption": "",
                "position": 130
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x3.png",
                "caption": "",
                "position": 135
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x4.png",
                "caption": "(a)Transformer",
                "position": 141
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x4.png",
                "caption": "(a)Transformer",
                "position": 144
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x5.png",
                "caption": "(b)NeuTRENO",
                "position": 149
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x6.png",
                "caption": "(c)DenseFormer",
                "position": 154
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x7.png",
                "caption": "(d)ResFormer",
                "position": 159
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x8.png",
                "caption": "(e)SVFormer",
                "position": 164
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17897/x9.png",
                "caption": "Figure 3:Average token similarity between the outputs of different mapping methods and that of Eqn.2.",
                "position": 281
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x9.png",
                "caption": "Figure 3:Average token similarity between the outputs of different mapping methods and that of Eqn.2.",
                "position": 284
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x10.png",
                "caption": "Figure 4:Ablation study on sharing keys or values in every two layers, with CLAttention denoting sharing both.",
                "position": 289
            }
        ]
    },
    {
        "header": "4Pretrain Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17897/x11.png",
                "caption": "Figure 5:(Left) The relative training curve between a 82M ResFormer and Transformer across different training sequence lengths. (Middle) Average training loss for the final 50 steps across different model sizes and the corresponding fitted curves. (Right) The relative training curve across different model size for a fixed 2,048 training sequence length.",
                "position": 407
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x11.png",
                "caption": "",
                "position": 410
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x12.png",
                "caption": "",
                "position": 414
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x13.png",
                "caption": "",
                "position": 418
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x14.png",
                "caption": "",
                "position": 479
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x15.png",
                "caption": "Figure 7:Ablation study of adding residual connection to queries or keys.",
                "position": 485
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x15.png",
                "caption": "Figure 7:Ablation study of adding residual connection to queries or keys.",
                "position": 488
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x16.png",
                "caption": "Figure 8:Ablation study of adding residual connection using different mapping matrix.",
                "position": 493
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x17.png",
                "caption": "Figure 9:Ablation studies on which historical layer‚Äôs value to include in residual connections.",
                "position": 498
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x18.png",
                "caption": "(a)Token importance.",
                "position": 508
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x18.png",
                "caption": "(a)Token importance.",
                "position": 511
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x19.png",
                "caption": "(b)Norms of value states.",
                "position": 516
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x20.png",
                "caption": "(c)Norms of hidden states.",
                "position": 521
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x21.png",
                "caption": "(a)Transformer.",
                "position": 528
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x21.png",
                "caption": "(a)Transformer.",
                "position": 531
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x22.png",
                "caption": "(b)NeuTRENO.",
                "position": 536
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x23.png",
                "caption": "(c)Resformer.",
                "position": 541
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x24.png",
                "caption": "Figure 12:Left: Distribution of eigenvalues for the value states in the first layer of ResFormer and Transformer. Right: Maximum eigenvalue for each layer of ResFormer and Transformer.",
                "position": 628
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x24.png",
                "caption": "",
                "position": 631
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x25.png",
                "caption": "",
                "position": 635
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x26.png",
                "caption": "Figure 13:The relative training loss for SVFormer and otherK‚Å¢VùêæùëâKVitalic_K italic_Vefficient model compared with vanilla attention. The numbers in parentheses represent the training sequence length. Left: Model with nearly1/212{1}/{2}1 / 2K‚Å¢VùêæùëâKVitalic_K italic_Vcache. Right: Model with nearly1/818{1}/{8}1 / 8K‚Å¢VùêæùëâKVitalic_K italic_Vcache.",
                "position": 661
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x26.png",
                "caption": "",
                "position": 664
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x27.png",
                "caption": "",
                "position": 668
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x28.png",
                "caption": "Figure 14:Left: The relative training loss for SVFormer under different sequence lengths with a fixed batch size of 2M tokens. Right: Analysis of critical point, and we predict it for length 64,000 using linear regression with the last 1,000 data points.",
                "position": 674
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x28.png",
                "caption": "",
                "position": 677
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x29.png",
                "caption": "",
                "position": 681
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x30.png",
                "caption": "(a)Learning Rate.",
                "position": 700
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x30.png",
                "caption": "(a)Learning Rate.",
                "position": 703
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x31.png",
                "caption": "(b)Warmup Steps.",
                "position": 708
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x32.png",
                "caption": "(c)Model Size.",
                "position": 713
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x33.png",
                "caption": "(d)Architecture.",
                "position": 718
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x34.png",
                "caption": "Figure 16:Ablation study of sharing first layer‚Äôs query(key) across all layers.",
                "position": 729
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x34.png",
                "caption": "Figure 16:Ablation study of sharing first layer‚Äôs query(key) across all layers.",
                "position": 732
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x35.png",
                "caption": "Figure 17:Ablation study on sharing values from different numbers of layers.",
                "position": 737
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17897/x36.png",
                "caption": "Figure 18:The average token similarity of hidden states across layers in Llama and Mistral.",
                "position": 1487
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x37.png",
                "caption": "(a)Transformer.",
                "position": 1500
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x38.png",
                "caption": "",
                "position": 1515
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x39.png",
                "caption": "",
                "position": 1520
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x40.png",
                "caption": "",
                "position": 1532
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x41.png",
                "caption": "",
                "position": 1537
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x42.png",
                "caption": "",
                "position": 1542
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x43.png",
                "caption": "(a)Transformer.",
                "position": 1553
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x44.png",
                "caption": "(b)NeuTRENO.",
                "position": 1558
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x45.png",
                "caption": "(c)Resformer.",
                "position": 1563
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x46.png",
                "caption": "(a)Length: 20 tokens.",
                "position": 1570
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x46.png",
                "caption": "(a)Length: 20 tokens.",
                "position": 1573
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x47.png",
                "caption": "(b)Length: 100 tokens.",
                "position": 1578
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x48.png",
                "caption": "(c)Length: 500 tokens.",
                "position": 1583
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x49.png",
                "caption": "(d)Length: 2,048 tokens.",
                "position": 1588
            },
            {
                "img": "https://arxiv.org/html/2410.17897/x50.png",
                "caption": "Figure 21:Ablation study of differentŒªùúÜ\\lambdaitalic_Œªfor NeuTRENO.",
                "position": 1602
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]