[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.01874/x1.png",
                "caption": "((a))One-stepframework yields unstructured reasoning, whiledecoupledpipeline modularly disentangles the flow. We adopt a cognitive-inspired three-stage framework with knowledge internalization.",
                "position": 141
            },
            {
                "img": "https://arxiv.org/html/2601.01874/x1.png",
                "caption": "((a))One-stepframework yields unstructured reasoning, whiledecoupledpipeline modularly disentangles the flow. We adopt a cognitive-inspired three-stage framework with knowledge internalization.",
                "position": 144
            },
            {
                "img": "https://arxiv.org/html/2601.01874/x2.png",
                "caption": "((b))Reasoning drift analysis across three representative pipelines, where higher precision indicates less reasoning drift.",
                "position": 151
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3CogFlow: A Cognitive-Inspired Hierarchical Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.01874/x3.png",
                "caption": "Figure 2:Overview of the proposed visual mathematical reasoning frameworkCogFlow.Inspired by the canonical three-stage human reasoning flow,CogFlowadopts a hierarchical pipeline that integrates Synergistic Visual Rewards (SynVRs) for enhanced perception, a Knowledge Internalization Reward (IntlzR) to bridge perception and reasoning, and Visual-Gated Policy Optimization (VGPO) with Inference Reward (InfR) to anchor multi-step reasoning in perceptual accuracy.",
                "position": 246
            },
            {
                "img": "https://arxiv.org/html/2601.01874/x4.png",
                "caption": "Figure 3:Workflow of SynVRs.SynVRs consist of a Visual Semantic Reward and a Visual Parametric Reward, ensuring local geometric fidelity and global perceptual coherence respectively.\nTogether, these two complementary visual rewards provide a unified supervision mechanism for training robust and accurate visual perception.",
                "position": 264
            },
            {
                "img": "https://arxiv.org/html/2601.01874/x4.png",
                "caption": "Figure 3:Workflow of SynVRs.SynVRs consist of a Visual Semantic Reward and a Visual Parametric Reward, ensuring local geometric fidelity and global perceptual coherence respectively.\nTogether, these two complementary visual rewards provide a unified supervision mechanism for training robust and accurate visual perception.",
                "position": 267
            },
            {
                "img": "https://arxiv.org/html/2601.01874/x5.png",
                "caption": "Figure 4:Pipeline of VGPO.VGPO introduces visual gate and multiple rewards to strengthen multi-step visual reasoning.\nCoupling perceptual quality control with outcome-based optimization promotes stability.",
                "position": 275
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.01874/x6.png",
                "caption": "Table 5:Ablation of three proposed components (i.e., SynVRs, IntlzR and VGPO). The visual gate is always enabled during inference.",
                "position": 2419
            },
            {
                "img": "https://arxiv.org/html/2601.01874/x6.png",
                "caption": "Figure 5:Ablation analysis of SynVRs.Variants exhibit consistent improvements.",
                "position": 2492
            },
            {
                "img": "https://arxiv.org/html/2601.01874/x7.png",
                "caption": "((a))Impact of different error types, whereAllindicates all types are used.",
                "position": 2498
            },
            {
                "img": "https://arxiv.org/html/2601.01874/x7.png",
                "caption": "((a))Impact of different error types, whereAllindicates all types are used.",
                "position": 2501
            },
            {
                "img": "https://arxiv.org/html/2601.01874/x8.png",
                "caption": "((b))Impact of DPO variants.",
                "position": 2507
            },
            {
                "img": "https://arxiv.org/html/2601.01874/x9.png",
                "caption": "Figure 7:The distribution of visual reward values among different post-training methods.A higher concentration of values indicates stronger perceptual grounding achieved by the corresponding training strategy.",
                "position": 2518
            },
            {
                "img": "https://arxiv.org/html/2601.01874/x9.png",
                "caption": "Figure 7:The distribution of visual reward values among different post-training methods.A higher concentration of values indicates stronger perceptual grounding achieved by the corresponding training strategy.",
                "position": 2520
            },
            {
                "img": "https://arxiv.org/html/2601.01874/x10.png",
                "caption": "Figure 8:Ablation analysis of visual gate.TrainingandInferenceindicates visual gate is only used in training and inference phases respectively.",
                "position": 2527
            },
            {
                "img": "https://arxiv.org/html/2601.01874/x11.png",
                "caption": "Figure 9:Error-type analysis.We analyze error-type distributions forCogFlowvariants alongside specialized visual–math models, the GRPO-style model, and the decoupled method. The baseline is denoted by the SFT+GRPO setting.",
                "position": 2533
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Table of Contents",
        "images": []
    },
    {
        "header": "Appendix AGeneral Discussions",
        "images": []
    },
    {
        "header": "Appendix BAdditional Details ofMathCogDataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.01874/x12.png",
                "caption": "Figure 10:Curation pipeline ofMathCog.We labelMathCog-SFTandMathCog-RLbased on the raw data. To constructMathCog-IntlzR, we sample positive examples fromMathCog-SFTand generate five typical negative examples. Then, we adopt Softmax-DPO to train the IntlzR model.",
                "position": 2666
            },
            {
                "img": "https://arxiv.org/html/2601.01874/x13.png",
                "caption": "Figure 11:The illustration of data for Knowledge Internalization Reward.",
                "position": 2762
            },
            {
                "img": "https://arxiv.org/html/2601.01874/x14.png",
                "caption": "Figure 12:Distribution of Word Length among QUESTION, WATCHING, THINKING.We\npresent the distribution of word length,\nwith the horizontal axis representing word length and the vertical axis depicting the corresponding probability distribution.",
                "position": 2847
            },
            {
                "img": "https://arxiv.org/html/2601.01874/x15.png",
                "caption": "Figure 13:Representative cases in theMathCog-SFTandMathCog-RL.",
                "position": 2858
            }
        ]
    },
    {
        "header": "Appendix CAdditional Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.01874/x16.png",
                "caption": "Figure 14:Effectiveness ofCogFlow.We compare the base model (Qwen2.5-VL-7B) and its SFT-Model againstCogFlow.",
                "position": 3253
            },
            {
                "img": "https://arxiv.org/html/2601.01874/x16.png",
                "caption": "Figure 14:Effectiveness ofCogFlow.We compare the base model (Qwen2.5-VL-7B) and its SFT-Model againstCogFlow.",
                "position": 3256
            },
            {
                "img": "https://arxiv.org/html/2601.01874/x17.png",
                "caption": "Figure 15:Perception F1 of SynVRs variants.The W/SynVRs achieves the best performance.",
                "position": 3261
            },
            {
                "img": "https://arxiv.org/html/2601.01874/x18.png",
                "caption": "Table 14:Analysis of Inference Schemes. We evaluateCogFlowand the VLM-R1 baseline under these three settings from the FlowVerse withk=3k=3.",
                "position": 3608
            },
            {
                "img": "https://arxiv.org/html/2601.01874/x18.png",
                "caption": "Figure 16:The distribution of visual reward scores during training.Attempts with scores above the “Threshold” are accepted.",
                "position": 3661
            },
            {
                "img": "https://arxiv.org/html/2601.01874/x19.png",
                "caption": "Figure 17:Scalability and broader applicability ofCogFlow.",
                "position": 3813
            },
            {
                "img": "https://arxiv.org/html/2601.01874/x20.png",
                "caption": "Table 17:Comparison of pre-trained models on FlowVerse.Results show that using FG-CLIP with a ViT-L-14 backbone yields the best performance.",
                "position": 3871
            },
            {
                "img": "https://arxiv.org/html/2601.01874/x20.png",
                "caption": "Figure 18:Effect of varyingα\\alphaon FlowVerse performance.",
                "position": 3907
            },
            {
                "img": "https://arxiv.org/html/2601.01874/x21.png",
                "caption": "Figure 19:An example of the solution generated byCogFlow.",
                "position": 3933
            },
            {
                "img": "https://arxiv.org/html/2601.01874/x22.png",
                "caption": "Figure 20:Example solution generated byCogFlowunder partially erroneous perception.",
                "position": 3936
            }
        ]
    },
    {
        "header": "Appendix DAdditional Analysis",
        "images": []
    }
]