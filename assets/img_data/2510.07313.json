[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07313/x1.png",
                "caption": "Figure 1:We presentWristWorld, a framework that synthesizes realistic wrist-view videos from anchor views through a two-stage process:\nareconstruction stagefor estimating wrist-view projections, and ageneration stagefor producing coherent wrist-view videos.\nThe generated wrist observations effectively expanding training data to novel view and lead to significant performance improvements for downstream VLA models across various tasks.",
                "position": 104
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07313/x2.png",
                "caption": "Figure 2:Overview of our method.We introduce a two-stage 4D Generative World Model. In the reconstruction stage, VGGTWang et al. (2025b)is extended with a wrist head to regress wrist pose, guided by a Spatial Projection Consistency Loss that supervises directly from RGB without depth or extrinsics. The predicted pose projects point clouds into the wrist view. In the generation stage, these projections, combined with external-view CLIP embeddings, condition a video generator to synthesize wrist-view sequences. Without first-frame guidance, the model produces additional wrist views for VLA datasets, yielding substantial performance gains.",
                "position": 177
            },
            {
                "img": "https://arxiv.org/html/2510.07313/x3.png",
                "caption": "Figure 3:Spatial Projection Consistency (SPC) loss.We first establish anchor–wrist 2D point matching and then lift the matched pixels to 2D–3D correspondences using the reconstructed point cloud.\nThe 3D points are subsequently projected into the wrist view with the predicted wrist pose, after which the SPC loss is computed to enforce geometric consistency.",
                "position": 304
            },
            {
                "img": "https://arxiv.org/html/2510.07313/x4.png",
                "caption": "Figure 4:Visualization of our generation result.As illustrated in the figure, we compare our generated condition maps against the 3D Base (VGGT without the SPC Loss), where our approach demonstrates superior viewpoint consistency. Furthermore, in comparison to the WoW 14B(Chi et al.,2025c)baseline which based on Wan 14B(Wan et al.,2025), our method achieves both higher generation quality and improved viewpoint alignment accuracy. These results highlight the effectiveness of our framework and underscore its potential to serve as training data for downstream VLA models.",
                "position": 394
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07313/x5.png",
                "caption": "Figure 5:Visualization on the Calvin(Mees et al.,2022)benchmark.We compare our generated wrist-view videos (bottom row) with the ground truth (second row) and a baseline method (third row, Stable Video Diffusion(Blattmann et al.,2023)).\nOur approach achieves better spatial and viewpoint consistency than the baseline,\nwhile also producing more faithful wrist-view frames.\nThese results highlight the effectiveness of our method in bridging anchor-view and wrist-view perspectives.",
                "position": 743
            },
            {
                "img": "https://arxiv.org/html/2510.07313/x6.png",
                "caption": "Figure 6:Visualization on Franka real-robot data.Multiple input anchor views (left) are used to generate wrist-view sequences (top right),\nwhich are compared with ground-truth wrist observations (bottom right).\nOur approach yields highly consistent predictions that closely match real data,\ndemonstrating strong generalization from third-view to wrist-view perspectives.",
                "position": 752
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07313/x7.png",
                "caption": "Figure 7:Qualitative visualization on Franka Panda.For each method, we generate videos and visualize themiddle frameof each sequence to probe long-horizon stability.Rowsdenote methods (the top row shows ground-truth wrist-camera frames), andcolumnsdenote different manipulation scenes from the Franka Panda setup.\nOur approach maintains superior geometric consistency (crisp boundaries, coherent occlusions, stable perspective/scale) and wrist-following behavior (viewpoint motion aligned with end-effector motion and consistent object-relative poses) compared with prior models.",
                "position": 1698
            },
            {
                "img": "https://arxiv.org/html/2510.07313/x8.png",
                "caption": "Figure 8:Experimental setup with the Franka Panda manipulator. The system is equipped with multiple Intel RealSense cameras: wrist-mounted, top, left, and right. This configuration enables multi-view visual input for robust perception and manipulation.",
                "position": 1711
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]