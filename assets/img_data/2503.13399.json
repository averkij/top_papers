[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13399/x1.png",
                "caption": "",
                "position": 190
            },
            {
                "img": "https://arxiv.org/html/2503.13399/x2.png",
                "caption": "Figure 2:MicroVQA taxonomy of sub-tasks.",
                "position": 348
            },
            {
                "img": "https://arxiv.org/html/2503.13399/x3.png",
                "caption": "Figure 3:Composition of scientific MLLM benchmarks regarding estimated Bloom’s taxonomy[11]. Higher levels are more cognitively challenging. MicroVQA has more questions at higher levels compared to other benchmarks, for example, MMMU[87]and ScienceQA[53], while perception-driven medical benchmarks like OmniMedVQA are at lower levels.",
                "position": 663
            },
            {
                "img": "https://arxiv.org/html/2503.13399/x4.png",
                "caption": "Figure 4:Constructing the MicroVQA multiple choice questions. (0) We defined tasks with domain biological scientists and created 1,061 raw VQA samples. (1) The raw samples were aligned to an exam-style MCQ by manually transforming a small set and optimizing an LLM prompt to match that alignment. (2) MCQs are further improved using RefineBot, a new iterative method to make MCQs more challenging. The lower panel shows an example MCQ from raw VQA to final: the annotations highlight key changes that we further explore inSec.E.2, whereredindicates issues, andgreenindicates good attributes.",
                "position": 699
            },
            {
                "img": "https://arxiv.org/html/2503.13399/x5.png",
                "caption": "Figure 5:Performance by sub-task and Bloom’s level for best models: Gemini-1.5-Pro (closed source), VILA1.5-40B (open-source), and LlaVA-Med-Mistral-7B (medical).",
                "position": 1086
            },
            {
                "img": "https://arxiv.org/html/2503.13399/extracted/6287634/figures/language-shortcuts.png",
                "caption": "Figure 10:Three types of language shortcut relevant to MicroVQA. The target VQA has an image that is fluorescence microscopy stained with TOMM20 which would show a pattern consistent with visualizing mitochondria.",
                "position": 3510
            },
            {
                "img": "https://arxiv.org/html/2503.13399/x6.png",
                "caption": "Figure 11:Examples of changes to questions and options between stage 1 and stage 2 (RefineBot) of our MCQ generation method. Inredelements that need to be improved and ingreenimprovements.",
                "position": 3539
            },
            {
                "img": "https://arxiv.org/html/2503.13399/x7.png",
                "caption": "Figure 12:Performance by image modality type for the best models: Gemini-1.5-Pro (closed source), VILA1.5-40B (open-source), and LlaVA-Med-Mistral-7B (medical).",
                "position": 3796
            },
            {
                "img": "https://arxiv.org/html/2503.13399/x8.png",
                "caption": "Figure 13:",
                "position": 3952
            },
            {
                "img": "https://arxiv.org/html/2503.13399/x9.png",
                "caption": "Figure 14:",
                "position": 3956
            },
            {
                "img": "https://arxiv.org/html/2503.13399/x10.png",
                "caption": "Figure 15:",
                "position": 3960
            },
            {
                "img": "https://arxiv.org/html/2503.13399/x11.png",
                "caption": "Figure 16:",
                "position": 3964
            },
            {
                "img": "https://arxiv.org/html/2503.13399/x12.png",
                "caption": "Figure 17:",
                "position": 3972
            },
            {
                "img": "https://arxiv.org/html/2503.13399/x13.png",
                "caption": "Figure 18:",
                "position": 3976
            },
            {
                "img": "https://arxiv.org/html/2503.13399/x14.png",
                "caption": "Figure 19:",
                "position": 3980
            },
            {
                "img": "https://arxiv.org/html/2503.13399/x15.png",
                "caption": "Figure 20:",
                "position": 3984
            },
            {
                "img": "https://arxiv.org/html/2503.13399/x16.png",
                "caption": "Figure 21:",
                "position": 3992
            },
            {
                "img": "https://arxiv.org/html/2503.13399/x17.png",
                "caption": "Figure 22:",
                "position": 3996
            },
            {
                "img": "https://arxiv.org/html/2503.13399/x18.png",
                "caption": "Figure 23:",
                "position": 4004
            },
            {
                "img": "https://arxiv.org/html/2503.13399/x19.png",
                "caption": "Figure 24:Reasoning trace categorization with an LLM (GPT-4o): Correct (blue) vs. incorrect (red) answers. Comparison of Claude-Sonnet’s responses without (left) and with (right) image access.",
                "position": 4415
            },
            {
                "img": "https://arxiv.org/html/2503.13399/extracted/6287634/figures/collage_1.jpg",
                "caption": "Figure 25:Collage of images from MicroVQA.",
                "position": 4427
            }
        ]
    },
    {
        "header": "",
        "images": []
    }
]