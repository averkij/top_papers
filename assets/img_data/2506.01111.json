[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01111/x1.png",
                "caption": "Figure 1:Human auditory perception integrates multisensory cues.",
                "position": 185
            },
            {
                "img": "https://arxiv.org/html/2506.01111/x2.png",
                "caption": "Table 1:Comparison of generated captions for a sample audio clip with associated visual context. Hallucinations in prior work are highlighted inred. Improvements from our multimodal approach, FusionAudio, are highlighted ingreen, demonstrating enhanced accuracy and detail by leveraging visual and comprehensive auditory cues.",
                "position": 191
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method: Fine-grained Audio Caption with Multimodal Contextual Fusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01111/x3.png",
                "caption": "Figure 2:Overview of our proposed multimodal audio captioning pipeline. The process involves initial vocal separation, followed by a two-stage approach: multimodal contextual cue extraction and LLM-driven contextual synthesis.",
                "position": 405
            }
        ]
    },
    {
        "header": "4The Resulted Dataset: FusionAudio-1.2M",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01111/x4.png",
                "caption": "(a)Top 5 Audio Labels Distribution",
                "position": 527
            },
            {
                "img": "https://arxiv.org/html/2506.01111/x4.png",
                "caption": "(a)Top 5 Audio Labels Distribution",
                "position": 535
            },
            {
                "img": "https://arxiv.org/html/2506.01111/x5.png",
                "caption": "(b)Caption Length Distribution",
                "position": 541
            },
            {
                "img": "https://arxiv.org/html/2506.01111/x5.png",
                "caption": "(b)Caption Length Distribution",
                "position": 544
            },
            {
                "img": "https://arxiv.org/html/2506.01111/x6.png",
                "caption": "(c)Diversity of Object Types",
                "position": 549
            },
            {
                "img": "https://arxiv.org/html/2506.01111/x7.png",
                "caption": "(d)CLAP Score Distribution",
                "position": 555
            },
            {
                "img": "https://arxiv.org/html/2506.01111/x8.png",
                "caption": "(e)Modal Usage Frequency",
                "position": 560
            },
            {
                "img": "https://arxiv.org/html/2506.01111/x9.png",
                "caption": "(a)FusionAudio",
                "position": 612
            },
            {
                "img": "https://arxiv.org/html/2506.01111/x9.png",
                "caption": "(a)FusionAudio",
                "position": 615
            },
            {
                "img": "https://arxiv.org/html/2506.01111/x10.png",
                "caption": "(b)AudioSetCaps",
                "position": 620
            },
            {
                "img": "https://arxiv.org/html/2506.01111/x11.png",
                "caption": "(c)Auto-ACD",
                "position": 625
            },
            {
                "img": "https://arxiv.org/html/2506.01111/x12.png",
                "caption": "(d)Sound-VECaps",
                "position": 630
            },
            {
                "img": "https://arxiv.org/html/2506.01111/x13.png",
                "caption": "",
                "position": 641
            }
        ]
    },
    {
        "header": "5Applications of FusionAudio-1.2M",
        "images": []
    },
    {
        "header": "6Ablation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01111/x14.png",
                "caption": "(a)Audio Understanding",
                "position": 1293
            },
            {
                "img": "https://arxiv.org/html/2506.01111/x14.png",
                "caption": "(a)Audio Understanding",
                "position": 1296
            },
            {
                "img": "https://arxiv.org/html/2506.01111/x15.png",
                "caption": "(b)Audio-text Retrieval",
                "position": 1301
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Ackownledgement",
        "images": []
    },
    {
        "header": "Limitation",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AHuman Evaluation",
        "images": []
    },
    {
        "header": "Introduction",
        "images": []
    },
    {
        "header": "1. Detailing",
        "images": []
    },
    {
        "header": "Key Things to Look For:",
        "images": []
    },
    {
        "header": "Scoring Guidelines:",
        "images": []
    },
    {
        "header": "2. Hallucinations",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01111/x16.png",
                "caption": "Figure 7:Detailness and Hallucination Rates Distribution of Human Rating",
                "position": 1992
            },
            {
                "img": "https://arxiv.org/html/2506.01111/x16.png",
                "caption": "",
                "position": 1995
            },
            {
                "img": "https://arxiv.org/html/2506.01111/x17.png",
                "caption": "",
                "position": 2000
            }
        ]
    },
    {
        "header": "Appendix BPrompt for models",
        "images": []
    },
    {
        "header": "Appendix CDataset Samples",
        "images": []
    },
    {
        "header": "Appendix DMore on Dataset Statistics",
        "images": []
    }
]