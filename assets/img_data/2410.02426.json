[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02426/extracted/5898626/overview.png",
                "caption": "Figure 1:Illustration of our proposed task: prediction of a white chess piece move (red arrow) given a board state in standard algebraic chess notation (SAN). The chessboard has standard algebraic notation ranks and files along the board edges.",
                "position": 100
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02426/extracted/5898626/queensgambit.png",
                "caption": "Figure 2:Illustration of the initial array (left board diagram) and the standard algebraic chess notation (SAN) for the \"Queen‚Äôs Gambit Declined,\" a popular opening move sequence in the 1920‚Äôs and 1930‚Äôs (Encyclopedia of Chess Openings, sequences D30-42). The moves, in SAN, are as follows: 1. d4 d5, 2. c4 e6. The outcome of the two move sequences is shown on the right board diagram.",
                "position": 179
            }
        ]
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02426/extracted/5898626/scalinglegal.png",
                "caption": "Figure 3:Influence of increasing instruction fine-tuning examples. Percentage of legal proposed moves\nversus count of the instruction fine-tuning examples for the TinyStories-28M (blue) and OPT-125M (orange) language models, instruction fine-tuned with learning rate = 2e-4, batch size = 4, and epochs = 3. The performance of the instruction fine-tuned language models was evaluated using 10,000 test instances of chess board states drawn from WSM-10M to assess the model‚Äôs ability to generate a legal proposed move.",
                "position": 380
            },
            {
                "img": "https://arxiv.org/html/2410.02426/extracted/5898626/puzzleslegal.png",
                "caption": "Figure 4:Percentage of legal proposed moves versus count of the instruction fine-tuning examples for the TinyStories-28M (blue) and OPT-125M (orange) language models, instruction fine-tuned with learning rate = 2e-4, batch size = 4, and epochs = 3. The performance of each instruction fine-tuned language model was evaluated using 1,000 test instances of chess problems drawn from Check/Mate-in-1 to assess the model‚Äôs ability to generate a legal proposed move.",
                "position": 464
            },
            {
                "img": "https://arxiv.org/html/2410.02426/extracted/5898626/puzzlesmate.png",
                "caption": "Figure 5:Percentage of proposed moves which were legal and resulted in check or checkmate versus count of the instruction fine-tuning examples for the TinyStories-28M (blue) and OPT-125M (orange) language models, instruction fine-tuned with learning rate = 2e-4, batch size = 4, and epochs = 3. The performance of each instruction fine-tuned language model was evaluated using 1,000 test instances of chess problems drawn from Check/Mate-in-1 to assess the model‚Äôs ability to generate a legal move that resulted in check or checkmate.",
                "position": 471
            },
            {
                "img": "https://arxiv.org/html/2410.02426/extracted/5898626/puzzleshallucinate.png",
                "caption": "Figure 6:Percentage of proposed moves which were either illegal (orange), or the proposed piece was not on the board (blue), to achieve either check or checkmate versus count of the instruction fine-tuning examples for the OPT-125M language model, instruction fine-tuned with learning rate = 2e-4, batch size = 4, and epochs = 3. The performance of each instruction fine-tuned language model was evaluated using 1,000 test instances of chess problems drawn from Check/Mate-in-1 to assess the model‚Äôs propensity to generate an illegal move that resulted in check or checkmate.",
                "position": 490
            },
            {
                "img": "https://arxiv.org/html/2410.02426/extracted/5898626/puzzlesepochs.png",
                "caption": "Figure 7:Percentage of proposed moves which were legal (orange) and resulted in check or checkmate (blue) versus count of the instruction fine-tuning epochs for the OPT-125M language model, instruction fine-tuned with 100,000 examples drawn from the WSM-10M dataset, learning rate = 2e-4, and batch size = 4. The performance of each instruction fine-tuned language model was evaluated using 1,000 test instances of chess problems drawn from Check/Mate-in-1 to assess the model‚Äôs ability to generate a legal move that resulted in check or checkmate.",
                "position": 605
            },
            {
                "img": "https://arxiv.org/html/2410.02426/extracted/5898626/tempvslegal.png",
                "caption": "Figure 8:Percentage of proposed moves which were legal (orange) and resulted in check or checkmate (blue) versus theGenerationConfig()temperature parameter value when required to generate a legal move based on the board state. The model was allowed up to 100 iterations to achieve this requirement before proceeding to the next example in the test dataset. All instances used an instruction fine-tuned OPT-125M language model, instruction fine-tuned with 1,000,000 examples drawn from the WSM-10M dataset, learning rate = 2e-4, and batch size = 4. The performance of each instruction fine-tuned language model was evaluated using 1,000 test instances of chess problems drawn from Check/Mate-in-1 to assess the model‚Äôs ability to generate a legal move that resulted in check or checkmate.",
                "position": 630
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02426/extracted/5898626/top10whitepiecemoves.png",
                "caption": "Figure A1:Top 10 most prevalent white piece single-moves. Moves are shown by their standard algebraic chess notations (SAN). Thexùë•xitalic_x-axis of the plot displays the total single-move count for each move, with data labels indicating their percentage of the overall total.",
                "position": 1519
            },
            {
                "img": "https://arxiv.org/html/2410.02426/extracted/5898626/top10boards.png",
                "caption": "Figure A2:Top 10 most prevalent chess boards preceding white piece single-moves (i.e., next move = white). Common move openings and sequences are described by their Encyclopedia of Chess Openings (ECO) traditional names. Thexùë•xitalic_x-axis of the plot displays the total board count for each board status, with data labels indicating their percentage of the overall total.",
                "position": 1529
            },
            {
                "img": "https://arxiv.org/html/2410.02426/extracted/5898626/diagramstop8boards.png",
                "caption": "Figure A3:Top 8 (excluding the initial array) most common board configurations in the white piece single-move dataset where white moves next. Their associated Encyclopedia of Chess Openings (ECO) traditional names and codes, and paired-move sequences in SAN, are included.",
                "position": 1532
            },
            {
                "img": "https://arxiv.org/html/2410.02426/extracted/5898626/sourcesbyplayers.png",
                "caption": "Figure A4:Notable chess players in the dataset were identified by the main player linked to each book or data source, covering all games/setups and the associated 20 million single piece moves. Thexùë•xitalic_x-axis of the plot displays the single-move count for each player, with data labels indicating their percentage of the overall total.",
                "position": 1536
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]