[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Theoretical Analysis",
        "images": []
    },
    {
        "header": "4Findings",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01863/x1.png",
                "caption": "Figure 1:Comparison ofρ𝜌\\rhoitalic_ρfits for scaling law forms from Table1:(a, left)shows quantizations scaling laws,(b, middle)and(c, right)demonstrate the match between noise injection and QuEST quantization for weight-only and weights+activations quantization.",
                "position": 468
            },
            {
                "img": "https://arxiv.org/html/2506.01863/x2.png",
                "caption": "(a)",
                "position": 569
            },
            {
                "img": "https://arxiv.org/html/2506.01863/x2.png",
                "caption": "(a)",
                "position": 572
            },
            {
                "img": "https://arxiv.org/html/2506.01863/x3.png",
                "caption": "(b)",
                "position": 577
            }
        ]
    },
    {
        "header": "5Applications",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01863/x4.png",
                "caption": "Figure 3:Comparison of floating point and integer data-types in terms of𝐺𝑀𝑆𝐸𝐺𝑀𝑆𝐸\\mathord{\\it GMSE}italic_GMSE, and C4 Validation Loss when trained using the corresponding formats via QuEST, and the resulting capacityρ⁢(R)𝜌𝑅\\rho(R)italic_ρ ( italic_R ). Observe the high correlation between ranking in terms of𝐺𝑀𝑆𝐸𝐺𝑀𝑆𝐸\\mathord{\\it GMSE}italic_GMSE(top), and Val. Loss (bottom).",
                "position": 615
            },
            {
                "img": "https://arxiv.org/html/2506.01863/x5.png",
                "caption": "Figure 4:Representation capacityρ⁢(R)𝜌𝑅\\rho(R)italic_ρ ( italic_R )versus MSE for(a)group-wise quantization, with markers indicate group counts (color encodes quantization bitwidth), and(b)outlier-aware quantization.",
                "position": 618
            },
            {
                "img": "https://arxiv.org/html/2506.01863/x5.png",
                "caption": "",
                "position": 621
            },
            {
                "img": "https://arxiv.org/html/2506.01863/x6.png",
                "caption": "",
                "position": 625
            },
            {
                "img": "https://arxiv.org/html/2506.01863/extracted/6505206/plots/backward-heuristics/FW-Tk.png",
                "caption": "(a)",
                "position": 664
            },
            {
                "img": "https://arxiv.org/html/2506.01863/extracted/6505206/plots/backward-heuristics/FW-Tk.png",
                "caption": "(a)",
                "position": 667
            },
            {
                "img": "https://arxiv.org/html/2506.01863/extracted/6505206/plots/backward-heuristics/RMS-Tp.png",
                "caption": "(b)",
                "position": 673
            },
            {
                "img": "https://arxiv.org/html/2506.01863/extracted/6505206/plots/backward-heuristics/RMS-Tp-Tk.png",
                "caption": "(c)",
                "position": 679
            },
            {
                "img": "https://arxiv.org/html/2506.01863/extracted/6505206/plots/backward-heuristics/efficiency-baseline-vs-heuristics.png",
                "caption": "(d)",
                "position": 685
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Discussion and Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix Roadmap",
        "images": []
    },
    {
        "header": "Appendix AExperimental setup",
        "images": []
    },
    {
        "header": "Appendix BFactorization of Representation Capacity",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01863/x7.png",
                "caption": "Figure 6:Representation capacity coefficients for independent quantization of weights and activations. Element-wiseρ𝜌\\rhoitalic_ρfitting error is not greater than5⋅10−3⋅5superscript1035\\cdot 10^{-3}5 ⋅ 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT.",
                "position": 1341
            },
            {
                "img": "https://arxiv.org/html/2506.01863/x8.png",
                "caption": "Figure 7:Representation capacity coefficients with fit errors in case of sparsity combined with the QuEST quantization.",
                "position": 1344
            },
            {
                "img": "https://arxiv.org/html/2506.01863/x9.png",
                "caption": "Figure 8:Representation capacity fit errors for sparse+quantized weights and quantized activations. Error bars denote±1plus-or-minus1\\pm 1± 1standard deviation from the mean.",
                "position": 1347
            },
            {
                "img": "https://arxiv.org/html/2506.01863/x10.png",
                "caption": "Figure 9:Representation capacity coefficients matrix for sparsity applied with uniform quantization. Element-wiseρ𝜌\\rhoitalic_ρfitting error is not greater than2⋅10−3⋅2superscript1032\\cdot 10^{-3}2 ⋅ 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT.",
                "position": 1350
            }
        ]
    },
    {
        "header": "Appendix CAblation studies on Law Formulation",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01863/x11.png",
                "caption": "(a)Effect of input noise distribution on the mappingρ⁢(M⁢S⁢E)𝜌𝑀𝑆𝐸\\rho(MSE)italic_ρ ( italic_M italic_S italic_E ).",
                "position": 1369
            },
            {
                "img": "https://arxiv.org/html/2506.01863/x11.png",
                "caption": "(a)Effect of input noise distribution on the mappingρ⁢(M⁢S⁢E)𝜌𝑀𝑆𝐸\\rho(MSE)italic_ρ ( italic_M italic_S italic_E ).",
                "position": 1372
            },
            {
                "img": "https://arxiv.org/html/2506.01863/x12.png",
                "caption": "(b)L⁢(M⁢S⁢E)𝐿𝑀𝑆𝐸L(MSE)italic_L ( italic_M italic_S italic_E )fit for the best functional fit,t⁢a⁢n⁢h𝑡𝑎𝑛ℎtanhitalic_t italic_a italic_n italic_h.",
                "position": 1377
            }
        ]
    },
    {
        "header": "Appendix DScaling Laws for Vector Quantization",
        "images": []
    },
    {
        "header": "Appendix ETheoretical Support",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01863/extracted/6505206/plots/backward-heuristics/rigl-gmp-rbbm.png",
                "caption": "(a)Pre-training Llama-30M with different sparsities using our MP baseline, RBBM heuristic and RigL variations.",
                "position": 2591
            },
            {
                "img": "https://arxiv.org/html/2506.01863/extracted/6505206/plots/backward-heuristics/rigl-gmp-rbbm.png",
                "caption": "(a)Pre-training Llama-30M with different sparsities using our MP baseline, RBBM heuristic and RigL variations.",
                "position": 2594
            },
            {
                "img": "https://arxiv.org/html/2506.01863/extracted/6505206/plots/backward-heuristics/gmp-schedule-rbbm.png",
                "caption": "(b)Pre-training Llama-30M with different sparsities using our constant MP (CMP) baseline, GMP and RBBM heuristic with GMP schedule.",
                "position": 2599
            }
        ]
    },
    {
        "header": "Appendix FImproved Sparse Training via RBBM",
        "images": []
    }
]