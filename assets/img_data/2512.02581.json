[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02581/x1.png",
                "caption": "Figure 1:Illustration of the ‚Äúmode-covering‚Äù problem:\nwhen the optimal policy (blue) is multimodal, a unimodal policy (red) assigns high probability to the low-reward region between modes, leading to suboptimal behavior.",
                "position": 239
            }
        ]
    },
    {
        "header": "2Background and Preliminaries",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02581/x2.png",
                "caption": "Figure 2:Overview of theGoRLframework.(a) Latent optimization:The decodergœïg_{\\phi}is frozen while the encoderœÄŒ∏\\pi_{\\theta}is optimized in the latent space using standard policy gradients,\nwith a KL penalty towardùí©‚Äã(0,I)\\mathcal{N}(0,I).(b) Decoder refinement:The encoder is frozen and the decodergœïg_{\\phi}is updated via supervised learning on recent rollouts, mapping the fixed\nGaussian prior overŒµ\\varepsilonto actions using an expressive generative\nloss (e.g., flow matching).",
                "position": 456
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02581/x3.png",
                "caption": "Figure 3:Visual overview of the DMControl tasks.The benchmark covers diverse control challenges:\nhigh-speed locomotion (CheetahRun),\nbipedal gait control (WalkerWalk),\nobject manipulation (FingerSpin,FingerTurnHard),\nand fine-grained stabilization with complex contacts (HopperStand,FishSwim).",
                "position": 846
            },
            {
                "img": "https://arxiv.org/html/2512.02581/x4.png",
                "caption": "Figure 4:Learning curves across six DMControl tasks.\nCurves are smoothed using Gaussian filtering (œÉ=100.0\\sigma=100.0) for visual clarity.\nShaded regions denote standard deviation across five seeds.",
                "position": 902
            },
            {
                "img": "https://arxiv.org/html/2512.02581/x5.png",
                "caption": "Figure 5:Ablation of latent regularization onCheetahRun. Varying the KL coefficientŒ≤\\betasignificantly affects stability.",
                "position": 995
            },
            {
                "img": "https://arxiv.org/html/2512.02581/x6.png",
                "caption": "Figure 6:Impact of staged decoder refinement.Performance improves systematically as the decoder evolves from an identity mapping (Stage 0) to a generative map.\nResults are shown forCheetahRun(left) andHopperStand(right).\nOnCheetahRun, gains saturate by Stage 3, indicating that the alternating schedule converges efficiently to a high-capacity policy.",
                "position": 1019
            },
            {
                "img": "https://arxiv.org/html/2512.02581/x7.png",
                "caption": "Figure 7:Evolution of action distributions onHopperStand.Gaussian KDE plots of policy outputs at a fixed state.\nInitially (60M),GoRLbehaves similarly to the unimodal Gaussian baseline, ensuring stability.\nBy 180M,GoRLevolves a clearbimodal structureto capture distinct high-reward strategies, whereas baselines (PPO, FPO) remain restricted to unimodal or unstructured distributions.",
                "position": 1042
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix AWhy Generative Policies Are Hard to Update with Standard Policy Gradients",
        "images": []
    },
    {
        "header": "Appendix BReinforcement Learning Background",
        "images": []
    },
    {
        "header": "Appendix CGenerative Policy Details",
        "images": []
    },
    {
        "header": "Appendix DProofs of Theoretical Guarantees",
        "images": []
    },
    {
        "header": "Appendix ETraining and Implementation Details",
        "images": []
    }
]