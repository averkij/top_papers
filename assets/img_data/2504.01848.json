[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01848/x1.png",
                "caption": "Figure 1:PaperBench is a benchmark for evaluating AI agents’ abilities to replicate AI research. Each sample includes a research paper and a grading rubric that specifies the assessment criteria for a complete replication. Agents create a codebase from scratch as their submission (1), which is then executed to verify result reproduction (2) and graded against the rubric by an LLM-based judge (3).",
                "position": 218
            }
        ]
    },
    {
        "header": "2PaperBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01848/x2.png",
                "caption": "Figure 2:Rubrics hierarchically decompose the replication task into a tree of increasingly granular requirements. Leaf nodes are graded for binary pass/fail criteria, and a parent’s score is the weighted average of its children. In the example above, the final Replication Score is 55%.",
                "position": 301
            }
        ]
    },
    {
        "header": "3Dataset",
        "images": []
    },
    {
        "header": "4LLM Judge",
        "images": []
    },
    {
        "header": "5Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01848/",
                "caption": "Figure 3:Comparing human versus agent performance on a 4-paper subset of PaperBench. o1 initially outperforms the human baseline but plateaus after the first hour, leading it to fall behind the humans by the end. Note that the human attempt fortest-time-model-adaptationends at the 24 hour mark and is thus excluded from the ‘3-paper subset’ discussed elsewhere in the paper. Error bars on model performance is SEM over 3 repeats.",
                "position": 1093
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AFuture Directions in AI Evaluation",
        "images": []
    },
    {
        "header": "Appendix BPaper Selection Process",
        "images": []
    },
    {
        "header": "Appendix CRubric and Addendum Creation Process",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01848/extracted/6328448/assets/rubric_excerpt_json.png",
                "caption": "Figure 4:An excerpt of the rubric for one of the papers in PaperBench. Shown in the underlying JSON (left) and in a GUI (right).",
                "position": 2168
            },
            {
                "img": "https://arxiv.org/html/2504.01848/extracted/6328448/assets/rubric_excerpt_json.png",
                "caption": "",
                "position": 2171
            },
            {
                "img": "https://arxiv.org/html/2504.01848/extracted/6328448/assets/rubric_excerpt_gui.png",
                "caption": "",
                "position": 2176
            }
        ]
    },
    {
        "header": "Appendix DSimpleJudge Implementation",
        "images": []
    },
    {
        "header": "Appendix EMonitor Implementation",
        "images": []
    },
    {
        "header": "Appendix FAgent Implementation",
        "images": []
    },
    {
        "header": "Appendix GMore on JudgeEval",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01848/x4.png",
                "caption": "Figure 5:Performance on JudgeEval vs average cost per paper in JudgeEval for various model backends in SimpleJudge. Model cost measured in terms of input+output tokens multiplied by their respective cost-per-token on the OpenAI API. Human cost estimated at 12 hours of work at an hourly rate of $100 USD/hr. Reasoning models are run with with reasoning effort set to “high”.",
                "position": 2305
            }
        ]
    },
    {
        "header": "Appendix HPruned Rubric Grading",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01848/x5.png",
                "caption": "Figure 6:The replication score assigned by o3-mini SimpleJudge to the ‘rice/0’ submission in JudgeEval, over different depths of pruning. Pruning at depth 100 is equivalent to not pruning for this paper. We plot the ground truth human Judge grade in red. Error bars are standard error of the mean over 3 repeats.",
                "position": 2401
            }
        ]
    },
    {
        "header": "Appendix IFull Results",
        "images": []
    }
]