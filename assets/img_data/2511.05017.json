[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.05017/x1.png",
                "caption": "Figure 1:Hallucinations in Video-LLaVA(lin2023video).",
                "position": 112
            },
            {
                "img": "https://arxiv.org/html/2511.05017/x2.png",
                "caption": "Figure 2:Top:Architecture of typical LVLMs like Video-LLaVA, which fuse language and vision embeddings by simple concatenation.Bottom:Our modified architecture with aconcatenation block that appends the averaged vision embedding to each token embedding, followed by a projection layer. This encourages the model to learn visually informed textual embeddings and better attend to visual input during training.",
                "position": 128
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Background",
        "images": []
    },
    {
        "header": "4Evaluating the Attention Score Distribution",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.05017/x3.png",
                "caption": "Figure 3:Attention score distributions across the first six attention layers of the baseline Video-LLaVA model (top row) and the VisAlign-enhanced model (bottom row). Video-LLaVA concatenates tokens in a fixed order: 35 initial text tokens, followed by 256 visual embeddings, and then the remaining text tokens. In each map, the x-axis denotes attended tokens (keys), and the y-axis denotes attending tokens (queries). Color intensity reflects attention weight:blueindicates low attention,red/white indicates high attention, and dark (near-black) regions indicate masked or negligible attention due to causal masking in autoregressive LVLMs.",
                "position": 198
            }
        ]
    },
    {
        "header": "5Improving Attention Score Distribution by Refining Textual Embeddings",
        "images": []
    },
    {
        "header": "6Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.05017/x4.png",
                "caption": "Figure 4:Qualitative results from theMMVP-MLLM Benchmark:\nBelow each image, the baseline modelâ€™s response is shown first, followed by the response from the model trained withVisAlign.",
                "position": 320
            },
            {
                "img": "https://arxiv.org/html/2511.05017/x5.png",
                "caption": "Figure 5:Qualitative examples fromPOPE A-OKVQA, HallusionBench, MMVP, andMementosbenchmarks illustrating various hallucination types. Input prompts are shown in orange, baseline Video-LLaVA outputs in yellow, and VisAlign-enhanced outputs in green. VisAlign consistently improves performance across object, action, attribute, and relation hallucinations.",
                "position": 578
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Limitations and Future work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.05017/x6.png",
                "caption": "Figure 6:Qualitative results on the Mementos benchmark(wang2024mementos). Text highlighted in red indicates hallucinated content, while text in blue shows the corresponding corrections.",
                "position": 783
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]