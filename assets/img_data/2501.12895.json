[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12895/x1.png",
                "caption": "Figure 1:Training-time preference optimization (e.g., RLHF and DPO) compared with test-time preference optimization (TPO), where the model aligns with human preferences during test-time with the model parameters fixed.",
                "position": 152
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12895/x2.png",
                "caption": "Figure 2:Framework of test-time preference optimization (TPO), shown here on a real example from AlpacaEval 2.\nConcretely, the model samples responses and scores them with a reward model (Left), interprets reward model feedback of chosen responsev3subscriptùë£3v_{3}italic_v start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPTand rejected responsev1subscriptùë£1v_{1}italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT(Middle),\nand provides critiques, generates improvement suggestions (Right),\nand then updates new responses for the next iteration.\nIn analogy to traditional gradient-based methods, TPO performs gradient descent (loss calculation, gradient computation and variable optimization) intextualform to tailor model outputs based on numerical feedback from the reward model.",
                "position": 202
            }
        ]
    },
    {
        "header": "3Preliminary",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12895/x3.png",
                "caption": "Figure 3:Test-time training curve for the (unaligned) SFT model and (aligned) instruct models.\nThe colored lines represent the test-time training performance (reward model score) w.r.t. training steps (i.e., number of TPO iterations), while the dashed horizontal lines indicate scores for models without test-time training.",
                "position": 486
            }
        ]
    },
    {
        "header": "5Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12895/x4.png",
                "caption": "Table 1:Benchmark performance of the unaligned model (Llama-3.1-70B-SFT) with TPO, compared against training-time aligned baselines (Llama-3.1-70B-DPO and Llama-3.1-70B-Instruct).\nTheboldandunderlinednumbers indicate the best and second-best performances, respectively.\nBy default, the maximum number of iterationsDùê∑Ditalic_Dis set to 2, and the number of samplesNùëÅNitalic_Nis set to 5.\nTo showcase the potential of TPO, we present an ultra setting, in which the number of iterations is increased to 5 and the number of samples to 20.‚ãÜ‚ãÜ\\star‚ãÜdenotes the models optimized with TPO using the reward modelFsfairX-LLaMA3-RM-v0.1, while‚Ä†‚Ä†\\dagger‚Ä†denotesLlama-3.1-Tulu-3-8B-RM.",
                "position": 537
            }
        ]
    },
    {
        "header": "6Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12895/x4.png",
                "caption": "Figure 4:Inference stability of models with and without TPO.",
                "position": 758
            }
        ]
    },
    {
        "header": "7Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12895/x5.png",
                "caption": "Figure 5:Test-time training curves on the HH-RLHF dataset, evaluated under varying sampling widths (i.e., the number of responses sampled per iteration).",
                "position": 806
            },
            {
                "img": "https://arxiv.org/html/2501.12895/x6.png",
                "caption": "Figure 6:Win-rates of TPO against Best-of-N sampling (BoN).",
                "position": 816
            },
            {
                "img": "https://arxiv.org/html/2501.12895/x7.png",
                "caption": "Figure 7:Test-time training curve of Llama-3.1-8B-Instruct (red line) on the HH-RLHF dataset.",
                "position": 847
            }
        ]
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APrompt Design",
        "images": []
    },
    {
        "header": "Appendix BData Statistics",
        "images": []
    },
    {
        "header": "Appendix CTest-time Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12895/x8.png",
                "caption": "Figure 8:Test-time training curves of the Llama-3.1-70B-SFT usingFsfairX-LLaMA3-RM-v0.1.",
                "position": 1543
            },
            {
                "img": "https://arxiv.org/html/2501.12895/x9.png",
                "caption": "Figure 9:Test-time training curves of the Llama-3.1-70B-SFT usingLlama-3.1-Tulu-3-8B-RM.",
                "position": 1546
            },
            {
                "img": "https://arxiv.org/html/2501.12895/x10.png",
                "caption": "Figure 10:Test-time training curves of the aligned models: Llama-3.1-70B-Instruct and Mistral-Small-Instruct-2409 (Reward model:FsfairX-LLaMA3-RM-v0.1).",
                "position": 1549
            }
        ]
    },
    {
        "header": "Appendix DCase Study",
        "images": []
    }
]