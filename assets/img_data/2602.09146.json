[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09146/x1.png",
                "caption": "",
                "position": 75
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09146/images/single_frames/kinetics/dunking_basketball.jpg",
                "caption": "Figure 2:Current benchmarks are appearance-centric.We show random frames from popular video-retrieval datasets. In many cases, static objects (e.g., a cello, a razor) or scene context (e.g., a basketball court) suffice to identify the action label (e.g., Playing Cello, Shaving Beard) without observing motion. This bias enables high accuracy from purely appearance-based cues, discouraging models from learning true temporal dynamics.",
                "position": 151
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/single_frames/ucf101/playing_cello.jpg",
                "caption": "",
                "position": 161
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/single_frames/hmdb51/ride_horse.jpg",
                "caption": "",
                "position": 162
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/single_frames/kinetics/juggling_balls.jpg",
                "caption": "",
                "position": 170
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/single_frames/ucf101/shaving_beard.jpg",
                "caption": "",
                "position": 171
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/single_frames/hmdb51/brush_hair.jpg",
                "caption": "",
                "position": 172
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/synthetic/static/generated.png",
                "caption": "Figure 3:Controlled variation in SimMotion-Synthetic.We visualize sample pairs from the five distinct categories in our benchmark. From left to right:Static Object(background varies),Dynamic Appearance(subject clothing/attributes vary),Dynamic Object(subject identity varies),View(camera angle varies), andScene Style(rendering style varies). In each column, the top and bottom videos are temporally synchronized and share identical motion dynamics, differing only in the specified visual factor.",
                "position": 192
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/synthetic/dyn_attr/generated.png",
                "caption": "",
                "position": 207
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/synthetic/dyn_obj/generated.png",
                "caption": "",
                "position": 208
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/synthetic/view/generated.png",
                "caption": "",
                "position": 209
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/synthetic/style/generated.png",
                "caption": "",
                "position": 210
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/synthetic/static/edited.png",
                "caption": "",
                "position": 213
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/synthetic/dyn_attr/edited.png",
                "caption": "",
                "position": 214
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/synthetic/dyn_obj/edited.png",
                "caption": "",
                "position": 215
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/synthetic/view/edited.png",
                "caption": "",
                "position": 216
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/synthetic/style/edited.png",
                "caption": "",
                "position": 217
            }
        ]
    },
    {
        "header": "3The “SimMotion” benchmarks",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09146/images/walks/video1_base.jpg",
                "caption": "Figure 4:Motion-focused similarity with moment statistics.(a) Appearance-altered edits preserve the same underlying motion for each motion groupmim_{i}, while changing visual style.\n(b) Baseline embeddings yield similarity heatmaps that are sensitive to appearance rather than motion.\n(c) Our moment-based embedding (using the first three moments over patch features) produces clearer motion-consistent clusters (corresponding to shared motionmim_{i}) than global mean pooling. Brighter cells indicate higher cosine similarity.",
                "position": 237
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/walks/video1_pixar.jpg",
                "caption": "",
                "position": 253
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/walks/video1_paintbrush.jpg",
                "caption": "",
                "position": 254
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/walks/video2_base.jpg",
                "caption": "",
                "position": 256
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/walks/video2_pixar.jpg",
                "caption": "",
                "position": 257
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/walks/video2_paintbrush.jpg",
                "caption": "",
                "position": 258
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/walks/video3_base.jpg",
                "caption": "",
                "position": 260
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/walks/video3_pixar.jpg",
                "caption": "",
                "position": 261
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/walks/video3_paintbrush.jpg",
                "caption": "",
                "position": 262
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/walks/video4_base.jpg",
                "caption": "",
                "position": 264
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/walks/video4_pixar.jpg",
                "caption": "",
                "position": 265
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/walks/video4_paintbrush.jpg",
                "caption": "",
                "position": 266
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/heatmaps/x_clip.png",
                "caption": "",
                "position": 289
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/heatmaps/slow_fast.png",
                "caption": "",
                "position": 290
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/heatmaps/videoMoco_mean.png",
                "caption": "",
                "position": 291
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/heatmaps/MaCLR_mean.png",
                "caption": "",
                "position": 292
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/heatmaps/dino_mean.png",
                "caption": "",
                "position": 309
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/heatmaps/videoprism_mean.png",
                "caption": "",
                "position": 310
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/heatmaps/video_mae_fintuned.png",
                "caption": "",
                "position": 311
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/heatmaps/dino_3rd_moment.png",
                "caption": "",
                "position": 317
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/heatmaps/videoprism_3rd_moment.png",
                "caption": "",
                "position": 318
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/heatmaps/videomae_3rd_moment.png",
                "caption": "",
                "position": 319
            },
            {
                "img": "https://arxiv.org/html/2602.09146/x2.png",
                "caption": "Figure 5:SemanticMoments pipeline.Patch-wise features are extracted per frame using a pretrained embedder (e.g., DINO) and summarized over time using the first three temporal moments (mean, variance, and skewness). Spatial aggregation yields one descriptor per moment, which are combined into a global motion-centric video embedding.",
                "position": 343
            }
        ]
    },
    {
        "header": "4Analysis",
        "images": []
    },
    {
        "header": "5SemanticMoments",
        "images": []
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09146/images/teaser/dog_yoga/1_ref/frame_1.jpg",
                "caption": "Figure 6:Motion vs. Appearance Bias.Left:the dominant motion is a dog walking; while VideoPrism retrieves a static yoga pose (bottom row) based on the background or inferred label ”woman doing yoga”, our Semantic Moments (middle row) successfully retrieves a dog walking despite the different background.Right:although the motion is opening a door, VideoMAE retrieves a video matching the “ambulance” context. In contrast, our method aligns with the underlying dynamics, ignoring static appearance or coarse semantics.",
                "position": 521
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/teaser/dog_yoga/1_ref/frame_3.jpg",
                "caption": "",
                "position": 530
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/teaser/dog_yoga/1_ref/frame_4.jpg",
                "caption": "",
                "position": 531
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/teaser/ambulance_open_door/1_ref/frame_1.jpg",
                "caption": "",
                "position": 532
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/teaser/ambulance_open_door/1_ref/frame_2.jpg",
                "caption": "",
                "position": 533
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/teaser/ambulance_open_door/1_ref/frame_3.jpg",
                "caption": "",
                "position": 534
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/teaser/dog_yoga/2_positive/frame_1.jpg",
                "caption": "",
                "position": 541
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/teaser/dog_yoga/2_positive/frame_2.jpg",
                "caption": "",
                "position": 542
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/teaser/dog_yoga/2_positive/frame_3.jpg",
                "caption": "",
                "position": 543
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/teaser/ambulance_open_door/2_positive/frame_1.jpg",
                "caption": "",
                "position": 544
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/teaser/ambulance_open_door/2_positive/frame_2.jpg",
                "caption": "",
                "position": 545
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/teaser/ambulance_open_door/2_positive/frame_3.jpg",
                "caption": "",
                "position": 546
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/teaser/dog_yoga/3_negative/frame_1.jpg",
                "caption": "",
                "position": 553
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/teaser/dog_yoga/3_negative/frame_3.jpg",
                "caption": "",
                "position": 554
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/teaser/dog_yoga/3_negative/frame_4.jpg",
                "caption": "",
                "position": 555
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/teaser/ambulance_open_door/3_negative/frame_1.jpg",
                "caption": "",
                "position": 556
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/teaser/ambulance_open_door/3_negative/frame_2.jpg",
                "caption": "",
                "position": 557
            },
            {
                "img": "https://arxiv.org/html/2602.09146/images/teaser/ambulance_open_door/3_negative/frame_3.jpg",
                "caption": "",
                "position": 558
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]