[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13303/x1.png",
                "caption": "Figure 1:The illustration of our proposed creative table visualization task and ShowTable pipeline. Given a table about a specific topic, our task requires the model to produce a visualization infographic that is aesthetic and faithful to the data points. The ShowTable pipeline employs four steps: rewriting, generation, reflection, and refinement, thus achieving high-fidelity visualization. We use Wan2.5-Preview[wan2.5]here for generation and refinement.",
                "position": 107
            },
            {
                "img": "https://arxiv.org/html/2512.13303/x2.png",
                "caption": "Figure 2:The proposed ShowTable pipeline, which synergizes an MLLM as the central orchestrator with a diffusion model as the executor. Given a table, the MLLM first rewrites a detailed prompt for the diffusion model’s initial generation. The MLLM then iteratively reflects on the output to identify errors (marked in red) and provides precise instructions for refinement (corrected results shown in green).",
                "position": 132
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13303/x3.png",
                "caption": "Figure 3:Generation comparison among different rewriting. The model tends to directly render the table text, failed to visualize when disabling rewriting. General LLMs also tend to miss some data or wrongly classify data, especially for complex tables.",
                "position": 191
            },
            {
                "img": "https://arxiv.org/html/2512.13303/x4.png",
                "caption": "Figure 4:Demonstrating the refinement model’s capability as a critical pipeline bottleneck. The base model’s performance degrades with each correction, indicating an inability to process iterative feedback. In contrast, the Wan2.5-I2I-Preview shows consistent improvement. This confirms our pipeline structure is sound and that the bottleneck is the model’s capability, motivating our specialized training for the refinement module.",
                "position": 207
            },
            {
                "img": "https://arxiv.org/html/2512.13303/x5.png",
                "caption": "Figure 5:Dataset construction pipeline. We initially collect and filter images from public datasets with SOTA MLLMs, and then propose three different kinds of training data construction pipelines: rewriting training data, refinement training data, and reward training data.",
                "position": 210
            }
        ]
    },
    {
        "header": "4Dataset and Benchmark",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13303/x6.png",
                "caption": "Figure 6:Case studies of the ShowTable pipeline with four examples: top row shows one case requiring one-round reflection and refinement (left) and one proper initial generation (right), while other rows display results refined through multiple reflection rounds. Examples demonstrate adaptive correction of text, proportions, and visual elements. We mark the error parts with red boxes for better reading.",
                "position": 720
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix AMore Related Works",
        "images": []
    },
    {
        "header": "Appendix BMore Method Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13303/x7.png",
                "caption": "Figure A1:The system prompt and user prompt for the rewriting module. We use the same prompts for all models.",
                "position": 955
            },
            {
                "img": "https://arxiv.org/html/2512.13303/x8.png",
                "caption": "Figure A2:The system prompt and user prompt for the reflection module. We use the same prompts for all models.",
                "position": 958
            },
            {
                "img": "https://arxiv.org/html/2512.13303/x9.png",
                "caption": "Figure A3:The reward comparison between our RM and direct LLM scoring, we achieve a stable reward increase.",
                "position": 974
            },
            {
                "img": "https://arxiv.org/html/2512.13303/x10.png",
                "caption": "Figure A4:The Statistical information about our proposed TableVisBench. (a) (Left): the data length distribution of TableVisBench. (b) (Right): The word cloud of the topic in TableVisBench.",
                "position": 1029
            }
        ]
    },
    {
        "header": "Appendix CMore Dataset and Benchmark Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13303/x11.png",
                "caption": "Figure A5:The system prompt and user prompt of the Data Accuracy dimension.",
                "position": 1115
            },
            {
                "img": "https://arxiv.org/html/2512.13303/x12.png",
                "caption": "Figure A6:The system prompt and user prompt of the Text Rendering dimension.",
                "position": 1118
            },
            {
                "img": "https://arxiv.org/html/2512.13303/x13.png",
                "caption": "Figure A7:The system prompt and user prompt of the Relative Relationship dimension.",
                "position": 1121
            },
            {
                "img": "https://arxiv.org/html/2512.13303/x14.png",
                "caption": "Figure A8:The system prompt and user prompt of the Additional information Accuracy dimension.",
                "position": 1124
            }
        ]
    },
    {
        "header": "Appendix DMore Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13303/x15.png",
                "caption": "Figure A9:The qualitative comparison of different generation baselines of the same case on our proposed creative table visualization task. The first column presents the results without rewriting. The results via our ShowTable pipeline are shown from the second column.",
                "position": 1329
            },
            {
                "img": "https://arxiv.org/html/2512.13303/x16.png",
                "caption": "Figure A10:Some detailed pipeline visualizations. We use Qwen-Image here for base generation module with our pipeline.",
                "position": 1344
            },
            {
                "img": "https://arxiv.org/html/2512.13303/x17.png",
                "caption": "Figure A11:Some detailed pipeline visualizations. We use Qwen-Image here for base generation module with our pipeline.",
                "position": 1347
            },
            {
                "img": "https://arxiv.org/html/2512.13303/x18.png",
                "caption": "Figure A12:Some detailed pipeline visualizations. We use Qwen-Image here for base generation module with our pipeline.",
                "position": 1350
            },
            {
                "img": "https://arxiv.org/html/2512.13303/x19.png",
                "caption": "Figure A13:Some detailed pipeline visualizations with more complex and difficult cases. We use Wan2.5-Preview here for generation module and refinement module with our pipeline.",
                "position": 1353
            },
            {
                "img": "https://arxiv.org/html/2512.13303/x20.png",
                "caption": "Figure A14:Some detailed pipeline visualizations with more complex and difficult cases. We use Wan2.5-Preview here for generation module and refinement module with our pipeline.",
                "position": 1356
            }
        ]
    },
    {
        "header": "Appendix ELimitations and Future Work",
        "images": []
    }
]