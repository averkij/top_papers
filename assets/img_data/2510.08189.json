[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08189/x1.png",
                "caption": "Figure 1:Actual versus theoretical accuracy of R1-series models onR-Horizondatasets.",
                "position": 181
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3R-Horizon",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08189/x2.png",
                "caption": "Figure 2:TheR-Horizondata composition pipeline is illustrated in (a)-(c). We leverageR-Horizonto construct a comprehensive long-horizon reasoning evaluation benchmark spanning 6 tasks and generate multi-horizon training data for long-horizon reinforcement learning.",
                "position": 254
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08189/x3.png",
                "caption": "Figure 3:Evaluation results ofR-HorizonBenchmark.",
                "position": 457
            },
            {
                "img": "https://arxiv.org/html/2510.08189/x4.png",
                "caption": "Figure 4:Training curves comparing single and composed data onAIME24avg@8\\text{AIME24}_{\\text{avg@8}}and reward.",
                "position": 491
            }
        ]
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08189/x5.png",
                "caption": "Figure 5:Error type distribution across different query numbers.\nFour error categories:Problem Reasoning Errorrepresents reasoning errors made by the model for specific problems;Dependency Reasoning Errorindicates the model correctly solved previous problems but made errors when calculating the dependencies;Early Stopindicates the model prematurely terminated generation after solving previous problems;Output Truncationindicates generation exceeded token limit.",
                "position": 632
            },
            {
                "img": "https://arxiv.org/html/2510.08189/x6.png",
                "caption": "Figure 6:Analysis of accuracy and error position with R1-Qwen-7B and R1-Qwen-32B.",
                "position": 645
            },
            {
                "img": "https://arxiv.org/html/2510.08189/x7.png",
                "caption": "Figure 7:Reflection analysis on MATH500 dataset. Reflection Frequency refers to the average number of reflections per question.\nLong Reflection Rate refers to the proportion of questions whose reflection range exceeds the current question.",
                "position": 656
            },
            {
                "img": "https://arxiv.org/html/2510.08189/x8.png",
                "caption": "Figure 8:The thinking budget allocation for different query configurations (1-5 queries) across R1-Qwen-7B, R1-Qwen-32B, and Deepseek-R1 models on AIME24 datasets.",
                "position": 668
            },
            {
                "img": "https://arxiv.org/html/2510.08189/x9.png",
                "caption": "Figure 9:Analysis of reinforcement learning effects with single and composed datasets.\n(a) Math500 performance comparison, (b) error position analysis, (c) reflection analysis, and (d) token budget allocation across multi-horizon scenarios.",
                "position": 685
            }
        ]
    },
    {
        "header": "6Conlusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AR-HorizonDatasets Construction for code and agentic tasks",
        "images": []
    },
    {
        "header": "Appendix BHow Reinforcement Learning improves long-horizon Reasoning",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08189/x10.png",
                "caption": "Figure 10:The AIME24, AIME25 performance for single query and 2-query settings and response length evolution during multi-stage training progression\nacross 8k, 16k, and 32k context lengths. Vertical dashed lines mark stage transitions.",
                "position": 1299
            }
        ]
    },
    {
        "header": "Appendix CTraining Dynamics of RL withR-Horizon",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08189/x11.png",
                "caption": "Figure 11:Training dynamics comparison across different training data compositions (n=1, n=2, n=4) showing response length, training time per step, and entropy loss evolution during the RL training process.",
                "position": 1315
            }
        ]
    },
    {
        "header": "Appendix DAblation study",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08189/x12.png",
                "caption": "Figure 12:Comparison between multiple dependent and independent problems.",
                "position": 1325
            },
            {
                "img": "https://arxiv.org/html/2510.08189/x13.png",
                "caption": "Figure 13:R1-Qwen models showing anomalous behavior in sequential reasoning. Left:Accall\\text{Acc}_{\\text{all}}vs.Acclast\\text{Acc}_{\\text{last}}revealing increasing divergence. Right: Anomalous sample counts where models correctly answer final problems despite preceding errors.",
                "position": 1338
            },
            {
                "img": "https://arxiv.org/html/2510.08189/x14.png",
                "caption": "Figure 14:Ablation study on the impact of query difficulty ordering for R1-Qwen-7B, R1-Qwen-32B, and DeepsSeek-R1 models. (a) Performance comparison between easy-to-hard and hard-to-easy query orderings. (b) Thinking budget allocation in the easy-to-hard scenario. (c) Thinking budget allocation in the hard-to-easy scenario.",
                "position": 1351
            }
        ]
    },
    {
        "header": "Appendix EEvaluation Implementation Details",
        "images": []
    },
    {
        "header": "Appendix FTraining Implementation Details",
        "images": []
    },
    {
        "header": "Appendix GThe use of large language models",
        "images": []
    },
    {
        "header": "Appendix HCase Study",
        "images": []
    }
]