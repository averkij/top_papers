[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.18111/extracted/5882692/assets/icon-64px.png",
                "caption": "",
                "position": 87
            },
            {
                "img": "https://arxiv.org/html/2409.18111/x1.png",
                "caption": "Figure 1:Task definitions in E.T. Bench.The 12 tasks derives from 4 essential capabilities for time-sensitive video understanding:referring,grounding,dense captioning, andcomplex understanding.",
                "position": 103
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Event-Level & Time-Sensitive Video Understanding Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.18111/x2.png",
                "caption": "Figure 2:Left:Task taxonomy and sample distribution.Right:Generation pipeline for E.T. Bench. We conduct a thorough process of pre-filtering, annotation repurposing, instruction writing, manual check, and sampling to obtain high-quality fine-grained annotations. Details discussed in Section2.",
                "position": 288
            },
            {
                "img": "https://arxiv.org/html/2409.18111/x2.png",
                "caption": "",
                "position": 291
            },
            {
                "img": "https://arxiv.org/html/2409.18111/x3.png",
                "caption": "",
                "position": 295
            },
            {
                "img": "https://arxiv.org/html/2409.18111/x4.png",
                "caption": "Figure 3:Left:Word cloud of text queries shows a considerable degree of diversity.Right:Distribution of averaged video durations (in seconds) across 12 tasks.",
                "position": 308
            },
            {
                "img": "https://arxiv.org/html/2409.18111/x4.png",
                "caption": "",
                "position": 311
            },
            {
                "img": "https://arxiv.org/html/2409.18111/x5.png",
                "caption": "",
                "position": 315
            }
        ]
    },
    {
        "header": "3Our Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.18111/x6.png",
                "caption": "Figure 4:Overall architecture of E.T. Chat.We reformulate timestamp prediction as an embedding matching problem. See Section3for details.",
                "position": 348
            },
            {
                "img": "https://arxiv.org/html/2409.18111/x6.png",
                "caption": "Figure 4:Overall architecture of E.T. Chat.We reformulate timestamp prediction as an embedding matching problem. See Section3for details.",
                "position": 351
            },
            {
                "img": "https://arxiv.org/html/2409.18111/x7.png",
                "caption": "Figure 5:Detailed illustration of frame compressor.It accepts video patch embeddingsùêètsubscriptùêèùë°\\mathbf{P}_{t}bold_P start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTand the text promptùêìùêì\\mathbf{T}bold_Tas inputs, and compress video frame features into a single token.",
                "position": 499
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.18111/x8.png",
                "caption": "Figure 6:Left:Performance comparison between E.T. Chat and representative models.Right:Ranking of MLLMs on E.T. Bench, whereredmeans higher ranks andbluerepresents lower ranks.",
                "position": 2796
            },
            {
                "img": "https://arxiv.org/html/2409.18111/x8.png",
                "caption": "",
                "position": 2799
            },
            {
                "img": "https://arxiv.org/html/2409.18111/x9.png",
                "caption": "",
                "position": 2803
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ABenchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.18111/x10.png",
                "caption": "Figure 7:Frequency distribution of verbs in E.T. Bench.We only visualize the top 25 out of 461 verbs for clarity. The x- and y-axes denote the verbs and their frequencies, respectively.",
                "position": 3642
            },
            {
                "img": "https://arxiv.org/html/2409.18111/x11.png",
                "caption": "Figure 8:Frequency distribution of nouns in E.T. Bench.We only visualize the top 25 out of 3,090 nouns for clarity. The x- and y-axes denote the nouns and their frequencies, respectively.",
                "position": 3645
            }
        ]
    },
    {
        "header": "Appendix BMethod",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.18111/x12.png",
                "caption": "Figure 9:Design space for timestamp processing.Existing MLLMs handle timestamps in videos or coordinates in images via a) numerical expressions, b) special tokens, or c) external modules. Details are discussed in SectionB.1.",
                "position": 3666
            }
        ]
    },
    {
        "header": "Appendix CInstruction-Tuning Dataset",
        "images": []
    },
    {
        "header": "Appendix DExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.18111/extracted/5882692/assets/snow.png",
                "caption": "Table 14:Comparison on learnable modules.ATTN and FFN represent the attention and feed-forward layers in Q-Former, respectively.",
                "position": 6333
            },
            {
                "img": "https://arxiv.org/html/2409.18111/extracted/5882692/assets/fire.png",
                "caption": "",
                "position": 6440
            }
        ]
    },
    {
        "header": "Appendix ELimitations and Future Work",
        "images": []
    },
    {
        "header": "Appendix FLicenses",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.18111/x13.png",
                "caption": "Figure 10:Qualitative comparison on[RAR](left) and[ECA](right).",
                "position": 7167
            },
            {
                "img": "https://arxiv.org/html/2409.18111/x13.png",
                "caption": "",
                "position": 7170
            },
            {
                "img": "https://arxiv.org/html/2409.18111/x14.png",
                "caption": "",
                "position": 7174
            },
            {
                "img": "https://arxiv.org/html/2409.18111/x15.png",
                "caption": "Figure 11:Qualitative comparison on[RVQ](left) and[TVG](right).",
                "position": 7180
            },
            {
                "img": "https://arxiv.org/html/2409.18111/x15.png",
                "caption": "",
                "position": 7183
            },
            {
                "img": "https://arxiv.org/html/2409.18111/x16.png",
                "caption": "",
                "position": 7187
            },
            {
                "img": "https://arxiv.org/html/2409.18111/x17.png",
                "caption": "Figure 12:Qualitative comparison on[EPM](left) and[TAL](right).",
                "position": 7193
            },
            {
                "img": "https://arxiv.org/html/2409.18111/x17.png",
                "caption": "",
                "position": 7196
            },
            {
                "img": "https://arxiv.org/html/2409.18111/x18.png",
                "caption": "",
                "position": 7200
            },
            {
                "img": "https://arxiv.org/html/2409.18111/x19.png",
                "caption": "Figure 13:Qualitative comparison on[EVS](left) and[VHD](right).",
                "position": 7206
            },
            {
                "img": "https://arxiv.org/html/2409.18111/x19.png",
                "caption": "",
                "position": 7209
            },
            {
                "img": "https://arxiv.org/html/2409.18111/x20.png",
                "caption": "",
                "position": 7213
            },
            {
                "img": "https://arxiv.org/html/2409.18111/x21.png",
                "caption": "Figure 14:Qualitative comparison on[DVC](left) and[SLC](right).",
                "position": 7219
            },
            {
                "img": "https://arxiv.org/html/2409.18111/x21.png",
                "caption": "",
                "position": 7222
            },
            {
                "img": "https://arxiv.org/html/2409.18111/x22.png",
                "caption": "",
                "position": 7226
            },
            {
                "img": "https://arxiv.org/html/2409.18111/x23.png",
                "caption": "Figure 15:Qualitative comparison on[TEM](left) and[GVQ](right).",
                "position": 7232
            },
            {
                "img": "https://arxiv.org/html/2409.18111/x23.png",
                "caption": "",
                "position": 7235
            },
            {
                "img": "https://arxiv.org/html/2409.18111/x24.png",
                "caption": "",
                "position": 7239
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]