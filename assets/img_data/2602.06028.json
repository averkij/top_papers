[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06028/x1.png",
                "caption": "",
                "position": 109
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06028/x2.png",
                "caption": "Figure 2:Training paradigms for AR video diffusion models.(a) Self-forcing: A student matches a teacher capable of generating only 5s video using a 5s self-rollout. (b) Longlive(Yanget al.,2025): The student performs long rollouts supervised by a memoryless 5s teacher on random chunks. The teacherâ€™s inability to see beyond its 5s window creates a student-teacher mismatch. (c)Context Forcing (Ours): The student is supervised by a long-context teacher aware of the full generation history, resolving the mismatch in (b).",
                "position": 176
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06028/x3.png",
                "caption": "Figure 3:Context Forcing and Context Management System.We use KV Cache as the context memory, and we organize it into three parts: sink, slow memory and fast memory. During contextual DMD training, the long teacher provides supervision to the long student by utilizing the same context memory mechanism.",
                "position": 202
            },
            {
                "img": "https://arxiv.org/html/2602.06028/x4.png",
                "caption": "Figure 4:Comparison on 1-min Video Generation.Our method keeps both the background and subject consistent across 1-min video, while other baselines have different levels drifting or identity shift.",
                "position": 290
            },
            {
                "img": "https://arxiv.org/html/2602.06028/x5.png",
                "caption": "Figure 5:Qualitative Results of Context Forcing.Our method enables minute-level video generation with minimal drifting and high consistency across diverse scenarios.",
                "position": 349
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06028/x6.png",
                "caption": "Figure 6:Video Continuation with Robust Context Teacher.Context teacher can generate next segment videos with context generated by student.",
                "position": 457
            },
            {
                "img": "https://arxiv.org/html/2602.06028/x7.png",
                "caption": "Figure 7:Ablation on Error-Recycling Fine-Tuning (ERFT). With ERFT, context teacher is more robust to accumulate error.",
                "position": 1079
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APreliminaries",
        "images": []
    },
    {
        "header": "Appendix BVisual artifacts in LongLive.",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06028/x8.png",
                "caption": "Figure 8:Visual artifacts in LongLive.The model exhibits a sudden flashback artifact, where the video abruptly resets to the initial frame after 524 frames, disrupting temporal continuity.",
                "position": 1623
            }
        ]
    },
    {
        "header": "Appendix CAlgorithm of Context Forcing.",
        "images": []
    }
]