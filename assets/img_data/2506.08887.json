[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.08887/x1.png",
                "caption": "Figure 1:When parameter-efficient transferring image-level CLIP to video-text retrieval, we identify three key discrepancies: vision, alignment, and language. Unlike previous methods, our DiscoVLA aims to address all these discrepancies. More visualization examples are provided in Figure4.",
                "position": 80
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.08887/x2.png",
                "caption": "Figure 2:The overall framework of DiscoVLA. Initially, we generate pseudo image captions for each sampled image. In both vision and text encoders, we utilize image-level layers to acquire pretrained image-level knowledge and employ IVFusion layers to enhance spatio-temporal information.\nThe single video caption is encoded through the text encoder, utilizing IVFusion layer as image-level (image-lvl) layer.\nFinally, AlignDistill is applied to distill image-level alignment to video-level alignment. For fair comparisons with previous methods, we do not generate pseudo image captions during the inference phase.",
                "position": 193
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.08887/x3.png",
                "caption": "Figure 3:Illustration of the encoder layers for vision and text encoders. (a) Image-Level Attn operates on each of theFùêπFitalic_Fimages or image captions individually. (b) Video-Level Attn concatenates tokens across all sampled images or image captions.\n(c) IVFusion Attn employs a lightweight adapter to integrate the efficiency of Image-Level Attn with the effectiveness of Video-Level Attn.\nHere, we illustrate the application of IVFusion within the vision encoder. The text encoder adopts the same approach for processing pseudo image captions.",
                "position": 257
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.08887/x4.png",
                "caption": "Figure 4:Visualization of discrepancies between video-level and image-level data. Each example consists of a paired video and video caption, with four sampled images from the video. Video captions are highlighted with orange solid lines, and pseudo image captions generated by LLaVA-NeXT[35]are indicated with green dashed lines. While video captions convey the general context of the video, image captions focus on the detailed context of each individual frame.",
                "position": 1405
            },
            {
                "img": "https://arxiv.org/html/2506.08887/x5.png",
                "caption": "Figure 5:Comparison of image caption generation with (W/) and without (W/O) video caption guidance (VidCap) in PImgAlign.",
                "position": 1408
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Experimental Settings",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.08887/x6.png",
                "caption": "Figure 6:Ablation study onŒ±ùõº\\alphaitalic_Œ±in Eq. (14) for text-to-video results on MSRVTT using CLIP (ViT-B/32).Œ±ùõº\\alphaitalic_Œ±represents the weight of the image-level alignment loss‚ÑíA‚Å¢(Simimg)subscript‚Ñíùê¥superscriptSimimg\\mathcal{L}_{A}({\\rm Sim}^{\\rm img})caligraphic_L start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT ( roman_Sim start_POSTSUPERSCRIPT roman_img end_POSTSUPERSCRIPT ). All other hyperparameters are kept constant.",
                "position": 2354
            },
            {
                "img": "https://arxiv.org/html/2506.08887/x7.png",
                "caption": "Figure 7:Ablation study onŒ≤ùõΩ\\betaitalic_Œ≤in Eq. (14) for text-to-video results on MSRVTT using CLIP (ViT-B/32).Œ≤ùõΩ\\betaitalic_Œ≤represents the weight of the distillation loss‚ÑíK‚Å¢Lsubscript‚Ñíùêæùêø\\mathcal{L}_{KL}caligraphic_L start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT. All other hyperparameters are kept constant.",
                "position": 2359
            },
            {
                "img": "https://arxiv.org/html/2506.08887/x8.png",
                "caption": "Figure 8:Ablation study onHVsubscriptùêªùëâH_{V}italic_H start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPTfor text-to-video results on MSRVTT using CLIP (ViT-B/32).HVsubscriptùêªùëâH_{V}italic_H start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPTrepresents the number of IVFusion layers in the vision encoder. All other hyperparameters are kept constant.",
                "position": 2371
            },
            {
                "img": "https://arxiv.org/html/2506.08887/x9.png",
                "caption": "Figure 9:Ablation study onHLsubscriptùêªùêøH_{L}italic_H start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPTfor text-to-video results on MSRVTT using CLIP (ViT-B/32).HLsubscriptùêªùêøH_{L}italic_H start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPTrepresents the number of IVFusion layers in the text encoder. All other hyperparameters are kept constant.",
                "position": 2376
            }
        ]
    },
    {
        "header": "Appendix BAdditional Experimental Results",
        "images": []
    }
]