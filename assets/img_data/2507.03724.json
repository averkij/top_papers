[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.03724/x1.png",
                "caption": "",
                "position": 198
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.03724/extracted/6596874/img/maintest.png",
                "caption": "Figure 1:MemOSachieves state-of-the-art performance across all reasoning tasks.This figure summarizes LLM-Judge scores on the LOCOMO benchmark, covering four task categories (Single-hop, Multi-hop, Open-domain, Temporal Reasoning) and the overall average.MemOS(MemOS-0630) consistently ranks first in all categories, outperforming strong baselines such as mem0, LangMem, Zep, and OpenAI-Memory, with especially large margins in challenging settings like multi-hop and temporal reasoning. Error bars indicate standard deviation. Full metric breakdown is provided in Table3.",
                "position": 354
            },
            {
                "img": "https://arxiv.org/html/2507.03724/extracted/6596874/img/memory_circuitry_theory_updated.png",
                "caption": "Figure 2:Categorization of LLM knowledge, including the memory hierarchy.\nThe explicit memories, extracted from model activations, lie half-way between raw data and model parameters,\nso a dotted line is used to indicate that they may or may not be regarded as parameters.\nReproduced from[1].",
                "position": 407
            }
        ]
    },
    {
        "header": "2Memory in Large Language Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.03724/x2.png",
                "caption": "Figure 3:Illustration of the evolution of memory systems in large language models, highlighting the progression from definition and exploration, to human-like memory development, and to tool-based memory management.",
                "position": 531
            }
        ]
    },
    {
        "header": "3MemOSDesign Philosophy",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.03724/x3.png",
                "caption": "Figure 4:Phased transitions in model performance: from pretraining and post-training to the Mem-training stage.MemOSserves as the foundational infrastructure enabling the next era of scaling laws.",
                "position": 881
            }
        ]
    },
    {
        "header": "4Memory Modeling inMemOS",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.03724/x4.png",
                "caption": "Figure 5:Transformation paths among three types of memory, forming a unified, controllable, and evolvable memory space.",
                "position": 1101
            },
            {
                "img": "https://arxiv.org/html/2507.03724/x5.png",
                "caption": "Figure 6:MemCube: A unified encapsulation structure for heterogeneous memory scheduling. EachMemCubeconsists of a structured Metadata Header (supporting lifecycle, permission, and storage policy) and a Memory Payload (encapsulating plaintext, activation states, or parameter deltas). It is the minimal memory unit withinMemOSthat can be scheduled and composed for downstream reasoning.",
                "position": 1118
            }
        ]
    },
    {
        "header": "5Architecture ofMemOS",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.03724/x6.png",
                "caption": "Figure 7:Overview of theMemOSframework. The architecture illustrates the full pipeline from user input through semantic parsing and API abstraction in the interface layer, to memory scheduling and lifecycle control in the operation layer, and finally interaction with the infrastructure layer for memory injection, retrieval, and governance. The unified data structure,MemCube, serves as the foundation for dynamic memory flow throughout model execution.",
                "position": 1181
            },
            {
                "img": "https://arxiv.org/html/2507.03724/x7.png",
                "caption": "Figure 8:Overview of MemOS architecture and memory interaction flow. The system is composed of the interface layer, operation layer, and infrastructure layer. From left to right, it shows the complete memory processing pipeline from user input to parsing, scheduling, injection, and response generation. Each stage corresponds to coordinated module invocation, with MemoryCube serving as the carrier across layers for structured, governable, and traceable memory lifecycle management.",
                "position": 1234
            }
        ]
    },
    {
        "header": "6Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.03724/extracted/6596874/img/ab_result.png",
                "caption": "Figure 9:Performance trends ofMemOSacross memory configurations.We vary the number of retrieved memory chunks (Top-K, upper x-axis) and chunk size (lower x-axis, total memory tokens), and report performance on the LOCOMO benchmark across multiple metrics and task types. MemOS consistently maintains top-tier performance as memory capacity increases, with clear gains on multi-hop and temporal reasoning tasks. Cosine similarity results indicate stable semantic alignment throughout.",
                "position": 2047
            },
            {
                "img": "https://arxiv.org/html/2507.03724/extracted/6596874/img/kvtest.png",
                "caption": "Figure 10:Time to First Token (TTFT) comparison across models, context lengths, and query lengths. KV-based memory injection consistently achieves lower latency with identical output.",
                "position": 2657
            }
        ]
    },
    {
        "header": "7MemOSfor Architecture Innovation and Applications",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]