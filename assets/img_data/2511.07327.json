[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07327/x1.png",
                "caption": "",
                "position": 126
            },
            {
                "img": "https://arxiv.org/html/2511.07327/x2.png",
                "caption": "Figure 1:Performance of IterResearch against state-of-the-art open-source long-horizon agents.",
                "position": 127
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07327/x3.png",
                "caption": "Figure 2:(Top)The mono-contextual approach linearly accumulates all information into a single, ever-expanding context, leading to context suffocation and noise contamination.(Bottom)IterResearch models deep research as an extended MDP with workspace reconstruction.\nEach round begins with a reconstructed workspacests_{t}containing the question, an evolving report‚Ñ≥t\\mathcal{M}_{t}, and immediate context.\nThe agent generates structured decisionsdt=d_{t}=(Think,Report,Action) and interacts with environment‚Ñ∞\\mathcal{E}.\nThe transition functionùíØ\\mathcal{T}reconstructs the workspace, maintaining the Markov property while preventing context bloat and enabling sustained reasoning and information-seeking.",
                "position": 211
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07327/x4.png",
                "caption": "Figure 3:Interaction Scaling.",
                "position": 758
            },
            {
                "img": "https://arxiv.org/html/2511.07327/x5.png",
                "caption": "Figure 4:Performance comparison between IterResearch and ReAct as Prompting Strategies.",
                "position": 781
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix AAddtional Related Work",
        "images": []
    },
    {
        "header": "Appendix BMore Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07327/x6.png",
                "caption": "Figure 5:Training dynamics of our RL.(Left)Training Rewards Curve.(Right)Accuracy Curve.",
                "position": 1038
            },
            {
                "img": "https://arxiv.org/html/2511.07327/x7.png",
                "caption": "",
                "position": 1041
            }
        ]
    },
    {
        "header": "Appendix CMore Implementation Details",
        "images": []
    },
    {
        "header": "Appendix DCase Study of IterResearch",
        "images": []
    },
    {
        "header": "Appendix EInstruction Templates",
        "images": []
    }
]