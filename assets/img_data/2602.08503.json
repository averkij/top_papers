[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08503/x1.png",
                "caption": "Figure 1:Comparison of accuracy and training efficiency across different RL methods initialized on Qwen3-8B-VL-Instruct. Octopus achieves the best average accuracy across seven benchmarks while requiring substantially less rollout time.",
                "position": 105
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Learning Self-Correction from Paired Rollouts",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08503/x2.png",
                "caption": "Figure 2:The percentage of different correction behaviors during RL training with a self-correction–encouraging prompt.",
                "position": 293
            },
            {
                "img": "https://arxiv.org/html/2602.08503/x3.png",
                "caption": "Figure 3:Left: Octopus augmentation pairs responses before and after the<sc>token to explicitly construct effective self-correction examples (wrong→\\rightarrowcorrect), increasing their count from 0 to 4. It also produces an equal number of positive and negative samples (4 each), balancing the advantage distribution within each training group.Right: Our two-stage RL pipeline. In Stage I, we decouple self-correction learning by applying masks and KL regularization too1o_{1}. In Stage II, we selectively unmasko1o_{1}only for samples with non-conflicting reward signals, while keeping it masked for the remaining samples.",
                "position": 296
            }
        ]
    },
    {
        "header": "4Training Recipe",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08503/x4.png",
                "caption": "Figure 4:Training dynamics of different methods. GSPO is initialized from the baseπθ\\pi_{\\theta}and trained with standard RL. In-dis and Mixed Sampling are initialized from their corresponding SFT models and trained with Octopus RL strategy introduced in §4.3.",
                "position": 332
            },
            {
                "img": "https://arxiv.org/html/2602.08503/x5.png",
                "caption": "Figure 5:Teaching self-correction with binary and shaped rewards. (a) Reward curves before and after self-correction under a binary reward setting, showing limited self-correction learning. (b) Reward curves with the shaped reward defined in Eq. (3), highlighting the emergence of reward hacking.",
                "position": 373
            },
            {
                "img": "https://arxiv.org/html/2602.08503/x6.png",
                "caption": "Figure 6:Training curves for Stage I. (a) The reward gap betweeno1o_{1}ando2o_{2}gradually widens during training, indicating effective learning of self-correction. (b) The self-correction rewardrscr_{\\text{sc}}(Eq. (4.3)) before and after Octopus augmentation. Octopus balances positive and negative samples, leading to stable reward dynamics.",
                "position": 388
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08503/x7.png",
                "caption": "Figure 7:Test-time scaling (TTS) performance on MMStar.Left: Sequential TTS achieved by appending<sc>tokens to trigger self-correction.Greenpoints denote the original performance without TTS, andbluepoints indicate responses with TTS triggers. The x-axis shows the average cumulative number of tokens during inference.Right: Comparison of pass@kkperformance.",
                "position": 869
            }
        ]
    },
    {
        "header": "6Related Works",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BEvaluation Details",
        "images": []
    },
    {
        "header": "Appendix CFailure Attempts",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08503/x8.png",
                "caption": "Figure 8:Failure attempts. Training collapse during RL training.",
                "position": 1734
            },
            {
                "img": "https://arxiv.org/html/2602.08503/asset/case2.png",
                "caption": "",
                "position": 1751
            },
            {
                "img": "https://arxiv.org/html/2602.08503/asset/case1.png",
                "caption": "",
                "position": 1819
            },
            {
                "img": "https://arxiv.org/html/2602.08503/asset/case3.png",
                "caption": "",
                "position": 1852
            },
            {
                "img": "https://arxiv.org/html/2602.08503/asset/case4.png",
                "caption": "",
                "position": 1880
            }
        ]
    },
    {
        "header": "Appendix DCase Study",
        "images": []
    }
]