[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.24176/x1.png",
                "caption": "Figure 1:Visualization Results.We visualize our latent diffusion system with proposed IG together with LightningDiT-XL trained on ImageNet 256 × 256 resolution. With an IG scale of 1.4 and a CFG scale of 1.45, and further combining the guidance interval, we ultimately achieved the state-of-the-art FID = 1.19. More uncurated samples are provided inSupplementary Material.",
                "position": 107
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.24176/x2.png",
                "caption": "Figure 2:The overall framework of our proposed Internal Guidance.We introduce an additional auxiliary supervision loss during training, and utilize the intermediate layer outputs during sampling process to guide the final outputs.",
                "position": 212
            },
            {
                "img": "https://arxiv.org/html/2512.24176/x3.png",
                "caption": "Figure 3:A fractal-like 2D distribution with two classes indicated with gray and orange regions.Approximately 99% of the probability mass is inside the shown contours. (a) Conditional sampling using a small denoising\ndiffusion model generates outliers due to its limited fitting capability. (b) Classifier-free guidance (w = 2.5) eliminates the vast majority of outliers but reduces diversity by over-emphasizing the class.\n(c) Internal guidance (w = 2) can maintain diversity as Autoguidance[karras2024guiding]while allowing some outliers at the ends of the branches. (d) The combination of IG and CFG can significantly reduce outliers without reducing diversity.",
                "position": 256
            }
        ]
    },
    {
        "header": "4Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.24176/x4.png",
                "caption": "Figure 4:Internal guidance inspires new training acceleration methods.(a) Conditional sampling using a not well-trained denoising diffusion model generates a large number of outliers. (b) Outliers can gradually be eliminated with sufficient training. (c) Internal guidance can also eliminate outliers with not well-trained denoising diffusion model. (d) The loss function we proposed can accelerate convergence. In SiT-B/2 experiments, our proposed loss function demonstrates superior accelerated convergence performance compared to REPA[yu2024representation].",
                "position": 309
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.24176/x5.png",
                "caption": "Figure 5:The combination of IG and CFGcan yield a higher FID value, which is superior to simply apply CFG. In particular, the FID value generated by the combination of a lower IG coefficient and CFG is higher than that of a higher coefficient.",
                "position": 905
            },
            {
                "img": "https://arxiv.org/html/2512.24176/x6.png",
                "caption": "Figure 6:Scalability of IG.The relative improvement of IG over the vanilla model (DiTs and SiTs) becomes increasingly significant as the model size grows.",
                "position": 1007
            },
            {
                "img": "https://arxiv.org/html/2512.24176/x7.png",
                "caption": "Figure 7:The combination of IG and CFGcan produce better visual quality. When only adding IG, better generation results can be obtained. Incorporating CFG can further reduce the inaccurate content in the generated results.",
                "position": 1057
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix AEvaluation Metric",
        "images": []
    },
    {
        "header": "Appendix BComputational Cost Comparison",
        "images": []
    },
    {
        "header": "Appendix CComparison of Guidance Methods",
        "images": []
    },
    {
        "header": "Appendix D512×512 ImageNet",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.24176/x8.png",
                "caption": "Figure 8:Samples on ImageNet 512×512from SiT-XL/2 + IG (1.4) using CFG withw=1.8w=1.8.",
                "position": 1448
            }
        ]
    },
    {
        "header": "Appendix EHyperparameter and More Implementation Details",
        "images": []
    },
    {
        "header": "Appendix FDetails of the 2D toy example",
        "images": []
    },
    {
        "header": "Appendix GMore Discussion on Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.24176/x9.png",
                "caption": "Figure 9:Uncurated visualization results of LightningDiT-XL/1 + IG (1.8) use CFG withw=2.5w=2.5.",
                "position": 2101
            },
            {
                "img": "https://arxiv.org/html/2512.24176/x10.png",
                "caption": "Figure 10:Uncurated visualization results of LightningDiT-XL/1 + IG (1.8) use CFG withw=2.5w=2.5.",
                "position": 2106
            },
            {
                "img": "https://arxiv.org/html/2512.24176/x11.png",
                "caption": "Figure 11:Uncurated visualization results of LightningDiT-XL/1 + IG (1.8) use CFG withw=2.5w=2.5.",
                "position": 2111
            }
        ]
    },
    {
        "header": "Appendix HMore Qualitative Results",
        "images": []
    }
]