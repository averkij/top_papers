[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.10458/x1.png",
                "caption": "",
                "position": 98
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.10458/x2.png",
                "caption": "Figure 2:(a)Reconstruction-based diffusion finetuning integrates image features using a diffusion loss, focusing on pixel-level image reconstruction without reasoning.(b)ThinkDiff aligns a VLM to an LLM decoder by vision-language training on image-caption datasets. In inference (dotted lines), it transfers multimodal in-context reasoning capabilities from the VLM to a diffusion decoder.",
                "position": 121
            },
            {
                "img": "https://arxiv.org/html/2502.10458/x3.png",
                "caption": "Figure 3:Several diffusion models share a language encoder with encoder-decoder LLMs, allowing aligning with diffusion decoders through aligning with LLM decoders.",
                "position": 132
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.10458/x4.png",
                "caption": "Figure 4:(a) InThinkDiff-LVLMtraining, the LVLM processes an image and a text to generate text tokens and token features, with some token features randomly masked. Unmasked token features are passed to a trainable aligner network and an LLM decoder, predicting masked text tokens supervised by cross-entropy loss. In inference, the LLM decoder is replaced by a diffusion decoder, enabling in-context reasoning image generation from interleaved images and texts. (b) InThinkDiff-CLIPtraining, a CLIP vision model extracts image token features which are then mapped by a trainable aligner network. A part of the image caption is encoded by the LLM encoder and concatenated with image tokens. These combined tokens are passed to the LLM decoder to predict the next part of the caption supervised by cross-entropy loss. In inference, the LLM decoder is replaced by a diffusion encoder, allowing coherent image generation based on multimodal context.",
                "position": 194
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.10458/x5.png",
                "caption": "Figure 5:2-shot evaluation results on CoBSAT. The input structure is similar to Figure1a. Given multimodal inputs, ThinkDiff-LVLM accurately captures both implicit attributes (e.g., wicker material) and explicit attributes (e.g. car), and generates a logically correct image (wicker car). In contrast, methods such as SEED-LLaMA(Ge et al.,2024), Emu(Sun et al.,2023)and GILL(Koh et al.,2024)produce inaccurate and lower-quality images. The ground truth implicit attribute is highlighted in red for readers’ reference. See more results in Appendix Figure9and10.",
                "position": 295
            },
            {
                "img": "https://arxiv.org/html/2502.10458/x6.png",
                "caption": "Figure 6:Generation results for single image (I) and single image with text prompt (I + T) inputs. Our method effectively integrates semantic details of both image and text modalities to produce coherent images. FLUX excels at replicating the input image but struggles to maintain consistency with additional text prompts. See more results in Figure11.",
                "position": 412
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.10458/x7.png",
                "caption": "Figure 7:Training losses (log scale) of ThinkDiff-LVLM comparing different RMSNorm designs. Disabling RMSNorm (w/o RMSNorm) or using the default RMSNorm initialization (RMSNorm w/ Default init.) results in significantly unstable training.",
                "position": 520
            },
            {
                "img": "https://arxiv.org/html/2502.10458/x8.png",
                "caption": "Figure 8:Results ofThinkDiff-CLIPcomposing two images. It creatively merge semantic details of both images. See more results in Appendix Figure12.",
                "position": 665
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Impact Statements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "APPENDIX",
        "images": []
    },
    {
        "header": "Appendix ALimitation",
        "images": []
    },
    {
        "header": "Appendix BDataset details",
        "images": []
    },
    {
        "header": "Appendix CMore high-quality results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.10458/x9.png",
                "caption": "Figure 9:More 2-shot reasoning results of ThinkDiff-LVLM on CoBSAT benchmark.",
                "position": 1537
            },
            {
                "img": "https://arxiv.org/html/2502.10458/x10.png",
                "caption": "Figure 10:More 2-shot reasoning results of ThinkDiff-LVLM on CoBSAT benchmark.",
                "position": 1540
            },
            {
                "img": "https://arxiv.org/html/2502.10458/x11.png",
                "caption": "Figure 11:Generation results of a single image and a text prompt of ThinkDiff-CLIP.",
                "position": 1553
            },
            {
                "img": "https://arxiv.org/html/2502.10458/x12.png",
                "caption": "Figure 12:Multiple input image generation results of ThinkDiff-CLIP.",
                "position": 1556
            },
            {
                "img": "https://arxiv.org/html/2502.10458/x13.png",
                "caption": "Figure 13:Generation results for multiple images (2I) and multiple images with a text prompt (2I + T) of ThinkDiff-CLIP.",
                "position": 1559
            },
            {
                "img": "https://arxiv.org/html/2502.10458/x14.png",
                "caption": "Figure 14:Image + text to video generation results of ThinkDiff-CLIP.",
                "position": 1570
            }
        ]
    },
    {
        "header": "Appendix DVideo results of ThinkDiff-CLIP",
        "images": []
    }
]