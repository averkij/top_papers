[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20969/x1.png",
                "caption": "(a)Typical pipeline stages of a RAG application.",
                "position": 101
            },
            {
                "img": "https://arxiv.org/html/2502.20969/x1.png",
                "caption": "(a)Typical pipeline stages of a RAG application.",
                "position": 104
            },
            {
                "img": "https://arxiv.org/html/2502.20969/x2.png",
                "caption": "(b)Different scenarios for RAG and the illustration of the proposedlookahead retrievalmechanism. It prefetches relevant data for retrieval from CPU to GPU, overlaps data transfer with the pre-retrieval stage, and accelerates retrieval with GPU-CPU cooperation.",
                "position": 110
            }
        ]
    },
    {
        "header": "2Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20969/x3.png",
                "caption": "Figure 2:Overview of RAG.",
                "position": 207
            }
        ]
    },
    {
        "header": "3Analyzing Latency of RAG Pipelines",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20969/x4.png",
                "caption": "Figure 3:Latency breakdown of six RAG pipelines on NQ dataset[54].",
                "position": 371
            },
            {
                "img": "https://arxiv.org/html/2502.20969/x5.png",
                "caption": "Figure 4:The breakdown of memory consumption at GPU and CPU for two different strategies: CPU offloading and GPU-based retrieval. The dotted line indicates the memory capacity of a RTX4090 GPU, which is a common used GPU for local deployment.",
                "position": 374
            },
            {
                "img": "https://arxiv.org/html/2502.20969/x6.png",
                "caption": "Figure 5:Latency breakdown of CPU-offload and runtime-fetch GPU retrieval, averaged over 512 random NQ queries.",
                "position": 422
            }
        ]
    },
    {
        "header": "4Design ofTeleRAG",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20969/x7.png",
                "caption": "Figure 6:(a) The overview ofTeleRAG‚Äôslookahead retrievaland comparison to the baseline CPU-offloaded retrieval. (b) System design ofTeleRAG. It first loads the entire index data into CPU memory and identifies the clusters (Cinsubscriptùê∂inC_{\\text{in}}italic_C start_POSTSUBSCRIPT in end_POSTSUBSCRIPT) for the query before the transformation (qinsubscriptùëûinq_{\\text{in}}italic_q start_POSTSUBSCRIPT in end_POSTSUBSCRIPT). It then transfers the data ofCinsubscriptùê∂inC_{\\text{in}}italic_C start_POSTSUBSCRIPT in end_POSTSUBSCRIPTto GPU memory while running the pre-retrieval stage and generating the retrieval queryqoutsubscriptùëûoutq_{\\text{out}}italic_q start_POSTSUBSCRIPT out end_POSTSUBSCRIPT. During the search, the overlapped clusters are searched on GPU, and the missed clusters are searched on CPU. The retrieval results are then merged and passed to the next stage.",
                "position": 512
            }
        ]
    },
    {
        "header": "5Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20969/x8.png",
                "caption": "Figure 7:Overview of six RAG pipelines that we evaluate.",
                "position": 820
            },
            {
                "img": "https://arxiv.org/html/2502.20969/x9.png",
                "caption": "(a)End-to-end latency speedup with Llama-3.2-3B with a single RTX4090 GPU.",
                "position": 964
            },
            {
                "img": "https://arxiv.org/html/2502.20969/x9.png",
                "caption": "(a)End-to-end latency speedup with Llama-3.2-3B with a single RTX4090 GPU.",
                "position": 967
            },
            {
                "img": "https://arxiv.org/html/2502.20969/x10.png",
                "caption": "(b)End-to-end latency speedup with Llama-3-8B with a single RTX4090 GPU.",
                "position": 973
            },
            {
                "img": "https://arxiv.org/html/2502.20969/x11.png",
                "caption": "(c)End-to-end latency speedup with Llama-3-8B with a single H100 GPU.",
                "position": 979
            },
            {
                "img": "https://arxiv.org/html/2502.20969/x12.png",
                "caption": "(d)End-to-end latency speedup with Llama-2-13B with a single H100 GPU.",
                "position": 985
            },
            {
                "img": "https://arxiv.org/html/2502.20969/x13.png",
                "caption": "(a)OnDesktopwith a RTX4090 GPU.",
                "position": 1070
            },
            {
                "img": "https://arxiv.org/html/2502.20969/x13.png",
                "caption": "(a)OnDesktopwith a RTX4090 GPU.",
                "position": 1073
            },
            {
                "img": "https://arxiv.org/html/2502.20969/x14.png",
                "caption": "(b)OnServerwith a H100 GPU.",
                "position": 1079
            },
            {
                "img": "https://arxiv.org/html/2502.20969/x15.png",
                "caption": "(a)With Llama-3-8B.",
                "position": 1086
            },
            {
                "img": "https://arxiv.org/html/2502.20969/x15.png",
                "caption": "(a)With Llama-3-8B.",
                "position": 1089
            },
            {
                "img": "https://arxiv.org/html/2502.20969/x16.png",
                "caption": "(b)With Llama-2-13B.",
                "position": 1095
            },
            {
                "img": "https://arxiv.org/html/2502.20969/x17.png",
                "caption": "Table 5:The prefetch budget and corresponding averaged cluster hit rate for each pipeline and hardware setup on NQ dataset. The target retrievalnprobeis 512.",
                "position": 1102
            },
            {
                "img": "https://arxiv.org/html/2502.20969/x18.png",
                "caption": "Figure 11:The cluster coverage rate for HyDE with different numbers of prefetched clusters. The target retrievalnprobeis512512512512. Results are the average of NQ, HotpotQA and TriviaQA.",
                "position": 1208
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]