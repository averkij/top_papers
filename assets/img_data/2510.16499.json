[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Problem Definition",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16499/figures/composition.png",
                "caption": "Figure 1:The problem of agent composition assumes there is existing inventory of agentic components and a task description. Using these inputs, the agent composition solution should select the most relevant components to build an agent (left). The agentic system can then be deployed as it has the correct set of components to accomplish goals as set by the user at inference.",
                "position": 159
            }
        ]
    },
    {
        "header": "4Composer Agents: From Semantic Retrieval to Knapsack Selection",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16499/figures/online.png",
                "caption": "Figure 2:Overview of our proposed online knapsack composer. Similar to offline baselines, the workflow begins with generating skills and queries from the given task descriptions (AppendixA.1). Then, the composer retrieves components from the inventory given skill descriptions. With the retrieved components, the composer tests them individually. If value-to-price ratio meets the online knapsack threshold, then it is added as part of the agentic system. Otherwise, the search continues.",
                "position": 185
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16499/figures/gaia_sonnet3.5v2.png",
                "caption": "Figure 3:Results of Claude 3.5 Sonnet single-agent experiments where we evaluate all the approaches to select the most relevant tools given task description. After equipping the agent with those tools, we then run evaluation on the dataset and plot success rate against the budget spent on tools. We plot the pareto frontier which represents that no other agent can perform better simultaneously in both success rate and cost. Overall, theonline knapsack with AvaTaR optimization($30) shows to be cost-effective and highest performing approach, followed byonline knapsack without optimization($30).",
                "position": 504
            },
            {
                "img": "https://arxiv.org/html/2510.16499/figures/simpleqa_sonnet3.5v2.png",
                "caption": "",
                "position": 513
            },
            {
                "img": "https://arxiv.org/html/2510.16499/figures/medqa_sonnet3.5v2.png",
                "caption": "",
                "position": 519
            },
            {
                "img": "https://arxiv.org/html/2510.16499/figures/travel.png",
                "caption": "Figure 4:Results of Claude 3.5 Sonnet multi-agent experiments where we evaluate all approaches to select most relevant tools given task description. After equipping the agent with those tools, we then run evaluation on the dataset and plot success rate against tool costs. Overall,online knapsackshows to be cost-effective and highest performing approach.",
                "position": 576
            },
            {
                "img": "https://arxiv.org/html/2510.16499/figures/mortgage.png",
                "caption": "",
                "position": 585
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments and Disclosure of Funding",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]