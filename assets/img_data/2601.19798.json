[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.19798/x1.png",
                "caption": "Figure 1:Youtu-VL achieves competitive performance on both general multimodal tasks and vision-centric tasks.The concentric rings illustrate the capability scope of different models across various tasks. Colored regions indicate that the model supports the corresponding task, while white regions denote a lack of support. Unlike prior models that exhibit functional gaps, Youtu-VL accommodates a comprehensive range of vision-centric and multimodal tasks via a standard architecture, achieving competitive performance without relying on task-specific modules.",
                "position": 129
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.19798/x2.png",
                "caption": "Figure 2:Comparison between the previous ”vision as input” paradigm and the Youtu-VL ”vision as target” paradigm.The left panel shows the previous text-dominant VLM, which relies solely on text supervision. The right panel illustrates the Youtu-VL paradigm, which incorporates Vision-Language Unified Autoregressive Supervision (VLUAS), treating vision as a target to achieve unified supervision for both image and text.",
                "position": 135
            }
        ]
    },
    {
        "header": "2Architecture and Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.19798/x3.png",
                "caption": "Figure 3:Overview of the Youtu-VL Framework.Left: The architecture integrates a Vision Encoder and Youtu-LLM via a Spatial Merge Projector, operating under the proposed VLUAS paradigm for unified autoregressive modeling.\nMiddle: The Synergistic Vision Tokenizer. We construct a unified vocabulary by fusing semantic and geometric features via cross-attention, optimized with perception and adversarial losses.\nRight: Dense prediction mechanism. Our proposed NTP-M enables robust multi-label supervision with a relevant negative sampling. Unlike conventional approaches, Youtu-VL achieves direct dense prediction without auxiliary decoders or task-specific tokens.",
                "position": 179
            }
        ]
    },
    {
        "header": "3Pre-training",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.19798/figures/recipe.png",
                "caption": "Figure 4:The pre-training recipe for Youtu-VL. The top panel illustrates the evolution of the data mixture from Stage 1 to Stage 4: Stages 1 and 2 exclusively utilize pure text data to establish a strong linguistic foundation, while Stages 3 and 4 progressively enhance multimodal capabilities. The bottom panel presents the learning rate schedule aligned with the training stages.",
                "position": 341
            },
            {
                "img": "https://arxiv.org/html/2601.19798/x4.png",
                "caption": "Figure 5:Data synthesis pipeline for open-world scenarios. The framework processes massive vision-centric data through two parallel branches: (top) object detection and semantic segmentation, utilizing grounding models for raw data and data binding strategies for labeled data; and (bottom) depth estimation, employing depth models and quantization. The pipeline applies specific augmentations to generate a comprehensive dataset for open-world tasks.",
                "position": 370
            },
            {
                "img": "https://arxiv.org/html/2601.19798/x5.png",
                "caption": "Figure 6:The workflow for synthesizing knowledge-dense image caption and knowledge data. Starting from massive raw image-text pairs, the pipeline proceeds through three main stages: (1) a multi-stage filtration protocol to ensure basic quality; (2) a core enhancement phase featuring Concept-based Sampling, Rare Class Mining, and Knowledge-Injected Recaptioning to maximize information density and diversity; and (3) a final purification stage for deduplication.",
                "position": 386
            },
            {
                "img": "https://arxiv.org/html/2601.19798/x6.png",
                "caption": "Figure 7:Data construction pipeline for STEM. The pipeline consists of three key phases: (1) Multi-Dimensional Quality Filtering to ensure high visual and logical standards; (2) Synthesis and Consistency Verification to enhance reasoning details and ensure fidelity; and (3) Visual-Grounded Question Expansion to augment the dataset with diverse queries and real-world simulations.",
                "position": 431
            },
            {
                "img": "https://arxiv.org/html/2601.19798/x7.png",
                "caption": "Figure 8:Ablation Study on VLUAS Effectiveness.Comparative scaling curves of the model trained with (red) and without (blue) the proposed Unified Pre-training (VLUAS) strategy. The results indicate a critical divergence in scaling behavior: while the baseline model (wo/ VLUAS) exhibits clear signs of performance saturation during the later phases of Stage 3, the VLUAS-enhanced model maintains a superior performance trajectory with higher data efficiency. This consistent gap across both Stage 3 and Stage 4 empirically validates that incorporating visual supervision significantly alleviates data saturation and effectively raises the upper bound of multimodal capabilities.",
                "position": 496
            },
            {
                "img": "https://arxiv.org/html/2601.19798/x8.png",
                "caption": "(a)Scalability of Unified Pre-training.",
                "position": 515
            },
            {
                "img": "https://arxiv.org/html/2601.19798/x8.png",
                "caption": "(a)Scalability of Unified Pre-training.",
                "position": 518
            },
            {
                "img": "https://arxiv.org/html/2601.19798/x9.png",
                "caption": "(b)Power-Law Scaling Dynamics.",
                "position": 524
            },
            {
                "img": "https://arxiv.org/html/2601.19798/x10.png",
                "caption": "Figure 10:Visualization of Vision Token Representations. This figure compares the Principal Component Analysis (PCA) visualizations(Oquab et al.,2023)of the last-layer hidden states of vision tokens. We contrast Youtu-VL-4B (with vision token supervision) against three models without such supervision: Youtu-VL-4B w/o, Qwen2.5-VL-3B-Instruct(Yang et al.,2024), and Qwen3-VL-4B-Instruct(Bai et al.,2025). Leveraging vision token supervision, our model exhibits outstanding feature separation and visualization quality compared to VLMs lacking this supervision.",
                "position": 543
            }
        ]
    },
    {
        "header": "4Post-training",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.19798/x11.png",
                "caption": "Figure 11:The Architecture of our multi-stage reinforcement learning framework.",
                "position": 584
            }
        ]
    },
    {
        "header": "5Evaluation",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Contributions and Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AEvaluation Details",
        "images": []
    },
    {
        "header": "Appendix BExtended Experiments and Comparison",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.19798/cases/det1.png",
                "caption": "Figure 12:Qualitative example of object detection from the COCO dataset.",
                "position": 5309
            },
            {
                "img": "https://arxiv.org/html/2601.19798/cases/det2.jpeg",
                "caption": "Figure 13:Qualitative example of object detection from the COCO dataset.",
                "position": 5325
            },
            {
                "img": "https://arxiv.org/html/2601.19798/cases/det_all.png",
                "caption": "Figure 14:Qualitative example of open-world detection from the Objects365 dataset.",
                "position": 5339
            },
            {
                "img": "https://arxiv.org/html/2601.19798/cases/dnc.jpeg",
                "caption": "Figure 15:Qualitative example of detect-then-count from a photo taken by an author from Youtu-VL Team.",
                "position": 5360
            },
            {
                "img": "https://arxiv.org/html/2601.19798/cases/seg1.jpeg",
                "caption": "Figure 16:Qualitative example of semantic segmentation from the ADE20k dataset.",
                "position": 5386
            },
            {
                "img": "https://arxiv.org/html/2601.19798/cases/seg2.jpeg",
                "caption": "Figure 17:Qualitative example of semantic segmentation from the ADE20k dataset.",
                "position": 5400
            },
            {
                "img": "https://arxiv.org/html/2601.19798/cases/ground1.png",
                "caption": "Figure 18:Qualitative example of visual grounding from the RefCOCO dataset.",
                "position": 5414
            },
            {
                "img": "https://arxiv.org/html/2601.19798/cases/ground2.png",
                "caption": "Figure 19:Qualitative example of visual grounding from the RefCOCO dataset.",
                "position": 5428
            },
            {
                "img": "https://arxiv.org/html/2601.19798/cases/ref2.png",
                "caption": "Figure 20:Qualitative example of referring expression segmentation from the RefCOCO dataset.",
                "position": 5442
            },
            {
                "img": "https://arxiv.org/html/2601.19798/cases/depth.png",
                "caption": "Figure 21:Qualitative example of depth estimation from the NYUv2 dataset.",
                "position": 5466
            },
            {
                "img": "https://arxiv.org/html/2601.19798/cases/depth2.png",
                "caption": "Figure 22:Qualitative example of depth estimation from the NYUv2 dataset.",
                "position": 5480
            },
            {
                "img": "https://arxiv.org/html/2601.19798/cases/pose1.jpeg",
                "caption": "Figure 23:Qualitative example of human pose estimation from generated image (HY).",
                "position": 5494
            },
            {
                "img": "https://arxiv.org/html/2601.19798/cases/case_creative_writing.png",
                "caption": "Figure 24:Qualitative example of visual-language creative generation. The model perceives the visual aesthetics of the snowy sunrise and composes a coherent Chinese poem matching the atmosphere.",
                "position": 5510
            },
            {
                "img": "https://arxiv.org/html/2601.19798/cases/case_math_reasoning.png",
                "caption": "Figure 25:Qualitative example of mathematical reasoning. The model successfully extracts geometric constraints (isosceles triangle properties) from the image, formulates the algebraic equation, and solves for the specific side length step-by-step.",
                "position": 5529
            },
            {
                "img": "https://arxiv.org/html/2601.19798/cases/im1.png",
                "caption": "Figure 26:Qualitative example of chart understanding and multi-step reasoning. The model accurately interprets visual cues (color legends) to distinguish data categories, applies character-level text analysis to filter labels, and performs arithmetic operations on the extracted values to derive the answer.",
                "position": 5655
            },
            {
                "img": "https://arxiv.org/html/2601.19798/cases/86.png",
                "caption": "Figure 27:Qualitative example of fine-grained visual perception. The model accurately recognizes the number hidden in the Ishihara color test plate, demonstrating the capability to distinguish subtle chromatic differences and patterns to identify the target information.",
                "position": 5735
            },
            {
                "img": "https://arxiv.org/html/2601.19798/cases/latex.png",
                "caption": "Figure 28:Qualitative example of mathematical formula recognition. The model accurately parses the complex spatial structure (integrals, fractions, subscripts) and specific symbols from the image, translating them into syntactically correct LaTeX code.",
                "position": 5750
            },
            {
                "img": "https://arxiv.org/html/2601.19798/cases/gui_case.png",
                "caption": "Figure 29:Qualitative example of GUI agent performance. The ground-truth target is highlighted in green, and the model’s click location is shown in red. The model demonstrates robust, precise interaction with interface elements across multiple windows. The screenshot is selected from the ScreenSpot Pro[102]evaluation set.",
                "position": 5769
            },
            {
                "img": "https://arxiv.org/html/2601.19798/cases/gui_case_4.png",
                "caption": "Figure 30:Qualitative example of GUI agent performance. This case demonstrates the model’s ability to interpret instructions within a specific context and execute precise edits accordingly in the editor environment. The screenshot is selected from the ScreenSpot Pro[102]evaluation set.",
                "position": 5786
            }
        ]
    },
    {
        "header": "Appendix CCase Study",
        "images": []
    }
]