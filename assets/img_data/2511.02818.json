[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02818/Figs/logo1.png",
                "caption": "",
                "position": 122
            }
        ]
    },
    {
        "header": "1  Introduction",
        "images": []
    },
    {
        "header": "2  Related Work",
        "images": []
    },
    {
        "header": "3  Proposed Approach: Orion-MSP",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02818/Figs/Archi2.png",
                "caption": "Figure 1:An overview of Orion-MSP architecture. First, column-wise embedding transforms input table into embedding vectorsEE. Next, multi-scale sparse row interaction prepends learnable[CLS]and[GLOBAL]tokens toEE, processes features at multiple granularities (scales 1, 4, and 16) with sparse attention transformers, and aggregates[CLS]outputs across scales to yield row embeddingsHH. Cross-component Perceiver memory enables bidirectional communication: training rows write to latent memory, which all rows read for enhanced representationsRR. Finally, ICL predicts test labels fromRRin a single forward pass.",
                "position": 228
            },
            {
                "img": "https://arxiv.org/html/2511.02818/Figs/sparse_attention.png",
                "caption": "Figure 2:Building blocks of the attention mechanism used in Orion-MSP. White color indicates absence of attention. (a) special attention includesC​L​S=4CLS=4and global attention withG​B=4GB=4, (b) sliding window attention withw=8w=8, (c) random attention withr=2r=2, (d) the combined row representation of Orion-MSP model.",
                "position": 524
            }
        ]
    },
    {
        "header": "4  Experimental Evaluation",
        "images": []
    },
    {
        "header": "5  Conclusion",
        "images": []
    },
    {
        "header": "Appendix APretraining and Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02818/Figs/lr1.png",
                "caption": "(a)Cosine decay for stage 1",
                "position": 2790
            },
            {
                "img": "https://arxiv.org/html/2511.02818/Figs/lr1.png",
                "caption": "(a)Cosine decay for stage 1",
                "position": 2793
            },
            {
                "img": "https://arxiv.org/html/2511.02818/Figs/lr2.png",
                "caption": "(b)Polynomial decay for stage 2",
                "position": 2798
            },
            {
                "img": "https://arxiv.org/html/2511.02818/Figs/lr3.png",
                "caption": "(c)Constant lr for stage 3",
                "position": 2803
            }
        ]
    },
    {
        "header": "Appendix BFurther Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02818/Figs/relative_acc_improvement_over_talent.png",
                "caption": "(a)Relative accuracy improvement over XGBoost on TALENT Benchmark",
                "position": 3144
            },
            {
                "img": "https://arxiv.org/html/2511.02818/Figs/relative_acc_improvement_over_talent.png",
                "caption": "(a)Relative accuracy improvement over XGBoost on TALENT Benchmark",
                "position": 3147
            },
            {
                "img": "https://arxiv.org/html/2511.02818/Figs/relative_acc_improvement_over_tabzilla.png",
                "caption": "(b)Relative accuracy improvement over XGBoost on TabZilla Benchmark",
                "position": 3153
            },
            {
                "img": "https://arxiv.org/html/2511.02818/Figs/relative_acc_improvement_over_openml.png",
                "caption": "(c)Relative accuracy improvement over XGBoost on OPENML-CC18 Benchmark",
                "position": 3159
            },
            {
                "img": "https://arxiv.org/html/2511.02818/Figs/accuracy_ranking_talent.png",
                "caption": "(a)Relative accuracy improvement over XGBoost on TALENT Benchmark",
                "position": 3169
            },
            {
                "img": "https://arxiv.org/html/2511.02818/Figs/accuracy_ranking_talent.png",
                "caption": "(a)Relative accuracy improvement over XGBoost on TALENT Benchmark",
                "position": 3172
            },
            {
                "img": "https://arxiv.org/html/2511.02818/Figs/accuracy_ranking_tabzilla.png",
                "caption": "(b)Relative accuracy improvement over XGBoost on TabZilla Benchmark",
                "position": 3178
            },
            {
                "img": "https://arxiv.org/html/2511.02818/Figs/accuracy_ranking_openml-cc18.png",
                "caption": "(c)Relative accuracy improvement over XGBoost on OPENML-CC18 Benchmark",
                "position": 3184
            },
            {
                "img": "https://arxiv.org/html/2511.02818/Figs/benchmark_scatter.png",
                "caption": "Figure 6:Column and row distribution of the evaluated datasets.",
                "position": 3201
            }
        ]
    },
    {
        "header": "Appendix CDatasets",
        "images": []
    }
]