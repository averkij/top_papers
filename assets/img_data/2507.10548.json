[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.10548/x1.png",
                "caption": "Figure 1:EmbRACE-3Kdataset. The dataset contains over 3k language-guided tasks and 26k decision steps set in diverse, photorealistic environments. Each task involves high-level natural language instructions, grounded actions (e.g., move, turn, look, interact), egocentric visual observations, and step-wise reasoning. Agents interpret visual inputs and follow instructions to execute multi-step decision trajectories, with each step annotated with natural language rationalesâ€”forming a coherent and interpretable decision process over a long horizon that involves spatial reasoning.",
                "position": 101
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Pilot Study",
        "images": []
    },
    {
        "header": "4Data Collection and Benchmark Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.10548/x2.png",
                "caption": "Figure 2:Multi-stage Embodied Task Data Collection Pipeline.\nTheEmbRACE-3Kdataset is built in four stages: (1) sampling diverse 6-DoF agent poses with ego views in virtual environments, (2) generating grounded task instructions using Gemini, (3) collecting human demonstrations, and (4) annotating each action with step-wise natural language reasoning to explain agent decisions and enhance interpretability.",
                "position": 283
            },
            {
                "img": "https://arxiv.org/html/2507.10548/x3.png",
                "caption": "Figure 3:The Token Number and Word-Cloud Distribution ofEmbRACE-3K.\n(a)Token number distribution of reasoning and the length distribution of action trajectories.\n(b)The word clouds of task instructions and agent thinking processes inEmbRACE-3K.",
                "position": 338
            },
            {
                "img": "https://arxiv.org/html/2507.10548/x4.png",
                "caption": "Figure 4:Distribution of task types.",
                "position": 359
            },
            {
                "img": "https://arxiv.org/html/2507.10548/x5.png",
                "caption": "Figure 5:Two-stage Training framework for Embodied Agent onEmbRACE-3K. (a) SFT training pipeline for agent in open-environment; (b) GRPO training pipeline for agent in open-environment",
                "position": 362
            }
        ]
    },
    {
        "header": "5Reasoning Training Pipeline",
        "images": []
    },
    {
        "header": "6Experiments of Embodied Agent",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]