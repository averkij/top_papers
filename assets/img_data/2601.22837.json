[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22837/fig/motivation1.png",
                "caption": "Figure 1:Illustration of our motivation.\n(a) Existingdisordered tokenizationoverlooks the essential requirement of relational modeling in the generation stage, as it fails to introduce any token dependency constraints during tokenization, resulting in a gap between the two stages of image generation.\n(b) Our approach proposesnative visual tokenization, which considers not only reconstruction quality but also imposes relational constraints during tokenization, thereby coupling the two stages of image generation.",
                "position": 88
            },
            {
                "img": "https://arxiv.org/html/2601.22837/x1.png",
                "caption": "Figure 2:The overview of our NativeTok framework. (a) In Meta Image Transformer (MIT), the image information is initially modeled by a Pixel Transformer, then compresses the image into the latent space. During the subsequent generation process, the latent space information of the image remains locked. (b) During the token sequence generation process, we define Mixture of Causal Expert Transformer (MoCET). Theit​hi^{th}expert transformer block is responsible for generating theit​hi^{th}token. In each generation step, we concatenate the locked latent space image information, all previously generated tokens, and the current mask token, and feed them into the corresponding expert transformer. Once a token is generated, it remains fixed. (c) Once all tokens are generated, they are fed into the decoder to reconstruct the image.",
                "position": 108
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22837/x2.png",
                "caption": "Figure 3:Hierarchical Native Training: We freeze the Meta Image Transformer and existing experts, training only newly added experts initialized with reused weights. This reduces training costs while ensuring the new experts inherit prior modeling capabilities.",
                "position": 217
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22837/x3.png",
                "caption": "Figure 4:In (a), the top and bottom rows correspond to the original images and their reconstructed results. In (b), we showcase examples of generated images.",
                "position": 245
            },
            {
                "img": "https://arxiv.org/html/2601.22837/x4.png",
                "caption": "Figure 5:Visualization:In the right-hand figure, the x-axis denotes the token position index, and the y-axis represents the corresponding probability values.\nWhen modifying adjacent token representations, we compare how the probability distribution of the next token changes accordingly.",
                "position": 651
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "NativeTok: Native Visual Tokenization for Improved Image Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22837/x5.png",
                "caption": "Figure 6:Reconstructions during the token generation process where the token count increases from 0 to 32.",
                "position": 1093
            },
            {
                "img": "https://arxiv.org/html/2601.22837/x6.png",
                "caption": "Figure 7:We provide a demonstration of reconstruction results. From left to right: the original image, reconstruction image by NativeTok128, reconstruction image by NativeTok64, and reconstruction image by NativeTok32.",
                "position": 1102
            },
            {
                "img": "https://arxiv.org/html/2601.22837/x7.png",
                "caption": "Figure 8:Generation examples",
                "position": 1105
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]