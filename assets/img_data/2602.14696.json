[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Targeted Instruction Selection",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.14696/x1.png",
                "caption": "Figure 1:Disentangled view of targeted instruction selection.First, the query set (stars) and candidate pool (dots) are encoded as data representations. Then, for a given budget, using the data representations for the query and candidates, we perform targeted selection (denoted by the dotted line) using a selection algorithm such as greedy round-robin.",
                "position": 267
            }
        ]
    },
    {
        "header": "3A Disentangled View of Instruction Selection",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.14696/x2.png",
                "caption": "Figure 2:Query loss vs. subset-query distance quantile.We stratify candidates into 10 distance quantiles (1 = closest, 10 = farthest) using each representation, select 500 examples per quantile using the RR selection algorithm, and train the Llama 2 7B model. We report query-set cross-entropy loss and Spearman correlation per target task. LESS (RR) exhibits a strong monotonic increase in loss with distance (high positive Spearman correlation), whereas RDS+ (RR) and EMBED (RR) show weak or inconsistent correlations.",
                "position": 367
            },
            {
                "img": "https://arxiv.org/html/2602.14696/x3.png",
                "caption": "Figure 3:Downstream performance vs. subset-query distance quantile.Using the same quantile construction and training protocol as Figure2, we evaluate downstream task performance across distance quantiles and report Spearman correlation per target task. LESS (RR) shows a strong negative correlation across most target tasks, while RDS+ (RR) and EMBED (RR) exhibit weaker, less consistent trends.",
                "position": 370
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Experiments and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.14696/x4.png",
                "caption": "Figure 4:Query loss vs. budget for different data representations (fixed selection algorithm).Using greedy round-robin selection and the query-candidate pool similarity, we select subsets of sizeB∈{500,1000,2500,5000,10000}B\\in\\{500,1000,2500,5000,10000\\}, train Llama 2 7B on them, and report average cross entropy loss averaged across three seeds and the standard error. Random averages over three uniformly sampled subsets from the candidate pool. LESS (RR) achieves the lowest loss across target tasks, while RDS+ (RR) and EMBED (RR) can underperform Random at larger budgets.",
                "position": 428
            },
            {
                "img": "https://arxiv.org/html/2602.14696/x5.png",
                "caption": "Figure 5:Downstream performance vs. budget for different data representations (fixed selection algorithm).With the same greedy round-robin selection and budgets as Figure4, we report the downstream performance for different data representations averaged across three random seeds and the standard error. LESS (RR) performs best on BBH, TyDiQA, and MMLU-Pro, whereas RDS+ (RR) performs the best on GSM8K and is competitive with Random on Codex.",
                "position": 431
            },
            {
                "img": "https://arxiv.org/html/2602.14696/x6.png",
                "caption": "Figure 6:Query loss vs. budget for different selection algorithms (fixed data representation).Using LESS representations and the query-candidate pool cosine similarity (or distance), we select subsets of sizeB∈{500,1000,2500,5000,10000}B\\in\\{500,1000,2500,5000,10000\\}with each selection algorithm, train Llama 2 7B on them, and report average cross entropy loss on the query set averaged across three seeds and the standard error. Random averages over three uniformly sampled subsets from the candidate pool. UOT achieves the lowest loss on three of the five datasets and remains competitive on the others, while DG often underperforms, yielding the highest loss on three datasets.",
                "position": 488
            },
            {
                "img": "https://arxiv.org/html/2602.14696/x7.png",
                "caption": "Figure 7:Downstream performance vs. budget for different selection algorithms (fixed data representation).With the same data representation and the budgets as Figure6, we report downstream performance for different selection algorithms averaged across three seeds and the standard error. RR tends to perform best at smaller budgets, whereas UOT and KNN-KDE perform better at larger budgets; DG consistently underperforms across three of the five datasets.",
                "position": 491
            }
        ]
    },
    {
        "header": "6A Unifying View: Instruction Selection as Set Distance Minimization",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.14696/x8.png",
                "caption": "Figure 8:Convergence of LESS variants toward random sampling as budget increases.We plot the loss gap between random sampling and the LESS variants as a function of budgetBB(log-scale) on MMLU-Pro.\nThe gray reference line indicates theB−1/dB^{-1/d}decay predicted by Theorem6.2.\nWe report the average difference in loss across one seed of LESS and three randomly sampled multisets of sizeBB, along with the standard error.\nFor visual comparison of decay rates, we apply a constant offset to each LESS curve so that all methods start atB0−1/dB_{0}^{-1/d}whereB0=500B_{0}=500andd=8192d=8192.",
                "position": 677
            }
        ]
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AFragmented Literature on Targeted Instruction Selection",
        "images": []
    },
    {
        "header": "Appendix BTarget Tasks",
        "images": []
    },
    {
        "header": "Appendix CBackground on Optimal Transport",
        "images": []
    },
    {
        "header": "Appendix DDetails on LESS",
        "images": []
    },
    {
        "header": "Appendix EImplementation Details: Data Representation",
        "images": []
    },
    {
        "header": "Appendix FImplementation Details: Selection Algorithms",
        "images": []
    },
    {
        "header": "Appendix GTraining Details and Hyperparameters",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.14696/x9.png",
                "caption": "Figure 9:Query loss across subset-query distance sub-quantiles and Spearman correlation.We further stratify the first distance quantile from Section5.1into 10 sub-quantiles (1 = closest, 10 = farthest), select 500 examples per sub-quantile, and train the Llama 2 7B model. We report loss on the query set and Spearman correlation per dataset. LESS (RR) maintains a strong monotonic increase in loss with distance (high Spearman correlation), whereas RDS+ (RR) and EMBED (RR) show weak or inconsistent correlations.",
                "position": 2263
            },
            {
                "img": "https://arxiv.org/html/2602.14696/x10.png",
                "caption": "Figure 10:Downstream performance across subset-query distance sub-quantiles and Spearman correlation.Using the same sub-quantile construction and training protocol as Figure9, we evaluate downstream task performance across sub-quantiles and report Spearman correlation per dataset (more negative is better).\nLESS (RR) shows a stronger negative correlation on average, but the performance differences are small, suggesting that many subsets within the closest quantile result in similar downstream performance.",
                "position": 2266
            }
        ]
    },
    {
        "header": "Appendix HFine-Grained Stratification of the Nearest Distance Quantile",
        "images": []
    },
    {
        "header": "Appendix IDifferences between Selected Subsets",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.14696/x11.png",
                "caption": "Figure 11:Average token length of the query set and the selected subsets with different data representations.We find that LESS (RR) is biased towards shorter sequences, whereas RDS+ (RR) and EMBED (RR) select subsets with longer sequences.",
                "position": 2299
            },
            {
                "img": "https://arxiv.org/html/2602.14696/x12.png",
                "caption": "Figure 12:Jaccard index between selected subsets created using different data representations.We find that model-based representations (RDS+ and EMBED) have a higher Jaccard index compared to LESS.",
                "position": 2302
            }
        ]
    },
    {
        "header": "Appendix JCheaper Proxies for LESS",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.14696/x13.png",
                "caption": "Figure 13:Cheaper proxies for LESS (fixed selection algorithm).With the same greedy round-robin selection procedure and the budgets from Section5.2, we report downstream performance of Llama 2 7B when LESS representations are computed using proxy models, averaged across three seeds and the standard error.\nSmolLM-135M and SmolLM2-135M consistently match or outperform the Random baseline across target tasks, whereas Pythia-160M often matches Random on several tasks.\nLlama 3.2 3B matches or outperforms LESS computed with Llama 2 7B on multiple target tasks, suggesting a trade-off between proxy and target model size when approximating instruction selection.",
                "position": 2331
            }
        ]
    },
    {
        "header": "Appendix KKNN-Uniform and KNN-KDE with L2 Distance",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.14696/x14.png",
                "caption": "Figure 14:KNN-Uniform and KNN-KDE with L2 distance (fixed data representation).With the same Llama 2 7B LESS representations and the budgets from Section5.3, we report downstream performance for KNN-Uniform and KNN-KDE when distances are computed with L2 (followingLiu et al. (2024b)), averaged across three seeds and the standard error.\nWe compare against cosine distance variants and find similar performance trends across budgets.",
                "position": 2374
            }
        ]
    },
    {
        "header": "Appendix LProofs",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.14696/x15.png",
                "caption": "Figure 15:Ablation experiments with Llama 3.2 3B.",
                "position": 2721
            },
            {
                "img": "https://arxiv.org/html/2602.14696/x16.png",
                "caption": "",
                "position": 2725
            },
            {
                "img": "https://arxiv.org/html/2602.14696/x17.png",
                "caption": "",
                "position": 2727
            },
            {
                "img": "https://arxiv.org/html/2602.14696/x18.png",
                "caption": "",
                "position": 2729
            },
            {
                "img": "https://arxiv.org/html/2602.14696/x19.png",
                "caption": "Figure 16:Ablation experiments with SmolLM3 3B.",
                "position": 2733
            },
            {
                "img": "https://arxiv.org/html/2602.14696/x20.png",
                "caption": "",
                "position": 2737
            },
            {
                "img": "https://arxiv.org/html/2602.14696/x21.png",
                "caption": "",
                "position": 2739
            },
            {
                "img": "https://arxiv.org/html/2602.14696/x22.png",
                "caption": "",
                "position": 2741
            },
            {
                "img": "https://arxiv.org/html/2602.14696/x23.png",
                "caption": "Figure 17:Ablation experiments with Qwen3 4B.",
                "position": 2745
            },
            {
                "img": "https://arxiv.org/html/2602.14696/x24.png",
                "caption": "",
                "position": 2749
            },
            {
                "img": "https://arxiv.org/html/2602.14696/x25.png",
                "caption": "",
                "position": 2751
            },
            {
                "img": "https://arxiv.org/html/2602.14696/x26.png",
                "caption": "",
                "position": 2753
            },
            {
                "img": "https://arxiv.org/html/2602.14696/x27.png",
                "caption": "Figure 18:Ablation experiments with Olmo 3 7B.",
                "position": 2757
            },
            {
                "img": "https://arxiv.org/html/2602.14696/x28.png",
                "caption": "",
                "position": 2761
            },
            {
                "img": "https://arxiv.org/html/2602.14696/x29.png",
                "caption": "",
                "position": 2763
            },
            {
                "img": "https://arxiv.org/html/2602.14696/x30.png",
                "caption": "",
                "position": 2765
            }
        ]
    },
    {
        "header": "Appendix MAblations",
        "images": []
    }
]