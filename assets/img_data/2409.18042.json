[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.18042/extracted/5961461/figure/icon_256.png",
                "caption": "",
                "position": 190
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.18042/x1.png",
                "caption": "Figure 1:Model architecture ofEMOVA.The vision encoder extracts continuous visual features, which are projected into the text embedding space as visual tokens, while the input speech is encoded and quantized into discrete units.\nGiven the omni-modal inputs,EMOVAcan generate both textual and speech responses with vivid emotional controls.\nCheck Sec.3for more architectural details.",
                "position": 282
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Architecture",
        "images": []
    },
    {
        "header": "4Omni-modal Alignment and Instruction Tuning",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.18042/x2.png",
                "caption": "Figure 2:Comparison between omni-modal alignment paradigms.1)Jointtraining achieves consistent improvements overVLandSpeech, suggesting omni-modal alignment can be beneficial across modalities.\n2)Jointtraining outperforms bothVL→→\\rightarrow→SpeechandSpeech→→\\rightarrow→VL, revealing that joint training is more superior and efficient than sequential training.\n3)Jointis superior toJoint-entangled, highlighting the effectiveness of the semantic-acoustic disentanglement.",
                "position": 567
            },
            {
                "img": "https://arxiv.org/html/2409.18042/x3.png",
                "caption": "Figure 3:Demonstration of the omni-modal instruction tuning.1) To empower emotional spoken dialogues,EMOVAis trained to explicitly select the speech style labels (e.g., emotions and pitches) with output speech units.\n2) For the ease of parsing, data components are arranged inJSONformat.",
                "position": 617
            },
            {
                "img": "https://arxiv.org/html/2409.18042/x4.png",
                "caption": "Figure 4:Overview of the data composition forEMOVAomni-modal instruction tuning.(Left) Distribution of instruction data across categories, with the outer circle representing overall categories and the inner circle depicting subset distributions.\n(Right) Quantitative breakdown of data sources.",
                "position": 658
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.18042/x5.png",
                "caption": "Figure 5:EMOVAengages in omni-modal spoken dialogue withstructural data understanding.",
                "position": 1062
            },
            {
                "img": "https://arxiv.org/html/2409.18042/x6.png",
                "caption": "Figure 6:EMOVAengages in omni-modalemotional spoken dialogueexpressing sadness.",
                "position": 1065
            },
            {
                "img": "https://arxiv.org/html/2409.18042/x7.png",
                "caption": "Figure 7:Confusion matrix between the generated and recognized emotions.The emotions generated by our U2S detokenizer are recognized with high probability.",
                "position": 1131
            },
            {
                "img": "https://arxiv.org/html/2409.18042/x8.png",
                "caption": "Figure 8:EMOVAengages inspoken dialogue with a cheerful tone.",
                "position": 1208
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AMore on Speech Tokenizer",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.18042/x9.png",
                "caption": "Figure 9:U2S detokenizer with style control.",
                "position": 2458
            }
        ]
    },
    {
        "header": "Appendix BMore on Omni-modality",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.18042/x10.png",
                "caption": "Figure 10:Overview ofEMOVAomni-modal alignment data composition.",
                "position": 2527
            }
        ]
    },
    {
        "header": "Appendix CMore on Benchmark Evaluation",
        "images": []
    },
    {
        "header": "Appendix DMore on Evaluation of Speech-Language Capabilities",
        "images": []
    },
    {
        "header": "Appendix EMore on Vision-language",
        "images": []
    },
    {
        "header": "Appendix FLimitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.18042/x11.png",
                "caption": "Figure 11:EMOVAexhibitsadvanced vision-language abilities in humor sense.",
                "position": 3069
            },
            {
                "img": "https://arxiv.org/html/2409.18042/x12.png",
                "caption": "Figure 12:EMOVAexhibitsadvanced vision-language abilities in numerical calculations.",
                "position": 3072
            },
            {
                "img": "https://arxiv.org/html/2409.18042/x13.png",
                "caption": "Figure 13:EMOVAexhibitsadvanced vision-language abilities in coding.",
                "position": 3075
            },
            {
                "img": "https://arxiv.org/html/2409.18042/x14.png",
                "caption": "Figure 14:EMOVAengages inemotional spoken dialogue.",
                "position": 3078
            },
            {
                "img": "https://arxiv.org/html/2409.18042/x15.png",
                "caption": "Figure 15:EMOVAexhibitsadvanced vision-language abilities in math.",
                "position": 3081
            },
            {
                "img": "https://arxiv.org/html/2409.18042/x16.png",
                "caption": "Figure 16:EMOVAengages inomni-modal spoken dialogue with structural data understanding (i.e., poster).",
                "position": 3084
            },
            {
                "img": "https://arxiv.org/html/2409.18042/x17.png",
                "caption": "Figure 17:EMOVAengages inomni-modal emotional spoken dialogue.",
                "position": 3087
            },
            {
                "img": "https://arxiv.org/html/2409.18042/x18.png",
                "caption": "Figure 18:EMOVAengages inomni-modal spoken dialogue with structural data understanding (i.e., movie posters).",
                "position": 3090
            },
            {
                "img": "https://arxiv.org/html/2409.18042/x19.png",
                "caption": "Figure 19:EMOVAengages inomni-modal spoken dialogue with structural data understanding (i.e., résumé).",
                "position": 3093
            },
            {
                "img": "https://arxiv.org/html/2409.18042/x20.png",
                "caption": "Figure 20:EMOVAengages inomni-modal spoken dialogue with structural data understanding (i.e., chart).",
                "position": 3096
            },
            {
                "img": "https://arxiv.org/html/2409.18042/x21.png",
                "caption": "Figure 21:EMOVAengages inomni-modal spoken dialogue with structural data understanding (i.e., website).",
                "position": 3099
            },
            {
                "img": "https://arxiv.org/html/2409.18042/x22.png",
                "caption": "Figure 22:Promptused to obtainstyle labels of the speech instruction dataset.",
                "position": 3102
            },
            {
                "img": "https://arxiv.org/html/2409.18042/x23.png",
                "caption": "Figure 23:Promptused to obtainUnit-Input-Text-Output ScoreandText-Input-Text-Output Score.",
                "position": 3105
            },
            {
                "img": "https://arxiv.org/html/2409.18042/x24.png",
                "caption": "Figure 24:Promptused to obtainEnd-to-end Spoken Dialogue Score.",
                "position": 3108
            },
            {
                "img": "https://arxiv.org/html/2409.18042/x25.png",
                "caption": "Figure 25:Promptused to obtainClassification Accuracy of Style Label.",
                "position": 3111
            }
        ]
    },
    {
        "header": "Appendix GQualitative Results",
        "images": []
    }
]