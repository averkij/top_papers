[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.04946/maindiag.png",
                "caption": "Figure 1:Non-streaming and streaming training procedures for REINAStream. For non-streaming training we use a trainable MT encoder to train on parallel NMT data. During streaming training we a) pass a full audio and truncated audio through the model, b) compute the cross-entropy (CE) loss of each, c) predict a policy using the policy network on top of the partial-audio output of the decoder, and finally d) calculate the REINA loss using the CE terms and policy predictions.",
                "position": 144
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.04946/mustc_graphs/mustc_all_plots.png",
                "caption": "Figure 2:Average Lagging (AL) vs. BLEU score on MUST-C. Horizontal lines represent non-streaming performance.",
                "position": 557
            },
            {
                "img": "https://arxiv.org/html/2508.04946/mid_paper_graph.png",
                "caption": "Figure 3:AL/BLEU curve on Esâ†’\\rightarrowEn split of the CVSS-C dataset. We report ASR-BLEU only for StreamSpeech.",
                "position": 876
            }
        ]
    },
    {
        "header": "5Conclusion and Future Work",
        "images": []
    }
]