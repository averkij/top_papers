[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19852/x1.png",
                "caption": "Figure 1:We present Radial Attention, a sparse attention mechanism with𝒪⁢(n⁢log⁡n)𝒪𝑛𝑛\\mathcal{O}(n\\log n)caligraphic_O ( italic_n roman_log italic_n )computational complexity. Radial Attention accelerates pre-trained HunyuanVideokong2024hunyuanvideoby 1.9× at its default video length while maintaining comparable video quality. When generating 4× longer videos, it reduces tuning costs by up to 4.4× and speeds up inference by up to 3.7× versus dense attention.",
                "position": 93
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19852/x2.png",
                "caption": "Figure 2:Radial Attention reduces the computational complexity of attention from𝒪⁢(n2)𝒪superscript𝑛2\\mathcal{O}(n^{2})caligraphic_O ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )to𝒪⁢(n⁢log⁡n)𝒪𝑛𝑛\\mathcal{O}(n\\log n)caligraphic_O ( italic_n roman_log italic_n ). When generating a 500-frame 720p video with HunyuanVideo, it reduces the attention computation by 9×, achieves 3.7× speedup, and saves 4.6× tuning costs.",
                "position": 101
            },
            {
                "img": "https://arxiv.org/html/2506.19852/x3.png",
                "caption": "Figure 3:Attention pipelines of SVGxi2025sparseand our Radial Attention. Softmax is omitted for clarity.(a)SVG dynamically selects either a spatial or temporal attention for each head to speed up inference. However, it does not overcome the original model’s length limitation and cannot be trained on unseen distributions like longer videos.(b)Our Radial Attention uses a static mask that unifies spatial and temporal attention with𝒪⁢(n⁢log⁡n)𝒪𝑛𝑛\\mathcal{O}(n\\log n)caligraphic_O ( italic_n roman_log italic_n )computational complexity. This static design enables efficient longer-video adaptation.",
                "position": 121
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19852/x4.png",
                "caption": "Figure 4:(a)Example spatial and temporal attention maps from HunyuanVideo (defined in Section4.1).(b)Attention score distributions.\n(b1): Average score between tokens at the same spatial location decreases with temporal distance (b2): Average attention score within a frame decreases with spatial distance.SpatialandTemporal Attentionrefer to the distributions derived from the corresponding maps in (a).Averagemeans averaging over multiple random maps and diffusion steps. The plots indicate that spatial attention shows a high temporal decay and relatively low spatial decay, while temporal attention exhibits the opposite.",
                "position": 189
            },
            {
                "img": "https://arxiv.org/html/2506.19852/x5.png",
                "caption": "Figure 5:(a)The compute density pattern. The attention map is divided into2⁢⌈log2⁡(max⁡(f,2))⌉−12subscript2𝑓212\\lceil\\log_{2}(\\max(f,2))\\rceil-12 ⌈ roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( roman_max ( italic_f , 2 ) ) ⌉ - 1bands (here, the number of framesf=12𝑓12f=12italic_f = 12) based on the temporal distance between tokens. The central band has full compute density, while each successive outer band has half the density of the previous one. Except for band±1plus-or-minus1\\pm 1± 1, each band also doubles the diagonal width of its predecessor.(b)The corresponding attention mask for (a). The compute density is reflected in the compute diagonal width of each frame-to-frame block. When the diagonal width drops below 1, we reduce the frequency of diagonals. We additionally add an attention sink.(c)An example mask used in HunyuanVideo, illustrating the final sparsity pattern in practice.",
                "position": 233
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19852/x6.png",
                "caption": "Figure 6:Examples of videos generated by Radial Attention and the original Wan2.1-14B in the default video length. Radial Attention mirrors the video quality of the original model.",
                "position": 1198
            },
            {
                "img": "https://arxiv.org/html/2506.19852/x7.png",
                "caption": "Figure 7:Visual comparison of HunyuanVideo with 4× length extension (509 frames). LoRA fine-tuned models using Radial Attention achieve higher vision rewards, outperforming dense attention baselines while providing significant speedups.",
                "position": 1214
            },
            {
                "img": "https://arxiv.org/html/2506.19852/x8.png",
                "caption": "Figure 8:Radial Attention LoRA is compatible with existing style LoRAs. On HunyuanVideo, it extends video length by 4× while maintaining a vision reward comparable to that of the original-length LoRA video.",
                "position": 1229
            },
            {
                "img": "https://arxiv.org/html/2506.19852/x9.png",
                "caption": "Figure 9:(a)Radial Attention outperforms dense attention in generation quality. When combined with LoRA, it further improve the quality while significantly reducing training costs.(b)We model the attention decay curves using the exponential functiony=exp⁡(−a⁢x+b)𝑦𝑎𝑥𝑏y=\\exp{(-ax+b)}italic_y = roman_exp ( - italic_a italic_x + italic_b ). It fits the data well, achievingR2>0.985superscript𝑅20.985R^{2}>0.985italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT > 0.985.",
                "position": 1245
            }
        ]
    },
    {
        "header": "6Conclusion & Discussion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Appendix ADerivations and Proofs",
        "images": []
    },
    {
        "header": "Appendix BAdditional Implementation Details",
        "images": []
    },
    {
        "header": "Appendix CVisualization of the generated videos",
        "images": []
    },
    {
        "header": "Appendix DBroader Impacts",
        "images": []
    },
    {
        "header": "Appendix ELicense",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19852/x10.png",
                "caption": "Figure A:Comparison of Dense Attention and Radial Attention on HunyuanVideo Text-to-Video generation at the default length (5 seconds, 117 frames, 768p).",
                "position": 1685
            },
            {
                "img": "https://arxiv.org/html/2506.19852/x11.png",
                "caption": "Figure B:Comparison of Dense Attention and Radial Attention on Wan2.1-14B Text-to-Video generation at the default length (4 seconds, 69 frames, 768p).",
                "position": 1691
            },
            {
                "img": "https://arxiv.org/html/2506.19852/x12.png",
                "caption": "Figure C:Comparison of all baselines and Radial Attention at 4× default length (21 seconds, 509 frames) Text-to-Video video generation from HunyuanVideo. Radial Attention achieves the best Vision Reward score with good visual quality and consistency. In contrast, Original HunyuanVideo and RIFLEx generate blurred videos with poor visual quality. Temporal Head generates distorting figures. Spatial Head, Long LoRA, and PowerAttention generate temporally inconsistent video backgrounds. Dense Attention generates less dynamic videos.",
                "position": 1697
            },
            {
                "img": "https://arxiv.org/html/2506.19852/x13.png",
                "caption": "Figure D:Comparison of all baselines and Radial Attention at 4× default length (22 seconds, 667 frames) Text-to-Video video generation from Mochi 1. Radial Attention achieves the highest Vision Reward score because it has excellent visual quality and consistency. In contrast, Original Mochi 1 generates blurred videos with poor visual quality. Spatial Head, Temporal Head, Long LoRA, PowerAttention, and Dense Attention generate videos with either inconsistent backgrounds or inconsistent figures.",
                "position": 1703
            },
            {
                "img": "https://arxiv.org/html/2506.19852/x14.png",
                "caption": "Figure E:Comparison of all baselines and Radial Attention at 2× default length (10 seconds, 161 frames) Text-to-Video video generation from Wan2.1-14B. Radial Attention achieves the highest Vision Reward score because it has excellent visual quality and consistency. In contrast, Original Wan2.1-14B generates blurred videos with poor visual quality. Dense Attention generates videos with inconsistent figures.",
                "position": 1709
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]