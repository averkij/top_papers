[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09279/x1.png",
                "caption": "",
                "position": 114
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Benchmarking Existing Alternatives",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09279/x2.png",
                "caption": "Figure 2:Overall pipeline of our proposed Cockatiel.\nOur training pipeline successfully ensemble both the advantages of base models and human preferences, yielding our Cockatiel captioner series.\nThrough the ensembling synthetic and human preferenced training, Cockatiel-13B achieves significant VDC performance while being preferred by humans.",
                "position": 556
            }
        ]
    },
    {
        "header": "3The Proposed Cockatiel",
        "images": []
    },
    {
        "header": "4Results & Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09279/x3.png",
                "caption": "Figure 3:Qualitative comparison between Cockatiel-13B and the current sota VDC models.\nFor a detailed comparison between Cockatiel-13B and all leading VDC models, please refer to the supplementary files.\nThe caption content that is exclusively captured by our model, captured by our model and other baselines, or misaligned with the detailed visual elements in the videos are emphasized using red, yellow and green backgrounds.",
                "position": 728
            },
            {
                "img": "https://arxiv.org/html/2503.09279/x4.png",
                "caption": "Figure 5:Human evaluation results. Our method, Cockatiel-13B, is obviously more human-preferred compared to baselines.",
                "position": 757
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    },
    {
        "header": "Appendix BLimitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09279/x5.png",
                "caption": "Figure 6:A snapshot of our annotation user interface, where the whole annotation procedure is carried on.\nIn each annotation task, the annotators are present with a video on the left, a caption supposed to align with it on the top, and a question related to the detailed video-caption alignment on the right of the user interface.\n“O”, “F”, “A”, “C”, “B” are abbreviations for “Object”, “object Feature”, “object Action”, “Camera” and “Background”, respectively.\nNotably, our user interface is highly praised by our annotators for its user-friendly and intuitive design, which ensures both the quality and quantity of the annotated data.",
                "position": 1640
            }
        ]
    },
    {
        "header": "Appendix CApplications",
        "images": []
    },
    {
        "header": "Appendix DRelated Work",
        "images": []
    },
    {
        "header": "Appendix EMore Annotation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09279/x6.png",
                "caption": "Figure 7:The training pipeline of our scorer.\nOur selection policy is critical as its performance determines which knowledge and strengths is ensembled into Cockatiel-13B.\nAs a consequence, since our aim is to infuse Cockatiel-13B all the strengths of the base models and human preferences, we devise a selection policy with threshold setting based on our human-aligned caption quality scorer.\nTo obtain our caption quality scorer, we need human-annotated data on it or off-the-shelf models.\nSince no publicly available dataset nor model suits this need, we build them on our own, as demonstrated in this figure.",
                "position": 1723
            },
            {
                "img": "https://arxiv.org/html/2503.09279/x7.png",
                "caption": "Figure 8:The first qualitative case compared with three base models in the main content.\nThe caption content that is exclusively captured by our model, captured by our model and other baselines, or misaligned with the detailed visual elements in the videos are emphasized using red, yellow and green backgrounds.",
                "position": 2378
            },
            {
                "img": "https://arxiv.org/html/2503.09279/x8.png",
                "caption": "Figure 9:The second qualitative case compared with three base models in the main content.\nThe caption content that is exclusively captured by our model, captured by our model and other baselines, or misaligned with the detailed visual elements in the videos are emphasized using red, yellow and green backgrounds.",
                "position": 2384
            },
            {
                "img": "https://arxiv.org/html/2503.09279/x9.png",
                "caption": "Figure 10:More video detailed captions generated by Cockatiel-13B.\nThe caption content that is captured by our models and align with the detailed visual elements in the videos are emphasized using yellow backgrounds.",
                "position": 2390
            }
        ]
    },
    {
        "header": "Appendix FMore Experiment Results",
        "images": []
    }
]