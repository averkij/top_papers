[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.22594/2602.22594v1/x1.png",
                "caption": "Figure 1:Overview of the existing methods and the proposed method. Existing diffusion-based methods (left) perform full-sequence denoising using the same noise level across all frames. In contrast, our proposed CMDM (right) introduces a causal diffusion forcing mechanism that operates on semantic causal latent features with frame-wise noise levels.",
                "position": 130
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.22594/2602.22594v1/x2.png",
                "caption": "Figure 2:Overview of the proposed CMDM framework. CMDM consists of three key components: (a) MAC-VAE, which encodes motion sequences into motion–language–aligned and temporally causal latent features using a causal encoder–decoder structure supervised by motion-language model alignment; (b) Causal-DiT, which performs diffusion denoising with causal self-attention and cross-attention to text embeddings, ensuring temporally ordered and semantically consistent frame refinement; and (c) Causal Diffusion Forcing, which applies independent frame-level noise during training and a causal uncertainty schedule during inference, where theredness intensityrepresents the noise level. This design enables CMDM to achieve temporally consistent, semantically aligned, and efficient text-to-motion generation suitable for streaming and long-horizon synthesis.",
                "position": 218
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.22594/2602.22594v1/x3.png",
                "caption": "Figure 3:Qualitative results of long-horizon motion generation. Comparison between our CMDM and previous methods. The generated motion is continuous and seamless; for visualization purposes, we split each long sequence into shorter segments corresponding to their captions. Please refer to the videos in the supplementary materials for the complete motion sequences.",
                "position": 936
            }
        ]
    },
    {
        "header": "5Limitations",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BAdditional Quantitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.22594/2602.22594v1/x4.png",
                "caption": "Figure 4:Qualitative results of long-horizon motion generation on HumanML3D. Comparison between our CMDM and previous methods. The generated motion is continuous and seamless; for visualization purposes, we split each long sequence into shorter segments corresponding to their captions. Please refer to the videos in the supplementary materials for the complete motion sequences.",
                "position": 2071
            },
            {
                "img": "https://arxiv.org/html/2602.22594/2602.22594v1/x5.png",
                "caption": "Figure 5:Qualitative results of long-horizon motion generation on SnapMoGen. Comparison between our CMDM and previous methods. The generated motion is continuous and seamless; for visualization purposes, we split each long sequence into shorter segments corresponding to their captions. Please refer to the videos in the supplementary materials for the complete motion sequences.",
                "position": 2074
            },
            {
                "img": "https://arxiv.org/html/2602.22594/2602.22594v1/x6.png",
                "caption": "Figure 6:Qualitative results of text-to-motion generation on HumanML3D. CMDM produces motions that better capture fine-grained textual semantics and maintain natural body articulation compared to previous methods. Please refer to the supplementary videos for clearer visualization.",
                "position": 2080
            },
            {
                "img": "https://arxiv.org/html/2602.22594/2602.22594v1/x7.png",
                "caption": "Figure 7:Qualitative results of text-to-motion generation on SnapMoGen. Comparison between our CMDM and previous methods. We directly use the raw text prompts without any LLM-based augmentation and CMDM still achieves strong generation quality. Please refer to the supplementary videos for clearer visualization.",
                "position": 2083
            }
        ]
    },
    {
        "header": "Appendix CAdditional Qualitative results",
        "images": []
    },
    {
        "header": "Appendix DSample Code",
        "images": []
    }
]