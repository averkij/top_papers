[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.14256/x1.png",
                "caption": "Figure 1:(Top)Unified modeling, in terms of both tasks and token dimensions. We propose implicit neuralHyper-networks forUnifiedVisualRepresentation,HUVR, with goodclassification,reconstruction, andsegmentation(shown for ViT-B/16 on ImageNet, ImageNet, and ADE20K, respectively). We design our model to generate not only standard-sized tokens, but alsoTinyTokens(TinToks). Here, the tiny embeddings of DINOv3 are generated via principle component analysis (PCA).(Bottom)Reconstruction. We unify recognition and generative task families.",
                "position": 100
            },
            {
                "img": "https://arxiv.org/html/2601.14256/x2.png",
                "caption": "",
                "position": 104
            },
            {
                "img": "https://arxiv.org/html/2601.14256/x3.png",
                "caption": "",
                "position": 106
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.14256/x4.png",
                "caption": "Figure 2:INR Hyper-Network for unified visual representation. The standard and compressed encodings from our model have powerful recognition and reconstruction capabilities, enabling downstream tasks ranging from classification to image generation.",
                "position": 196
            }
        ]
    },
    {
        "header": "3Methods",
        "images": []
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.14256/x5.png",
                "caption": "Figure 3:Training time improves both classification and reconstruction performance, although longer training yields incrementally smaller gains.",
                "position": 1089
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Implicit Neural Representation and Hyper-network Tutorial",
        "images": []
    },
    {
        "header": "7Ablations",
        "images": []
    },
    {
        "header": "8Detailed Settings",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.14256/x6.png",
                "caption": "Figure 4:Generated samples with HUVR embeddings.We use a DiT-XL train on HUVR embeddings with TinTokdt=256d_{t}=256. This HUVR is trained with LPIPS and SSIM losses in addition to the pixel-wise and DINOv3 MSE losses. Compared to our Table2, this DiT is trained for 4500k steps instead of 400k steps. For reference, DiT-XL/2 trains their final model for 7000k steps. These results demonstrate many degradations and artifacts, but we hope they work as a proof-of-concept to convey the promise of HUVR for generation. Future work could apply techniques, such as those in RAE[117], to improve the quality.",
                "position": 3590
            }
        ]
    },
    {
        "header": "9Diffusion with HUVR",
        "images": []
    }
]