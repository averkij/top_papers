[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01248/x1.png",
                "caption": "Figure 1:TRivia-3B learns from unlabeled table images and achieves TR quality beyond the limit attainable by fine-tuning with labeled data.\nUnlike proprietary systems such as Gemini 2.5 Pro, it is open-sourced, compact, and can be deployed offline for privacy-sensitive document processing.",
                "position": 79
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01248/x2.png",
                "caption": "Figure 2:TRivia features (a) an adaptive dataset curation module to set the stage for (b) reinforcement learning to learn TR from unlabeled data.\nDuring dataset curation, TRivia uses a response-consistency sampling strategy (Section3.2.1) to identify informative samples and generate verifiable, diverse QA for each image through an attention-guided module (Section3.2.2).\nBased on curated data, TRivia fine-tunes the VLM to recognize, structure, and reason over tables through QA-based rewards (Section3.1).",
                "position": 143
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01248/x3.png",
                "caption": "Figure 3:Single-time QA generation captures limited table content, while multiple samplings introduce redundant or overlapping QA pairs.\nThe proposed attention-guided QA generation leverages attention distributions to diversify question sources, producing concise and comprehensive QA pairs.",
                "position": 264
            }
        ]
    },
    {
        "header": "4TRivia-3B",
        "images": []
    },
    {
        "header": "5Experimental Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01248/x4.png",
                "caption": "Figure 4:The teacher modelMQAM_{\\text{QA}}used for QA generation could not directly generate correct annotations, but is sufficient to create QAs with TRivia to fine-tune the base model and lead to TRivia-3B that can handle complex structures.",
                "position": 614
            },
            {
                "img": "https://arxiv.org/html/2512.01248/figs/val_TEDS_1.png",
                "caption": "Figure 5:TRivia-3B benefits significantly from the diverse QAs generated by the attention-guided mechanism and the training samples that yield diverse outputs.",
                "position": 798
            },
            {
                "img": "https://arxiv.org/html/2512.01248/figs/QA_reward.png",
                "caption": "Figure 6:Illegal-sample filtering is crucial for stabilizing the fine-tuning of the TR model, as it suppresses reward noise caused by invalid responses.",
                "position": 819
            }
        ]
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "7Acknowledgments",
        "images": []
    },
    {
        "header": "Appendix AMore Training Details",
        "images": []
    },
    {
        "header": "Appendix BDataset Construction",
        "images": []
    },
    {
        "header": "Appendix CMore Details about Experiments",
        "images": []
    },
    {
        "header": "Appendix DPrompt Template",
        "images": []
    }
]