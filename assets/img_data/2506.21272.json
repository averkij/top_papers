[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21272/x1.png",
                "caption": "Figure 1.We present FairyGen, a visual story generation framework to generate multi-shot cartoon videos from a single child-drawn character with consistent style and motion between the foreground and the background. Project page:https://jayleejia.github.io/FairyGen/.",
                "position": 74
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": []
    },
    {
        "header": "2.Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21272/x2.png",
                "caption": "Figure 2.The pipeline of the whole FairyGen.",
                "position": 119
            }
        ]
    },
    {
        "header": "3.Preliminarires",
        "images": []
    },
    {
        "header": "4.Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21272/x3.png",
                "caption": "Figure 3.The pipeline of the storyboard generation.We first plan the whole story using the M-LLM and build a storyboard containing the scenes, events, character action, background, and camera shots. Then, we crop the stylized image using different camera shot and generate final shot images.",
                "position": 174
            },
            {
                "img": "https://arxiv.org/html/2506.21272/extracted/6570993/figures/fig4.png",
                "caption": "Figure 4.Style Consistent Scene Generation.",
                "position": 187
            },
            {
                "img": "https://arxiv.org/html/2506.21272/x4.png",
                "caption": "Figure 5.Two-stage motion train stratage.We first use unorded frames to learn character spatial features without temporal bias. Then, with the identity LoRA frozen, motion residuals are learned from sequential video frames.",
                "position": 246
            },
            {
                "img": "https://arxiv.org/html/2506.21272/x5.png",
                "caption": "Figure 6.Compare with Motion Customization.We compare the proposed motion customization method with the depth-guided image-to-video method using Wan2.1(Wan et al.,2025), pose-guided image-to-video character animation method,i.e., Animate-X(Tan et al.,2024), the proposed method shows very similar results to the original motion sequence with this complex motion.",
                "position": 286
            }
        ]
    },
    {
        "header": "5.Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21272/x6.png",
                "caption": "Figure 7.Compare with Stylization Methods.We compare our method with different stylization methods on stylization customization.",
                "position": 304
            },
            {
                "img": "https://arxiv.org/html/2506.21272/x7.png",
                "caption": "Figure 8.Ablation Study on Style Customization.Compared with the baseline LoRA(Hu et al.,2021)and DoRA(Liu et al.,2024b), the proposed method can successfully propagate the foreground style to the background with different prompts. Best viewed with zoom in.",
                "position": 418
            },
            {
                "img": "https://arxiv.org/html/2506.21272/x8.png",
                "caption": "Figure 9.Ablation on two-stage Motion Adapter.We ablated the two-stage adapters in our proposed motion customization in image-to-video generation. Here, the first stage of training improves the identity similarity.",
                "position": 446
            },
            {
                "img": "https://arxiv.org/html/2506.21272/x9.png",
                "caption": "Figure 10.Comparsion on Multi-Event Video Generation.Our method splits the foreground and the background modelling, which is easy for longer and multi-event video generation. Here, we use the same story prompt to generate the video, where the proposed method shows much consistent results as well as the text description.",
                "position": 449
            },
            {
                "img": "https://arxiv.org/html/2506.21272/x10.png",
                "caption": "Figure 11.Comparison on the appearance and motion customization method.We compare our method with state-of-the-art appearance and motion customization method,i.e., DreamVideo(Wei et al.,2024), the proposed method shows significantly better results considering the stylization, motions, and overall quality.",
                "position": 452
            },
            {
                "img": "https://arxiv.org/html/2506.21272/x11.png",
                "caption": "Figure 12.Ablation Study on timestep shift.The proposed timestep shift strategy in the motion customization can learn to represent the motion better.",
                "position": 455
            },
            {
                "img": "https://arxiv.org/html/2506.21272/x12.png",
                "caption": "Figure 13.Limitation.Due to the uncontrollable generative prior of the video diffusion model, the proposed method might only generate a still background with the animated foreground motion (e.g., running).",
                "position": 476
            }
        ]
    },
    {
        "header": "6.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]