[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.19320/x1.png",
                "caption": "Figure 1:Overview of the dataset pipeline.The process consists of three main stages: (1) Data collection and pre-processing; (2) Annotation and data construction; (3) Post-processing.",
                "position": 134
            },
            {
                "img": "https://arxiv.org/html/2508.19320/x2.png",
                "caption": "Figure 2:Illustration of our Deep Compression Autoencoder (DC-AE).We first train the DC-AE with a spatial compression ratio of 64. In the second phase, we perform causal temporal module training. Then we apply full-model fine-tuning using an 8-frame temporal window in the third phase.",
                "position": 152
            },
            {
                "img": "https://arxiv.org/html/2508.19320/x3.png",
                "caption": "Figure 3:Overview of our model architecture.Our streaming generation framework processes inputs in chunks, where each chunk contains condition tokens (audio, pose, text) followed by frame tokens.\nWe apply teacher forcing during training, while performing next-frame prediction during inference. We corrupt frame tokens with Gaussian noise to mitigate exposure bias. The AR output serves as guidance for the diffusion head for denoising. Here,nndenotes the number of frames per chunk, and chunk boundaries (dashed lines) indicate positions where the inference process can be restarted with updated conditioning inputs.",
                "position": 168
            },
            {
                "img": "https://arxiv.org/html/2508.19320/x4.png",
                "caption": "Figure 4:Illustration of our causal attention mask for multimodal streaming generation.",
                "position": 219
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.19320/results/dialog.png",
                "caption": "Figure 5:Multi-speaker digital conversation with audio-driven avatars.Speaker 1 (top) and Speaker 2 (bottom) demonstrating turn-taking dialogue, with corresponding audio waveforms shown in blue and green.",
                "position": 314
            },
            {
                "img": "https://arxiv.org/html/2508.19320/x5.png",
                "caption": "Figure 6:Cross-lingual singing generation with synchronized lip movements.Our model accurately renders lip synchronization across multiple languages, demonstrating the modelâ€™s fine-grained understanding of cross-lingual phoneme-to-viseme mapping without explicit language identification.",
                "position": 319
            },
            {
                "img": "https://arxiv.org/html/2508.19320/x6.png",
                "caption": "Figure 7:General controllable video generation on Minecraft dataset.By incorporating directional control into our multimodal condition encoding, we realize a real-time interactive world model that exhibits remarkable visual consistency and memory capabilities.",
                "position": 322
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]