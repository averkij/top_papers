[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3The\\emonetSuite: Dataset Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09827/extracted/6543041/images/annotator_agreement_detailed_human.png",
                "caption": "Figure 1:Annotator agreement for human ratings on perceived emotions in audio samples. Stacked horizontal bars display the proportion of audio-emotion instances for each emotion, categorized by agreement type. These categories include full agreement on emotion presence (e.g., ’3:0 (+)’, ’2:0 (+)’), partial agreement where presence is favored (e.g., ’2:1 (+ favored)’), disagreement (e.g., ’1:1’), partial agreement where absence is favored (e.g., ’1:2 (- favored)’), and full agreement on emotion absence (e.g., ’2:0 (-)’, ’3:0 (-)’). Instances with other rating configurations are grouped under ’Other’. The numbers to the right of each bar indicate the total number of instances (n) for that emotion, along with the percentage of these instances rated by two (%2r) or three (%3r) annotators; ’Other’ denotes instances with four annotators. The annotation process ensured all audio-emotion pairs were initially rated by two annotators. If both these annotators marked an emotion as present (rating > 0), the instance was subsequently rated by a third annotator. Additionally, a random subset of instances received a fourth annotation.",
                "position": 685
            }
        ]
    },
    {
        "header": "4Experiments: Do they hear what we hear?",
        "images": []
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendices",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09827/extracted/6543041/images/emonet-voice-instructions.png",
                "caption": "Figure 2:Instructions given to the human annotator for the expert annotation ofEmoNet-Voice Bench.",
                "position": 4390
            },
            {
                "img": "https://arxiv.org/html/2506.09827/extracted/6543041/images/emonet-voice-annotation.png",
                "caption": "Figure 3:UI of our expert annotation tool forEmoNet-Voice Bench.",
                "position": 4393
            }
        ]
    },
    {
        "header": "Appendix BAnnotation Platform Instructions and UI",
        "images": []
    }
]