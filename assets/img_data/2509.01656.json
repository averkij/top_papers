[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01656/x1.png",
                "caption": "Figure 1:OurReVPT-3B and 7B models outperform their instruct and text-only GRPO counterparts on perception-centric tasks while maintaining strong general capabilities across multiple benchmarks.",
                "position": 299
            },
            {
                "img": "https://arxiv.org/html/2509.01656/x2.png",
                "caption": "",
                "position": 308
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01656/x3.png",
                "caption": "Figure 2:ReVPT-3B and 7B are able to solve problems through thinking and selectively employing visual tools to reach strong visual perception. We illustrate some step-by-step visual reasoning breakdowns for challenging examples that model leveraging visual tools to solve complex perception tasks and outperform commercial models GPT-4.1.",
                "position": 320
            }
        ]
    },
    {
        "header": "2ReVPT: Thinking with Images and Tools",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01656/x4.png",
                "caption": "Figure 3:An overall pipeline of ourReVPT.(Top): Model-generated tool requests are managed by a local environment-based Tool Controller, which independently deploys visual tool services (e.g., Depth, Object Detection). These tools’ outputs are then fed back to the LVLM for iterative reasoning.(Bottom): When processing a visual reasoning problem,ReVPTemploys K-turn rollouts where the model interacts with the tool environment to learn an adaptive policy, culminating in the final model.",
                "position": 340
            },
            {
                "img": "https://arxiv.org/html/2509.01656/x5.png",
                "caption": "Figure 4:Reinforced visual tool-usage training requires high-quality and verified data. We transform two datasets into multiple-choice question, then filter out too easy or hard questions for Qwen2.5-VL-7B by multiple inference.",
                "position": 437
            }
        ]
    },
    {
        "header": "3Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01656/x6.png",
                "caption": "Figure 5:ReVPT-3B effectively learns tool utilization after cold-start process.ReVPT-3B shows bias toward object detection and depth estimation over zoom and edge detection tools due to cold-start data construction. Future work will address data balance and generalized perception objectives.",
                "position": 885
            },
            {
                "img": "https://arxiv.org/html/2509.01656/x6.png",
                "caption": "Figure 5:ReVPT-3B effectively learns tool utilization after cold-start process.ReVPT-3B shows bias toward object detection and depth estimation over zoom and edge detection tools due to cold-start data construction. Future work will address data balance and generalized perception objectives.",
                "position": 888
            },
            {
                "img": "https://arxiv.org/html/2509.01656/x7.png",
                "caption": "Figure 6:ReVPT-3B learns to improve accuracy by changing its tool usage after reinforcement learning. This strategic shift results in a higher proportion of correct answers, demonstrating that the RL phase optimizes model’s tool selection.",
                "position": 893
            },
            {
                "img": "https://arxiv.org/html/2509.01656/x8.png",
                "caption": "Figure 7:Case studies illustrating various failure modes ofReVPTwhen using visual tools.Examples include incorrect tool output (top left) , misinterpretation of a correct tool output (top right) , inappropriate tool usage that interferes with perception (bottom left) , and the selection of an unhelpful tool for the given task (bottom right).",
                "position": 1017
            }
        ]
    },
    {
        "header": "4Discussion and Future Works",
        "images": []
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADataset Construction Details",
        "images": []
    },
    {
        "header": "Appendix BExperiment Setup Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01656/x9.png",
                "caption": "Figure 8:Our reward gradually upgrade and reach converge.",
                "position": 2096
            },
            {
                "img": "https://arxiv.org/html/2509.01656/figures/edge0.jpg",
                "caption": "Figure 9:Case study for the tool type: Edge Detection.",
                "position": 2135
            },
            {
                "img": "https://arxiv.org/html/2509.01656/figures/edge1.jpg",
                "caption": "",
                "position": 2144
            },
            {
                "img": "https://arxiv.org/html/2509.01656/figures/zoomin0.jpg",
                "caption": "Figure 10:Case study for the tool type: Zoom In.",
                "position": 2165
            },
            {
                "img": "https://arxiv.org/html/2509.01656/figures/zoomin.jpg",
                "caption": "",
                "position": 2174
            },
            {
                "img": "https://arxiv.org/html/2509.01656/figures/error_object.png",
                "caption": "Figure 11:Case study for the tool type: Object Detection.",
                "position": 2195
            },
            {
                "img": "https://arxiv.org/html/2509.01656/figures/error_object1.png",
                "caption": "",
                "position": 2204
            },
            {
                "img": "https://arxiv.org/html/2509.01656/figures/depth0.png",
                "caption": "Figure 12:Case study for the tool type: Depth Estimation.",
                "position": 2225
            },
            {
                "img": "https://arxiv.org/html/2509.01656/figures/depth1.png",
                "caption": "",
                "position": 2234
            },
            {
                "img": "https://arxiv.org/html/2509.01656/figures/toolr.png",
                "caption": "Figure 13:Case study for the error:Model’s Misunderstanding of Tool Results (Object Detection).",
                "position": 2255
            },
            {
                "img": "https://arxiv.org/html/2509.01656/figures/toolr1.png",
                "caption": "",
                "position": 2264
            },
            {
                "img": "https://arxiv.org/html/2509.01656/figures/error_depth.jpg",
                "caption": "Figure 14:Case study for the error: Model’s Misunderstanding of Tool Results (Depth Estimation).",
                "position": 2285
            },
            {
                "img": "https://arxiv.org/html/2509.01656/figures/error_depth1.jpg",
                "caption": "",
                "position": 2294
            },
            {
                "img": "https://arxiv.org/html/2509.01656/figures/56_0.png",
                "caption": "Figure 15:Case study for the error: Flawed Tool Results (Object Detection).",
                "position": 2315
            }
        ]
    },
    {
        "header": "Appendix CCase Study",
        "images": []
    }
]