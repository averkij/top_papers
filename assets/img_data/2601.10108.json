[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10108/x1.png",
                "caption": "Figure 1:Comparison of long-context multimodal evaluation paradigms.\n(a) Current NIAH approaches embed artificial “needles” into irrelevant noise, focusing on surface-level retrieval.\n(b) The proposed FITO paradigm evaluates deep comprehension within the native document ecosystem (“ocean”).\nIt requires the model to aggregate interconnected knowledge units (“fish”) across sections to form an evidence chain for reasoning.",
                "position": 155
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3The Fish-in-the-Ocean Paradigm",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10108/x2.png",
                "caption": "Figure 2:The framework ofSIN-Bench.(a) SIN-Data Infrastructure: We parse raw source packages into a unified Scientific Interleaved format using a semantic-first strategy.(c) Construction Pipeline: Based on the data, we employ an iterative Multi-MLLM synthesis loop with cross-validation and human auditing to generate high-quality samples.(b) Task & Metrics: The benchmark features four hierarchical tasks evaluated under the “No Evidence, No Score” protocol. We assess evidence chains across Matching, Relevance, and Logic dimensions.",
                "position": 273
            }
        ]
    },
    {
        "header": "4SIN-Bench: Evidence-based Evaluation",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10108/x3.png",
                "caption": "Figure 3:Task-level overall performance heatmap across models inSIN-Bench. Darker cells imply higher scores.",
                "position": 770
            },
            {
                "img": "https://arxiv.org/html/2601.10108/x4.png",
                "caption": "Figure 4:Qualitative examples of reasoning failures by Gemini-3-pro in the SIN-Find and SIN-QA tasks.",
                "position": 842
            },
            {
                "img": "https://arxiv.org/html/2601.10108/x5.png",
                "caption": "Figure 5:Impact of interleaved input and modality encodings on SIN-QA and SIN-Summary (Gemini-3-pro).",
                "position": 845
            },
            {
                "img": "https://arxiv.org/html/2601.10108/x6.png",
                "caption": "Figure 6:The score density distribution across varying input token lengths for SIN-QA and SIN-Summary.",
                "position": 868
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethical considerations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ADetails of Benchmark Creation",
        "images": []
    },
    {
        "header": "Appendix BOn the Principle of “No Evidence, No Score”",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10108/x7.png",
                "caption": "Figure 8:Golden examples fromSIN-Bench: SIN-Find and SIN-QA.\nWe visualize one curated instance per task with explicit markers forDocument,Question,Answer, and the goldEvidence chain.",
                "position": 1697
            },
            {
                "img": "https://arxiv.org/html/2601.10108/x8.png",
                "caption": "Figure 9:Golden examples fromSIN-Bench: SIN-Verify and SIN-Summary.\nWe visualize one curated instance per task with explicit markers forDocument,Question,Answer, and the goldEvidence chain.",
                "position": 1701
            }
        ]
    },
    {
        "header": "Appendix CComparisons with Existing Benchmarks",
        "images": []
    },
    {
        "header": "Appendix DDetails of Experimental Setting",
        "images": []
    },
    {
        "header": "Appendix EAdditional Experiments and Result Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10108/x9.png",
                "caption": "Figure 10:Domain-wise performance onSIN-Bench. Radar plot of average overall scores (across all tasks) for GPT-5, Gemini-3-pro, and Qwen3-VL-8B across1212primary disciplines. Higher is better, highlighting substantial domain-dependent variability in evidence-grounded scientific comprehension.",
                "position": 2574
            },
            {
                "img": "https://arxiv.org/html/2601.10108/x10.png",
                "caption": "Figure 11:Accuracy on SIN-Verify with two negative sets. Easy negatives are clearly irrelevant evidence; hard negatives are cross-validation near-misses with correct answers but insufficient/ambiguous evidence support.",
                "position": 2592
            },
            {
                "img": "https://arxiv.org/html/2601.10108/x11.png",
                "caption": "Figure 12:Overall score frequency vs. text-token length for SIN-Find, SIN-QA and SIN-Summary. Normalized 2D frequency of samples binned by text prompt length and overall scores.",
                "position": 2595
            },
            {
                "img": "https://arxiv.org/html/2601.10108/x12.png",
                "caption": "Figure 13:Overall score frequency vs. total (text and image) token length. Same asFig.˜12but with prompt length computed from text plus image tokens from the API request.",
                "position": 2598
            },
            {
                "img": "https://arxiv.org/html/2601.10108/x13.png",
                "caption": "Figure 14:Error cases on SIN-QA.",
                "position": 2659
            }
        ]
    },
    {
        "header": "Appendix FCase Study",
        "images": []
    }
]