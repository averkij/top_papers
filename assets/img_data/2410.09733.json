[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.09733/extracted/5922487/figures/example.png",
                "caption": "Figure 1:MMCompositioncomprises 13 categories of high-quality VL composition QA pairs, covering a wide range of complex compositions. In the example, GPT-4o failed to understand the compositional aspects of the visual and textual components, misidentifying a three-story building as a double-decker structure. This misinterpretation highlights the limitations of current VLMs.",
                "position": 108
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.09733/extracted/5922487/figures/main_stat.png",
                "caption": "Figure 2:The statistics of 13 distinct categories of QA pairs inMMCompositionand some modelsâ€™ performance on each category.",
                "position": 272
            }
        ]
    },
    {
        "header": "3MMComposition",
        "images": []
    },
    {
        "header": "4Revisiting the Compositionality of Pre-trained Vision-Language Models",
        "images": []
    },
    {
        "header": "5Diagnostic Analysis of Factors Influencing Model Compositionality",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.09733/extracted/5922487/figures/explain1.jpg",
                "caption": "Figure 3:Interpretable analysis of different VLMs.Greenletters indicate correct answers, whileredletters represent wrong (predicted) answers.",
                "position": 1679
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.09733/extracted/5922487/figures/difficulty_levels.png",
                "caption": "Figure 4:Distribution of difficulty levels across the question set, illustrating the challenging nature of tasks.",
                "position": 2999
            },
            {
                "img": "https://arxiv.org/html/2410.09733/extracted/5922487/figures/difficulty_levels.png",
                "caption": "Figure 4:Distribution of difficulty levels across the question set, illustrating the challenging nature of tasks.",
                "position": 3002
            },
            {
                "img": "https://arxiv.org/html/2410.09733/extracted/5922487/figures/option_count_distribution.png",
                "caption": "Figure 5:Distribution of option counts per question, showing the variety in answer choices provided to evaluate VLMs.",
                "position": 3007
            },
            {
                "img": "https://arxiv.org/html/2410.09733/extracted/5922487/figures/resolution_analysis.png",
                "caption": "Figure 6:Resolution distribution of images in our benchmark, reflecting the portion of high-quality images inMMComposition.",
                "position": 3013
            },
            {
                "img": "https://arxiv.org/html/2410.09733/extracted/5922487/figures/resolution_analysis.png",
                "caption": "Figure 6:Resolution distribution of images in our benchmark, reflecting the portion of high-quality images inMMComposition.",
                "position": 3016
            },
            {
                "img": "https://arxiv.org/html/2410.09733/extracted/5922487/figures/wordcloud.png",
                "caption": "Figure 7:Word cloud of key terms from the questions, illustrating the diversity of compositional content evaluated in the benchmark.",
                "position": 3021
            },
            {
                "img": "https://arxiv.org/html/2410.09733/extracted/5922487/figures/difficulty_levels_low.png",
                "caption": "Figure 8:Distribution of difficulty levels of questions with low CPV, illustrating the authenticity of the difficulty distribution inMMComposition.",
                "position": 3170
            },
            {
                "img": "https://arxiv.org/html/2410.09733/extracted/5922487/figures/difficulty_levels_low.png",
                "caption": "Figure 8:Distribution of difficulty levels of questions with low CPV, illustrating the authenticity of the difficulty distribution inMMComposition.",
                "position": 3173
            },
            {
                "img": "https://arxiv.org/html/2410.09733/extracted/5922487/figures/option_count_distribution_low.png",
                "caption": "Figure 9:Distribution of option counts for questions with low CPV.",
                "position": 3178
            },
            {
                "img": "https://arxiv.org/html/2410.09733/extracted/5922487/figures/resolution_analysis_low.png",
                "caption": "Figure 10:Resolution distribution of images with low CPV.",
                "position": 3184
            },
            {
                "img": "https://arxiv.org/html/2410.09733/extracted/5922487/figures/resolution_analysis_low.png",
                "caption": "Figure 10:Resolution distribution of images with low CPV.",
                "position": 3187
            },
            {
                "img": "https://arxiv.org/html/2410.09733/extracted/5922487/figures/wordcloud_low.png",
                "caption": "Figure 11:Word cloud of key terms from the questions with low CPV, illustrating the keywords appearing in the questions that VLMs are hard to answer currently.",
                "position": 3192
            },
            {
                "img": "https://arxiv.org/html/2410.09733/extracted/5922487/figures/gpt4o-weak.jpg",
                "caption": "Figure 12:GPT-4o Weak Category Analysis. The logos of the models or human displayed to the right of the option(s) indicate that the model or human has selected the option(s) as the correct answer(s).",
                "position": 3205
            },
            {
                "img": "https://arxiv.org/html/2410.09733/extracted/5922487/figures/gpt4o-qwen2vl.png",
                "caption": "Figure 13:Performance gap between images whose shortest side>>>768px and thoseâ‰¤\\leqâ‰¤768px, defined asgap=Aâ¢câ¢c>768â¢pâ¢xâˆ’Aâ¢câ¢câ‰¤768â¢pâ¢xgapð´ð‘subscriptð‘absent768ð‘ð‘¥ð´ð‘subscriptð‘absent768ð‘ð‘¥\\text{gap}=Acc_{>768px}-Acc_{\\leq 768px}gap = italic_A italic_c italic_c start_POSTSUBSCRIPT > 768 italic_p italic_x end_POSTSUBSCRIPT - italic_A italic_c italic_c start_POSTSUBSCRIPT â‰¤ 768 italic_p italic_x end_POSTSUBSCRIPT. The histogram shows the distribution of performance gaps across 13 tasks. The average performance gap for GPT-4o is 14.26, while for Qwen2-VL, it is 9.05. The smaller gap for Qwen2-VL indicates its greater effectiveness in processing high-resolution images. Additionally, Qwen2-VLâ€™s performance gaps are more consistently positive across different tasks, further highlighting its robustness in handling high-resolution images.",
                "position": 3208
            },
            {
                "img": "https://arxiv.org/html/2410.09733/extracted/5922487/figures/explain3.jpg",
                "caption": "Figure 14:More interpretable analysis of different VLMs.Greenindicates correct answers, whileredrepresents the predicted wrong answers.",
                "position": 3218
            },
            {
                "img": "https://arxiv.org/html/2410.09733/extracted/5922487/figures/explain2.jpg",
                "caption": "Figure 15:More interpretable analysis of different VLMs.Greenindicates correct answers, whileredrepresents the predicted wrong answers.",
                "position": 3221
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]