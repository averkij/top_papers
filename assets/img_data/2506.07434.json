[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07434/x1.png",
                "caption": "Figure 1:Illustration of our motivation. (1)¬†It is hard for a base LLM¬†(as the big bot) to directly align with human preferences. (2)¬†However, a small draft model¬†(as the small bot) can guide the base LLM by providing well-aligned responses at the beginning. (3)¬†This contributes to better alignment from beginning to end.",
                "position": 108
            },
            {
                "img": "https://arxiv.org/html/2506.07434/x2.png",
                "caption": "Figure 2:Results of the preliminary experiment.",
                "position": 130
            }
        ]
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07434/x3.png",
                "caption": "Figure 3:The illustration of the WSD framework. Given a user query in¬†(a), the draft modelmùëömitalic_mfirst proposesymsuperscriptùë¶ùëöy^{m}italic_y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT, which is then checked by the base modelMùëÄMitalic_Mto determine the switch point, shown in (b).MùëÄMitalic_Mcontinues the generation in (c) to acquire the final responseyùë¶yitalic_yin (d).",
                "position": 148
            },
            {
                "img": "https://arxiv.org/html/2506.07434/x4.png",
                "caption": "Figure 4:The illustrations of alignment tax in fine-tuning Llama-3.2-3B-it to Pilot-3B. The LC win-rate of AlpacaEval 2, representing its ability of preference alignment, exhibits a clear increase, while its performance on mathematics and code¬†(Accuracy of GSM8K and Pass@1 of HumanEval) has dropped.",
                "position": 250
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07434/x5.png",
                "caption": "Figure 5:Results of ablation study on Llama-3-70B.",
                "position": 714
            },
            {
                "img": "https://arxiv.org/html/2506.07434/x6.png",
                "caption": "Figure 6:Cummulative distribution of AlpacaEval 2 and GSM8K, along with increasing decoding steps on Llama-3-70B.",
                "position": 738
            },
            {
                "img": "https://arxiv.org/html/2506.07434/x7.png",
                "caption": "Figure 7:Evaluating the effect of different draft models. (a)¬†Llama-3-70B; (b)¬†Llama-3.1-70B; (c)¬†Gemma-2-27B.",
                "position": 752
            },
            {
                "img": "https://arxiv.org/html/2506.07434/x8.png",
                "caption": "Figure 8:Scalability of WSD on models of different sizes in Gemma-2 series. (a)¬†Results of AlpacaEval 2. (b)¬†Results of GSM8K.",
                "position": 755
            },
            {
                "img": "https://arxiv.org/html/2506.07434/x9.png",
                "caption": "Figure 9:Case study. (a)¬†Model switching happens after the partial answer is drafted. (b)¬†Model switching at the beginning of decoding.",
                "position": 790
            }
        ]
    },
    {
        "header": "4Related Works",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails of GenerAlign",
        "images": []
    },
    {
        "header": "Appendix BEvaluation Details",
        "images": []
    },
    {
        "header": "Appendix CMore Results of Ablation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07434/x10.png",
                "caption": "Figure 10:Results of ablation study on Llama-3.1-70B.",
                "position": 1522
            },
            {
                "img": "https://arxiv.org/html/2506.07434/x11.png",
                "caption": "Figure 11:Results of ablation study on Gemma-2-27B.",
                "position": 1525
            },
            {
                "img": "https://arxiv.org/html/2506.07434/x12.png",
                "caption": "(a)Cummulative distribution of AlpacaEval 2 and GSM8K, along with increasing decoding steps on Llama-3.1-70B.",
                "position": 1528
            },
            {
                "img": "https://arxiv.org/html/2506.07434/x12.png",
                "caption": "(a)Cummulative distribution of AlpacaEval 2 and GSM8K, along with increasing decoding steps on Llama-3.1-70B.",
                "position": 1531
            },
            {
                "img": "https://arxiv.org/html/2506.07434/x13.png",
                "caption": "(b)Cummulative distribution of AlpacaEval 2 and GSM8K, along with increasing decoding steps on Gemma-2-27B.",
                "position": 1536
            }
        ]
    },
    {
        "header": "Appendix DCases",
        "images": []
    }
]