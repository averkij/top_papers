[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15418/x1.png",
                "caption": "Figure 1:Normalized performance on LLaSO-Eval, comparing LLaSO-Base (orange) with the strongest prior models (blue) across 20 linguistic, semantic, and paralinguistic tasks. For each task, scores are normalized by setting the best-performing model’s result to 1. Details are in Table13.",
                "position": 197
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15418/x2.png",
                "caption": "Figure 2:Overview of LLaSO Corpus, comprising LLaSO-Align, LLaSO-Instruct, and LLaSO-Eval. It encompasses 25.5M audio-text pairs spanning 20 task types, including 18 paralinguistic tasks, and supports linguistic, semantic, and paralinguistic abilities. Instances include both real (73%) and synthetic (27%) audio, covering multiple input combinations and tasks. Further statistics are detailed in Table12.",
                "position": 346
            }
        ]
    },
    {
        "header": "3LLaSOCorpus",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15418/x3.png",
                "caption": "Figure 3:LLaSO Corpus construction pipeline.We first aggregate heterogeneous sources, including text-based QA corpora and speech datasets covering acoustic, paralinguistic, and semantic tasks, then normalize format, sample rate, and instruction style, etc. We construct LLaSO-Align (12.0 M) for aligning speech and text modality via ASR, while LLaSO-Instruct (13.5 M) for multi-task instruction tuning including classification, recognition, and AQA. When synthesize audio, we use vocal style mixing strategy as Figure4by utilizing controllable TTS and voice-cloning for richer speaker variation, enablingpure-audio,text plus audio, andaudio plus textformatted samples with diverse gender, accent, speed, and tone.",
                "position": 355
            },
            {
                "img": "https://arxiv.org/html/2508.15418/x4.png",
                "caption": "Figure 4:Overview of the Vocal Style Mixing.Vocal style mixing is a strategy in our data construction pipeline in Figure3, where utterances are systematically synthesized with diverse speaker traits. This strategy is applied across all three modalities input, expanding acoustic diversity and simulating realistic multi-speaker scenarios, resulting a wide range of speech-language interactions in Figure5.",
                "position": 459
            },
            {
                "img": "https://arxiv.org/html/2508.15418/x5.png",
                "caption": "IInteraction formatssupported in LLaSO Corpus.",
                "position": 512
            },
            {
                "img": "https://arxiv.org/html/2508.15418/x5.png",
                "caption": "IInteraction formatssupported in LLaSO Corpus.",
                "position": 515
            },
            {
                "img": "https://arxiv.org/html/2508.15418/x6.png",
                "caption": "IITask prototypes.(a) closed-setclassification; (b) multi-granularity/open-setrecognition; (c) open-endedAQA.",
                "position": 521
            }
        ]
    },
    {
        "header": "4Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15418/x7.png",
                "caption": "Figure 6:Overview of the LLaSO-Base architecture and two training phases. The reference model architecture and input flow (Bottom), three supported modality input layouts (Top Left) , and the training recipe comprising alignment and instruction tuning (Top Right).",
                "position": 984
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15418/x8.png",
                "caption": "Figure 8:Overall model performance after min-max normalization. Each bar represents a model’s aggregated performance score, normalized to the [0, 1] range for direct comparison. Higher values indicate better overall performance.",
                "position": 1947
            },
            {
                "img": "https://arxiv.org/html/2508.15418/x9.png",
                "caption": "Figure 9:Stability and modality-wise performance of LSLMs.Top:Model stability across modality configurations, quantified by the sum of absolute differences between each model’s average GPT-4o score on the primary modality (TA) and those on the other two modalities, i.e.,|TA−PA|+|TA−AT||\\mathrm{TA}-\\mathrm{PA}|+|\\mathrm{TA}-\\mathrm{AT}|. Lower values reflect greater robustness to modality variation.Bottom:Average GPT-4o score of each model on the three major modality configurations. Colors are consistent across both plots for direct comparison.Abbreviations:PA = pure audio; TA = text instruction + audio input; AT = audio instruction + text input.",
                "position": 1978
            },
            {
                "img": "https://arxiv.org/html/2508.15418/x10.png",
                "caption": "Figure 10:Task coverage vs. model performance and abstention. Each scatter plot visualizes 11 models by theirTask Count(number of training tasks; for models with private, incomplete, or ambiguously reported training data, we use the number of evaluation tasks as a proxy for task coverage).(a)Overall performance (min-max normalized over all LLaSO-Eval tasks, cf. Figure8).(b)Average Closed-ended task performance.(c)Average abstention rate on closed-ended tasks. Closed-ended scores and abstention rates are calculated only on tasks that require categorical selection. Higher scores indicate better performance; lower abstention rates indicate stronger instruction following.",
                "position": 1987
            },
            {
                "img": "https://arxiv.org/html/2508.15418/x11.png",
                "caption": "Figure 11:Comparison of LSLM performance on content-centric versus speaker-centric paralinguistic tasks.Top:For each model, min-max normalized performance scores are shown on content-centric (blue) and speaker-centric (red) tasks, with dumbbell lines indicating the magnitude and direction of intra-model performance differences.Bottom:Average abstention rates (lower is better) for closed-ended tasks in the same two centrics within the paralinguistic category. All evaluations are conducted under the text instruction paired with audio input configuration.",
                "position": 1994
            },
            {
                "img": "https://arxiv.org/html/2508.15418/x12.png",
                "caption": "IThree evaluated cases from Salmonn across the primary modality configurations.\nThe correct one corresponds to the model’s supported training format; the two errors are from modality formats with limited or no training exposure.",
                "position": 2005
            },
            {
                "img": "https://arxiv.org/html/2508.15418/x12.png",
                "caption": "IThree evaluated cases from Salmonn across the primary modality configurations.\nThe correct one corresponds to the model’s supported training format; the two errors are from modality formats with limited or no training exposure.",
                "position": 2008
            },
            {
                "img": "https://arxiv.org/html/2508.15418/x13.png",
                "caption": "IIThree Llama-Omni responses sampled from the three major modality configurations. The correct example comes from its supported format; the two errors are from its unsupported modality configurations.",
                "position": 2016
            },
            {
                "img": "https://arxiv.org/html/2508.15418/x14.png",
                "caption": "IThree samples from Typhoon-Audio: one from a well-represented task with correct prediction, and two errors from tasks that are absent in its training.",
                "position": 2034
            },
            {
                "img": "https://arxiv.org/html/2508.15418/x14.png",
                "caption": "IThree samples from Typhoon-Audio: one from a well-represented task with correct prediction, and two errors from tasks that are absent in its training.",
                "position": 2037
            },
            {
                "img": "https://arxiv.org/html/2508.15418/x15.png",
                "caption": "IISelected answers from GLM-4-Voice illustrate success on a task with ample training exposure and failure on two tasks that fall outside its main training coverage.",
                "position": 2043
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitation",
        "images": []
    },
    {
        "header": "Appendix BEthical Statement",
        "images": []
    },
    {
        "header": "Appendix CBenchmarking Candidates",
        "images": []
    },
    {
        "header": "Appendix DTask Category Definitions",
        "images": []
    },
    {
        "header": "Appendix ECases",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15418/x16.png",
                "caption": "Figure 14:Representative case examples from LLaSO-Base demonstrating the three modality configurations in LLaSO-Eval: pure audio (left), text instruction with audio input (middle), and audio instruction with text input (right). Each column presents distinct tasks under its respective format.",
                "position": 3989
            },
            {
                "img": "https://arxiv.org/html/2508.15418/x17.png",
                "caption": "",
                "position": 3999
            }
        ]
    },
    {
        "header": "Appendix FTraining Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15418/x18.png",
                "caption": "Figure 15:Training loss visualization with Raw Loss and Smoothed Loss. From left to right: (1) alignment stage; (2) instruction tuning stage with frozen encoder; (3) instruction tuning stage with unfrozen encoder.",
                "position": 4112
            },
            {
                "img": "https://arxiv.org/html/2508.15418/x19.png",
                "caption": "",
                "position": 4115
            },
            {
                "img": "https://arxiv.org/html/2508.15418/x20.png",
                "caption": "",
                "position": 4116
            }
        ]
    },
    {
        "header": "Appendix GSystem Prompt",
        "images": []
    },
    {
        "header": "Appendix HFour Styles Instructional Prompts",
        "images": []
    },
    {
        "header": "Appendix IMulti-granularity Setting Details",
        "images": []
    },
    {
        "header": "Appendix JPrompts for Pure Audio Modality Format",
        "images": []
    },
    {
        "header": "Appendix KEvalation Template",
        "images": []
    },
    {
        "header": "Appendix LDetails for LLaSO-Align and LLaSO-Instruct",
        "images": []
    },
    {
        "header": "Appendix MBaseline Performance Details",
        "images": []
    }
]