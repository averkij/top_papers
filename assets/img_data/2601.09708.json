[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09708/x1.png",
                "caption": "Figure 1:Overview of Fast-ThinkAct.Previous reasoning VLAs generate lengthy reasoning traces (‚àº\\sim250 tokens). Our approach learns compact continuous tokens (e.g., 6) (blue) and parallel spatial tokens (green) as internal reasoning. The bottom-right plot shows that we achieve9.3√ó9.3\\timesfaster inference than ThinkAct-7Bhuang2025thinkact, while delivering improved performance on the SimplerEnv-Google benchmark.",
                "position": 204
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09708/x2.png",
                "caption": "Figure 2:Overview of Fast-ThinkAct.(a) Given observationoto_{t}and instructionll, the Textual Teacher VLM‚Ñ±Œ∏T\\mathcal{F}_{\\theta}^{T}generates explicit reasoning chains. The Latent Student VLM‚Ñ±Œ∏\\mathcal{F}_{\\theta}distills these into compact latent tokensùê≥\\mathbf{z}guided by reward preferences. Verbalizer LLMùí±œà\\mathcal{V}_{\\psi}decodes latents to text for preference-based learning via‚Ñíverb\\mathcal{L}_{\\text{verb}}, while‚Ñídistill\\mathcal{L}_{\\text{distill}}transfers visual planning capability from teacher, and spatial tokens enable parallel visual trajectory prediction via‚Ñíans\\mathcal{L}_{\\text{ans}}, ensuring latents are verbalizable and grounded in visual planning. (b) Reasoning-Enhanced Policy Learning. The Action ModelœÄœï\\pi_{\\phi}is trained with‚ÑíIL\\mathcal{L}_{\\text{IL}}while freezing the latent student‚Ñ±Œ∏\\mathcal{F}_{\\theta}and state encoder.",
                "position": 276
            },
            {
                "img": "https://arxiv.org/html/2601.09708/x3.png",
                "caption": "Figure 3:Evaluation of robot manipulation and reasoning efficiency.(a)-(e) Success rates on LIBEROliu2023liberoand SimplerEnvli24simplerbenchmarks compared with state-of-the-art 7B reasoning VLAs. (f) Latency comparison across 3B and 7B reasoning VLAs. Our approach achieves up to 89.3% inference latency reduction while maintaining superior task success rates.",
                "position": 404
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09708/x4.png",
                "caption": "Figure 4:Visualization of predicted visual trajectories and action execution results on long-horizon tasks.Examples from (a) SimplerEnv-Google, (b) LIBERO-Long, and (c) RoboTwin2.0-Hard with long (278) steps. Yellow traces indicate single-arm/left gripper trajectories; red traces indicate right gripper trajectories for bimanual tasks.",
                "position": 938
            },
            {
                "img": "https://arxiv.org/html/2601.09708/x5.png",
                "caption": "Figure 5:Failure recovery capability on RoboFAClu2025robofac.Left: Qualitative examples (from both simulation and real robot) of corrective guidance for manipulation errors. Right: Quantitative evaluation on simulation (RoboFAC-Sim) and real-robot (RoboFAC-Real) settings.",
                "position": 941
            },
            {
                "img": "https://arxiv.org/html/2601.09708/x6.png",
                "caption": "Figure 6:Few-shot adaptation results on RoboTwin2.0 benchmark.We use 10 demonstrations per task for fine-tuning.",
                "position": 951
            },
            {
                "img": "https://arxiv.org/html/2601.09708/x7.png",
                "caption": "Figure 7:Reasoning trace comparison on RoboVQA.(a) Teacher‚Äôs textual reasoning. (b) Student‚Äôs verbalized latent reasoning.Green: relevant content;orange: less relevant content.",
                "position": 960
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix AAdditional Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09708/x8.png",
                "caption": "Figure 8:Visualization of predicted visual trajectories and action execution results on RoboTwin2.0.Yellow traces indicate left gripper trajectories; red traces indicate right gripper trajectories for bimanual tasks.",
                "position": 1577
            },
            {
                "img": "https://arxiv.org/html/2601.09708/x9.png",
                "caption": "Figure 9:Failure identification and analysis capabilities on RoboFAClu2025robofac.Top row shows identification of failure types and execution stages. Bottom row demonstrates failure root cause analysis.",
                "position": 1580
            },
            {
                "img": "https://arxiv.org/html/2601.09708/x10.png",
                "caption": "Figure 10:Reasoning trace comparison on OpenEQA.(a) Teacher‚Äôs textual reasoning. (b) Student‚Äôs verbalized latent reasoning.Green: reasonable reasoning trace;red: incorrect trace.",
                "position": 1583
            }
        ]
    },
    {
        "header": "Appendix BAdditional Experiment Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09708/x11.png",
                "caption": "Table 7:Additional ablation study of training objectives and learning stages on robot manipulation benchmarks.",
                "position": 1653
            },
            {
                "img": "https://arxiv.org/html/2601.09708/x11.png",
                "caption": "Table 8:Ablation of Latent Reasoning StepsMM.",
                "position": 1743
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]