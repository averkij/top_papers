[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16793/x1.png",
                "caption": "Figure 1:Human egocentric supervision improves first-person embodied brains and transfers to control.Left:EgoThink radar plot comparing egocentric VLM performance across six dimensions (Activity, Forecast, Localization, Object, Planning, Reasoning) for representative baselines.Right Top:\"Phys\" means that the VLM was supervised fine-tuning on our annotated first-person (egocentric) data (described in Sec.3.1), both VST-7B and Qwen2.5-VL-7B achievesignificantlybetter EgoThink performance, with the most pronounced gains onPlanning.Right Bottom:when used as the VLM backbone in a standard VLA fine-tuning pipeline, the same Phys-enhanced backbones yieldsubstantiallyhigher SimplerEnv success rates, indicating that better egocentric planning and interaction reasoning translate to improved downstream manipulation.",
                "position": 156
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Egocentric Embodied Supervision",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16793/fig/data_pipeline.jpg",
                "caption": "Figure 2:Illustration of the Egocentric2Embodiment Translation Pipeline.",
                "position": 251
            },
            {
                "img": "https://arxiv.org/html/2512.16793/fig/data_sum.jpg",
                "caption": "Figure 3:Overview and Data Distribution Statistics of E2E-3M dataset.",
                "position": 299
            }
        ]
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16793/fig/Framework.jpg",
                "caption": "Figure 4:VLA architecture built on PhysBrain.Given an egocentric observation sequence and a language instruction, PhysBrain encodes multimodal context for action generation.(a) PhysGR00Tconditions a flow-matching diffusion action expert on thelast-layerhidden states of PhysBrain.(b) PhysPImore tightly couples PhysBrain and the action expert by injectingmultipleVLM layers via layer-wise cross-attention.",
                "position": 362
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations and Future Work",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7VLA Training Hyperparameters",
        "images": []
    },
    {
        "header": "8PhysPI Architecture Experiment",
        "images": []
    }
]