[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.14366/x1.png",
                "caption": "Figure 1:Reasoning LLMs performance comparison betweenATLASand other commonly used reasoning benchmarks.",
                "position": 167
            },
            {
                "img": "https://arxiv.org/html/2511.14366/x1.png",
                "caption": "Figure 1:Reasoning LLMs performance comparison betweenATLASand other commonly used reasoning benchmarks.",
                "position": 170
            },
            {
                "img": "https://arxiv.org/html/2511.14366/x2.png",
                "caption": "Figure 2:Average final answer token length for mainstream reasoning datasets.",
                "position": 175
            },
            {
                "img": "https://arxiv.org/html/2511.14366/x3.png",
                "caption": "Figure 3:Overview ofATLAS, which contains 7 stem subjects and 57 corresponding sub-fields.",
                "position": 182
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3ATLASConstruction Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.14366/x4.png",
                "caption": "Figure 4:Overview ofATLASconstruction pipeline.",
                "position": 415
            },
            {
                "img": "https://arxiv.org/html/2511.14366/x5.png",
                "caption": "Figure 5:Overview of howATLASrefine the final answer for natural questions.",
                "position": 490
            }
        ]
    },
    {
        "header": "4Dataset Analysis: TheATLASCorpus",
        "images": []
    },
    {
        "header": "5Evaluation and Performance",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.14366/x6.png",
                "caption": "Figure 6:Overview of the evaluation workflow. During the evaluation process, the LLM is prompted to provide formatted predictions, from which the answers are extracted and input into the Judge LLMs for the computation of evaluation metrics.",
                "position": 843
            },
            {
                "img": "https://arxiv.org/html/2511.14366/x7.png",
                "caption": "Figure 7:The performance of different LLMs across different subjects ofATLAS’s validation set.",
                "position": 991
            }
        ]
    },
    {
        "header": "6Discussion and Future Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Appendix AATLASDetails",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.14366/x8.png",
                "caption": "Figure 8:Hierarchical Distribution of Question Types inATLAS",
                "position": 2053
            }
        ]
    },
    {
        "header": "Appendix BQuestion Contributors",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.14366/x9.png",
                "caption": "Figure 9:Distribution of Question Contributing Institutions.",
                "position": 2064
            }
        ]
    },
    {
        "header": "Appendix CExpert Review",
        "images": []
    },
    {
        "header": "Appendix DExpert Review",
        "images": []
    },
    {
        "header": "Appendix EPrompts for Evaluation",
        "images": []
    },
    {
        "header": "Appendix FPerformance on the Test Set ofATLAS",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.14366/x10.png",
                "caption": "Figure 10:The performance of different LLMs across different subjects ofATLAS’s test set.",
                "position": 2886
            }
        ]
    },
    {
        "header": "Appendix GContributors",
        "images": []
    }
]