[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22083/x1.png",
                "caption": "Figure 1:Comparison between DPO and GANPO.Offline preference optimization methods (e.g., DPO) optimize an implicit reward defined by preference data. GANPO augments this objective with a latent-space discriminator, whose adversarial interaction induces a regularization between the latent representation distributions of the policy model and the reference model.",
                "position": 109
            },
            {
                "img": "https://arxiv.org/html/2601.22083/x2.png",
                "caption": "Figure 2:Latent space vs token space.Anchor (“Hi there.”) is the reference point for distance measurements. Semantically similar paraphrases exhibit large token-level variation yet remain close in latent space, while semantically different phrases show smaller token changes but larger latent space differences.",
                "position": 116
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Latent Adversarial Regularization",
        "images": []
    },
    {
        "header": "4GAN Preference Optimization",
        "images": []
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22083/x3.png",
                "caption": "(a)AlpacaEval.GANPO widens the performance gap over DPO as entropy increases (T≥1.0T\\geq 1.0), demonstrating better quality retention under stochastic sampling.",
                "position": 639
            },
            {
                "img": "https://arxiv.org/html/2601.22083/x3.png",
                "caption": "(a)AlpacaEval.GANPO widens the performance gap over DPO as entropy increases (T≥1.0T\\geq 1.0), demonstrating better quality retention under stochastic sampling.",
                "position": 642
            },
            {
                "img": "https://arxiv.org/html/2601.22083/x4.png",
                "caption": "(b)IFEval Strict Accuracy.While DPO suffers from rapid structural degradation as temperature rises, GANPO exhibits resilience, maintaining high instruction adherence even in high-noise regimes.",
                "position": 648
            },
            {
                "img": "https://arxiv.org/html/2601.22083/x5.png",
                "caption": "(a)T=1.5T=1.5",
                "position": 686
            },
            {
                "img": "https://arxiv.org/html/2601.22083/x5.png",
                "caption": "(a)T=1.5T=1.5",
                "position": 689
            },
            {
                "img": "https://arxiv.org/html/2601.22083/x6.png",
                "caption": "(b)T=2.0T=2.0",
                "position": 695
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelativistic Average Divergence",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details and Hyperparameters",
        "images": []
    },
    {
        "header": "Appendix CComputational Cost Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22083/x7.png",
                "caption": "Figure 5:Win rate as a function of response length for DPO and GANPO.",
                "position": 1085
            },
            {
                "img": "https://arxiv.org/html/2601.22083/imgs/ganpo_dpo_gemma.png",
                "caption": "Figure 6:Evolution of reward margins during training on Gemma2-2B-it, comparing DPO and GANPO.",
                "position": 1095
            },
            {
                "img": "https://arxiv.org/html/2601.22083/imgs/ganpo_simpo_gemma.png",
                "caption": "Figure 7:Reward margin comparison between SimPO and GANPO on Gemma2-2B-it.",
                "position": 1098
            },
            {
                "img": "https://arxiv.org/html/2601.22083/imgs/ganpo_dpo_llama.png",
                "caption": "Figure 8:Evolution of reward margins during training on Llama-3-8B-Instruct, comparing DPO and GANPO.",
                "position": 1101
            },
            {
                "img": "https://arxiv.org/html/2601.22083/imgs/ganpo_simpo_llama.png",
                "caption": "Figure 9:Reward margin comparison between SimPO and GANPO on Llama-3-8B-Instruct.",
                "position": 1104
            }
        ]
    },
    {
        "header": "Appendix DAdditional Empirical Visualizations",
        "images": []
    }
]