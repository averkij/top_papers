[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.00810/x1.png",
                "caption": "Figure 1:An example of GUI-AIMA with optional two-step GUI grounding for high-res screenshots.",
                "position": 130
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.00810/x2.png",
                "caption": "Figure 2:With user queryùí¨\\mathcal{Q}, screenshot patchesùí±\\mathcal{V}and multi-head attentions{ùêÄl,h}l‚àà[L],h‚àà[H]\\{\\mathbf{A}^{l,h}\\}_{l\\in[L],h\\in[H]}from the MLLM, the vanilla attention grounding needs additional aggregation between all query tokens‚Äô grounding vectors. In our proposed simplified version, a special<ANCHOR>token can learn to implicitly aggregate all query tokens. Then we aggregate grounding vectors of<ANCHOR>token across layers and heads with carefully designed weights to produce the patch-wise predictions.",
                "position": 194
            },
            {
                "img": "https://arxiv.org/html/2511.00810/x3.png",
                "caption": "Figure 3:Details about how to compute the final patch-wise prediction based on the grounding vectors of the<ANCHOR>token: GUI-AIMA first specifies visual-sink query tokens via computing hidden state similarities between query tokens and visual patches; Then it computes weightsùíò\\bm{w}of each attention head based on visual-sink query tokens inEq.8; Finally, GUI-AIMA aggregates the grounding vectors of<ANCHOR>token across layers and heads inEq.4.",
                "position": 282
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.00810/x4.png",
                "caption": "Table 4:Ablation results on ScreenSpot-v2 and ScreenSpot-Pro of coordinate-free GUI grounding methods fine-tuned with a fixed 45k randomly sampled dataset. Variants inbluerepresent the selected settings for GUI-AIMA.",
                "position": 1233
            },
            {
                "img": "https://arxiv.org/html/2511.00810/x4.png",
                "caption": "Figure 4:Analysis experiment results on ScreenSpot-pro.Relax@kmeasures how many previously incorrect offset predictions are recovered when the ground-truth bounding box is expanded by k visual patches along each dimension.Recoveredrefers to the number of offset predictions corrected by the second step, whileLostrefers to the number of predictions that degrade after the second step.",
                "position": 1461
            },
            {
                "img": "https://arxiv.org/html/2511.00810/x4.png",
                "caption": "Figure 5:Model convergence of the 45k ablation dataset on ScreenSpot-Pro benchmark.",
                "position": 1548
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix AAnalysis for Visual-sink Query Token",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.00810/x5.png",
                "caption": "Figure 6:Magnitude of normalized visual correlation of query tokens computed from hidden states (‚ÄúEmbedding‚Äù) and from multi-head self-attention (‚ÄúAttention‚Äù).",
                "position": 1572
            },
            {
                "img": "https://arxiv.org/html/2511.00810/x5.png",
                "caption": "",
                "position": 1575
            },
            {
                "img": "https://arxiv.org/html/2511.00810/x6.png",
                "caption": "",
                "position": 1584
            },
            {
                "img": "https://arxiv.org/html/2511.00810/x7.png",
                "caption": "",
                "position": 1590
            },
            {
                "img": "https://arxiv.org/html/2511.00810/x8.png",
                "caption": "",
                "position": 1598
            },
            {
                "img": "https://arxiv.org/html/2511.00810/x9.png",
                "caption": "",
                "position": 1608
            }
        ]
    },
    {
        "header": "Appendix BImplementations of Baselines",
        "images": []
    },
    {
        "header": "Appendix CExtra details of ANCHOR token Implementations",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.00810/fig/vis_v2/lock.png",
                "caption": "(a)Lock the memo.",
                "position": 1647
            },
            {
                "img": "https://arxiv.org/html/2511.00810/fig/vis_v2/lock.png",
                "caption": "(a)Lock the memo.",
                "position": 1650
            },
            {
                "img": "https://arxiv.org/html/2511.00810/fig/vis_v2/view.png",
                "caption": "(b)View the language & region settings.",
                "position": 1656
            },
            {
                "img": "https://arxiv.org/html/2511.00810/fig/vis_v2/scan.png",
                "caption": "(c)Scan QR code.",
                "position": 1661
            },
            {
                "img": "https://arxiv.org/html/2511.00810/fig/vis_v2/settings.png",
                "caption": "(d)View settings of cocowhip light.",
                "position": 1667
            },
            {
                "img": "https://arxiv.org/html/2511.00810/fig/vis_v2/add.png",
                "caption": "(e)Add a new page.",
                "position": 1673
            },
            {
                "img": "https://arxiv.org/html/2511.00810/fig/vis_pro/add.png",
                "caption": "(a)Add long title.",
                "position": 1680
            },
            {
                "img": "https://arxiv.org/html/2511.00810/fig/vis_pro/add.png",
                "caption": "(a)Add long title.",
                "position": 1683
            },
            {
                "img": "https://arxiv.org/html/2511.00810/fig/vis_pro/change.png",
                "caption": "(b)Change color theme in PyCharm.",
                "position": 1689
            },
            {
                "img": "https://arxiv.org/html/2511.00810/fig/vis_pro/save.png",
                "caption": "(a)Save a file.",
                "position": 1696
            },
            {
                "img": "https://arxiv.org/html/2511.00810/fig/vis_pro/save.png",
                "caption": "(a)Save a file.",
                "position": 1699
            },
            {
                "img": "https://arxiv.org/html/2511.00810/fig/vis_pro/select.png",
                "caption": "(b)Select the only feature.",
                "position": 1705
            }
        ]
    },
    {
        "header": "Appendix DGUI Grounding Examples of GUI-AIMA",
        "images": []
    }
]