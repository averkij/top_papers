[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.23127/x1.png",
                "caption": "Figure 2:Overall architecture of DualCamCtrl.DualCamCtrl adopts a dual-branch framework that simultaneously generates RGB and depth video latents from an input image and its corresponding depth map.\nThe two latents are then element-wise added to the encoded Plücker embedding and concatenated with noise (Sec.3.2).\nSubsequently, the two modalities interact through our proposed SIGMA mechanism and fusion block (Sec.3.3).\nDuring training, both predictions are supervised by their respective loss functions (Sec.3.4).",
                "position": 236
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.23127/x2.png",
                "caption": "Figure 3:(a) Illustration of modality misalignment.Independent RGB and depth latent evolution leads misalignment across frames. This motivates the design of our SIGMA strategy to establish coherent cross-modal alignment.(b) Comparison with one-way alignment.One-way alignment transfers information unidirectionally, leading to misalignment on local semantics.(c) Comparison with geometry-guided alignmentUnder the geometry-guided setting, geometry cues evolved too quickly and become inconsistent with RGB motion.",
                "position": 296
            },
            {
                "img": "https://arxiv.org/html/2511.23127/x3.png",
                "caption": "Figure 4:Comparison of fusion strategies.(1.a) Previous shallow linear fusion operates in a pixel-wise manner, ignoring temporal and spatial context and causing inconsistency over time.\n(1.b) We introduce 3D Fusion strategy to extend the fusion formulation from 1D to 3D by incorporating 3D operations.\n(2) Traditional linear-layer fusion often leads to visual artifacts, while our methods produce smoother, more coherent results.",
                "position": 312
            },
            {
                "img": "https://arxiv.org/html/2511.23127/x4.png",
                "caption": "",
                "position": 316
            }
        ]
    },
    {
        "header": "4Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.23127/x5.png",
                "caption": "Figure 5:CKAvs.pose; early-stage effects.(a) RGB branch shows strongest camera motion alignment in early–mid layers.\n(b) Extra early steps yield the largest quality gains.\n(c) Stronger early-stage weights lower CKA variance and improve FVD.",
                "position": 391
            },
            {
                "img": "https://arxiv.org/html/2511.23127/x6.png",
                "caption": "Figure 6:Importance of late denoising stages.(1) Rotation/Translation errorvs.number of denoising steps under different strengths: once early steps pass a threshold, adding more early steps yields diminishing returns in geometric consistency, whereas late steps are key for finer appearance fidelity.\n(2) Late denoising sharpens object boundaries and surface details, indirectly improving pose-aware consistency. Please zoom in for better view.",
                "position": 574
            },
            {
                "img": "https://arxiv.org/html/2511.23127/x7.png",
                "caption": "",
                "position": 578
            },
            {
                "img": "https://arxiv.org/html/2511.23127/x8.png",
                "caption": "Figure 7:Comparison between our method and other state-of-the-art approaches.Given the same camera pose and input image as generation conditions, our method achieves the best alignment between camera motion and scene dynamics, producing the most visually accurate video.\nThe ’++’ signs marked in the figure serve asanchors to indicate specific reference points for better visual comparison.",
                "position": 586
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.23127/x9.png",
                "caption": "Figure 8:Effect of Two-Stage Training:Top: The single-stage model fails to converge properly.Bottom: Consequently, the geometry alignment is suboptimal, highlighting the importance of disentangling appearance and geometry learning in the two-stage training approach.",
                "position": 801
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Further Details of the DualCamCtrl Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.23127/x10.png",
                "caption": "Figure 9:Architecture of 3D-Aware Fusion Block.(a) Design of the fusion block. The fusion block consists of a bottleneck embedding, a series of 3D convolutional layers, a zero-initialized convolutional output layer.\n(b) Detail of the 3D convolutional blocks. Here, BE stands for bottleneck embedding. DW-Conv stands for depthwise 3D convolution. PW-Conv stands for pointwise 3D convolution. Please zoom in for a better view.",
                "position": 900
            },
            {
                "img": "https://arxiv.org/html/2511.23127/x11.png",
                "caption": "Figure 10:Training scheme of DualCamCtrl.The overall training follows a two-stage training scheme: (I) In the decouple stage (denoted by only the black arrows), the RGB and depth branches are trained separately (with no interaction) to learn modality-specific representations. (II) In the fusion stage (both red and black arrows). In this process, SIGMA mechanism is applied through a 3D-aware fusion module, enabling effective cross-modal integration and improved synthesis consistency.",
                "position": 904
            },
            {
                "img": "https://arxiv.org/html/2511.23127/x12.png",
                "caption": "Figure 11:Architecture of DualCamCtrl under the T2V setting.The model operates without any visual conditioning. The textual input is first encoded by a T5 encoder and fed into two diffusion transformers to condition both RGB and depth generation.",
                "position": 921
            }
        ]
    },
    {
        "header": "8Findings and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.23127/x13.png",
                "caption": "Figure 12:Illustration ofdepth-conditioned generation. The depth-conditioned generation may initially be processed similarly to low-light or hazy RGB input. The resulting videos maintain structural coherence, reflecting cross-modal adaptability.",
                "position": 940
            },
            {
                "img": "https://arxiv.org/html/2511.23127/x14.png",
                "caption": "Figure 13:CKA analysis of depth evolution during denoising; importance of early stage denoising.(a) Depth branch exhibits a gradual and persistent increase in similarity, indicating continuous geometric refinement and improved spatial consistency as denoising progresses. (b) Insufficient early-stage denoising results in high variance of CKA similarity across steps, suggesting unstable feature alignment. This instability propagates to later stages, degrading overall spatial coherence and FVD performance.",
                "position": 954
            },
            {
                "img": "https://arxiv.org/html/2511.23127/x15.png",
                "caption": "Figure 14:Comparison of learning rate.High learning rate (i.e.1​e−51e^{-5}) lead to unstable training and fail to converge properly. Please zoom in for better visualization.",
                "position": 970
            },
            {
                "img": "https://arxiv.org/html/2511.23127/x16.png",
                "caption": "Figure 15:Solely Relying on Plücker Embedding Lacks Scene Understanding.Given the same camera motion, variations in camera movement are observed in the generated video by[wan2025123]across different scenes, exhibiting distinct behaviors shown in (a), (b), and (c).",
                "position": 973
            }
        ]
    },
    {
        "header": "9Additional Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.23127/x17.png",
                "caption": "Figure 16:Comparison of fusion strategy.Single-frame depth conditioning fails to maintain a coherent understanding across the entire video, resulting in artifacts and unnatural generation. In contrast, our multi-frame depth synthesis methods help produce more plausible outputs. Please zoom in for more details.",
                "position": 1109
            }
        ]
    },
    {
        "header": "10Additional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.23127/x18.png",
                "caption": "Figure 17:Illustration of coupling the RGB and depth modalities within a single branch leads to interference.This interference causes misalignment between the two modalities and adversely affecting the generation quality.",
                "position": 1147
            },
            {
                "img": "https://arxiv.org/html/2511.23127/x19.png",
                "caption": "Figure 18:Failure case under large motion.When the inter-frame motion is large, severe artifacts emerge. Please zoom in for details.",
                "position": 1243
            },
            {
                "img": "https://arxiv.org/html/2511.23127/x20.png",
                "caption": "Figure 19:More qualitative results of DualCamCtrl.Please zoom in for better visualization.",
                "position": 1555
            },
            {
                "img": "https://arxiv.org/html/2511.23127/x21.png",
                "caption": "Figure 20:More qualitative results of DualCamCtrl.Please zoom in for better visualization.",
                "position": 1558
            }
        ]
    },
    {
        "header": "11Discussion and Limitation",
        "images": []
    }
]