[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14967/x1.png",
                "caption": "Figure 1:Proportion of zero-advantage groups during training—IGPO vs. GRPO on Qwen2.5-7B/3B-Instruct.",
                "position": 153
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14967/x2.png",
                "caption": "Figure 2:The training pipeline of IGPO. (Upper) Turn-level information gain rewards are computed by measuring changes in ground-truth probability and combined with the outcome reward to derive discounted advantages. (Lower) Each rollout contains at mostT−1T-1interaction turns, where each turn includes a reasoning step, a tool call, and the returned tool response, followed by a final answer turn. During optimization, the loss on tool response is masked out.",
                "position": 239
            }
        ]
    },
    {
        "header": "3Information Gain-based Policy Optimization",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14967/x3.png",
                "caption": "(a)NQ",
                "position": 847
            },
            {
                "img": "https://arxiv.org/html/2510.14967/x3.png",
                "caption": "(a)NQ",
                "position": 850
            },
            {
                "img": "https://arxiv.org/html/2510.14967/x4.png",
                "caption": "(b)TQ",
                "position": 855
            },
            {
                "img": "https://arxiv.org/html/2510.14967/x5.png",
                "caption": "(c)HotpotQA",
                "position": 860
            },
            {
                "img": "https://arxiv.org/html/2510.14967/x6.png",
                "caption": "(d)2Wiki",
                "position": 865
            },
            {
                "img": "https://arxiv.org/html/2510.14967/x7.png",
                "caption": "(e)Musique",
                "position": 871
            },
            {
                "img": "https://arxiv.org/html/2510.14967/x8.png",
                "caption": "(f)Bamboogle",
                "position": 876
            },
            {
                "img": "https://arxiv.org/html/2510.14967/x9.png",
                "caption": "(g)PopQA",
                "position": 881
            },
            {
                "img": "https://arxiv.org/html/2510.14967/x10.png",
                "caption": "Figure 4:Mean reduction in ground-truth answer entropy from the initial query (Turn 0) to the last non-answer turn (T−1T\\!-\\!1) during training.",
                "position": 924
            },
            {
                "img": "https://arxiv.org/html/2510.14967/x10.png",
                "caption": "Figure 4:Mean reduction in ground-truth answer entropy from the initial query (Turn 0) to the last non-answer turn (T−1T\\!-\\!1) during training.",
                "position": 927
            },
            {
                "img": "https://arxiv.org/html/2510.14967/x11.png",
                "caption": "Figure 5:Token Efficiency: average performance with respect to the number of tokens used for gradient updates.",
                "position": 932
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion, Limitations and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATheoretical Analysis",
        "images": []
    },
    {
        "header": "Appendix BProof for Theoretical Analysis",
        "images": []
    },
    {
        "header": "Appendix CMore Implementation Details",
        "images": []
    },
    {
        "header": "Appendix DMore Discussion and\nExperimental Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14967/x12.png",
                "caption": "Figure 6:Case study showing a scenario where the final answer is incorrect but contains a single correct retrieval turn. IGPO provides a process reward for this turn, improving token utilization.",
                "position": 1949
            },
            {
                "img": "https://arxiv.org/html/2510.14967/x13.png",
                "caption": "Figure 7:Case study illustrating a situation where the first round of retrieval failed, but the second and third rounds successfully located the correct evidence and produced the right answer. In this case, IGPO imposes a penalty on the erroneous retrieval in the first round.",
                "position": 1952
            }
        ]
    },
    {
        "header": "Appendix EComparison between GRPO and IGPO",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14967/x14.png",
                "caption": "Figure 8:Prompt template used in our experiments.",
                "position": 2113
            }
        ]
    },
    {
        "header": "Appendix FPrompt template used in our experiments.",
        "images": []
    }
]