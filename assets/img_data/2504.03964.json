[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03964/extracted/6337549/icd.png",
                "caption": "Figure 1:Medical Code Ontologies Construction:An illustration of structured ontology construction across multiple ICD code versions. Each row represents a distinct medical concept identified by its version-specific code and description, which is then converted into a standardized, descriptive natural language representation. This process facilitates alignment and interoperability across evolving coding schemes. This setup is inspired by methods like(Hegselmann et¬†al.,2023; Ono and Lee,2024)which use text templates to serialize tabular data.",
                "position": 437
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03964/extracted/6337549/final-tsne-side-by-side2.png",
                "caption": "Figure 2:ICD-9 tSNE Latent Space Visualization:A tSNE visualization of the ICD 9 Diagnoses codes using modernBERT versus Clinical ModernBERT. This visualization provides the added use of adding the medical code ontologies as a pre-training source to encode coded language seen frequently in clinical practice.",
                "position": 887
            },
            {
                "img": "https://arxiv.org/html/2504.03964/extracted/6337549/time.png",
                "caption": "Figure 3:Comparative Performance Analysis of BERT Models:This figure demonstrates the processing time requirements across three BERT variants (Distil-BERT, BioClinicalBERT, and Clinical ModernBERT) as data volume increases from 10,000 to 100,000 points. BioClinicalBERT consistently shows the highest computational demand, requiring approximately 1.4x the processing time of Distil-BERT and 1.6x that of Clinical ModernBERT at maximum load. Clinical ModernBERT demonstrates superior efficiency, maintaining the lowest processing times across all data volumes, making it optimal for resource-constrained environments.",
                "position": 900
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADataset Details",
        "images": []
    },
    {
        "header": "Appendix BModel Optimizations",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03964/extracted/6337549/wandb.png",
                "caption": "Figure 4:Masked Language Modeling (MLM) Top-K Accuracies and Loss:We report top-K accuracies fork=1,5,10,25ùëò151025k=1,5,10,25italic_k = 1 , 5 , 10 , 25alongside MLM loss across three pre-training runs initialized with different learning rates (top to bottom:3√ó10‚àí33superscript1033\\times 10^{-3}3 √ó 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT,5√ó10‚àí45superscript1045\\times 10^{-4}5 √ó 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT,1√ó10‚àí51superscript1051\\times 10^{-5}1 √ó 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT). Higher learning rates yielded more stable convergence and avoided shallow local minima, suggesting improved exploration of the loss landscape. As expected, larger learning rates also introduced noisier gradient updates, which aligns with standard intuitions in stochastic optimization.",
                "position": 1590
            },
            {
                "img": "https://arxiv.org/html/2504.03964/extracted/6337549/wandb2.png",
                "caption": "",
                "position": 1594
            },
            {
                "img": "https://arxiv.org/html/2504.03964/extracted/6337549/wandb3.png",
                "caption": "",
                "position": 1596
            }
        ]
    },
    {
        "header": "Appendix CPre-training Code and Model Weights",
        "images": []
    }
]