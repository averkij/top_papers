[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20256/x1.png",
                "caption": "Figure 1:Overview of our approach.Our method Adv-GRPO improves text-to-image (T2I) generation in three ways:1) Alleviate Reward Hacking, achieving higher perceptual quality while maintaining comparable benchmark performance (e.g., PickScore, OCR), as shown in the top-left human evaluation panel;2) Visual Foundation Model as Reward, leveraging visual foundation models (e.g., DINO) for rich visual priors, leading to overall improvements as shown in middle-top human evaluation results;3) RL-based Distribution Transfer, enabling style customization by aligning generations with reference domains.",
                "position": 99
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20256/x2.png",
                "caption": "Figure 2:Human evaluation comparing Flow-GRPO and SD3 under PickScore and OCR rewards.",
                "position": 123
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20256/x3.png",
                "caption": "Figure 3:Pipeline of Adv-GRPO.\nThe generator is optimized using the GRPO loss, while the discriminator is trained to distinguish between generated samples and reference images, treated as negative and positive samples, respectively.\nThe discriminator serves as a reward model to provide feedback for the generator.",
                "position": 273
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20256/x4.png",
                "caption": "Figure 4:Human evaluation under PickScore- and OCR-based rewards. Our method Adv-GRPO improves image quality and aesthetics with PickScore reward ina), and for all metrics with OCR reward inb).\nCompared with the original model (SD3), PickScore reward trade-off aesthetic improvements with image quality degradation inc), OCR reward trade-off text-alignment from aesthetics degradation ind).",
                "position": 402
            },
            {
                "img": "https://arxiv.org/html/2511.20256/x5.png",
                "caption": "Figure 5:Visualizations under PickScore (Left) and OCR (Right) rewards. Our method Adv-GRPO alleviates reward hacking for both.",
                "position": 408
            },
            {
                "img": "https://arxiv.org/html/2511.20256/x6.png",
                "caption": "Figure 6:Human evaluation results under the visual foundation model (DINO) reward.\nUsing a foundation model as the reward, our RL method improves image aesthetics, quality, and text alignment compared with the original SD3 model (a), and significantly outperforms Flow-GRPO under the DINO similarity reward (b) and PickScore reward (c).",
                "position": 449
            },
            {
                "img": "https://arxiv.org/html/2511.20256/x7.png",
                "caption": "Figure 7:Visualizations under the DINO reward model.\nWithadversarial DINO reward, our method shows better visual quality.",
                "position": 453
            },
            {
                "img": "https://arxiv.org/html/2511.20256/x8.png",
                "caption": "Figure 8:Visual comparison between our method (DINO reward)\nand SD3 across different task prompts.",
                "position": 517
            },
            {
                "img": "https://arxiv.org/html/2511.20256/x9.png",
                "caption": "Figure 9:Ablation results. (a) Visualizations with different\nnumbers of reference images, showing effectiveness even\nwith 200 samples. (b) Visualizations of ablation studies on SFT, KL regularization, multi-reward optimization, and our method Adv-GRPO.",
                "position": 584
            },
            {
                "img": "https://arxiv.org/html/2511.20256/x10.png",
                "caption": "Figure 10:Human evaluation comparing our DINO-reward model with SFT, where our method performs better.",
                "position": 595
            },
            {
                "img": "https://arxiv.org/html/2511.20256/x11.png",
                "caption": "Figure 11:Application: Style transfer with the adversarial DINO reward.\nOur method successfully transfers the SD3 model to target visual domains,\nincludingAnimeandSci-Fistyles.",
                "position": 686
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7More Implementation Details",
        "images": []
    },
    {
        "header": "8Visualizations Under Our Method",
        "images": []
    },
    {
        "header": "9More Visualizations on Style Customization",
        "images": []
    },
    {
        "header": "10Using SigLIP for Optimization",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20256/x12.png",
                "caption": "Figure 12:More Visualizations about alleviating reward hacking under PickScore and OCR reward models.",
                "position": 1430
            },
            {
                "img": "https://arxiv.org/html/2511.20256/x13.png",
                "caption": "Figure 13:Additional visualizations using the DINO reward model. Our method produces images with consistently higher visual quality.",
                "position": 1435
            },
            {
                "img": "https://arxiv.org/html/2511.20256/x14.png",
                "caption": "Figure 14:More visualizations with DINO reward using different benchmark OCR and GenEval prompts.",
                "position": 1440
            },
            {
                "img": "https://arxiv.org/html/2511.20256/x15.png",
                "caption": "Figure 15:Visualizations with the SigLIP reward. Compared with SD3, using other visual foundation models such as SigLIP as the reward function can also lead to overall improvements in image quality.",
                "position": 1443
            },
            {
                "img": "https://arxiv.org/html/2511.20256/x16.png",
                "caption": "Figure 16:More style customization results. Using anime reference images, our method effectively transfers the base modelâ€™s style to an anime aesthetic, guided by the provided samples.",
                "position": 1448
            },
            {
                "img": "https://arxiv.org/html/2511.20256/x17.png",
                "caption": "Figure 17:Reward curves under different reward models.We shows the training dynamics of our method and the baseline under three reward models:\n(a) PickScore, (b) OCR accuracy, and (c) DINO cosine similarity.",
                "position": 1453
            },
            {
                "img": "https://arxiv.org/html/2511.20256/x18.png",
                "caption": "Figure 18:Screenshot of the interface used in our human evaluation study.",
                "position": 1460
            }
        ]
    },
    {
        "header": "11Reward Curve",
        "images": []
    },
    {
        "header": "12Human Evaluation",
        "images": []
    }
]