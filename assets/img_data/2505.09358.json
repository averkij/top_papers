[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": []
    },
    {
        "header": "IIRelated Work",
        "images": []
    },
    {
        "header": "IIIBase Model: Marigold-Depth",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.09358/x1.png",
                "caption": "Figure 1:Overview of the Marigold fine-tuning protocol.Starting from a pretrained Stable Diffusion, we encode the imageùê±ùê±\\mathbf{x}bold_xand depth0‚Å¢p‚Å¢t0ùëùùë°0pt0 italic_p italic_tinto the latent space using the original Stable Diffusion VAE.\nWe fine-tune just the U-Net by optimizing the standard diffusion objective relative to the depth latent code.\nImage conditioning is achieved by concatenating the two latent codes before feeding them into the U-Net.\nThe first layer of the U-Net is modified to accept concatenated latent codes.\nSee details in Sec.III-Band Sec.III-C.",
                "position": 452
            },
            {
                "img": "https://arxiv.org/html/2505.09358/x2.png",
                "caption": "Figure 2:Overview of the Marigold inference scheme.Given an imageùê±ùê±\\mathbf{x}bold_x, we encode it with the original Stable Diffusion VAE into the latent codeùê≥(ùê±)superscriptùê≥ùê±\\mathbf{z}^{(\\mathbf{x})}bold_z start_POSTSUPERSCRIPT ( bold_x ) end_POSTSUPERSCRIPT, and concatenate with the depth latentùê≥t(0‚Å¢p‚Å¢t)subscriptsuperscriptùê≥0ùëùùë°ùë°\\mathbf{z}^{(0pt)}_{t}bold_z start_POSTSUPERSCRIPT ( 0 italic_p italic_t ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTbefore giving it to the modified fine-tuned U-Net on every denoising iteration.\nAfter executing the schedule ofTùëáTitalic_Tsteps, the resulting depth latentùê≥0(0‚Å¢p‚Å¢t)subscriptsuperscriptùê≥0ùëùùë°0\\mathbf{z}^{(0pt)}_{0}bold_z start_POSTSUPERSCRIPT ( 0 italic_p italic_t ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTis decoded into an image whose 3 channels are averaged to get the final estimation0^‚Å¢p‚Å¢t^0ùëùùë°\\hat{0}ptover^ start_ARG 0 end_ARG italic_p italic_t.\nSee Sec.III-Dfor details.",
                "position": 525
            },
            {
                "img": "https://arxiv.org/html/2505.09358/extracted/6438250/img/qualitative_depth_2/nyu_8col.png",
                "caption": "Figure 3:Qualitative comparisonof monocular depth estimation methods across different datasets.\nMarigold excels at capturing thin structures¬†(e.g., chair legs) and preserving overall layout of the scene¬†(e.g., walls in ETH3D and chairs in DIODE).",
                "position": 1497
            },
            {
                "img": "https://arxiv.org/html/2505.09358/extracted/6438250/img/qualitative_depth_2/kitti_8col.png",
                "caption": "",
                "position": 1579
            },
            {
                "img": "https://arxiv.org/html/2505.09358/extracted/6438250/img/qualitative_depth_2/eth3d_8col.png",
                "caption": "",
                "position": 1586
            },
            {
                "img": "https://arxiv.org/html/2505.09358/extracted/6438250/img/qualitative_depth_2/scannet_8col.png",
                "caption": "",
                "position": 1593
            },
            {
                "img": "https://arxiv.org/html/2505.09358/extracted/6438250/img/qualitative_depth_2/diode_8col.png",
                "caption": "",
                "position": 1600
            },
            {
                "img": "https://arxiv.org/html/2505.09358/x3.png",
                "caption": "Figure 4:Ablation of ensemble size.We observe a monotonic improvement with the growth of ensemble size. This improvement starts to diminish after 10 predictions per sample.",
                "position": 1610
            },
            {
                "img": "https://arxiv.org/html/2505.09358/extracted/6438250/img/qualitative_normal/nyu_nromal_556_compressed.jpg",
                "caption": "Figure 5:Qualitative comparison (unprojected from depth, colored as normals)of monocular depth estimation methods across different datasets.\nMarigold-Depth stands out for its superior reconstruction of flat surfaces and detailed structures.",
                "position": 1616
            },
            {
                "img": "https://arxiv.org/html/2505.09358/extracted/6438250/img/qualitative_normal/scannet1_358_compressed.jpg",
                "caption": "",
                "position": 1682
            },
            {
                "img": "https://arxiv.org/html/2505.09358/extracted/6438250/img/qualitative_normal/diode426_compressed.jpg",
                "caption": "",
                "position": 1689
            },
            {
                "img": "https://arxiv.org/html/2505.09358/x4.png",
                "caption": "Figure 6:Ablation of denoising steps.One denoising step is sufficient with DDIM-trailing[23](default inv1.1).\nDDIM[18](default inv1.0) requires at least 4-10 steps.",
                "position": 1699
            }
        ]
    },
    {
        "header": "IVSurface Normals Estimation Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.09358/extracted/6438250/img/normals/nyuv2_quantitative_compressed.jpg",
                "caption": "Figure 7:Qualitative comparisonof monocular surface normals estimation methods across different datasets.\nCompared to baseline methods, Marigold-Normals demonstrates superior performance in handling complex scene layouts and shows greater robustness to motion blur and reflections.",
                "position": 2266
            },
            {
                "img": "https://arxiv.org/html/2505.09358/extracted/6438250/img/normals/diode_quantitative_compressed.jpg",
                "caption": "",
                "position": 2332
            },
            {
                "img": "https://arxiv.org/html/2505.09358/extracted/6438250/img/normals/scannet_quantitative_compressed.jpg",
                "caption": "",
                "position": 2339
            },
            {
                "img": "https://arxiv.org/html/2505.09358/extracted/6438250/img/normals/ibims_quantitative_compressed.jpg",
                "caption": "",
                "position": 2346
            },
            {
                "img": "https://arxiv.org/html/2505.09358/x5.png",
                "caption": "Figure 8:Ablation of ensemble sizefor Marigold-Normals.\nThe performance consistently improves with increasing ensemble size. Diminishing returns begin after 10 predictions per sample.",
                "position": 2388
            },
            {
                "img": "https://arxiv.org/html/2505.09358/x6.png",
                "caption": "Figure 9:Ablation of denoising stepsfor Marigold-Normals.\nThe best performance in benchmarks is obtained at 4 denoising steps.\nVisually, one step is sufficient in most cases.",
                "position": 2394
            },
            {
                "img": "https://arxiv.org/html/2505.09358/extracted/6438250/img/normals/fine_details.jpg",
                "caption": "Figure 10:Prediction granularity and denoising steps.\nFrom left to right, we visualize predictions with 1, 4, and 20 denoising steps during inference.\nBy increasing the number of steps, fine details, such as the cat‚Äôs fur, become more pronounced.",
                "position": 2450
            }
        ]
    },
    {
        "header": "VIntrinsic Decomposition Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.09358/extracted/6438250/img/albedo_interiorverse_v01.jpg",
                "caption": "Figure 11:Qualitative comparisonof Marigold-IID-Appearance (left) and Marigold-IID-Lighting (right).Left: albedo and material (with roughness in the red channel and metallicity in the green channel) predictions on the InteriorVerse test set.Right: albedo and diffuse shading predictions on the HyperSim test set.\nPredictions of Marigold-IID-Appearance and Marigold-IID-Lighting contain less baked-in shading and are more consistent with the ground truth.",
                "position": 2643
            },
            {
                "img": "https://arxiv.org/html/2505.09358/extracted/6438250/img/albedo_hypersim_v00.jpg",
                "caption": "",
                "position": 2644
            },
            {
                "img": "https://arxiv.org/html/2505.09358/extracted/6438250/img/VaryingLighting.jpg",
                "caption": "Figure 12:Robustness to varying lighting conditions. The Marigold-IID models generate consistent predictions across different environmental lighting setups of the same scene.",
                "position": 2653
            },
            {
                "img": "https://arxiv.org/html/2505.09358/extracted/6438250/img/gallery_v01.jpg",
                "caption": "Figure 13:Marigold in-the-wild results ‚Äì all modalities.\nOur fine-tuning protocol enables generalization across multiple modalities.\nNone of the fine-tuning datasets included humans, animals, food, engines, or toys, attesting to the successful carry-over of the rich generative prior to downstream tasks.",
                "position": 2658
            }
        ]
    },
    {
        "header": "VILatent Consistency Model (LCM)",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.09358/x7.png",
                "caption": "Figure 14:Overview of Marigold-LCM distillation.To train Marigold-LCM, we initialize three replicas of the base Marigold:\nthe Teacher (Œ¶Œ¶\\Phiroman_Œ¶), the Target (Œò‚àísuperscriptŒò\\Theta^{-}roman_Œò start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT), and the Student (ŒòŒò\\Thetaroman_Œò).\nThe student is updated via optimization of the consistency objective, and the target is updated via EMA of student weights.\nAt each training step, the student learns to predict the same clean latentùê≥ùüé(ùêù)subscriptsuperscriptùê≥ùêù0\\mathbf{z^{(d)}_{0}}bold_z start_POSTSUPERSCRIPT ( bold_d ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT bold_0 end_POSTSUBSCRIPTas produced by the teacher after applying the DDIM step of sizekùëòkitalic_k.\nEach model takes the image conditionùê≥(ùê±)superscriptùê≥ùê±\\mathbf{z^{(x)}}bold_z start_POSTSUPERSCRIPT ( bold_x ) end_POSTSUPERSCRIPTand the input timestep, similarly to Fig.1.",
                "position": 2764
            }
        ]
    },
    {
        "header": "VIIHigh Resolution Depth Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.09358/extracted/6438250/img/HighResolutionMethodv11.jpg",
                "caption": "Figure 15:High-resolution Marigold Pipeline.We first create a global predictionùêù^(g)superscript^ùêùùëî\\hat{\\mathbf{d}}^{(g)}over^ start_ARG bold_d end_ARG start_POSTSUPERSCRIPT ( italic_g ) end_POSTSUPERSCRIPTwith the original Marigold-Depth pipeline at the native processing resolution.\nThis prediction is then used as an additional conditioning variable in the upsampling diffusion process, which upsamples the prediction in a patch-based, MultiDiffusion forward pass.",
                "position": 2829
            },
            {
                "img": "https://arxiv.org/html/2505.09358/x8.png",
                "caption": "Figure 16:Quantitative comparisonof Marigold-HR, Marigold, and other SOTA methods.\nPredictions on Middlebury 2014 and Booster are aligned to the ground truth and visualized with the same color mapping. The predictions for the in-the-wild example are per sample normalized. Marigold-HR produces fine-grained outputs while also maintaining global context.",
                "position": 2923
            }
        ]
    },
    {
        "header": "VIIIConclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]