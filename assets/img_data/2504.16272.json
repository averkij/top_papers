[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16272/x1.png",
                "caption": "Figure 1:Overview of the bilevel optimization setup to find the best reward shape and the optimal policy. The pipeline involves an outer and inner training loop, where the outer step optimizes the Bayesian optimization model and samples the weights for our reward shape. The inner step optimizes the classic RLHF objective.",
                "position": 120
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Explainable Reward Shaping as a Bilevel Optimization",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16272/x2.png",
                "caption": "Figure 2:Redistribution sequence of the scalar reward prediction over the explanation feature attributions after softmax normalization. A darker red highlights a much stronger positive contribution, while a deeper blue indicates a more negative contribution.",
                "position": 270
            },
            {
                "img": "https://arxiv.org/html/2504.16272/x3.png",
                "caption": "Table 1:Comparison of HH-RLHF (Only Helpful) and Ultrafeedback. Winrate is calculated via AlpacaEval, with the reference model being the SFT model’s generation.\n* indicates strong baselines that we have implemented.\n# means the number of dense reward types used.\nScore refers to the average test set reward and Win\n(%) refers to the length-controlled win-rate against the baseline SFT generations. We usegpt4-turboas the judge for both AlpacaEval2 and MTBench. Bolded and underlined numbers highlight the best performance.",
                "position": 295
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16272/x3.png",
                "caption": "(a)SHAP vs. BO variants",
                "position": 477
            },
            {
                "img": "https://arxiv.org/html/2504.16272/x3.png",
                "caption": "(a)SHAP vs. BO variants",
                "position": 480
            },
            {
                "img": "https://arxiv.org/html/2504.16272/x4.png",
                "caption": "(b)Validation reward after each BO trial.",
                "position": 485
            },
            {
                "img": "https://arxiv.org/html/2504.16272/x5.png",
                "caption": "(c)Mean dense reward shape",
                "position": 490
            },
            {
                "img": "https://arxiv.org/html/2504.16272/x6.png",
                "caption": "(a)Explanation-based rewards training curves vs. baselines",
                "position": 498
            },
            {
                "img": "https://arxiv.org/html/2504.16272/x6.png",
                "caption": "(a)Explanation-based rewards training curves vs. baselines",
                "position": 501
            },
            {
                "img": "https://arxiv.org/html/2504.16272/x7.png",
                "caption": "(b)PPO Value loss",
                "position": 506
            },
            {
                "img": "https://arxiv.org/html/2504.16272/x8.png",
                "caption": "Figure 5:(Top)The weight transition between trials for SHAPley scores.(Bottom)The weight transition between trials for LIME scores. The black boxes indicate the ”best weights” sampled by the BO model.",
                "position": 533
            },
            {
                "img": "https://arxiv.org/html/2504.16272/x9.png",
                "caption": "Figure 6:(Ultrafeedback)The top left represents the baseline per-token reward without shaping. The color of each token represents the reward received, with a darker color representing a higher proportion of the reward assigned. A more uniform coloring indicates a more uniform assignment of the scalar reward to each token, while a contrasting light/dark coloring indicates a more skewed assignment.",
                "position": 555
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AComputing the Explainability Scores from a LLM Reward Model",
        "images": []
    },
    {
        "header": "Appendix BProof of Policy Invariance of Explainability Methods",
        "images": []
    },
    {
        "header": "Appendix CComputation Overhead of Explanation Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16272/x10.png",
                "caption": "",
                "position": 1788
            }
        ]
    },
    {
        "header": "Appendix DPractical Implementation Details",
        "images": []
    },
    {
        "header": "Appendix EQualitative Examples",
        "images": []
    }
]