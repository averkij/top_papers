[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19955/x1.png",
                "caption": "Figure 1:An overview of the framework of MLR-Bench, consisting of both an end-to-end evaluation (left) and a stepwise evaluation (right), each of which uses LLM judges to automatically assess performance over 201 tasks. For end-to-end evaluation, we use the same model as backbone in idea generation, proposal generation and paper writing. For stepwise evaluation, various models are tested and compared within each step.",
                "position": 123
            }
        ]
    },
    {
        "header": "2MLR-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19955/x2.png",
                "caption": "Figure 2:The number of tasks grouped by our ML primary categories.",
                "position": 197
            },
            {
                "img": "https://arxiv.org/html/2505.19955/x2.png",
                "caption": "Figure 2:The number of tasks grouped by our ML primary categories.",
                "position": 200
            }
        ]
    },
    {
        "header": "3How Well Can AI Agents Conduct Open-Ended Research?",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19955/x3.png",
                "caption": "Table 2:Evaluated models(o4mini2025,;claude2025,;deepseekai2025deepseekr1incentivizingreasoningcapability,;ministral2024,;qwenteam2025qwen3,;gemini2025,)in different research stages.",
                "position": 335
            },
            {
                "img": "https://arxiv.org/html/2505.19955/x3.png",
                "caption": "Figure 3:Scores of two LLM judge models across seven review dimensions on ten tasks.",
                "position": 422
            }
        ]
    },
    {
        "header": "4How Well Is MLR-Judge Aligned with Human Reviewers?",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19955/x4.png",
                "caption": "Figure 4:Comparison of human-human and human-LLM absolute rating differences across five criteria, with corresponding Mann-Whitney U test p-values shown in the top-left panel. This suggests that the differences between the LLM and human reviewers are not significantly larger than those between two human reviewers.",
                "position": 727
            }
        ]
    },
    {
        "header": "5What Are the Key Factors Affecting AI-Generated Research Quality?",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19955/x5.png",
                "caption": "Figure 5:For a given paper with its supplementary code, both MLR-Judge and the human reviewer flagged the results as invalid‚ÄîMLR-Judge by inspecting the provided code, and the human reviewer by noting unrealisticR2superscriptùëÖ2R^{2}italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTscores. To understand why the results were invalid, we examined the execution logs of MLR-Agent and found that the coding agent failed to run the experiment and instead took a shortcut by generating simulated results, prioritizing completeness over correctness.",
                "position": 742
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix Contents",
        "images": []
    },
    {
        "header": "Appendix AExperimental Details",
        "images": []
    },
    {
        "header": "Appendix BPrompt",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19955/x6.png",
                "caption": "Table 31:The Google Form interface used to collect review scores, following the same reviewer rubrics as MLR-Judge (LABEL:tab:end2end_eval_prompt).",
                "position": 5139
            },
            {
                "img": "https://arxiv.org/html/2505.19955/x6.png",
                "caption": "Table 31:The Google Form interface used to collect review scores, following the same reviewer rubrics as MLR-Judge (LABEL:tab:end2end_eval_prompt).",
                "position": 5330
            },
            {
                "img": "https://arxiv.org/html/2505.19955/x6.png",
                "caption": "Table 31:The Google Form interface used to collect review scores, following the same reviewer rubrics as MLR-Judge (LABEL:tab:end2end_eval_prompt).",
                "position": 5536
            },
            {
                "img": "https://arxiv.org/html/2505.19955/x6.png",
                "caption": "Table 31:The Google Form interface used to collect review scores, following the same reviewer rubrics as MLR-Judge (LABEL:tab:end2end_eval_prompt).",
                "position": 5691
            },
            {
                "img": "https://arxiv.org/html/2505.19955/x6.png",
                "caption": "Table 31:The Google Form interface used to collect review scores, following the same reviewer rubrics as MLR-Judge (LABEL:tab:end2end_eval_prompt).",
                "position": 5876
            },
            {
                "img": "https://arxiv.org/html/2505.19955/x6.png",
                "caption": "",
                "position": 6063
            },
            {
                "img": "https://arxiv.org/html/2505.19955/x7.png",
                "caption": "",
                "position": 6065
            }
        ]
    },
    {
        "header": "Appendix CHuman Study Details",
        "images": []
    }
]