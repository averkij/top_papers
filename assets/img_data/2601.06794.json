[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.06794/x1.png",
                "caption": "Figure 1:Comparison of critic paradigms.\n(a)Conventional Static Paradigms: Use decoupled, frozen critic modules initialized from off-the-shelf templates or fine-tuned separate models, resulting in static evaluation and inflexible feedback.\n(b)Our ECHO Paradigm: Policy and critic co-evolve organically.\nThe policy first generates an initial rolloutτo\\tau_{o}, refined toτr\\tau_{r}using the critic’s diagnostic guidancecc.\nBoth models are jointly updated, ensuring the critic’s diagnostic precision synchronizes with the policy’s evolving failure patterns.",
                "position": 213
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.06794/x2.png",
                "caption": "Figure 2:Overview of ECHO training with saturation-aware (SA) critic rewards. At steptt, the policyπθt\\pi_{\\theta_{t}}produces rolloutsτo\\tau_{o}, which are scored by a reward model to obtainsos_{o}. A criticπψt\\pi_{\\psi_{t}}generates critiques that are appended to the original query to elicit refined rolloutsτr\\tau_{r}, scored assrs_{r}. We compute the SA critic rewardrcr_{c}to emphasize last-mile improvements near saturation, and update the critic and policy synchronously to obtainπψt+1\\pi_{\\psi_{t+1}}andπθt+1\\pi_{\\theta_{t+1}}.",
                "position": 271
            }
        ]
    },
    {
        "header": "4Experiment Setup",
        "images": []
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.06794/x3.png",
                "caption": "Table 1:Main results on four open-world agent benchmarks.Boldindicates the best result within each benchmark.",
                "position": 467
            },
            {
                "img": "https://arxiv.org/html/2601.06794/x3.png",
                "caption": "Figure 3:Failure-pattern drift across training phases. We visualize failed trajectories fromearly,intermediate, andlatecheckpoints in a diagnosis embedding space using t-SNE, with contours indicating density regions.",
                "position": 636
            },
            {
                "img": "https://arxiv.org/html/2601.06794/x4.png",
                "caption": "Figure 4:Effect of saturation-aware gain shaping on last-mile refinement. We plot density scatter maps of pre-refinement and post-refinement rewards(so,sr)(s_{o},s_{r})on WebShop and SciWorld using Qwen3-4B. Points in the green region satisfysr>sos_{r}>s_{o}and correspond to reward-improving refinements, where higher density indicates more effective critiques. The highlighted high-score square marks the near-ceiling regime.",
                "position": 661
            },
            {
                "img": "https://arxiv.org/html/2601.06794/x5.png",
                "caption": "Figure 5:Training reward curves across four environments (Qwen3-4B).",
                "position": 719
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AEnvironments and Scoring Criteria",
        "images": []
    },
    {
        "header": "Appendix BMore Implementation Details",
        "images": []
    },
    {
        "header": "Appendix CPseudo-code for ECHO",
        "images": []
    },
    {
        "header": "Appendix DPrompt for Critic Model",
        "images": []
    },
    {
        "header": "Appendix ETask Examples and Case Studies",
        "images": []
    }
]