[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.11227/extracted/6285883/figures/example.jpg",
                "caption": "Figure 1:An illustration of several triples and graphs. The left half shows a generalized knowledge graph. The right half includes specific examples of triples from KG, EKG, CKG and demonstrates their progressive relationship.",
                "position": 150
            },
            {
                "img": "https://arxiv.org/html/2503.11227/extracted/6285883/figures/data_dis.png",
                "caption": "Figure 2:The illustration of the data distribution for all GKG sub-tasks.",
                "position": 162
            },
            {
                "img": "https://arxiv.org/html/2503.11227/extracted/6285883/figures/structure3.png",
                "caption": "Figure 3:Three-stage curriculum learning tuning framework of GKG-LLM. The upper part represents the GKG datasetğ’ŸGsubscriptğ’Ÿğº\\mathcal{D}_{G}caligraphic_D start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT, consisting of the unified datasets. The lower part shows the three stages of GKG training: theKG empowerment stageusing the KG datasets to build foundational skills, theEKG enhancement stageusing the EKG datasets to enhance specific capabilities, and theCKG generalization stageusing the CKG datasets and the counter task dataset to achieve generalization of the GKG-LLM capabilities. The thick arrows between the stages represent the delivery of model parameters from base model to each version of GKG-LLM.",
                "position": 165
            }
        ]
    },
    {
        "header": "2Methodology",
        "images": []
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.11227/extracted/6285883/figures/2.png",
                "caption": "Figure 4:Results of different fine-tuning orders. â€œK-E-Câ€ means the fine-tuning order is KG, EKG and CKG. The following sets of experiments are similar to this one.",
                "position": 751
            },
            {
                "img": "https://arxiv.org/html/2503.11227/extracted/6285883/figures/1.png",
                "caption": "Figure 5:Fine-tuning with a single type of graph and performance of different intermediate version in the GKG-LLM.",
                "position": 757
            }
        ]
    },
    {
        "header": "4Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.11227/extracted/6285883/figures/datascaling1.png",
                "caption": "Figure 6:Results of training with different proportions of complete data.",
                "position": 863
            },
            {
                "img": "https://arxiv.org/html/2503.11227/extracted/6285883/figures/OOD.png",
                "caption": "Figure 7:The average performance on OOD datasets, consisting TCR, Causal-TB and R8 datasets.",
                "position": 869
            }
        ]
    },
    {
        "header": "5Related Works",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails of Data Collection",
        "images": []
    },
    {
        "header": "Appendix BData Format",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.11227/extracted/6285883/figures/dataFormat2.jpg",
                "caption": "Figure 8:An example from the WIKEVENTS dataset. It consists of five fieldsIâ¢Dğ¼ğ·IDitalic_I italic_D, instructionsisubscriptğ‘ ğ‘–s_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, few-shotfâ¢sğ‘“ğ‘ fsitalic_f italic_s/ zero-shotzâ¢sğ‘§ğ‘ zsitalic_z italic_s, inputxisubscriptğ‘¥ğ‘–x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, and outputyisubscriptğ‘¦ğ‘–y_{i}italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT.",
                "position": 1844
            }
        ]
    },
    {
        "header": "Appendix CStage Generalization",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.11227/extracted/6285883/figures/stageGeneralization2.png",
                "caption": "Figure 9:Comparison of Results by different settings and GKG-LLM.",
                "position": 1860
            }
        ]
    },
    {
        "header": "Appendix DExploration of LoRA+ Hyperparameter Values",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.11227/extracted/6285883/figures/hyperparameters1.png",
                "caption": "Figure 10:Heatmap of Scores for DifferentÎ·Asubscriptğœ‚ğ´\\eta_{A}italic_Î· start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPTandÎ·Bsubscriptğœ‚ğµ\\eta_{B}italic_Î· start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPTValues for our training strategy.",
                "position": 1876
            }
        ]
    },
    {
        "header": "Appendix EHyper-parameters",
        "images": []
    },
    {
        "header": "Appendix FSub-tasks Introduction",
        "images": []
    }
]