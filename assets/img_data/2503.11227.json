[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.11227/extracted/6285883/figures/example.jpg",
                "caption": "Figure 1:An illustration of several triples and graphs. The left half shows a generalized knowledge graph. The right half includes specific examples of triples from KG, EKG, CKG and demonstrates their progressive relationship.",
                "position": 150
            },
            {
                "img": "https://arxiv.org/html/2503.11227/extracted/6285883/figures/data_dis.png",
                "caption": "Figure 2:The illustration of the data distribution for all GKG sub-tasks.",
                "position": 162
            },
            {
                "img": "https://arxiv.org/html/2503.11227/extracted/6285883/figures/structure3.png",
                "caption": "Figure 3:Three-stage curriculum learning tuning framework of GKG-LLM. The upper part represents the GKG dataset𝒟Gsubscript𝒟𝐺\\mathcal{D}_{G}caligraphic_D start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT, consisting of the unified datasets. The lower part shows the three stages of GKG training: theKG empowerment stageusing the KG datasets to build foundational skills, theEKG enhancement stageusing the EKG datasets to enhance specific capabilities, and theCKG generalization stageusing the CKG datasets and the counter task dataset to achieve generalization of the GKG-LLM capabilities. The thick arrows between the stages represent the delivery of model parameters from base model to each version of GKG-LLM.",
                "position": 165
            }
        ]
    },
    {
        "header": "2Methodology",
        "images": []
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.11227/extracted/6285883/figures/2.png",
                "caption": "Figure 4:Results of different fine-tuning orders. “K-E-C” means the fine-tuning order is KG, EKG and CKG. The following sets of experiments are similar to this one.",
                "position": 751
            },
            {
                "img": "https://arxiv.org/html/2503.11227/extracted/6285883/figures/1.png",
                "caption": "Figure 5:Fine-tuning with a single type of graph and performance of different intermediate version in the GKG-LLM.",
                "position": 757
            }
        ]
    },
    {
        "header": "4Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.11227/extracted/6285883/figures/datascaling1.png",
                "caption": "Figure 6:Results of training with different proportions of complete data.",
                "position": 863
            },
            {
                "img": "https://arxiv.org/html/2503.11227/extracted/6285883/figures/OOD.png",
                "caption": "Figure 7:The average performance on OOD datasets, consisting TCR, Causal-TB and R8 datasets.",
                "position": 869
            }
        ]
    },
    {
        "header": "5Related Works",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails of Data Collection",
        "images": []
    },
    {
        "header": "Appendix BData Format",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.11227/extracted/6285883/figures/dataFormat2.jpg",
                "caption": "Figure 8:An example from the WIKEVENTS dataset. It consists of five fieldsI⁢D𝐼𝐷IDitalic_I italic_D, instructionsisubscript𝑠𝑖s_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, few-shotf⁢s𝑓𝑠fsitalic_f italic_s/ zero-shotz⁢s𝑧𝑠zsitalic_z italic_s, inputxisubscript𝑥𝑖x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, and outputyisubscript𝑦𝑖y_{i}italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT.",
                "position": 1844
            }
        ]
    },
    {
        "header": "Appendix CStage Generalization",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.11227/extracted/6285883/figures/stageGeneralization2.png",
                "caption": "Figure 9:Comparison of Results by different settings and GKG-LLM.",
                "position": 1860
            }
        ]
    },
    {
        "header": "Appendix DExploration of LoRA+ Hyperparameter Values",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.11227/extracted/6285883/figures/hyperparameters1.png",
                "caption": "Figure 10:Heatmap of Scores for DifferentηAsubscript𝜂𝐴\\eta_{A}italic_η start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPTandηBsubscript𝜂𝐵\\eta_{B}italic_η start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPTValues for our training strategy.",
                "position": 1876
            }
        ]
    },
    {
        "header": "Appendix EHyper-parameters",
        "images": []
    },
    {
        "header": "Appendix FSub-tasks Introduction",
        "images": []
    }
]