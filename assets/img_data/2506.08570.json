[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.08570/x1.png",
                "caption": "Figure 1:Multi-stream delay pattern modeling. Each row represents a single codebook stream, illustrating the applied delay pattern apperent in the shifting ofCBjsubscriptCBùëó\\text{CB}_{j}CB start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPTbyj‚àí1ùëó1j-1italic_j - 1sequence steps.",
                "position": 309
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.08570/x2.png",
                "caption": "Figure 2:Temporal Conditioning Injection.œÑisubscriptùúèùëñ\\tau_{i}italic_œÑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTdenotes the temporal index in the sequence. In the auto-regressive case we apply a delayed concatenation where the conditions are stacked and concatenated over the channel axis one timestep prior to timestep they correspond to.",
                "position": 537
            }
        ]
    },
    {
        "header": "5Comparative study: AR vs FM",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.08570/x3.png",
                "caption": "Figure 3:FM performance as a function of inference steps using Euler‚Äôs method. Decreasing the number if inference steps show a steep degradation in score.",
                "position": 660
            },
            {
                "img": "https://arxiv.org/html/2506.08570/x4.png",
                "caption": "Figure 4:Inference speed versus batch size.\nLeft: throughput; right: latency.\nAR gains steadily from KV caching, whereas FM plateaus after batch size8888.\nEuler using10101010steps is the fastest configuration but has the worst FAD (4.164.164.164.16).",
                "position": 879
            },
            {
                "img": "https://arxiv.org/html/2506.08570/x5.png",
                "caption": "Figure 5:Objective scores after500500500500k updates as a function of batch size and segment duration.\nBoth paradigms improve with more tokens per update step, but AR is more sensitive to the change.",
                "position": 895
            },
            {
                "img": "https://arxiv.org/html/2506.08570/x5.png",
                "caption": "",
                "position": 898
            },
            {
                "img": "https://arxiv.org/html/2506.08570/x6.png",
                "caption": "",
                "position": 903
            }
        ]
    },
    {
        "header": "6Conclusions and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATemporal Controls Preprocessing",
        "images": []
    },
    {
        "header": "Appendix BSampling Hyperparameter Search",
        "images": []
    },
    {
        "header": "Appendix CLatent representation and transformer model specifications.",
        "images": []
    },
    {
        "header": "Appendix DConstricting Sampling Steps",
        "images": []
    },
    {
        "header": "Appendix EInpainting algorithms",
        "images": []
    }
]