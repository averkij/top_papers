[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14237/x1.png",
                "caption": "Figure 1:Flux(right) employs flexible sampling and token selection to achieve Token Optimization. Common methods(left) use rigid sampling(and use token reduction for applications directly).",
                "position": 150
            },
            {
                "img": "https://arxiv.org/html/2503.14237/x2.png",
                "caption": "Figure 2:Overview of our Flux method.The same-scaled FluxViT and InternVideo2[87]series models are both pre-trained with the InternVideo2-1b model as the teacher using the same dataset. The “FluxViT+” refers to the results using Token Optimization at test time with the same GFLOPS.",
                "position": 170
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14237/x3.png",
                "caption": "Figure 3:Our proposed Flux method with UMT framework.We show that our proposed Flux training is easy to integrate with mainstream video training frameworks.",
                "position": 199
            },
            {
                "img": "https://arxiv.org/html/2503.14237/x4.png",
                "caption": "Figure 4:Our proposed essential modules for Flux.From the model side, Flux modules include Group-dynamic token selector, dual patch norm, and Global-Local positional embedding.",
                "position": 213
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14237/x5.png",
                "caption": "Figure 5:Comparison between different training methods on K400 using a fixed number of 2048 tokens.Note the three lines and all the points share similar training and inference costs. The shaded part shows results for theAnyRes Distilled AnyRes Tunedmodel with spatial resolution in range (196, 252), while others use a fixed spatial resolution at 224.",
                "position": 705
            }
        ]
    },
    {
        "header": "5Conclusion and Future work",
        "images": []
    },
    {
        "header": "Appendix AMore Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14237/x6.png",
                "caption": "Figure 6:Overview of Flux-Multi Tuning.",
                "position": 2204
            },
            {
                "img": "https://arxiv.org/html/2503.14237/x7.png",
                "caption": "Figure 7:Gradient norms of main projector modules of Flux-Multi trained InternVideo2 on K400.We report the L2 gradient norm using bs=32.",
                "position": 2209
            },
            {
                "img": "https://arxiv.org/html/2503.14237/x8.png",
                "caption": "Figure 8:Convergence analysis of Flux-Single tuning using 3072 tokens but different frame counts directly on K400.",
                "position": 2220
            },
            {
                "img": "https://arxiv.org/html/2503.14237/x9.png",
                "caption": "Figure 9:Overall gradient norm trend during Flux-UMT per-training.We report the overall training dynamics with our ablation setting. The FluxViT modules can lower the overall norm.",
                "position": 2225
            }
        ]
    },
    {
        "header": "Appendix BMore implementation details",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]