[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1.Introduction",
        "images": []
    },
    {
        "header": "2.Related Work",
        "images": []
    },
    {
        "header": "3.Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02928/extracted/6179503/figures/pycapsule.png",
                "caption": "Figure 1.PyCapsule framework as Python code generation service of OpenSI-CoSMIC(Adnan etÂ al.,2024).\nPyCapsule consists of iterative code generation with debugging, error handling, and code execution within a Docker container.",
                "position": 193
            }
        ]
    },
    {
        "header": "4.Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02928/extracted/6179503/figures/pass_distribution.png",
                "caption": "Figure 2.Distribution of relative success ratios along the self-debugging attempts.\nThe relative success ratio refers to the number of successful test cases with given attempts over the total number of successful test cases, measured by unit %.\nResults on all the datasets consistently show a decreasing relative success ratio as the number of attempts increases.",
                "position": 506
            },
            {
                "img": "https://arxiv.org/html/2502.02928/extracted/6179503/figures/trend.png",
                "caption": "Figure 3.The left subfigure shows the normalized influence of each self-debugging attempt through independent accuracy improvements.\nThe right subfigure highlights the mean influence of each self-debugging attempt on the overall accuracy through the accumulated influence without normalization.",
                "position": 526
            }
        ]
    },
    {
        "header": "5.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AContribution of Each Debugging Attempt",
        "images": []
    },
    {
        "header": "Appendix BHumanEval Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02928/extracted/6179503/figures/he_exp_mean_std.png",
                "caption": "Figure 4.Visualisation of Debugging Attempts. The left column represents the number of debugging attempts on the HumanEval benchmark. Each point represents a problem, with failed tasks highlighted in red. The right column displays the mean number of debugging attempts and the associated sample standard deviations across three experiments for each model. The shaded areas in the right column indicate the variability in performance, illustrating the consistency and reliability of each model.",
                "position": 1317
            },
            {
                "img": "https://arxiv.org/html/2502.02928/extracted/6179503/figures/he_all_exp.png",
                "caption": "Figure 5.HumanEval experiment results using GPT-4-1106 across three repeats. Each line represents the progression of debugging attempts (up to 5) for tasks in the HumanEval dataset. Red markers denoting failed attempts. The coloured lines correspond to Experiment 1 (blue), Experiment 2 (green), and Experiment 3 (orange). The y-axis reflects the number of attempts taken to resolve each task.",
                "position": 1320
            }
        ]
    },
    {
        "header": "Appendix CModules (Detailed)",
        "images": []
    },
    {
        "header": "Appendix DPseudo-code of PyCapsule",
        "images": []
    },
    {
        "header": "Appendix ESystem Prompts",
        "images": []
    }
]