[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2How to Sample?",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.20544/x1.png",
                "caption": "Figure 1:Overview of themultilingual multi-taskexperimental scope. New methods are marked with\\texttwemojilemon.",
                "position": 217
            },
            {
                "img": "https://arxiv.org/html/2506.20544/x2.png",
                "caption": "Figure 2:Multilingual win-rates gainsvs greedy output on the dev set of m-ArenaHard as we increase the sample size from 1 to 20. Performance improvements are steepest at low sample sizes (3-5) with more modest changes beyond that for both selection methods. Results shown for Aya-8B across French, Japanese, and Russian.",
                "position": 300
            },
            {
                "img": "https://arxiv.org/html/2506.20544/x3.png",
                "caption": "Figure 3:Quality under single temperature samplingFor each temperature, we evaluate best, worst and mean quality forN=5ùëÅ5N=5italic_N = 5samples from Aya for English, Japanese, French and Russian on each of the dev sets of the tasks (rows: Arena, MGSM, WMT). While best-case scores improve over greedy outputs (œÑ=0ùúè0\\tau=0italic_œÑ = 0), the effect varies across languages and tasks, with a notable drop in worst-case quality for Japanese and Russian Arena and WMT at high temperatures.",
                "position": 336
            },
            {
                "img": "https://arxiv.org/html/2506.20544/x4.png",
                "caption": "(a)Sampling methods comparison",
                "position": 346
            },
            {
                "img": "https://arxiv.org/html/2506.20544/x4.png",
                "caption": "(a)Sampling methods comparison",
                "position": 349
            }
        ]
    },
    {
        "header": "3How to Select?",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.20544/x5.png",
                "caption": "Figure 5:Comparison ofbaselines vs RM and LLM JudgeonN=5ùëÅ5N=5italic_N = 5generations in terms of the win-rate comparing to greedy outputs on mArenaHard. Averaged across models, and non-English languages.",
                "position": 559
            },
            {
                "img": "https://arxiv.org/html/2506.20544/x6.png",
                "caption": "Figure 6:Self-improvement with parallel scaling:Command A win rates against the greedy single sample baseline for each of the selection methods. Our methods, which also use Command A for selection, outperform the RM BoN in the majority of languages.",
                "position": 806
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "6Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetailed Experimental Setup",
        "images": []
    },
    {
        "header": "Appendix BChoosing Judge and Reward Model",
        "images": []
    },
    {
        "header": "Appendix CTemperature Sensitivity",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.20544/x7.png",
                "caption": "Figure 7:Arena:Evaluation score under different temperatures withN=5ùëÅ5N=5italic_N = 5samples.",
                "position": 2254
            },
            {
                "img": "https://arxiv.org/html/2506.20544/x8.png",
                "caption": "Figure 8:MGSM:Evaluation score under different temperatures withN=5ùëÅ5N=5italic_N = 5samples.",
                "position": 2257
            },
            {
                "img": "https://arxiv.org/html/2506.20544/x9.png",
                "caption": "Figure 9:WMTEvaluation score under different temperatures withN=5ùëÅ5N=5italic_N = 5samples.",
                "position": 2260
            }
        ]
    },
    {
        "header": "Appendix DSelection Prompts",
        "images": []
    },
    {
        "header": "Appendix EAblations",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.20544/x10.png",
                "caption": "(a)Multilingual win-rates.",
                "position": 2413
            },
            {
                "img": "https://arxiv.org/html/2506.20544/x10.png",
                "caption": "(a)Multilingual win-rates.",
                "position": 2416
            },
            {
                "img": "https://arxiv.org/html/2506.20544/x11.png",
                "caption": "(b)Machine translation",
                "position": 2421
            },
            {
                "img": "https://arxiv.org/html/2506.20544/x12.png",
                "caption": "(a)Win-rate delta on the Arena open-ended benchmark",
                "position": 2435
            },
            {
                "img": "https://arxiv.org/html/2506.20544/x12.png",
                "caption": "(a)Win-rate delta on the Arena open-ended benchmark",
                "position": 2438
            },
            {
                "img": "https://arxiv.org/html/2506.20544/x13.png",
                "caption": "(b)Accuracy on the GSM8K math benchmark",
                "position": 2443
            },
            {
                "img": "https://arxiv.org/html/2506.20544/x14.png",
                "caption": "Figure 12:BoN Reward score as we increase the sample size from from 1 sample to 40 samples",
                "position": 2457
            },
            {
                "img": "https://arxiv.org/html/2506.20544/x15.png",
                "caption": "Figure 13:Scaling pool sample sizeusingœÑ=0.7ùúè0.7\\tau=0.7italic_œÑ = 0.7hedged sampling for selected languages with different selection methods on mArenaHard dev set.",
                "position": 2460
            }
        ]
    },
    {
        "header": "Appendix FEvaluation Results",
        "images": []
    }
]