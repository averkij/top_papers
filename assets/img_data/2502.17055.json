[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17055/x1.png",
                "caption": "Figure 1:Performance of 4-bit LLM training.Experiments are conducted with LLaMA-130M/350M/1B models on C4 Dataset.Adam-BF16denotes that the model is trained with BF16 by Adam. Perplexity on validation set is reported.",
                "position": 102
            },
            {
                "img": "https://arxiv.org/html/2502.17055/x2.png",
                "caption": "Figure 2:Final validation loss when training LLaMA-130M on C4, sweeping across learning rates (LR).The vertical dotted line indicates that the model cannot be trained further as increasing the learning rate, i.e. Training loss becomes NaN. Red dashed horizontal lines indicate the best performance achieved.",
                "position": 137
            },
            {
                "img": "https://arxiv.org/html/2502.17055/x3.png",
                "caption": "Figure 3:Effect of SpikeClip(Huang et¬†al.,2025)on stabilizing training.Left: gradient norms before and after performing gradient spike clip. Right: training loss with and without gradient spike clip. Models are trained by Adam optimizer based on LLaMA-130M and C4.",
                "position": 146
            }
        ]
    },
    {
        "header": "24-bit Training Stability Investigation",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17055/x4.png",
                "caption": "Figure 4:Training loss and gradient norm of Adam using various learning rates with BF16 and FP4 precision.Experiments are conducted under the same training configuration with LLaMA-130M/350M.",
                "position": 167
            }
        ]
    },
    {
        "header": "3Stable-SPAM",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17055/x5.png",
                "caption": "Table 1:Comparison of various optimizers of INT4 and FP4 training of LLaMA models on C4444.Perplexity is reported.",
                "position": 260
            },
            {
                "img": "https://arxiv.org/html/2502.17055/x5.png",
                "caption": "Figure 5:StableSPAM under Extremely Low-Precision Training.Experiments are conducted with 350M models on C4 Dataset.BF16-Adamdenotes that the model is trained with BF16 by Adam. The final loss on validation set is reported.",
                "position": 356
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17055/x6.png",
                "caption": "Figure 6:Performance of BF16 training with various model sizes.Experiments are based on LLaMA models trained on C4 Dataset.",
                "position": 389
            },
            {
                "img": "https://arxiv.org/html/2502.17055/x7.png",
                "caption": "Figure 7:Effect ofAdaGNandAdaClipon stabilizing FP4 LLM training.The left two figures use LLaMA-130M (LR = 3e-3), and the right two figures use LLaMA-60M.",
                "position": 473
            },
            {
                "img": "https://arxiv.org/html/2502.17055/x8.png",
                "caption": "Figure 8:Hyper-parameter Analysis.Experiments are conducted with FP4 training on LLaMA-60M and C4 with 1.1B tokens.",
                "position": 617
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AArchitecture and Hyperparameters",
        "images": []
    },
    {
        "header": "Appendix BTime Series Forescasting Task",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17055/x9.png",
                "caption": "Figure 9:Test Loss during Training Process on Weather Time-series Data.Anomalous data is generated by adding Gaussian noise to 10% of randomly selected input values. Specifically, the anomalies data are conducted withX=X+Gaussin‚Å¢(0,Severity‚àóMax‚Å¢(X))ùëãùëãGaussin0SeverityMaxùëãX=X+\\texttt{Gaussin}(0,\\texttt{Severity}*\\texttt{Max}(X))italic_X = italic_X + Gaussin ( 0 , Severity ‚àó Max ( italic_X ) )whereXùëãXitalic_Xis the inputs.",
                "position": 1442
            }
        ]
    },
    {
        "header": "Appendix CPseudocode",
        "images": []
    }
]