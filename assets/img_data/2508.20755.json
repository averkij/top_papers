[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20755/x1.png",
                "caption": "Figure 1:Benefits of in-tool learning.Illustration of factual recall viain-weight learning(memorization) versusin-tool learning(external retrieval).\nWhile tool use may incur higher latency, it offloads storage, enabling scalable recall without increasing model size.\nIt also better preserves the model’s original capabilities during finetuning.",
                "position": 167
            }
        ]
    },
    {
        "header": "2Setting",
        "images": []
    },
    {
        "header": "3In-Weight Lower Bound",
        "images": []
    },
    {
        "header": "4In-Tool Upper Bound",
        "images": []
    },
    {
        "header": "5Controlled Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20755/x2.png",
                "caption": "Figure 2:Scaling of parameter requirements with the number of facts.Minimum number of parameters required to achieve at least a 95% recall as a function of the dataset size. Results are averaged across 10 runs with standard deviation.\nIn-weight models require increasingly more parameters as the number of facts grows, approximately following the linear trendy=α​x+βy=\\alpha x+\\betawith(α,β)=(8.14,5171)(\\alpha,\\beta)=(8.14,5171), consistent with Theorem3.2.\nIn contrast, in-tool models exhibit a sharp saturation: beyond a critical point (dashed vertical line around 1K facts), the parameter requirement flattens, indicating a transition to tool-based retrieval.",
                "position": 475
            },
            {
                "img": "https://arxiv.org/html/2508.20755/x3.png",
                "caption": "Figure 3:Transition from memorization to retrieval in in-tool models.Accuracy on out-of-distribution databases (i.e., databases of facts unseen during training) as a function of the dataset size.\nBelow the transition point (the same dashed vertical line as in Figure2), in-tool models memorize specific databases and generalize poorly, worse than the best random model that outputs the same value for all queries (red dashed line). Above this threshold, they learn to construct tool queries that generalize across datasets, resulting in stable accuracy even on new data.",
                "position": 481
            },
            {
                "img": "https://arxiv.org/html/2508.20755/x4.png",
                "caption": "Figure 4:Impact of the dependency between facts on in-weight memorization.Minimum parameter requirement to achieve at least a 95% recall on datasets generated with a correlation between facts ofα\\alpha.\nThe introduction of such a correlation breaks the independence between triplets, making the dataset more compressible thanks to common patterns between facts. Thus, as correlation increases, the number of “effective” units of facts decreases, requiring fewer parameters to store them.",
                "position": 535
            }
        ]
    },
    {
        "header": "6Large-Scale Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20755/x5.png",
                "caption": "Figure 5:Impact of factual memorization on general language capabilities (HellaSwag). Hellaswag accuracy as a function of the dataset size for pretrained models finetuned until reaching at least a 95% recall on the database. Tool-learning (dashed) preserves general capabilities almost perfectly across models and dataset sizes. In contrast, in-weight learning (solid) leads to noticeable degradation, especially for smaller models and larger factual loads. This supports our theoretical prediction that parameter-based memorization causes interference and loss of prior capabilities, due to finite capacity and update interference. Larger models are more robust to such forgetting, but still exhibit performance decay beyond  10k facts.",
                "position": 583
            },
            {
                "img": "https://arxiv.org/html/2508.20755/x6.png",
                "caption": "Figure 6:TV distance for different memorization loads. Similar setup to Figure5, where we display the Total Variation (TV) distance.\nIn-weight finetuned models deviate more from their base models when memorizing bigger datasets.",
                "position": 599
            },
            {
                "img": "https://arxiv.org/html/2508.20755/x7.png",
                "caption": "Figure 7:Training steps required for memorization. Number of training steps as a function of the dataset size for pretrained models finetuned until perfect recall. For the same optimization parameters, in-weight compute requirements scale with the memorization load. On the other hand, learning tool-use is faster and independent of dataset size in our setup. Llama models are better predisposed to learn tool-use from their pretraining compared to SmolLM models.",
                "position": 612
            }
        ]
    },
    {
        "header": "7Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExtended Related Work",
        "images": []
    },
    {
        "header": "Appendix BTheoretical Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20755/figures/transformer.png",
                "caption": "",
                "position": 1837
            },
            {
                "img": "https://arxiv.org/html/2508.20755/x8.png",
                "caption": "Figure 10:Scaling of parameter requirements with the level of factual recall.Minimum number of parameters required as a function of the factual recall. The higher the number of facts retrieved, the bigger the models. When the total number of facts to retrieve increases, we observe that the parameter requirement increases too. This shows that in-tool learning is all the more beneficial when very effective models are needed on large databases.",
                "position": 2206
            },
            {
                "img": "https://arxiv.org/html/2508.20755/x9.png",
                "caption": "Figure 11:HellaSwag performance (relative to base model) versus memorization load.Same setup as Figure5; the dashed line represents the worst (lowest) performance among tool-models (Smol-135M).",
                "position": 2219
            },
            {
                "img": "https://arxiv.org/html/2508.20755/x9.png",
                "caption": "Figure 11:HellaSwag performance (relative to base model) versus memorization load.Same setup as Figure5; the dashed line represents the worst (lowest) performance among tool-models (Smol-135M).",
                "position": 2222
            },
            {
                "img": "https://arxiv.org/html/2508.20755/x10.png",
                "caption": "Figure 12:Total Variation versus memorization load.Same setup as Figure6; the dashed line represents the worst (highest) TV distance attained among tool-models (Smol-135M).",
                "position": 2228
            },
            {
                "img": "https://arxiv.org/html/2508.20755/x11.png",
                "caption": "Figure 13:Metrics throughout training on a database of 500 facts. The dashed line represents the worst in-tool learning baseline, highlighting that tool use attains full recall while maintaining a very high(≥98%)(\\geq 98\\%)level of prior capabilities on HellaSwag, and by deviating less than0.040.04in TV from its base model.",
                "position": 2257
            },
            {
                "img": "https://arxiv.org/html/2508.20755/x12.png",
                "caption": "Figure 14:Metrics throughout training on a dataset of 50K facts. The worst in-tool model (dashed) mastered the tool in very few training steps, conserving higher HellaSwag capabilities and deviating less from its base model.",
                "position": 2260
            }
        ]
    },
    {
        "header": "Appendix CExperimental Details",
        "images": []
    }
]