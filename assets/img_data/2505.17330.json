[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17330/extracted/6469038/fs_dag_v2.jpg",
                "caption": "Figure 1:An illustration of the model architecture for FS-DAG. Given a document image (I); its text regions{ri}subscriptğ‘Ÿğ‘–\\{r_{i}\\}{ italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }are extracted using an OCR engine. We cluster and sort the{ri}subscriptğ‘Ÿğ‘–\\{r_{i}\\}{ italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }to create a reading sequence{s}ğ‘ \\{s\\}{ italic_s }; textual features{ti}subscriptğ‘¡ğ‘–\\{t_{i}\\}{ italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }are extracted using a linear projection layer on top of a pre-trained language model processing{s}ğ‘ \\{s\\}{ italic_s }. In contrast, visual features{vi}subscriptğ‘£ğ‘–\\{v_{i}\\}{ italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }are extracted using ROI-Align on top of the feature map from the Visual Model and{ri}subscriptğ‘Ÿğ‘–\\{r_{i}\\}{ italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }. The deep fusion module uses Kronecker product to fuse{ti}subscriptğ‘¡ğ‘–\\{t_{i}\\}{ italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }and{vi}subscriptğ‘£ğ‘–\\{v_{i}\\}{ italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }to initialize the node features{ni}subscriptğ‘›ğ‘–\\{n_{i}\\}{ italic_n start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }. The node features are propagated and aggregated in the GNN during the message passing, which uses positional embedding{pi}subscriptğ‘ğ‘–\\{p_{i}\\}{ italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }and multi-head attention to learn the edge features dynamically. The classification head will finally classify the node features into one of the key-value classes.",
                "position": 152
            }
        ]
    },
    {
        "header": "3Our Approach",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17330/extracted/6469038/category_1_small_horizontal.jpg",
                "caption": "Figure 2:Sample images from each of the five document types released as part of the Category 1 dataset.",
                "position": 2083
            },
            {
                "img": "https://arxiv.org/html/2505.17330/extracted/6469038/category_2_small_horizontal.jpg",
                "caption": "Figure 3:Sample images from each of the seven document types released as part of the Category 2 dataset.",
                "position": 2088
            }
        ]
    },
    {
        "header": "Appendix BExperiments, Extended",
        "images": []
    }
]