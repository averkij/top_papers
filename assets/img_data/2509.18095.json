[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.18095/x1.png",
                "caption": "Figure 1:Different multimodal retrieval frameworks at training-time.Upper Left:Single vector retrieval method computes a score for each pair of query and candidate and uses contrastive objective to maximize the score for corresponding pairs.Upper Right:In multi-vector retrieval, scores are computed via maximum similarity across vector pairs and aggregated before applying the contrastive objective.Lower:MetaEmbedstructures query and candidate vectors into hierarchical nested groups and trains coarse-to-fine multi-vector embeddings that enable scalable retrieval with flexible test-time control over embedding granularity.",
                "position": 156
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.18095/x2.png",
                "caption": "Figure 2:Illustration of test-time scaling with varying retrieval budgets.Left:MetaEmbedconstructs a nested multi-vector index that can be retrieved flexibly given different budgets.Middle:How the scoring latency grows with respect to the index size. Scoring latency is reported with 100,000 candidates per query on an A100 GPU.\nSee §5for full efficiency analysis.Right:MetaEmbed-7B performance curve with different retrieval budgets.\nSee Figure3(b) for full metrics.",
                "position": 329
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.18095/x3.png",
                "caption": "Table 1:Precision@1 (%) results on MMEB, which includes 36 tasks across four categories: Classification, Visual Question Answering (VQA), Retrieval, and Visual Grounding. IND and OOD represent the in-domain average and out-of-domain average metrics, respectively.Bolddenotes the best scores in the subset and the second-best scores are highlighted withunderline.",
                "position": 379
            },
            {
                "img": "https://arxiv.org/html/2509.18095/x3.png",
                "caption": "Table 2:NDCG@5 (%) results on the ViDoRe v2 benchmark, which covers 7 tasks on visual document retrieval.\n“Syn” denotes synthetic data, “Mul” indicates multilingual tasks, and “Bio” refers to biomedical domains.Bolddenotes the best scores in the subset and the second-best scores are highlighted withunderline.",
                "position": 636
            },
            {
                "img": "https://arxiv.org/html/2509.18095/x3.png",
                "caption": "Figure 3:Impact of retrieval budget on MMEB acrossMetaEmbedof varying model sizes. Retrieval budget is denoted as(rq,rc)(r_{q},r_{c}),i.e. a tuple of the number ofMeta Embeddingsused on query and candidate side. Increasing the retrieval budget from (1,1) to (16,64) consistently improves performance for all model sizes, with larger gains observed in higher-capacity models. The dashed green lines indicate the best single-vector retrieval performance and red arrows indicate the absolute gain (in percentage points) betweenMetaEmbedand single-vector retrieval.",
                "position": 824
            },
            {
                "img": "https://arxiv.org/html/2509.18095/x4.png",
                "caption": "(a)MetaEmbedwith(16,64)(16,64)retrieval budget shows less diminishing returns as model size scales. Green numbers denote the gain compared to the preceding model size.",
                "position": 837
            },
            {
                "img": "https://arxiv.org/html/2509.18095/x4.png",
                "caption": "(a)MetaEmbedwith(16,64)(16,64)retrieval budget shows less diminishing returns as model size scales. Green numbers denote the gain compared to the preceding model size.",
                "position": 840
            },
            {
                "img": "https://arxiv.org/html/2509.18095/x5.png",
                "caption": "(b)Average NDCG@5 (%) on ViDoRe v1 benchmark with varying retrieval budgets onMetaEmbed-3B with and without MMR design.",
                "position": 846
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Implementation Details",
        "images": []
    },
    {
        "header": "8Detailed MMEB Ablation Results",
        "images": []
    },
    {
        "header": "9Baseline Method Introduction",
        "images": []
    }
]