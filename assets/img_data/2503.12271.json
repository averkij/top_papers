[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12271/x1.png",
                "caption": "Figure 1:\\oursiteratively refines image generation by using a vision-language model (VLM) to critique generations and a Diffusion Transformer (DiT) to self-improve using past generations and feedback.Specifically, at each generation step N, feedback from previous iterations (N-3, N-2, N-1, ‚Ä¶) are incorporated to progressively improve future generations. Unlike traditional best-of-N sampling,\\oursactively corrects errors in object count, position, and attributes, enabling more precise generations with fewer samples.",
                "position": 127
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12271/x2.png",
                "caption": "Figure 2:Architecture of\\ours. Given a prompt, past images and feedback, we first encode the images into a set of vision embeddings[V1,V2,‚Ä¶]subscriptùëâ1subscriptùëâ2‚Ä¶[V_{1},V_{2},\\dots][ italic_V start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_V start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚Ä¶ ]using a vision encoder, and encode text feedback to a set of text embeddings[E1,E2‚Å¢‚Ä¶]subscriptùê∏1subscriptùê∏2‚Ä¶[E_{1},E_{2}...][ italic_E start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_E start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ‚Ä¶ ]. We then concatenate these embeddings into a single sequenceMùëÄMitalic_M, and pass it through the Context Transformer to obtainM‚Ä≤superscriptùëÄ‚Ä≤M^{\\prime}italic_M start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT. The extra contextM‚Ä≤superscriptùëÄ‚Ä≤M^{\\prime}italic_M start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPTis concatenated directly after the standard prompt embeddings and passed into the cross-attention layers of the Diffusion Transformer (DiT).",
                "position": 148
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12271/x3.png",
                "caption": "Figure 3:Side-by-side qualitative comparison of\\oursand best-of-N sampling.\\oursleverages feedback to iteratively refine image generations, resulting in more accurate and visually coherent outputs. In the first example,\\oursprogressively adjusts object positions to better satisfy the prompt ‚Äúa cupleft ofan umbrella,‚Äù achieving significantly better image-text alignment than best-of-N sampling. The second example demonstrates how\\ourscorrects multiple counting constraints (‚Äúfivemonarch butterflies‚Äù and ‚Äúa singledandelion‚Äù) over successive iterations, gradually converging to the correct solution. Lastly, in the rightmost example,\\oursuses in-context feedback to refine object shapes, producing a more precise and intentional design compared to best-of-N.",
                "position": 452
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12271/x4.png",
                "caption": "Figure 4:Comparison of\\ourswith other finetuning methods. We find that\\oursis able to consistently outperform finetuning methods, like supervised finetuning (SFT) and Diffusion-DPO (DPO). Using only 4 samples,\\ourscan outperform related finetuning baselines using best-of-20 sampling.",
                "position": 498
            },
            {
                "img": "https://arxiv.org/html/2503.12271/x5.png",
                "caption": "Figure 5:Human evaluation win-rate (%) on PartiPrompts dataset. We perform a user study to evaluate the effectiveness of\\oursin broadly improving text-to-image generation. Results show that human evaluators consistently prefer generations from\\oursover best-of-N sampling.",
                "position": 575
            },
            {
                "img": "https://arxiv.org/html/2503.12271/x6.png",
                "caption": "Figure 6:Illustration of the iterative refinement process in\\ours.\\oursstarts with an initial image generated from the prompt and progressively refines it based on textual feedback until the final output meets the desired criteria, demonstrating the effectiveness of our reflection-based approach. In the first sequence,\\ourshandles a complex scene by gradually repositioning multiple objects‚Äî‚Äúwoman,‚Äù ‚Äútree,‚Äù ‚Äúcat,‚Äù and ‚Äúdog‚Äù‚Äîto achieve correct spatial alignment. Additionally, it recognizes subtle object misclassifications, such as changing the second ‚Äúdog‚Äù to a ‚Äúcat‚Äù based on feedback. The second example demonstrates a counting problem, where the model iteratively adjusts the number of detached seeds until it converges to the correct count. The final example presents a particularly challenging scenario: the prompt requires the ‚Äúdog‚Äù to be positioned to the ‚Äúright ofa tie‚Äù, an unusual object to appear independently. Initially, the model misinterprets the instruction, generating a dog wearing a tie. However, through multiple refinement steps,\\ourslearns to separate the objects and ultimately produces the correct spatial arrangement.",
                "position": 581
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Results and Discussions",
        "images": []
    },
    {
        "header": "Appendix BLimitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12271/x7.png",
                "caption": "Figure 7:Failure cases of\\ours. Failure cases of\\ours. While\\oursdemonstrates strong refinement capabilities, the generated feedback can occasionally introduce errors between iterations. In the first example, the model fails to recognize that the specific lighting conditions signify a ‚Äúsunset‚Äù, leading to an incorrect adjustment. Similarly, in the second example, the model struggles to distinguish the color of the ‚Äúdining table‚Äù because the purple hue from the ‚Äúdog‚Äù reflects off the table, creating ambiguity. These cases highlight subjectivity in the VLM evaluation, where the model‚Äôs interpretation may still be reasonable. However, the final two examples illustrate more typical failure cases. In both images, objects (‚Äúboat‚Äù and ‚Äúbutterfly‚Äù) are completely overlooked by the feedback model. This issue likely arises because the objects are too small or unusually shaped, which makes them difficult to detect, resulting in incorrect evaluations.",
                "position": 1764
            }
        ]
    },
    {
        "header": "Appendix CTechnical Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12271/x8.png",
                "caption": "Figure 8:User interface for human annotators.",
                "position": 1904
            }
        ]
    },
    {
        "header": "Appendix DAdditional Qualitative Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12271/x9.png",
                "caption": "Figure 9:Additional qualitative examples from\\ours.",
                "position": 1916
            }
        ]
    },
    {
        "header": "Appendix EReproducibility Statement",
        "images": []
    }
]