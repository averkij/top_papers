[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01149/x1.png",
                "caption": "Figure 1:Pareto efficiency.ColModernVBERToutperforms models in its category on ViDoRe, achieving a leading performance-size tradeoff.",
                "position": 112
            }
        ]
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01149/figures/vbert_architecture.png",
                "caption": "Figure 2:MLM-based early fusion architecture.The visual encoder produces patch representations, which are passed to a language model.\nOur end-to-end bidirectional attention fused architecture is trained with Masked Language Modeling objectives and is perfectly suited for sequence and token-level representation tasks.",
                "position": 153
            }
        ]
    },
    {
        "header": "3What Makes a Great Visual Retriever?",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01149/x2.png",
                "caption": "Figure 3:Impact of Modality Alignment objective on downstream tasks.Early Fusion of vision and text models boosts document retrieval tasks regardless of the LM objective, but degrades natural image and classification tasks w.r.t. the standalone off-the-shelf vision model SigLIP. Reported scores are aggregated MIEB scores (nDCG, Accuracy.)",
                "position": 244
            },
            {
                "img": "https://arxiv.org/html/2510.01149/x3.png",
                "caption": "Figure 4:Modality alignment scaling of early fusion encoders for up to 1 epoch (3.5B tokens) of data.The dashed line indicates the vision encoder evaluated standalone without further training. Our findings show that retrieval tasks benefits from extended modality alignment phase, particularly in document retrieval, where performance quickly surpasses that of the standalone vision encoder.",
                "position": 265
            },
            {
                "img": "https://arxiv.org/html/2510.01149/x4.png",
                "caption": "Figure 5:Impact of attention masks and training objectives on document retrieval performances.We report the average nDCG@5 on English splits of ViDoRe benchmarks for models post-trained on ColPali.",
                "position": 283
            },
            {
                "img": "https://arxiv.org/html/2510.01149/x4.png",
                "caption": "Figure 5:Impact of attention masks and training objectives on document retrieval performances.We report the average nDCG@5 on English splits of ViDoRe benchmarks for models post-trained on ColPali.",
                "position": 286
            }
        ]
    },
    {
        "header": "4Building a Small yet Mighty Visual Retriever.",
        "images": []
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "Detailed Contributions",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01149/x5.png",
                "caption": "Figure 6:Example from the NatCap dataset",
                "position": 2276
            }
        ]
    },
    {
        "header": "Appendix BBaselines Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01149/x6.png",
                "caption": "Figure 7:Attention masks impact on modality alignment phase scaling.The dashed line marks the vision tower baseline. The orange curve shows the model initialized from a decoder LM with aCLMobjective, and the blue curve shows the model trained with anMLMobjective from an encoder LM. CLM performs better in low-data regimes, but MLM scales more effectively, surpassing CLM in document retrieval, while captioning and classification remain below the CLIP baseline.",
                "position": 2453
            },
            {
                "img": "https://arxiv.org/html/2510.01149/x7.png",
                "caption": "Figure 8:Contrastive training scaling. Each dot on the blue curve represents one fraction of the baseline contrastive training mix (ColPali + MSCOCO). Performance improves with more in-distribution data, surpassing the baseline on document benchmarks and narrowing the gap on image captioning. There is no clear improvement in image classification, highlighting the need for more diverse pairs.",
                "position": 2464
            },
            {
                "img": "https://arxiv.org/html/2510.01149/x8.png",
                "caption": "Figure 9:Optimal text-to-image ratio in contrastive training mix.Increasing the ratio in retrieval tasks consistently improves the performances.",
                "position": 2473
            },
            {
                "img": "https://arxiv.org/html/2510.01149/x9.png",
                "caption": "Figure 10:Merging model results across tasks. Benefits are task-dependent, with performance degradation w.r.t. both original models in Document Retrieval.",
                "position": 2641
            }
        ]
    },
    {
        "header": "Appendix CAdditional Ablations",
        "images": []
    }
]