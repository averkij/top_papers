[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01623/figs/first_radar2.png",
                "caption": "Figure 1:VLA-R1:pipeline from instruction to execution, with benchmark comparisons against baselines.",
                "position": 81
            }
        ]
    },
    {
        "header": "IIRelated Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01623/x1.png",
                "caption": "Figure 2:CoT Data Engine.After ingesting multimodal data, the system parses tasks based on type (e.g., affordance or trajectory), performs scene understanding and localization, validates feasibility, and generates structured CoT traces for training.",
                "position": 207
            }
        ]
    },
    {
        "header": "IIIMethod",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01623/figs/arch5.png",
                "caption": "Figure 3:Overall architecture of VLA-R1.Training has two stages:Stage 1uses SFT with CoT supervision to learn reasoning over images and instructions;Stage 2refines reasoning and actions via RL with verifiable rewards (GRPO).During inference, a control stack converts outputs into joint-level robot commands.",
                "position": 223
            },
            {
                "img": "https://arxiv.org/html/2510.01623/figs/cot_test.png",
                "caption": "Figure 4:Case Analysis:The figure illustrates VLA-R1’s reasoning process and outcomes for both affordance and trajectory tasks. VLA-R1 parses the action requirements, infers relevant objects and spatial relations, and outputs the corresponding bounding boxes or waypoint sequences. The affordance form and trajectory form are fixed prompt templates that instruct the model to produce outputs in a specified format.",
                "position": 247
            },
            {
                "img": "https://arxiv.org/html/2510.01623/figs/scene_test.png",
                "caption": "Figure 5:Visualization of evaluation in real-world scenarios.",
                "position": 329
            }
        ]
    },
    {
        "header": "IVExperiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01623/figs/sim.png",
                "caption": "Figure 6:Visualization of simulation.",
                "position": 395
            }
        ]
    },
    {
        "header": "VLimitation and Future Work",
        "images": []
    },
    {
        "header": "VIConclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01623/x2.png",
                "caption": "",
                "position": 1076
            },
            {
                "img": "https://arxiv.org/html/2510.01623/x3.png",
                "caption": "",
                "position": 1078
            },
            {
                "img": "https://arxiv.org/html/2510.01623/x4.png",
                "caption": "",
                "position": 1080
            },
            {
                "img": "https://arxiv.org/html/2510.01623/x5.png",
                "caption": "",
                "position": 1082
            },
            {
                "img": "https://arxiv.org/html/2510.01623/x6.png",
                "caption": "",
                "position": 1084
            },
            {
                "img": "https://arxiv.org/html/2510.01623/x7.png",
                "caption": "",
                "position": 1086
            },
            {
                "img": "https://arxiv.org/html/2510.01623/x8.png",
                "caption": "",
                "position": 1088
            },
            {
                "img": "https://arxiv.org/html/2510.01623/x9.png",
                "caption": "",
                "position": 1090
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]