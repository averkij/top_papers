[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13389/extracted/6452617/media/f1-method.png",
                "caption": "Figure 1:Overview ofVSA. (a)VSAintroduce a hierarchical attention with sparsity and different granularity (coarse & fine). (b) Larger tile sizes (left) blur attention pattern while small tiles let the coarse stage localize critical tokens close to token resolution. The red dots indicate critical tokens. (c) An illustration of(2,2,2)222(2,2,2)( 2 , 2 , 2 )cube partition (in practice we use(4,4,4)444(4,4,4)( 4 , 4 , 4 )inVSA).",
                "position": 130
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13389/extracted/6452617/media/unified.png",
                "caption": "Figure 2:VSAscaling experiments. (a): Video DiT trained withVSAachieves similar loss curve compared to one trained with full attention. (b):VSAconsistently produces a better Pareto frontier when scaling model size up to 1.4B. (c) & (d): The optimal Top-ùí¶ùí¶\\mathcal{K}caligraphic_Kvalue (dictating sparsity) depends on both sequence length and training compute. A largerùí¶ùí¶\\mathcal{K}caligraphic_Kis needed for a larger training budget.",
                "position": 511
            },
            {
                "img": "https://arxiv.org/html/2505.13389/extracted/6452617/media/human.jpg",
                "caption": "Table 2:Wan-1.3B finetuning results on VBench.",
                "position": 522
            },
            {
                "img": "https://arxiv.org/html/2505.13389/extracted/6452617/media/human.jpg",
                "caption": "Figure 3:VSAvs. SVG human evaluation on 200 randomly sampled prompts from MovieGen-Benchpolyak2025moviegencastmedia. SVG has 82.5% sparsity with (fp0.03, fl0.025, s0.1).",
                "position": 563
            },
            {
                "img": "https://arxiv.org/html/2505.13389/x1.png",
                "caption": "(a)",
                "position": 577
            },
            {
                "img": "https://arxiv.org/html/2505.13389/x1.png",
                "caption": "(a)",
                "position": 580
            },
            {
                "img": "https://arxiv.org/html/2505.13389/x2.png",
                "caption": "(b)",
                "position": 585
            },
            {
                "img": "https://arxiv.org/html/2505.13389/extracted/6452617/media/attn_probing.png",
                "caption": "Figure 5:Visualization of the attention pattern ofVSA. (a)-(f):VSAdynamically select different cubes to attend, where the blue cube indicates query and red cubes indicated selected key and values.(e):VSAcritical-token prediction accuracy.",
                "position": 597
            }
        ]
    },
    {
        "header": "4Qualitative Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13389/extracted/6452617/media/demo.png",
                "caption": "Figure 6:Qualitative examples. In (a), we sample the same middle frame at each step of the video. In (b), we uniformly sample four frames across the video.",
                "position": 611
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Limitation and Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APseudocode ofVSA",
        "images": []
    },
    {
        "header": "Appendix BExperimental Details",
        "images": []
    },
    {
        "header": "Appendix CCoarse Stage Runtime",
        "images": []
    },
    {
        "header": "Appendix DDiscussion and Extended Related Work",
        "images": []
    },
    {
        "header": "Appendix EBroader Impact",
        "images": []
    }
]