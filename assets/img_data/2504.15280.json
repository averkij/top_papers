[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15280/x1.png",
                "caption": "",
                "position": 85
            },
            {
                "img": "https://arxiv.org/html/2504.15280/x2.png",
                "caption": "",
                "position": 102
            },
            {
                "img": "https://arxiv.org/html/2504.15280/x3.png",
                "caption": "Figure 2:Overview ofAll-Angles Bench.Our benchmark targets a comprehensive view of multi-view understanding, spanning six primary question types. These question types are designed to investigate several major aspects of 3D scene understanding, from creating correspondence between objects to associating relative object and camera poses.",
                "position": 111
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15280/x4.png",
                "caption": "Figure 3:All-Angles Benchconstruction pipeline.(1) We collect and curate 90 diverse multi-view scenes and design six tasks that emphasize multi-view reasoning. (2) We generate initial questions via an MLLM, then refine and validate them throughhuman annotationto ensure correctness, clarity, and domain relevance. (3) We create paired questions by systematically rephrasing or altering each view perspective while preserving their underlying visual correspondences to evaluate model’s cross-view consistency. A final quality-control step removes inconsistent or ambiguous pairs. Note thatcountingandcamera pose estimationtasks utilize all available views per query, whereas other tasks employ two randomly selected viewpoints.",
                "position": 124
            }
        ]
    },
    {
        "header": "2All-Angles Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15280/x5.png",
                "caption": "Figure 4:Statistical overview ofAll-Angles Bench.The pie chart shows the distribution of 6 sub-tasks of multi-view understanding. The bar plot illustrates the percentage breakdown by primary and paired question-answers of each sub-task.",
                "position": 171
            }
        ]
    },
    {
        "header": "3MLLMs Have Multi-View Understanding?",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15280/x6.png",
                "caption": "Figure 6:Paired question-answers inconsistency across 6 MLLMs.We report the proportions ofICandCC+WW. Notably, GPT-4o struggles with relative distance (around 70% inconsistency). Gemini-2.0-Flash and Claude-3.7-Sonnet exhibit more balanced performance, whereas Ovis2-34B and GPT-4o vary considerably across tasks.",
                "position": 626
            },
            {
                "img": "https://arxiv.org/html/2504.15280/x7.png",
                "caption": "Figure 7:Complete- and Partial-visibility counting.While MLLMs often succeed when everyone is visible in one viewpoint, they sometimes fail to reconcile fragmented information across views, as shown by GPT‐4o occasionally picks the largest per‐view count rather than reconciling individuals across views.",
                "position": 629
            }
        ]
    },
    {
        "header": "4Why Do MLLMs Struggle with Multi-View Understanding?",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15280/x8.png",
                "caption": "Figure 8:Analysis of reasoning prompt strategies.We report the effectiveness ofZero-Shot CoT,Self-Consistency, andIdentification CoT— across GPT-4o, Ovis2-34B, and InternVL2.5-38B under complete-view and partial-view settings. While CoT variations delivers notable gains in partial-visibility scenarios in GPT-4o, its impact diminishes for models already be robust at multi-view counting (e.g., InternVL2.5-38B). These results indicate that refining reasoning prompt alone is insufficient; specialized multi-view training may be necessary to excel onAll-Angles Bench.",
                "position": 670
            },
            {
                "img": "https://arxiv.org/html/2504.15280/x9.png",
                "caption": "Figure 9:Visualization of multi-view scene reconstruction and camera pose alignment.Although GPT-4o and Gemini-2.0-Flash both display moderate proficiency in single-view scene reconstruction, they struggle when aligning two different camera perspectives. Misidentifying camera poses leads to incorrect directional reasoning — such as tracking a person’s trajectory from View 1 to View 2 which needs multi-view consistency in current MLLMs.",
                "position": 673
            },
            {
                "img": "https://arxiv.org/html/2504.15280/x10.png",
                "caption": "Figure 10:Visualization of camera pose estimation.When asked to order the camera poses in clockwise order, MLLMs fail completely despite providing detailed reasonings.",
                "position": 677
            }
        ]
    },
    {
        "header": "5Related Works",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Construction and Annotation Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15280/x11.png",
                "caption": "Figure 12:Left:A structured JSON representation of a question-answer pair.Right:A snapshot of the GUI-based Annotation Platform used for reviewing and refining annotations. Best viewed zoomed in for details.",
                "position": 1676
            },
            {
                "img": "https://arxiv.org/html/2504.15280/x12.png",
                "caption": "Figure 13:Comparison of a flagged ambiguous case before modification and its finalized version after cross-checking.The initial annotation was reviewed by multiple annotators, with ambiguities resolved through discussions to ensure clarity and consistency.",
                "position": 1679
            }
        ]
    },
    {
        "header": "8Experiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15280/x13.png",
                "caption": "Figure 15:Comparison of model outputs on the same questions under different prompting methods. (Case 1)",
                "position": 1825
            },
            {
                "img": "https://arxiv.org/html/2504.15280/x14.png",
                "caption": "Figure 16:Comparison of model outputs on the same questions under different prompting methods. (Case 2)",
                "position": 1828
            },
            {
                "img": "https://arxiv.org/html/2504.15280/x15.png",
                "caption": "Figure 17:All-Angles BenchSamples (Part I)",
                "position": 1831
            },
            {
                "img": "https://arxiv.org/html/2504.15280/x16.png",
                "caption": "Figure 18:All-Angles BenchSamples (Part II)",
                "position": 1834
            },
            {
                "img": "https://arxiv.org/html/2504.15280/x17.png",
                "caption": "Figure 19:Paired Data Samples (Part I)",
                "position": 1837
            },
            {
                "img": "https://arxiv.org/html/2504.15280/x18.png",
                "caption": "Figure 20:Paired Data Samples (Part II)",
                "position": 1840
            },
            {
                "img": "https://arxiv.org/html/2504.15280/x19.png",
                "caption": "Figure 21:Visualization of the reasoning process for two MLLMs (Part I).In this case, both MLLMs choose incorrect options due to errors in their reasoning process.",
                "position": 1843
            },
            {
                "img": "https://arxiv.org/html/2504.15280/x20.png",
                "caption": "Figure 22:Visualization of the reasoning process for two MLLMs (Part II).In the above case, GPT-4o selects the correct option but contain errors in its reasoning process. In the case below, both GPT-4o and Gemini-2.0-Flash follow a correct reasoning process and ultimately select the right answer.",
                "position": 1846
            },
            {
                "img": "https://arxiv.org/html/2504.15280/x21.png",
                "caption": "Figure 23:Questions that evaluators answered incorrectly, along with a detailed review of their reasoning for selecting the incorrect options.",
                "position": 1849
            },
            {
                "img": "https://arxiv.org/html/2504.15280/x22.png",
                "caption": "Figure 24:The visualization of all model performance across the 6 task categories inAll-Angles Bench.",
                "position": 1852
            }
        ]
    },
    {
        "header": "9Visualization Results",
        "images": []
    }
]