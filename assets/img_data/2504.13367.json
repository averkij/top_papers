[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13367/extracted/6370420/pics/terminator_logo.png",
                "caption": "",
                "position": 115
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13367/extracted/6370420/pics/calibration.png",
                "caption": "Figure 1:Question-level difficulty vs average token spend across models for three reasoning datasets. Difficulty scores are scaled by 10 and mapped to integers from 1 to 10 for readability.\nWe observe a clear relationship between question difficulty and token spend distribution.",
                "position": 161
            }
        ]
    },
    {
        "header": "2Difficulty Calibration in Reasoning Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13367/x1.png",
                "caption": "Figure 2:dumb500dataset composition and grading method.\nThe dataset contains four subsets,chat,code,task&math, which are each graded with subset-specific methods.mathare graded with traditional answer pairs.chatandtaskare graded using a combination of LM-judged rubrics and where appropriate, answers.codeoutputs are generated as test case coverage.",
                "position": 367
            }
        ]
    },
    {
        "header": "3Extending Overthinking Evaluation withdumb500",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13367/extracted/6370420/pics/acc_distr.png",
                "caption": "Figure 3:Total difficulty distribution of the four datasets we evaluate in this work. Difficulty scores are scaled by 10 and mapped to integers from 1 to 10 for readability. By includingdumb500in our analysis, we are able to characterize the overthinking behavior of current opening reasoning models more consistently accross the difficulty spectrum.",
                "position": 437
            },
            {
                "img": "https://arxiv.org/html/2504.13367/extracted/6370420/pics/dumb500_acc_tok.png",
                "caption": "Figure 4:Relationship between average token spendS‚Å¢pùëÜùëùSpitalic_S italic_p(Tokens) and average score for the evaluated models on each subset ofdumb500.",
                "position": 456
            }
        ]
    },
    {
        "header": "4ThoughtTerminator",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13367/x2.png",
                "caption": "Figure 5:ThoughtTerminatoruses a reasoning model‚Äôs (calibrated) estimate of the difficulty of a problem to set its intervention, periodically interrupting the reasoning model‚Äôs output to remind it of the amount of remaining tokens.\nOnce the token allotment has been used, it forces the model to provide an answer with constrained decoding.",
                "position": 483
            }
        ]
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13367/x3.png",
                "caption": "Figure 6:Comparison of the relationship between Pass@10 and token spend for the evaluated reasoning models in the ‚ÄùBase‚Äù setting and withThoughtTerminator.",
                "position": 537
            },
            {
                "img": "https://arxiv.org/html/2504.13367/x4.png",
                "caption": "Figure 7:Calibration ablation experiment usingDeepSeek-R1-1.5B.real-minrepresents using the previously observed minimum successful answer length (or, a fallback maximum for examples that were never solved correctly) as theThoughtTerminatordeadline.fix-{200,500,1000,2000}signify using the respective number as a fixed token count deadline for all samples.pred-diff-{gpt4o,ref,trained}refer to using question-level difficulty predictions as deadlines, produced from external LMs, a question-level reference difficulty key of token lengths from the other models, or trained RMs.",
                "position": 624
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13367/extracted/6370420/pics/math500_acc_corr.png",
                "caption": "Figure 8:Pearson correlation of accuracies across different models on the MATH500 dataset",
                "position": 1891
            },
            {
                "img": "https://arxiv.org/html/2504.13367/extracted/6370420/pics/gpqa_acc_corr.png",
                "caption": "Figure 9:Pearson correlation of accuracies across different models on the GPQA dataset",
                "position": 1894
            },
            {
                "img": "https://arxiv.org/html/2504.13367/extracted/6370420/pics/zebra_acc_corr.png",
                "caption": "Figure 10:Pearson correlation of accuracies across different models on the Zebra dataset",
                "position": 1897
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]