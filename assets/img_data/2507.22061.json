[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.22061/x1.png",
                "caption": "",
                "position": 87
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.22061/x2.png",
                "caption": "Figure 2:Given a motion of interest, our approach enables retrieval and indexing of relevant videos and their corresponding objects from the internet or personal collections. Notably, these motions of interest can be novel actions that are difficult to describe accurately using single-frame images or text alone.",
                "position": 121
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3MOVEBenchmark",
        "images": []
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.22061/x3.png",
                "caption": "Figure 3:Overview of our proposed method.",
                "position": 293
            },
            {
                "img": "https://arxiv.org/html/2507.22061/x4.png",
                "caption": "Figure 4:Decoupled Motion-Appearance (DMA) Module.",
                "position": 325
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.22061/x5.png",
                "caption": "Figure 5:t-SNE[55]visualization of prototypes in our model (a)w/o decouplingand (b)w/ decoupling. Different colors and different shapes represent the object categories (e.g., cat) and motion categories (e.g., surfing), respectively. The proposed DMA effectively extracts the motion-centric prototypes and makes those having the same motions closer in feature space.",
                "position": 1172
            },
            {
                "img": "https://arxiv.org/html/2507.22061/x6.png",
                "caption": "Figure 6:Qualitative comparison of representative cases fromMOVEbetween baseline methods, DANet[3]and HPAN[54], and our proposed DMA. (a) shows different object categories of “cat” (Support 1) and “person” (Query) performing the same action, “playing drums”. (b) presents temporally correlated motions: fingers transitioning “from pinching to opening” (Support 1) and “from opening to pinching” (Support 2 & Query videos). (c) is a misleading background in the Query video, playing “football” on the “basketball court”.",
                "position": 1176
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]