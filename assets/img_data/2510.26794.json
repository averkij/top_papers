[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.26794/x1.png",
                "caption": "Figure 1:Overview of our approach toward generalizable 3D human motion generation.(a)ViMoGen: Our model demonstrates superior generalization on challenging prompts including martial arts, dynamic sports, and multi-step behaviors. (b)MBench: Comprehensive benchmark evaluating models across dimensions, showing ViMoGen’s significant improvements over existing methods. (c)ViMoGen-228K: Large-scale dataset with 228,000 motion sequences from diverse sources covering simple indoor to complex outdoor activities.",
                "position": 134
            }
        ]
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.26794/fig/ICLR-pipeline7.jpg",
                "caption": "Figure 2:Overview ofViMoGen. (a) Our model takes a text prompt as input and leverages both a text encoder and an offline video generation model to produce textual and video motion tokens. These are fused with noisy motion inputs through a stack of gating Diffusion Blocks. (b) Each block includes self-attention, an adaptive gating module, and two cross-attention branches: Text-to-Motion (T2M) and Motion-to-Motion (M2M). Only one branch is activated at a time, enabling the model to adaptively balance robustness and generalization.",
                "position": 200
            }
        ]
    },
    {
        "header": "3ViMoGen-228K Dataset",
        "images": []
    },
    {
        "header": "4MBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.26794/x2.png",
                "caption": "Figure 3:Overview ofMBench. (a) MBench features more balanced distribution and vastly different prompt designs compared to HumanML3D. (b) MBench designed is to systematically evaluate motion generation algorithms across nine dimensions, focusing on motion quality, prompt-following, and generalization capability.",
                "position": 360
            },
            {
                "img": "https://arxiv.org/html/2510.26794/x3.png",
                "caption": "Figure 4:Qualitative comparison on MBench prompts. We show keywords in prompts for simplicity.",
                "position": 363
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusion and Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAI usage declaration",
        "images": []
    },
    {
        "header": "Appendix BRelated work",
        "images": []
    },
    {
        "header": "Appendix CMBench details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.26794/x4.png",
                "caption": "Figure 5:Human Preference Annotation Interface.Two rendered videos side-by-side with annotator choices.",
                "position": 2022
            },
            {
                "img": "https://arxiv.org/html/2510.26794/fig/combined_motion_quality_metrics.png",
                "caption": "Figure 6:Correlation between MBench automatic evaluations and human annotations across motion quality dimensions.",
                "position": 2044
            }
        ]
    },
    {
        "header": "Appendix DViMoGen-228K datails",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.26794/x5.png",
                "caption": "Figure 7:Visualization of our ViMoGen-228K dataset. (a) High-fidelity Optical MoCap data. (b) Diverse In-the-Wild Video Data. (c) Precisely controlled Synthetic Video Data.",
                "position": 2147
            }
        ]
    },
    {
        "header": "Appendix EText-to-motion Experiment on HumanML3D Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.26794/x6.png",
                "caption": "Figure 8:Qualitative comparison with state-of-the-art methods on the HumanML3D benchmark. For complex, multi-step prompts, our ViMoGen-light model generates motions that are more plausible and demonstrate superior text-motion alignment compared to prior works.",
                "position": 2542
            },
            {
                "img": "https://arxiv.org/html/2510.26794/x7.png",
                "caption": "Figure 9:Qualitative examples of our adaptive branch selection mechanism. This figure showcases how ViMoGen intelligently chooses between its Motion-to-Motion (M2M) and Text-to-Motion (T2M) branches based on the quality of the initial motion extracted from generated videos (Mocap Baseline).(Rows 1-2)For prompts where the ViGen model produces a plausible motion sequence (e.g., ”backflip”, ”windsurfer”), the adaptive gate selects theM2M branch. This branch successfully refines the semantically correct but noisy Mocap Baseline, reducing jitter and improving physical realism.(Rows 3-4)For prompts involving sudden movements where the ViGen model fails and produces distorted or incomplete motions (e.g., ”twist and throw”, ”stumble and fall”), the Mocap Baseline is unreliable. Here, the adaptive gate correctly falls back to the more robustT2M branch, which generates a stable motion directly from the text prompt, ignoring the flawed video prior.",
                "position": 2553
            },
            {
                "img": "https://arxiv.org/html/2510.26794/x8.png",
                "caption": "Figure 10:Qualitative examples illustrating the impact of different text prompt styles used during training and inference. The rows represent the style of text used to train the model (video-stylevs.motion-style), while the columns show the prompt style used for testing. We display results for two prompts: ”swaggering into the room” and ”chopping wood.” The visualizations demonstrate that the model trained on descriptivevideo-styletext (top row) is more robust and generalizes better, producing high-quality motions for both concisemotion-styleand richvideo-styleprompts at test time. Full prompt examples are provided below the images for clarity.",
                "position": 2559
            },
            {
                "img": "https://arxiv.org/html/2510.26794/x9.png",
                "caption": "Figure 11:Additional qualitative comparisons with state-of-the-art methods on MBench prompts. Both ViMoGen and ViMoGen-light consistently generate motions that more faithfully adhere to the detailed text descriptions, showcasing their superior semantic understanding and generation quality compared to prior works.",
                "position": 2562
            }
        ]
    },
    {
        "header": "Appendix FAdditional Qualitative Results",
        "images": []
    }
]