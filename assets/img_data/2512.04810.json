[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04810/x1.png",
                "caption": "Figure 1:Showcase of EMMA’s text-to-image generation capability.",
                "position": 104
            }
        ]
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04810/x2.png",
                "caption": "Figure 2:The overall architecture of our EMMA.\"Und Adapter\" and \"Gen Adapter\" represent understanding adapter and generation adapter for projecting visual tokens into unified multimodal architecture, respectively.",
                "position": 271
            }
        ]
    },
    {
        "header": "3Data Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04810/x3.png",
                "caption": "Figure 3:Overview of the general editing data construction pipeline.The data pipeline consists of two parts. The first is to generate image editing pairs: firstly, we use a VLM to generate editing instruction for the input image, then use an image editing model to generate the edited image as well as reversing the editing instruction to obtain the reversed pair. The second part is to filter the image editing pairs: we use a VLM to determine whether the edited image follows the editing instruction, and if the image contains portraits, we further filter the pair using face similarity.",
                "position": 416
            },
            {
                "img": "https://arxiv.org/html/2512.04810/x4.png",
                "caption": "Figure 4:Overview of the text editing data construction pipeline.The data pipeline consists two parts. The first is to generate text-edited image pairs: firstly, text detection is performed on the input image to extract text information, and then one or more words are randomly selected for replacement or removal, while generating the corresponding editing instruction. We further use a image editing model to produce the edited image. The second part is to filter the image pairs, where the OCR model is used to determine whether the edited image follows the instruction or not.",
                "position": 419
            }
        ]
    },
    {
        "header": "4Training Details",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04810/x5.png",
                "caption": "Figure 5:Showcase of EMMA’s image editing capability.EMMA effectively adheres to editing instructions while maintaining the character consistency. Furthermore, it natively supports both Chinese editing instructions and complex instructions. It is worth emphasizing that almost all editing training data used for EMMA consisted solely of single English instructions. The above results indicate that EMMA shows strong generalization capabilities, enabling it to handle complex multimodal tasks effectively.",
                "position": 1361
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]