[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3FreeScale",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15774/x1.png",
                "caption": "Figure 1:Overall framework of CineScale.(a)Tailored Self-Cascade Upscaling.CineScale first upsamples a generated image or video from the training resolution, then gradually adds noise to the high-resolution latent, and finally denoises it to achieve detail reconstruction. Part of the clean latent is reintroduced during denoising to stabilize generation and control detail.(b)Scale Fusion. For the UNet structure, we modify the self-attention layer to combine global and local attention, fusing high-frequency details and low-frequency semantics via Gaussian blur for the final output. We also useRestrained Dilated Convolutionto adapt the convolution layer of the model to high resolution for reducing repetition.(c)DiT Extention. To support DiT models, we additionally add NTK-RoPE and Attentional Scaling. Building on the tuning-free setup, Minimal LoRA Fine-Tuning is additionally introduced to help the model better adapt to the modified RoPE, leading to improved performance.",
                "position": 187
            },
            {
                "img": "https://arxiv.org/html/2508.15774/x2.png",
                "caption": "Figure 2:Structure gap.UNet-based LDMs and DiT-based LDMs will face different challenges in the higher-resolution generation task. UNet-based LDMs face repetition problems while DiT-based LDMs face blur problems.",
                "position": 270
            },
            {
                "img": "https://arxiv.org/html/2508.15774/x3.png",
                "caption": "Figure 3:Image qualitative comparisons with other baselines.Our method generates both204822048^{2}and409624096^{2}vivid images with better content coherence and local details.",
                "position": 351
            }
        ]
    },
    {
        "header": "4DiT Extension",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15774/x4.png",
                "caption": "Figure 4:Zoomed in details for the 8k image.FreeScale may regenerate the original blurred areas at low resolution based on the prior knowledge that the model has learned. As shown in the bottom row, two originally chaotic and blurry faces are clearly outlined at 8k resolution.",
                "position": 370
            }
        ]
    },
    {
        "header": "5Experiments for UNet Structure",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15774/x5.png",
                "caption": "Figure 5:Flexible aspect ratio generation.FreeScale can directly achieve a flexible aspect ratio (the resolution must be a multiple of512512) without any adaptation.",
                "position": 552
            },
            {
                "img": "https://arxiv.org/html/2508.15774/x6.png",
                "caption": "Figure 6:Image qualitative comparisons with super-resolution.FreeScale is not inferior to SDXL+Real-ESRGAN in visual quality, and adds more details. In addition, SR methods will faithfully follow the low-resolution input while FreeScale can regenerate the original blurred areas based on the prior knowledge that the model has learned.",
                "position": 669
            },
            {
                "img": "https://arxiv.org/html/2508.15774/x7.png",
                "caption": "Figure 7:Results of flexible control for detail level.A better result will be generated by adding the coefficient weight in the area of Griffons and reducing the coefficient weight in the other regions.",
                "position": 672
            },
            {
                "img": "https://arxiv.org/html/2508.15774/x8.png",
                "caption": "Figure 8:Local semantic editing of images.FreeScale makes the hair purple or edits the face to make this person look more Japanese in the higher-resolution (409624096^{2}).",
                "position": 675
            },
            {
                "img": "https://arxiv.org/html/2508.15774/x9.png",
                "caption": "Figure 9:UNet-based video qualitative comparisons with other baselines.While other baselines fail in video generation, FreeScale effectively generates higher-resolution videos with high fidelity. Best viewedZOOMED-IN.",
                "position": 679
            },
            {
                "img": "https://arxiv.org/html/2508.15774/x10.png",
                "caption": "Figure 10:Fast generation with SDXL-Turbo.FreeScale can help SDXL-Turbo generate results at204822048^{2}resolution with even22timesteps.",
                "position": 718
            },
            {
                "img": "https://arxiv.org/html/2508.15774/x11.png",
                "caption": "Figure 11:Qualitative image comparisons with ablations.Our full method performs the best. The resolution of results is409624096^{2}for better visualizing the difference between the various strategies.",
                "position": 722
            },
            {
                "img": "https://arxiv.org/html/2508.15774/x12.png",
                "caption": "Figure 12:Video ablations in Wan T2V without tuning in resolution𝟗𝟔𝟎×𝟏𝟔𝟔𝟒\\mathbf{960\\times 1664}.Although all variants can generate rough results. Our full method performs the best. Best viewedZOOMED-IN.",
                "position": 736
            },
            {
                "img": "https://arxiv.org/html/2508.15774/x13.png",
                "caption": "Figure 13:Video comparison with DiT-based models in resolution𝟗𝟔𝟎×𝟏𝟔𝟔𝟒\\mathbf{960\\times 1664}.Although other baselines can produce reasonable results at moderately higher resolutions, they still suffer from varying degrees of blurriness. In contrast, CineScale generates high-quality videos with rich visual details. Best viewedZOOMED-IN.",
                "position": 1056
            },
            {
                "img": "https://arxiv.org/html/2508.15774/x14.png",
                "caption": "Figure 14:Video comparison with DiT-based models in resolution𝟏𝟗𝟐𝟎×𝟑𝟑𝟐𝟖\\mathbf{1920\\times 3328}.At the resolution several times higher than those used during training, LTX and Wan-DI tend to fail completely. While UAV, a video super-resolution approach, can still produce visually reasonable results, it is unable to recover fine details that are ambiguous or missing in the low-resolution inputs. In contrast, CineScale consistently generates high-quality videos with rich and faithful visual details. Best viewedZOOMED-IN.",
                "position": 1232
            },
            {
                "img": "https://arxiv.org/html/2508.15774/x15.png",
                "caption": "Figure 15:Video ablations in Wan I2V without tuning in resolution𝟗𝟔𝟎×𝟏𝟔𝟔𝟒\\mathbf{960\\times 1664}.Although all variants can generate rough results. Our full method performs the best. Best viewedZOOMED-IN.",
                "position": 1288
            }
        ]
    },
    {
        "header": "6Experiments for DiT Structure",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15774/x16.png",
                "caption": "Figure 16:Video ablations in ReCamMaster V2V without tuning in resolution𝟗𝟔𝟎×𝟏𝟔𝟔𝟒\\mathbf{960\\times 1664}.Without NTK-RoPE, repeated patterns are prone to occur due to errors in positional encoding. Although all variants can generate rough results. Our full method performs the best.",
                "position": 1300
            },
            {
                "img": "https://arxiv.org/html/2508.15774/x17.png",
                "caption": "Figure 17:Local semantic editing for video generation.CineScale supports efficient editing by allowing users to preview results at low resolution while modifying high-resolution local semantics via prompts.",
                "position": 1382
            },
            {
                "img": "https://arxiv.org/html/2508.15774/x18.png",
                "caption": "Figure 18:4k text-to-video generation.With minimal LoRA fine-tuning, CineScale can achieve 4k (2176×38402176\\times 3840) text-to-video generation. We observe that at 4k resolution, faces can be generated with remarkable clarity even when they occupy only a small portion of the frame, and temporal consistency is also easier to maintain. Best viewedZOOMED-IN.",
                "position": 1396
            },
            {
                "img": "https://arxiv.org/html/2508.15774/x19.png",
                "caption": "Figure 19:4k image-to-video generation.With minimal LoRA fine-tuning, CineScale can achieve 4k (2176×38402176\\times 3840) image-to-video generation.",
                "position": 1399
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "8Biography Section",
        "images": []
    }
]