[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05076/x1.png",
                "caption": "Figure 1:The heatmap for one decoding step of Llama3-8B-Instruct(AI,2024a), where columns and rows indicate different Transformer layers and tokens in the KV cache, respectively. For each layer, the 5 tokens (10% sparsity) with the highest attention scores of the first attention head are highlighted in yellow, which are the tokens used for sparse attention.\nWe feed an input prompt “Use only the provided search results to write a high-quality, concise answer to the question.\\n<—begin_of_text—>\\n The magic number is: 15213. \\n\\n\\n Question: What is the magic number? Keep the response short and direct. Answer: ”, and the LLM outputs “15213”. The results show strong spatial coherence of tokens chosen for sparse attention in the decoding step.",
                "position": 131
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05076/x2.png",
                "caption": "Figure 2:An overview of the decoding step in TidalDecode, which performs full attention for the first two layers, full attention with token selection for the third layer and a middle layer, and position persistent sparse attention for all other layers.",
                "position": 217
            },
            {
                "img": "https://arxiv.org/html/2410.05076/x3.png",
                "caption": "(a)Overlap of Tokens with the Highest Attention Scores between Layers",
                "position": 253
            },
            {
                "img": "https://arxiv.org/html/2410.05076/x3.png",
                "caption": "(a)Overlap of Tokens with the Highest Attention Scores between Layers",
                "position": 256
            },
            {
                "img": "https://arxiv.org/html/2410.05076/x4.png",
                "caption": "(b)Recall Rate by Re-selection Layer",
                "position": 261
            },
            {
                "img": "https://arxiv.org/html/2410.05076/x5.png",
                "caption": "Figure 4:Cache Correction",
                "position": 360
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05076/x6.png",
                "caption": "(a)Token Budget = 2048",
                "position": 663
            },
            {
                "img": "https://arxiv.org/html/2410.05076/x6.png",
                "caption": "(a)Token Budget = 2048",
                "position": 666
            },
            {
                "img": "https://arxiv.org/html/2410.05076/x7.png",
                "caption": "(b)Token Budget = 4096",
                "position": 671
            },
            {
                "img": "https://arxiv.org/html/2410.05076/x8.png",
                "caption": "Figure 6:End-to-end latency results on LLaMA-2-7B model for Full attention baseline(Full), Quest, and TidalDecode(TD) when context length is 10K, 32K, and 100K, respectively.",
                "position": 783
            },
            {
                "img": "https://arxiv.org/html/2410.05076/x9.png",
                "caption": "",
                "position": 789
            },
            {
                "img": "https://arxiv.org/html/2410.05076/x10.png",
                "caption": "(a)32 Layers",
                "position": 792
            },
            {
                "img": "https://arxiv.org/html/2410.05076/x11.png",
                "caption": "(b)64 Layers",
                "position": 797
            },
            {
                "img": "https://arxiv.org/html/2410.05076/x12.png",
                "caption": "Figure 8:The breakdown latency results for the full attention, token selection attention, sparse attention, and Quest attention kernels over 10K, 32K, and 100K context length. We use full attention latency as a reference and report other kernels’ relative latency ratio. We use a token budget of K=512 for TidalDecode and Quest across all evaluations.",
                "position": 807
            },
            {
                "img": "https://arxiv.org/html/2410.05076/x13.png",
                "caption": "(a)LLaMA-2 Model",
                "position": 817
            },
            {
                "img": "https://arxiv.org/html/2410.05076/x13.png",
                "caption": "(a)LLaMA-2 Model",
                "position": 820
            },
            {
                "img": "https://arxiv.org/html/2410.05076/x14.png",
                "caption": "(b)LLaMA-3 Model",
                "position": 825
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]