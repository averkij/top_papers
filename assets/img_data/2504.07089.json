[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07089/extracted/6349016/Figures/home_logo.jpg",
                "caption": "",
                "position": 67
            },
            {
                "img": "https://arxiv.org/html/2504.07089/x1.png",
                "caption": "",
                "position": 68
            },
            {
                "img": "https://arxiv.org/html/2504.07089/x2.png",
                "caption": "",
                "position": 69
            },
            {
                "img": "https://arxiv.org/html/2504.07089/x3.png",
                "caption": "Figure 1:OmniCaptioner: the top section demonstrates its capability to process diverse visual domains. The bottom section highlights its applications in visual reasoning (associated with reasoning LLM), image generation (integrated with T2I generation models), and efficient downstream SFT tasks adaptation.",
                "position": 78
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07089/x4.png",
                "caption": "Figure 2:Performance comparison across different visual benchmarks for different LLMs/MLLMs (7B) with or without visual input. The bar with dashed borders denotes Qwen2-VL-Instruct, indicating it has pixel-level visual input, while others do not. Qwen2-VL-Ins.(NA) refers to a setting where only the question is provided as input. We divide the MME score by 100 to have the same scale as other benchmarks.",
                "position": 102
            },
            {
                "img": "https://arxiv.org/html/2504.07089/x5.png",
                "caption": "Figure 3:Illustration ofOmniCaptioner’s plug-and-play applications (Sub-figure a, b) and comparison betweenOmniCaptionerand LLava-OneVision-7B on non-natural image captioning (Sub-figure c). Sub-figure (a) shows thatOmniCaptionerleverages LLMs’ strong reasoning abilities to perform multimodal reasoning tasks. Sub-figure (b) highlights how hallucinated or inaccurate captions—like those from LLava-OneVision-7B can lead to inconsistent image conversion, revealing weakened alignment capabilities in text-to-image models when captions don’t faithfully represent the original content. Sub-figure (c) highlights that LLaVA-OneVision-7B, due to limited exposure to non-natural images during pretraining, struggles with perception in such domains, often leading to hallucinations, whereasOmniCaptionerprovides more accurate descriptions.",
                "position": 105
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3OmniCaptioner",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07089/x6.png",
                "caption": "Figure 4:OmniCaptioner’s diverse visual captioning pipeline.\nThe pipeline consists of Seed-Caption Generation to ensure precise pixel-to-word mapping, and Caption Extension to enrich caption styles to support image generation and visual reasoning tasks.OmniCaptionerutilizes a21M-caption dataset, covering diverse domains beyond natural images, enabling more comprehensive captioning capabilities. For further details about dataset composition, please refer to Fig.7in AppendixA.",
                "position": 153
            }
        ]
    },
    {
        "header": "4One Captioner to Rule Them All",
        "images": []
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07089/x7.png",
                "caption": "Figure 5:IntegrateOmniCaptionerinto different versions of LLMs, enabling them to handle tasks in multimodal scenarios.",
                "position": 684
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AOmniCaptioner Dataset Composition",
        "images": []
    },
    {
        "header": "Appendix BExperimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07089/extracted/6349016/Figures/token_len_dis.jpg",
                "caption": "Figure 6:Token length distribution for natural images.",
                "position": 1524
            }
        ]
    },
    {
        "header": "Appendix CSystem Prompt Example",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07089/x8.png",
                "caption": "Figure 7:Dataset composition for pretrainingOmniCaptioner.",
                "position": 1535
            }
        ]
    },
    {
        "header": "Appendix DCaption Visualization",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07089/x9.png",
                "caption": "Figure 8:Different system prompts used forOmniCaptioner.",
                "position": 1553
            },
            {
                "img": "https://arxiv.org/html/2504.07089/x10.png",
                "caption": "Figure 9:Natural image captioning.",
                "position": 1556
            },
            {
                "img": "https://arxiv.org/html/2504.07089/x11.png",
                "caption": "Figure 10:Table/Chart image captioning.",
                "position": 1560
            },
            {
                "img": "https://arxiv.org/html/2504.07089/x12.png",
                "caption": "Figure 11:Visual-Text image captioning.",
                "position": 1563
            },
            {
                "img": "https://arxiv.org/html/2504.07089/x13.png",
                "caption": "Figure 12:Math image captioning.",
                "position": 1566
            },
            {
                "img": "https://arxiv.org/html/2504.07089/x14.png",
                "caption": "Figure 13:UI captioning.",
                "position": 1569
            },
            {
                "img": "https://arxiv.org/html/2504.07089/x15.png",
                "caption": "Figure 14:PDF captioning.",
                "position": 1572
            },
            {
                "img": "https://arxiv.org/html/2504.07089/x16.png",
                "caption": "Figure 15:Video captioning.",
                "position": 1575
            },
            {
                "img": "https://arxiv.org/html/2504.07089/x17.png",
                "caption": "Figure 16:Natural image captioning with different system prompts.",
                "position": 1578
            },
            {
                "img": "https://arxiv.org/html/2504.07089/x18.png",
                "caption": "Figure 17:Structured image captioning with different system prompts.",
                "position": 1581
            },
            {
                "img": "https://arxiv.org/html/2504.07089/x19.png",
                "caption": "Figure 18:Visualization of thinking process withOmniCaptionerfor natural images.",
                "position": 1584
            },
            {
                "img": "https://arxiv.org/html/2504.07089/x20.png",
                "caption": "Figure 19:Visualization of thinking process withOmniCaptioner.",
                "position": 1587
            },
            {
                "img": "https://arxiv.org/html/2504.07089/x21.png",
                "caption": "Figure 20:Visualization of thinking process withOmniCaptionerfor math images.",
                "position": 1590
            },
            {
                "img": "https://arxiv.org/html/2504.07089/x22.png",
                "caption": "Figure 21:The detailed caption fromOmniCaptionerenhances the alignment capability of text-to-image generation by providing precise descriptions, ensuring that the generated image accurately reflects the intended concepts, attributes, and relationships. The generation model here is fine-tuned on images labeled byOmniCaptioner, using the SANA 1.0 model with 1.6B parameters.",
                "position": 1593
            },
            {
                "img": "https://arxiv.org/html/2504.07089/x23.png",
                "caption": "Figure 22:Image Conversion throughOmniCaptionerand SANA-1.0. The generation model, SANA-1.0, is fine-tuned on images annotated byOmniCaptioner, enabling more accurate and semantically aligned image generation.",
                "position": 1596
            }
        ]
    },
    {
        "header": "Appendix EText-to-Image Generation",
        "images": []
    }
]