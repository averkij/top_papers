[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.06788/x1.png",
                "caption": "Figure 1:Key Findings.(i)Projection matrices (Œî‚ÄãWQ\\Delta W_{Q}andŒî‚ÄãWV\\Delta W_{V}) exhibit volume-law internal entanglement profiles with distinctive entanglement valleys that differ between FFT, LoRA and MPS adaptation.(ii)Attention matrices show area-law scaling with logarithmic corrections. Random matrix theory explains this through the Attention Cardy Formula.(iii)Despite internal differences, external attention outputs remain invariant, revealing a no-hair‚Äìlike effect where the attention mechanism acts as a coarse-graining operator.",
                "position": 68
            }
        ]
    },
    {
        "header": "IIntroduction",
        "images": []
    },
    {
        "header": "IIResults",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.06788/x2.png",
                "caption": "Figure 2:Artificial entanglement profiling of a random Gaussian matrix. The orange curve (œá\\chi=‚àû\\infty) corresponds to the full SVD at each bond and closely matches the Page-curve prediction for Haar-random states (green dashed line). The blue curve (œá=32\\chi=32) demonstrates the effect of truncating the MPS bond dimension the entropy saturates once the Schmidt rank exceedsœá\\chi, forming a plateau, while the true entropy (the orange) continues to increase toward the Page limit.",
                "position": 254
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x3.png",
                "caption": "Figure 3:Artificial entanglement profiling ofŒî‚ÄãWQ\\Delta W_{Q}(a) andŒî‚ÄãWV\\Delta W_{V}(b) across different bi-partition positionskkduring FFT. Each curve corresponds to a training step, which shows howSSevolves and gradually converges as fine-tuning progresses.",
                "position": 274
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x4.png",
                "caption": "Figure 4:Artificial entanglement profiling ofŒî‚ÄãWQ\\Delta W_{Q}(a) andŒî‚ÄãWV\\Delta W_{V}(b) during LoRA fine-tuning, similar to FIG.3.",
                "position": 279
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x5.png",
                "caption": "Figure 5:Artificial entanglement profiling ofŒî‚ÄãWQ\\Delta W_{Q}andŒî‚ÄãWV\\Delta W_{V}as a function of LoRA rankrrin different steps.\n(a)SŒî‚ÄãWQS_{\\Delta W_{Q}}at the early time of fine-tuning (Step¬†1) and the late time (Step¬†1000).\n(b)SŒî‚ÄãWVS_{\\Delta W_{V}}at the early time of fine-tuning (Step¬†1) and the late time (Step¬†1000). These reveal how differingrrand training time jointly shape the entanglement structure of the learned updates.",
                "position": 295
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x6.png",
                "caption": "Figure 6:Test loss across training steps for different LoRA ranksrr. Whenrris small, increasingrrgenerally improves performance. While increasing to a specificrrthe test loss saturates and is not improved much. We keep other hyper-parameters fixed, such as the learning rate andŒ±\\alpha.",
                "position": 302
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x7.png",
                "caption": "Figure 7:Final test loss as a function of learning rate for FFT and LoRA when the scaling parameterŒ±\\alphais small (hereŒ±=16\\alpha=16). LoRA requires a larger learning rate than FFT to achieve the best test performance, consistent with previous observationsBidermanet¬†al.(2024); Schulman and Lab (2025).",
                "position": 305
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x8.png",
                "caption": "Figure 8:SŒî‚ÄãWQS_{\\Delta W_{Q}}andSŒî‚ÄãWVS_{\\Delta W_{V}}w.r.t the cut positionkk(withdL=2kd_{L}=2^{k}) across training steps when increasingŒ±=256\\alpha=256. Panels (a) and (b) show that at early time, the curves exhibit a pronounced ‚Äúentanglement valley‚Äù structure centered near the middle cuts. As training progresses, the curves ofŒî‚ÄãWQ\\Delta W_{Q}are less deepened as the cases in smallerŒ±=16\\alpha=16, while the curves ofŒî‚ÄãWV\\Delta W_{V}gradually lift and approach a saturated shape.",
                "position": 316
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x9.png",
                "caption": "Figure 9:Entanglement entropySSin the middle bi-partition for differentŒ±\\alpha.SSin early time (Step = 1) serves as a baseline. By showing the converged solution at late time (Step = 1000), this demonstrates howŒ±‚àà{1,2,4,8,16,32,64,128,256}\\alpha\\in\\{1,2,4,8,16,32,64,128,256\\}impacts the solution learned by LoRA.",
                "position": 319
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x10.png",
                "caption": "Figure 10:Test loss comparison across the scaling factorŒ±\\alphain LoRA.XX-axis is shown in log scale.",
                "position": 323
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x11.png",
                "caption": "Figure 11:Final test loss with respect to learning rate under different LoRA configurations. (a) Comparison of full fine-tuning with LoRA at multipleŒ±\\alphawhen rankr=256r=256. LargerŒ±\\alphavalues shift the optimal learning rate to be smaller, approaching the solutions from full-fine-tuning. (b) Joint variation ofrrandŒ±\\alphafor LoRA. The settingsr=32r=32andr=256r=256do not shift the optimal learning rate in differentŒ±\\alpha.",
                "position": 326
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x12.png",
                "caption": "Figure 12:Artificial Entanglement Profiling of the attention matrix (Head 10)SAS_{A}across training steps.SAS_{A}remains uniformly low throughout training and exhibits a characteristic of an approximate area-law scaling, where the entropy grows only mildly that roughly following a shallow logarithmic rise. This indicates that the attention matrix operates in a strongly low-entanglement regime.",
                "position": 350
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x13.png",
                "caption": "Figure 13:Normalized entanglement entropySA/log‚Å°(œá)S_{A}/\\log(\\chi)of six representative attention heads across training steps, whereœá=min‚Å°(dL,dR)\\chi=\\min(d_{L},d_{R})denotes the bond dimension, andlog‚Å°(œá)\\log(\\chi)represents the theoretical maximum entanglement entropy. Most heads exhibit consistently mild scaling throughout training, far from the volume law, indicating that the effective correlations encoded by the attention mechanism remain far below the theoretical maximum.",
                "position": 354
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x14.png",
                "caption": "Figure 14:Normalized entanglement entropySA/log‚Å°(œá)S_{A}/\\log(\\chi)of six representative\nattention heads across training steps and LoRA scaling coefficientsŒ±\\alpha.SSremains significantly below the theoretical maximum and shows negligible\nvariation with either training time or the choice ofŒ±\\alpha, suggesting a no-hair like behavior.",
                "position": 357
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x15.png",
                "caption": "Figure 15:Entanglement entropySùí™S_{\\mathcal{O}}of the output operatorùí™=X‚ÄãX‚ä§\\mathcal{O}=XX^{\\top}across bi-partition positions and training steps.\nPanels (a) and (b) show thatSùí™S_{\\mathcal{O}}remains significantly below the theoretical maximum. Panel (c) illustrates that this behavior is invariant across the variation of scaling coefficientŒ±\\alphaand training steps,\nindicating a no-hair like behavior as well.",
                "position": 364
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x16.png",
                "caption": "Figure 16:Entanglement entropySA/log‚Å°(œá)S_{A}/\\log(\\chi)across training steps for several\nattention heads under scaling coefficientsŒ±‚àà{8,16,32,64}\\alpha\\in\\{8,16,32,64\\}. Across all heads, the entanglement curves remain approximately invariant with respect to bothŒ±\\alphaand the training step. This is similar to the behavior observed in LoRA (See FIG.14).",
                "position": 496
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x17.png",
                "caption": "Figure 17:Overall framework for artificial entanglement analysis. (i) Reshaping matrices including the updates of weight projection matrices such asŒî‚ÄãWQ\\Delta W_{Q}andŒî‚ÄãWV\\Delta W_{V}from FFT or LoRA, and attention-related matrices into higher-order tensors by factorizing input and output dimensions into prime components; (ii) Performing a sequence of SVDs to decompose the tensor into a chain of Order-3 tensors connected by virtual bonds, where virtual bond indices capture correlations between sites; (iii) Interpreting the MPS factorization as a mathematical formalism of many-body states, formally analogous to the MPS representation of the amplitude tensor of a quantum many-body state; (iv) Computing the von Neumann entanglement entropy (SS) at each bond, yielding an artificial entanglement profile.",
                "position": 502
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x18.png",
                "caption": "Figure 18:Artificial entanglement profiling of the MPS adaptation ofŒî‚ÄãW\\Delta Wfor ranksr‚àà{4,32}r\\in\\{4,32\\}withŒ±=16\\alpha=16across training steps.\nBoth rank settings exhibit a characteristic ‚Äúentanglement valley‚Äù structure,\nwhile the rank-44case displays a more pronouncedbi-modal(dual-valley)\nprofile. In contrast, the rank-3232curves form a smoother single-valley landscape.",
                "position": 530
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x19.png",
                "caption": "Figure 19:Final test loss of LoRA and MPS adaptation across multiple learning rates and scaling coefficientsŒ±\\alpha. Compared to LoRA, the MPS adaptation exhibits an optimal learning-rate region that is consistently shifted toward larger learning rates.",
                "position": 539
            }
        ]
    },
    {
        "header": "IIIMethods",
        "images": []
    },
    {
        "header": "IVDiscussion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "VNotations",
        "images": []
    },
    {
        "header": "VIPreliminaries and Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.06788/x20.png",
                "caption": "Figure 20:Overview of Tensor Diagrammatic Notation. (a) Visual representations of a scalar, vector, matrix, and an arbitrary order-NNtensor. (b) Tensor contractions illustrating matrix multiplication and multiple tensor products. (c) The process of modeling an order-NNtensor as a Matrix Product State.",
                "position": 1635
            }
        ]
    },
    {
        "header": "VIIMathematical Foundations: Quantum Entanglement and Entropy Measures",
        "images": []
    },
    {
        "header": "VIIITheorems and Proofs",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.06788/x21.png",
                "caption": "Figure 21:Comparison of masked and unmasked attention entanglement entropy across multiple heads. For each selected head, we plot the normalized entanglement entropySA/log‚Å°(œá)S_{A}/\\log(\\chi)as with respect to the bi-partition during training. Removing the causal mask leads to markedly different behaviors across heads: some heads (Head 10, 25) exhibit a substantial increase in entanglement entropy when future positions become accessible, indicating a strong reliance on the causal constraint, while others remain almost unchanged, revealing intrinsically low-entanglement attention patterns that are insensitive to masking.",
                "position": 2581
            }
        ]
    },
    {
        "header": "IXMPS Adaptation: Method and Relationship to MPS Modeling",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.06788/x22.png",
                "caption": "Figure 22:Normalized singular value spectraŒªi(‚Ñì)\\lambda_{i}^{(\\ell)}at different cut positions‚Ñì\\ellforWQW_{Q}andWVW_{V}matrices at the first and last training steps. Each curve corresponds to a different cut position, with the singular values sorted in descending order and normalized by their‚Ñì2\\ell_{2}norm. The pronounced suppression of intermediate and small singular values near the mid-cut (row‚Äìcolumn bi-partition) provides a direct spectral explanation for the entanglement valley observed in the entanglement entropy profile.",
                "position": 2674
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x23.png",
                "caption": "Figure 23:Marchenko-Pastur distribution: theoretical baseline. The figure shows the theoretical density curves for different values of the parameterc=dmin/dmaxc=d_{\\min}/d_{\\max}, wheredmind_{\\min}anddmaxd_{\\max}are the smaller and larger dimensions of a random matrix, respectively. The MP distribution describes the limiting eigenvalue distribution of large random matrices with i.i.d. entries, providing a reference for comparing the singular value spectra observed in our MPS-decomposed weight projection matrices.",
                "position": 2903
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x24.png",
                "caption": "Figure 24:Comparison of normalized eigenvalue distributions from reduced density matrices at different cuts in MPS-decomposed weight projection matrices with the theoretical Marchenko-Pastur distribution. The figure shows empirical density distributions of normalized eigenvaluesŒª\\lambda(from reduced density matrices at different cuts) fromŒî‚ÄãWQ\\Delta W_{Q}andŒî‚ÄãWV\\Delta W_{V}matrices, plotted against the theoretical MP densityœÅ‚Äã(x)\\rho(x)wherex=dmin‚ãÖŒªx=d_{\\min}\\cdot\\lambda, illustrating how the observed eigenvalue spectra deviate from random matrix behavior.",
                "position": 2906
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x25.png",
                "caption": "Figure 25:Normalized entanglement entropySA/log‚Å°(œá)S_{A}/\\log(\\chi)for attention heads under extreme scaling coefficientsŒ±‚àà{16,512,4096}\\alpha\\in\\{16,512,4096\\}, probing the limits of the no-hair property. Compared to the moderate regime (FIG.16), whenŒ±=512\\alpha=512the entanglement curves remain approximately invariant, maintaining the no-hair-like property. However, whenŒ±\\alphascales further to40964096, pronounced differences emerge: for several attention heads,Œ±=4096\\alpha=4096produces substantially higher entanglement entropySS, with a sudden increase during early training steps, whileŒ±=16\\alpha=16andŒ±=512\\alpha=512remain almost flat throughout training. These results demonstrate that the no-hair property holds within a certain range ofŒ±\\alphavalues but breaks down under extreme scaling.",
                "position": 2916
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x26.png",
                "caption": "Figure 26:Entanglement entropySAS_{A}of the attention matrix (Head 30) with respect to the bi-partition position across training steps for LLaMA-3.2-1B fine-tuned on OpenThoughts3 dataset.",
                "position": 2932
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x27.png",
                "caption": "Figure 27:Entanglement entropySA/log‚Å°(œá)S_{A}/\\log(\\chi)across training steps for several attention heads under less-deviated scaling coefficientsŒ±‚àà{4,8,16,32}\\alpha\\in\\{4,8,16,32\\}for LLaMA-3.2-1B fine-tuned on OpenThoughts3 dataset. See FIG.14for details.",
                "position": 2935
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x28.png",
                "caption": "Figure 28:Normalized entanglement entropySA/log‚Å°(œá)S_{A}/\\log(\\chi)of six representative attention heads across training steps for LLaMA-3.2-1B fine-tuned on OpenThoughts3 dataset, whereœá=min‚Å°(dL,dR)\\chi=\\min(d_{L},d_{R})denotes the maximal entanglement capacity allowed by the bi-partition.",
                "position": 2938
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x29.png",
                "caption": "Figure 29:Entanglement entropySùí™S_{\\mathcal{O}}of the output operatorùí™=X‚ÄãX‚ä§\\mathcal{O}=XX^{\\top}across bi-partition positions and training steps for LLaMA-3.2-1B fine-tuned on OpenThoughts3 dataset.",
                "position": 2941
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x30.png",
                "caption": "Figure 30:Artificial entanglement profiling of the MPS adaptation ofŒî‚ÄãW\\Delta Wfor ranksr‚àà{1,2,4,8,16,32}r\\in\\{1,2,4,8,16,32\\}withŒ±=16\\alpha=16across training steps for LLaMA-3.2-1B fine-tuned on OpenThoughts3 dataset.",
                "position": 2948
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x31.png",
                "caption": "Figure 31:Final test loss with respect to learning rate under different LoRA configurations for LLaMA-3.2-1B fine-tuned on OpenThoughts3 dataset, including a comparison with full fine-tuning.",
                "position": 2951
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x32.png",
                "caption": "Figure 32:Final test loss of LoRA and MPS adaptation across multiple learning rates and scaling coefficientsŒ±\\alphafor LLaMA-3.2-1B fine-tuned on OpenThoughts3 dataset.",
                "position": 2954
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x33.png",
                "caption": "Figure 33:Entanglement entropySAS_{A}of the attention matrix (Head 20) with respect to the bi-partition position across training steps for LLaMA-3.1-8B fine-tuned on Tulu3 dataset.",
                "position": 2968
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x34.png",
                "caption": "Figure 34:Normalized entanglement entropySA/log‚Å°(œá)S_{A}/\\log(\\chi)of six representative attention heads across training steps for LLaMA-3.1-8B fine-tuned on Tulu3 dataset, whereœá=min‚Å°(dL,dR)\\chi=\\min(d_{L},d_{R})denotes the maximal entanglement capacity allowed by the bi-partition.",
                "position": 2971
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x35.png",
                "caption": "Figure 35:Normalized entanglement entropySA/log‚Å°(œá)S_{A}/\\log(\\chi)across training steps for several attention heads under different scaling coefficientsŒ±\\alphafor LLaMA-3.1-8B fine-tuned on Tulu3 dataset.",
                "position": 2974
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x36.png",
                "caption": "Figure 36:Entanglement entropySùí™S_{\\mathcal{O}}of the output operatorùí™=X‚ÄãX‚ä§\\mathcal{O}=XX^{\\top}across bi-partition positions and training steps for LLaMA-3.1-8B fine-tuned on Tulu3 dataset.",
                "position": 2981
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x37.png",
                "caption": "Figure 37:Artificial entanglement profiling ofŒî‚ÄãWQ\\Delta W_{Q}andŒî‚ÄãWV\\Delta W_{V}with respect to the cut position across training steps and scaling coefficientsŒ±‚àà{16,512}\\alpha\\in\\{16,512\\}for LLaMA-3.1-8B fine-tuned on Tulu3 dataset.",
                "position": 2988
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x38.png",
                "caption": "Figure 38:Final test loss of LoRA and MPS adaptation across multiple learning rates and scaling coefficientsŒ±\\alphafor LLaMA-3.1-8B fine-tuned on Tulu3 dataset.",
                "position": 2995
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x39.png",
                "caption": "Figure 39:Artificial entanglement profiling of the MPS adaptation ofŒî‚ÄãW\\Delta Wfor ranksr‚àà{1,2,4,8,16,32}r\\in\\{1,2,4,8,16,32\\}withŒ±=16\\alpha=16across training steps for LLaMA-3.1-8B fine-tuned on Tulu3 dataset.",
                "position": 2998
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x40.png",
                "caption": "Figure 40:Entanglement entropySAS_{A}of the attention matrix (Head 31) with respect to the bi-partition position across training steps for LLaMA-3.1-8B fine-tuned on OpenThoughts3 dataset.",
                "position": 3012
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x41.png",
                "caption": "Figure 41:Normalized entanglement entropySA/log‚Å°(œá)S_{A}/\\log(\\chi)across training steps for several attention heads under different scaling coefficientsŒ±\\alphafor LLaMA-3.1-8B fine-tuned on OpenThoughts3 dataset.",
                "position": 3015
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x42.png",
                "caption": "Figure 42:Artificial entanglement profiling ofŒî‚ÄãWQ\\Delta W_{Q}andŒî‚ÄãWV\\Delta W_{V}with respect to the cut position across training steps and scaling coefficientsŒ±‚àà{16,4096}\\alpha\\in\\{16,4096\\}for LLaMA-3.1-8B fine-tuned on OpenThoughts3 dataset.",
                "position": 3022
            },
            {
                "img": "https://arxiv.org/html/2601.06788/x43.png",
                "caption": "Figure 43:Final test loss of LoRA and MPS adaptation across multiple learning rates and scaling coefficientsŒ±\\alphafor LLaMA-3.1-8B fine-tuned on OpenThoughts3 dataset.",
                "position": 3029
            }
        ]
    },
    {
        "header": "XMore Experimental Results",
        "images": []
    }
]