[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.20025/figs/vis_rice.png",
                "caption": "(a)RICE (Ours)",
                "position": 79
            },
            {
                "img": "https://arxiv.org/html/2507.20025/figs/vis_rice.png",
                "caption": "(a)RICE (Ours)",
                "position": 82
            },
            {
                "img": "https://arxiv.org/html/2507.20025/figs/vis_dinov2.png",
                "caption": "(b)DINOv2",
                "position": 87
            },
            {
                "img": "https://arxiv.org/html/2507.20025/figs/vis_mlcd.png",
                "caption": "(c)MLCD",
                "position": 92
            },
            {
                "img": "https://arxiv.org/html/2507.20025/figs/vis_siglip.png",
                "caption": "(d)SigLIP",
                "position": 97
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.20025/x1.png",
                "caption": "Figure 2:Overview of our unified semantic region understanding framework. Our approach efficiently processes diverse semantic regions within the image using a single forward pass. The model jointly captures both general visual semantics (objects) and OCR semantics (texts), seamlessly integrating them into a unified representation.",
                "position": 143
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.20025/x2.png",
                "caption": "Figure 3:The region attention module processes batches of size-varying regions by leveraging a region-specific visibility mask and produces fixed-length region class embeddings. This approach enables efficient scaling of training while preserving representation fidelity.",
                "position": 191
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.20025/x3.png",
                "caption": "Figure 4:Token distance distributions observed during the training of MLCD and RICE models.",
                "position": 1036
            },
            {
                "img": "https://arxiv.org/html/2507.20025/figs/image_grid.png",
                "caption": "Figure 5:Tracking PCA. Using 2048-resolution images as input to a ViT-B/16 model, we project token features onto RGB channels via PCA to visualize the semantic structure. Sequential frames (arranged vertically) illustrate the evolution of model attention, consistently highlighting salient objects across time. The visualization reveals stable color patterns for tracked entities such as ice skaters, deers, motorcyclists, and cyclists, demonstrating the modelâ€™s ability to maintain semantic focus throughout the sequence.",
                "position": 1398
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]