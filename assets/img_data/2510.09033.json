[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09033/x1.png",
                "caption": "Figure 1:Illustration of three categories of knowledge. Associated hallucinations follow similar internal knowledge recall processes with factual associations, while unassociated hallucinations arise when the model’s output is detached from the input.",
                "position": 166
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09033/x2.png",
                "caption": "(a)Factual Associations",
                "position": 255
            },
            {
                "img": "https://arxiv.org/html/2510.09033/x2.png",
                "caption": "(a)Factual Associations",
                "position": 258
            },
            {
                "img": "https://arxiv.org/html/2510.09033/x3.png",
                "caption": "(b)Associated Hallucinations",
                "position": 263
            },
            {
                "img": "https://arxiv.org/html/2510.09033/x4.png",
                "caption": "(c)Unassociated Hallucinations",
                "position": 269
            }
        ]
    },
    {
        "header": "4Analysis of Internal States in LLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09033/x5.png",
                "caption": "Figure 3:Norm ratio curves of subject representations in LLaMA-3-8B, comparing AHs and UHs against FAs as the baseline.",
                "position": 373
            },
            {
                "img": "https://arxiv.org/html/2510.09033/x6.png",
                "caption": "Figure 4:Comparison of subspace overlap ratios.",
                "position": 391
            },
            {
                "img": "https://arxiv.org/html/2510.09033/x7.png",
                "caption": "Figure 5:Sample distribution across different subject popularity (low, mid, high) in LLaMA-3-8B, measured by monthly Wikipedia page views.",
                "position": 425
            },
            {
                "img": "https://arxiv.org/html/2510.09033/x8.png",
                "caption": "Figure 6:Subject-to-last attention contribution norms across layers in LLaMA-3-8B. Values show the norm of the attention contribution from subject tokens to the last token at each layer.",
                "position": 470
            },
            {
                "img": "https://arxiv.org/html/2510.09033/x9.png",
                "caption": "Figure 7:Cosine similarity of target-token hidden states across layers in LLaMA-3-8B.",
                "position": 489
            },
            {
                "img": "https://arxiv.org/html/2510.09033/x10.png",
                "caption": "Figure 8:t-SNE visualization of last token’s representations at layer 25 of LLaMA-3-8B.",
                "position": 494
            },
            {
                "img": "https://arxiv.org/html/2510.09033/x11.png",
                "caption": "Figure 9:Distribution of last token probabilities.",
                "position": 499
            }
        ]
    },
    {
        "header": "5Revisiting Hallucination Detection",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09033/x12.png",
                "caption": "Figure 10:Hallucination detection performance on theFullsetting (LLaMA-3-8B).",
                "position": 615
            }
        ]
    },
    {
        "header": "6Challenges of Refusal Tuning",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09033/x13.png",
                "caption": "Figure 11:Refusal tuning performance across three types of samples (LLaMA-3-8B).",
                "position": 648
            }
        ]
    },
    {
        "header": "7Conclusions and Future Work",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethical Considerations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ADatasets and Implementations",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09033/x14.png",
                "caption": "(a)Factual Associations",
                "position": 1622
            },
            {
                "img": "https://arxiv.org/html/2510.09033/x14.png",
                "caption": "(a)Factual Associations",
                "position": 1625
            },
            {
                "img": "https://arxiv.org/html/2510.09033/x15.png",
                "caption": "(b)Associated Hallucinations",
                "position": 1630
            },
            {
                "img": "https://arxiv.org/html/2510.09033/x16.png",
                "caption": "(c)Unassociated Hallucinations",
                "position": 1636
            },
            {
                "img": "https://arxiv.org/html/2510.09033/x17.png",
                "caption": "Figure 14:Norm ratio curves of subject representations in Mistral-7B-v0.3, comparing AHs and UHs against FAs as the baseline. At earlier layers, the norm of UH samples is significantly lower than that of AH samples.",
                "position": 1657
            },
            {
                "img": "https://arxiv.org/html/2510.09033/x18.png",
                "caption": "Figure 15:Sample distribution across different subject popularity (low, mid, high) in Mistral-7B-v0.3, measured by monthly Wikipedia page views.",
                "position": 1662
            },
            {
                "img": "https://arxiv.org/html/2510.09033/x19.png",
                "caption": "Figure 16:Subject-to-last attention contribution norms across layers in Mistral-7B-v0.3. Values show the norm of the attention contribution from subject tokens to the last token at each layer.",
                "position": 1678
            },
            {
                "img": "https://arxiv.org/html/2510.09033/x20.png",
                "caption": "Figure 17:Cosine similarity of target-token hidden states across layers in Mistral-7B-v0.3. From mid-layers onward, FAs and AHs diverge sharply as subject information propagates, while UHs remain more clustered, confirming weaker subject-dependent updates.",
                "position": 1683
            },
            {
                "img": "https://arxiv.org/html/2510.09033/x21.png",
                "caption": "Figure 18:t-SNE visualization of last token’s representations at layer 25 of Mistral-7B-v0.3.",
                "position": 1688
            },
            {
                "img": "https://arxiv.org/html/2510.09033/x22.png",
                "caption": "Figure 19:Hallucination detection performance on theFullsetting (Mistral-v0.3-7B).",
                "position": 1693
            },
            {
                "img": "https://arxiv.org/html/2510.09033/x23.png",
                "caption": "Figure 20:Refusal tuning performance across three\ntypes of samples (Mistral-v0.3-7B).",
                "position": 1699
            }
        ]
    },
    {
        "header": "Appendix BParallel Experiments on Mistral",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09033/x24.png",
                "caption": "Figure 21:t-SNE visualization of subject tokens’ representations at layer 11 of LLaMA-3-8B.",
                "position": 1717
            },
            {
                "img": "https://arxiv.org/html/2510.09033/x25.png",
                "caption": "Figure 22:t-SNE visualization of subject tokens’ representations at layer 11 of Mistral-7B-v0.3.",
                "position": 1722
            }
        ]
    },
    {
        "header": "Appendix CMore Visualization on Hidden States",
        "images": []
    }
]