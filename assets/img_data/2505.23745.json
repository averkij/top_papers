[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23745/x1.png",
                "caption": "Figure 1:(a) CLIP’s image and text embeddings are located in two completely separate regions of the embedding space. (b) The concept of \"dog\" and \"seaplane\" is more distinguishable in the image embedding space than in the text embedding space.",
                "position": 154
            },
            {
                "img": "https://arxiv.org/html/2505.23745/x2.png",
                "caption": "Figure 2:The proposed TrustVLM framework comprises three main steps. Initially, visual prototypes for each class are generated and stored using a pre-trained vision encoder. Subsequently, the VLMs perform zero-shot classification and yield an image-to-text similarity score,Si−tsubscript𝑆𝑖𝑡S_{i-t}italic_S start_POSTSUBSCRIPT italic_i - italic_t end_POSTSUBSCRIPT. In the third step, the initial prediction is verified using image-to-image similarity, providing an additional confidence score,Si−isubscript𝑆𝑖𝑖S_{i-i}italic_S start_POSTSUBSCRIPT italic_i - italic_i end_POSTSUBSCRIPT. Finally, these two scores are combined to determine the overall prediction confidence.",
                "position": 175
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23745/x3.png",
                "caption": "Figure 3:MisD performance on ImageNet and its variants using CLIP ViT-B/16, under distribution shifts defined by prototypes computed exclusively on ImageNet and deployed directly to variant datasets.",
                "position": 1350
            },
            {
                "img": "https://arxiv.org/html/2505.23745/x4.png",
                "caption": "Figure 4:Influence ofN𝑁Nitalic_Nin prototypes.",
                "position": 1848
            },
            {
                "img": "https://arxiv.org/html/2505.23745/x5.png",
                "caption": "Figure 5:Score distribution for correct and wrong predictions. Our TrustVLM achieves better separation between the score distributions of correct and wrong predictions, leading to improved performance in misclassification detection.",
                "position": 1858
            },
            {
                "img": "https://arxiv.org/html/2505.23745/x6.png",
                "caption": "Figure 6:Illustration of TrustVLM’s mechanism. Initially, the incorrect prediction receives a higher confidence scoreSi−tsubscript𝑆𝑖𝑡S_{i-t}italic_S start_POSTSUBSCRIPT italic_i - italic_t end_POSTSUBSCRIPTthan the correct one, indicating overconfidence. By performing verification in the image embedding space usingSi−isubscript𝑆𝑖𝑖S_{i-i}italic_S start_POSTSUBSCRIPT italic_i - italic_i end_POSTSUBSCRIPT, this overconfidence is mitigated. As a result, the final confidence scoreκ⁢(𝒙)𝜅𝒙\\kappa(\\bm{x})italic_κ ( bold_italic_x )is significantly higher for the correct prediction than for the incorrect one.",
                "position": 1862
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix ABroader Impact, Limitations, and Future Work",
        "images": []
    },
    {
        "header": "Appendix BMore Details on the Datasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23745/x7.png",
                "caption": "Figure 7:Representative examples from each dataset used in this work.",
                "position": 1893
            }
        ]
    },
    {
        "header": "Appendix CFurther Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23745/x8.png",
                "caption": "Figure 8:More illustration on TrustVLM’s mechanism.",
                "position": 1903
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]