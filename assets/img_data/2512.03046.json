[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03046/figs/v2_teaser.png",
                "caption": "",
                "position": 74
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03046/x1.png",
                "caption": "Figure 2:Overview of our Data Construction Pipeline. We start by synthesizing images depicting interactions. From these, we extract foreground objects, restore occluded items using a trained object completion LoRA, apply a series of augmentations (Relight, Perspective, Resolution), and finally composite the augmented object back into the scene to create the training data.",
                "position": 180
            },
            {
                "img": "https://arxiv.org/html/2512.03046/x2.png",
                "caption": "Figure 3:Overview of MagicQuill V2 Model Architecture.Our model processes layered visual cues and text instructions through distinct branches. The layered visual cues (left) are divided into a content layer (Lf​gL_{fg}) and multiple control layers (Lc​o​n​t​r​o​lL_{control}: spatial, structural, color), which are encoded into latents (ZyZ_{y},ZcZ_{c}). These are processed alongside text (ZtZ_{t}) and noisy image (ZxZ_{x}) latents in a unified control module (middle) adapted with dedicated control modules. A Causal Modulated Attention mechanism applies a bias matrix to the attention logits to precisely manage the influence and isolation of each control cue.",
                "position": 183
            },
            {
                "img": "https://arxiv.org/html/2512.03046/figs/V2_UI.png",
                "caption": "Figure 4:The interactive system interface of MagicQuill V2. (A) TheFill Brushin the toolbar allows users to define the spatial layer (maskMM) by painting on the canvas. (B) TheVisual Cue Managerholds content layer cues for drag-and-drop composition. (C) TheImage Segmentation Panel, triggered from the manager, enables precise cue extraction using SAM-based interactions.",
                "position": 318
            },
            {
                "img": "https://arxiv.org/html/2512.03046/x3.png",
                "caption": "Figure 5:Qualitative comparison of our content layer integration against state-of-the-art methods. Our model (MagicQuill V2) successfully integrates foreground objects by handling complex semantic interactions (rows 3, 4), harmonizing lighting (row 1), and correcting perspective distortions (rows 2, 5). These examples highlight our method’s ability to produce results that are significantly more realistic and faithful than Insert Anything[59], Nano Banana[16], Qwen-Image[1], and the specialized Kontext LoRA “Put it Here”[1].",
                "position": 321
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03046/x4.png",
                "caption": "Figure 6:Qualitative comparison for structural (edge) and color layer control. Our full model, by composing both cues, achieves high-precision edits, significantly outperforming baselines.",
                "position": 350
            },
            {
                "img": "https://arxiv.org/html/2512.03046/x5.png",
                "caption": "Figure 7:Qualitative ablation of our data construction pipeline (Sec.3.1). Removing Perspective, Relight, Resolution, or Object Completion augmentations leads to synthesis failures.",
                "position": 440
            },
            {
                "img": "https://arxiv.org/html/2512.03046/x6.png",
                "caption": "Figure 8:Qualitative analysis of the control strength parameterσ\\sigma. Asσ\\sigmaincreases, the model’s adherence to the user-provided edge layer (row 1) and color layer (row 2) progressively tightens.",
                "position": 526
            },
            {
                "img": "https://arxiv.org/html/2512.03046/x7.png",
                "caption": "Figure 9:Qualitative comparison for regional editing.",
                "position": 603
            },
            {
                "img": "https://arxiv.org/html/2512.03046/x8.png",
                "caption": "Figure 10:Qualitative comparison for object removal task.",
                "position": 606
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03046/x9.png",
                "caption": "Figure 11:Training data generation for the completion LoRA.",
                "position": 1677
            },
            {
                "img": "https://arxiv.org/html/2512.03046/x10.png",
                "caption": "Figure 12:Qualitative results of the completion LoRA.",
                "position": 1683
            },
            {
                "img": "https://arxiv.org/html/2512.03046/x11.png",
                "caption": "Figure 13:Overview of the relight augmentation pipeline. Completed objects are randomly relit using one of three light map categories to improve photometric robustness.",
                "position": 1689
            },
            {
                "img": "https://arxiv.org/html/2512.03046/x12.png",
                "caption": "Figure 14:Visual results of different edge/lineart extractors.",
                "position": 1710
            },
            {
                "img": "https://arxiv.org/html/2512.03046/x13.png",
                "caption": "Figure 15:Visualization of the data filtering strategy for the Spatial Layer. We calculate the area ratio of the convex hull covering the changed regions between the source and target images. The pipeline automatically rejects failed edits with no significant changes (Row 1) or excessive global changes (Row 2), retaining only high-quality local editing pairs (Rows 3 & 4) for training.",
                "position": 1751
            }
        ]
    },
    {
        "header": "7User Study for Content Layer",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03046/figs/appendix/user_study_pie.png",
                "caption": "Figure 16:User Preference Distribution.MagicQuill V2 (Ours) dominates with68.5%of the votes, significantly outperforming the strongest baseline, Nano Banana (15.8%).",
                "position": 1765
            }
        ]
    },
    {
        "header": "8Limitation",
        "images": []
    },
    {
        "header": "9Open Source Commitment",
        "images": []
    }
]