[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.08787/x1.png",
                "caption": "Figure 1:UniversalSkillrepresentations (UniSkill) are cross-embodiment skill representations shared across various embodiments (e.g., humans, Franka, WidowX) trained from both human and robot videos via skill dynamics modeling. Unlike prior works that require additional supervision (e.g., trajectory labels) or alignment between human and robot videos, UniSkill removes these constraints by learning solely from off-the-shelf video datasets‚Äìsuch as Something-Something V2[11]and H2O[12]for human videos, and DROID[13], Bridge V2[14], and LIBERO[15]for robot videos. UniSkill, trained on large-scale cross-embodiment videos, learns an embodiment-agnostic skill representation that enables interpreting human videos as skill sequences executable directly through a skill-conditioned policy.",
                "position": 200
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.08787/x2.png",
                "caption": "Figure 2:The overview of UniSkill. (a)¬†Inverse Skill Dynamics (ISD) and Forward Skill Dynamics (FSD) are jointly trained on diverse video datasets to encode dynamics information into universal skill representations by predicting skills and future frames, respectively. (b)¬†A universal skill-conditioned policy is trained on DROID and small target environment data. Here, skill representations are extracted from robot data using the pretrained ISD. (c)¬†Skills extracted from a human video prompt are sequentially executed by the skill-conditioned policy to reproduce the target behavior.",
                "position": 251
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.08787/x3.png",
                "caption": "Figure 3:Overview of our tabletop experiments. (a) Average results on the tabletop benchmark using Franka and human prompts. (b) Results on skill composition using Franka and human prompts.A:Open the trash bin,B:Pull out the tissue,C:Pick the blue towel and place it in the bowl,D:Close the trash bin. (c) Results from human prompts evaluated on unseen environments in (d).",
                "position": 398
            },
            {
                "img": "https://arxiv.org/html/2505.08787/x4.png",
                "caption": "Figure 4:Results on the Kitchen benchmark\nusing Franka, Human, and Anubis (a different robot embodiment) prompts.",
                "position": 412
            },
            {
                "img": "https://arxiv.org/html/2505.08787/x5.png",
                "caption": "Figure 5:Average success rates for the LIBERO benchmark withunseen human prompts (bottom). In human prompt videos, a human directly manipulates objects in a real-world environment similar to the LIBERO environment.",
                "position": 441
            },
            {
                "img": "https://arxiv.org/html/2505.08787/x6.png",
                "caption": "Figure 6:Qualitative results from FSD. The skill representation is extracted using ISD from each video prompt and conditioned on FSD to predict the future frame. (Left) Skills are extracted from two images using ISD. (Right) The predicted image generated by passing the current image and the extracted skill through FSD. Best viewed in color.",
                "position": 497
            },
            {
                "img": "https://arxiv.org/html/2505.08787/x7.png",
                "caption": "Figure 7:t-SNE visualization of UniSkill embeddings on the XSkill datasets. Circle markers indicate skill embeddings from human prompts, while cross markers represent those from robot prompts. Each color denotes a different skill. Example frames for each skill are shown inSectionA.6.2.",
                "position": 517
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.08787/x8.png",
                "caption": "Figure 8:We created a prompt video in which a human directly manipulates objects after arranging them in a real-world environment similar to the LIBERO task. Here, we visualize only 2 out of the 8 tasks here for clarity.",
                "position": 1314
            },
            {
                "img": "https://arxiv.org/html/2505.08787/x9.png",
                "caption": "Figure 9:Comparison of the inference pipelines for GCBC, GCBC-U and UniSkill. All three methods use the same frame intervalkùëòkitalic_k. GCBC uses theIt+ksubscriptùêºùë°ùëòI_{t+k}italic_I start_POSTSUBSCRIPT italic_t + italic_k end_POSTSUBSCRIPTframes as the sub-goal and predictions the actions required to achieve that state. In contrast, GCBC-U employs ISD and FSD to predict the sub-goal based on the current observation. UniSkill is directly conditioned on the skill representation from ISD rather than relying on a pixel-level goal condition.",
                "position": 1449
            },
            {
                "img": "https://arxiv.org/html/2505.08787/x10.png",
                "caption": "Figure 10:Comparison with Uniskill FSD and LAPA. A skill (UniSkill) or latent action (LAPA) was extracted between two frames, and the next frame was generated conditioned on the resulting vector. UniSkill FSD successfully reconstructs the video dynamics, while LAPA produces blurry images. (a): Result on DROID, (b): Result on XSkill.",
                "position": 1474
            },
            {
                "img": "https://arxiv.org/html/2505.08787/x11.png",
                "caption": "Figure 11:Ablation studies on camera speeds using human prompts on the LIBERO benchmark. Each success rate represents the average across all tasks in the benchmark.",
                "position": 1888
            },
            {
                "img": "https://arxiv.org/html/2505.08787/x12.png",
                "caption": "Figure 12:t-SNE visualization of UniSkill embeddings with and without depth on the XSkill dataset. Circle markers represent skill embeddings from human prompts, while cross markers represent those from robot prompts. Each color corresponds to a different task, with visual examples shown above for both human and robot executions.",
                "position": 1968
            },
            {
                "img": "https://arxiv.org/html/2505.08787/x13.png",
                "caption": "Figure 13:Visualization of the test environment and success rates across different initial towel positions. The towel‚Äôs position is shifted up to¬±12‚Å¢c‚Å¢mplus-or-minus12cm\\pm 12\\mathrm{cm}¬± 12 roman_c roman_mfrom the prompt location to evaluate UniSkill‚Äôs spatial sensitivity.",
                "position": 1983
            }
        ]
    },
    {
        "header": "Appendix BExperiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.08787/x14.png",
                "caption": "Figure 14:Our experiments are conducted in the DROID[13]environment.",
                "position": 2026
            },
            {
                "img": "https://arxiv.org/html/2505.08787/x15.png",
                "caption": "Figure 15:We designed five tasks within a real-world scene. The tasks were designed to share similar trajectories or involve the same objects across different tasks. This task setup allows for the evaluation of various actions, including push, pull, and pick-and-place.",
                "position": 2057
            },
            {
                "img": "https://arxiv.org/html/2505.08787/x16.png",
                "caption": "Figure 16:Unseen environments of tabletop benchmark used for skill robustness evaluation.",
                "position": 2116
            },
            {
                "img": "https://arxiv.org/html/2505.08787/x17.png",
                "caption": "Figure 17:Prompt videos with different embodiments. To evaluate cross-embodiment imitation using UniSkill, we record prompt videos with 3 different embodiments. Prompt videos are recorded using a Anubis Robot (Top row), Franka arm (middle row) and a human hand (bottom row).",
                "position": 2127
            }
        ]
    },
    {
        "header": "Appendix CImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.08787/x18.png",
                "caption": "Figure 18:Analysis of failure cases for UniSkill on the tabletop tasksPull out the tissueandPush the blue towel. In these cases, the primary failure mode is inaccurate contact with the target object.",
                "position": 2614
            }
        ]
    },
    {
        "header": "Appendix DFailure Cases",
        "images": []
    }
]