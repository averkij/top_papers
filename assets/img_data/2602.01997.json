[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background & Related Work",
        "images": []
    },
    {
        "header": "3Classification vs. Generative Benchmarks",
        "images": []
    },
    {
        "header": "4Layer-by-Layer Pruning for Generative Tasks",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01997/x1.png",
                "caption": "Figure 1:Effect of removing a single layer on model performance across generative benchmarks. Reasoning-intensive tasks such as GSM8K and HumanEval+exhibit severe performance degradation at specific layers, while XSUM remains comparatively robust except for layers whose removal induces general text degeneration. (We generally skip layer 0 for its poor results.)",
                "position": 253
            },
            {
                "img": "https://arxiv.org/html/2602.01997/x2.png",
                "caption": "Figure 2:Text degeneration under single-layer pruning, measured using 4-gram repetition (left) and Self-BLEU4 averaged across responses and normalized relative to the baseline.",
                "position": 271
            },
            {
                "img": "https://arxiv.org/html/2602.01997/x3.png",
                "caption": "Figure 3:Effect of single-layer pruning on the arithmetic ability of Llama.",
                "position": 290
            },
            {
                "img": "https://arxiv.org/html/2602.01997/x4.png",
                "caption": "Figure 4:Distribution of syntactic error types induced by single-layer pruning.",
                "position": 312
            }
        ]
    },
    {
        "header": "5Finetuning with Self-Generated Responses",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01997/x5.png",
                "caption": "Figure 5:Perplexity curves during training for both standard finetuning and for SGR for the Llama model (BI pruned: 25%) Results for other models inA.7.",
                "position": 340
            },
            {
                "img": "https://arxiv.org/html/2602.01997/x6.png",
                "caption": "Figure 6:Average accuracy on generative tasks for Qwen and Llama across three pruning strategies at varying pruning ratios. Classification performance at 25% pruning shows the substantial gap between classification and generative task recovery.",
                "position": 785
            }
        ]
    },
    {
        "header": "6Post-Recovery Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01997/x7.png",
                "caption": "Figure 7:Accuracy on an arithmetic task for baseline models, after pruning, and after pruning followed by finetuning. Results are shown at a 25% pruning ratio (Qwen: Iterative, Mistral: BI, Llama: BI).",
                "position": 809
            },
            {
                "img": "https://arxiv.org/html/2602.01997/x8.png",
                "caption": "Figure 8:Code evaluation outcomes across model families on MBPP+and HumanEval+. Green and blue denote syntactically valid code, with green passing all tests and blue failing assertions; remaining categories correspond to invalid code due to syntax or execution errors.",
                "position": 829
            }
        ]
    },
    {
        "header": "7Discussion & Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01997/x9.png",
                "caption": "Figure 9:Text degeneration results with layer pruning using N-gram repetition (left) and Self-BLEU4 score relative to baseline.",
                "position": 1581
            },
            {
                "img": "https://arxiv.org/html/2602.01997/x10.png",
                "caption": "(a)Qwen",
                "position": 1632
            },
            {
                "img": "https://arxiv.org/html/2602.01997/x10.png",
                "caption": "(a)Qwen",
                "position": 1635
            },
            {
                "img": "https://arxiv.org/html/2602.01997/x11.png",
                "caption": "(b)Mistral",
                "position": 1640
            },
            {
                "img": "https://arxiv.org/html/2602.01997/x12.png",
                "caption": "Figure 11:Comparison between full supervised finetuning (Full-FT) and QLoRA on GSM8K.",
                "position": 1938
            },
            {
                "img": "https://arxiv.org/html/2602.01997/x13.png",
                "caption": "Figure 12:Comparison between full supervised finetuning (Full-FT) and QLoRA on self-generated Dolci data.",
                "position": 1954
            },
            {
                "img": "https://arxiv.org/html/2602.01997/x14.png",
                "caption": "(a)Qwen",
                "position": 1971
            },
            {
                "img": "https://arxiv.org/html/2602.01997/x14.png",
                "caption": "(a)Qwen",
                "position": 1974
            },
            {
                "img": "https://arxiv.org/html/2602.01997/x15.png",
                "caption": "(b)Mistral",
                "position": 1979
            },
            {
                "img": "https://arxiv.org/html/2602.01997/x16.png",
                "caption": "Figure 14:Differences between finetuning with Self-Generated Responses (SGR) vs on Dolci dataset directly for the Qwen Model. At all pruning ratios, SGR is consistently better than the raw dataset.",
                "position": 2976
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]