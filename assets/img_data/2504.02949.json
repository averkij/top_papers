[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02949/x1.png",
                "caption": "Figure 1:A comparative analysis of various MLLMs across multiple visual comprehension benchmarks is presented. The remaining metrics are derived from standard visual question-answering benchmarks and multi-modal comprehension benchmarks. Notably, our VARGPT-v1.1 model demonstrates significant superiority over the compared baselines across all comprehension benchmarks.",
                "position": 80
            },
            {
                "img": "https://arxiv.org/html/2504.02949/x2.png",
                "caption": "Figure 2:Comparison of different model architectures, where, ‘AR’ denotes autoregressive, while ‘VAR’ signifies visual autoregressive. We present a comparative analysis of architectures designed for comprehension-only tasks, generation-only tasks, and unified comprehension and generation, alongside our proposed VARGPT-v1.1 an VARGPT[14]model. Our VARGPT-v1.1 and VARGPT are conceptualized as purely autoregressive multimodel model, achieving visual comprehension through next-token prediction and visual generation through next-scale prediction paradigms.",
                "position": 92
            },
            {
                "img": "https://arxiv.org/html/2504.02949/x3.png",
                "caption": "Figure 3:The illustration of the proposed VARGPT-v1.1 framework similar to VARGPT[107], which consists of (1) an LLM (Qwen2-7B-Instruct[87,2]), visual encoder and a understanding projector for visual understanding; (2) a visual decoder and dual generation projectors for visual generation. VARGPT-v1.1 employs causal attention in the LLM backbone while utilizing block causal attention in the visual decoder.",
                "position": 129
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02949/x4.png",
                "caption": "Figure 4:The three training stages of the VARGPT, including stage-1 pretraining, stage-2 visual instruction tuning and stage-3 iterative training.",
                "position": 154
            },
            {
                "img": "https://arxiv.org/html/2504.02949/x5.png",
                "caption": "Figure 5:The proposed iterative training strategy for the third stage gradually increases the resolution of the image, while using instruction fine-tuning and reinforcement learning iterative training. Finally, we use the instruction-follow dataset for image editing to stimulate the model’s visual editing ability.",
                "position": 157
            }
        ]
    },
    {
        "header": "4Instruction-following and Preference Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02949/x6.png",
                "caption": "Figure 6:We present the data distribution we construct and collect, encompassing the proportional breakdown of data across the three training stages. Our composite dataset for stage-2 training is derived from LLaVA-1.5, LLaVA-OneVision.",
                "position": 285
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02949/x7.png",
                "caption": "Figure 7:Some generated 512×\\times×512 samples by VARGPT-v1.1. Our VARGPT-v1.1 supports text-and-image instructions from user and outputs both text-and-image mixed modal data simultaneously.",
                "position": 934
            },
            {
                "img": "https://arxiv.org/html/2504.02949/x8.png",
                "caption": "Figure 8:Some visual display of image editing results from VARGPT-v1.1, with all images having a resolution of 512×\\times×512.",
                "position": 937
            },
            {
                "img": "https://arxiv.org/html/2504.02949/extracted/6334307/sec/Figure/Exp/15_image.png",
                "caption": "Table 6:VARGPT demonstrates the capability to comprehend and interpret humorous elements within visual content.",
                "position": 940
            },
            {
                "img": "https://arxiv.org/html/2504.02949/extracted/6334307/sec/Figure/Exp/15_image.png",
                "caption": "Table 6:VARGPT demonstrates the capability to comprehend and interpret humorous elements within visual content.",
                "position": 941
            }
        ]
    },
    {
        "header": "6Conclusion, Limitation and Future Work",
        "images": []
    },
    {
        "header": "7Qualitative Study for Understanding",
        "images": []
    },
    {
        "header": "8Qualitative Study for Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02949/extracted/6334307/sec/Figure/Appendix/understanding/22_image.png",
                "caption": "Table 7:The case of visual understanding in VARGPT-v1.1 shows that our VARGPT-v1.1 has achieved better understanding performance.",
                "position": 1105
            },
            {
                "img": "https://arxiv.org/html/2504.02949/extracted/6334307/sec/Figure/Appendix/understanding/22_image.png",
                "caption": "Table 7:The case of visual understanding in VARGPT-v1.1 shows that our VARGPT-v1.1 has achieved better understanding performance.",
                "position": 1106
            },
            {
                "img": "https://arxiv.org/html/2504.02949/extracted/6334307/sec/Figure/Appendix/understanding/2_image.png",
                "caption": "Table 8:The case of visual understanding in VARGPT-v1.1 shows that our VARGPT-v1.1 has achieved better understanding performance.",
                "position": 1165
            },
            {
                "img": "https://arxiv.org/html/2504.02949/extracted/6334307/sec/Figure/Appendix/understanding/2_image.png",
                "caption": "Table 8:The case of visual understanding in VARGPT-v1.1 shows that our VARGPT-v1.1 has achieved better understanding performance.",
                "position": 1166
            },
            {
                "img": "https://arxiv.org/html/2504.02949/extracted/6334307/sec/Figure/Appendix/understanding/10_image.png",
                "caption": "Table 9:The case of visual understanding in VARGPT-v1.1 shows that our VARGPT-v1.1 has achieved better understanding performance.",
                "position": 1225
            },
            {
                "img": "https://arxiv.org/html/2504.02949/extracted/6334307/sec/Figure/Appendix/understanding/10_image.png",
                "caption": "Table 9:The case of visual understanding in VARGPT-v1.1 shows that our VARGPT-v1.1 has achieved better understanding performance.",
                "position": 1226
            },
            {
                "img": "https://arxiv.org/html/2504.02949/extracted/6334307/sec/Figure/Appendix/understanding/24_image.png",
                "caption": "Table 10:The case of visual understanding in VARGPT-v1.1 shows that our VARGPT-v1.1 has achieved better understanding performance.",
                "position": 1285
            },
            {
                "img": "https://arxiv.org/html/2504.02949/extracted/6334307/sec/Figure/Appendix/understanding/24_image.png",
                "caption": "Table 10:The case of visual understanding in VARGPT-v1.1 shows that our VARGPT-v1.1 has achieved better understanding performance.",
                "position": 1286
            },
            {
                "img": "https://arxiv.org/html/2504.02949/extracted/6334307/sec/Figure/Appendix/understanding/52_image.png",
                "caption": "Table 11:The case of visual understanding in VARGPT-v1.1 shows that our VARGPT-v1.1 has achieved better understanding performance.",
                "position": 1345
            },
            {
                "img": "https://arxiv.org/html/2504.02949/extracted/6334307/sec/Figure/Appendix/understanding/52_image.png",
                "caption": "Table 11:The case of visual understanding in VARGPT-v1.1 shows that our VARGPT-v1.1 has achieved better understanding performance.",
                "position": 1346
            },
            {
                "img": "https://arxiv.org/html/2504.02949/x9.png",
                "caption": "Figure 9:Some generated 512×\\times×512 samples by VARGPT-v1.1. Our VARGPT-v1.1 supports text-and-image instructions from user and outputs both text-and-image mixed modal data simultaneously.",
                "position": 1405
            },
            {
                "img": "https://arxiv.org/html/2504.02949/x10.png",
                "caption": "Figure 10:Some visual display of image editing results from VARGPT-v1.1, with all images having a resolution of 512×\\times×512.",
                "position": 1408
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]