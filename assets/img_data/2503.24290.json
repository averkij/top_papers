[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.24290/x1.png",
                "caption": "Figure 1:Evaluation performance of Open-Reasoner-Zero-{7B, 32B} on benchmarks (averaged on 16 responses) during training.\nUsing the same base model as DeepSeek-R1-Zero-Qwen-32B, Open-Reasoner-Zero-32B achieves superior performance on AIME2024, MATH500, and GPQA Diamond benchmark-requiring only a tenth of the training steps.",
                "position": 138
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.24290/x2.png",
                "caption": "Figure 2:Train-time Scale up on Train Reward and Response Length of Open-Reasoner-Zero (ORZ) - {0.5B, 1.5B, 7B, 32B}.\nTrain Reward and Response Length increase steadily, demonstrating consistent scalability across model sizes.\nInterestingly, the ORZ-32B Response Length exhibits fluctuations without negatively impacting training stability, highlighting the robustness of our minimalist recipe.",
                "position": 230
            }
        ]
    },
    {
        "header": "2Scale-up Reinforcement Learning from a Base Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.24290/x3.png",
                "caption": "Figure 4:Percentage of responses following the reasoning format.\nResults demonstrate rapid adoption of structured reasoning patterns even by the base model using only a simple rule-based reward function.\nOur findings suggest that complicated reward functions are unnecessary for training Reasoner-Zero models.",
                "position": 411
            },
            {
                "img": "https://arxiv.org/html/2503.24290/x4.png",
                "caption": "Figure 5:Comparison of training and evaluation reward and average response length for the Open-Reasoner-Zero 7B model. All of benchmarks experience a sudden increase in reward and response length at a certain point, a phenomenon like emergent behavior.",
                "position": 458
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.24290/x5.png",
                "caption": "Figure 6:Reflection patterns in generated responses. The Average Correct Reflection Length consistently exceeds the Average Response Length throughout the training process. A particularly noteworthy phenomenon emerges around step 680, where we observe a simultaneous acceleration in three metrics: Reward in training set, Average Correct Reflection Length, and Average Response Length.",
                "position": 519
            },
            {
                "img": "https://arxiv.org/html/2503.24290/x6.png",
                "caption": "Figure 7:Comparison of different GAEλ𝜆\\lambdaitalic_λvalues. GAEλ=1.0𝜆1.0\\lambda=1.0italic_λ = 1.0shows better stability and performance compared toλ=0.95𝜆0.95\\lambda=0.95italic_λ = 0.95for both training reward and response length.",
                "position": 540
            },
            {
                "img": "https://arxiv.org/html/2503.24290/x7.png",
                "caption": "Figure 8:Comparisons to applying KL-related regularizations.\nNotably, training without KL constraints demonstrates superior average benchmark performance and length scaling property, compared to models trained with KL Loss and KL Penalty. Performance is evaluated on MATH500, AIME2024, and GPQA DIAMOND benchmarks using pass@1 metric.",
                "position": 544
            },
            {
                "img": "https://arxiv.org/html/2503.24290/x8.png",
                "caption": "Figure 9:Data scale ablation study. Training data from math train 7.5k to Open-Reasoner-Zero 57k, we observe a consistent increase in both training reward and response length for training and evaluation set, indicating that data scale plays a crucial role in training performance. Performance is evaluated on MATH500 benchmark using pass@1 metric.",
                "position": 550
            },
            {
                "img": "https://arxiv.org/html/2503.24290/x9.png",
                "caption": "Figure 10:Evaluation performance of Open-Reasoner-Zero-{0.5B, 1.5B}.\nWe report the average accuracy on the benchmark dataset for each question with 16 responses.",
                "position": 692
            }
        ]
    },
    {
        "header": "4Conclusion and Discussions",
        "images": []
    },
    {
        "header": "5Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.24290/x10.png",
                "caption": "Figure 11:Data Curation Ablation Study. CN represents Chinese data and EN represents English data. Our results demonstrate that the English-only dataset yields superior training stability and final model performance.",
                "position": 1060
            },
            {
                "img": "https://arxiv.org/html/2503.24290/x11.png",
                "caption": "Figure 12:Comparison of different Prompt, Rollout, Batch Size combinations.\nU.S. represents Update steps of model parameters in each generation steps.\nOn policy update setting performs better than off policy counterpart on both training reward and response length.",
                "position": 1065
            },
            {
                "img": "https://arxiv.org/html/2503.24290/x12.png",
                "caption": "Figure 13:Comparison of different KL Loss, KL Penalty, and GAEλ𝜆\\lambdaitalic_λvalues.",
                "position": 1080
            },
            {
                "img": "https://arxiv.org/html/2503.24290/x13.png",
                "caption": "Figure 14:Comparison of different sampling strategies. T represents temperature and topp represents top-p sampling.",
                "position": 1090
            }
        ]
    },
    {
        "header": "Appendix BMore Evaluation Results",
        "images": []
    }
]