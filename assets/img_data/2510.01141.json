[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01141/Apriel.png",
                "caption": "",
                "position": 95
            },
            {
                "img": "https://arxiv.org/html/2510.01141/x1.png",
                "caption": "Figure 1:Apriel-1.5-15B-Thinker compared to the best open source LLMs on the Artificial Analysis Intelligence Index.",
                "position": 97
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Architecture and Model Upscaling",
        "images": []
    },
    {
        "header": "3Continual Pretraining (CPT)",
        "images": []
    },
    {
        "header": "4Supervised Fine Tuning (SFT)",
        "images": []
    },
    {
        "header": "5Evaluation Methodology",
        "images": []
    },
    {
        "header": "6Results and Observations",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01141/x2.png",
                "caption": "(a)Apriel-1.5-15B-Thinker compared with state-of-the-art LLMs.",
                "position": 583
            },
            {
                "img": "https://arxiv.org/html/2510.01141/x2.png",
                "caption": "(a)Apriel-1.5-15B-Thinker compared with state-of-the-art LLMs.",
                "position": 586
            },
            {
                "img": "https://arxiv.org/html/2510.01141/x3.png",
                "caption": "(b)Apriel-1.5-15B-Thinker compared with SOTA open-source models.",
                "position": 592
            },
            {
                "img": "https://arxiv.org/html/2510.01141/x4.png",
                "caption": "Figure 3:Artificial Analysis Intelligence Index vs. Total Parameters (log scale).\nApriel-1.5-15B-Thinker lies in the “most attractive quadrant.”",
                "position": 599
            },
            {
                "img": "https://arxiv.org/html/2510.01141/x5.png",
                "caption": "Table 3:Evaluation (pass@1 or accuracy) on benchmarks withmaximum reasoning, as applicable: MMLU-Pro, GPQA Diamond, HLE, LiveCodeBench, SciCode, AIME2025, IF-Bench, AA-LCR, TerminalBench-Hard, andτ2\\tau^{2}-2Bench. Orange = proprietary models, blue =>>50B open-weight models, green =<<50B open-weight models.",
                "position": 621
            },
            {
                "img": "https://arxiv.org/html/2510.01141/x5.png",
                "caption": "Figure 4:Average performance across the benchmark suite (higher is better). The chart aggregates scores from MMMU[18], MMMU-Pro[28], LogicVista[21], MathVision[22], MathVista[23], MathVerse[24], MMStar[20], CharXiv[25], AI2D[26], and BLINK[27].",
                "position": 1166
            }
        ]
    },
    {
        "header": "7Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "8Contributions and Acknowledgments",
        "images": []
    }
]