[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IINTRODUCTION",
        "images": []
    },
    {
        "header": "IIRELATED WORK",
        "images": []
    },
    {
        "header": "IIIFuSe Finetuning",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04693/x1.png",
                "caption": "Figure 2:Architecture:We finetune pre-trained generalist robot policies by tokenizing all heteregoneous sensing modalities and passing them though a pre-trained transformer backbone. Crucially, we apply two auxiliary losses that help connect the semantic knowledge of pre-trained generalist policies with new heterogeneous modalities, such as touch and audio. Concretely, we apply both a contrastive loss that aims to maximize mutual information between different views and semantics of the same scene, and a language generation loss that predicts high-level semantics for each modality combination.",
                "position": 152
            }
        ]
    },
    {
        "header": "IVExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04693/x2.png",
                "caption": "Figure 3:Visualization of the various sensor modalities on our WidowX robot.",
                "position": 209
            },
            {
                "img": "https://arxiv.org/html/2501.04693/extracted/6132414/figures/test_objects.jpg",
                "caption": "(a)Objects used for evaluation purposes.",
                "position": 256
            },
            {
                "img": "https://arxiv.org/html/2501.04693/extracted/6132414/figures/test_objects.jpg",
                "caption": "(a)Objects used for evaluation purposes.",
                "position": 259
            },
            {
                "img": "https://arxiv.org/html/2501.04693/extracted/6132414/figures/train_objects.jpg",
                "caption": "(b)Objects included in the training data.",
                "position": 264
            },
            {
                "img": "https://arxiv.org/html/2501.04693/x3.png",
                "caption": "Figure 5:FuSe performance on evaluation tasks compared against baselines. Our approach outperforms baselines trained from scratch or finetuned with vision only, especially on the shopping bag task, which presents partially observable visual scenarios. Lighter shades of color represent intermediate task success, i.e., object touched but not fully grasped.",
                "position": 274
            },
            {
                "img": "https://arxiv.org/html/2501.04693/x4.png",
                "caption": "",
                "position": 278
            },
            {
                "img": "https://arxiv.org/html/2501.04693/x5.png",
                "caption": "",
                "position": 279
            },
            {
                "img": "https://arxiv.org/html/2501.04693/x6.png",
                "caption": "",
                "position": 280
            },
            {
                "img": "https://arxiv.org/html/2501.04693/x7.png",
                "caption": "",
                "position": 281
            },
            {
                "img": "https://arxiv.org/html/2501.04693/x8.png",
                "caption": "Figure 6:Results on the compositional tasks devised in the button pressing environment. On the left, the instructions are of the type “pick theobjectthat has the same color as the button that playpiano”. On the right, the whole multi-step task is represented by an instruction like “press the train button that plays the same sound as thebluebutton”.",
                "position": 440
            },
            {
                "img": "https://arxiv.org/html/2501.04693/x9.png",
                "caption": "",
                "position": 443
            },
            {
                "img": "https://arxiv.org/html/2501.04693/x10.png",
                "caption": "",
                "position": 445
            },
            {
                "img": "https://arxiv.org/html/2501.04693/x11.png",
                "caption": "Figure 7:We study the effect of the proposed losses in an ablation experiment in the shopping bag environment. Our model that includes both contrastive and language generative losses outperforms models trained with only one of the two auxiliary losses or neither.",
                "position": 449
            },
            {
                "img": "https://arxiv.org/html/2501.04693/x12.png",
                "caption": "Figure 8:Performance of a PaliGemma FuSe 3B parameter VLA, trained on our multimodal dataset. Our policy achieves robust performance, showcasing the applicability of FuSe to widely different generalist policies.",
                "position": 475
            },
            {
                "img": "https://arxiv.org/html/2501.04693/x13.png",
                "caption": "",
                "position": 478
            },
            {
                "img": "https://arxiv.org/html/2501.04693/x14.png",
                "caption": "",
                "position": 479
            }
        ]
    },
    {
        "header": "VCONCLUSIONS",
        "images": []
    },
    {
        "header": "ACKNOWLEDGMENT",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]