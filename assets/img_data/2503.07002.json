[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07002/x1.png",
                "caption": "Figure 1:Multi-turn multimodal dialogue:(a) Saliency tracking.The MLLM needs to focus on both the red triangle agent and the purple key, which scatter on the image, to answer the question correctly.(b) Saliency recall.The MLLM needs to retain focus on the region where the agent will stop after the last question.",
                "position": 111
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3MMDiag: A New Benchmark for Multi-Turn Multimodal Dialogue",
        "images": []
    },
    {
        "header": "4DiagNote",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07002/x2.png",
                "caption": "Figure 2:Model architecture of DiagNote. Regions with blue backgrounds represent a deliberation step and the interaction between the Deliberate and Gaze modules. At each turn, the Deliberate module processes the original image, dialogue context, and buffers from both modules. It produces two outputs: (1) a Deliberate step, stored in the Deliberate buffer, and (2) a Gaze query, which is processed by the Gaze module. The resulting bounding boxes are then stored in the Gaze buffer.",
                "position": 339
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07002/x3.png",
                "caption": "Figure 3:Comparison for an example of the Minigrid scenario, one of the subsets in MMDiag.\nWe give DiagNote (green) and GPT-4o (orange) the same environmental description and question. DiagNote focuses on the key regions and gives the correct reasoning process and the final answer. In contrast, GPT-4o fails to locate the object and thus gives the wrong answer.\nExamples for the MMDiag subsets of everyday scenarios and tabular scenes can be found inSectionF.",
                "position": 792
            },
            {
                "img": "https://arxiv.org/html/2503.07002/extracted/6266231/arxiv_submission/main_image/case_study/Model.jpg",
                "caption": "(a)DiagNote",
                "position": 818
            },
            {
                "img": "https://arxiv.org/html/2503.07002/extracted/6266231/arxiv_submission/main_image/case_study/Model.jpg",
                "caption": "(a)DiagNote",
                "position": 821
            },
            {
                "img": "https://arxiv.org/html/2503.07002/extracted/6266231/arxiv_submission/main_image/case_study/DINO.png",
                "caption": "(b)Grounding DINO",
                "position": 827
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "ADataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07002/x4.png",
                "caption": "Figure 5:The first example prompt for generating data samples in everyday scenes.",
                "position": 1628
            },
            {
                "img": "https://arxiv.org/html/2503.07002/x5.png",
                "caption": "Figure 6:The second example prompt for generating data samples in everyday scenes.",
                "position": 1631
            },
            {
                "img": "https://arxiv.org/html/2503.07002/x6.png",
                "caption": "Figure 7:The third example prompt for generating data samples in everyday scenes.",
                "position": 1634
            },
            {
                "img": "https://arxiv.org/html/2503.07002/x7.png",
                "caption": "Figure 8:The prompt structure to generate samples in tabular scenes.",
                "position": 1643
            },
            {
                "img": "https://arxiv.org/html/2503.07002/x8.png",
                "caption": "Figure 9:The question-answer (QA) and Chain-of-Thought (CoT) examples for line charts.",
                "position": 1646
            },
            {
                "img": "https://arxiv.org/html/2503.07002/x9.png",
                "caption": "Figure 10:The question-answer (QA) and Chain-of-Thought (CoT) examples for pie charts.",
                "position": 1649
            },
            {
                "img": "https://arxiv.org/html/2503.07002/x10.png",
                "caption": "Figure 11:The question-answer (QA) and Chain-of-Thought (CoT) examples for bar charts.",
                "position": 1652
            },
            {
                "img": "https://arxiv.org/html/2503.07002/x11.png",
                "caption": "Figure 12:The mission and plan input example of Minigrid settings.",
                "position": 1663
            },
            {
                "img": "https://arxiv.org/html/2503.07002/x12.png",
                "caption": "Figure 13:The prompt structure to generate data samples in Minigrid settings.",
                "position": 1666
            },
            {
                "img": "https://arxiv.org/html/2503.07002/extracted/6266231/arxiv_submission/supp_image/dataset/data_format/data_format_vg.jpg",
                "caption": "(a)the original image",
                "position": 1673
            },
            {
                "img": "https://arxiv.org/html/2503.07002/extracted/6266231/arxiv_submission/supp_image/dataset/data_format/data_format_vg.jpg",
                "caption": "(a)the original image",
                "position": 1676
            },
            {
                "img": "https://arxiv.org/html/2503.07002/x13.png",
                "caption": "(b)the sample format",
                "position": 1682
            },
            {
                "img": "https://arxiv.org/html/2503.07002/extracted/6266231/arxiv_submission/supp_image/dataset/data_format/data_format_chart.png",
                "caption": "(a)the original image",
                "position": 1689
            },
            {
                "img": "https://arxiv.org/html/2503.07002/extracted/6266231/arxiv_submission/supp_image/dataset/data_format/data_format_chart.png",
                "caption": "(a)the original image",
                "position": 1692
            },
            {
                "img": "https://arxiv.org/html/2503.07002/x14.png",
                "caption": "(b)the sample format",
                "position": 1698
            },
            {
                "img": "https://arxiv.org/html/2503.07002/extracted/6266231/arxiv_submission/supp_image/dataset/data_format/data_format_babyai.jpg",
                "caption": "(a)the original image",
                "position": 1705
            },
            {
                "img": "https://arxiv.org/html/2503.07002/extracted/6266231/arxiv_submission/supp_image/dataset/data_format/data_format_babyai.jpg",
                "caption": "(a)the original image",
                "position": 1708
            },
            {
                "img": "https://arxiv.org/html/2503.07002/x15.png",
                "caption": "(b)the sample format",
                "position": 1714
            },
            {
                "img": "https://arxiv.org/html/2503.07002/x16.png",
                "caption": "Figure 17:The evaluation prompt structure given to Gemini-1.5-Pro. The content in ‘[]’ is added when the CoT process is evaluated.",
                "position": 1736
            },
            {
                "img": "https://arxiv.org/html/2503.07002/extracted/6266231/arxiv_submission/supp_image/case_study/Gaze/Model_Chart.png",
                "caption": "(a)DiagNote",
                "position": 1889
            },
            {
                "img": "https://arxiv.org/html/2503.07002/extracted/6266231/arxiv_submission/supp_image/case_study/Gaze/Model_Chart.png",
                "caption": "(a)DiagNote",
                "position": 1892
            },
            {
                "img": "https://arxiv.org/html/2503.07002/extracted/6266231/arxiv_submission/supp_image/case_study/Gaze/DINO_Chart.png",
                "caption": "(b)Grounding DINO",
                "position": 1897
            },
            {
                "img": "https://arxiv.org/html/2503.07002/extracted/6266231/arxiv_submission/supp_image/case_study/Gaze/Model_Minigrid.jpg",
                "caption": "(a)DiagNote",
                "position": 1904
            },
            {
                "img": "https://arxiv.org/html/2503.07002/extracted/6266231/arxiv_submission/supp_image/case_study/Gaze/Model_Minigrid.jpg",
                "caption": "(a)DiagNote",
                "position": 1907
            },
            {
                "img": "https://arxiv.org/html/2503.07002/extracted/6266231/arxiv_submission/supp_image/case_study/Gaze/DINO_Minigrid.png",
                "caption": "(b)Grounding DINO",
                "position": 1912
            }
        ]
    },
    {
        "header": "BDiagNote",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07002/x17.png",
                "caption": "Figure 20:The description of Minigrid Scene added to the prompts.",
                "position": 1930
            },
            {
                "img": "https://arxiv.org/html/2503.07002/x18.png",
                "caption": "Figure 21:The prompt structure of the Deliberate module when the last Query output of the Deliberate module is not ‘END’.",
                "position": 1938
            },
            {
                "img": "https://arxiv.org/html/2503.07002/x19.png",
                "caption": "Figure 22:The prompt structure of the Deliberate module when the last Query output of the Deliberate module is ‘END’.",
                "position": 1941
            },
            {
                "img": "https://arxiv.org/html/2503.07002/x20.png",
                "caption": "Figure 23:The prompt structure of the Gaze module.",
                "position": 1949
            }
        ]
    },
    {
        "header": "CImplementation",
        "images": []
    },
    {
        "header": "DQualitative Comparison of Grounding",
        "images": []
    },
    {
        "header": "EAblation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07002/x21.png",
                "caption": "Figure 24:The second example of comparison between different MLLMs under everyday scenes.",
                "position": 1974
            },
            {
                "img": "https://arxiv.org/html/2503.07002/x22.png",
                "caption": "Figure 25:The first example of comparison between different MLLMs under everyday scenes.",
                "position": 2100
            },
            {
                "img": "https://arxiv.org/html/2503.07002/x23.png",
                "caption": "Figure 26:The second example of comparison between different MLLMs under everyday scenes.",
                "position": 2103
            },
            {
                "img": "https://arxiv.org/html/2503.07002/x24.png",
                "caption": "Figure 27:One example of comparison between different MLLMs under tabular scenes.",
                "position": 2118
            }
        ]
    },
    {
        "header": "FQualitative Comparison of Multi-Turn Multimodal Dialogue",
        "images": []
    }
]