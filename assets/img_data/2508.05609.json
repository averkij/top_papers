[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05609/x1.png",
                "caption": "Figure 1:Overview of Hi3DEval, a unified framework for 3D generation evaluation with three key components:1) Hierarchical evaluation protocolsthat jointly assess object-level and part-level quality, with extended material evaluation via reflectance cues.2) A large-scale benchmarkfeaturing a diverse set of 3D generative models and extensive human-aligned annotations generated via a multi-agent, multi-modal LLMs pipeline.3) A hybrid automated scoring systemthat integrates video-based and naive 3D-based representations to enhance evaluators’ perceptions of 3D structure.",
                "position": 100
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related works",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05609/x2.png",
                "caption": "Figure 2:Annotation of the Hi3DBench Dataset.The top left illustrates the annotation pipeline ofM²AP, while the top right presents a human alignment experiment comparing annotations from a single agent and M²AP. M²AP integrates advanced multimodal agents, incorporates reflection to address hallucination issues, and utilizes an elaborate scoring prompt along with rendered rotation videos and multi-view images to produce the final evaluation. The bottom row displays three annotation examples at object, part, and material levels.",
                "position": 165
            }
        ]
    },
    {
        "header": "3Hierarchical 3D scoring dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05609/x3.png",
                "caption": "Figure 3:Overview of video-based scoring pipeline.Left: Contrastive learning aligns the video encoder with the prompt encoder under diverse rendering conditions.Right: Quality heads are trained to regress scores. Specifically, we apply cosine similarity for prompt-aware dimensions.",
                "position": 295
            }
        ]
    },
    {
        "header": "4Hybrid 3D-aware scoring system",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05609/x4.png",
                "caption": "Figure 4:Overview of the 3D-based scoring pipeline.Left: Visualizations of the raw mesh, face embeddings, and part masks used in the pipeline, from top to bottom, exhibiting the strong capability of PartFieldPartFieldin capturing local geometric features.Right: Illustration of the scoring pipeline. We first project the pretrained features into a latent space tailored for the scoring task, then apply attention modules to enable information flow with the global context and within each part.",
                "position": 361
            },
            {
                "img": "https://arxiv.org/html/2508.05609/Figures/fig_leaderboard.png",
                "caption": "Figure 5:Radar visualization of Hi3DBench.Models are ranked top-down by their total score across all dimensions, each predicted by our automated scoring system. The two charts on the left show object-level results, while the two right show material-subject results. For clarity, only the top 6 models are shown in the charts. And the full leaderboards are provided in SectionB.3.",
                "position": 385
            }
        ]
    },
    {
        "header": "5Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05609/x5.png",
                "caption": "Figure 6:Visual examples of the object-level scoring model.We present representative examples in both image-to-3D and text-to-3D settings. For each object, we compare the predicted scores from our model with human ground-truth annotations across all evaluation dimensions. These results demonstrate the model’s ability to produce accurate and consistent assessments at the object level.",
                "position": 630
            },
            {
                "img": "https://arxiv.org/html/2508.05609/x6.png",
                "caption": "Figure 7:Visual examples of the part-level scoring model.We apply a normalized colormap to visualize part-level scores within objects, where blue indicates high-quality regions and red denotes low-quality regions.Left: Our scoring can locate surface distortions and abnormal structures in terms of geometry plausibility.Right: Our scoring can reflect the spatial distribution of geometric details.",
                "position": 633
            },
            {
                "img": "https://arxiv.org/html/2508.05609/Figures/fig_material_scoring_example.png",
                "caption": "Figure 8:Visual examples of the material-level scoring model.We compare our score rating results with baseline metrics and human annotations. Our model can accurately capture the texture representations for detail analysis, observe obvious lighting differences or shading artifacts for lighting conditions, and understand basic reflection rules for material plausibility.",
                "position": 636
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AMore details about scoring dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05609/x7.png",
                "caption": "Figure S1:Visualization about 3D part segmentation.Left: Due to varying complexity across prompts, assigning a fixed number of parts to all objects is suboptimal.Right: We propose to estimate prompt-specific part counts with GPT4V to better reflect meaningful structural granularity.",
                "position": 1728
            },
            {
                "img": "https://arxiv.org/html/2508.05609/Figures/fig_material_HDRI.png",
                "caption": "Figure S2:Visualization about HDRI maps.We select six HDRI maps from Poly Haven333Poly Haven website:https://polyhaven.com/hdris, including natural, artificial, indoor, outdoor, daylight and nightlight conditions.",
                "position": 1731
            },
            {
                "img": "https://arxiv.org/html/2508.05609/Figures/fig_material_visualization.png",
                "caption": "Figure S3:Visualization about relighting.We present the first frame of each object under varying illumination conditions. The leftmost metallic sphere serves as a reference, reflecting the HDRI environment or point light source position. Additionally, for the right-top and right-bottom light configurations, we adjust the camera elevation to ensure full object coverage.",
                "position": 1734
            }
        ]
    },
    {
        "header": "Appendix BMore details about video-based scoring model",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05609/x8.png",
                "caption": "Figure S4:Scoring criterion of each dimension at the object level.",
                "position": 2726
            },
            {
                "img": "https://arxiv.org/html/2508.05609/x9.png",
                "caption": "Figure S5:Scoring criterion of each dimension at the part level and material level.",
                "position": 2731
            }
        ]
    },
    {
        "header": "Appendix CMore details about 3D-based scoring model",
        "images": []
    }
]