[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20802/x1.png",
                "caption": "Figure 1:SDPO substantially outperforms an improved version of Group Relative Policy Optimization (GRPO) on LCB v6 with Qwen3-8B.Further, SDPO achieves GRPO’s final accuracy in4×4\\timesfewer generations.\nClaude Sonnet 4 is the strongest instruct model on the public LCBv6 leaderboard.\nShaded regions show the standard deviation across 3 seeds.",
                "position": 201
            },
            {
                "img": "https://arxiv.org/html/2601.20802/x2.png",
                "caption": "Figure 2:Comparison of RLVR and RLRF settings.In Reinforcement Learning with Verifiable Rewards (RLVR), the agent learns from a scalar rewardrr, which often acts as an information bottleneck by masking the underlying environment state.\nIn contrast, Reinforcement Learning with Rich Feedback (RLRF) utilizes tokenized feedback.\nThis provides a significantly richer signal than a scalar reward, as the feedback can encapsulate both the reward as well as detailed observations of the state (such as runtime errors from a code environment or feedback from an LLM judge).",
                "position": 234
            }
        ]
    },
    {
        "header": "2SDPO: Self-Distillation Policy Optimization",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20802/x3.png",
                "caption": "Figure 4:Example of self-teaching with Qwen3-8B. The answer is generated by the model before seeing the feedback. Then, we re-evaluate the log-probs of the original attempt with theself-teacherafter seeing the feedback. We show the per-tokenlog⁡(ℙ​(self-teacher)/ℙ​(student))\\log(\\nicefrac{{\\mathbb{P}\\left(\\text{self-teacher}\\right)}}{{\\mathbb{P}\\left(\\text{student}\\right)}}), with red indicating negative values (self-teacher disagrees)\nand white indicating values around zero. Notably, in this example, Qwen3-8B identifies the error through retrospection without an explicit solution. Further, the activation is sparse, identifying where mistakes happen and adjusting to the students’ response distribution.",
                "position": 454
            },
            {
                "img": "https://arxiv.org/html/2601.20802/x4.png",
                "caption": "Figure 5:Time per step for SDPO vs GRPO (solid: without code environment, light: with code environment).",
                "position": 544
            }
        ]
    },
    {
        "header": "3Learning without Rich Environment Feedback",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20802/x5.png",
                "caption": "Figure 6:Training progression of Olmo3-7B-Instruct on Chemistry. We report the average accuracy across 16 samples per question and a rolling average of response lengths over 5 steps. We report GRPO with the optimal hyperparameters for this model and task.",
                "position": 584
            }
        ]
    },
    {
        "header": "4Learning with Rich Environment Feedback",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20802/x6.png",
                "caption": "Figure 8:SDPO improves with model size.We compare the final LCBv6 validation accuracy of SDPO and GRPO at train step 80, across model sizes from Qwen3.\nThe ability of SDPO’s teacher to perform accurate retrospection appears to be an emergent phenomenon with scale.\nWe include an additional scaling study with Qwen2.5-Instruct in the appendix (cf.Figure˜17) which further supports this finding.\nError bars indicate the standard error across 3 seeds.",
                "position": 853
            },
            {
                "img": "https://arxiv.org/html/2601.20802/x7.png",
                "caption": "Figure 9:Dense credit assignment in SDPO in the example fromFigure˜4. Shown in blue are tokens which become more likely under the self-teacher. The self-teacher identifies how the returned range has to be modified so that it does not containn.",
                "position": 888
            },
            {
                "img": "https://arxiv.org/html/2601.20802/x8.png",
                "caption": "Figure 10:Left: Rich feedback in RLRF and dense credit assignment of SDPO are complementary.We compare logit-level, token-level, and sequence-level SDPO advantages to GRPO. While denser credit assignment in SDPO is beneficial (logit-level > token-level > sequence-level), even sequence-level SDPO significantly outperforms GRPO due to leveraging the rich feedback. Error bars indicate the standard error across 3 seeds.Right: The self-teacher improves during training.We display the generative accuracy of the self-teacher compared to student on the current training batch (with a rolling average over 5 steps). The final student score is taken at step 80. Notably, the performance of the student significantly surpasses the initial teacher’s accuracy. Error bars indicate the standard deviation across 3 seeds.",
                "position": 928
            },
            {
                "img": "https://arxiv.org/html/2601.20802/x9.png",
                "caption": "Figure 11:We compare the LCBv6 validation accuracy at step 80, across model sizes from Qwen3.\nSDPO+GRPO significantly outperforms SDPO on the weaker Qwen3-0.6B, while slightly underperforming SDPO on stronger models.\nWe useλ=0.9\\lambda=0.9.\nError bars indicate the standard error across 3 seeds.",
                "position": 1097
            }
        ]
    },
    {
        "header": "5Solving Hard Questions via Test-Time Self-Distillation",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20802/x10.png",
                "caption": "Figure 12:Compressing context into model weights via self-distillation.We illustrate the process of distilling the interaction history (contextcc) into the model parametersθ\\theta.\nThe modelπθ\\pi_{\\theta}repeatedly attempts a fixed hard questionxx, generating an answeryyand receiving feedbackff.\nRather than appending this history to the context window, the model updates its weightsθt→θt+1\\theta_{t}\\to\\theta_{t+1}with SDPO (batch size11) based on the feedback, effectively “fixing” mistakes by encodingπθ(⋅∣x,c)\\pi_{\\theta}(\\cdot\\mid x,c)directly into the policyπθ′(⋅∣x)\\pi_{\\theta^{\\prime}}(\\cdot\\mid x).",
                "position": 1260
            },
            {
                "img": "https://arxiv.org/html/2601.20802/x11.png",
                "caption": "Figure 13:Self-distillation at test-time solves LiveCodeBench questions that neither the base model nor multi-turn conversations can solve.Left:Very hard questions (9 total) from LCBv6 where the base model achievespass​@​64<0.03\\text{pass}@64<0.03, i.e., in less than 3% cases, sampling 64 responses yields any success.Right:Hard questions (19 total) from LCBv6 where the base model achievespass​@​64<0.5\\text{pass}@64<0.5.\nWe report thediscovery​@​k\\text{discovery}@kmetric, representing the probability of discovering at least one solution withinkktotal generations.\nAcross both difficulty levels, SDPO achieves higherdiscovery​@​k\\text{discovery}@krates at almost all generation budgets, compared to the base model and a multi-turn conversation baseline that receives the feedback in-context. We report the mean and bootstrapped 90% confidence intervals of the mean across 5 random seeds per question.",
                "position": 1285
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion, Limitations, and Future Work",
        "images": []
    },
    {
        "header": "Author Contributions",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Contents",
        "images": []
    },
    {
        "header": "Appendix AImplementation of SDPO",
        "images": []
    },
    {
        "header": "Appendix BTheoretical Analysis",
        "images": []
    },
    {
        "header": "Appendix CAdditional Related Work",
        "images": []
    },
    {
        "header": "Appendix DAdditional Results & Ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20802/x12.png",
                "caption": "Figure 15:Average accuracy during training until step 80, stratified by difficulty. Error bars show standard deviation across 3 seeds.",
                "position": 3946
            },
            {
                "img": "https://arxiv.org/html/2601.20802/x13.png",
                "caption": "Figure 16:Accuracy (pass@1) for varying train batch sizes (4, 8, 16, 32) and number of rollouts (4, 8) for training SDPO and GRPO with Qwen3-8B(Yang et al.,2025)on LCBv6,±\\pmstderr across 3 seeds. Different shades of the same color correspond to different runs.",
                "position": 3963
            },
            {
                "img": "https://arxiv.org/html/2601.20802/x14.png",
                "caption": "Figure 17:Average validation accuracy by model size,±\\pmstd across 3 seeds. With Qwen2.5-Instruct(Qwen et al.,2024)and Qwen3(Yang et al.,2025)on LCBv6. Until step 65 for Qwen2.5 and until step 80 for Qwen3.",
                "position": 3976
            },
            {
                "img": "https://arxiv.org/html/2601.20802/x15.png",
                "caption": "Figure 18:Loss, entropy, avg. gradient norm and avg. response length during training of SDPO on LCBv6 (Section˜4",
                "position": 3993
            },
            {
                "img": "https://arxiv.org/html/2601.20802/x16.png",
                "caption": "Figure 19:Ablations self-distillation at test-time on hard tasks.Left:Impact of SDPO batch size onpass​@​k\\text{pass}@kcurves. While smaller batch sizes (8 and 16) can lead to slightly earlier discoveries at very low generation budgets (k<26k<2^{6}), larger batch sizes (16, 32) result in more stable updates that significantly improve the discovery rate as the budget scales.Right:Comparison of multi-turn reprompting templates on a subset of hard questions. The “Only feedback” template concatenates the feedback from previous attempts using a first-in, first-out sliding window. The “Attempts + Feedback” template concatenates the full turn, also using a sliding window. Including only the feedback substantially outperforms concatenating full conversations.",
                "position": 4345
            },
            {
                "img": "https://arxiv.org/html/2601.20802/x17.png",
                "caption": "Figure 20:Individual task results self-distillation at test-time.Discovery​@​k\\text{Discovery}@kfor each of the 19 questions evaluated inSection˜5. In most cases, SDPO finds a successful solution significantly earlier than both the base model and the multi-turn baseline. Notably, for one question (Q3) where the base model and the multi-turn baseline maintain adiscovery​@​k\\text{discovery}@kof zero for the entire budget up to 2750 , SDPO discovers a solution after 321 attempts. Curves represent the mean and 90% confidence intervals across 5 random seeds per question.",
                "position": 4356
            }
        ]
    },
    {
        "header": "Appendix EExperiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20802/x18.png",
                "caption": "Figure 21:Visualization of advantages in SDPO and GRPO with Olmo3-7B-Instruct in a batch from the Chemistry task ofSection˜3. Each row corresponds to the beginning of a response. The color indicates the advantage value at that token position, with positive advantages shown in blue and negative advantages shown in red.",
                "position": 4902
            },
            {
                "img": "https://arxiv.org/html/2601.20802/x19.png",
                "caption": "Figure 22:Dense credit assignment through self-teaching in SDPO.The answer is generated by then model (Qwen3-8B) before seeing the feedback. Then, we re-evaluate the log-probs of the original attempt with the self-teacher after seeing the feedback. We show the per-tokenlog⁡(ℙ​(self-teacher)/ℙ​(student))\\log(\\nicefrac{{\\mathbb{P}\\left(\\text{self-teacher}\\right)}}{{\\mathbb{P}\\left(\\text{student}\\right)}}), with red indicating negative values (self-teacher disagrees), blue indicating positive values (teacher reinforces), and white indicating values around zero. Using binary rewards, GRPO would assign the same, negative advantage to all tokens in the sequence. In contrast, SDPO turns the feedback into dense credit assignment across the sequence. The first row shows the tokens of the generated response. The 3 other rows show the top-kklogits of the self-teacher that are used during self-distillation, suggesting alternative tokens. Notably, in this example, the self-teacher identifies the error through retrospection without an explicit solution. The credit assignment on the generated sequence, and the alternative top-kklogits correctly show that replacingsetwithdictmaintains the order of elements. Further, in the seventh shown position, the model also identifies an alternative solution path which starts with theseentoken, instead of directly returning the output. The activation is sparse, identifying where mistakes happen and adjusting to the students’ response distribution for specifically these few tokens.",
                "position": 5541
            }
        ]
    },
    {
        "header": "Appendix FQualitative Examples",
        "images": []
    }
]