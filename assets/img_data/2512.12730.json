[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.12730/x1.png",
                "caption": "Figure 1:An example NL2Repo task document, illustrating the structured specification (project description, supports, and API usage guide) that agents receive before repository generation.",
                "position": 129
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3NL2Repo-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.12730/x2.png",
                "caption": "Figure 2:Construction of NL2Repo-Bench.",
                "position": 202
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.12730/x3.png",
                "caption": "Figure 3:Leaderboard of model performance on NL2Repo-Bench benchmark. Pass rates are evaluated on the official test suite of the target repository. While Claude-Sonnet-4.5 based agents (OpenHands, Claude Code) achieve the highest performance, the best pass rate remains little above 40%, highlighting the significant challenge of end-to-end repository generation.",
                "position": 448
            },
            {
                "img": "https://arxiv.org/html/2512.12730/x4.png",
                "caption": "Figure 4:The total times using different tools for models in NL2Repo-Bench Tasks.",
                "position": 842
            },
            {
                "img": "https://arxiv.org/html/2512.12730/x5.png",
                "caption": "Figure 5:Early termination rates across models. Thinking models (e.g., Qwen3-T) exhibit an alarmingly high early stop rate of 49.0% and non-finish rate of 46.2%, leading to unsatisfactory scores.",
                "position": 1033
            },
            {
                "img": "https://arxiv.org/html/2512.12730/x6.png",
                "caption": "Figure 6:Tool usage distribution across models. All models allocate 50-60% of calls to code editing, 27-35% to command execution, but differ significantly in task planning usage.",
                "position": 1158
            },
            {
                "img": "https://arxiv.org/html/2512.12730/x7.png",
                "caption": "Figure 7:Tool call efficiency: quantity vs. quality. The dashed line represents Claudeâ€™s efficiency baseline. GPT-5 operates above this line (high efficiency but low volume), while most other models fall below it.",
                "position": 1173
            },
            {
                "img": "https://arxiv.org/html/2512.12730/x8.png",
                "caption": "Figure 8:Impact of context window size on performance. The 1M context model (Claude) substantially outperforms 256K models. GPT-5 (256K+) underperforms despite a larger context, suggesting context size is necessary but not sufficient.",
                "position": 1183
            },
            {
                "img": "https://arxiv.org/html/2512.12730/x9.png",
                "caption": "Figure 9:The effect of limitations on iteration rounds on the performance of Claude-Sonnet-4.5 on different difficulty level tasks.",
                "position": 1246
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Contributions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Difficulty Level in NL2Repo Tasks",
        "images": []
    },
    {
        "header": "8Available Tools in OpenHands CodeAct Framework",
        "images": []
    },
    {
        "header": "9Tutorial for NL2Repo-Bench Annotators",
        "images": []
    }
]