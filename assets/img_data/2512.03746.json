[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03746/images/intro.png",
                "caption": "Figure 1:Diagnostic results on image orientation identification.",
                "position": 114
            },
            {
                "img": "https://arxiv.org/html/2512.03746/images/adv.png",
                "caption": "Figure 2:Three advantages of CodeVision we observe in the training and inference stage.",
                "position": 123
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03746/images/sftdata.png",
                "caption": "Figure 3:Pipeline for cold-start SFT data construction.",
                "position": 166
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03746/images/rollout.png",
                "caption": "Figure 4:Rollout and inference process, and token masking used during SFT/RL.",
                "position": 207
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03746/images/reward_main.png",
                "caption": "Figure 5:RL training curves for outcome, strategy, and total rewards. The consistent upward trend demonstrates that the agent effectively learns to use tools strategically to solve tasks.",
                "position": 754
            },
            {
                "img": "https://arxiv.org/html/2512.03746/images/exam1.png",
                "caption": "Figure 6:A multi-turn example of error recovery. The model initially calls the wrong tool (flip-horizontal), but after receiving the execution result, it identifies the mistake and corrects it by applying the right tool (rotate-90).",
                "position": 769
            },
            {
                "img": "https://arxiv.org/html/2512.03746/images/r_emerge.png",
                "caption": "Figure 7:Reward curve for successful emergent tool use during RL training.",
                "position": 779
            },
            {
                "img": "https://arxiv.org/html/2512.03746/images/tools.png",
                "caption": "Figure 8:A word cloud of emergent tools discovered during RL training.",
                "position": 788
            },
            {
                "img": "https://arxiv.org/html/2512.03746/images/exam2.png",
                "caption": "Figure 9:An example of emergent and efficient tool use. The model chains two tools, contrast enhancement and grayscale conversion, within a single turn to fulfill the user’s request. These tools did not appear in the RL training set, demonstrating the generalization capability of the code-as-tool framework.",
                "position": 794
            },
            {
                "img": "https://arxiv.org/html/2512.03746/images/exam7.png",
                "caption": "Figure 10:An example of emergent and efficient tool use. The model chains “brightness up, contrast up, crop, rotate90, sharpness” tools to solve the user’s request.",
                "position": 797
            },
            {
                "img": "https://arxiv.org/html/2512.03746/images/exam3.png",
                "caption": "Figure 11:An example of reward hacking from a model trained without constraint penalties. After correctly applying rotate90, the agent continues to call superfluous tools, leading to task failure.",
                "position": 800
            },
            {
                "img": "https://arxiv.org/html/2512.03746/images/exam4.png",
                "caption": "Figure 12:A successful multi-step reasoning case. The model first performs an initial crop, then recognizes its incompleteness and refines the cropped region in a second step to include all necessary information before providing the final answer.",
                "position": 822
            },
            {
                "img": "https://arxiv.org/html/2512.03746/images/exam5.png",
                "caption": "Figure 13:A successful but inefficient localization case. The model corrects the orientation and successfully crops a region containing the target text. However, the crop is a long, narrow strip that includes much irrelevant information, pointing to potential improvements in generating more precise bounding boxes.",
                "position": 825
            },
            {
                "img": "https://arxiv.org/html/2512.03746/images/exam6.png",
                "caption": "Figure 14:A failure case highlighting the model’s limitations in precise localization. The model correctly adjusts the orientation and identifies the general area of interest, but provides slightly inaccurate coordinates for the crop, resulting in a cropped region that just misses the target.",
                "position": 828
            },
            {
                "img": "https://arxiv.org/html/2512.03746/images/ablation_reward_R_spurious.png",
                "caption": "(a)Penalty Term",
                "position": 915
            },
            {
                "img": "https://arxiv.org/html/2512.03746/images/ablation_reward_R_spurious.png",
                "caption": "(a)Penalty Term",
                "position": 918
            },
            {
                "img": "https://arxiv.org/html/2512.03746/images/ablation_actor_entropy.png",
                "caption": "(b)Entropy",
                "position": 923
            },
            {
                "img": "https://arxiv.org/html/2512.03746/images/ablation_reward_NumTurns.png",
                "caption": "(c)Tool Turns",
                "position": 928
            },
            {
                "img": "https://arxiv.org/html/2512.03746/images/experiment_actor_entropy.png",
                "caption": "(a)Entropy",
                "position": 935
            },
            {
                "img": "https://arxiv.org/html/2512.03746/images/experiment_actor_entropy.png",
                "caption": "(a)Entropy",
                "position": 938
            },
            {
                "img": "https://arxiv.org/html/2512.03746/images/experiment_reward_NumTurns.png",
                "caption": "(b)Tool Turns",
                "position": 943
            },
            {
                "img": "https://arxiv.org/html/2512.03746/images/experiment_reward_R_acc.png",
                "caption": "(c)Accuracy Reward",
                "position": 948
            },
            {
                "img": "https://arxiv.org/html/2512.03746/images/experiment_reward_R_mandatory.png",
                "caption": "(d)Strategy Reward",
                "position": 953
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03746/images/ocr_plot.png",
                "caption": "(a)OCRBench-Rot90",
                "position": 972
            },
            {
                "img": "https://arxiv.org/html/2512.03746/images/ocr_plot.png",
                "caption": "(a)OCRBench-Rot90",
                "position": 975
            },
            {
                "img": "https://arxiv.org/html/2512.03746/images/chartqa_plot.png",
                "caption": "(b)ChartQAPro-Hori",
                "position": 980
            },
            {
                "img": "https://arxiv.org/html/2512.03746/images/mvtool_plot.png",
                "caption": "(c)MVToolBench",
                "position": 985
            }
        ]
    },
    {
        "header": "6Limitations and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]