[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13145/x1.png",
                "caption": "",
                "position": 96
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13145/x2.png",
                "caption": "Figure 2:Initialize Mamba-2 from Transformer.By comparing the mechanism similarity in Sec.3, we directly inheritğ‘¾Qsubscriptğ‘¾ğ‘„\\boldsymbol{W}_{Q}bold_italic_W start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT,ğ‘¾Ksubscriptğ‘¾ğ¾\\boldsymbol{W}_{K}bold_italic_W start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT,ğ‘¾Vsubscriptğ‘¾ğ‘‰\\boldsymbol{W}_{V}bold_italic_W start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT,ğ‘¾Osubscriptğ‘¾ğ‘‚\\boldsymbol{W}_{O}bold_italic_W start_POSTSUBSCRIPT italic_O end_POSTSUBSCRIPTparameters (blue) from trained Transformer layer and carefully initialize the extra parameters (orange) includingağ‘aitalic_a,ğ‘¾Î³subscriptğ‘¾ğ›¾\\boldsymbol{W}_{\\gamma}bold_italic_W start_POSTSUBSCRIPT italic_Î³ end_POSTSUBSCRIPT,ğ‘¾convsubscriptğ‘¾conv\\boldsymbol{W}_{\\text{conv}}bold_italic_W start_POSTSUBSCRIPT conv end_POSTSUBSCRIPT, andğ‘¾Gsubscriptğ‘¾ğº\\boldsymbol{W}_{G}bold_italic_W start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPTin Mamba-2 to initially mimic the Transformerâ€™s behavior, providing a strong foundation for subsequent distillation.",
                "position": 286
            },
            {
                "img": "https://arxiv.org/html/2502.13145/x3.png",
                "caption": "Figure 3:Progressive distillation pipeline of our mmMamba.We keep MLP layers, text and image patch embedding layers and freeze them in subsequent distillation training stages. Stage-1: Train the newly-introduced SSM-specific parameters while freezing inherited Transformer parameters in a layer-wise manner. Stage-2: Train all parameters to align Mambaâ€™s state representation with Transformer in a layer-wise manner. Stage-3: Train all the Mamba layers of the model to align the end-to-end behavior with the teacher Transformer-based VLM.",
                "position": 304
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]