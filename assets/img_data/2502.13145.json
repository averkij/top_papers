[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13145/x1.png",
                "caption": "",
                "position": 96
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13145/x2.png",
                "caption": "Figure 2:Initialize Mamba-2 from Transformer.By comparing the mechanism similarity in Sec.3, we directly inherit𝑾Qsubscript𝑾𝑄\\boldsymbol{W}_{Q}bold_italic_W start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT,𝑾Ksubscript𝑾𝐾\\boldsymbol{W}_{K}bold_italic_W start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT,𝑾Vsubscript𝑾𝑉\\boldsymbol{W}_{V}bold_italic_W start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT,𝑾Osubscript𝑾𝑂\\boldsymbol{W}_{O}bold_italic_W start_POSTSUBSCRIPT italic_O end_POSTSUBSCRIPTparameters (blue) from trained Transformer layer and carefully initialize the extra parameters (orange) includinga𝑎aitalic_a,𝑾γsubscript𝑾𝛾\\boldsymbol{W}_{\\gamma}bold_italic_W start_POSTSUBSCRIPT italic_γ end_POSTSUBSCRIPT,𝑾convsubscript𝑾conv\\boldsymbol{W}_{\\text{conv}}bold_italic_W start_POSTSUBSCRIPT conv end_POSTSUBSCRIPT, and𝑾Gsubscript𝑾𝐺\\boldsymbol{W}_{G}bold_italic_W start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPTin Mamba-2 to initially mimic the Transformer’s behavior, providing a strong foundation for subsequent distillation.",
                "position": 286
            },
            {
                "img": "https://arxiv.org/html/2502.13145/x3.png",
                "caption": "Figure 3:Progressive distillation pipeline of our mmMamba.We keep MLP layers, text and image patch embedding layers and freeze them in subsequent distillation training stages. Stage-1: Train the newly-introduced SSM-specific parameters while freezing inherited Transformer parameters in a layer-wise manner. Stage-2: Train all parameters to align Mamba’s state representation with Transformer in a layer-wise manner. Stage-3: Train all the Mamba layers of the model to align the end-to-end behavior with the teacher Transformer-based VLM.",
                "position": 304
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]