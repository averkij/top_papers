[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Methodology",
        "images": []
    },
    {
        "header": "5Experiments of Large Language Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10672/x1.png",
                "caption": "Figure 1:CEREBRAS-GPT: Time comparison",
                "position": 500
            },
            {
                "img": "https://arxiv.org/html/2410.10672/x2.png",
                "caption": "(a)Cross-Entropy Loss",
                "position": 579
            },
            {
                "img": "https://arxiv.org/html/2410.10672/x2.png",
                "caption": "(a)Cross-Entropy Loss",
                "position": 582
            },
            {
                "img": "https://arxiv.org/html/2410.10672/x3.png",
                "caption": "(b)Perplexity",
                "position": 587
            },
            {
                "img": "https://arxiv.org/html/2410.10672/x4.png",
                "caption": "(c)Matrix Entropy",
                "position": 592
            },
            {
                "img": "https://arxiv.org/html/2410.10672/x5.png",
                "caption": "(d)Matrix Nuclear-Norm",
                "position": 597
            },
            {
                "img": "https://arxiv.org/html/2410.10672/x6.png",
                "caption": "Figure 3:Results of sentence operation. Shuffling and reversing disrupt the text structure and diminish the informational content, leading to an increase in Matrix Nuclear-Norm.",
                "position": 625
            },
            {
                "img": "https://arxiv.org/html/2410.10672/x7.png",
                "caption": "Figure 4:The Matrix Nuclear-Norm values for contexts of varying lengths show that as text length increases, the Matrix Nuclear-Norm continues to rise and tends to converge.",
                "position": 632
            }
        ]
    },
    {
        "header": "6Implementing Proposed Metrics: Evaluating and Ranking Language Models in Practice",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Limitations",
        "images": []
    },
    {
        "header": "9Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10672/x8.png",
                "caption": "(a)Cross-Entropy Loss",
                "position": 1783
            },
            {
                "img": "https://arxiv.org/html/2410.10672/x8.png",
                "caption": "(a)Cross-Entropy Loss",
                "position": 1786
            },
            {
                "img": "https://arxiv.org/html/2410.10672/x9.png",
                "caption": "(b)Matrix Entropy",
                "position": 1791
            },
            {
                "img": "https://arxiv.org/html/2410.10672/x10.png",
                "caption": "(c)Matrix Nuclear-Norm",
                "position": 1796
            },
            {
                "img": "https://arxiv.org/html/2410.10672/x11.png",
                "caption": "Figure 6:Pythia: Time Comparison of Matrix Entropy and Nuclear-Norm",
                "position": 1889
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]