[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25077/x1.png",
                "caption": "Figure 1:We present BRIDGE, showcasing its RL-optimized Depth-to-Image (D2I) data generation engine which is used for generating realistic and geometrically accurate RGB images from source depth maps and Monocular Depth Estimation (MDE) model which after being trained on the massive high-quality data generated by the D2I engine, achieves superior depth prediction in complex scenes.",
                "position": 75
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2related works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25077/x2.png",
                "caption": "Figure 2:Reward Model Process.Our D2I model is trained via reward-gradient-driven direct optimization , avoiding complex proxy objective functions and improving training memory efficiency.",
                "position": 220
            }
        ]
    },
    {
        "header": "4Implementation Details",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25077/x3.png",
                "caption": "Figure 3:Comparison between our model and leading methods, including Depth Anything V2(Yang et al.,2024d), Depth Pro(Bochkovskii et al.,2024)and Marigold(Ke et al.,2025)on open-world images.",
                "position": 1606
            },
            {
                "img": "https://arxiv.org/html/2509.25077/x4.png",
                "caption": "Figure 4:Additional comparison between our model and leading methods, including Depth Anything V2(Yang et al.,2024d), Depth Pro(Bochkovskii et al.,2024)and Marigold(Ke et al.,2025)on “in-the-wild” images.",
                "position": 1609
            },
            {
                "img": "https://arxiv.org/html/2509.25077/x5.png",
                "caption": "Figure 5:Our model, BRIDGE, generates superior, high-fidelity depth maps that enable ControlNet(Zhang et al.,2023)to synthesize new images with a zero-shot capability, precisely replicating the depth field of the source image. In contrast, Depth Anything V2(Yang et al.,2024d)struggles to produce an accurate depth field, as demonstrated by the clear discrepancies between its corresponding ControlNet output and the source images. The prompts used for ControlNet are displayed in the lower left corners, and all images were generated with the same random seed.",
                "position": 1613
            }
        ]
    },
    {
        "header": "Appendix AExperiments",
        "images": []
    }
]