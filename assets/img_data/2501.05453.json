[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.05453/x1.png",
                "caption": "Figure 1:Overall Framework.Starting with images and video frames from a collection of datasets, we tokenize each frame/image into discrete visual tokens independently.\nWe pre-train the transformer by predicting the next visual tokens, with a context length of 4K tokens of images or video frames. Once trained, we take the intermediate representations and evaluate them on various tasks.",
                "position": 161
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/toto_blue.png",
                "caption": "Figure 2:Training Loss Curves:We show the training loss curves for base, large, and 1b models trained with tokens from dVAE(Ramesh et al.,2021)with a vocabulary size of 8k and context length of 4k tokens (equivalent to 16 images or video frames).",
                "position": 195
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/tokens1gram_blue.png",
                "caption": "Figure 3:1-gram Distribution of Various Tokens:This Figure shows the distribution of 1-gram tokens of various tokenizers (dVAE(Ramesh et al.,2021), VQGAN-1k, VQGAN-16k(Esser et al.,2020)) on Imagenet validation set. Note that, dVAE has almost full convergence of the tokens while VQGAN has less than 50% coverage of the tokens.",
                "position": 376
            },
            {
                "img": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/plot2_toto_blue.png",
                "caption": "",
                "position": 538
            },
            {
                "img": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/tracking.png",
                "caption": "Figure 5:Semi-Supervised Tracking:We follow the protocol in STC(Jabri et al.,2020), start with the GT segmentation mask, and propagate the labels using the features computed byToto-large. The mask was propagated up to 60 frames without losing much information.",
                "position": 815
            },
            {
                "img": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/robots/franka_pick_toto_blue.png",
                "caption": "(a)Franka Pick",
                "position": 820
            },
            {
                "img": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/robots/franka_pick_toto_blue.png",
                "caption": "(a)Franka Pick",
                "position": 823
            },
            {
                "img": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/robots/kuka_pick_toto_blue.png",
                "caption": "(b)Kuka Pick",
                "position": 828
            },
            {
                "img": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/robots/franka_cabinet_toto_blue.png",
                "caption": "(c)Franka Cabinet",
                "position": 833
            },
            {
                "img": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/robots/kuka_cabinet_toto_blue.png",
                "caption": "(d)Kuka Cabinet",
                "position": 838
            },
            {
                "img": "https://arxiv.org/html/2501.05453/x2.png",
                "caption": "Figure 7:Real-world Deployment:We show an example episode of our policy performing the cube picking task on a Franka robot in the real world. We useToto-base to run the robot at real time, despite being a small model,Totowas able to achieve about 63% success rate in real world setting.",
                "position": 952
            },
            {
                "img": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/plot2x_vgpt_blue.png",
                "caption": "Figure 8:Probing Across Layers, Models, and Tasks:We study the behavior of our models across multiple layers and tasks. For image classification, action recognition, and object tracking, all the models behave similarly and peak around 50% of the model depth. This behavior is observed across all model sizes. Robot tasks show a similar behaviour, where the middle layers perform good at picking the objects, but last layers also perform good as middle layers. These plots suggests, in decoder-only model, first half of the model starts to behave like an encoder, and compress the information, and then rest of the model, projects the compressed semantic features back to input space.",
                "position": 1000
            },
            {
                "img": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/plot2x_vgpt_blue.png",
                "caption": "",
                "position": 1003
            },
            {
                "img": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/plot3_toto_blue.png",
                "caption": "",
                "position": 1007
            },
            {
                "img": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/plot4_toto_blue.png",
                "caption": "",
                "position": 1011
            },
            {
                "img": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/plot5_vgpt_blue.png",
                "caption": "",
                "position": 1015
            },
            {
                "img": "https://arxiv.org/html/2501.05453/x3.png",
                "caption": "Figure 9:ScalingToto:We train multiple variants ofToto, with increasing hidden size and depth, with optimal learning rates. We plot the validation loss vs the compute spent on training in MACs. This shows a clear scaling behavior with optimal compute.",
                "position": 1025
            }
        ]
    },
    {
        "header": "5Limitations",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/toto-large-k400-val-set.png",
                "caption": "Figure 10:Average Validation Loss Over Tokens:We show the average loss per token for kinetics validation set. It clearly shows the redundancy in videos, as the first frame has higher prediction loss, and rest of the frames on average has lower loss than the first frame.",
                "position": 1931
            },
            {
                "img": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/mup_vqgan_blue.png",
                "caption": "Table 13:Full Fine Tuning Performance:Comparison of different methods performance on ImageNet-1K.",
                "position": 1949
            },
            {
                "img": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/mup_vqgan_blue.png",
                "caption": "Table 15:TotoVarients:We scaleTotomodels by increasing hidden dimension and number of layers linearly while keeping number of heads constant following(Yang et al.,2022; Touvron et al.,2023).",
                "position": 2011
            },
            {
                "img": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/tokens2gram_blue.png",
                "caption": "Figure 12:2-gram Distribution of Various Tokens:We compute the 2-gram distribution on 10000 images from the ImageNet validation set. Compared to VQGAN 1k and 16k vocabulary tokenizers, the dVAE tokenizer has a larger set of token combinations.",
                "position": 2088
            },
            {
                "img": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/tokens3gram_blue.png",
                "caption": "Figure 13:3-gram Distribution of Various Tokens:We compute the 3-gram distribution on 10000 images from the ImageNet validation set. All the tokenizers has similar almost flat distribution when it comes to 3-gram tokens.",
                "position": 2091
            },
            {
                "img": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/frames1.png",
                "caption": "",
                "position": 2167
            },
            {
                "img": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/frames2.png",
                "caption": "",
                "position": 2172
            },
            {
                "img": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/frames3.png",
                "caption": "",
                "position": 2177
            },
            {
                "img": "https://arxiv.org/html/2501.05453/extracted/6091816/figs/plot2_extra.png",
                "caption": "Figure 14:Training Loss Curves:We show the training loss curves for multiple variants of our models.",
                "position": 2185
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]