[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01328/x1.png",
                "caption": "Figure 1:Comparison between the mainstream video MLLM architecture and the proposed slow-fast architecture.Rather than relying on carefully-designed video representation compression strategies, the slow-fast architecture utilizes highly compressed “fast” visual tokens as a preview for the LLM while allowing text embeddings to extract relevant information from uncompressed “slow” visual tokens via cross-attention. This approach extends a 16-frame baseline to a 96-frame input with only a 2% increase in computation, yielding a 14% average performance improvement across five benchmarks.",
                "position": 104
            },
            {
                "img": "https://arxiv.org/html/2504.01328/x2.png",
                "caption": "Figure 2:Illustration of the Slow-Fast Architecture and Hybrid Decoder.The video input is first processed intoslow visual tokensthrough a vision encoder and projector.\nTheseslow visual tokensare then condensed into a smaller set offast visual tokensvia strided sampling and temporal pooling.\nThefast visual tokensare concatenated with text embeddings and fed into the LLM, serving as a preview context.\nMeanwhile, theslow visual tokensinteract with text embeddings through cross-attention in hybrid decoder layers distributed within the LLM, enabling instruction-aware visual information extraction with linear complexity.",
                "position": 142
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Slow-Fast Architecture for Video MLLM",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01328/x3.png",
                "caption": "Figure 3:Qualitative examples and comparisons between different input frame numbers.For the video on the left, models trained and tested with 64 and 96 frames are compared, denoted as “64x” and “96x”. In the video on the right, we further apply test time augmentation by increasing the input frames to 192.More comparisons are available in the supplement.",
                "position": 1003
            },
            {
                "img": "https://arxiv.org/html/2504.01328/x4.png",
                "caption": "Figure 4:Visualizations of the cross-attention map and the dynamic gate in the hybrid decoder.The cross-attention maps are averaged across different decoder layers, text tokens, and attention heads. The absolute value of the dynamic gate from all the four hybrid decoder layers are visualized.",
                "position": 1087
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01328/x5.png",
                "caption": "Figure 5:More qualitative examples of our model.",
                "position": 1871
            },
            {
                "img": "https://arxiv.org/html/2504.01328/x6.png",
                "caption": "Figure 6:Comparison with other state-of-the-art Video MLLMs on video description.",
                "position": 1874
            },
            {
                "img": "https://arxiv.org/html/2504.01328/x7.png",
                "caption": "Figure 7:Comparison with other state-of-the-art Video MLLMs on information summarization.",
                "position": 1877
            },
            {
                "img": "https://arxiv.org/html/2504.01328/x8.png",
                "caption": "Figure 8:Comparison with other state-of-the-art Video MLLMs on video question understanding.",
                "position": 1880
            }
        ]
    },
    {
        "header": "Appendix BModel Architecture Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01328/x9.png",
                "caption": "Figure 9:Detailed architecture illustration of the hybrid decoder layer.",
                "position": 1929
            }
        ]
    },
    {
        "header": "Appendix CPrompts for Evaluation",
        "images": []
    }
]