[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14462/x1.png",
                "caption": "Figure 1:Pipeline of constructing the SAM-FB dataset.\nThe background is inpainted and high-quality foreground objects are preserved through a data quality control stage.",
                "position": 189
            },
            {
                "img": "https://arxiv.org/html/2412.14462/x2.png",
                "caption": "Figure 2:The framework of MADD. Foreground objects are encoded using a DINOv2 encoder, serving as the guidance signal through the cross-attention mechanism. The position prompt encoder unifies different types of position prompts, which are then concatenated with the latent maskùê¶ùê≠subscriptùê¶ùê≠\\mathbf{m_{t}}bold_m start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT. The background is encoded using a VAE encoder and then concatenated with the latent imageùê≥ùê≠subscriptùê≥ùê≠\\mathbf{z_{t}}bold_z start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT. We use a dual branch structure to denoise RGB imageùê≥ùê≥\\mathbf{z}bold_zand object maskùê¶ùê¶\\mathbf{m}bold_msimultaneously.",
                "position": 193
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14462/x3.png",
                "caption": "Figure 3:Qualitative results of MADD on the SAM-FB test set. Each row corresponds to one type of prompt,i.e., point, bounding box, mask, and null, respectively. Our MADD simultaneously predicts the RGB image and the object mask.",
                "position": 356
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14462/x4.png",
                "caption": "(a)Location Adjustment",
                "position": 378
            },
            {
                "img": "https://arxiv.org/html/2412.14462/x4.png",
                "caption": "(a)Location Adjustment",
                "position": 381
            },
            {
                "img": "https://arxiv.org/html/2412.14462/x5.png",
                "caption": "(b)View Adjustment",
                "position": 386
            },
            {
                "img": "https://arxiv.org/html/2412.14462/x6.png",
                "caption": "(c)Size Adjustment",
                "position": 392
            },
            {
                "img": "https://arxiv.org/html/2412.14462/x7.png",
                "caption": "(d)Automatic Localization",
                "position": 397
            },
            {
                "img": "https://arxiv.org/html/2412.14462/extracted/6077377/imgs/diverse.png",
                "caption": "Figure 5:MADD can give different feasible solutions for ambiguous prompts such as point and blank.",
                "position": 576
            },
            {
                "img": "https://arxiv.org/html/2412.14462/x8.png",
                "caption": "(a)Rank distribution for different methods. Our method has the highest proportion of rank 1 and the least proportion of rank 5.",
                "position": 579
            },
            {
                "img": "https://arxiv.org/html/2412.14462/x8.png",
                "caption": "(a)Rank distribution for different methods. Our method has the highest proportion of rank 1 and the least proportion of rank 5.",
                "position": 582
            },
            {
                "img": "https://arxiv.org/html/2412.14462/extracted/6077377/imgs/criteria.png",
                "caption": "(b)Rank-1 distribution for each criterion. Each pie chart represents the proportion of times each model achieved Rank-1 for a specific evaluation criterion. Our method dominates every metric.",
                "position": 587
            },
            {
                "img": "https://arxiv.org/html/2412.14462/x9.png",
                "caption": "Figure 7:MADD can work on images of higher resolution, generating sharper edges, clearer reflections, and improved texture details.",
                "position": 594
            },
            {
                "img": "https://arxiv.org/html/2412.14462/x10.png",
                "caption": "Figure 8:More in-the-wild examples with null prompts. The model can generate an affordance-feasible solution to insert the foreground objects according to the background scene.",
                "position": 597
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14462/extracted/6077377/imgs/FID-CLIP.png",
                "caption": "Figure 9:FID-CLIP score curve on128√ó128128128128\\times 128128 √ó 128resolution with different guidance scale[1.0,3.0,4.0,5.0,6.0,7.0]1.03.04.05.06.07.0[1.0,3.0,4.0,5.0,6.0,7.0][ 1.0 , 3.0 , 4.0 , 5.0 , 6.0 , 7.0 ].",
                "position": 1395
            },
            {
                "img": "https://arxiv.org/html/2412.14462/extracted/6077377/imgs/sample.png",
                "caption": "(a)Examples for foreground quality control",
                "position": 1402
            },
            {
                "img": "https://arxiv.org/html/2412.14462/extracted/6077377/imgs/sample.png",
                "caption": "(a)Examples for foreground quality control",
                "position": 1405
            },
            {
                "img": "https://arxiv.org/html/2412.14462/extracted/6077377/imgs/category.png",
                "caption": "(b)Word cloud of foreground categories",
                "position": 1410
            },
            {
                "img": "https://arxiv.org/html/2412.14462/x11.png",
                "caption": "Figure 11:Example of objects with details. Our model could keep the appearance better even with some details compared with SD[39], GLI-GEN[29]and PBE[47]. The first row demonstrates the ability to keep some image texture, and the second row illustrates the ability to keep text texture.",
                "position": 1747
            }
        ]
    },
    {
        "header": "Appendix BEvaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14462/x12.png",
                "caption": "Figure 12:Samples on SAM-FB test split. Our model inserted the bag with an authentic appearance.",
                "position": 1850
            },
            {
                "img": "https://arxiv.org/html/2412.14462/x13.png",
                "caption": "Figure 13:Example of in-the-wild insertion results with details. Our model could keep the appearance better and adjust the foreground‚Äôs properties better compared with SD[39], GLI-GEN[29]and PBE[47]on common objects. In the last row, the model generated reasonable insertion when provided with ambiguous prompts.",
                "position": 1861
            },
            {
                "img": "https://arxiv.org/html/2412.14462/x14.png",
                "caption": "Figure 14:Some failure cases when using our model to perform affordance insertion.",
                "position": 1885
            }
        ]
    },
    {
        "header": "Appendix CMore Results",
        "images": []
    }
]