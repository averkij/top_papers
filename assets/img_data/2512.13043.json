[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3The GTR-Turbo Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13043/x1.png",
                "caption": "Figure 1:Illustration of the original GTR framework(wei2025gtr).It uses a multi-modal API model as the corrector, such as GPT or Gemini, to evaluate and refine the agent’s reasoning content (i.e., thoughtt​htth_{t}) at each RL step, which is costly, time-consuming, and potentially inaccessible, constraining its own scalability.",
                "position": 187
            },
            {
                "img": "https://arxiv.org/html/2512.13043/x2.png",
                "caption": "Figure 2:The performance comparison of the merged checkpoint and the current checkpoint on Points24.We adopt the Qwen2.5-VL-7B as the base model and highlight that model merging leads to a stronger and more stable agentπmerged(k)\\pi^{(k)}_{\\text{merged}}(red line) that can serve as a teacher to guide the following RL for trainingπθ(k)\\pi_{\\theta}^{(k)}.",
                "position": 279
            },
            {
                "img": "https://arxiv.org/html/2512.13043/x3.png",
                "caption": "Figure 3:Overview of the GTR-Turbo framework.Beyond the GTR training of VLM agents (Figure1), GTR-Turbo stores historical checkpoints and merges them into a teacher model (blue region), and then incorporates the PPO update (orange region) with thought guidance by minimizing either SFT loss (green region) or KL divergence (purple region), enabling flexiable, scalable, and self-guided agentic RL training.",
                "position": 282
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13043/x4.png",
                "caption": "",
                "position": 402
            },
            {
                "img": "https://arxiv.org/html/2512.13043/x5.png",
                "caption": "",
                "position": 511
            },
            {
                "img": "https://arxiv.org/html/2512.13043/x6.png",
                "caption": "",
                "position": 805
            },
            {
                "img": "https://arxiv.org/html/2512.13043/x7.png",
                "caption": "",
                "position": 826
            },
            {
                "img": "https://arxiv.org/html/2512.13043/x8.png",
                "caption": "Figure 12:Comparison among different KL estimation methods.All methods with non-negative output can achieve increased performance. The clipping method presents the best result, since it controls the magnitude of the KL value, leading to finer-grained updates and improved stability. The slightly lower result of forward KL proves the mode-seeking advantage of reverse KL.",
                "position": 837
            },
            {
                "img": "https://arxiv.org/html/2512.13043/x9.png",
                "caption": "Figure 13:Comparing different weights assignment methods.Simple SMA already yields strong performance. A balanced choice ofα\\alphais critical for realizing the benefit of EMA.",
                "position": 840
            }
        ]
    },
    {
        "header": "5Conclusions and Limitations",
        "images": []
    },
    {
        "header": "Appendix APseudocodes",
        "images": []
    },
    {
        "header": "Appendix BAdditional Details on Training",
        "images": []
    },
    {
        "header": "Appendix CAdditional Details on Environments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13043/x10.png",
                "caption": "Figure 14:The Points24 task.",
                "position": 1220
            },
            {
                "img": "https://arxiv.org/html/2512.13043/x11.png",
                "caption": "Figure 15:The ALFWorld task.",
                "position": 1251
            },
            {
                "img": "https://arxiv.org/html/2512.13043/x12.png",
                "caption": "Figure 16:Result of Qwen3-VL-8B on ALFWorld.",
                "position": 1266
            }
        ]
    },
    {
        "header": "Appendix DAdditional Experiment Results",
        "images": []
    }
]