[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06032/x1.png",
                "caption": "",
                "position": 75
            },
            {
                "img": "https://arxiv.org/html/2512.06032/x2.png",
                "caption": "",
                "position": 75
            },
            {
                "img": "https://arxiv.org/html/2512.06032/x3.png",
                "caption": "",
                "position": 75
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Conceptual Break Between Prompt-Based and Concept-Based Segmentation",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06032/x4.png",
                "caption": "Figure 2:(a) SAM2 architecture: a prompt-driven vision–temporal pipeline where segmentation depends on spatial prompts and memory retrieval across frames ; (b) SAM3 architecture: a multimodal vision-language system with a new detector enabling open-vocabulary, concept-level segmentation ;(c) SAM2 orchard workflow showing prompt-based apple segmentation without semantic understanding ; and (d) SAM3 workflow demonstrating text-prompted, concept-aware segmentation, identifying all relevant apple instances through multimodal fusion.",
                "position": 126
            }
        ]
    },
    {
        "header": "3Architectural Divergence Undermining Knowledge Transfer",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06032/x5.png",
                "caption": "Figure 5:SAM3 architecture, highlighting new multimodal components in yellow, inherited SAM2ravi2024sammodules in blue, and the Perception Encoderbolya2025perceptionin cyan. The model integrates vision, text, geometry, and exemplar prompts through a dual encoder–decoder transformer, enabling concept-level segmentation beyond SAM2’s purely prompt-driven capabilities. Image sourced fromcarion2025sam",
                "position": 262
            },
            {
                "img": "https://arxiv.org/html/2512.06032/x6.png",
                "caption": "Figure 6:Scientific overview of the six core reasons SAM2 expertise fails to transfer to SAM3. The infographic highlights differences in optimization objectives, multimodal prompting, semantic embedding spaces, DETR-style decoding, ambiguity modeling, and PE-driven representation learning illustrating the fundamental architectural and conceptual discontinuity between prompt-based and concept-driven segmentation.",
                "position": 346
            }
        ]
    },
    {
        "header": "4Dataset and Annotation Differences",
        "images": []
    },
    {
        "header": "5Training and Hyperparameter Distinctions",
        "images": []
    },
    {
        "header": "6Evaluation, Metrics, and Failure Modes",
        "images": []
    },
    {
        "header": "7Comprehensive Tables Describing the SAM2-to-SAM3 Gap",
        "images": []
    },
    {
        "header": "8Conclusion and Future Directions: Toward a Concept-Driven Segmentation Era",
        "images": []
    },
    {
        "header": "Declaration of Competing Interest",
        "images": []
    }
]