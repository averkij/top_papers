[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.05303/x1.png",
                "caption": "",
                "position": 123
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.05303/x2.png",
                "caption": "Figure 2:Overview ofInteractVLM.Given a color image, ourVLMperforms the core reasoning, and guides a novelMV-Locmodel to localize contacts on both bodies and objects in 3D.\nHere we show only the body;\nfor details, and object contact, seeFig.3.",
                "position": 221
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.05303/x3.png",
                "caption": "Figure 3:Method overview.Given a single in-the-wild color image, our novelInteractVLMmethod estimates 3D contact points on both humans and objects (a). Then, we reconstruct a 3D human and object in interaction by exploiting these contacts (b).\nMore specifically:(a) Contact estimation.Given an image,I𝐼Iitalic_I, and\nprompt text,Ti⁢n⁢psubscript𝑇𝑖𝑛𝑝T_{inp}italic_T start_POSTSUBSCRIPT italic_i italic_n italic_p end_POSTSUBSCRIPT,\nourVLM,ΨΨ\\Psiroman_Ψ, produces contact tokens for humans and objects, <HCON> and <OCON>, which are projected (ΓΓ\\Gammaroman_Γ) into feature embeddings,EHsuperscript𝐸𝐻E^{H}italic_E start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPTandEOsuperscript𝐸𝑂E^{O}italic_E start_POSTSUPERSCRIPT italic_O end_POSTSUPERSCRIPT.\nThese guide a “Multi-View [contact]\nLocalization” model.\nThis renders the 3D human and object geometry via cameras,K𝐾{K}italic_K, into multi-view 2D renders and passes these to encoder,ΘΘ\\Thetaroman_Θ, while decoders,ΩHsuperscriptΩ𝐻\\Omega^{H}roman_Ω start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT,ΩOsuperscriptΩ𝑂\\Omega^{O}roman_Ω start_POSTSUPERSCRIPT italic_O end_POSTSUPERSCRIPT, estimate and highlight 2D contacts in these renders. Then,\ntheFeatLiftmodule,ΦΦ\\Phiroman_Φ,\ntransforms\ntheVLM’s features (EHsuperscript𝐸𝐻E^{H}italic_E start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT,EOsuperscript𝐸𝑂E^{O}italic_E start_POSTSUPERSCRIPT italic_O end_POSTSUPERSCRIPT)\nto become 3D-aware\n(E3⁢DHsubscriptsuperscript𝐸𝐻3𝐷E^{H}_{3D}italic_E start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 3 italic_D end_POSTSUBSCRIPT,E3⁢DOsubscriptsuperscript𝐸𝑂3𝐷E^{O}_{3D}italic_E start_POSTSUPERSCRIPT italic_O end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 3 italic_D end_POSTSUBSCRIPT)\nby exploiting the camera parameters,K𝐾{K}italic_K.\nA final module lifts the detected 2D contacts to 3D.(b) 3DHOIreconstruction.For joint human-object reconstruction, we useInteractVLM’s inferred contacts in an optimization framework.",
                "position": 315
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.05303/x4.png",
                "caption": "Figure 4:“Semantic Human Contact” estimation (Sec.4.2).Given an image and an object label,InteractVLMinfers body contacts for this object.InteractVLMoutperforms a Semantic-DECO[60]baseline.\nObjects are shown in green circles, and contacts as red patches.",
                "position": 703
            },
            {
                "img": "https://arxiv.org/html/2504.05303/x5.png",
                "caption": "Figure 5:InteractVLM’s reliance on 3D annotations.We evaluate performance for “binary human contact” (F1 score, Y-axis) for models trained on a varying percentage ofDAMON[60]training data (X-axis).\nTheDECObaseline trains on 100% ofDAMON.\nInstead,InteractVLMtrains on a varying (smaller) portion of this dataset. Yet, it achieves a significantly higher performance,\nby leveraging the broad visual knowledge of foundation models.",
                "position": 731
            },
            {
                "img": "https://arxiv.org/html/2504.05303/x6.png",
                "caption": "Figure 6:3DHOIreconstruction (Sec.5).We build an optimization method that fits aSMPL-Xbody andOpenShape-retrieved object to an in-the-wild image.\nWe evaluate against theSotAmethodPHOSA[69].\nReconstruction is guided byInteractVLM-inferred contacts.",
                "position": 1065
            }
        ]
    },
    {
        "header": "5Joint Human-Object Reconstruction",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "S.1Human Contact Prediction",
        "images": []
    },
    {
        "header": "S.2Ablation Studies",
        "images": []
    },
    {
        "header": "S.3Impact of RLL",
        "images": []
    },
    {
        "header": "S.4Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.05303/x7.png",
                "caption": "Figure S.1:Contact Estimation Failure Cases.Our method struggles with unusual human poses (left). For objects (right), training on affordances rather than actual contacts can sometimes lead to ambiguous contact predictions, especially for large objects like chairs.\nHowever, no dataset exists for 3D object contacts for in-the-wild images.",
                "position": 3295
            },
            {
                "img": "https://arxiv.org/html/2504.05303/x8.png",
                "caption": "Figure S.2:Object Retrieval Failure Cases.The retrieved object meshes (right) differ notably from the actual objects in the input images (left), especially in cases of significant occlusion, atypical object instances, or limited database coverage. Despite these inaccuracies, the retrieval consistently selects objects within the correct semantic category.",
                "position": 3302
            }
        ]
    },
    {
        "header": "S.5Failure Cases",
        "images": []
    },
    {
        "header": "S.6Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.05303/x9.png",
                "caption": "Figure S.3:Object Affordance Prediction.Here we compare ourInteractVLMmethod trained for object affordance prediction onPIAD[67]dataset with the state-of-the-artIAGNetmethod.\nWe train for affordance detection because there exists no dataset of in-the-wild images paired with ground-truth 3D contacts for objects.\nNote that given an image of a person performing an action like “sit” or “grasp”, the affordance prediction task estimates “contact possibilities”\non the object.",
                "position": 3369
            },
            {
                "img": "https://arxiv.org/html/2504.05303/x10.png",
                "caption": "Figure S.4:Semantic Human Contact estimation.Here we show results for “semantic human contact” estimation from in-the-wild images.\nEach row shows a person in contact with multiple objects.\nNote howInteractVLMestimates contact on bodies that is specific to the object.",
                "position": 3378
            },
            {
                "img": "https://arxiv.org/html/2504.05303/x11.png",
                "caption": "Figure S.5:3DHOIreconstruction.Here we show results of ourInteractVLMmethod for 3DHOIreconstruction from in-the-wild images.\nWe use theInteractVLM’s inferred contacts on both bodies and objects for joint 3D reconstruction.",
                "position": 3386
            }
        ]
    },
    {
        "header": "S.7Future Work",
        "images": []
    }
]