[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.23205/2602.23205v1/x1.png",
                "caption": "",
                "position": 119
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Proposed Capture System",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.23205/2602.23205v1/x2.png",
                "caption": "Figure 2:EmbodMocap: We propose an affordable dataset capture and processing system. From left to right, the four stages (Stage-I to Stage-IV) illustrate our core logic: leveraging high-quality camera matrices provided by SpectacularAI[55]and aligning sequence coordinates to the sceneâ€™s world frame. For detailed explanations, please refer toSec.3.",
                "position": 612
            }
        ]
    },
    {
        "header": "4Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.23205/2602.23205v1/x3.png",
                "caption": "Figure 3:Our dual viewvs.single view results in optical studio.",
                "position": 1190
            }
        ]
    },
    {
        "header": "5Downstream Tasks",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.23205/2602.23205v1/x4.png",
                "caption": "Figure 4:Quality results of proposed 4D Human & Scene Reconstruction pipeline on EMDB dataset.",
                "position": 1478
            },
            {
                "img": "https://arxiv.org/html/2602.23205/2602.23205v1/x5.png",
                "caption": "(a)Qualitative comparison on 4 basic skills.",
                "position": 2328
            },
            {
                "img": "https://arxiv.org/html/2602.23205/2602.23205v1/x5.png",
                "caption": "(a)Qualitative comparison on 4 basic skills.",
                "position": 2331
            },
            {
                "img": "https://arxiv.org/html/2602.23205/2602.23205v1/x6.png",
                "caption": "(b)Qualitative comparison on 2 additional skills.",
                "position": 2337
            },
            {
                "img": "https://arxiv.org/html/2602.23205/2602.23205v1/x7.png",
                "caption": "Figure 6:We present qualitative results of scene-aware motion tracking, showing four long-term motion examples in different scenes (a, b, c, and d), including daily indoor and outdoor interactions such as walking, sitting, lying, stair climbing, and touching. Our motion tracking framework not only accurately tracks the reference motion but also ensures physical realism, resolving subtle issues, such as interpenetration and floating artifacts, present in the reference data (see zoomed-in views on the right).",
                "position": 2351
            },
            {
                "img": "https://arxiv.org/html/2602.23205/2602.23205v1/x8.png",
                "caption": "Figure 7:A real-world humanoid robot imitating human motions depicted in videos.",
                "position": 2637
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations and Future Work.",
        "images": []
    },
    {
        "header": "8Acknowledge",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "9More Details of EmbodMocap",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.23205/2602.23205v1/x9.png",
                "caption": "Figure 8:Capture technique.",
                "position": 3576
            },
            {
                "img": "https://arxiv.org/html/2602.23205/2602.23205v1/x10.png",
                "caption": "Figure 9:An example in finding the contact marker in software (e.g., Meshlab) and corresponding keyframe index(the frames selected here are just for demo).",
                "position": 3631
            }
        ]
    },
    {
        "header": "10More Details of Monocular Human-Scene Reconstruction Pipeline",
        "images": []
    },
    {
        "header": "11More Details of Human-Object Interaction Skills",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.23205/2602.23205v1/x11.png",
                "caption": "(a)Camera Trajectory Length Distribution.",
                "position": 4009
            },
            {
                "img": "https://arxiv.org/html/2602.23205/2602.23205v1/x11.png",
                "caption": "(a)Camera Trajectory Length Distribution.",
                "position": 4012
            },
            {
                "img": "https://arxiv.org/html/2602.23205/2602.23205v1/x12.png",
                "caption": "(b)Human Trajectory Length Distribution.",
                "position": 4018
            },
            {
                "img": "https://arxiv.org/html/2602.23205/2602.23205v1/x13.png",
                "caption": "(c)Scene Mesh Area Distribution.",
                "position": 4024
            },
            {
                "img": "https://arxiv.org/html/2602.23205/2602.23205v1/x14.png",
                "caption": "(d)Sequence Length Distribution.",
                "position": 4030
            },
            {
                "img": "https://arxiv.org/html/2602.23205/2602.23205v1/x15.png",
                "caption": "Figure 11:Rendered SMPL and depth images of the captured dataset in camera space.",
                "position": 4037
            }
        ]
    },
    {
        "header": "12More Details of Scene-Aware Imitation Policy",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.23205/2602.23205v1/x16.png",
                "caption": "Figure 12:3D demo of the captured dataset.",
                "position": 4143
            }
        ]
    },
    {
        "header": "13More Details of Captured Dataset Used in Main Paper",
        "images": []
    }
]