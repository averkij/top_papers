[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20811/x1.png",
                "caption": "Figure 1:Our standardized caption format presents each individualâ€™s detailed attributes, body actions, and interactions in chronological order, making it easier to distinguish individuals and comprehend their behaviors.",
                "position": 110
            },
            {
                "img": "https://arxiv.org/html/2502.20811/x2.png",
                "caption": "Figure 2:Our data generation pipeline. (a) The video accumulation stage collects videos featuring clear human actions from the Internet. Based on this, (b) HAICTrain is curated through Gemini-1.5-Pro re-captioning, and\n(c) HAICBench is created by LLM-assisted human annotation.",
                "position": 223
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3HAIC Data Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20811/x3.png",
                "caption": "Figure 3:Statistics of HAICBench. Although videos are relatively short, the video captions are of high details including various action details and sequential actions.",
                "position": 330
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20811/x4.png",
                "caption": "Figure 4:A video caption example in HAICBench.",
                "position": 796
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAutomatic Annotation",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20811/x5.png",
                "caption": "",
                "position": 1649
            }
        ]
    },
    {
        "header": "Appendix BEvaluation datasets",
        "images": []
    },
    {
        "header": "Appendix CQA Pair Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20811/x6.png",
                "caption": "Figure 6:Prompt for action interaction QA generation.",
                "position": 1707
            },
            {
                "img": "https://arxiv.org/html/2502.20811/x7.png",
                "caption": "Figure 7:Prompt for action details QA generation.",
                "position": 1710
            },
            {
                "img": "https://arxiv.org/html/2502.20811/x8.png",
                "caption": "Figure 8:Prompt for action sequence QA generation.",
                "position": 1713
            },
            {
                "img": "https://arxiv.org/html/2502.20811/x9.png",
                "caption": "Figure 9:Prompt for action count QA generation.",
                "position": 1716
            },
            {
                "img": "https://arxiv.org/html/2502.20811/x10.png",
                "caption": "Figure 10:Prompt for human attribute QA generation.",
                "position": 1719
            },
            {
                "img": "https://arxiv.org/html/2502.20811/x11.png",
                "caption": "Figure 11:Prompt caption evaluation setting.",
                "position": 1722
            }
        ]
    },
    {
        "header": "Appendix DCaption Evaluation Setting",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20811/x12.png",
                "caption": "Figure 12:Videos generated by captions from LLaVA-Video and LLaVA-Video-ActionPro of the first sample in MovieGenBench. The main subject in this case is one woman walking along the street. LLaVA-Video-ActionPro provides a more detailed appearance of the woman than LLaVA-Video.",
                "position": 1736
            },
            {
                "img": "https://arxiv.org/html/2502.20811/x13.png",
                "caption": "Figure 13:Videos generated by captions from LLaVA-Video and LLaVA-Video-ActionPro of the 17th sample in MovieGenBench. The main subject in this case is one blue animated character. LLaVA-Video incorrectly identifies the main subject.",
                "position": 1739
            },
            {
                "img": "https://arxiv.org/html/2502.20811/x14.png",
                "caption": "Figure 14:Videos generated by captions from LLaVA-Video and LLaVA-Video-ActionPro of the 129th sample in MovieGenBench. The main subjects in this case are five cars lined in two rows. LLaVA-Video incorrectly identifies the number of the main subjects.",
                "position": 1742
            }
        ]
    },
    {
        "header": "Appendix ECase Study on Text-to-Video Generation",
        "images": []
    },
    {
        "header": "Appendix FPotential Risks",
        "images": []
    }
]