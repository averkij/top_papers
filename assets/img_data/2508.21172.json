[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21172/x1.png",
                "caption": "Figure 1:Architectural organization of the proposed DeepResESN. (a) Structure of a genericll-th reservoir layer in a DeepResESN. The reservoir structure (shown in blue) consists of an input weight matrixùêñx(l)\\mathbf{W}_{x}^{(l)}, a recurrent weight matrixùêñh(l)\\mathbf{W}_{h}^{(l)}, and a non-linear activation functionœï\\phi. The temporal residual connection (shown in purple) is modulated by an orthogonal matrixùêé\\mathbf{O}. The temporal residual and non-linear paths are scaled by positive coefficientsŒ±(l)\\alpha^{(l)}andŒ≤(l)\\beta^{(l)}, respectively. (b) Complete illustration of a DeepResESN architecture withNLN_{L}reservoir layers. The first layer acts as a residual reservoir in a traditional shallow architecture and is fed the external inputùê±(1)\\mathbf{x}^{(1)}. Subsequent layers receive as input the output of the previous reservoir,ùê°(l‚àí1)\\mathbf{h}^{(l-1)}. The readout may be fed either the final layer states or the concatenation of states from all layers. See SectionIIIfor details.",
                "position": 104
            }
        ]
    },
    {
        "header": "IIReservoir Computing",
        "images": []
    },
    {
        "header": "IIIDeep Residual Echo State Networks",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21172/x2.png",
                "caption": "Figure 2:Structure of the three orthogonal matrices (10√ó1010\\times 10) used in the temporal residual connections.",
                "position": 184
            },
            {
                "img": "https://arxiv.org/html/2508.21172/x3.png",
                "caption": "Figure 3:Spectral frequencies of (a) DeepResESNR, (b) DeepResESNC, and (c) DeepResESNI, in progressively deeper layers (columns). In each layer, and for all configurations, we considerNh=100N_{h}=100recurrent neurons,œÅ=1\\rho=1,Œ±=0.9\\alpha=0.9,Œ≤=0.1\\beta=0.1,œâx=1\\omega_{x}=1andœâb=0\\omega_{b}=0. Results are averaged over1010trials. Magnitudes have been normalized to ease visualization. Red arrows highlight the trend in spectral magnitudes.",
                "position": 194
            }
        ]
    },
    {
        "header": "IVStability Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21172/x4.png",
                "caption": "Figure 4:Eigenvalues of the Jacobian of a DeepResESNRfor spectral radii (a)œÅ=0.5\\rho=0.5, (b)œÅ=1\\rho=1, and (c)œÅ=2\\rho=2, for progressively deeper layers (columns). In each layer, we considerNh=100N_{h}=100recurrent neurons,œÅ\\rhoas specified in each subplot,Œ±=0.5\\alpha=0.5,Œ≤=1\\beta=1,œâx=1\\omega_{x}=1, andœâb=0\\omega_{b}=0. Model dynamics are driven by a random input vector and a random hidden state, both uniformly distributed in(‚àí1,1)(-1,1). In orange the unitary circle.",
                "position": 424
            }
        ]
    },
    {
        "header": "VExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21172/x5.png",
                "caption": "(a)",
                "position": 527
            },
            {
                "img": "https://arxiv.org/html/2508.21172/x5.png",
                "caption": "(a)",
                "position": 530
            },
            {
                "img": "https://arxiv.org/html/2508.21172/x6.png",
                "caption": "(b)",
                "position": 536
            }
        ]
    },
    {
        "header": "VIConclusions",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Appendix AProofs",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Biography",
        "images": []
    }
]