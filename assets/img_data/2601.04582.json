[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04582/x1.png",
                "caption": "Figure 1:An example from Text2VisRahman etÂ al. (2025b). Given the data table and question as input, (a) the baseline model, Qwen2.5-14B-Instruct generates runnable visualization code, but the visualization is not aligned with the query as it shows the growth of renewables quantity instead of the share of total, leading to an incorrect answer. (b) Our RL-Text2Vis-14B produces a correct, query-aligned, and interpretable visualization.",
                "position": 135
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04582/x2.png",
                "caption": "Figure 2:GRPO training architecture for text-to-visualization generation, showing policy outputs, multi-objective rewards (answer, code, visualization), combined reward computation, and advantage calculation for policy updates.",
                "position": 181
            }
        ]
    },
    {
        "header": "3RL-Text2Vis",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Results",
        "images": []
    },
    {
        "header": "6Error Analysis",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Ethical Considerations",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04582/x3.png",
                "caption": "Figure 3:Error analysis before and after GRPO.GRPO significantly improves text-to-visualization generation by resolving errors such as syntax, value errors, enhancing readability, visual quality, and alignment with the query.",
                "position": 1644
            },
            {
                "img": "https://arxiv.org/html/2601.04582/Train1.png",
                "caption": "Figure 4:Completion length and epoch progression. The model stabilizes in output length while training steps progress linearly.",
                "position": 1727
            },
            {
                "img": "https://arxiv.org/html/2601.04582/Train2.png",
                "caption": "Figure 5:Gradient norm and KL divergence. Indicates optimization stability and policy deviation from the reference model.",
                "position": 1730
            },
            {
                "img": "https://arxiv.org/html/2601.04582/Train3.png",
                "caption": "Figure 6:Learning rate schedule and training loss. Warm-up and decay patterns are followed, with loss trends influenced by reward-maximizing objectives.",
                "position": 1733
            },
            {
                "img": "https://arxiv.org/html/2601.04582/Train4.png",
                "caption": "Figure 7:Average reward and reward standard deviation. Reflects growing reward consistency and reduced variance across outputs.",
                "position": 1736
            },
            {
                "img": "https://arxiv.org/html/2601.04582/Train5.png",
                "caption": "Figure 8:Format reward and Composite reward function score. Tracks alignment with structural and multimodal feedback objectives.",
                "position": 1739
            }
        ]
    },
    {
        "header": "Appendix AAppendices",
        "images": []
    }
]