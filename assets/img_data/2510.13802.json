[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13802/x1.png",
                "caption": "Figure 1:Any video‚àócan be represented in 4D with aTrajectory Field, a dense mapping assigning each pixel in each frame to a parametric 3D trajectory. We proposeTrace Anything, a neural network that predicts the trajectory field with a single forward pass.",
                "position": 111
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13802/x2.png",
                "caption": "Figure 2:Given the input frames (left), atrajectory fieldrepresents the video at the atomic level, mapping each pixel in each frame to a 3D trajectory, expressed as a parametric curve (right).",
                "position": 125
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13802/x3.png",
                "caption": "Figure 3:Trace Anything pipeline.Input frames are processed by a geometric backbone consisting of an image encoder and a fusion transformer. The control point head outputs dense control point mapsùêèi‚àà‚ÑùD√óH√óW√ó3\\mathbf{P}_{i}\\in\\mathbb{R}^{D\\times H\\times W\\times 3}, whereùêèi,u,v(k)\\mathbf{P}^{(k)}_{i,u,v}is thekk-th control point for pixel(u,v)(u,v)in frameIiI_{i}. These define continuous 3D trajectoriesùê±i,u,v‚Äã(t)\\mathbf{x}_{i,u,v}(t)via cubic B-splines, yielding a 4D reconstruction.",
                "position": 201
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13802/x4.png",
                "caption": "Figure 4:Sample renderings from our data platform.",
                "position": 427
            }
        ]
    },
    {
        "header": "4Trace Anything Data Platform",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13802/x5.png",
                "caption": "Figure 5:Video-based trajectory field estimation on DAVIS[41].Trace Anything predicts trajectory fields that can yield dynamic point cloud sequences and dense 3D trajectories, while remaining robust to complex non-rigid motion and occlusions.",
                "position": 482
            },
            {
                "img": "https://arxiv.org/html/2510.13802/x6.png",
                "caption": "Figure 6:Image-pair-based trajectory field estimation (goal-conditioned manipulation) on Bridge[53].Given an initial and a goal image, Trace Anything predicts a trajectory field that interpolates the 3D motion of both the robot arm and manipulated objects. We further show the projected 2D trajectories (seeSectionÀúC.1andFigureÀúAfor details).",
                "position": 487
            },
            {
                "img": "https://arxiv.org/html/2510.13802/x7.png",
                "caption": "Figure 7:Qualitative results with image pairs as input.",
                "position": 492
            },
            {
                "img": "https://arxiv.org/html/2510.13802/x8.png",
                "caption": "Figure 8:Trajectory fields and camera poses estimated from an unstructured, unordered image set. No sequence information is provided to the model.",
                "position": 499
            },
            {
                "img": "https://arxiv.org/html/2510.13802/x9.png",
                "caption": "Figure 9:Stage-wise runtime vs. number of input frames.",
                "position": 720
            },
            {
                "img": "https://arxiv.org/html/2510.13802/x9.png",
                "caption": "Figure 9:Stage-wise runtime vs. number of input frames.",
                "position": 723
            },
            {
                "img": "https://arxiv.org/html/2510.13802/x10.png",
                "caption": "Figure 10:Velocity-based forecasting.Per-pixel trajectories are extrapolated by tangent continuation,\nwith reconstructed trajectories in red and extrapolated ones in blue.",
                "position": 728
            },
            {
                "img": "https://arxiv.org/html/2510.13802/x11.png",
                "caption": "Figure 11:Spatio-temporal fusion.The trajectory field can be leveraged to fuse observations of the dynamic entity across different frames into a canonical frame.",
                "position": 736
            },
            {
                "img": "https://arxiv.org/html/2510.13802/x12.png",
                "caption": "Figure 12:Instruction-based forecasting.\nFuture states are generated using Seedance 1.0.",
                "position": 755
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "AFields",
        "images": []
    },
    {
        "header": "BParametric Curves",
        "images": []
    },
    {
        "header": "CAdditional Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13802/x13.png",
                "caption": "Figure A:Projected 2D trajectories overlaid on the first input frame.",
                "position": 1896
            },
            {
                "img": "https://arxiv.org/html/2510.13802/x14.png",
                "caption": "Figure B:Dynamic mask estimation.",
                "position": 1903
            },
            {
                "img": "https://arxiv.org/html/2510.13802/x15.png",
                "caption": "Figure C:4D reconstruction and scene flow from a single image pair.From a non-consecutive image pair in theSpringdataset, our method recovers both the 4D reconstruction and the scene flow, withxxandzzcomponents color-coded for visualization.",
                "position": 1909
            },
            {
                "img": "https://arxiv.org/html/2510.13802/x16.png",
                "caption": "Figure D:Estimated camera poses over the 4D reconstruction.",
                "position": 1915
            },
            {
                "img": "https://arxiv.org/html/2510.13802/x17.png",
                "caption": "Figure E:Qualitative comparison on DAVIS[41].Our method better recovers fine details and handles complex motion while disentangling static and dynamic objects.",
                "position": 1925
            }
        ]
    },
    {
        "header": "DLimitations",
        "images": []
    },
    {
        "header": "ELLM Usage Declarations",
        "images": []
    }
]