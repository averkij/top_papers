[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16670/images/Teaser.png",
                "caption": "",
                "position": 84
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16670/images/method/Pipeline.png",
                "caption": "Figure 2:FrameDiffuser Architecturewith dual conditioning: ControlNet processes 10-channel input comprising 9 G-buffer channels for structural guidance and 1 pred. irradiance channel for lighting guidance, computed from the previous frame‚Äôs model output and basecolor. ControlLoRA conditions on the previous frame encoded in VAE latent space for temporal coherence. The generated output at timettis used to compute the irradiance input for the next frame at timet+1t+1, enabling autoregressive frame generation. The encoder‚Ñ∞\\mathcal{E}and decoderùíü\\mathcal{D}represent the VAE components operating in latent space. The training strategy on the right shows our three-stage approach: first, we train ControlNet on the G-buffer to image translation task without irradiance. Second, we add ControlLoRA and irradiance for temporal conditioning. Third, we train autoregressively using the model‚Äôs own generated frames as previous-frame inputs to make the model robust against its own generation errors.",
                "position": 142
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16670/images/gbuffer_figure.png",
                "caption": "Figure 3:Qualitative Resultsshowing FrameDiffuser‚Äôs autoregressive generation across multiple frames, including long-term stability at frame 4000. From top to bottom: ground truth, our autoregressive output, and the G-buffer channels (basecolor, Normal, Depth, Roughness, Metallic) alongside computed Irradiance. The model maintains temporal consistency and accurate material properties across extended sequences.",
                "position": 190
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16670/images/comparison_figure.png",
                "caption": "Figure 4:Qualitative Comparisonwith X‚ÜíRGB across Downtown West (urban) and Hillside Sample (indoor) environments. Our method achieves high-detail lighting while maintaining temporal consistency across frames over long sequences, while X‚ÜíRGB applies more uniform lighting.",
                "position": 247
            }
        ]
    },
    {
        "header": "4Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16670/images/plots/metrics_over_time_smoothed.png",
                "caption": "Figure 5:Temporal Stability Analysisover 3000+ consecutive validation frames of pure autoregressive generation inside the Hillside Sample Project Environment[hillsidesample]Figure4. We compare our model output against pure VAE reconstruction to measure encoder degradation. Metrics show degradation between frames 800‚Äì1700 when the camera enters very dark rooms; the model‚Äôs bias towards lit areas causes it to insufficiently capture extreme darkness. After frame 3000, all metrics degrade when camera movement reduces and G-buffer changes become minimal, causing error accumulation.",
                "position": 360
            },
            {
                "img": "https://arxiv.org/html/2512.16670/images/SelfCond_figure.png",
                "caption": "Figure 6:Self-conditioning (SC) and Noise Injection (NI) Impacton autoregressive generation. Without these mechanisms, severe degradation occurs within 5 frames. With self-conditioning and noise injection, quality remains stable over extended sequences.",
                "position": 443
            },
            {
                "img": "https://arxiv.org/html/2512.16670/images/editing.png",
                "caption": "Figure 7:Scene Editingwhen objects are added to the scene through G-buffer modifications, FrameDiffuser automatically synthesizes appropriate lighting, shading, and cast shadows. Left: A barrel insertion demonstrates correct lighting direction and shadow casting. Right: A large concrete statue shows the model‚Äôs ability to infer complex shadow patterns and lighting intensities on geometric additions, maintaining photorealistic consistency.",
                "position": 446
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledgements",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16670/images/suppl/comparison_figure_dr.png",
                "caption": "Figure 8:DiffusionRenderer Comparisonacross two environments. GT: Ground Truth, Ours: FrameDiffuser, DR: DiffusionRenderer[liang2025]. FrameDiffuser maintains closer alignment with ground truth lighting and atmospheric effects. See accompanying videosDiffusionRenderer_comparison_CityPark.mp4andDiffusionRenderer_comparison_Hillside.mp4.",
                "position": 525
            }
        ]
    },
    {
        "header": "ADiffusionRenderer Comparison",
        "images": []
    },
    {
        "header": "BOut-of-Distribution Generalization",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16670/images/suppl/out_of_distribution.png",
                "caption": "Figure 9:Out-of-Distribution Generalization.Top left: Downtown West training environment. Top right: Starting frame for generation. Bottom: Frames 1, 50, and 100 from a 100-frame autoregressive sequence on City Sample, showing GT (top row), FrameDiffuser (middle), and X‚Üí\\rightarrowRGB (bottom). The warm Downtown West style is slightly transferred to the cold City Sample environment.",
                "position": 606
            }
        ]
    },
    {
        "header": "CTemporal Flicker Analysis",
        "images": []
    },
    {
        "header": "DGBufferDiffuser: Inverse Rendering",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16670/images/suppl/GBufferDiffuser.png",
                "caption": "Figure 10:GBufferDiffuser Qualitative Comparison.GBufferDiffuser produces more accurate reconstructions with better shape and color prediction compared to RGB‚Üí\\rightarrowX[zeng2024], due to specialization to the G-buffer type and environment.",
                "position": 760
            }
        ]
    },
    {
        "header": "EStyle Transfer",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16670/images/suppl/style_transfer.png",
                "caption": "Figure 11:Style Transfer: Burning Style.The original frame is stylized via image-to-image transformation (Edited), then used as previous frame input for autoregressive generation. Fire effects persist through temporal propagation but gradually diminish as the model reverts to its training distribution.",
                "position": 776
            }
        ]
    },
    {
        "header": "FImplementation Details",
        "images": []
    }
]