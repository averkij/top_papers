[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16317/images/teaser.jpg",
                "caption": "",
                "position": 77
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16317/x1.png",
                "caption": "Figure 2:Illustration of the fundamental challenges in multi-view diffusion (MVD) texturing, compared with the proposed NaTex.",
                "position": 113
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16317/x2.png",
                "caption": "Figure 3:Overall architecture of NaTex: it mainly consists of a geometry-aware color VAE for reconstruction and a multi-control color DiT for generation, adaptable for diverse applications. Left-most assets are all generated by NaTex.",
                "position": 181
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16317/x3.png",
                "caption": "Figure 4:Illustration of multi-control mechanisms of the proposed color DiT. Color control is useful for texture-conditioned tasks.",
                "position": 257
            },
            {
                "img": "https://arxiv.org/html/2511.16317/x4.png",
                "caption": "Figure 5:Visual results showcasing representative applications of NaTex. Additional results are provided in the Appendix.",
                "position": 271
            },
            {
                "img": "https://arxiv.org/html/2511.16317/x5.png",
                "caption": "Figure 6:Visual comparison of texture reconstruction results.",
                "position": 274
            },
            {
                "img": "https://arxiv.org/html/2511.16317/x6.png",
                "caption": "Figure 7:Visual comparison of different methods for generating textured 3D assets from images: commercial models use their own geometries, while other methods share the same geometry from Hunyuan3D 2.5[lai2025hunyuan3d]. All methods are rendered with albedo only.",
                "position": 319
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16317/x7.png",
                "caption": "Figure 8:Visual comparison of conventional inpainting and our neural inpainting. Two views of multiview images are shown on the left. We need to inpaint the occlusion in the window.",
                "position": 379
            },
            {
                "img": "https://arxiv.org/html/2511.16317/x8.png",
                "caption": "Figure 9:Visual comparison of different geometry conditioning methods: with RoPE and geometry embedding from the geometry-aware color VAE, texture and color alignment are optimized.",
                "position": 382
            },
            {
                "img": "https://arxiv.org/html/2511.16317/x9.png",
                "caption": "Figure 10:Illustration of different inference setups. Without additional training and distillation, more tokens improve quality and alignment, and we achieve one-step generation for free.",
                "position": 385
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16317/x10.png",
                "caption": "Figure 11:Illustration of our material generation results from a case study, with individual components visualized separately.",
                "position": 496
            },
            {
                "img": "https://arxiv.org/html/2511.16317/x11.png",
                "caption": "Figure 12:Illustration of our material generation results under different lightings, rendered using various environment maps.",
                "position": 499
            },
            {
                "img": "https://arxiv.org/html/2511.16317/x12.png",
                "caption": "Figure 13:Visual comparison between our NaTex material generation pipeline and a conventional MVD-based material pipeline. Our method produces more accurate and better-aligned materials compared to prior approaches.",
                "position": 502
            },
            {
                "img": "https://arxiv.org/html/2511.16317/x13.png",
                "caption": "Figure 14:High-quality PBR-textured assets generated by NaTex. Geometry obtained from Hunyuan3D 2.5[lai2025hunyuan3d].",
                "position": 505
            }
        ]
    },
    {
        "header": "Appendix BMore Details on Applications",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16317/x14.png",
                "caption": "Figure 15:Visual results of part segmentation using a finetuned version of NaTex-2B. We provide a 2D mask as the input image for the given geometry, and NaTex textures the model accordingly.",
                "position": 519
            },
            {
                "img": "https://arxiv.org/html/2511.16317/x15.png",
                "caption": "Figure 16:Illustration of part texturing using NaTex without any additional training. Our model generates textures for different parts without suffering from occlusion issues between them, as shown in the two renders with varying part arrangements.",
                "position": 537
            },
            {
                "img": "https://arxiv.org/html/2511.16317/x16.png",
                "caption": "Figure 17:Visual examples of additional part texturing results generated by NaTex.",
                "position": 540
            },
            {
                "img": "https://arxiv.org/html/2511.16317/x17.png",
                "caption": "Figure 18:Illustration of texture refinement using NaTex with color control. As shown, NaTex effectively corrects errors in the input mesh caused by occluded regions and inconsistencies.",
                "position": 547
            }
        ]
    },
    {
        "header": "Appendix CLimitations and Future Works",
        "images": []
    }
]