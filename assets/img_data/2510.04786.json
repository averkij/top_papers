[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04786/x1.png",
                "caption": "Figure 1:Test-time curricula (TTCs) lead to remarkable improvements in math and coding by practicing onself-curatedtask-related problems at test-time.The plots show the pass@1 test accuracy ofQwen3-8Bthroughout its test-time training. Our method, TTC-RL (solid red line), consistently improves performance, learning faster and achieving a higher final accuracy than standard RL post-training (dashed gray line). Notably, the final pass@1 accuracy of TTC-RL approaches the model’s initial pass@8 performance (dotted gray line), which represents a proxy for the performance ceiling of the initial model. The stars indicate the final pass@8 values after TTC-RL, demonstrating a significant improvement over the initial pass@8, which indicates that the model learns new solution strategies at test-time.",
                "position": 200
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04786/x2.png",
                "caption": "Figure 2:TTC-RL performs targeted practice on similar problems to the target task at test-time.The agent is given a target task (red) and self-curates a curriculum of related tasks (blue). It then explores solution strategies on this curriculum, reinforcing successful approaches (✓\\checkmark). This experience enables the agent to more effectively solve the original, more difficult target task.",
                "position": 226
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Test-Time Curricula",
        "images": []
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04786/x3.png",
                "caption": "Figure 3:TTC-RL substantially outperforms general-purpose RL post-training for a range of data sizes. We evaluateQwen3-8Bon all seven benchmarks and report the average test accuracy when training for 250 steps.",
                "position": 553
            },
            {
                "img": "https://arxiv.org/html/2510.04786/x4.png",
                "caption": "Figure 4:TTC-RL scales test-time compute in way that is complementary to other means of test-time scaling.Left:The pass@kkof TTC-RL onQwen3-8B, averaged over benchmarks, increases substantially for small and largekk, indicating that TTC-RL raises the model’s performance ceiling.Middle:TTC-RL also improves the performance of majority voting (across math and GPQA-D), with the initial pass@1 significantly outperforming maj@64 on the initial model.Right:We evaluateQwen3-8Bin non-thinking and thinking mode, as well as the non-thinking model + TTC-RL. The color indicates the relative accuracy per column. We find that TTC-RL significantly improves the non-thinking model, allowing it to perform close to the thinking variant in several domains, despite reasoning over 8k rather than 30k context tokens.",
                "position": 589
            },
            {
                "img": "https://arxiv.org/html/2510.04786/x5.png",
                "caption": "Figure 5:Left: Per-task TTC-RL outperforms a benchmark-level TTC in AIME25.We perform TTC-RL and maj-TTRL (cf.Section˜5.2) onQwen3-8B, and find that per-task TTC-RL even outperforms the benchmark-level TTC.Middle: TTC-RL improves “correctness” of reasoning, not only learning the answer format.We evaluate the difference in accuracy between TTC-RL and the initialQwen3-8B, averaged over benchmarks. The latent improvement is a lower bound on the accuracy gain that is not due to merely learning the format (cf.Section˜5.1).Right: TTC-RL yields models that are specialized to their target tasks.We plot the accuracy ofQwen3-8Btrained for given target tasks (rows) when evaluated on other benchmarks (columns).\nWe normalize accuracies across all evaluations of a particular benchmark. Notably, the model trained via TTC-RL for the “right” target tasks (i.e., the diagonal) always performs best.",
                "position": 646
            }
        ]
    },
    {
        "header": "5Further Analysis",
        "images": []
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "Contributions",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendices",
        "images": []
    },
    {
        "header": "Contents",
        "images": []
    },
    {
        "header": "Appendix AWhy Imitation Learning is ill-suited for TTC’s",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04786/x6.png",
                "caption": "Figure 6:Training on the test set with SFT leads to an initial accuracy drop, indicating that SFT is ill-suited for TTT.",
                "position": 2018
            },
            {
                "img": "https://arxiv.org/html/2510.04786/x7.png",
                "caption": "Figure 7:Training Llama-3.2-1B-Instruct on the GSM8K test set with SFT. After the first full epoch, the performance hasdroppedsignificantly compared to the initial model, before then overfitting to the SFT data.",
                "position": 2046
            }
        ]
    },
    {
        "header": "Appendix BBackground",
        "images": []
    },
    {
        "header": "Appendix CAutobalancing Achievability with TTC’s",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04786/x8.png",
                "caption": "Figure 8:Comparison of train and test accuracy of standard TTC-RL vs. A-TTC-RL averaged across math benchmarks (MATH500, AIME24, AIME25) on theQwen3-0.6Bmodel.",
                "position": 2288
            }
        ]
    },
    {
        "header": "Appendix DExtended Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04786/x9.png",
                "caption": "Figure 9:Increasing theϵhigh\\epsilon_{\\text{high}}to0.280.28prevents the collapse of policy entropy and leads to strong performance on the test set. We plot the test accuracy and the policy entropy over the course of the training for various values ofϵhigh\\epsilon_{\\text{high}}on theQwen3-8Bmodel trained on the Codeforces dataset.\nGRPO’s default value isϵhigh\\epsilon_{\\text{high}}.",
                "position": 2408
            },
            {
                "img": "https://arxiv.org/html/2510.04786/x10.png",
                "caption": "Figure 10:TTC-RL shows strong improvements over standard RL Post-Training across most considered models on the math and coding benchmarks. We plot the individual performance of all considered models on the main benchmarks.",
                "position": 2427
            },
            {
                "img": "https://arxiv.org/html/2510.04786/x11.png",
                "caption": "Figure 11:Restricting RL post-training to include only problems in a code environment explains only a fraction of the improvement on challenging coding tasks (Codeforces, CodeElo) seen by TTC-RL.",
                "position": 2444
            },
            {
                "img": "https://arxiv.org/html/2510.04786/x12.png",
                "caption": "Figure 12:Combining TTC-RL and Maj-TTRL combines the strengths of both methods and yields the strongest results on all math benchmarks. We show the results on theQwen3-8Bfor math.",
                "position": 2611
            },
            {
                "img": "https://arxiv.org/html/2510.04786/x13.png",
                "caption": "Figure 13:Improvement of TTC-RL over RL post-training across several models.",
                "position": 2726
            },
            {
                "img": "https://arxiv.org/html/2510.04786/x14.png",
                "caption": "Figure 14:Training on the test set vs TTC-RL (Codeforces & AIME25).",
                "position": 2736
            },
            {
                "img": "https://arxiv.org/html/2510.04786/x15.png",
                "caption": "Figure 15:Continued training with a decreased clip-high parameter (ϵhigh=0.2\\epsilon_{\\text{high}}=0.2) does not yield improved performance. We plot the average performance averaged over the main math, code and general reasoning benchmarks on theQwen3-8Bmodel.",
                "position": 3002
            }
        ]
    },
    {
        "header": "Appendix EExperiment Details",
        "images": []
    },
    {
        "header": "Appendix FQualitative Examples",
        "images": []
    }
]