[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2In-place Feedback",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.00777/x1.png",
                "caption": "Figure 1:Illustration of common failure cases in multi-turn refinement and in-place feedback. After in-place feedback, the LLM continues generation from the green word “requires”.",
                "position": 207
            },
            {
                "img": "https://arxiv.org/html/2510.00777/x2.png",
                "caption": "Figure 2:Representative examples of in-place feedback on a toy problem. Red marks incorrect reasoning, blue indicates the user corrections with in-place feedback, and green shows the subsequent reasoning based on the corrected context. Additional examples are provided inAppendixC.1.",
                "position": 220
            },
            {
                "img": "https://arxiv.org/html/2510.00777/x3.png",
                "caption": "Figure 3:Comparison of in-place and multi-turn accuracies across models in MATH-hard, MMLU-pro, and GPQA. Across all datasets and LLM models, our in-place feedback approach consistently outperforms the multi-turn based feedback approach.",
                "position": 241
            }
        ]
    },
    {
        "header": "3Empirical Study on the Effect of In-place Feedback",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.00777/x4.png",
                "caption": "Figure 4:Number of input and generated tokens across multiple turns. In-place feedback consistently requires fewer tokens than multi-turn feedback across all datasets and LLMs.",
                "position": 276
            }
        ]
    },
    {
        "header": "4Feedback effectiveness in controlled experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.00777/x5.png",
                "caption": "Figure 5:Grid and cell accuracy of LLMs on the Zebralogic dataset. Across both top-2 and top-4 feedback settings, in-place feedback consistently outperforms multi-turn feedback.",
                "position": 376
            },
            {
                "img": "https://arxiv.org/html/2510.00777/x6.png",
                "caption": "(a)Top-2 feedback",
                "position": 379
            },
            {
                "img": "https://arxiv.org/html/2510.00777/x6.png",
                "caption": "(a)Top-2 feedback",
                "position": 382
            },
            {
                "img": "https://arxiv.org/html/2510.00777/x7.png",
                "caption": "(b)Top-4 feedback",
                "position": 387
            },
            {
                "img": "https://arxiv.org/html/2510.00777/x8.png",
                "caption": "Figure 7:Grid and cell accuracy of LLMs on the ZebraLogic dataset without accumulated history compared against the in-place feedback. Multi-turn (w/o) shows the accuracy of responses only on the previous answer and the feedback,i.e.,y~t+1=ℳ​(x,yt,ft)\\tilde{y}_{t+1}=\\mathcal{M}(x,y_{t},f_{t}), without accumulated history.",
                "position": 427
            },
            {
                "img": "https://arxiv.org/html/2510.00777/x9.png",
                "caption": "Figure 8:Change of FAR with respect to the number of incorrect cells. The x-axis denotes the number of incorrect cells in the previous LLM response across the entire puzzle, and FAR is measured under the setting where feedback for up to four cells is provided.",
                "position": 443
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperimental Details",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.00777/x10.png",
                "caption": "Figure A1:Grid and cell accuracy of LLMs on the Zebralogic dataset. In-place feedback consistently outperforms multi-turn feedback under an oracle setting.",
                "position": 1057
            },
            {
                "img": "https://arxiv.org/html/2510.00777/x11.png",
                "caption": "Figure A2:Correctness-Preserving Rate (CPR), Feedback Acceptance Rate (FAR), and Comparison of Correction Through Reasoning Ratio (CTRR) for 10-turn conversations of LLMs on the ZebraLogic. The Oracle feedback function is used. The points with a black border represent the second response of the LLMs (i.e.,y1y_{1}), and the subsequent responses across turns are connected by lines.",
                "position": 1060
            }
        ]
    },
    {
        "header": "Appendix CQualitative Examples",
        "images": []
    },
    {
        "header": "Appendix DThe Use of Large Language Models",
        "images": []
    }
]