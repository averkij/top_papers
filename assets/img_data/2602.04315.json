[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04315/x1.png",
                "caption": "Figure 1:Overview of GeneralVLA, VLAs and earlier imitation learning methods.GeneralVLA’s hierarchical design results in better generalization.\nIt enables 3D trajectory planning framework that fully exploits the prior knowledge of foundation models.",
                "position": 122
            }
        ]
    },
    {
        "header": "IIRelated Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04315/x2.png",
                "caption": "Figure 2:Inference workflow of of GeneralVLA.(a) The high-level ASM is called to generate the 2D points and corresponding semantic information. (b) The mid-level Knowledge-Guided Trajectory Planning carries out task understanding, 3D reasoning and planning to produce a 3D path indicating the desired robot end-effector trajectory.\n(c) The intermediate 3D path prediction is then served as guidance to the low-level, 3D-aware control policy enhanced by HGM for precise manipulation.",
                "position": 187
            }
        ]
    },
    {
        "header": "IIIThe Proposed GeneralVLA",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04315/x3.png",
                "caption": "Figure 3:Detailed framework of ASM and 3DAgent.(a) Given the input image and task text as query, the multimodal LLM (e.g., LLaVA[36]) generates text output. The last-layer embedding for the<SEG>token is then decoded into the segmentation mask via the decoder. We use LoRA[19]for efficient fine-tuning. The choice of vision backbone can be flexible (e.g., SAM3[7]).",
                "position": 227
            },
            {
                "img": "https://arxiv.org/html/2602.04315/x4.png",
                "caption": "Figure 4:Example GeneralVLA rolloutsdemonstrate its strong performance in multi-object, multi-stage scenes, achieved by leveraging ASM’s segmentation capability, 3DAgent’s spatial reasoning ability, and the robust execution of the low-level 3D policy.",
                "position": 265
            }
        ]
    },
    {
        "header": "IVExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04315/x5.png",
                "caption": "Figure 5:GeneralVLA is an open-vocabulary robot demonstration generation system. We show zero-shot demonstrations for 4 tasks in the real world.",
                "position": 789
            }
        ]
    },
    {
        "header": "VAblation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04315/x6.png",
                "caption": "Figure 6:The multi-view robustness of ASM.This assist 3DAgent in reasoning about the direction for pulling out the block.",
                "position": 871
            },
            {
                "img": "https://arxiv.org/html/2602.04315/x7.png",
                "caption": "Figure 7:Scaling experiment.Scaling effect of model performance with increasing training demonstrations.",
                "position": 964
            }
        ]
    },
    {
        "header": "VIConclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "VIIVLM Finetuning Dataset Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04315/x8.png",
                "caption": "Figure 8:Dataflow diagram of GeneralVLA.The inputs of GeneralVLA are a depth map and an RGB image, while the output is the motion trajectory of the robotic arm’s end-effector.",
                "position": 2175
            }
        ]
    },
    {
        "header": "VIIIImplementation and Architecture Details",
        "images": []
    },
    {
        "header": "IXExperiments Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04315/figs/rlbench.png",
                "caption": "Figure 9:Simulation scene setup.",
                "position": 2217
            },
            {
                "img": "https://arxiv.org/html/2602.04315/x9.png",
                "caption": "Figure 10:Task execution in four real-world scenarios, with each column representing one task. From left to right, the images illustrate the execution process of the four tasks listed inTab.V",
                "position": 2287
            }
        ]
    },
    {
        "header": "XPrompts",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04315/x10.png",
                "caption": "Figure 11:Performance Distribution of GeneralVLA in play jenga task.",
                "position": 2502
            }
        ]
    },
    {
        "header": "XIFailure Analysis",
        "images": []
    }
]