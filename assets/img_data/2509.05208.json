[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.05208/x1.png",
                "caption": "Figure 1:Qualitative results of symbolic graphics programming. We use reinforcement learning with customized verifiable reward to finetune Qwen-2.5-7B. As training progresses, we can observe that the model acquires better compositional drawing ability, producing semantically accurate symbolic graphics programs.",
                "position": 696
            },
            {
                "img": "https://arxiv.org/html/2509.05208/x2.png",
                "caption": "Figure 2:(a) Symbolic controllability of SGPs: to generate a “regular octagon”, SGPs can deliver precise representation and fine-grained controllability, in contrast to the result from Qwen-Image[32]. (b) Procedural generation: this example illustrates the procedural generation of a park scene, where items are represented at different steps.",
                "position": 742
            },
            {
                "img": "https://arxiv.org/html/2509.05208/x3.png",
                "caption": "Figure 3:Overview of the proposed SGP-GenBench and some examples.",
                "position": 803
            },
            {
                "img": "https://arxiv.org/html/2509.05208/x4.png",
                "caption": "Figure 4:An illustration of the RL pipeline. Given a text description, we sample a group of SVG codes from the model and render them as images. Each SVG code is scored by the alignment between the rendered image and the text description. The advantages are calculated based on the scores, and used for updating the model.",
                "position": 969
            },
            {
                "img": "https://arxiv.org/html/2509.05208/x5.png",
                "caption": "Figure 5:Qualitative comparison of SVGs generated by frontier LLMs and our RL-trained model. Our model achieves results comparable in quality to state-of-the-art commercial models, while generating graphics that are more natural and detailed.",
                "position": 1036
            },
            {
                "img": "https://arxiv.org/html/2509.05208/x6.png",
                "caption": "(a)Best-of-NNmetrics vs.log⁡N\\log N",
                "position": 1907
            },
            {
                "img": "https://arxiv.org/html/2509.05208/x6.png",
                "caption": "(a)Best-of-NNmetrics vs.log⁡N\\log N",
                "position": 1910
            },
            {
                "img": "https://arxiv.org/html/2509.05208/x7.png",
                "caption": "(b)Δ\\Delta(Best-of-NN) metrics vs.log⁡N\\log N",
                "position": 1915
            },
            {
                "img": "https://arxiv.org/html/2509.05208/x8.png",
                "caption": "(a)Evolution of elements count",
                "position": 1937
            },
            {
                "img": "https://arxiv.org/html/2509.05208/x8.png",
                "caption": "(a)Evolution of elements count",
                "position": 1940
            },
            {
                "img": "https://arxiv.org/html/2509.05208/x9.png",
                "caption": "(b)Evolution of code length",
                "position": 1945
            },
            {
                "img": "https://arxiv.org/html/2509.05208/x10.png",
                "caption": "Figure 8:Although both express the concept of “motorcycle,” the early-stage model at training step 30 only divides the concept into four levels, with poor semantic representation and inaccurate relative positioning across levels. By step 900, the later model splits the concept into eight parts, achieving accurate semantics and precise positional encoding at every level.",
                "position": 1952
            },
            {
                "img": "https://arxiv.org/html/2509.05208/x11.png",
                "caption": "Figure 9:Examples of optional details generated without explicit prompting. (a) Given only “A group of people sit at a table with cake”, the model adds sprinkles on the cake. (b) Given only a beach-related prompt, the model introduces waves, sand, and a surfer.\nThese unrequested elements are consistent with the scene and enhance its naturalness and completeness.",
                "position": 1965
            },
            {
                "img": "https://arxiv.org/html/2509.05208/x12.png",
                "caption": "(a)Comment-to-element ratio vs. training step",
                "position": 1969
            },
            {
                "img": "https://arxiv.org/html/2509.05208/x12.png",
                "caption": "(a)Comment-to-element ratio vs. training step",
                "position": 1972
            },
            {
                "img": "https://arxiv.org/html/2509.05208/x13.png",
                "caption": "(b)Optional comment ratio vs. training step",
                "position": 1977
            },
            {
                "img": "https://arxiv.org/html/2509.05208/x14.png",
                "caption": "Table 5:Color choices under different reward models. CLIP prefers canonical colors like red and blue, while SigLIP prefers low-saturation colors like #948E8F.",
                "position": 2006
            },
            {
                "img": "https://arxiv.org/html/2509.05208/x15.png",
                "caption": "",
                "position": 2072
            },
            {
                "img": "https://arxiv.org/html/2509.05208/x16.png",
                "caption": "",
                "position": 2091
            },
            {
                "img": "https://arxiv.org/html/2509.05208/x17.png",
                "caption": "",
                "position": 2110
            },
            {
                "img": "https://arxiv.org/html/2509.05208/x18.png",
                "caption": "Figure 11:Evolution of SVG element type distribution throughout training. The numbers behind each legend denotes the average numbers of this element per SVG code across steps.",
                "position": 4402
            },
            {
                "img": "https://arxiv.org/html/2509.05208/img/supp/inside.png",
                "caption": "Figure 12:Qualitative examples from the model that draws a full elephant whose body extends outside theviewBox(right). The renderer clips the excess, yielding a neatly framed composition (left).",
                "position": 4424
            },
            {
                "img": "https://arxiv.org/html/2509.05208/img/supp/WechatIMG88.jpg",
                "caption": "",
                "position": 4427
            },
            {
                "img": "https://arxiv.org/html/2509.05208/x19.png",
                "caption": "Table 11:Evolution of our RL-trained model’s Chain-of-Thought on the caption“Person driving a plated motorcycle on a track with people watching.”",
                "position": 4438
            },
            {
                "img": "https://arxiv.org/html/2509.05208/x20.png",
                "caption": "",
                "position": 4488
            },
            {
                "img": "https://arxiv.org/html/2509.05208/x21.png",
                "caption": "",
                "position": 4504
            },
            {
                "img": "https://arxiv.org/html/2509.05208/x22.png",
                "caption": "",
                "position": 4520
            },
            {
                "img": "https://arxiv.org/html/2509.05208/x23.png",
                "caption": "",
                "position": 4536
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]