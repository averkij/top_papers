[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.05707/x1.png",
                "caption": "Figure 1:Multiagent finetuning improves reasoning performance over multiple rounds of finetuning.\nOur multiagent finetuning procedure enables models to improve across multiple iterations of finetuing. Results reported on the MATH dataset.",
                "position": 147
            }
        ]
    },
    {
        "header": "2Multiagent Finetuning of Language Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.05707/x2.png",
                "caption": "Figure 2:Overview of Multiagent Finetuning.We first use multiagent debate and majority voting to create the finetuning datasets (left). These datasets are then used to finetune the generation and critic agents (right). When finetuning generation models, we use the majority voted result (”correct” output) to select first-round responses from each agent. We then finetune critic models using responses from the final round based on whether responses match the majority voted result (mix of ”correct and incorrect” outputs). The finetuned models are combined through multiagent debate to generate more accurate answers. In this figure, we illustrate a single finetuning iteration. Applying multiple rounds of finetuning iterations can significantly boost performance.",
                "position": 165
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": []
    },
    {
        "header": "4Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.05707/x3.png",
                "caption": "Figure 3:Diversity is preserved and can improve across iterations of finetuning.We measure the response diversity of our method and the single-agent finetuning method on the MATH dataset using two diversity measures. The diversity of our method remains consistent over finetuning iterations for one metric and improves for another metric, whereas the diversity of the single-agent method drops significantly.",
                "position": 734
            },
            {
                "img": "https://arxiv.org/html/2501.05707/x4.png",
                "caption": "Figure 4:Relationship between accuracy and diversity.We visualize the relationship between embedding dissimilarity and MATH accuracy across rounds of finetuning. Our multiagent finetuning preserves diversity across rounds of finetuning while improving accuracy.",
                "position": 771
            },
            {
                "img": "https://arxiv.org/html/2501.05707/x5.png",
                "caption": "Figure 5:Zero-shot generalization of the proposed method. Our method demonstrates zero-shot generalization capabilities. When trained on the MATH dataset, it can effectively generalize to the GSM dataset. It outperforms all the baselines that are trained on the GSM dataset.",
                "position": 785
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion and Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix Summary",
        "images": []
    },
    {
        "header": "Appendix BMethodology Details",
        "images": []
    },
    {
        "header": "Appendix CDiversity Metrics",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.05707/x6.png",
                "caption": "Figure 6:Consensus: Response diversity across finetuning iterations. We measure the response diversity based on agent consensus of our method and the single-agent finetuning method on the MATH dataset. The diversity of our method remains consistent over finetuning iterations, whereas the diversity of the single-agent method drops significantly.",
                "position": 1583
            },
            {
                "img": "https://arxiv.org/html/2501.05707/x7.png",
                "caption": "Figure 7:KL-Divergence: Response diversity across finetuning iterations. We measure diversity based on the KL-divergence between the probabilities of the output tokens between agents. Similar to our likelihood measurement, we find that diversity is preserved across rounds of finetuning.",
                "position": 1607
            },
            {
                "img": "https://arxiv.org/html/2501.05707/x8.png",
                "caption": "Figure 8:KL Diversity between finetuned and unfinetuned LLM.We measure the KL-divergence between likelihoods of responses from finetuned agents and base LLM agents for single-agent finetuning and generation/critic agents from multiagent finetuning. Likelihoods are calculated using Gemma-2 (2B). We find that our method diverges from the base LLM probabilities and furthermore, critic agents have better divergence in responses and our method has better diversity metrics than single-agent FT.",
                "position": 1620
            },
            {
                "img": "https://arxiv.org/html/2501.05707/x9.png",
                "caption": "Figure 9:Embedding Dissimilarity: Response diversity across finetuning iterations.We measure the response diversity based on the embedding dissimilarity between the responses of different agents, where embeddings are computed using the T5-3B encoder. We notice that similar to likelihood measurement, we find that diversity is preserved across rounds of finetuning.",
                "position": 1636
            }
        ]
    },
    {
        "header": "Appendix DCooperative Finetuning",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.05707/x10.png",
                "caption": "Figure 10:Inducing diversity through increasing temperature. We introduce an additional baseline where we apply the Single-Agent FT baselin with a temperature of 2. By increasing the sampling temperature, we allow the model to generate more diverse responses. We observe that our method out-performs higher temperature settings, which demonstrates that temperature does not increase diversity in a way that is useful for accuracy.",
                "position": 1687
            }
        ]
    },
    {
        "header": "Appendix EAdditional Comparisons",
        "images": []
    },
    {
        "header": "Appendix FAdditional Agents in Debate",
        "images": []
    },
    {
        "header": "Appendix GMathematical Model of Diversity Over Rounds of Finetuning",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.05707/x11.png",
                "caption": "Figure 11:Multiple iterations of finetuning over all levels of MATH. We apply multiple iterations of finetuning over 500 examples of MATH sampled from all levels. Even over a more difficult domain, we see significant improvements from multiagent finetuning that continue to self-improve.",
                "position": 1913
            },
            {
                "img": "https://arxiv.org/html/2501.05707/x12.png",
                "caption": "Figure 12:Testing zero-shot generalization across 1000 GSM problemsWe test the zero-shot capabilities of our method using models trained on the MATH dataset. We find that over 1000 problems of GSM, our method performs better than all baselines.",
                "position": 1976
            },
            {
                "img": "https://arxiv.org/html/2501.05707/x13.png",
                "caption": "Figure 13:Zero-shot generalization after arithmetic finetuning. We evaluate the ability of our method to generalize after finetuning Mistral on the arithmetic task and evaluating on GSM. We find that this aids in GSM performance, even more than finetuning with MATH.",
                "position": 1979
            }
        ]
    },
    {
        "header": "Appendix HAdditional Evaluations",
        "images": []
    }
]