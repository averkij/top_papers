[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10592/figures/scaling_curve.png",
                "caption": "Figure 1:Scaling onAction100Mimproves zero-shot action recognition consistently.",
                "position": 116
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Action100M Data Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10592/figures/data_pipeline.png",
                "caption": "Figure 2:Action100MData Pipeline.Our pipeline first applies hierarchical temporal segmentation to decompose the video into semantically coherent segments at multiple temporal scales. For each segment, we generate video caption and frame captions, capturing both temporal and spatial information. Next, we prompt LLM to aggregate the captions, extracting final annotations.",
                "position": 522
            },
            {
                "img": "https://arxiv.org/html/2601.10592/figures/trees/NYRlBWgLbKU.jpg",
                "caption": "Figure 3:Example of hierarchical structure in Action100M annotations(with brief action description labels shown). Source video:url.Brief captionof the entire video:A woman roasts almonds, blends them into butter, and pours the butter into a jar.Detailed caption:The video opens with the presenter in a bright kitchen speaking to the camera. She spreads raw almonds on a parchment‑lined tray, places the tray in a pre‑heated 350 °F oven, and after roasting lets the nuts cool to room temperature. She then transfers the almonds to a Vitamix blender, removes the lid, inserts a tamper, and sets the machine on high. While the blender runs she presses the almonds down with the tamper, first creating a fine flour and then a thick creamy butter within about one minute. She pours the almond butter into a clear storage jar, scoops it with a large wooden spoon and stirs it to smooth the surface, then concludes the segment with a brief thank‑you.",
                "position": 545
            }
        ]
    },
    {
        "header": "4Dataset Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10592/figures/metadata_distributions.png",
                "caption": "Figure 4:Statistics of Action100M source videos and metadata.Distributions of (left to right) video upload year, view count, video duration, and transcript length, computed over the subset of videos for which metadata is available.",
                "position": 552
            },
            {
                "img": "https://arxiv.org/html/2601.10592/figures/wordcloud.png",
                "caption": "Figure 5:Word cloud of video titles in Action100M.Frequently occurring words reflect the instructional and procedural nature of the dataset, with dominant terms related to cooking, DIY activities, and everyday physical tasks.",
                "position": 555
            },
            {
                "img": "https://arxiv.org/html/2601.10592/figures/word_histograms.png",
                "caption": "Figure 6:Statistics of generated textual annotations in Action100M.(a)Word-count distributions for four annotation types: brief action, brief caption, detailed action, and detailed caption. Dashed lines indicate the mean length of each annotation type.(b-c)Top unigrams, bigrams, and trigrams (excluding stop words) in brief action descriptions and brief video captions.",
                "position": 568
            },
            {
                "img": "https://arxiv.org/html/2601.10592/figures/action_sunburst.png",
                "caption": "Figure 7:Sunburst visualizations of frequent action compositions.Each sunburst shows the hierarchical co-occurrence structure centered on one of the five most frequent verbs in brief action descriptions.",
                "position": 574
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10592/figures/performance_curve_classification.png",
                "caption": "Figure 8:Zero-shot action recognition accuracy in each training stage.Stage 1: image pretraining with single frame input;Stage 2: main Action100M pretraining with 8 frames input;Stage 2: 32 frames input with unfrozen encoder.\nThe x-axis of each stage represents number of training samples seen (i.e.,number of iterations) in log-scale. The performance at the end of each stage is annotated.",
                "position": 1113
            },
            {
                "img": "https://arxiv.org/html/2601.10592/figures/ablation_annotation_type_classification.png",
                "caption": "Figure 9:Performance improvements of stage 2 video pretraining with different data upon stage 1.",
                "position": 1130
            },
            {
                "img": "https://arxiv.org/html/2601.10592/figures/resampling.png",
                "caption": "Figure 10:Effectiveness of semantic resampling.",
                "position": 1143
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10592/x1.png",
                "caption": "Figure 11:Distribution of duplicated brief action descriptions.",
                "position": 2337
            },
            {
                "img": "https://arxiv.org/html/2601.10592/x2.png",
                "caption": "(a)Distribution of texts within clusters fork=103k=10^{3}",
                "position": 2343
            },
            {
                "img": "https://arxiv.org/html/2601.10592/x2.png",
                "caption": "(a)Distribution of texts within clusters fork=103k=10^{3}",
                "position": 2346
            },
            {
                "img": "https://arxiv.org/html/2601.10592/x3.png",
                "caption": "",
                "position": 2350
            },
            {
                "img": "https://arxiv.org/html/2601.10592/x4.png",
                "caption": "(b)Distribution of texts within clusters fork=104k=10^{4}",
                "position": 2357
            },
            {
                "img": "https://arxiv.org/html/2601.10592/x5.png",
                "caption": "",
                "position": 2361
            },
            {
                "img": "https://arxiv.org/html/2601.10592/x6.png",
                "caption": "(c)Distribution of texts within clusters fork=105k=10^{5}",
                "position": 2368
            },
            {
                "img": "https://arxiv.org/html/2601.10592/x7.png",
                "caption": "",
                "position": 2372
            },
            {
                "img": "https://arxiv.org/html/2601.10592/x8.png",
                "caption": "Figure 13:UMAP visualization of the overlap between the semantic clusters and downstream datasets.Each panel containing samples from a specific downstream dataset (colored points) and their overlap with thek=104k=10^{4}cluster of theAction100Mdataset. This highlights the diversity and coverage of theAction100Mdataset with respect to multiple downstream benchmarks.",
                "position": 2383
            }
        ]
    },
    {
        "header": "Appendix BStatistics of Duplications and Semantic Resampling",
        "images": []
    }
]