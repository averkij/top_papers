[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16561/x1.png",
                "caption": "",
                "position": 65
            },
            {
                "img": "https://arxiv.org/html/2512.16561/x2.png",
                "caption": "",
                "position": 90
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3The Proposed Framework: N3D-VLM",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16561/x3.png",
                "caption": "Figure 2:Illustration of our data construction pipeline.We first lift annotations from existing 2D detection datasets with diverse object categories into 3D space, resulting in a large-scale and category-rich 3D detection annotation repository. Based on this repository, we generate data for 3D detection, 3D grounding, and 3D spatial reasoning QA tasks.",
                "position": 178
            },
            {
                "img": "https://arxiv.org/html/2512.16561/x4.png",
                "caption": "Figure 3:Illustration of our model design and quantitative comparison.(a) Overview of our model architecture and the cascaded spatial reasoning process. (b) Quantitative comparison showing that our model outperforms existing methods. (c) Definition of structured language representation for 3D bounding boxes.",
                "position": 200
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16561/x5.png",
                "caption": "Figure 4:Qualitative comparison of 3D grounding capability with Qwen3-VL-8B[29].Compared to Qwen3-VL-8B, our N3D-VLM generates 3D bounding boxes that more accurately close to the ground truth, reflecting stronger 3D understanding and localization precision. In the visualization,green boxesrepresent ground truth 3D bounding boxes, andred boxesindicate model’s predictions.",
                "position": 600
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16561/x6.png",
                "caption": "Figure 5:Qualitative comparison of 3D grounding capability with SpatialLM[22]and Qwen3-VL-8B[29]in indoor scenes.our N3D-VLM accurately localizes objects such as pillows and washing machines, while baselines either miss objects or exhibit inaccurate prediction. In the visualization,green boxesrepresent ground truth 3D bounding boxes, andred boxesindicate model’s predictions.",
                "position": 1278
            },
            {
                "img": "https://arxiv.org/html/2512.16561/x7.png",
                "caption": "Figure 6:Qualitative comparison of 3D grounding capability with SpatialLM[22]and Qwen3-VL-8B[29]in outdoor scenes.Our N3D-VLM outperforms Qwen3-VL-8B and SpatialLM, highlighting its superior native 3D grounding capability for reliable spatial reasoning. In the visualization,green boxesrepresent ground truth 3D bounding boxes, andred boxesindicate model’s predictions.",
                "position": 1281
            },
            {
                "img": "https://arxiv.org/html/2512.16561/x8.png",
                "caption": "Figure 7:Qualitative comparison of 3D spatial reasoning capability with GPT-4o[11]and Qwen3-VL-8B[29].Our N3D-VLM outperforms GPT-4o and Qwen3-VL-8B by leveraging accurate 3D grounding and 3D spatial reasoning.",
                "position": 1299
            },
            {
                "img": "https://arxiv.org/html/2512.16561/x9.png",
                "caption": "Figure 8:Qualitative comparison of 3D spatial reasoning capability with SpatialRGPT[8]and SpatialReasoner[21].N3D-VLM outperforms SpatialRGPT and SpatialReasoner by accurately interpreting the question and reasoning over explicit 3D bounding boxes.",
                "position": 1302
            },
            {
                "img": "https://arxiv.org/html/2512.16561/x10.png",
                "caption": "Figure 9:Failure cases of our native 3D grounding.N3D-VLM misidentifies a specular reflection and misses several objects, suggesting room for improvement in handling reflections and dense object scenes.",
                "position": 1667
            }
        ]
    },
    {
        "header": "Appendix BVideo Demo",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16561/x11.png",
                "caption": "Figure 10:Distribution of object classes in N3D-Bench.",
                "position": 1690
            },
            {
                "img": "https://arxiv.org/html/2512.16561/x12.png",
                "caption": "Figure 11:Distribution of question types in N3D-Bench.",
                "position": 1694
            }
        ]
    },
    {
        "header": "Appendix CN3D-Bench Details",
        "images": []
    }
]