[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.18392/x1.png",
                "caption": "Figure 1:TheCLEARFramework.(a) Pipeline- Given a dataset (Dğ·Ditalic_D) and a target system (sğ‘ sitalic_s), the system generates responses (Rğ‘…Ritalic_R). A judge (Jğ½Jitalic_J) provides per-instance textual feedback and a score ({ji}i=1Nsuperscriptsubscriptsubscriptğ‘—ğ‘–ğ‘–1ğ‘\\{j_{i}\\}_{i=1}^{N}{ italic_j start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT). A Key Point Analysis module (Kğ¾Kitalic_K) extracts recurring issues and maps them to the individualjisubscriptğ‘—ğ‘–j_{i}italic_j start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTâ€™s. The discovered issues can be explored via the UI (b).",
                "position": 169
            }
        ]
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.18392/x2.png",
                "caption": "Figure 2:The figure presents the key components of theCLEARtool for analyzing model evaluation results. (a) Entry point to the interface for exploring the model results and issues. (b) Issues View visualizes the distribution of detected model errors. (c) The Filtering Mechanism allows filtering based on issue types and scores to isolate relevant examples. (d) Comparison View contrasts issue frequencies between the full dataset and filtered subsets, highlighting co-occurrence patterns. (e/f) Model Behavior and Instance-Level View offer detailed, example-level insights to facilitate fine-grained error analysis and model diagnosis.",
                "position": 221
            }
        ]
    },
    {
        "header": "3CLEARFramework",
        "images": []
    },
    {
        "header": "4CLEAR: Case Study",
        "images": []
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion and Future work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitations",
        "images": []
    },
    {
        "header": "Appendix BMethod Comparison",
        "images": []
    },
    {
        "header": "Appendix CImpact of evaluation mode",
        "images": []
    },
    {
        "header": "Appendix DUser Study Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.18392/extracted/6649968/Figures/study_inst.png",
                "caption": "Figure 3:Instructions to the study participants.",
                "position": 1408
            },
            {
                "img": "https://arxiv.org/html/2507.18392/extracted/6649968/Figures/study_use.png",
                "caption": "Figure 4:Section 1- Usefulness questions.",
                "position": 1411
            },
            {
                "img": "https://arxiv.org/html/2507.18392/extracted/6649968/Figures/study_compare.png",
                "caption": "Figure 5:Section 2- Comparative value questions.",
                "position": 1414
            },
            {
                "img": "https://arxiv.org/html/2507.18392/extracted/6649968/Figures/study_trust.png",
                "caption": "Figure 6:Section 3- Trust & Reliability questions.",
                "position": 1417
            }
        ]
    },
    {
        "header": "Appendix EImplementation Details",
        "images": []
    }
]