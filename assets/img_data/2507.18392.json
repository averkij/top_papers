[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.18392/x1.png",
                "caption": "Figure 1:TheCLEARFramework.(a) Pipeline- Given a dataset (D𝐷Ditalic_D) and a target system (s𝑠sitalic_s), the system generates responses (R𝑅Ritalic_R). A judge (J𝐽Jitalic_J) provides per-instance textual feedback and a score ({ji}i=1Nsuperscriptsubscriptsubscript𝑗𝑖𝑖1𝑁\\{j_{i}\\}_{i=1}^{N}{ italic_j start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT). A Key Point Analysis module (K𝐾Kitalic_K) extracts recurring issues and maps them to the individualjisubscript𝑗𝑖j_{i}italic_j start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT’s. The discovered issues can be explored via the UI (b).",
                "position": 169
            }
        ]
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.18392/x2.png",
                "caption": "Figure 2:The figure presents the key components of theCLEARtool for analyzing model evaluation results. (a) Entry point to the interface for exploring the model results and issues. (b) Issues View visualizes the distribution of detected model errors. (c) The Filtering Mechanism allows filtering based on issue types and scores to isolate relevant examples. (d) Comparison View contrasts issue frequencies between the full dataset and filtered subsets, highlighting co-occurrence patterns. (e/f) Model Behavior and Instance-Level View offer detailed, example-level insights to facilitate fine-grained error analysis and model diagnosis.",
                "position": 221
            }
        ]
    },
    {
        "header": "3CLEARFramework",
        "images": []
    },
    {
        "header": "4CLEAR: Case Study",
        "images": []
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion and Future work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitations",
        "images": []
    },
    {
        "header": "Appendix BMethod Comparison",
        "images": []
    },
    {
        "header": "Appendix CImpact of evaluation mode",
        "images": []
    },
    {
        "header": "Appendix DUser Study Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.18392/extracted/6649968/Figures/study_inst.png",
                "caption": "Figure 3:Instructions to the study participants.",
                "position": 1408
            },
            {
                "img": "https://arxiv.org/html/2507.18392/extracted/6649968/Figures/study_use.png",
                "caption": "Figure 4:Section 1- Usefulness questions.",
                "position": 1411
            },
            {
                "img": "https://arxiv.org/html/2507.18392/extracted/6649968/Figures/study_compare.png",
                "caption": "Figure 5:Section 2- Comparative value questions.",
                "position": 1414
            },
            {
                "img": "https://arxiv.org/html/2507.18392/extracted/6649968/Figures/study_trust.png",
                "caption": "Figure 6:Section 3- Trust & Reliability questions.",
                "position": 1417
            }
        ]
    },
    {
        "header": "Appendix EImplementation Details",
        "images": []
    }
]