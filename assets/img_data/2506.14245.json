[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.14245/x1.png",
                "caption": "Figure 1:An illustration of our perspective:RLVR implicitly incentivizes correct reasoning in base LLMs.\nWe visualize how different explanation frameworks lead to varying reasoning paths being activated, with our perspective shown in the lower left and a recent popular hypothesis explainingPâ¢aâ¢sâ¢sâ¢@â¢Kğ‘ƒğ‘ğ‘ ğ‘ @ğ¾Pass@Kitalic_P italic_a italic_s italic_s @ italic_Kobservations(Yue etÂ al.,2025)summarized in the upper left.\nIn this diagram, the line width represents the sampling probability of a reasoning path, while the color distinguishes correct paths (green) from incorrect ones (red).\nIf all reasoning paths after applying RLVR are already present in the base model, the reasoning model merely adjusts the sampling probabilities of these existing paths (visualized in dashed lines).\nThis hypothesis effectively accounts for the key observation shown in the upper-right part, where, for a moderately largeKğ¾Kitalic_K, a base LLM can catch up to the reasoning model after RLVR using thePâ¢aâ¢sâ¢sâ¢@â¢Kğ‘ƒğ‘ğ‘ ğ‘ @ğ¾Pass@Kitalic_P italic_a italic_s italic_s @ italic_Kmetric.\nHowever, we challenge this hypothesis using a refined metric,Câ¢oâ¢Tğ¶ğ‘œğ‘‡CoTitalic_C italic_o italic_T-Pâ¢aâ¢sâ¢sâ¢@â¢Kğ‘ƒğ‘ğ‘ ğ‘ @ğ¾Pass@Kitalic_P italic_a italic_s italic_s @ italic_K, which emphasizes both the correctness of answers and the validity of the reasoning chains.\nAs demonstrated by experimental evidence in the lower right, we observe that RLVR improvesCâ¢oâ¢Tğ¶ğ‘œğ‘‡CoTitalic_C italic_o italic_T-Pâ¢aâ¢sâ¢sâ¢@â¢Kğ‘ƒğ‘ğ‘ ğ‘ @ğ¾Pass@Kitalic_P italic_a italic_s italic_s @ italic_Kfor all values ofKğ¾Kitalic_K, suggesting thatthe direction of incentivization is improving the CoT correctness fromPâ¢aâ¢sâ¢sâ¢@â¢Kğ‘ƒğ‘ğ‘ ğ‘ @ğ¾Pass@Kitalic_P italic_a italic_s italic_s @ italic_Kof a very largeKğ¾Kitalic_K.",
                "position": 108
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3A Theoretical Foundation of RLVR for LLMs",
        "images": []
    },
    {
        "header": "4Revisiting Pass@K Experiments with CoT-Pass@K",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.14245/x2.png",
                "caption": "Figure 2:Comparisons ofPâ¢aâ¢sâ¢sâ¢@â¢Kğ‘ƒğ‘ğ‘ ğ‘ @ğ¾Pass@Kitalic_P italic_a italic_s italic_s @ italic_K(the top row) andCâ¢oâ¢Tğ¶ğ‘œğ‘‡CoTitalic_C italic_o italic_T-Pâ¢aâ¢sâ¢sâ¢@â¢Kğ‘ƒğ‘ğ‘ ğ‘ @ğ¾Pass@Kitalic_P italic_a italic_s italic_s @ italic_K(the bottom row) on five math benchmarks (different columns) to show different observations of how RLVR incentivizes reasoning in LLMs. Here the base LLM is Qwen2.5-32B, and the post-RLVR model is DAPO-Qwen-32B. ForCâ¢oâ¢Tğ¶ğ‘œğ‘‡CoTitalic_C italic_o italic_T-Pâ¢aâ¢sâ¢sâ¢@â¢Kğ‘ƒğ‘ğ‘ ğ‘ @ğ¾Pass@Kitalic_P italic_a italic_s italic_s @ italic_K, we perform multiple verifications for each CoT using DeepSeek-R1-0528-Qwen3-8B, and display the results determined byany-correct,all-correct, andmajority-correctstrategies, which constitute the shaded area in lower subplots.",
                "position": 732
            }
        ]
    },
    {
        "header": "5Analyzing the Training Dynamics of RLVR",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.14245/x3.png",
                "caption": "(a)Distributions ofPâ¢(Câ¢A)(q)ğ‘ƒsuperscriptğ¶ğ´ğ‘P(CA)^{(q)}italic_P ( italic_C italic_A ) start_POSTSUPERSCRIPT ( italic_q ) end_POSTSUPERSCRIPTandPâ¢(Câ¢C|Câ¢A)(q)ğ‘ƒsuperscriptconditionalğ¶ğ¶ğ¶ğ´ğ‘P(CC|CA)^{(q)}italic_P ( italic_C italic_C | italic_C italic_A ) start_POSTSUPERSCRIPT ( italic_q ) end_POSTSUPERSCRIPTforeasytraining questions in DAPO.",
                "position": 749
            },
            {
                "img": "https://arxiv.org/html/2506.14245/x3.png",
                "caption": "(a)Distributions ofPâ¢(Câ¢A)(q)ğ‘ƒsuperscriptğ¶ğ´ğ‘P(CA)^{(q)}italic_P ( italic_C italic_A ) start_POSTSUPERSCRIPT ( italic_q ) end_POSTSUPERSCRIPTandPâ¢(Câ¢C|Câ¢A)(q)ğ‘ƒsuperscriptconditionalğ¶ğ¶ğ¶ğ´ğ‘P(CC|CA)^{(q)}italic_P ( italic_C italic_C | italic_C italic_A ) start_POSTSUPERSCRIPT ( italic_q ) end_POSTSUPERSCRIPTforeasytraining questions in DAPO.",
                "position": 752
            },
            {
                "img": "https://arxiv.org/html/2506.14245/x4.png",
                "caption": "(b)Distributions ofPâ¢(Câ¢A)(q)ğ‘ƒsuperscriptğ¶ğ´ğ‘P(CA)^{(q)}italic_P ( italic_C italic_A ) start_POSTSUPERSCRIPT ( italic_q ) end_POSTSUPERSCRIPTandPâ¢(Câ¢C|Câ¢A)(q)ğ‘ƒsuperscriptconditionalğ¶ğ¶ğ¶ğ´ğ‘P(CC|CA)^{(q)}italic_P ( italic_C italic_C | italic_C italic_A ) start_POSTSUPERSCRIPT ( italic_q ) end_POSTSUPERSCRIPTforhardtraining questions in DAPO.",
                "position": 760
            },
            {
                "img": "https://arxiv.org/html/2506.14245/x5.png",
                "caption": "(c)Generalization performance on AIME 2024 across different training steps.",
                "position": 768
            },
            {
                "img": "https://arxiv.org/html/2506.14245/x6.png",
                "caption": "Figure 4:RevisitingPâ¢aâ¢sâ¢sâ¢@â¢Kğ‘ƒğ‘ğ‘ ğ‘ @ğ¾Pass@Kitalic_P italic_a italic_s italic_s @ italic_KandCâ¢oâ¢Tğ¶ğ‘œğ‘‡CoTitalic_C italic_o italic_T-Pâ¢aâ¢sâ¢sâ¢@â¢Kğ‘ƒğ‘ğ‘ ğ‘ @ğ¾Pass@Kitalic_P italic_a italic_s italic_s @ italic_Kexperiments on AIME 2024 and AIME 2025 using early and mid-stage checkpoints of our DAPO reproduction.\nThe base LLM and post-RLVR model are Qwen2.5-32B and DAPO-Qwen-32B, respectively.",
                "position": 811
            }
        ]
    },
    {
        "header": "6Discussions",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.14245/x7.png",
                "caption": "Figure 5:An intuitive diagram to illustrate the benefits of our multi-verification system: simultaneously consideringany-correct,all-correct, andmajority-correcthelps us to mitigate false positives and false negatives within individual verifications.",
                "position": 1740
            },
            {
                "img": "https://arxiv.org/html/2506.14245/extracted/6548052/figures/aime24-15.png",
                "caption": "",
                "position": 1895
            },
            {
                "img": "https://arxiv.org/html/2506.14245/extracted/6548052/figures/aime25-2.png",
                "caption": "",
                "position": 2060
            },
            {
                "img": "https://arxiv.org/html/2506.14245/extracted/6548052/figures/aime25-11.png",
                "caption": "",
                "position": 2113
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]