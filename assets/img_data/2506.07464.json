[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07464/x1.png",
                "caption": "Figure 1:DeepVideo-R1 significantly improves the reasoning capabilities of VideoLLMs.Our VideoLLM, DeepVideo-R1, is trained to explicitly predict the advantageA^(i)superscript^ùê¥ùëñ\\hat{A}^{(i)}over^ start_ARG italic_A end_ARG start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPTthrough Regressive GRPO loss.\nNotably, model training becomes significantly effective and achieves a 10.06 performance improvement compared to GRPO.",
                "position": 139
            },
            {
                "img": "https://arxiv.org/html/2506.07464/x2.png",
                "caption": "",
                "position": 142
            }
        ]
    },
    {
        "header": "2Related works",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07464/x3.png",
                "caption": "Figure 2:Overview of the difficulty-aware data augmentation.First, we assess the difficulty of responses given the input video and question by comparing the average reward within the group with the global reward‚Ñõglobalsubscript‚Ñõglobal\\mathcal{R}_{\\text{global}}caligraphic_R start_POSTSUBSCRIPT global end_POSTSUBSCRIPT.\nFor hard samples, it augments the input prompts with the reasoning cues extracted from successful reasoning paths¬†(Difficulty decreasement augmentation), while the easy samples are perturbed with the noise¬†(Difficulty increasement augmentation).\nThe scale of the guidance level or noise level is adaptively determined based on the difficulty of the current sample.",
                "position": 397
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07464/x4.png",
                "caption": "Figure 3:Vanishing advantage ratio comparisonon GRPO and GRPO+DA-Aug¬†(Difficulty-aware augmentation)¬†(Left).Reward curvesof DeepVideo-R1¬†(Ours) and GRPO¬†(Right).",
                "position": 1384
            },
            {
                "img": "https://arxiv.org/html/2506.07464/x4.png",
                "caption": "",
                "position": 1388
            },
            {
                "img": "https://arxiv.org/html/2506.07464/x5.png",
                "caption": "",
                "position": 1392
            },
            {
                "img": "https://arxiv.org/html/2506.07464/x6.png",
                "caption": "Figure 4:Qualitative result of DeepVideo-R1-Qwen2.5-7Bin comparison of Qwen2.5-VL-7B+GRPO.",
                "position": 1435
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADiscussion and Derivation of Reg-GRPO",
        "images": []
    },
    {
        "header": "NeurIPS Paper Checklist",
        "images": []
    }
]