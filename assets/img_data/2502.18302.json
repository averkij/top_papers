[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.18302/x1.png",
                "caption": "",
                "position": 89
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.18302/x2.png",
                "caption": "Figure 2:Overview of LDGen. The dashed box shows our language representation strategy, with the bottom is our LLM alignment and cross-modal refiner training process. The detailed design of the cross-modal refiner is shown in the green box on the right.",
                "position": 141
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.18302/extracted/6232305/fig/encoder_language_distribution_pie_charts.png",
                "caption": "Figure 3:Distribution of text encoder and supported languages. English-based CLIP/T5 series models remain the primary text encoders.",
                "position": 185
            },
            {
                "img": "https://arxiv.org/html/2502.18302/extracted/6232305/fig/human_i.png",
                "caption": "Figure 4:The red words in Sana‚Äôs generated result highlight elements that do not align with the image. Providing incorrect instructions can change the original caption, potentially creating inaccurate descriptions.",
                "position": 189
            },
            {
                "img": "https://arxiv.org/html/2502.18302/x3.png",
                "caption": "Figure 5:Comparison of our method with recent enhancement generative models ELLAHu et¬†al. (2024), baseline Models SDXLPodell et¬†al. (2023)and PixArt-Œ±ùõº\\alphaitalic_Œ±Chen et¬†al. (2023). Our method achieves the best results in terms of instruction adherence and visual appeal.",
                "position": 369
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.18302/extracted/6232305/fig/multi.png",
                "caption": "Figure 6:Multilingual qualitative visualization results. For each panel‚Äôs eight images, we generate them using eight different languages but only display the prompt in one of the languages used. Note that LDGen uses only English prompts during training but achieves zero-shot multilingual generation due to the capabilities of the LLM.",
                "position": 531
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.18302/x4.png",
                "caption": "Figure 7:More comparisons with Pixart-Œ±ùõº\\alphaitalic_Œ±. Our method achieves better results in terms of prompt adherence and visua appeal.",
                "position": 1501
            },
            {
                "img": "https://arxiv.org/html/2502.18302/x5.png",
                "caption": "Figure 8:We provide detailed comparisons using human instructions ranging from simple to complex, comprehensively evaluating the effectiveness of our method.",
                "position": 1519
            },
            {
                "img": "https://arxiv.org/html/2502.18302/x6.png",
                "caption": "Figure 9:More multilingual qualitative visualization results. For each panel‚Äôs eight images, we generate them using eight different languages but only display the prompt in one of the languages used.",
                "position": 1523
            },
            {
                "img": "https://arxiv.org/html/2502.18302/x7.png",
                "caption": "Figure 10:More samples generated from LDGen.",
                "position": 1527
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]