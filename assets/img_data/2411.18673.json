[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.18673/extracted/6029846/images/teaser_horizontal.png",
                "caption": "",
                "position": 162
            },
            {
                "img": "https://arxiv.org/html/2411.18673/extracted/6029846/images/cam0.png",
                "caption": "",
                "position": 167
            },
            {
                "img": "https://arxiv.org/html/2411.18673/extracted/6029846/images/cam1.png",
                "caption": "",
                "position": 177
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.18673/x1.png",
                "caption": "Figure 2:VDiT-CCmodel with ControlNet[187,71]camera conditioning built on top ofVDiT.\nVideo synthesis is performed by large 4,096-dimensionalDiT-XLblocks of the frozen VDiT¬†backbone, while VDiT-CC¬†only processes and injects the camera information through lightweight 128-dimensionalDiT-XSblocks (FCstands for fully-connected layers); seeSection3.2for details.",
                "position": 247
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.18673/x2.png",
                "caption": "Figure 3:Comparing motion spectral volumes for scenes with different motion types.\nVideos with camera motion (purple) exhibit stronger overall motion than the videos with scene motion (orange), especially for the low-frequency range, suggesting that the motion induced by camera transitions is heavily biased towards low-frequency components.",
                "position": 322
            },
            {
                "img": "https://arxiv.org/html/2411.18673/x3.png",
                "caption": "(a)A generated video at different diffusion timesteps. The camera has already been decided by the model even att=0.9ùë°0.9t=0.9italic_t = 0.9(first 10% of the denoising process) and does not change after that.",
                "position": 347
            },
            {
                "img": "https://arxiv.org/html/2411.18673/x3.png",
                "caption": "(a)A generated video at different diffusion timesteps. The camera has already been decided by the model even att=0.9ùë°0.9t=0.9italic_t = 0.9(first 10% of the denoising process) and does not change after that.",
                "position": 350
            },
            {
                "img": "https://arxiv.org/html/2411.18673/x4.png",
                "caption": "(b)Motion spectral volumes ofVDiT‚Äôs generated videos for different diffusion timesteps (left) and their ratio w.r.t. the motion spectral volume att=0ùë°0t=0italic_t = 0(i.e., a fully denoised video).",
                "position": 355
            },
            {
                "img": "https://arxiv.org/html/2411.18673/x5.png",
                "caption": "Figure 5:Video DiT is secretly a camera pose estimator.We perform linear probing of camera poses in each ofVDiTblocks for various noise levels and observe that video DiT performs pose estimation under the hood.\nIts middle blocks carry the most accurate information about the camera locations and orientations, which indicates that the camera signal emerges in the early layers to help the middle and late blocks render other visual features aligned with the viewpoint.",
                "position": 391
            },
            {
                "img": "https://arxiv.org/html/2411.18673/extracted/6029846/images/getty-static/re1.png",
                "caption": "Figure 6:RealEstate10k[198]videos (upper 2 rows) contain diverse camera trajectories, but are strongly biased towards static scenes.\nTo mitigate this bias and also increase the concepts diversity, we curate20202020K videos with stationary cameras, but dynamic content (lower 2 rows).\nSuch dataset is easy to construct, and surprisingly effective.Section4.3shows that integrating it into our training allows to improve visual quality on out-of-distribution prompts by17%percent1717\\%17 %.",
                "position": 434
            },
            {
                "img": "https://arxiv.org/html/2411.18673/extracted/6029846/images/getty-static/re2.png",
                "caption": "",
                "position": 449
            },
            {
                "img": "https://arxiv.org/html/2411.18673/extracted/6029846/images/getty-static/ge2.png",
                "caption": "",
                "position": 463
            },
            {
                "img": "https://arxiv.org/html/2411.18673/extracted/6029846/images/getty-static/ge4.png",
                "caption": "",
                "position": 466
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AEthics Statement",
        "images": []
    },
    {
        "header": "Appendix BLimitations",
        "images": []
    },
    {
        "header": "Appendix CImplementation details",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.18673/x6.png",
                "caption": "Figure 7:Comparing rectified flow noise schedules: (orange) vanilla standard logit-normal noise schedule proposed by[28]and used for baseline experiments; (purple) biased but non-truncated noise schedule; (pink) biased and truncated noise schedule.",
                "position": 3699
            }
        ]
    },
    {
        "header": "Appendix DMotion analysis details",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.18673/x7.png",
                "caption": "Figure 8:Our annotations collected for 200 randomly generated videos fromVDiTand used in our camera motion analysis inSection3.3.",
                "position": 3721
            },
            {
                "img": "https://arxiv.org/html/2411.18673/extracted/6029846/images/latents-pca.png",
                "caption": "Figure 9:Frames of the generated videos by theVDiTmodel (upper row) and the corresponding PCA projections of their latents (lower row).",
                "position": 3737
            }
        ]
    },
    {
        "header": "Appendix ELinear probing details",
        "images": []
    },
    {
        "header": "Appendix FDataset construction details",
        "images": []
    },
    {
        "header": "Appendix GScaling bias",
        "images": []
    },
    {
        "header": "Appendix HAdditional Related Work",
        "images": []
    }
]