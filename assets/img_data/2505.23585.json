[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Method: On-Policy RL with Optimal Reward Baseline (OPO)",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23585/x1.png",
                "caption": "Figure 1:Training dynamics of on-policy and off-policy training.Left: Training rewards;Middle: KL divergence;Right: Entropy.",
                "position": 628
            },
            {
                "img": "https://arxiv.org/html/2505.23585/x2.png",
                "caption": "Figure 2:Left: Comparison of KL divergence and math performance between OPO and GRPO. Both OPO and GRPO follow the exact on-policy training from the SFT policy. The x-axis represents KL divergence, and the y-axis denotes math performance.Middle: Training dynamics of KL divergence.Right: Training dynamics of entropy.",
                "position": 631
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADerivation of the Optimal Baseline",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23585/x3.png",
                "caption": "Figure 3:Training dynamics of OPO and Reinforce++. Both OPO and Reinforce++ follow the exact on-policy training.Left: Training rewards;Middle: KL divergence;Right: Entropy.",
                "position": 1334
            }
        ]
    },
    {
        "header": "Appendix BExperiments on Reinforce++ Algorithm",
        "images": []
    }
]