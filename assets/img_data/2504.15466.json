[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15466/x1.png",
                "caption": "Figure 1:Serialized search(Gandhi et¬†al.,2024)(top) vs Adaptive Parallel Reasoning (bottom)illustrated on an example of the Countdown task, with a target number of 27 and input numbers of {22, 26, 31, 53}. Each box represents a node in the search representing the value of the intermediate expression (xùë•xitalic_x). Edges are annotated with explored arithmetic operations relative to the parent-nodexùë•xitalic_xusing remaining input numbers. In serialized search, the context window of the single inference thread is exhausted before a solution is found. In Adaptive Parallel Reasoning, the parent thread (blue) spawns two child threads (orange), which are executed in parallel. Child threads have access only to a limited context passed to them by the parent thread and return a summary of their execution to the parent thread. The parent thread can then continue to decode with access to these summaries. This parallel distribution of computation prevents context window exhaustion while reducing latency.",
                "position": 140
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Adaptive Parallel Reasoning",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15466/x2.png",
                "caption": "Figure 2:Overview of Adaptive Parallel Reasoning (APR).While previous chain-of-thought methods directly linearize the reasoning tree, Adaptive Parallel Reasoning alternates between the parent thread and parallel child threads to traverse the reasoning tree more efficiently.",
                "position": 235
            },
            {
                "img": "https://arxiv.org/html/2504.15466/x3.png",
                "caption": "Figure 3:An example trajectory of our APR method solving the Countdown task.The model starts with the parent thread, solving the task through reasoning in language, and generatesspawncommands to instantiate two child threads for parallel reasoning, which arejoined back when finished, after which the main thread continues decoding.Orangedenotes prefix input tokens, whileGreendenotes LM-generated tokens. Bold parts correspond to thespawnandjoinoperations.",
                "position": 277
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15466/x4.png",
                "caption": "(a)Performance vs. Total Compute",
                "position": 352
            },
            {
                "img": "https://arxiv.org/html/2504.15466/x4.png",
                "caption": "(a)Performance vs. Total Compute",
                "position": 355
            },
            {
                "img": "https://arxiv.org/html/2504.15466/x5.png",
                "caption": "(b)Performance vs. Context Window Size",
                "position": 360
            },
            {
                "img": "https://arxiv.org/html/2504.15466/x6.png",
                "caption": "Figure 5:Comparison of model performance and statistics before and after reinforcement learning¬†(RL).RL significantly improves performance by optimizing the APR policy beyond what can be learned from supervised imitation learning. Post-RL models strategically increase sequence length (total tokens and avg token per seq) and, more substantially, child thread count (the number of spawned child threads), demonstrating an advantage of broadening rather than deepening search in reasoning. This optimization results in substantially higher accuracy (from 75.5% to 83.4%).",
                "position": 399
            },
            {
                "img": "https://arxiv.org/html/2504.15466/x7.png",
                "caption": "Figure 6:Efficiency comparison between APR and SoS+.Left: accuracy vs. sequential token usage. Right: accuracy vs. wall clock latency.\nAPR consistently achieves higher accuracy with fewer tokens and lower latency, demonstrating superior efficiency.",
                "position": 409
            },
            {
                "img": "https://arxiv.org/html/2504.15466/x8.png",
                "caption": "Figure 7:RL provides a larger performance gain to APR (7.9%) compared to SoS+ (2.7%), resulting in substantial advantages of APR over SoS+ after RL (83.4% vs 60.0%). Ablations with the number of child threads enforced show that such performance gain mainly comes from teaching the model how to allocate compute resources instead of making more optimal decisions with the same resources.",
                "position": 415
            }
        ]
    },
    {
        "header": "5Conclusions, Limitations, and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15466/x9.png",
                "caption": "Figure 8:Ablation of¬†APR and¬†SoS+ vs. total compute under different temperature settings.Across all settings, APR demonstrates more reliable and efficient scaling behavior.",
                "position": 1247
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]