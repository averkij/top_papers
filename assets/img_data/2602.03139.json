[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03139/figure/teaser_original.png",
                "caption": "(a)SD3.5-M (60 NFEs)",
                "position": 121
            },
            {
                "img": "https://arxiv.org/html/2602.03139/figure/teaser_dmd.png",
                "caption": "(b)DMD (4 NFEs)",
                "position": 124
            },
            {
                "img": "https://arxiv.org/html/2602.03139/figure/teaser_dpdmd.png",
                "caption": "(c)DP-DMD(Ours, 4 NFEs)",
                "position": 127
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03139/x1.png",
                "caption": "Figure 2:Training pipeline of DP-DMD.The first denoising step of the student is guided by a Flow-Matching diversity loss using a teacher-derived intermediate state, with gradients stopped thereafter. The remaining steps are optimized via the DMD objective, leveraging teacher and fake-model scores to refine sample quality. This role separation preserves diversity while maintaining high-fidelity generation in few-step distillation.",
                "position": 182
            }
        ]
    },
    {
        "header": "3Background: Flow Matching and DMD",
        "images": []
    },
    {
        "header": "4DP-DMD for Diffusion Distillation",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03139/x2.png",
                "caption": "Figure 3:Gradient stopping in DP-DMD.Training dynamics of diversity and preference for DMD,DP-DMD, and a variant without gradient stopping after the first step. All curves start from the 100-th training iteration and are smoothed using exponential moving average.",
                "position": 806
            },
            {
                "img": "https://arxiv.org/html/2602.03139/x3.png",
                "caption": "Figure 4:Qualitative comparison of diversity supervision methods.Visual results of four distillation variants on the same prompts: (a) vanilla DMD, (b) DMD-LPIPS, (c) DMD-GAN, and (d) the proposedDP-DMD. While perceptual and GAN-based approaches provide limited or unstable diversity gains and often suffer quality degradation,DP-DMDpreserves rich sample diversity while maintaining high visual fidelity, demonstrating a more favorable diversity–quality trade-off.",
                "position": 834
            },
            {
                "img": "https://arxiv.org/html/2602.03139/x4.png",
                "caption": "Figure 5:Qualitative comparisonwith open-source few-step distillation methods.",
                "position": 997
            }
        ]
    },
    {
        "header": "6Discussion and Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AObservation of Early and Late Denoising Steps",
        "images": []
    },
    {
        "header": "Appendix BMotivation for First-Step Diversity Supervision",
        "images": []
    },
    {
        "header": "Appendix CDP-DMD on Diffusion Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03139/x5.png",
                "caption": "Figure A:Progressive denoising dynamics.Visualization of SD3.5-MEsseret al.(2024)inference exhibits a stage-wise denoising pattern. The left panel shows a trajectory from step 1 to step 17, while the right panel highlights early steps under different noise initializations. Early steps recover the global structural layout, already showing variation across samples and suggesting a strong link to samplediversity, whereas later steps refine fine-grained appearance details and textures.",
                "position": 1719
            },
            {
                "img": "https://arxiv.org/html/2602.03139/x5.png",
                "caption": "",
                "position": 1722
            },
            {
                "img": "https://arxiv.org/html/2602.03139/x6.png",
                "caption": "",
                "position": 1726
            },
            {
                "img": "https://arxiv.org/html/2602.03139/x7.png",
                "caption": "Figure B:User study on diversity and image quality.We run pairwise comparisons on5050prompts with1010participants for (left) diversity and (right) image quality. Bars show win rates (%) ofDP-DMDagainst DMDYinet al.(2024a), DMD-LPIPS, and DMD-GAN; the dashed line marks 50%.DP-DMDis consistently preferred, achieving higher diversity while maintaining strong image quality.",
                "position": 1807
            },
            {
                "img": "https://arxiv.org/html/2602.03139/x8.png",
                "caption": "Figure C:Sample diversity under identical prompts.Comparison of images generated with the same text prompts and different random seeds, showing thatDP-DMDproduces more diverse global structures and semantic variations than baseline methods. All models generate samples with 4 NFEs.",
                "position": 1810
            },
            {
                "img": "https://arxiv.org/html/2602.03139/x9.png",
                "caption": "Figure D:Sample quality of DP-DMD.Images generated at1024×10241024\\times 1024resolution byDP-DMD, distilled from SD3.5-MEsseret al.(2024). All samples are produced with 4 NFEs, demonstrating high visual fidelity and coherent global structures under few-step inference.",
                "position": 1813
            }
        ]
    },
    {
        "header": "Appendix DUser Study",
        "images": []
    },
    {
        "header": "Appendix EMore Visualizations",
        "images": []
    }
]