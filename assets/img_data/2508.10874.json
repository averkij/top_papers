[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.10874/x1.png",
                "caption": "Figure 1:Left: Prior methods like Search-R1(Jin et¬†al.,2025b)and ZeroSearch(Sun et¬†al.,2025)rely on external sources (e.g., search engines, knowledge bases, or fine-tuned LLMs), representing full or semi-real search. We propose full-sim search, where a policy model generates information internally (Self-Search). Right: Self-Search with test-time scaling shows strongpass@kperformance as compute increases. Self-Search Reinforcement Learning (SSRL) further boosts results across models and tasks, especially with sim-to-real generalization.",
                "position": 221
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Inference-time Scaling of Self-Search",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.10874/x2.png",
                "caption": "(a)",
                "position": 540
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x2.png",
                "caption": "(a)",
                "position": 543
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x3.png",
                "caption": "(b)",
                "position": 548
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x4.png",
                "caption": "(c)",
                "position": 553
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x5.png",
                "caption": "Figure 3:TTS on BrowseComp leads to consistent performance gains within all models. It indicates predictive performance gains, with average MAE for the LLaMA, Qwen 2.5, and Qwen 3 families at 0.34 %, 0.22 %, and 0.26 %, respectively.",
                "position": 572
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x6.png",
                "caption": "(a)",
                "position": 615
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x6.png",
                "caption": "(a)",
                "position": 618
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x7.png",
                "caption": "(b)",
                "position": 623
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x8.png",
                "caption": "(c)",
                "position": 628
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x9.png",
                "caption": "(a)",
                "position": 635
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x9.png",
                "caption": "(a)",
                "position": 638
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x10.png",
                "caption": "(b)",
                "position": 643
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x11.png",
                "caption": "(c)",
                "position": 648
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x12.png",
                "caption": "(d)",
                "position": 653
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x13.png",
                "caption": "(a)",
                "position": 660
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x13.png",
                "caption": "(a)",
                "position": 663
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x14.png",
                "caption": "(b)",
                "position": 668
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x15.png",
                "caption": "(c)",
                "position": 673
            }
        ]
    },
    {
        "header": "3SSRL: Self-Search Reinforcement Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.10874/x16.png",
                "caption": "Table 3:Main results of our trained models on the six benchmarks measured by EM. The columnùêíùêûùêöùê´ùêúùê°‚ÄãùêÑùêßùê†ùê¢ùêßùêû\\mathbf{Search\\ Engine}refers to the external search engine used in the training stage and the evaluation stage. We use‚àÖ\\emptysetto denote that the baseline does not undergo the stage and ‚Äú-‚Äù to denote using internal knowledge. We useto denote the Simulation LLM andto denote Wikipedia for simplification. We useto denote Google. The largest score of each model is denoted usingùêõùê®ùê•ùêù\\mathbf{bold}.",
                "position": 812
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x17.png",
                "caption": "",
                "position": 881
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x18.png",
                "caption": "",
                "position": 916
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x19.png",
                "caption": "",
                "position": 916
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x20.png",
                "caption": "",
                "position": 929
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x21.png",
                "caption": "",
                "position": 929
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x22.png",
                "caption": "",
                "position": 942
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x23.png",
                "caption": "",
                "position": 942
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x24.png",
                "caption": "",
                "position": 955
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x25.png",
                "caption": "",
                "position": 955
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x26.png",
                "caption": "",
                "position": 1017
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x27.png",
                "caption": "",
                "position": 1030
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x28.png",
                "caption": "",
                "position": 1065
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x29.png",
                "caption": "",
                "position": 1065
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x30.png",
                "caption": "",
                "position": 1078
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x31.png",
                "caption": "",
                "position": 1078
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x32.png",
                "caption": "",
                "position": 1091
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x33.png",
                "caption": "",
                "position": 1091
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x34.png",
                "caption": "",
                "position": 1104
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x35.png",
                "caption": "",
                "position": 1104
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x39.png",
                "caption": "",
                "position": 1138
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x40.png",
                "caption": "",
                "position": 1138
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x41.png",
                "caption": "",
                "position": 1138
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x42.png",
                "caption": "Figure 7:The training curves of Llama-3.2-3B-Instruct and Llama-3.1-8B-Instruct. The first figure is the training reward. The second figure is the response length, and the third figure is the number of searches included in the response.",
                "position": 1174
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x43.png",
                "caption": "Figure 8:The training curve of Llama-3.1-8B-Instruct using SSRL and ZeroSearch. The first figure is the time used during training. The second figure is the training reward on the same training step, and the third figure is the training reward consuming the same time.",
                "position": 1177
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x44.png",
                "caption": "Figure 9:Pareto frontier illustrating the trade-off between performance and the number of real searches across different models. The Sim2Real models are evaluated using the maximum score within Sim2Real(K=1)(K=1), Sim2Real(K=3)(K=3)for fair comparison.",
                "position": 1195
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x45.png",
                "caption": "Figure 10:The performance of Llama-3.2-3B-Instruct and Llama-3.1-8B-Instruct when trained with and without the information mask.",
                "position": 2011
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x46.png",
                "caption": "Figure 11:The performance of Llama-3.2-3B-Instruct and Llama-3.1-8B-Instruct when trained with and without format reward. All of the models are trained with an information mask.",
                "position": 2014
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AInference-time Scaling of Self-Search",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.10874/x47.png",
                "caption": "Figure 12:The results of repeated sampling of seven benchmarks",
                "position": 4130
            }
        ]
    },
    {
        "header": "Appendix BSelf-Search Reinforcement Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.10874/x48.png",
                "caption": "Figure 13:The performance of TTRL of various models on BrowseComp.",
                "position": 4659
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x49.png",
                "caption": "Figure 14:Left: Comparison of the search number with and without information mask on Llama-3.1-8B-Instruct.\nRight: Group size comparison.",
                "position": 4903
            },
            {
                "img": "https://arxiv.org/html/2508.10874/x50.png",
                "caption": "",
                "position": 4906
            }
        ]
    },
    {
        "header": "Appendix CFormat Reward Code",
        "images": []
    }
]