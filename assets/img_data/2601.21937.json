[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21937/x1.png",
                "caption": "Figure 1:Closed-book QA measures intrinsic knowledge, while standard RAG entangles retrieval and reasoning—distractors can hide whether errors come from bad retrieval or failed evidence-based reasoning.DeR2decouples the two by evaluating the same question under controlled inputs (instruction/concepts/related/full), isolating failure causes.",
                "position": 134
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21937/x2.png",
                "caption": "Figure 2:(a) Existing benchmarks.Closed-book QA only evaluates intrinsic (parametric) knowledge. RAG pipelines couple retrieval and reasoning end-to-end, creating a reasoning–retrieval tangle that masks the true cause of failure.(b) Decoupled Retrieval and Reasoning Benchmark.DeR2provides a controlled setting that decouples retrieval from reasoning by comparing Instruction-only, Full DocSet (related + noise), Related-only, and Concepts-only conditions, enabling diagnosis of grounding, mode-switching, and concept execution errors.",
                "position": 185
            },
            {
                "img": "https://arxiv.org/html/2601.21937/x3.png",
                "caption": "Figure 3:Pipeline for constructingDeR2data:PhD annotators gather theoretical papers from academic platforms, extracting instructions, concepts, CoT, and concise answers. These questions are designed to be challenging and represent cutting-edge academic problems. During the difficulty control phase, offline AI models must fail to answer correctly three times without concepts, but have a chance to answer correctly when provided with the relevant concepts. When the difficulty control conditions are satisfied, retrieve the related documents and noise documents from the references of the source paper. Finally, the reviewer conducts logical quality inspection and format quality inspection on the submitted data, and verifies the difficulty control conditions again.",
                "position": 239
            },
            {
                "img": "https://arxiv.org/html/2601.21937/x4.png",
                "caption": "Figure 4:Distribution of problem domains and answer types, showing the disciplinary coverage of the benchmark and the structural diversity of target answers.",
                "position": 253
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21937/x5.png",
                "caption": "Figure 5:Structural and contextual complexity of the dataset.(a)Reports the distributions of reasoning step counts and required concept counts per problem, reflecting multi-step and multi-concept reasoning demands.(b)Presents the distributions of related documents, noise documents, and total documents per instance, characterizing the degree of retrieval difficulty and noise exposure in the document sets.\nTogether, these statistics illustrate the controlled diversity of DeR² across reasoning depth, conceptual load, and retrieval conditions.",
                "position": 462
            },
            {
                "img": "https://arxiv.org/html/2601.21937/x6.png",
                "caption": "Figure 6:Impact of Document Characteristics on Model Performance.(a) Under theFull-setsetting, model score as a function of the number of noise documents.\n(b) Under theFull-setsetting, model score as a function of the number of concepts.",
                "position": 665
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Contributions and Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APrompts",
        "images": []
    },
    {
        "header": "Appendix BCase Analysis",
        "images": []
    }
]