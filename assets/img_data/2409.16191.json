[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16191/x1.png",
                "caption": "Figure 1:The overview of HelloBench.(In the Middle): The six levels of Bloom’s Taxonomy, from bottom to top, areremember,understand,apply,analyze,evaluate, andcreate. These correspond to the five tasks in HelloBench and HelloEval. Detailed examples are provided in AppendixA.",
                "position": 201
            }
        ]
    },
    {
        "header": "2HelloBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16191/x2.png",
                "caption": "Figure 2:HelloBench Categories and Subcategories Distribution: The inner ring shows the categories and their proportions within the HelloBench. The outer ring details the subcategories and their respective proportions relative to the HelloBench.",
                "position": 376
            }
        ]
    },
    {
        "header": "3HelloEval",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16191/x3.png",
                "caption": "Figure 3:The pipeline of HelloEval has two stages. (top): In the preparation stage, we aim to determine the weighted score for each checklist. First, we have human annotators assign checklist results to each instruction-response pair. Then, the annotators give an overall score. By using linear regression, we can obtain the weighted scores for the checklists that align with humans. (bottom): In the execution stage, we use LLM to evaluate the checklist results for the instruction-response pairs, and then sum these scores based on the previously fitted weighted scores to get the overall score.",
                "position": 439
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16191/x4.png",
                "caption": "Table 2:Main Experiments: The evaluation results of open-source LLMs, proprietary LLMs, and long text generation capabilities enhanced LLMs on HelloBench. “OEQA” represents open-ended QA, “Summ” represents summarization, “TC” represents text completion, “HTG” represents heuristic text generation, “AVG” represents average score on five tasks, “S” represents rescaled score, and “WC” represents word count. The results are in descending order.",
                "position": 499
            },
            {
                "img": "https://arxiv.org/html/2409.16191/x5.png",
                "caption": "",
                "position": 668
            },
            {
                "img": "https://arxiv.org/html/2409.16191/x6.png",
                "caption": "",
                "position": 853
            },
            {
                "img": "https://arxiv.org/html/2409.16191/x7.png",
                "caption": "Figure 4:Scaling Law of Model Size and Performance for open-source LLMs.",
                "position": 889
            },
            {
                "img": "https://arxiv.org/html/2409.16191/x8.png",
                "caption": "Figure 5:The scores and generated length of different LLMs under various length constraints. We consider “without constraint” as “length constraint = 0”. The gray area on the right figure indicates regions where the generated lengths do not meet the length constraints.",
                "position": 1063
            }
        ]
    },
    {
        "header": "5Analysis and Discussion",
        "images": []
    },
    {
        "header": "6Related Works",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExamples for each task in HelloBench",
        "images": []
    },
    {
        "header": "Appendix BDetails of dataset collection",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16191/x9.png",
                "caption": "Figure 7:The distribution of categories after labeling in Step 1 and clustering in Step 3.",
                "position": 3151
            }
        ]
    },
    {
        "header": "Appendix CData Quality of HelloBench",
        "images": []
    },
    {
        "header": "Appendix DAdditional Materials for Dataset Statistics",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16191/x10.png",
                "caption": "Figure 9:Illustration of Word Lengths of Instructions in HelloBench",
                "position": 3616
            },
            {
                "img": "https://arxiv.org/html/2409.16191/x10.png",
                "caption": "Figure 9:Illustration of Word Lengths of Instructions in HelloBench",
                "position": 3619
            }
        ]
    },
    {
        "header": "Appendix EConstruction of Checklists",
        "images": []
    },
    {
        "header": "Appendix FHuman Annotation",
        "images": []
    },
    {
        "header": "Appendix GDetails of Linear Regression",
        "images": []
    },
    {
        "header": "Appendix HPrompt Template",
        "images": []
    },
    {
        "header": "Appendix ILLM-as-a-Judge Experiments",
        "images": []
    },
    {
        "header": "Appendix JDetails of Experiments",
        "images": []
    },
    {
        "header": "Appendix KDetailed error mode cases and statistics",
        "images": []
    },
    {
        "header": "Appendix LLimitations",
        "images": []
    },
    {
        "header": "Appendix MSocial Impact and Potential Bias",
        "images": []
    }
]