[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23751/x1.png",
                "caption": "Figure 1:Visualizations of alternate patch sequence orderings.Six different patch orders—row-major, column-major, Hilbert curve, spiral, diagonal, and snake—are shown as trajectories over a14×14141414\\times 1414 × 14grid of patches. Each trajectory begins at the red dot and progresses to the black dot, illustrating the 1-D ordering imposed on the 2-D patch grid.",
                "position": 137
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23751/x2.png",
                "caption": "Figure 2:Patch order affects the performance of long-sequence models.This figure compares the top-1 accuracy of Vision Transformer (ViT), Longformer, Mamba, and Transformer-XL (T-XL) on ImageNet-1K and Functional Map of the World when using alternate patch orderings, relative to their standard row-major performance. As expected, ViT remains equivariant to patch sequence permutations. In contrast, long-sequence models exhibit substantial performance variability depending on the patch ordering. No single ordering consistently outperforms others across models or datasets, necessitating dynamic patch ordering strategies.",
                "position": 350
            }
        ]
    },
    {
        "header": "4Does Patch Order Matter?",
        "images": []
    },
    {
        "header": "5Learning an Optimal Patch Ordering withREOrder",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23751/x3.png",
                "caption": "Figure 3:Compression of 1-D sequences can serve as a weak prior for optimal patch ordering.Top-1 accuracy is compared to percentage reduction for different patch orderings across four models for both ImageNet-1K and FMoW.",
                "position": 404
            }
        ]
    },
    {
        "header": "6Learning to Order Patches",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23751/x4.png",
                "caption": "Figure 4:The logits of the Plackett-Luce model, and therefore the permutation order, changes over the course of training. Longformer is initialized with column and row-major patch ordering and optimized withREOrder. The image is of the class “keyboard.” We track two patches over the course of the policy curriculum: a keyboard key (light red arrow) and an irrelevant orange beak (dark red arrow). As the policy learns to order patches, we see the patches related to the target class move to the end of the ordering.",
                "position": 535
            },
            {
                "img": "https://arxiv.org/html/2505.23751/x5.png",
                "caption": "Figure 5:REOrderfinds improvements over the best patch ordering prior in almost all cases.Across all models,REOrdercan find a better patch ordering than a static prior and improve accuracy across both ImageNet-1K and Functional Map of the World.",
                "position": 552
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ADiscussion and Limitations",
        "images": []
    },
    {
        "header": "Appendix BProof of Proposition3.1",
        "images": []
    },
    {
        "header": "Appendix CModel Details",
        "images": []
    },
    {
        "header": "Appendix DAdditional Model Training Details",
        "images": []
    },
    {
        "header": "Appendix EDataset Licenses and References",
        "images": []
    },
    {
        "header": "Appendix FRasterization Orders",
        "images": []
    },
    {
        "header": "Appendix GAttention Patterns for Tested Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23751/x6.png",
                "caption": "Figure 6:Token-level attention coverage across different model architectures.Each grid cell represents a patch in a 224×\\times×224 input image split into 49 non-overlapping 32×\\times×32 patches, plus a[CLS]token. Numbers indicate how many tokens attend to each patch. ViT and Mamba exhibit full attention to all patches. In contrast, Transformer-XL’s causal attention and Longformer’s local attention restrict the number of tokens that can attend to each patch, leading to a strong asymmetry and localized attention, respectively.",
                "position": 1406
            },
            {
                "img": "https://arxiv.org/html/2505.23751/x7.png",
                "caption": "Figure 7:Mamba observes patches most related to the class label move to the end of the sequence.Mamba is trained with column- (top) and row-major (bottom) patch orderings and optimized withREOrder. The image is of the class “keyboard.” We track two patches over the course of the policy curriculum: a keyboard key (light red arrow) and an irrelevant orange beak (dark red arrow). As training progresses, the keyboard-related patches shift into the final indices of the sequence.",
                "position": 1439
            },
            {
                "img": "https://arxiv.org/html/2505.23751/x8.png",
                "caption": "Figure 8:Transformer-XL observes patches most related to the class label move to the end of the sequence.Transformer-XL is initialized with column- (top) and row-major (bottom) patch orderings and optimized withREOrder. The image is of the class “keyboard.” We track two patches over the course of the policy curriculum: a keyboard key (light red arrow) and an irrelevant orange beak (dark red arrow). As training progresses, the keyboard-related patches shift into the final indices of the sequence.",
                "position": 1442
            }
        ]
    },
    {
        "header": "Appendix HPolicy Evolution During Training",
        "images": []
    }
]