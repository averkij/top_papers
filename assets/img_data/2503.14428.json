[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14428/x1.png",
                "caption": "",
                "position": 97
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14428/x2.png",
                "caption": "Figure 2:Detailed architecture of MagicComp.The dual-phase refinement strategy of MagicComp contains two core steps: (a) Semantic Anchor Disambiguation (SAD) module for inter-subject disambiguation during the conditioning stage. We only display disambiguation process of subject “cat\" for simplicity, other subjects follow the similar way. (b) Dynamic Layout Fusion Attention (DLFA) module for precise attribute–location binding of each subject during the denoising stage.",
                "position": 149
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14428/x3.png",
                "caption": "Figure 3:Visualization of the disambiguation effect brought by SAD.(a) Cos similarity between the pooled embeddings of “brown dog\" and “gray cat\" under different settings. “Standard\" indicates the cos similarity is computed when each subject are independently encoded by T5. (b) Cross attention maps between the middle frame video tokens and the pooled subject tokens.",
                "position": 174
            },
            {
                "img": "https://arxiv.org/html/2503.14428/x4.png",
                "caption": "Figure 4:Comparison of different masking strategy.(a) Visualization of prior layout mask and model-adaptive perception layout. (b) Comparison of the generated videos.",
                "position": 238
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14428/x5.png",
                "caption": "Figure 5:Qualitative Comparison on T2V-CompBench.Our MagicComp significantly outperforms existing approaches across various compositional generation tasks, and the methods such as Vico[47]and CogVideoX[48]struggle to capture fine-grained concepts.",
                "position": 699
            },
            {
                "img": "https://arxiv.org/html/2503.14428/x6.png",
                "caption": "Figure 6:Application on complex prompt-based video generation.It is evident that among all models, only MagicComp strictly follows the prompt to generate complex scenarios.",
                "position": 746
            },
            {
                "img": "https://arxiv.org/html/2503.14428/x7.png",
                "caption": "Figure 7:Application about trajectory-controllable video generation.By incorporating the proposed methods, CogVideoX[48]can achieve trajectory control seamlessly without additional cost.",
                "position": 766
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "1Subject-aware Masked Attention in DLFA",
        "images": []
    },
    {
        "header": "2Adapt MagicComp on VideoCrafter2",
        "images": []
    },
    {
        "header": "3More Implementation Details",
        "images": []
    },
    {
        "header": "4Enhanced input prompt via LLM",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14428/x8.png",
                "caption": "Figure 1:Instruction prompt for prior layout generation.",
                "position": 1618
            }
        ]
    },
    {
        "header": "5More Qualitative Results Demonstration",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14428/x9.png",
                "caption": "Figure 2:Qualitative results on Consist-attr.",
                "position": 1635
            },
            {
                "img": "https://arxiv.org/html/2503.14428/x10.png",
                "caption": "Figure 3:Qualitative results on Consist-attr.",
                "position": 1638
            },
            {
                "img": "https://arxiv.org/html/2503.14428/x11.png",
                "caption": "Figure 4:Qualitative results on Motion.",
                "position": 1641
            },
            {
                "img": "https://arxiv.org/html/2503.14428/x12.png",
                "caption": "Figure 5:Qualitative results on Action & Motion.",
                "position": 1644
            },
            {
                "img": "https://arxiv.org/html/2503.14428/x13.png",
                "caption": "Figure 6:Qualitative results on Numeracy.",
                "position": 1647
            },
            {
                "img": "https://arxiv.org/html/2503.14428/x14.png",
                "caption": "Figure 7:Qualitative results on VideoCrafter2.",
                "position": 1650
            }
        ]
    },
    {
        "header": "6Limitation",
        "images": []
    }
]