[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20279/x1.png",
                "caption": "Figure 1:VLM-3R: 3D Spatial-Temporal Reasoning from Monocular Video.Our VLM-3R framework (b) utilizes an end-to-end architecture with integrated encoders to process video directly, bypassing the reliance of prior methods (a) on explicit 3D data (e.g., depth sensors, pre-built maps). This enables a deep understanding of spatial context, instance layout, and temporal dynamics, achieving leading performance on VSI-Bench and our novel VSTemporalI-Bench (results in c).",
                "position": 187
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Scalable Visual-Spatial and Visual-Spatial-Temporal Datasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20279/x2.png",
                "caption": "Figure 2:VSTemporalI-Bench Overview.(a) Statistical distribution of QA pairs by primary categories (inner ring of chart, detailed in legend) and their sub-categories (outer ring). (b) Example Question-Answer pairs illustrating different task types within a scene from the benchmark.",
                "position": 270
            }
        ]
    },
    {
        "header": "4VLM-3R Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20279/x3.png",
                "caption": "Figure 3:Network Architecture.Our method takes monocular video and language instruction as input. Visual Encoder coupled with Spatial Encoder extract frame-level appearance, camera view position, and globally aligned geometry. Visual-Geometry Fusion consists of 2D-3D attention and one layer projector were adopted to integrate the spatial context into latent tokens. During the inference stage, our approach is capable of performing reliable spatial and temporal reasoning thanks for the fusion of global geometry and frame-level view token.",
                "position": 343
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusion, Limitations, and Broader Impact",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Implementation Details",
        "images": []
    },
    {
        "header": "Appendix BDataset Curation and Benchmark Design",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20279/x4.png",
                "caption": "Figure 4:Illustrative diverse trajectories generated by the Habitat simulator within a single scene. These demonstrate fundamental navigation actions (e.g.,‘‘Turn Right’’,‘‘Turn Left’’,‘‘Turn Back’’), which are foundational for generating route planning QA data via our specialized templates.",
                "position": 2544
            },
            {
                "img": "https://arxiv.org/html/2505.20279/x5.png",
                "caption": "",
                "position": 2553
            },
            {
                "img": "https://arxiv.org/html/2505.20279/x6.png",
                "caption": "",
                "position": 2558
            },
            {
                "img": "https://arxiv.org/html/2505.20279/x7.png",
                "caption": "",
                "position": 2564
            },
            {
                "img": "https://arxiv.org/html/2505.20279/x8.png",
                "caption": "",
                "position": 2569
            },
            {
                "img": "https://arxiv.org/html/2505.20279/x9.png",
                "caption": "",
                "position": 2574
            }
        ]
    },
    {
        "header": "Appendix CAdditional Experimental Analysis",
        "images": []
    }
]