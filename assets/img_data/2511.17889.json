[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17889/images/mobilevlar1_logo.png",
                "caption": "",
                "position": 56
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17889/x1.png",
                "caption": "Figure 1:Architecture of MobileVLA-R1.MobileVLA-R1 is an end-to-end framework that integrates natural-language instructions with multimodal perception. It processes RGB, depth, and point cloud observations together with textual commands to generate continuous locomotion actions, enabling mobile robots to follow complex instructions and adapt to diverse environments in real time.",
                "position": 129
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Datasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17889/x2.png",
                "caption": "Figure 2:CoT Data Engine.We construct the MobileVLA-CoT by defining navigation and step-level instructions, integrating RGB–Depth visual inputs, and specifying structured reasoning prompts. These inputs are fed into Gemini-2.5-Flash, which generates multi-granularity Chain-of-Thought (CoT) annotations with corresponding action outputs.",
                "position": 251
            }
        ]
    },
    {
        "header": "4The Proposed Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17889/x3.png",
                "caption": "Figure 3:The pipeline of RL policy.The model generatesNNresponses from a given input, rewards are then computed for each response. After normalizing and clipping, these rewards are conflated with a KL-divergence term, which prevents the model from over-updating, to update the policy.",
                "position": 317
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17889/x4.png",
                "caption": "Figure 4:(a) Hardware platform:the Unitree Go2 quadruped robot is equipped with a Jetson Orin Nano (on-board PC) as the computation module, an L2 LiDAR for 3D environment perception, and an Intel RealSense D435i RGB-D camera for visual sensing.(b) Deployment process:RGB–Depth and 3D point cloud data are transmitted to MobileVLA-R1, which performs multimodal reasoning and action generation. The resulting velocity and motion commands are sent back to the on-board PC for real-time execution on the robot.(c) Real-World qualitative results:MobileVLA-R1 effectively integrates RGB, depth, and map observations to follow long-horizon language instructions with coherent spatial reasoning.",
                "position": 1037
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7More Ablation Study",
        "images": []
    },
    {
        "header": "8CoT Data Generation and Quality Control",
        "images": []
    },
    {
        "header": "9Test-Time Efficiency and Practical Considerations",
        "images": []
    },
    {
        "header": "10Visualization",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17889/x5.png",
                "caption": "Figure 5:Qualitative visualization on R2R-CE.MobileVLA-R1 executes English navigation instructions in the simulator.",
                "position": 2418
            },
            {
                "img": "https://arxiv.org/html/2511.17889/x6.png",
                "caption": "Figure 6:Qualitative visualization on RxR-CE.MobileVLA-R1 follows multilingual instructions with complex spatial semantics, maintaining coherent reasoning and stable motion across unseen environments.",
                "position": 2422
            },
            {
                "img": "https://arxiv.org/html/2511.17889/x7.png",
                "caption": "Figure 7:Real-world qualitative results.The robot demonstrates spatially grounded perception–reasoning–action behaviors, such as approaching targets, interacting with objects, and avoiding obstacles in realistic indoor environments.",
                "position": 2426
            },
            {
                "img": "https://arxiv.org/html/2511.17889/x8.png",
                "caption": "Figure 8:Outdoor real-world qualitative results.MobileVLA-R1 delivers highly reliable long-horizon reasoning and control in diverse outdoor environments, showcasing its strong real-world generalization.",
                "position": 2430
            },
            {
                "img": "https://arxiv.org/html/2511.17889/x9.png",
                "caption": "Figure 9:Additional outdoor real-world qualitative results.MobileVLA-R1 exhibits fine-grained obstacle avoidance, target-aware spatial reasoning, and high-precision locomotion control in diverse real-world outdoor scenes.",
                "position": 2434
            }
        ]
    },
    {
        "header": "11Limitation and Future Work",
        "images": []
    }
]