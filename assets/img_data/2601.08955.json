[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08955/x1.png",
                "caption": "Figure 1:Comparison between ourITPframework and conventional agent learning frameworks.",
                "position": 166
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08955/x2.png",
                "caption": "Figure 2:Overview of the proposed Imageine-then-Plan (ITP) framework. It consists of two variants: (a)ITPI\\texttt{ITP}_{\\text{I}}, which is training-free and enables LLM agents to learn from the imagination at inference time. (b)ITPR\\texttt{ITP}_{\\text{R}}, which leverages imagined futures to optimize the action policy more effectively and more efficiently.",
                "position": 224
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08955/x3.png",
                "caption": "Figure 3:Ablation results ofITPR\\texttt{ITP}_{\\text{R}}on ALFWorld and ScienceWorld benchmarks.",
                "position": 1002
            },
            {
                "img": "https://arxiv.org/html/2601.08955/x4.png",
                "caption": "Figure 4:Comparison between ouradaptivelookahead mechanism and baselines withfixedlookahead steps, with success rate (left) and computational cost (right).",
                "position": 1039
            },
            {
                "img": "https://arxiv.org/html/2601.08955/x5.png",
                "caption": "(a)OurITPI\\texttt{ITP}_{\\text{I}}vs. ReAct + Random Lookahead",
                "position": 1071
            },
            {
                "img": "https://arxiv.org/html/2601.08955/x5.png",
                "caption": "(a)OurITPI\\texttt{ITP}_{\\text{I}}vs. ReAct + Random Lookahead",
                "position": 1074
            },
            {
                "img": "https://arxiv.org/html/2601.08955/x6.png",
                "caption": "(b)OurITPR\\texttt{ITP}_{\\text{R}}vs. SFT + Random Lookahead",
                "position": 1080
            },
            {
                "img": "https://arxiv.org/html/2601.08955/x7.png",
                "caption": "(a)ITPI\\texttt{ITP}_{\\text{I}}(Ours)",
                "position": 1114
            },
            {
                "img": "https://arxiv.org/html/2601.08955/x7.png",
                "caption": "(a)ITPI\\texttt{ITP}_{\\text{I}}(Ours)",
                "position": 1117
            },
            {
                "img": "https://arxiv.org/html/2601.08955/x8.png",
                "caption": "(b)ITPR\\texttt{ITP}_{\\text{R}}(Ours)",
                "position": 1122
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADatasets and Preprocessing",
        "images": []
    },
    {
        "header": "Appendix BAdditional Implementation Details",
        "images": []
    }
]