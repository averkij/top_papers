[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02702/x1.png",
                "caption": "Figure 1:TheMultiSessionCollabbenchmark with our long-term collaborative agent. Each session involves a user seeking help for a problem. The user maintains a draft answer that represents what they have learned from the interaction. They update their draft answer when the agent’s responses are both helpful and preference-aligned. When responses violate preferences, the user enforces them, as indicated by the red text boxes. After each session, the agent reflects on the interaction to identify user preference information that will be useful for future interactions and update their memory accordingly. We measure collaboration quality in each session with task success, conversation length, and user effort.",
                "position": 134
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3MultiSessionCollab",
        "images": []
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02702/x2.png",
                "caption": "Figure 2:RL framework for improving session-level reflections. The policy model generatesnnreflection rollouts for a conversation. The judge model evaluates each reflection againstε\\varepsilon(the subset of user utterances that enforce preferences) and assigns rewards. Advantages are computed and the policy is updated via GRPO.",
                "position": 316
            }
        ]
    },
    {
        "header": "5Experimental Setup",
        "images": []
    },
    {
        "header": "6Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02702/figures/learning_dynamics_llama70b.png",
                "caption": "Figure 3:Performance across sessions for Llama-3.3-70B-Instruct. Each graph plots the delta between agents with memory and agents without memory across 20 sessions for Task Success (ΔiT​S\\Delta_{i}^{TS})↑\\uparrow, User Effort (ΔiU​E\\Delta_{i}^{UE})↓\\downarrow, and Conversation Length (ΔiL​e​n\\Delta_{i}^{Len})↓\\downarrow.",
                "position": 743
            }
        ]
    },
    {
        "header": "7Real-World User Study",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "9Limitations",
        "images": []
    },
    {
        "header": "Appendix AUser Interaction Preferences",
        "images": []
    },
    {
        "header": "Appendix BAgent Prompts",
        "images": []
    },
    {
        "header": "Appendix CEnvironment Validation",
        "images": []
    },
    {
        "header": "Appendix DTraining Hyperparameters",
        "images": []
    },
    {
        "header": "Appendix EPerformance Across Sessions",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02702/figures/learning_dynamics_qwen7b_grpo.png",
                "caption": "Figure 4:Performance across sessions for Qwen-2.5-7B-Instruct (after GRPO). Each graph plots the delta between agents with memory and agents without memory across 20 sessions for Task Success (ΔiT​S\\Delta_{i}^{TS}), User Effort (ΔiU​E\\Delta_{i}^{UE}), and Conversation Length (ΔiL​e​n\\Delta_{i}^{Len}).",
                "position": 1696
            },
            {
                "img": "https://arxiv.org/html/2601.02702/figures/learning_dynamics_llama8b_grpo.png",
                "caption": "Figure 5:Performance across sessions for Llama-3.1-8B-Instruct (after GRPO). Each graph plots the delta between agents with memory and agents without memory across 20 sessions for Task Success (ΔiT​S\\Delta_{i}^{TS}), User Effort (ΔiU​E\\Delta_{i}^{UE}), and Conversation Length (ΔiL​e​n\\Delta_{i}^{Len}).",
                "position": 1699
            },
            {
                "img": "https://arxiv.org/html/2601.02702/figures/learning_dynamics_oss20b.png",
                "caption": "Figure 6:Performance across sessions for gpt-oss-20b. Each graph plots the delta between agents with memory and agents without memory across 20 sessions for Task Success (ΔiT​S\\Delta_{i}^{TS}), User Effort (ΔiU​E\\Delta_{i}^{UE}), and Conversation Length (ΔiL​e​n\\Delta_{i}^{Len}).",
                "position": 1702
            },
            {
                "img": "https://arxiv.org/html/2601.02702/figures/user_study_1.png",
                "caption": "Figure 7:User study interface initial instructions.",
                "position": 1788
            },
            {
                "img": "https://arxiv.org/html/2601.02702/figures/user_study_2.png",
                "caption": "",
                "position": 1791
            },
            {
                "img": "https://arxiv.org/html/2601.02702/figures/user_study_3.png",
                "caption": "Figure 8:User study interface problem-solving session example.",
                "position": 1795
            },
            {
                "img": "https://arxiv.org/html/2601.02702/figures/user_study_4.png",
                "caption": "Figure 9:User study interface post session survey.",
                "position": 1798
            },
            {
                "img": "https://arxiv.org/html/2601.02702/figures/user_study_graphs_coding.png",
                "caption": "(a)Coding domain",
                "position": 2011
            },
            {
                "img": "https://arxiv.org/html/2601.02702/figures/user_study_graphs_coding.png",
                "caption": "(a)Coding domain",
                "position": 2014
            },
            {
                "img": "https://arxiv.org/html/2601.02702/figures/user_study_graphs_mixed.png",
                "caption": "(b)Mixed domain",
                "position": 2019
            }
        ]
    },
    {
        "header": "Appendix FReal-World User Study",
        "images": []
    }
]