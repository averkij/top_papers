[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.12877/x1.png",
                "caption": "",
                "position": 116
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.12877/x2.png",
                "caption": "Figure 2:Limitations of previous SOTA methods. (a) ControlVideo[70]relies on single global captions, and (b) GAV[25]depends on bounding box conditions that can sometimes overlap. Both are susceptible to unfaithful editing (red arrow) and attention leakage (blue arrow).",
                "position": 132
            },
            {
                "img": "https://arxiv.org/html/2412.12877/x3.png",
                "caption": "Figure 3:The overall framework ofMulti-InstanceVideoEditing (MIVE). Our Disentangled Multi-instance Sampling (DMS,Sec.3.2) consists of latent parallel sampling (LPS, blue box), latent fusion (yellow box) to fuse different instance latents, re-inversion (red box) to harmonize the latents after fusion, and noise parallel sampling (NPS, green box). In addition, our Instance-centric Probability Redistribution (IPR,Sec.3.3) provides better spatial control.",
                "position": 207
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Proposed Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.12877/x4.png",
                "caption": "Figure 4:A comparative illustration of our IPR versus others (top) and details of our IPR (bottom).",
                "position": 346
            }
        ]
    },
    {
        "header": "4Evaluation Data and Metric",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.12877/x5.png",
                "caption": "Figure 5:Qualitative comparison for three videos (with increasing difficulty from left to right) in our MIVE dataset.\n(a) shows the color-coded masks overlaid on the input frames to match the corresponding instance captions.\n(b)-(f) use global target captions for editing.\n(g) uses global and instance target captions along with bounding boxes (omitted in (a) for better visualization).\nOur MIVE in (h) uses instance captions and masks.\nUnfaithful editing examples are shown inred arrowand attention leakage are shown ingreen arrow.",
                "position": 731
            },
            {
                "img": "https://arxiv.org/html/2412.12877/x6.png",
                "caption": "Figure 6:Ablation study on DMS (Sec.3.2).",
                "position": 874
            },
            {
                "img": "https://arxiv.org/html/2412.12877/x7.png",
                "caption": "Figure 7:Ablation study on IPR (Sec.3.3).",
                "position": 891
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "APreliminaries",
        "images": []
    },
    {
        "header": "BDemo Videos",
        "images": []
    },
    {
        "header": "CDataset and Metrics Additional Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.12877/x8.png",
                "caption": "Figure 8:MIVE Dataset caption generation pipeline for each video.\nYellow box: The process starts by prompting LLaVA[31]to generate caption for each video that includes all instances in the video.\nSince LLaVa can only accept images, we perform the prompting for each frame and select one representative caption that includes the most instances.\nRed box: We utilize Llama 3[14]to summarize the initial caption generated by LLaVa.\nBlue box: We manually include all of the instances of interest that are not included in the caption and manually add tags to map the instance captions to corresponding segmentation masks.\nPurple box: We utilize Llama 3 to generate target captions by retexturing or swapping instances similar to[71]for each instance.",
                "position": 1972
            },
            {
                "img": "https://arxiv.org/html/2412.12877/x9.png",
                "caption": "Figure 9:Sample frames and captions from our MIVE Dataset (Part 1). The colored texts are the instance target captions. For each video, the instance masks are color-coded to correspond with the instance target captions in the global target caption.",
                "position": 1982
            },
            {
                "img": "https://arxiv.org/html/2412.12877/x10.png",
                "caption": "Figure 10:Sample frames and captions from our MIVE Dataset (Part 2). The colored texts are the instance target captions. For each video, the instance masks are color-coded to correspond with the instance target captions in the global target caption.",
                "position": 1987
            },
            {
                "img": "https://arxiv.org/html/2412.12877/x11.png",
                "caption": "Figure 11:Visualization of the Cross-Instance Accuracy (CIA) Score computation. We calculate the Local Textual Faithfulness (LTF) between each cropped instance and all the instance captions. For each row, we assign 1 to the maximum LTF (shown inred) and 0 to the rest. The CIA Score is calculated as the mean of the diagonal elements (shown ingreen).",
                "position": 2023
            }
        ]
    },
    {
        "header": "DComparison with State-of-the-Art Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.12877/x12.png",
                "caption": "Figure 12:Qualitative comparison with SOTA video editing methods (Part 1).\n(a) shows the color-coded masks overlaid on the input frames to match the corresponding instance captions.\n(b)-(f) use global target captions for editing.\n(g) uses global and instance target captions along with bounding boxes (omitted in (a) for better visualization).\nOur MIVE in (h) uses instance captions and masks.\nUnfaithful editing examples are shown inred arrowand attention leakage are shown ingreen arrow.",
                "position": 2058
            },
            {
                "img": "https://arxiv.org/html/2412.12877/x13.png",
                "caption": "Figure 13:Qualitative comparison with SOTA video editing methods (Part 2).\n(a) shows the color-coded masks overlaid on the input frames to match the corresponding instance captions.\n(b)-(f) use global target captions for editing.\n(g) uses global and instance target captions along with bounding boxes (omitted in (a) for better visualization).\nOur MIVE in (h) uses instance captions and masks.\nUnfaithful editing examples are shown inred arrowand attention leakage are shown ingreen arrow.",
                "position": 2068
            },
            {
                "img": "https://arxiv.org/html/2412.12877/x14.png",
                "caption": "Figure 14:Qualitative comparison with SOTA video editing methods (Part 3).\n(a) shows the color-coded masks overlaid on the input frames to match the corresponding instance captions.\n(b)-(f) use global target captions for editing.\n(g) uses global and instance target captions along with bounding boxes (omitted in (a) for better visualization).\nOur MIVE in (h) uses instance captions and masks.\nUnfaithful editing examples are shown inred arrowand attention leakage are shown ingreen arrow.",
                "position": 2078
            },
            {
                "img": "https://arxiv.org/html/2412.12877/x15.png",
                "caption": "Figure 15:Our user study interface and questionnaire form.\nParticipants are presented with an input video with a source caption, an annotated video with a target caption, and 7 randomly ordered videos edited using our MIVE and six other SOTA video editing methods.\nEach instance mask in the annotated video is color-coded to correspond with its instance target caption.\nParticipants are tasked to select the video with theBest Temporal Consistency,Best Textual Faithfulness, andLeast Editing Leakage.",
                "position": 2712
            },
            {
                "img": "https://arxiv.org/html/2412.12877/x16.png",
                "caption": "Figure 16:Video-P2P results on recursive multi-instance editing.\nThe artifacts that accumulate when Video-P2P is used repeatedly for multi-instance editing is shown inred arrow.\nOur MIVE prevents this error accumulation since we do not edit the frames recursively.",
                "position": 2752
            }
        ]
    },
    {
        "header": "EAdditional Analysis and Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.12877/x17.png",
                "caption": "Figure 17:Ablation study on DMS: (a) Ablation on alternating LPS and NPS on all sampling steps (T=50ùëá50T=50italic_T = 50).\nIncreasing the number of LPS steps while setting the number of NPS steps to 1 produces more details as shown in (1) - (4).\nIncreasing the number of NPS steps while setting the number of LPS steps to 1 produces cloudy artifacts and less details as shown in (5).\nSetting the number of LPS steps to 9 is a good tradeoff between qualitative and quantitative performance.\nSeeTab.8-(a) for quantitative results.",
                "position": 2905
            },
            {
                "img": "https://arxiv.org/html/2412.12877/x18.png",
                "caption": "Figure 18:Ablation study on DMS: (b) Ablation on last NPS after alternating sampling.\nReducing the number of alternating LPS-NPS steps during the initial sampling stage while increasing the number of NPS steps in the final stage enhances the quantitative performance of MIVE.\nQuantitative results are provided inTab.8-(b).\nHowever, we set the number of final NPS steps to only 10 to avoid degradation in object edges as shown in in (3) and (4).",
                "position": 2914
            },
            {
                "img": "https://arxiv.org/html/2412.12877/x19.png",
                "caption": "Figure 19:Ablation study on DMS: (c) Ablation on re-inversion step on alternating sampling.\nIncreasing the number of re-inversion stepsLùêøLitalic_Lin the alternating LPS-NPS steps improves quantitative performance as shown inTab.8-(c).\nWe also observe better details with higher values ofLùêøLitalic_L.\nUltimately, we setL=3ùêø3L=3italic_L = 3for the re-inversion in the alternating LPS-NPS stage.",
                "position": 2922
            },
            {
                "img": "https://arxiv.org/html/2412.12877/x20.png",
                "caption": "Figure 20:Ablation study on DMS: (d) Ablation on re-inversion step on last NPS=10 with alternating LPS=9 & NPS=1 & re-inversion L=3.\nIncreasing the number of re-inversion stepsLùêøLitalic_Lin the last NPS steps of the sampling improves quantitative performance (seeTab.8-(d)).\nHigher number of NPS steps, however, give the edited objects an animation-like appearance.\nThus, we limit the number of re-inversion steps in the final NPS stage toL=2ùêø2L=2italic_L = 2.",
                "position": 2930
            },
            {
                "img": "https://arxiv.org/html/2412.12877/x21.png",
                "caption": "Figure 21:Ablation study on DMS: (e) Ablation on Re-inversion step using 2D vs 3D Model.\nUsing the 3D model for re-inversion improves the quantitative performance of our framework (seeTab.8-(e)).\nFurther, using the 3D model renders sharper edges as shown above.\nWe, thus, use the 3D model during the re-inversion steps.",
                "position": 2938
            },
            {
                "img": "https://arxiv.org/html/2412.12877/x22.png",
                "caption": "Figure 22:Ablation study on DMS: (f) ablation on another alternative configuration.\nReducing the number of LPS steps during the alternating LPS-NPS stage to 4 improves FA and IA performance (seeTab.8-(f)).\nHowever, setting the number of LPS steps to 9 better preserves subtle details, as shown in (2).",
                "position": 2946
            },
            {
                "img": "https://arxiv.org/html/2412.12877/x23.png",
                "caption": "Figure 23:IPR analysis: Effect of altering the attention probability values of padding tokenAi,j‚ààPsubscriptùê¥ùëñùëóPA_{i,j\\in\\textbf{P}}italic_A start_POSTSUBSCRIPT italic_i , italic_j ‚àà P end_POSTSUBSCRIPT.",
                "position": 3273
            },
            {
                "img": "https://arxiv.org/html/2412.12877/x24.png",
                "caption": "Figure 24:IPR analysis: Various scenarios of redistributing the attention probability values of tokensSùëÜSitalic_S,T, andEùê∏Eitalic_E.\nRow 2-4: Redistributing the probability values from eitherTorEùê∏Eitalic_Etokens toSùëÜSitalic_Sdecreases the editing faithfulness.\nRow 5-6: Redistributing the probability values fromSùëÜSitalic_Stoken by a constant factorŒªSsubscriptùúÜùëÜ\\lambda_{S}italic_Œª start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPTto theTandEùê∏Eitalic_Etokens improves the editing faithfulness.\nOurs: Redistributing the probability value of theSùëÜSitalic_Stoken using our proposed dynamic approach achieves the best editing faithfulness.",
                "position": 3278
            },
            {
                "img": "https://arxiv.org/html/2412.12877/x25.png",
                "caption": "Figure 25:Ablation study on IPR: (a)ŒªùúÜ\\lambdaitalic_Œª. IncreasingŒªùúÜ\\lambdaitalic_Œªenhances details and textures, but increasing it too much toŒª=0.6ùúÜ0.6\\lambda=0.6italic_Œª = 0.6may cause artifacts,e.g., the wood carvings get blurry. The best trade-off between details and emergence of artifacts isŒª=0.4ùúÜ0.4\\lambda=0.4italic_Œª = 0.4.",
                "position": 3506
            },
            {
                "img": "https://arxiv.org/html/2412.12877/x26.png",
                "caption": "Figure 26:Ablation study on IPR: (b)ŒªrsubscriptùúÜùëü\\lambda_{r}italic_Œª start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT. DecreasingŒªrsubscriptùúÜùëü\\lambda_{r}italic_Œª start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPTtends to improve overall editing faithfulness but introduces more noise due to the enhanced details,e.g., the shirt becomes more noisy whenŒªrsubscriptùúÜùëü\\lambda_{r}italic_Œª start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPTis reduced. The best trade-off between editing faithfulness and noise isŒªr=0.6subscriptùúÜùëü0.6\\lambda_{r}=0.6italic_Œª start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT = 0.6.",
                "position": 3511
            },
            {
                "img": "https://arxiv.org/html/2412.12877/x27.png",
                "caption": "Figure 27:Ablation study on IPR: (c) percentage of sampling step where we apply our IPR. Increasing the IPR steps percentage generally enhances editing faithfulness, but excessively increasing the percentage (100%percent100100\\%100 %) may introduce artifacts,e.g., the appearance of noise on the glass table. The best trade-off between editing faithfulness and emergence of artifacts for percentage sampling step is80%percent8080\\%80 %.",
                "position": 3516
            }
        ]
    },
    {
        "header": "FAdditional Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.12877/x28.png",
                "caption": "Figure 28:Running time of our method. (a) Running time of our method vs number of video frames. (b) Running time of our method vs number of instances",
                "position": 3571
            },
            {
                "img": "https://arxiv.org/html/2412.12877/x29.png",
                "caption": "Figure 29:Failure case. Our method is less faithful when there is a large number of instances to edit. Note that other SOTA methods also fail to handle this large number of instances to edit.",
                "position": 3591
            },
            {
                "img": "https://arxiv.org/html/2412.12877/x30.png",
                "caption": "Figure 30:Limitation. Our method struggles in scenes with reflective surfaces.",
                "position": 3607
            }
        ]
    },
    {
        "header": "GFailure Case and Limitation",
        "images": []
    }
]