[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13718/x1.png",
                "caption": "Figure 1:Our two stage methodology involving a(1)1(1)( 1 )warmupphase and a(2)2(2)( 2 )target-task adaptationphase for training reasoning models in resource-constrained settings",
                "position": 142
            }
        ]
    },
    {
        "header": "2Warmup phase",
        "images": []
    },
    {
        "header": "3Target-Domain Adaptation",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13718/x2.png",
                "caption": "Figure 2:Absolute percentage increase relative to the base model for MATH, HumanEval+, and MMLU-Pro subsets (Physics & History) for various models: RLVR on base model, warmup only, and RLVR on warmed-up model",
                "position": 398
            },
            {
                "img": "https://arxiv.org/html/2505.13718/x3.png",
                "caption": "Figure 3:Generalization results for base+RLVR, warmup-only, and warmup+RLVR models. Top labels indicate RLVR training domain; bottom labels indicate evaluation domain. Warmup uses no domain-specific data.",
                "position": 437
            },
            {
                "img": "https://arxiv.org/html/2505.13718/x4.png",
                "caption": "Figure 4:Relative change in completion length (vs. base model) after training on each dataset (y-axis) and evaluating on others (x-axis). Top: base model; Bottom: warmed-up model.",
                "position": 450
            },
            {
                "img": "https://arxiv.org/html/2505.13718/x5.png",
                "caption": "",
                "position": 454
            }
        ]
    },
    {
        "header": "4Discussion & Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13718/x6.png",
                "caption": "Figure 5:Results of Qwen2.5-3B K&K distillation. Loss curve shown ontop& performance on MATH500 shown onbottom. Choosing a higher learning rate,2‚Å¢e‚àí52ùëí52e-52 italic_e - 5has a chance of overfitting to the K&K domain rather than learning generalizable reasoning behaviors.",
                "position": 1547
            },
            {
                "img": "https://arxiv.org/html/2505.13718/x7.png",
                "caption": "",
                "position": 1551
            },
            {
                "img": "https://arxiv.org/html/2505.13718/x8.png",
                "caption": "Figure 6:Training curves for Reward(left)and Completion Length(right)for MATH training, smoothed with a moving average (window size 10)",
                "position": 1761
            },
            {
                "img": "https://arxiv.org/html/2505.13718/x9.png",
                "caption": "",
                "position": 1764
            },
            {
                "img": "https://arxiv.org/html/2505.13718/x10.png",
                "caption": "Figure 7:Training curves for Reward(left)and Completion Length(right)for HumanEval+training, smoothed with a moving average (window size 10)",
                "position": 1768
            },
            {
                "img": "https://arxiv.org/html/2505.13718/x11.png",
                "caption": "",
                "position": 1771
            },
            {
                "img": "https://arxiv.org/html/2505.13718/x12.png",
                "caption": "Figure 8:Training curves for Reward(left)and Completion Length(right)for Physics training, smoothed with a moving average (window size 10)",
                "position": 1775
            },
            {
                "img": "https://arxiv.org/html/2505.13718/x13.png",
                "caption": "",
                "position": 1778
            },
            {
                "img": "https://arxiv.org/html/2505.13718/x14.png",
                "caption": "Figure 9:Training curves for Reward(left)and Completion Length(right)for History training, smoothed with a moving average (window size 10)",
                "position": 1782
            },
            {
                "img": "https://arxiv.org/html/2505.13718/x15.png",
                "caption": "",
                "position": 1785
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]