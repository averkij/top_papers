[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.05467/x1.png",
                "caption": "",
                "position": 67
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.05467/x2.png",
                "caption": "Figure 1:Illustration of streaming scenarios. Top: Multi-turn interactions. User issues queries at different timestamps, with each turn involving a new video segment along with accumulated visual and text history. Bottom: Proactive responses. The assistant actively delivers timely feedback or guidance based on the incoming visual stream, without requiring explicit user prompts.",
                "position": 88
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.05467/x3.png",
                "caption": "Figure 2:Overview ofStreamBridge. ‚ûÄ‚ûÇ: Incoming frames are encoded and stored into the memory buffer one by one. ‚ûÅ: A queryùí¨ùí¨\\mathcal{Q}caligraphic_Qis posed. ‚ûÉ: The activation model monitors incoming frames and returns a binary signalùíüùíü\\mathcal{D}caligraphic_D, indicating whether LLM should start answering.‚äótensor-product\\otimes‚äómeans concatenation.",
                "position": 149
            },
            {
                "img": "https://arxiv.org/html/2505.05467/x4.png",
                "caption": "Figure 3:An overview of the proposed activation model. We label the lastP%percentùëÉP\\%italic_P %of frames of each video clip to be true.",
                "position": 317
            }
        ]
    },
    {
        "header": "4Stream-IT Dataset",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.05467/x5.png",
                "caption": "Figure 4:Inference Latency (y-axis) vs. Frame Number (x-axis).",
                "position": 1275
            },
            {
                "img": "https://arxiv.org/html/2505.05467/x5.png",
                "caption": "Figure 4:Inference Latency (y-axis) vs. Frame Number (x-axis).",
                "position": 1278
            },
            {
                "img": "https://arxiv.org/html/2505.05467/x6.png",
                "caption": "Figure 5:Ablation studies on the activation thresholdŒ±ùõº\\alphaitalic_Œ±.",
                "position": 1283
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADatasets Used to Train the Activation Model",
        "images": []
    },
    {
        "header": "Appendix BStream-ITConstruction",
        "images": []
    },
    {
        "header": "Appendix CMore Implementation Details",
        "images": []
    },
    {
        "header": "Appendix DBenchmarks and Metrics",
        "images": []
    },
    {
        "header": "Appendix EFull Results on OVO-Bench and Streaming-Bench",
        "images": []
    },
    {
        "header": "Appendix FLimitations",
        "images": []
    },
    {
        "header": "Appendix GPseudo Code of the Round-Decayed Compression in a PyTorch-like Style",
        "images": []
    }
]