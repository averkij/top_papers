[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06694/x1.png",
                "caption": "Figure 1:Perplexity comparison on WikiText-2.NanoQuantachieves state-of-the-art results among post-training quantization (PTQ) methods and is the only framework effectively enabling sub-1-bit compression while outperforming existing binary baselines.",
                "position": 402
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06694/x2.png",
                "caption": "Figure 2:Illustration of theNanoQuantcompression scheme. The process proceeds in three stages: (a)Factorization, where the weight matrix is decomposed into continuous latent factors (ùêîFP,ùêïFP\\mathbf{U}_{\\text{FP}},\\mathbf{V}_{\\text{FP}}) and floating-point scales (ùê¨1,ùê¨2\\mathbf{s}_{1},\\mathbf{s}_{2}) which are fine-tuned to minimize reconstruction error; (b)Binarization, where these optimized factors are quantized into binary matrices (ùêî¬±1,ùêï¬±1\\mathbf{U}_{\\pm 1},\\mathbf{V}_{\\pm 1}) containing{‚àí1,+1}\\{-1,+1\\}values; and (c)Packing, where these values are mapped to bits (‚àí1‚Üí0,+1‚Üí1-1\\to 0,+1\\to 1) and efficiently packed into integer formats (e.g.,8-bit blocks) for memory efficiency.",
                "position": 472
            }
        ]
    },
    {
        "header": "3NanoQuant",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06694/x3.png",
                "caption": "Figure 3:TheNanoQuantblock reconstruction pipeline for compressing linear layers. The process sequentially optimizes each transformer block through three key phases: (1)Error Propagation Mitigationto adjust full-precision weights for accumulated errors; (2)Low-Rank Binary Initialization, which utilizes Latent Binary ADMM (LB-ADMM) to precisely generate latent binary factors and scales; and (3)Factorized Component Refinement., which fine-tunes the continuous latent matrices and scales using Straight-Through Estimators (STE) before final packing.",
                "position": 508
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06694/x4.png",
                "caption": "Figure 4:On 1 NVIDIA RTX 3050 (8GB),NanoQuantdelivers up to3.6√ó3.6\\timeshigher decoding throughput,5.4√ó5.4\\timeslower peak memory usage, and3.9√ó3.9\\timesgreater energy efficiency compared toBF16baselines for Llama-3.2-1B and 3B models.",
                "position": 1509
            },
            {
                "img": "https://arxiv.org/html/2602.06694/x5.png",
                "caption": "Figure 5:Datacenter inference efficiency on a single NVIDIA H100 (80GB).NanoQuantenables faster decoding throughput while maintaining superior memory and energy efficiency for Llama-2-13B and Qwen-3-32B, compared to the PyTorchBF16baseline.",
                "position": 1530
            },
            {
                "img": "https://arxiv.org/html/2602.06694/x6.png",
                "caption": "Figure 6:Pareto optimality analysis for models in the Qwen3 family.NanoQuantestablishes a new efficiency frontier in the low-bit regime, offering superior accuracy-per-bit trade-offs compared to existing state-of-the-art binary PTQ methods.",
                "position": 1585
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATheoretical Analysis: Magnitude Balancing",
        "images": []
    },
    {
        "header": "Appendix BConvergence Analysis of Low-Rank Binary Initialization",
        "images": []
    },
    {
        "header": "Appendix CImplementation Details",
        "images": []
    },
    {
        "header": "Appendix DFurther Ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06694/x7.png",
                "caption": "Figure 7:LLM decoding performance ofNanoQuant, compared with PyTorch BF16 and vector quantization methods (AQLM, PV-Tuning, QTIP) for 128 input tokens and various output sequence lengths, on 1 NVIDIA H100 GPU.NanoQuantshows superior inference speed, memory efficiency, and energy efficiency, compared to vector quantization methods and BF16.",
                "position": 2807
            },
            {
                "img": "https://arxiv.org/html/2602.06694/x8.png",
                "caption": "(a)self_attn.q_proj(Flip: 0.60%)",
                "position": 2909
            },
            {
                "img": "https://arxiv.org/html/2602.06694/x8.png",
                "caption": "(a)self_attn.q_proj(Flip: 0.60%)",
                "position": 2912
            },
            {
                "img": "https://arxiv.org/html/2602.06694/x9.png",
                "caption": "(b)self_attn.k_proj(Flip: 0.47%)",
                "position": 2917
            },
            {
                "img": "https://arxiv.org/html/2602.06694/x10.png",
                "caption": "(c)self_attn.v_proj(Flip: 2.26%)",
                "position": 2923
            },
            {
                "img": "https://arxiv.org/html/2602.06694/x11.png",
                "caption": "(d)self_attn.o_proj(Flip: 2.11%)",
                "position": 2928
            },
            {
                "img": "https://arxiv.org/html/2602.06694/x12.png",
                "caption": "(e)mlp.gate_proj(Flip: 6.82%)",
                "position": 2934
            },
            {
                "img": "https://arxiv.org/html/2602.06694/x13.png",
                "caption": "(f)mlp.up_proj(Flip: 6.22%)",
                "position": 2939
            },
            {
                "img": "https://arxiv.org/html/2602.06694/x14.png",
                "caption": "(g)mlp.down_proj(Flip: 1.73%)",
                "position": 2945
            }
        ]
    },
    {
        "header": "Appendix EInference Ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06694/x15.png",
                "caption": "Figure 9:On the NVIDIA Jetson TX2, our custom GEMV kernels show significantly faster inference speeds than PyTorch FP16 for various matrix shapes, even for batch sizes up to 16.",
                "position": 3033
            },
            {
                "img": "https://arxiv.org/html/2602.06694/x16.png",
                "caption": "Figure 10:Custom GEMM kernels forNanoQuantachieve competitive batched inference performance withBF16PyTorch on a single NVIDIA A100 80GB GPU.",
                "position": 3140
            }
        ]
    },
    {
        "header": "Appendix FDetailed Model Size Analysis of Binary Weight Quantization Methods",
        "images": []
    }
]