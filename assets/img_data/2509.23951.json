[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23951/assets/banner/banner.jpg",
                "caption": "Figure 1:Multi-ratio text-to-image samples from HunyuanImage 3.0, demonstrating its powerful prompt-following, reasoning, concept generalization and text-rendering capabilities.",
                "position": 106
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Data Preparation",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23951/assets/data/caption_0.png",
                "caption": "Figure 2:Image Captioning Pipeline.",
                "position": 205
            }
        ]
    },
    {
        "header": "3Model Design",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23951/assets/model/model.png",
                "caption": "Figure 3:Illustration of HunyuanImage 3.0.",
                "position": 240
            },
            {
                "img": "https://arxiv.org/html/2509.23951/assets/model/attention.png",
                "caption": "Figure 4:Two types of attention implementation.",
                "position": 272
            },
            {
                "img": "https://arxiv.org/html/2509.23951/assets/model/rope_2d.png",
                "caption": "Figure 5:Illustration of the comparison between 1D RoPE andGeneralized 2D RoPEwith backward compatibility. The text tokens in 1D RoPE has exactly the same positions as 2D RoPE. Intuitively, this generalization is implemented by reshape the 1D image positions into 2D positions and place it in the middle of two text sections.",
                "position": 289
            }
        ]
    },
    {
        "header": "4Model Training",
        "images": []
    },
    {
        "header": "5Model Performance",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23951/assets/performance/ssae_side_by_side_comparison.png",
                "caption": "Figure 6:SSAE evaluation results.",
                "position": 447
            },
            {
                "img": "https://arxiv.org/html/2509.23951/assets/performance/gsb.png",
                "caption": "Figure 7:GSB evaluation results.",
                "position": 472
            },
            {
                "img": "https://arxiv.org/html/2509.23951/assets/performance/image_expert_activation_ratio.png",
                "caption": "Figure 8:Left: Heatmap ofvi​j∑jvi​jvi​j∑jvi​j+ti​j∑jti​j\\frac{\\frac{v_{ij}}{\\sum_{j}v_{ij}}}{\\frac{v_{ij}}{\\sum_{j}v_{ij}}+\\frac{t_{ij}}{\\sum_{j}t_{ij}}}, wherevi​jv_{ij}andti​jt_{ij}denote the image-token and text-token activation counts, respectively, for thejj-th expert in theii-th layer. The darker an expert appears, the more specialized it is to image tokens. Right: KL divergence between{vi​k∑jvi​j}k=064\\{\\frac{v_{ik}}{\\sum_{j}v_{ij}}\\}_{k=0}^{64}and{ti​k∑jti​j}k=064\\{\\frac{t_{ik}}{\\sum_{j}t_{ij}}\\}_{k=0}^{64}for all experts of each layer. As the layer goes deeper, the KL divergence increases and the expert activation distributions become more dispersed across modalities.",
                "position": 475
            },
            {
                "img": "https://arxiv.org/html/2509.23951/assets/performance/image_expert_activation_ratio.png",
                "caption": "",
                "position": 478
            },
            {
                "img": "https://arxiv.org/html/2509.23951/assets/performance/image_text_activated_expert_distribution_kl_divergence.png",
                "caption": "",
                "position": 482
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Project Contributors",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]