[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.22230/x1.png",
                "caption": "Figure 1:Overview of the RLHF Training Framework. Our proposed pipeline consists of two sequential phases: (1) Reward Model Training, where we construct three complementary reward modelsâ€”namely, the Bradley-Terry (BT) model, the Generative Reward Model (GenRM), and Reasoning Task Verifiers (RTV). Specifically, the BT model is trained on pairwise comparisons to capture human preferences, while the GenRM assigns explicit reward scores aligned with these preferences using either ground-truth solutions (for reasoning tasks) or the best-of-N selections identified by the BT model (for general tasks). The RTV component implements specialized validators tailored to specific task requirements, such as code-execution sandboxes for evaluating programming tasks; and (2) Reinforcement Learning Optimization, in which the language model is iteratively optimized using PPO under guidance from both GenRM and RTV. This stage leverages carefully selected training prompts identified through our Pre-PPO prompt-selection method and employs strategic optimization techniques to robustly enhance model performance and alignment.",
                "position": 150
            },
            {
                "img": "https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/train_all.png",
                "caption": "Figure 2:Overall test scores from the initial run using an expanded dataset combining newly collected data (six million prompts) with the original dataset (one million prompts). Despite increasing dataset size substantially, RLHF did not yield improvements in performance. Additionally, the best performance was observed at around the 3,500-step mark, after which test scores gradually declined.",
                "position": 192
            },
            {
                "img": "https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/QRM.png",
                "caption": "Figure 3:Distribution of reward scores for newly collected prompts. The x-axis shows the percentage of prompts. The y-axis represents the reward score range from 0 to 1, with 0.5 indicating parity with the reference. Approximately 90% of prompts received scores above 0.5 for both small-size and large-size models, suggesting apparent superiority over reference outputs. However, manual inspection revealed that many high-scoring outputs exhibited reward hacking behavior and were qualitatively inferior to the original best-selected outcomes.",
                "position": 195
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/FRAC.png",
                "caption": "Figure 4:The distribution of prompts across both math and coding task during the training phases",
                "position": 232
            },
            {
                "img": "https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/AblationStudy.png",
                "caption": "Figure 5:Ablation study on small-size model. We do the ablation study to demonstrate the effectiveness of each strategy. Early Training Emphasis refers to early training emphasis on mathematical and coding tasks",
                "position": 549
            },
            {
                "img": "https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/codemath_first.png",
                "caption": "Figure 6:Early emphasis on mathematical and coding tasks significantly improves RLHF performance in both coding and STEM areas on Testset-V1.0. Notably, the coding performance with this approach surpasses the baseline within just 1000 training steps.",
                "position": 677
            },
            {
                "img": "https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/RM_CMP.png",
                "caption": "Figure 7:Comparison of Reward Hacking Susceptibility and Performance Trends for RTV, GenRM, and BT Reward Models During RLHF Training",
                "position": 684
            },
            {
                "img": "https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/datascale_preppo.png",
                "caption": "Figure 8:Impact of data scaling on Pre-PPO strategy performance. The graph shows the overall RLHF performance as the percentage of newly collected training data increases from 10% to 20% and 50%. Counter-intuitively, increasing the amount of training data leads to a noticeable degradation in performance, suggesting that high-quality training prompts are scarce in real-world settings and that simply scaling data quantity does not guarantee improvement.",
                "position": 687
            },
            {
                "img": "https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/Pre-PPO-BOOST.png",
                "caption": "Figure 9:Data Scale method boost both math and code performance.",
                "position": 740
            },
            {
                "img": "https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/QRM_Score.png",
                "caption": "Figure 10:Comparison of Reward Model Scores across Different Edit Distance Bins for GenRM with and without Ground Truth.",
                "position": 746
            },
            {
                "img": "https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/Score_Diff.png",
                "caption": "Figure 11:Comparison of Score Difference across Different Edit Distance Bins for GenRM with and without Ground Truth, and RTV.",
                "position": 775
            }
        ]
    },
    {
        "header": "5Discussions",
        "images": []
    },
    {
        "header": "6Acknowledgment",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "8The deterioration of model response diversity",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/entropy.png",
                "caption": "(a)Response entropy change during the RLHF training process",
                "position": 1277
            },
            {
                "img": "https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/entropy.png",
                "caption": "(a)Response entropy change during the RLHF training process",
                "position": 1280
            },
            {
                "img": "https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/Entropy_Creation.png",
                "caption": "(b)The comparison of response entropy change during the RLHF training process",
                "position": 1285
            },
            {
                "img": "https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/Entropy_Math.png",
                "caption": "(c)The comparison of response entropy change during the RLHF training process",
                "position": 1291
            },
            {
                "img": "https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/Entropy_RTV.png",
                "caption": "(d)The comparison of response entropy change during the RLHF training process",
                "position": 1296
            }
        ]
    },
    {
        "header": "9Prompt Distribution",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/Big_Dist.png",
                "caption": "Figure 13:Prompts Distribution covering varies domains.",
                "position": 1310
            }
        ]
    },
    {
        "header": "10Case Study",
        "images": []
    }
]