[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17242/x1.png",
                "caption": "Figure 1:LVSM supports feed-forward novel view synthesis from sparse posed image inputs (even from a single view) on both objects and scenes. LVSM achieves significant quality improvements compared with the previous SOTA method, i.e., GS-LRM(Zhang et al.,2024). (Please zoom in for more details.)",
                "position": 107
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17242/x2.png",
                "caption": "Figure 2:LVSM model architecture.LVSM first patchifies the posed input images into tokens. The target view to be synthesized is represented by its Plücker ray embeddings and is also tokenized. The input view and target tokens are sent to a full transformer-based model to predict the tokens that are used to regress the target view pixels. We study two LVSM transformer architectures, as aDecoder-onlyarchitecture (left) and aEncoder-Decoderarchitecture (right).",
                "position": 172
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17242/x3.png",
                "caption": "Figure 3:Object-level visual comparison at 512 resolution.Given 4 sparse input posed images (leftmost column), we compare our high-res object-level novel-view rendering results with two baselines: Instant3D’sTriplane-LRM(Li et al.,2023)andGS-LRM(Res-512)(Zhang et al.,2024). Both our Encoder-Decoder and Decoder-Only models exhibit fewer floaters (first example) and fewer blurry artifacts (second example), compared to the baselines. Our Decoder-Only model effectively handles complex geometry, including small holes (third example) and thin structures (fourth example). Additionally, it preserves the details of high-frequency texture (last example).",
                "position": 565
            },
            {
                "img": "https://arxiv.org/html/2410.17242/x4.png",
                "caption": "Figure 4:Scene-level visual comparison.We evaluate our encoder-decoder and decoder-only models on scene-level view synthesis, comparing them against the prior leading baseline methods, namely pixelSplat(Charatan et al.,2024), MVSplat(Chen et al.,2024), and GS-LRM(Zhang et al.,2024). Our methods exhibit fewer texture and geometric artifacts, generate more accurate and realistic specular reflections, and are closer to the ground truth images.",
                "position": 581
            },
            {
                "img": "https://arxiv.org/html/2410.17242/extracted/5946542/figures/NVS_PSNR_under_different_input_view_number_gso.png",
                "caption": "Figure 7:Zero-shot generalization to different number of input imageson the GSO dataset(Downs et al.,2022). We note that all models are trained with just 4 input views.",
                "position": 753
            },
            {
                "img": "https://arxiv.org/html/2410.17242/extracted/5946542/figures/NVS_PSNR_under_different_input_view_number_gso.png",
                "caption": "Figure 7:Zero-shot generalization to different number of input imageson the GSO dataset(Downs et al.,2022). We note that all models are trained with just 4 input views.",
                "position": 756
            },
            {
                "img": "https://arxiv.org/html/2410.17242/extracted/5946542/figures/NVS_PSNR_under_different_input_view_fps.png",
                "caption": "Figure 8:Rendering FPS with different number of input images.We refer to rendering as the decoding process, which synthesizes novel views from latent tokens or input images.",
                "position": 763
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17242/x5.png",
                "caption": "Figure 9:Object-level visual comparison at 256 resolution.Comparing with the two baselines:LGM(Tang et al.,2024)andGS-LRM(Res-256)(Zhang et al.,2024),\nboth our Encoder-Decoder and Decoder-Only models have fewer floater artifacts (last example), and can generate more accurate view-dependent effects (third example). Our Decoder-Only model can better preserve the texture details (first two examples).",
                "position": 1847
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]