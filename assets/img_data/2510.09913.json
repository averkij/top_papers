[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09913/x1.png",
                "caption": "Figure 1:Model responses are not monolithic: they switch across diverse skills which favor different model checkpoints in the training pipeline, thus we introduce model-guided collaborative inference to optimally use models with diverse skills for different segments of response generation.",
                "position": 97
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09913/x2.png",
                "caption": "Figure 2:Overview ofSwitch Generation, where multiple model checkpoints in the training pipeline (e.g., pretrained, finetuned, and aligned LM checkpoints) are dynamically selected to generate text segments in a sequence. (Up) We derive training data for the switcher LMffby rolling out which model would lead to the best average outcome for a particular query and trace. (Down) At inference time, multiple models are guided by the trained switcher LM to generate text segments as part of a response when their skills and strengths are most needed.",
                "position": 137
            }
        ]
    },
    {
        "header": "2Methodology",
        "images": []
    },
    {
        "header": "3Experiment Settings",
        "images": []
    },
    {
        "header": "4Results",
        "images": []
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09913/x3.png",
                "caption": "Figure 3:Distilling the collaboration patterns ofSwitch Generationback into the aligned model. Distillation recovers 58% of the collaboration gains with only one fourth of the inference cost.",
                "position": 724
            },
            {
                "img": "https://arxiv.org/html/2510.09913/x4.png",
                "caption": "Figure 4:Correlation between the performance and helpfulness of the pretrained model. While not being the best individual model, it is consistently helpful in the model collaboration system.",
                "position": 732
            },
            {
                "img": "https://arxiv.org/html/2510.09913/x5.png",
                "caption": "Figure 5:Frequency and treatment effect of 2-length (left) and 3-length (middle) switching sequences, as well as their correlation across three task categories for 3-length sequences (right).Switch Generationlearns to identify helpful switching patterns and frequently leverages them.",
                "position": 972
            },
            {
                "img": "https://arxiv.org/html/2510.09913/x6.png",
                "caption": "Figure 6:Roles that each model plays in their generated segments, averaged across all datasets.",
                "position": 1054
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09913/x7.png",
                "caption": "Figure 7:Frequency of pretrained, finetuned, and aligned models being used in the begin/middle/end of the sequence.",
                "position": 2094
            }
        ]
    },
    {
        "header": "Appendix AAnalysis (cont.)",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09913/x8.png",
                "caption": "Figure 8:Switching frequency and P-helpfulness across tasks.",
                "position": 2115
            }
        ]
    },
    {
        "header": "Appendix BExperiment Details",
        "images": []
    }
]