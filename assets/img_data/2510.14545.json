[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14545/x1.png",
                "caption": "Figure 1.Performance overview of AEPO algorithm.",
                "position": 178
            },
            {
                "img": "https://arxiv.org/html/2510.14545/x2.png",
                "caption": "Figure 2.Two high-entropy challenges in agentic RL.(1) High-Entropy Rollout Collapse: Over-branching at high-entropy steps along specific paths, limiting exploration of other potential correct branches;(2) High-Entropy Token Gradient Clipping:Consistent clipping of high-entropy token gradients during policy updates hinders learning effective exploration behaviors.",
                "position": 181
            },
            {
                "img": "https://arxiv.org/html/2510.14545/x3.png",
                "caption": "Figure 3.Quantitative statistics of two entropy-based challenges in web agent RL training.",
                "position": 245
            }
        ]
    },
    {
        "header": "2.Preliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14545/x4.png",
                "caption": "Figure 4.The overview of Agentic Entropy-Balanced Policy Optimization.",
                "position": 305
            }
        ]
    },
    {
        "header": "3.Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14545/x5.png",
                "caption": "Figure 5.The comparison analysis of Qwen3-14B using ARPO and AEPO across Pass@1 to Pass@5 metrics.",
                "position": 2479
            }
        ]
    },
    {
        "header": "4.Experiment Settings",
        "images": []
    },
    {
        "header": "5.Experiment Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14545/figures/tsne_visual.png",
                "caption": "Figure 6.Visualization of Rollout diversity: ARPO (left) and AEPO (right)",
                "position": 2920
            },
            {
                "img": "https://arxiv.org/html/2510.14545/x6.png",
                "caption": "Figure 7.The comparison of branch sampling distribution in rollout (left); The comparison of tool-call efficiency across four RL algorithms (right).",
                "position": 2949
            },
            {
                "img": "https://arxiv.org/html/2510.14545/x7.png",
                "caption": "Figure 8.Visualization of training dynamics, including entropy loss(left) and accuracy (right) across training steps",
                "position": 2986
            }
        ]
    },
    {
        "header": "6.Related Work",
        "images": []
    },
    {
        "header": "7.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProof of the Gradient of AEPO",
        "images": []
    },
    {
        "header": "Appendix BDiscussion of the Gradient Forms in Clipping-optimized RL",
        "images": []
    },
    {
        "header": "Appendix CBaselines",
        "images": []
    },
    {
        "header": "Appendix DThe Overall Algorithm Workflow of AEPO",
        "images": []
    }
]