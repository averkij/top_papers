[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21500/x1.png",
                "caption": "Figure 1:ViewSpatial-Bench for multi-perspective spatial reasoning. Our benchmark evaluates spatial localization capabilities from both camera and human perspectives across five task types.",
                "position": 128
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3ViewSpatial-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21500/x2.png",
                "caption": "Figure 2:ViewSpatial-Bench construction pipeline. From data collection to QA generation across camera perspective () and human perspective () tasks. The pipeline includes metadata creation, automatic filtering, spatial relation extraction, and manual verification.",
                "position": 212
            },
            {
                "img": "https://arxiv.org/html/2505.21500/x3.png",
                "caption": "Figure 3:Distribution of task categories in ViewSpatial-Bench, balanced between ScanNet-Source and CoCo-Source approaches, with five distinct subtasks for comprehensive evaluation of spatial reasoning across different viewpoints.",
                "position": 399
            },
            {
                "img": "https://arxiv.org/html/2505.21500/x4.png",
                "caption": "Figure 4:Frequency distributions in ViewSpatial-Bench. (a) Distribution of spatial prepositions, showing comprehensive coverage of directional relationships. (b) Frequency of the top 20 objects, demonstrating the benchmark’s focus on common entities encountered in everyday environments.",
                "position": 408
            },
            {
                "img": "https://arxiv.org/html/2505.21500/x5.png",
                "caption": "",
                "position": 418
            }
        ]
    },
    {
        "header": "4Multi-View Spatial Model",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21500/x6.png",
                "caption": "Figure 5:The image compares spatial reasoning performance between GPT-4o and MVSM on the VSI-App dataset, showing several examples where MVSM correctly answers perspective-taking questions about object locations, while GPT-4o makes errors when attempting to determine spatial relationships from another person’s viewpoint.",
                "position": 1261
            }
        ]
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21500/x7.png",
                "caption": "Figure 6:Wordcloud of object categories.",
                "position": 1826
            }
        ]
    },
    {
        "header": "Appendix BData Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21500/x8.png",
                "caption": "Figure 7:ViewSpatial-Bench Examples (Part1). Performance comparison of three models (Qwen2.5-VL(3B), GPT-4o, and MVSM) on five spatial reasoning tasks from camera perspective () and human perspective ().",
                "position": 2050
            },
            {
                "img": "https://arxiv.org/html/2505.21500/x9.png",
                "caption": "Figure 8:ViewSpatial-Bench Examples (Part2).",
                "position": 2053
            },
            {
                "img": "https://arxiv.org/html/2505.21500/x10.png",
                "caption": "Figure 9:ViewSpatial-Bench Examples (Part3).",
                "position": 2056
            }
        ]
    },
    {
        "header": "Appendix CExperiments",
        "images": []
    }
]