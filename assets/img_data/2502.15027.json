[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15027/extracted/6214140/figures/hands-human-ai.png",
                "caption": "",
                "position": 75
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15027/x1.png",
                "caption": "Figure 1:Illustration of an interactive feedback scenario. When models generate incorrect responses, human users provide pertinent feedback to iteratively refine the answers.",
                "position": 107
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15027/x2.png",
                "caption": "Figure 2:Overview of the test data construction process for InterFeedback-Bench. For each LMM serving as the feedback receiver, we process each instance from a target dataset (e.g., MathVerse) and collect the error cases to form a negative set. The feedback provider then processes the same instances to build a positive set. Finally, we curate test data by selecting the intersection of both sets.",
                "position": 173
            }
        ]
    },
    {
        "header": "3InterFeedback-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15027/x3.png",
                "caption": "Figure 3:Overview of the proposed framework InterFeedback for assessing an LMMâ€™s ability to improve itself through feedback. The model interacts with humans to progressively solve a problem, and after each conversation round, we verify the correctness of the answer. If the answer is incorrect, an LMM-stimulated human will provide constructive feedback. We implement two types of feedback to investigate the behavior of LMMs.",
                "position": 229
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15027/x4.png",
                "caption": "Figure 4:Distribution of samples being corrected in each round. We can observe that Claude-3.5-Sonnet archives the best performance in round 0.",
                "position": 887
            },
            {
                "img": "https://arxiv.org/html/2502.15027/x5.png",
                "caption": "",
                "position": 895
            },
            {
                "img": "https://arxiv.org/html/2502.15027/x6.png",
                "caption": "",
                "position": 900
            },
            {
                "img": "https://arxiv.org/html/2502.15027/x7.png",
                "caption": "",
                "position": 905
            },
            {
                "img": "https://arxiv.org/html/2502.15027/x8.png",
                "caption": "Figure 5:Distribution of corrected samples across various task categories. Visual logic tasks are mostly resolved within the first two rounds, whereas Math (Text-only) and MMMU-Pro tasks show little corrections in rounds 1 and 2. In contrast, Coding (Text-only) and MathVerse tasks exhibit corrections during rounds 1 and 2.",
                "position": 917
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Experimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15027/x9.png",
                "caption": "Figure 6:Qualitative results on different LMMs.",
                "position": 1835
            },
            {
                "img": "https://arxiv.org/html/2502.15027/x10.png",
                "caption": "Figure 7:Qualitative results on different LMMs.",
                "position": 1838
            },
            {
                "img": "https://arxiv.org/html/2502.15027/x11.png",
                "caption": "Figure 8:An example that model tends to guess answers.",
                "position": 1841
            },
            {
                "img": "https://arxiv.org/html/2502.15027/x12.png",
                "caption": "Figure 9:An example that model tends to guess answers.",
                "position": 1844
            },
            {
                "img": "https://arxiv.org/html/2502.15027/x13.png",
                "caption": "Figure 10:Qualitative results by removing GT answer in level 3 feedback.",
                "position": 1847
            }
        ]
    },
    {
        "header": "Appendix BQualitative Examples.",
        "images": []
    }
]