[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21688/imgs/rocket_16761028.png",
                "caption": "",
                "position": 58
            },
            {
                "img": "https://arxiv.org/html/2511.21688/imgs/github-logo_25231.png",
                "caption": "",
                "position": 64
            },
            {
                "img": "https://arxiv.org/html/2511.21688/x1.png",
                "caption": "",
                "position": 77
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21688/x2.png",
                "caption": "Figure 2:Our model,G2VLM, employs an architecture inspired by the two-streams hypothesis. It features two experts: ageometric perceptionexpert (our “where pathway”) for visual geometry learning and asemantic perceptionexpert (our “what pathway”) for multimodal understanding.",
                "position": 110
            },
            {
                "img": "https://arxiv.org/html/2511.21688/x3.png",
                "caption": "Figure 3:We presentG2VLM, a unified model that integrates both a geometric perception expert for 3D reconstruction and a semantic perception expert for multimodal understanding and spatial reasoning tasks. All tokens can do shared multi-modal self attention in each transformer block.",
                "position": 113
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Unified Spatial Vision-Language Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21688/imgs/loss_plot.png",
                "caption": "Figure 4:Comparison of three different loss supervision mechanisms for the joint-training stage. Note that for visual geometry scores, lower is better. TheVG + CE Lossapproach yields the best performance, demonstrating that combining visual geometry and spatial understanding supervision mutually benefits spatial reasoning tasks.",
                "position": 298
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21688/x4.png",
                "caption": "Figure 5:Qualitative results of our model.G2VLMeffectively reconstructs a diverse set of open-domain images, spanning object-level, structure-level, indoor, and outdoor scenes, including both dynamic and static content.",
                "position": 862
            },
            {
                "img": "https://arxiv.org/html/2511.21688/imgs/clip_dino_and_attloss_plot2.png",
                "caption": "Figure 6:Experimental study results. (a) The dual encoder design, with both a semantic-rich CLIP encoder and a low-level vision DINO encoder, yields the best performance on both visual geometry and spatial understanding tasks. (b) Training loss curves for three different attention mechanisms during geometric perception expert training; global attention is consistently the best variant.",
                "position": 897
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "AArchitecture Details",
        "images": []
    },
    {
        "header": "BTraining Details",
        "images": []
    },
    {
        "header": "CMore results",
        "images": []
    }
]