[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03013/x1.png",
                "caption": "",
                "position": 117
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03013/x2.png",
                "caption": "Figure 2:Overview of Sync-LoRA.Given a source videoSS, an edited first frameII, and an edit promptPP, Sync-LoRA denoises the target videoTTconditioned on these inputs.\nDuring training, only the edited branch is noised, while the source branch stays clean and provides motion and identity cues through shared attention, so the model copies motion fromSSand propagates the local edit across all frames.",
                "position": 166
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03013/x3.png",
                "caption": "Figure 3:Data generation and curation pipeline.Our process constructs synchronized video pairs for Sync-LoRA training.(Top)Portrait images are generated, edited, and converted into side-by-side talking-head videos.(Middle)Facial and pose landmarks yield motion signals for speech, gaze, blink, and pose.(Bottom)Pairs are scored and filtered by synchronization quality, keeping only the most aligned examples for training.",
                "position": 235
            },
            {
                "img": "https://arxiv.org/html/2512.03013/x4.png",
                "caption": "Figure 4:Synchronization signal visualization.Two synchronization cues used in our filtering process.Top:Eye landmarks are used to compute the Eye Aspect Ratio (EAR). Note how the plotted peaks (reference in green, edited in orange) correspond directly to the blink event shown in the frames above.Bottom:Upper-body pose landmarks are used to track the right elbow angle. The plots again show tightly correlated motion, confirming the arm movement is synchronized across both videos.",
                "position": 257
            },
            {
                "img": "https://arxiv.org/html/2512.03013/x5.png",
                "caption": "Figure 5:Comparison of portrait video editing methods.The rows show the source video and results from LucyEdit, VACE, AnyV2V, FlowEdit, and Sync-LoRA (Ours).\nThe columns depict different temporal positions.\nOur method, VACE, AnyV2V, and FlowEdit utilize the same edited first frame as visual input, whereas the text-based LucyEdit operates from text guidance alone.",
                "position": 274
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03013/x6.png",
                "caption": "Figure 6:Necessity of all synchronization cues.Each column shows results when training without the specified motion cue (pose, gaze, speech, or blink) from the filtering stage, compared to our full setup. The source video is shown on the left. Omitting any cue causes motion drift or misalignment across frames.",
                "position": 551
            },
            {
                "img": "https://arxiv.org/html/2512.03013/x7.png",
                "caption": "Figure 7:Effect of dataset composition strategy.Qualitative comparison of different dataset composition strategies. Each column shows a distinct training setup: ID-Only (identical pairs), Edit-Only (edited pairs), Random (unfiltered pairs). Our full method is on rightmost column and the source video is on the left.",
                "position": 559
            },
            {
                "img": "https://arxiv.org/html/2512.03013/x8.png",
                "caption": "Figure 8:Expression editing.Sync-LoRA performs expression editing (Happy,Angry,Sad) while keeping motion synchronized and geometry consistent, even under occlusions.",
                "position": 721
            }
        ]
    },
    {
        "header": "5Conclusions, Limitations and Future work",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Additional Implementation Details",
        "images": []
    },
    {
        "header": "7Data Curation Pipeline",
        "images": []
    },
    {
        "header": "8Evaluation Benchmark and Metrics",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03013/x9.png",
                "caption": "Figure 9:Benchmark Examples.examples",
                "position": 1876
            }
        ]
    },
    {
        "header": "9User Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03013/x10.png",
                "caption": "Figure 10:User Study Results.Pairwise preference rates for Sync-LoRA (Ours) against four baselines. Our method is strongly preferred across all four criteria: Edit Fidelity, Synchronization, Identity Preservation, and Overall Preference.",
                "position": 1982
            }
        ]
    },
    {
        "header": "10Supplementary Videos",
        "images": []
    },
    {
        "header": "11Expression Modification",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03013/x11.png",
                "caption": "Figure 11:Expression editing robustness.Comparison between LivePortrait[20]and Sync-LoRA (Ours).",
                "position": 2003
            },
            {
                "img": "https://arxiv.org/html/2512.03013/x12.png",
                "caption": "Figure 12:Limitations of Sync-LoRA.The figure illustrates two primary failure modes. (Left) Spatial and synchronization degradation during non-aligned geometric edits (zoom-out), resulting in blurred facial features and temporal drift. (Right) Loss of detail and warping artifacts on fast-moving, complex regions (hands) during complex appearance modifications.",
                "position": 2019
            },
            {
                "img": "https://arxiv.org/html/2512.03013/x13.png",
                "caption": "Figure 13:Data Curation Filtering.Examples from our automatic filtering pipeline. (Top)Acceptedclips exhibit high synchronization scores, indicating precise temporal alignment and stable motion. (Bottom)Rejectedclips receive low synchronization scores due to temporal or spatial misalignments and motion drift.",
                "position": 2022
            }
        ]
    },
    {
        "header": "12Limitations and Failure Cases",
        "images": []
    }
]