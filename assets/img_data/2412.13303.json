[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.13303/",
                "caption": "(a)Qwen2-0.5B",
                "position": 119
            },
            {
                "img": "https://arxiv.org/html/2412.13303/",
                "caption": "(a)Qwen2-0.5B",
                "position": 122
            },
            {
                "img": "https://arxiv.org/html/2412.13303/",
                "caption": "(b)Vicuna-7B",
                "position": 128
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.13303/",
                "caption": "Figure 2:Overview of the FastVLM architecture.FastVLM consists of our novel vision encoder, FastViTHD, trained using the same setup as LLaVa. The FastViTHD architecture is designed and trained for low latency at high resolution,\nutilizing novel multi-scale pooling, additional self-attention layers, and downsampling to generate 4√ó\\times√ófewer tokens than FastViT, and 16√ó\\times√ófewer tokens than ViT-L/14 at resolution 336.",
                "position": 191
            },
            {
                "img": "https://arxiv.org/html/2412.13303/",
                "caption": "Figure 3:Novel scaling strategy of FastViTHD lowers latency\nat various image resolutions.FastViT-Naive, a naive scaling of the FastViT architecture, and our proposed FastViTHD have the same number of parameters. ConvNeXt-L is provided for reference. All models are benchmarked on M1 Macbook Pro and trained with LLaVA-1.5 setup and Vicuna 7B. Note that theyùë¶yitalic_y-axis is in log scale.",
                "position": 361
            },
            {
                "img": "https://arxiv.org/html/2412.13303/",
                "caption": "Figure 4:FastViTHD improves the Pareto-Optimal curve for accuracy versus time to first token compared with FastViT.Comparison of FastViT and FastViTHD backbones paired with Qwen2[79]family (chat variant) LLMs of varying sizes and different image resolutions (annotated for each point). The Pareto-optimal curve is highlighted for the two vision backbones. Training setup is LLaVA-1.5. Note that thexùë•xitalic_x-axis is in log scale.",
                "position": 591
            },
            {
                "img": "https://arxiv.org/html/2412.13303/",
                "caption": "Figure 5:Vision latency dominates at high resolution.Breakdown of FastVLM‚Äôs time to first token for varying image resolutions. Vision encoder is FastViTHD and LLM is Qwen2-1.5B.",
                "position": 596
            },
            {
                "img": "https://arxiv.org/html/2412.13303/",
                "caption": "Figure 6:Dynamic input resolution (AnyRes) is only optimal at the highest resolution when using fewer tiles (2√ó\\times√ó2).The vision encoder is FastViTHD. The tile grid size is specified in parenthesis. Training setup is LLaVA-1.5 with Vicuna 7B. Note that thexùë•xitalic_x-axis is in log scale.",
                "position": 601
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "ATraining Setup",
        "images": []
    },
    {
        "header": "BArchitecture Details",
        "images": []
    },
    {
        "header": "CAdditional Results",
        "images": []
    },
    {
        "header": "DDatasets",
        "images": []
    }
]