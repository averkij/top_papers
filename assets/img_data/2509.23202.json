[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background on Microscaling Floating-Point Formats",
        "images": []
    },
    {
        "header": "3A Quantization Error Analysis of NVFP4 and MXFP4",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23202/x1.png",
                "caption": "Figure 1:Distribution fits for aggregate weights and activations of Llama-3.1-8B-Instruct, with and without rotations. The Normal distribution is clearly a good fit for rotated weights and activations, while the Laplace distribution provides a good fit for the native distributions. Although native weights appear Normal, they have much heavier tails, as evidenced by the Kurtosis value.",
                "position": 270
            },
            {
                "img": "https://arxiv.org/html/2509.23202/x2.png",
                "caption": "Figure 2:The effect of Hadamard Transform (HT) on MXFP4 (E8M0) and NVFP4 (E4M3) quantization on Laplace distribution samples and Llama-3.1-8B-Instruct weights and activations for various group sizes.",
                "position": 457
            },
            {
                "img": "https://arxiv.org/html/2509.23202/x3.png",
                "caption": "",
                "position": 461
            }
        ]
    },
    {
        "header": "4MR-GPTQ: An FP4-Focused Variant of the GPTQ Algorithm",
        "images": []
    },
    {
        "header": "5Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23202/x4.png",
                "caption": "",
                "position": 889
            },
            {
                "img": "https://arxiv.org/html/2509.23202/x5.png",
                "caption": "(a)NVFP4 accuracy across GPTQ variants.",
                "position": 1072
            },
            {
                "img": "https://arxiv.org/html/2509.23202/x5.png",
                "caption": "(a)NVFP4 accuracy across GPTQ variants.",
                "position": 1075
            },
            {
                "img": "https://arxiv.org/html/2509.23202/x6.png",
                "caption": "(b)MXFP4 accuracy across GPTQ variants.",
                "position": 1080
            },
            {
                "img": "https://arxiv.org/html/2509.23202/x7.png",
                "caption": "(c)NVFP4 averages and standard deviations.",
                "position": 1086
            },
            {
                "img": "https://arxiv.org/html/2509.23202/x8.png",
                "caption": "(d)MXFP4 averages and standard deviations.",
                "position": 1091
            },
            {
                "img": "https://arxiv.org/html/2509.23202/x9.png",
                "caption": "Figure 6:QuTLASS performance for weights and activations while increasing batch size, for a single linear LLM layer (left), and end-to-end using our vLLM integration (right).",
                "position": 1137
            },
            {
                "img": "https://arxiv.org/html/2509.23202/x9.png",
                "caption": "",
                "position": 1140
            },
            {
                "img": "https://arxiv.org/html/2509.23202/x10.png",
                "caption": "",
                "position": 1144
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Technical Appendices and Supplementary Material",
        "images": []
    },
    {
        "header": "Appendix AWeight-Only Quantization Results",
        "images": []
    },
    {
        "header": "Appendix BReal Quantization Results",
        "images": []
    },
    {
        "header": "Appendix CScale quantization analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23202/x11.png",
                "caption": "Figure 7:MSErel\\mathrm{MSE}^{\\mathrm{rel}}for the weights of 15th block in the Llama-3.1-8B-Instruct model.",
                "position": 3250
            },
            {
                "img": "https://arxiv.org/html/2509.23202/x12.png",
                "caption": "Figure 8:MSErel\\mathrm{MSE}^{\\mathrm{rel}}for the activations of 15th block in the Llama-3.1-8B-Instruct model.",
                "position": 3253
            }
        ]
    },
    {
        "header": "Appendix DOutliers Analysis",
        "images": []
    },
    {
        "header": "Appendix EQuTLASS results on GeForce GPUs",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23202/x13.png",
                "caption": "Figure 9:Illustration of QuTLASS performance for weights and activations on MXFP4 while increasing batch size, for a single linear LLM layer, showing the low-overhead of the quantization-related ops, and end-to-end using the Transformers library.",
                "position": 3485
            },
            {
                "img": "https://arxiv.org/html/2509.23202/x13.png",
                "caption": "",
                "position": 3488
            },
            {
                "img": "https://arxiv.org/html/2509.23202/figures/qwen3-8b-end-to-end-prefill-speedup-mxfp4-vs-bf16-on-rtx5090.png",
                "caption": "",
                "position": 3492
            }
        ]
    },
    {
        "header": "Appendix FStandard deviation",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23202/x14.png",
                "caption": "Figure 10:Accuracy results for NVFP4 and MXFP4 across different combinations of MR-GPTQ components, averaged over five random seeds using vLLM kernels on the benchmark suite.",
                "position": 3512
            },
            {
                "img": "https://arxiv.org/html/2509.23202/x14.png",
                "caption": "",
                "position": 3515
            },
            {
                "img": "https://arxiv.org/html/2509.23202/x15.png",
                "caption": "",
                "position": 3520
            },
            {
                "img": "https://arxiv.org/html/2509.23202/x16.png",
                "caption": "Figure 11:Accuracy results for MXFP4 across different MR-GPTQ component combinations on the Platinum benchmark tasks.",
                "position": 3526
            },
            {
                "img": "https://arxiv.org/html/2509.23202/x17.png",
                "caption": "Figure 12:Accuracy results for NVFP4 across different MR-GPTQ component combinations on the Platinum benchmark tasks.",
                "position": 3529
            },
            {
                "img": "https://arxiv.org/html/2509.23202/x18.png",
                "caption": "Figure 13:Average recovery scores and standard deviations for NVFP and MXFP methods on the Platinum benchmarks.",
                "position": 3532
            },
            {
                "img": "https://arxiv.org/html/2509.23202/x18.png",
                "caption": "",
                "position": 3535
            },
            {
                "img": "https://arxiv.org/html/2509.23202/x19.png",
                "caption": "",
                "position": 3540
            }
        ]
    },
    {
        "header": "Appendix GThe Effect of Different Linear Transforms",
        "images": []
    }
]