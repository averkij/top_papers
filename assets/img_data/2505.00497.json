[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.00497/x1.png",
                "caption": "Figure 1:KeySync‚Äôs contributions.Unlike existing methods, KeySync generates high-resolution lip-synced videos that are closely aligned with the driving audio while minimizing leakage from the input video and seamlessly handling facial occlusions.",
                "position": 144
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.00497/x2.png",
                "caption": "Figure 2:Overview of the KeySync framework.KeySync consists of two stages, both of which involve generating video using latent diffusion conditioned on an input video and audio, differing only in the reference frames selection, as described in (b). During keyframe generation, the model receives an identity framexidsubscriptùë•idx_{\\text{id}}italic_x start_POSTSUBSCRIPT id end_POSTSUBSCRIPT, which is repeated and concatenated with the noised video input. During interpolation, the model is conditioned on two successive keyframeszisubscriptùëßùëñz_{i}italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTandzi+1subscriptùëßùëñ1z_{i+1}italic_z start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT, along with intermediate learnable embeddingszmsubscriptùëßùëöz_{m}italic_z start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT. Both stages integrate audio embeddingsaùëéaitalic_afrom HuBERT[22]. In (c), we illustrate our occlusion handling pipeline, which we apply during inference.",
                "position": 200
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.00497/x3.png",
                "caption": "Figure 3:Qualitative comparison with other works.The top row (‚ÄúTarget lips‚Äù) shows lip movements corresponding directly to the provided audio input, and can therefore be seen as the target for the lips in the generated videos.",
                "position": 627
            },
            {
                "img": "https://arxiv.org/html/2505.00497/x4.png",
                "caption": "Figure 4:Qualitative leakage comparison.We condition the models on silent audio and non-silent video (first row).",
                "position": 630
            },
            {
                "img": "https://arxiv.org/html/2505.00497/x5.png",
                "caption": "Figure 5:MAR over time.If MAR exceeds the threshold, the mouth is considered open, indicating leakage.",
                "position": 633
            },
            {
                "img": "https://arxiv.org/html/2505.00497/x6.png",
                "caption": "Figure 6:Occlusion handling comparison.We present qualitative results on the left and quantitative results on the right.",
                "position": 636
            },
            {
                "img": "https://arxiv.org/html/2505.00497/x7.png",
                "caption": "Figure 7:Examples of different masking techniques.",
                "position": 787
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADatasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.00497/x8.png",
                "caption": "Figure 8:Examples of problematic videos in CelebV-HQ and CelebV-Text.",
                "position": 1703
            }
        ]
    },
    {
        "header": "Appendix BImplementation details",
        "images": []
    },
    {
        "header": "Appendix CLipLeak",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.00497/x9.png",
                "caption": "Figure 9:LipLeak measurement example.",
                "position": 1920
            },
            {
                "img": "https://arxiv.org/html/2505.00497/x10.png",
                "caption": "Figure 10:LipLeak as a function of the MAR threshold.",
                "position": 1923
            }
        ]
    },
    {
        "header": "Appendix DOcclusion handling",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.00497/x11.png",
                "caption": "Figure 11:Effectiveness of Occlusion Handling Across Different Methods.",
                "position": 1959
            }
        ]
    },
    {
        "header": "Appendix EUser study results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.00497/x12.png",
                "caption": "Figure 12:User study interface.Participants were shown side-by-side videos and asked to select the preferred one based on lip synchronization, coherence, and quality.",
                "position": 1969
            },
            {
                "img": "https://arxiv.org/html/2505.00497/x13.png",
                "caption": "Figure 13:Elo ratings in the reconstruction setting.Higher ratings indicate better performance in generating videos with original audio as input.",
                "position": 1997
            },
            {
                "img": "https://arxiv.org/html/2505.00497/x14.png",
                "caption": "Figure 14:Elo ratings in the cross-sync setting.Higher ratings indicate better performance in generating videos with different audio from input.",
                "position": 2000
            },
            {
                "img": "https://arxiv.org/html/2505.00497/x15.png",
                "caption": "Figure 15:Distribution of Elo ratings across all evaluated models.This histogram illustrates the spread of Elo scores, highlighting performance gaps or clustering amongst different models.",
                "position": 2009
            },
            {
                "img": "https://arxiv.org/html/2505.00497/x16.png",
                "caption": "Figure 16:Win rate matrix for pairwise model comparisons.Each cell represents the proportion of matchups where one model outperforms another, offering insight into head-to-head performance.",
                "position": 2018
            }
        ]
    },
    {
        "header": "Appendix FAdditional ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.00497/x17.png",
                "caption": "Figure 17:Examples of inconsistent mouth regions obtained by training with an additional LPIPS pixel loss.",
                "position": 2137
            }
        ]
    },
    {
        "header": "Appendix GLimitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.00497/x18.png",
                "caption": "Figure 18:Impact of head pose on model performance.",
                "position": 2151
            },
            {
                "img": "https://arxiv.org/html/2505.00497/x19.png",
                "caption": "Figure 19:Examples of generated videos at different angles.",
                "position": 2154
            }
        ]
    },
    {
        "header": "Appendix HAdditional qualitative results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.00497/x20.png",
                "caption": "Figure 20:Additional qualitative comparison.",
                "position": 2164
            },
            {
                "img": "https://arxiv.org/html/2505.00497/x21.png",
                "caption": "Figure 21:Qualitative comparison on non-human ids.",
                "position": 2167
            }
        ]
    },
    {
        "header": "Appendix IEthical Considerations and Social Impact",
        "images": []
    }
]