[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.16406/x1.png",
                "caption": "(a)Parameter-Efficient-Fine-Tuning",
                "position": 204
            },
            {
                "img": "https://arxiv.org/html/2506.16406/x1.png",
                "caption": "(a)Parameter-Efficient-Fine-Tuning",
                "position": 207
            },
            {
                "img": "https://arxiv.org/html/2506.16406/x2.png",
                "caption": "(b)Drag-and-Drop",
                "position": 212
            }
        ]
    },
    {
        "header": "2Drag-and-Drop Your LLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.16406/x3.png",
                "caption": "Figure 2:Our approach obtains dragging ability via two processes: prepare training data (upper left) and training the parameter generator (upper right). When preparing training data, we explicitly pair parameters with dataset-specific conditions. During training,DnDtakes condition as input and generate parameters, using original parameters as supervision.",
                "position": 294
            },
            {
                "img": "https://arxiv.org/html/2506.16406/x4.png",
                "caption": "Figure 3:Block details of parameter generator. Each block of hyper-convolution contains three hyper-convolution modules, extracting and fusing features in different dimensions. More details are in AppendixA.3.",
                "position": 373
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.16406/x5.png",
                "caption": "(a)Random selection and pairing is better than using the same conditions for all parameters.",
                "position": 881
            },
            {
                "img": "https://arxiv.org/html/2506.16406/x5.png",
                "caption": "(a)Random selection and pairing is better than using the same conditions for all parameters.",
                "position": 884
            },
            {
                "img": "https://arxiv.org/html/2506.16406/x6.png",
                "caption": "(b)DnD can reach comparable or even better performance than full-shot while being 2.5∼similar-to\\sim∼12K×\\times×faster.",
                "position": 889
            },
            {
                "img": "https://arxiv.org/html/2506.16406/x7.png",
                "caption": "(c)DnD outperforms popular few-shot tuning and ICL before 256 shots while avoiding using answers.",
                "position": 894
            },
            {
                "img": "https://arxiv.org/html/2506.16406/x8.png",
                "caption": "Figure 6:DnD and RPG perform well in most close-set tests.\nHowever, RPG can hardly generate parameters for novel dataset while DnD still presents strong zero-shot ability on open-set test.",
                "position": 975
            },
            {
                "img": "https://arxiv.org/html/2506.16406/x9.png",
                "caption": "Figure 7:DnD generates parameters with close distribution to original ones in the weight space and promising performance.",
                "position": 987
            }
        ]
    },
    {
        "header": "4Related Works",
        "images": []
    },
    {
        "header": "5Discussion and Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AHyper-parameter Settings",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experiment Results",
        "images": []
    }
]