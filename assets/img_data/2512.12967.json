[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.12967/x1.png",
                "caption": "",
                "position": 142
            },
            {
                "img": "https://arxiv.org/html/2512.12967/x2.png",
                "caption": "Figure 1:Overall results of QwenLong-L1.5 across six long-context reasoning benchmarks. Starting from Qwen3-30B-A3B-Thinking, QwenLong-L1.5-30B-A3B achieves an average gain of 9.9 points, surpassing DeepSeek-R1-0528, Gemin2.5-Flash-Thinking, Qwen3-Max-Thinking, and comparable to Gemini-2.5-Pro.",
                "position": 147
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.12967/x3.png",
                "caption": "Figure 2:Memory agent workflow for processing ultra-long contexts",
                "position": 289
            }
        ]
    },
    {
        "header": "3Long-Context Data Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.12967/x4.png",
                "caption": "Figure 3:Comparison of Training Sample Input Length Distributions between QwenLong-L1 and QwenLong-L1.5.",
                "position": 416
            },
            {
                "img": "https://arxiv.org/html/2512.12967/x5.png",
                "caption": "Figure 4:Overview of our end-to-end RL data synthesis framework",
                "position": 484
            },
            {
                "img": "https://arxiv.org/html/2512.12967/x6.png",
                "caption": "Figure 5:Overview of our proposed multi-agent self-evolved data synthesis framework.",
                "position": 498
            }
        ]
    },
    {
        "header": "4Long-Context Post-Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.12967/x7.png",
                "caption": "Figure 6:Post-training pipeline of QwenLong-L1.5.",
                "position": 520
            },
            {
                "img": "https://arxiv.org/html/2512.12967/x8.png",
                "caption": "Figure 7:Two-dimensional UMAP projection(mcinnes2018umap)of three datasets (Open-R1 Codeforces(openr1), DAPO-Math-17K(yu2025dapo), and QwenLong-L1.5 training set) using Qwen3-30B-A3B-Thinking-2507 embeddings.",
                "position": 560
            },
            {
                "img": "https://arxiv.org/html/2512.12967/x9.png",
                "caption": "Figure 8:Comparison of training dynamics. By integrating task-balanced sampling with task-specific advantage estimation, our method achieves similar reward growth as the GRPO baseline while ensures a more stable training dynamics, evidenced by the stabilized entropy and the controlled response length growth.",
                "position": 652
            },
            {
                "img": "https://arxiv.org/html/2512.12967/images/entropy_vs_grad.png",
                "caption": "Figure 9:Correlation between token entropy and gradient norm in negative rollouts. The Spearman’sρ=0.96​(p<0.0001)\\rho=0.96(p<0.0001)",
                "position": 724
            },
            {
                "img": "https://arxiv.org/html/2512.12967/images/entropy_vs_grad.png",
                "caption": "Figure 9:Correlation between token entropy and gradient norm in negative rollouts. The Spearman’sρ=0.96​(p<0.0001)\\rho=0.96(p<0.0001)",
                "position": 727
            },
            {
                "img": "https://arxiv.org/html/2512.12967/x10.png",
                "caption": "(a)Token-level. Clipping either low- or high-entropy tokens reduces the model’s overall entropy, but the results in Table4shows that clipping high-entropy tokens yields larger gains.",
                "position": 732
            },
            {
                "img": "https://arxiv.org/html/2512.12967/x10.png",
                "caption": "(a)Token-level. Clipping either low- or high-entropy tokens reduces the model’s overall entropy, but the results in Table4shows that clipping high-entropy tokens yields larger gains.",
                "position": 735
            },
            {
                "img": "https://arxiv.org/html/2512.12967/x11.png",
                "caption": "(b)Sequence-level. Clipping either low- and high-entropy sequences reduces model entropy, and entropy decreases faster than with token-level clipping because more negative gradients are removed.",
                "position": 740
            },
            {
                "img": "https://arxiv.org/html/2512.12967/x12.png",
                "caption": "Figure 11:Entropy dynamics of AEPO algorithm on Qwen3-30B-A3B-Thinking.",
                "position": 929
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.12967/x13.png",
                "caption": "Table 8:Comparison of Qwen3-30B-A3B-Thinking-2507 and QwenLong-L1.5 on general, agentic memory, and dialogue memory benchmarks.",
                "position": 1183
            },
            {
                "img": "https://arxiv.org/html/2512.12967/x13.png",
                "caption": "Figure 12:Two-dimensional UMAP projection of AIME24/25, GPQA-Diamond, MMLU-Pro, and QwenLong-L1.5 training set.",
                "position": 1262
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations and Future Works",
        "images": []
    },
    {
        "header": "Appendix ADetailed Results on LongBench-V2 and LongBench-V1 QA Subsets",
        "images": []
    },
    {
        "header": "Appendix BQwenLong-L1.5 Synthetic Data Cases",
        "images": []
    },
    {
        "header": "Appendix CCase Study",
        "images": []
    },
    {
        "header": "Appendix DPrompt Template",
        "images": []
    }
]