[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10388/x1.png",
                "caption": "Figure 1:Efficiency frontier of instruction following datasets (see AppendixH.3for details). Our method achieves a win rate on AlpacaEval¬†2.0 comparable to MAGPIE while using only 2K synthetic samples (vs. 300K samples used by MAGPIE).",
                "position": 129
            }
        ]
    },
    {
        "header": "1‚ÄÇIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10388/x2.png",
                "caption": "Figure 2:FAC Synthesis: a coverage-guided synthetic framework.(1) SAE is used to decompose model activations into interpretable task-relevant features.\n(2) Task-relevant SAE features are extracted fromùíü\\mathcal{D}andùíügen\\mathcal{D}_{\\text{gen}}, respectively, and their set difference defines the missing setFmissF_{\\mathrm{miss}}.\n(3)FmissF_{\\mathrm{miss}}is then used to guide data synthesis, generating samples that improve the coverage of task-relevant features.",
                "position": 170
            }
        ]
    },
    {
        "header": "2‚ÄÇRelated Work",
        "images": []
    },
    {
        "header": "3‚ÄÇPreliminaries",
        "images": []
    },
    {
        "header": "4‚ÄÇQuantify Generalization of Synthetic Data",
        "images": []
    },
    {
        "header": "5‚ÄÇReduce Distribution Gap in Feature Space",
        "images": []
    },
    {
        "header": "6‚ÄÇReduce the Sampling Error underùíügen\\mathcal{D}_{\\mathrm{gen}}",
        "images": []
    },
    {
        "header": "7‚ÄÇExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10388/x3.png",
                "caption": "Figure 3:The results of the relationship between FAC and AUPRC on the toxicity detection task.",
                "position": 729
            },
            {
                "img": "https://arxiv.org/html/2602.10388/x4.png",
                "caption": "Figure 4:Performance of models under different SAE feature activation proportions on toxicity detection task.",
                "position": 745
            },
            {
                "img": "https://arxiv.org/html/2602.10388/x5.png",
                "caption": "Figure 5:FAC of datasets synthesized with One-Step and Two-Step strategies under different activation thresholds.",
                "position": 748
            },
            {
                "img": "https://arxiv.org/html/2602.10388/x6.png",
                "caption": "Figure 6:The number of missing features and corresponding AUPRC under different SAE activation thresholds.",
                "position": 1007
            },
            {
                "img": "https://arxiv.org/html/2602.10388/x7.png",
                "caption": "Figure 7:Effect of the number of synthesized samples per missing feature on AUPRC and data efficiency.",
                "position": 1010
            }
        ]
    },
    {
        "header": "8‚ÄÇConclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADefinition of Notations",
        "images": []
    },
    {
        "header": "Appendix BProof ofTheorem4.1(Generalization Error Upper Bound)",
        "images": []
    },
    {
        "header": "Appendix CAnalysing the Residual TermŒµcond\\varepsilon_{\\mathrm{cond}}inEquation2",
        "images": []
    },
    {
        "header": "Appendix DProof of Minimizing the Distribution Gap betweenPZP_{Z}andQZQ_{Z}",
        "images": []
    },
    {
        "header": "Appendix EProof of Lemma6.1(Upper Bound of Sampling Error)",
        "images": []
    },
    {
        "header": "Appendix FAnalysing the Feature Alignment and Uncertainty Reduction",
        "images": []
    },
    {
        "header": "Appendix GRelated Work",
        "images": []
    },
    {
        "header": "Appendix HExperimental Setup",
        "images": []
    },
    {
        "header": "Appendix ITraining Details",
        "images": []
    },
    {
        "header": "Appendix JDetails of Identifying Task-relevant Features",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10388/x8.png",
                "caption": "Figure 8:SAE reconstruction loss across different LLaMA-3.1-8B-Instruct layers.",
                "position": 3161
            },
            {
                "img": "https://arxiv.org/html/2602.10388/x9.png",
                "caption": "Figure 9:Correlation between Diversity Metrics and Downstream AUPRC (Pearson & Spearman). From left to right: Word Level Correlation, Syntax Level Correlation, and Embedding Level Correlation.",
                "position": 3388
            },
            {
                "img": "https://arxiv.org/html/2602.10388/x10.png",
                "caption": "Figure 10:Performance Stability of Instruction Following on AlpacaEval 2.0 Across Different Random Seeds.",
                "position": 3876
            },
            {
                "img": "https://arxiv.org/html/2602.10388/x11.png",
                "caption": "Figure 11:Missing Features Across Different Model Families in the Toxicity Detection Task.",
                "position": 3882
            },
            {
                "img": "https://arxiv.org/html/2602.10388/x12.png",
                "caption": "Figure 12:Model Performance and Parameter Efficiency Score under Different Training Scales.",
                "position": 3905
            },
            {
                "img": "https://arxiv.org/html/2602.10388/x13.png",
                "caption": "Figure 13:Standard vs. Robust accuracy for behavior steering task.",
                "position": 3914
            },
            {
                "img": "https://arxiv.org/html/2602.10388/x14.png",
                "caption": "(a)First group of synthesis datasets.",
                "position": 3926
            },
            {
                "img": "https://arxiv.org/html/2602.10388/x14.png",
                "caption": "(a)First group of synthesis datasets.",
                "position": 3929
            },
            {
                "img": "https://arxiv.org/html/2602.10388/x15.png",
                "caption": "(b)Second group of synthesis datasets.",
                "position": 3935
            },
            {
                "img": "https://arxiv.org/html/2602.10388/x16.png",
                "caption": "Figure 15:Performance comparison between the Base Model and the Self-Improved Model.",
                "position": 3955
            },
            {
                "img": "https://arxiv.org/html/2602.10388/x17.png",
                "caption": "Figure 16:Cross-model performance under different feature sources and generators.",
                "position": 3958
            },
            {
                "img": "https://arxiv.org/html/2602.10388/x18.png",
                "caption": "Figure 17:Weak-to-strong generalization gap across Source‚ÄìGenerator‚ÄìBackbone configurations.",
                "position": 3975
            },
            {
                "img": "https://arxiv.org/html/2602.10388/x19.png",
                "caption": "Figure 18:Performance gaps of Qwen2-7B-Instruct as the backbone under different feature sources (Qwen2-7B-Instruct v.s. Llama-3.1-8B-Instruct) and the same generator.",
                "position": 3978
            }
        ]
    },
    {
        "header": "Appendix KAdditional Experimental Results",
        "images": []
    }
]