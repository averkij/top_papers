[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12566/x1.png",
                "caption": "Figure 1:Comparison of Mono-InternVL, Mono-InternVL-1.5 and existing MLLMs.Compared with modular MLLMs, Mono-InternVL and Mono-InternVL-1.5 embed visual experts into the pre-trained LLM and integrates visual encoding and language decoding into a single LLM. Through endogenous visual pre-training (EViP), Mono-InternVL significantly pushes the performance boundaries of monolithic MLLMs. With EViP++, Mono-InternVL-1.5 not only significantly reduces data costs, but also maintains the competitive performance of downstream tasks.",
                "position": 132
            },
            {
                "img": "https://arxiv.org/html/2507.12566/x2.png",
                "caption": "Figure 2:Monolithic architecture of Mono-InternVL and Mono-InternVL-1.5.Mono-InternVL is designed as a multimodal MoE structure, where visual and textual tokens are processed by the corresponding experts. Mono-InternVL-1.5 further integrates the attention experts and the MoE CUDA kernel to facilitate the visual pre-training while retaining the model efficiency.",
                "position": 196
            }
        ]
    },
    {
        "header": "IIRelated Work",
        "images": []
    },
    {
        "header": "IIIMono-InternVL",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12566/x3.png",
                "caption": "Figure 3:The training recipe of Mono-InternVL (top) and Mono-InternVL-1.5 (bottom).In the first stage, Mono-InternVL is progressively pre-trained on massive data via three sub-stages (S1.1, S1.2, S1.3), where most parameters of LLM are frozen to preserve the pre-trained knowledge. In the second stage (S2), the entire model is optimized to accommodate various instructions. Compared to Mono-InternVL, Mono-InternVL-1.5 integrates visual attention experts and reduces up to 58% training data.",
                "position": 320
            },
            {
                "img": "https://arxiv.org/html/2507.12566/x4.png",
                "caption": "Figure 4:Illustration of Mono-InternVL-1.5 fused kernel workflow.The left thread blocks handle textual tokens while those on the right handle visual tokens. Although two thread blocks are assigned per data block, nearly half exit immediately upon entry, making the kernel effectively behave as a single-branch implementation.",
                "position": 493
            }
        ]
    },
    {
        "header": "IVMono-InternVL-1.5",
        "images": []
    },
    {
        "header": "VExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12566/x5.png",
                "caption": "Figure 5:Ablation studies of EViP and EViP++ with the increase of pre-training data size across three sub-stages: (S1.1) Concept learning; (S1.2) Semantic learning; (S1.3) Alignment learning.For each data point, we fine-tune the corresponding pre-trained model on the instruction data of LLaVA-665k and obtain the downstream performance.",
                "position": 1412
            },
            {
                "img": "https://arxiv.org/html/2507.12566/x6.png",
                "caption": "Figure 6:Visualization of attention maps in Mono-InternVL and Mono-InternVL-1.5.The first blue segment, green segment and the second green segment in the axes represent the system prompt tokens (text), image tokens (visual) and user prompt tokens (text), respectively. The numbers on the left side of attention maps indicate the number of tokens.",
                "position": 1870
            }
        ]
    },
    {
        "header": "VIConclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]