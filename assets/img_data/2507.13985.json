[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "Iintroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.13985/x1.png",
                "caption": "Figure 1:DreamScene exhibits significant advantages compared with current state-of-the-art text-to-3D scene generation methods.\nText2Room[22]and Set-the-Scene[21]require complex user-specified object placement.\nText2Room, Text2NeRF[25]and many inpainting-based methods suffer from low scene consistency, producing incoherent geometry across camera poses. GALA3D[32], CG3D[33]generate scenes with low visual quality and do not generate 3D environments.\nMoreover, most existing methods[25,22,23,27]produce entangled geometry without object-level separation[34], leading to limited or no editability.\nIn contrast, DreamScene supportsautomatic layout planning, ensuresscene-wide consistency, achieveshigh visual fidelity, and enablesflexibleediting of each individual objects.",
                "position": 424
            }
        ]
    },
    {
        "header": "IIRelated Work",
        "images": []
    },
    {
        "header": "IIIpreliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.13985/x2.png",
                "caption": "Figure 2:Our framework enables automatic 3D scene generation from natural language, supporting both direct descriptions and interactive dialogues. A GPT-4 agent first performs scene decomposition by inferring object semantics, layout constraints, and spatial relations, and constructs a constraint graph to plan collision-free object placements. Each object is generated using Formation Pattern Sampling (FPS), which integrates multi-timestep sampling, 3D Gaussian filtering, and reconstructive generation. These objects are placed into the global scene using predicted affine transformations. We then apply a three-stage camera sampling strategy to optimize the environment and ensure scene-wide consistency. DreamScene also supports structure-aware scene editing, including object relocation, appearance modification, and 4D editing.",
                "position": 513
            }
        ]
    },
    {
        "header": "IVmethod",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.13985/x3.png",
                "caption": "Figure 3:Overview of the Scene Planning process. Given either an open-ended prompt or an interactive dialogue, a GPT-4 agent infers object categories, real-world sizes, textual prompts, spatial placements, and inter-object relations. These constraints are used to plan the layout through a constraint graph and GCP algorithm. The resulting arrangement provides a physically plausible and semantically coherent layout that supports environment generation.",
                "position": 576
            },
            {
                "img": "https://arxiv.org/html/2507.13985/x4.png",
                "caption": "Figure 4:Comparison of the generation quality between the ECCV version and the TPAMI version of DreamScene.",
                "position": 809
            },
            {
                "img": "https://arxiv.org/html/2507.13985/x5.png",
                "caption": "Figure 5:Formation Pattern Sampling. (a)Multi-timestep Sampling.At varying timesteps, the 2D text-to-image diffusion model provides different information(represented by the pseudo-GTx^0t)\\hat{x}_{0}^{t})over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT )obtained fromxtx_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTin a single-step by Eq.8in LucidDreamer[8]. (b)3D Gaussian Filtering. 3D Gaussians that are located closer to the rendering plane and possess larger volumes make a greater contribution to the rendering process. (c)Reconstructive Generation.During the later stages of optimization, generation can be directly accomplished using reconstruction based on denoised images, leading to 3D representations with refined and plausible textures.",
                "position": 828
            },
            {
                "img": "https://arxiv.org/html/2507.13985/x6.png",
                "caption": "Figure 6:Schematic diagram of camera sampling in environment generation.",
                "position": 857
            },
            {
                "img": "https://arxiv.org/html/2507.13985/x7.png",
                "caption": "Figure 7:Diversity of layout generation.",
                "position": 925
            },
            {
                "img": "https://arxiv.org/html/2507.13985/x8.png",
                "caption": "Figure 8:Visual consistency and generation quality under diverse scene-wide camera poses in the outdoor scenes.",
                "position": 928
            }
        ]
    },
    {
        "header": "Vexperiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.13985/x9.png",
                "caption": "Figure 9:Visual consistency and generation quality under diverse scene-wide camera poses in the indoor scenes.",
                "position": 951
            },
            {
                "img": "https://arxiv.org/html/2507.13985/x10.png",
                "caption": "Figure 10:Comparison with baselines in text-to-3D object generation.",
                "position": 954
            },
            {
                "img": "https://arxiv.org/html/2507.13985/x11.png",
                "caption": "Figure 11:DreamScene editing results. (a) shows object-level edits, including relocation, addition, and removal. (b) demonstrates style modifications applied to both objects and environments. (c) presents the4Dgeneration results from multiple viewpoints.",
                "position": 957
            },
            {
                "img": "https://arxiv.org/html/2507.13985/x12.png",
                "caption": "Figure 12:Ablation results of time window strategy in MTS.",
                "position": 1081
            },
            {
                "img": "https://arxiv.org/html/2507.13985/x13.png",
                "caption": "Figure 13:Ablation results of 3D Gaussian filtering algorithm in reconstruction and generation tasks. (a) Data in NeRF-360[16]. (b) Data in generating process.",
                "position": 1084
            },
            {
                "img": "https://arxiv.org/html/2507.13985/x14.png",
                "caption": "Figure 14:The ablation results of various camera sampling strategies.\n(a) Randomly camera sampling. (b) No distinction between environment and ground. (c) DreamScene three-step camera sampling strategy",
                "position": 1111
            }
        ]
    },
    {
        "header": "VIConclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATheoretical Derivation of Multi-timestep Sampling (MTS)",
        "images": []
    },
    {
        "header": "Appendix BScene Planning Template",
        "images": []
    },
    {
        "header": "Appendix CScene Planning Template",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.13985/x15.png",
                "caption": "(a)SDS[1]",
                "position": 2035
            },
            {
                "img": "https://arxiv.org/html/2507.13985/x15.png",
                "caption": "(a)SDS[1]",
                "position": 2038
            },
            {
                "img": "https://arxiv.org/html/2507.13985/x16.png",
                "caption": "(b)DreamTime[6]",
                "position": 2043
            },
            {
                "img": "https://arxiv.org/html/2507.13985/x17.png",
                "caption": "(c)MTS",
                "position": 2049
            },
            {
                "img": "https://arxiv.org/html/2507.13985/x18.png",
                "caption": "(d)FPS",
                "position": 2054
            },
            {
                "img": "https://arxiv.org/html/2507.13985/x19.png",
                "caption": "Figure 19:The ablation results of different timestep sizeΔ​T\\Delta Troman_Δ italic_T.",
                "position": 2118
            }
        ]
    },
    {
        "header": "Appendix DAdditional Experiments",
        "images": []
    }
]