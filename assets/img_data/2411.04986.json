[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04986/extracted/5982855/figures/overview_cropped.png",
                "caption": "Figure 1:Examples of the semantic hub effect across input data types. For every other layer, we show the closest output token to the hidden state based on the logit lens. Llama-3‚Äôs hidden states are often closest to English tokens when processing Chinese texts, arithmetic expressions, and code, in a semantically corresponding way. LlaVa, a vision-language model, and SALMONN, an audio-language model, have similar behavior when processing images/audio. As shown for the arithmetic expression example, models can be intervened cross-lingually or cross-modally, such as using English even though the input is non-English, and be steered towards corresponding effects. Boldface is only for emphasis.",
                "position": 147
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2The Semantic Hub Hypothesis",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04986/extracted/5982855/figures/method_cropped.png",
                "caption": "Figure 2:An illustration of our hypothesis, where semantically equivalent inputs (across data types) have similar representations, and this representation is close to the continuation token in the dominant data type. Here,z‚ãÜsuperscriptùëß‚ãÜz^{\\star}italic_z start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPTis English andz‚àòsuperscriptùëßz^{\\circ}italic_z start_POSTSUPERSCRIPT ‚àò end_POSTSUPERSCRIPTis Chinese.",
                "position": 201
            }
        ]
    },
    {
        "header": "3Evidence of A Semantic Hub",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04986/x1.png",
                "caption": "(a)The cosine similarity of intermediate representations of English and Chinese parallel texts.",
                "position": 283
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x1.png",
                "caption": "(a)The cosine similarity of intermediate representations of English and Chinese parallel texts.",
                "position": 286
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x2.png",
                "caption": "(b)Same as (a), but subtracted by a baseline over non-parallel texts.",
                "position": 291
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x3.png",
                "caption": "(c)Llama-3 logit lens log prob. of parallel English vs. Chinese tokens when processing Chinese text.",
                "position": 296
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x4.png",
                "caption": "Figure 4:Language probabilities of English and Chinese (and the top language, when it is neither, which only happens for Bloom).Regardless of the input language, the dominant LM language is more salient in the early-middle layers, and the input language is more salient in the final layers. Bloom does not have a clear intermediate latent language.",
                "position": 303
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x5.png",
                "caption": "(a)Cosine similarity between an arithmetic expression in Arabic numerals vs. English words, broken down into separate categories.",
                "position": 331
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x5.png",
                "caption": "(a)Cosine similarity between an arithmetic expression in Arabic numerals vs. English words, broken down into separate categories.",
                "position": 334
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x6.png",
                "caption": "(b)Same as (a), but only the exact translation similarities subtracted by the others.",
                "position": 339
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x7.png",
                "caption": "(c)Logit lens log probability when predicting a number, between either the number itself or its English equivalent.",
                "position": 344
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x8.png",
                "caption": "Figure 6:The Llama-3 hidden representation evolution when predicting a number, projected by PCA where the principal components are learned on the output embeddings of 20 number tokens, 10 in English and 10 numerals.",
                "position": 361
            },
            {
                "img": "https://arxiv.org/html/2411.04986/extracted/5982855/figures/code_examples_cropped.png",
                "caption": "Figure 7:Logit lens analysis on Llama-2 processing Python programs. For every other layer, we show the closest token (sometimes whitespace) to the hidden statesbeforethe grayed-out texts.The model tends to verbalize the future prediction in English that corresponds to the code continuations (in gray).",
                "position": 377
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x9.png",
                "caption": "(a)Llama-2 logit lens log probabilities (and the 95% CI) at commas in Python list literals, of the English ‚Äúand‚Äù token (and baseline tokens) vs. the actual next token in the program, from MBPP.",
                "position": 384
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x9.png",
                "caption": "(a)Llama-2 logit lens log probabilities (and the 95% CI) at commas in Python list literals, of the English ‚Äúand‚Äù token (and baseline tokens) vs. the actual next token in the program, from MBPP.",
                "position": 387
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x10.png",
                "caption": "(b)The distance between Llama-2 hidden states when predicting a function argument, to the unembedding of the argument‚Äôs name (its semantic role) vs. the actual argument expression, in MBPP.",
                "position": 392
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x11.png",
                "caption": "(a)Similarity between a sentence and its semantic structure, subtracted by a baseline over non-parallel texts.",
                "position": 418
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x11.png",
                "caption": "(a)Similarity between a sentence and its semantic structure, subtracted by a baseline over non-parallel texts.",
                "position": 421
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x12.png",
                "caption": "(b)Similar as (a), but the baseline is computed by swapping the positions of names in the semantic structure.",
                "position": 426
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x13.png",
                "caption": "(c)Same as (b), but we shuffle the predicates in the semantic structure to ensure robustness.",
                "position": 431
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x14.png",
                "caption": "(a)Agent",
                "position": 438
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x14.png",
                "caption": "(a)Agent",
                "position": 441
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x15.png",
                "caption": "(b)Recipient",
                "position": 446
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x16.png",
                "caption": "(c)Theme",
                "position": 451
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x17.png",
                "caption": "(d)Agent",
                "position": 457
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x18.png",
                "caption": "(e)Recipient",
                "position": 462
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x19.png",
                "caption": "(f)Theme",
                "position": 467
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x20.png",
                "caption": "(a)The cosine similarity difference between intermediate representations of matching images and captions, over non-matching ones.",
                "position": 502
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x20.png",
                "caption": "(a)The cosine similarity difference between intermediate representations of matching images and captions, over non-matching ones.",
                "position": 505
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x21.png",
                "caption": "(b)The frequency of the closest token to LLaVa‚Äôs hidden states describing the image color, against a baseline using ‚Äúwhite‚Äù.",
                "position": 510
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x22.png",
                "caption": "(c)The cosine similarity difference between intermediate SALMONN representations of matching audios and labels, over non-matching ones.",
                "position": 515
            }
        ]
    },
    {
        "header": "4Intervening in the Semantic Hub",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04986/x23.png",
                "caption": "(a)Steering arithmetic expressions‚Äô results to a different value.",
                "position": 720
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x23.png",
                "caption": "(a)Steering arithmetic expressions‚Äô results to a different value.",
                "position": 723
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x24.png",
                "caption": "(b)Steering a single-argument ‚Äúrange(end)‚Äù call to be predicted as double argument.",
                "position": 728
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x25.png",
                "caption": "(c)Replacing image representations of a color with English tokens of another color, and expecting the model to predict the latter.",
                "position": 733
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x26.png",
                "caption": "(d)Steering mammal sounds to be predicted as non-mammal sounds using English words of non-mammals; vice versa.",
                "position": 738
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperimental Details for ¬ß3",
        "images": []
    },
    {
        "header": "Appendix BExperimental Details for ¬ß4",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04986/x27.png",
                "caption": "(a)Caption, LLaVa",
                "position": 2328
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x27.png",
                "caption": "(a)Caption, LLaVa",
                "position": 2331
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x28.png",
                "caption": "(b)Caption, Chameleon",
                "position": 2336
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x29.png",
                "caption": "(c)Segment., LLaVa",
                "position": 2341
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x30.png",
                "caption": "(d)Segment., Chameleon",
                "position": 2346
            },
            {
                "img": "https://arxiv.org/html/2411.04986/x31.png",
                "caption": "Figure 14:When SALMONN processes an audio clip, the logit lens probabilities of the English words in the audio label vs. another random label.The audio representations better match the semantically corresponding label in English.",
                "position": 2371
            }
        ]
    },
    {
        "header": "Appendix CToken-Level Multimodal Experiments Using the Logit Lens",
        "images": []
    }
]