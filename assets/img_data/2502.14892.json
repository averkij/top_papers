[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14892/x1.png",
                "caption": "Figure 1:EgoSpeak models speech initiation in real time from the camera wearer’s (camera icon) egocentric video stream, mirroring how a real-world agent would perceive and engage in dynamic, multi-speaker environments.",
                "position": 191
            },
            {
                "img": "https://arxiv.org/html/2502.14892/x2.png",
                "caption": "Figure 2:Overview of the EgoSpeak framework. At each time step, the model processes an untrimmed egocentric video and audio stream, classifying them in real time into three categories: background (no speech), other person speaking, and target speaker (camera wearer) speaking. These probabilities are visualized at the bottom, where the model anticipates near-future frames and enables proactive speech initiation for conversational agents.",
                "position": 194
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3EgoSpeak Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14892/x3.png",
                "caption": "Figure 3:Converting Transcript to Per-Frame Labels. Colors indicate: gray - background, orange - target speaker speaking, purple - other speaker speaking. Labels are one-hot encoded for classification.",
                "position": 355
            },
            {
                "img": "https://arxiv.org/html/2502.14892/x4.png",
                "caption": "Figure 4:Sample frames from YT-Conversation dataset. The dataset includes a diverse range of conversational scenarios from YouTube, such as podcasts, interviews, and informal dialogues, representing various real-world conversation formats.",
                "position": 378
            },
            {
                "img": "https://arxiv.org/html/2502.14892/x5.png",
                "caption": "Figure 5:Video duration distribution for YT-Conversation. Our online formulation allows the use of long video clips, some even exceeding 900 seconds.",
                "position": 397
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14892/x6.png",
                "caption": "Figure 6:Utterance initiation prediction with varying transformer memory length. a shorter context window for short-term memory and a longer context window for long-term memory generally show better results.",
                "position": 982
            },
            {
                "img": "https://arxiv.org/html/2502.14892/x7.png",
                "caption": "Figure 7:Qualitative results on EasyCom. The predicted scores are shown in lines and the ground-truth label is shown in regions. The blue line represents a model with RGB input, the red line represents a model with audio input, and the purple line represents a model with audio and visual input.",
                "position": 985
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "8Ethical Statement",
        "images": []
    },
    {
        "header": "9Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14892/x8.png",
                "caption": "Figure 8:Attention weight of a transformer encoders. Transformer models focus on mostly local context for utterance initiation.",
                "position": 1742
            }
        ]
    },
    {
        "header": "Appendix BImportance of Recent Frames",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14892/x9.png",
                "caption": "Figure 9:Failure case on EasyCom. The orange region represents the target speaker speaking, and the red region represents the target speaker’s backchanneling.",
                "position": 1804
            }
        ]
    },
    {
        "header": "Appendix CError Analysis",
        "images": []
    },
    {
        "header": "Appendix DDescriptive Statistics of Experimental Results",
        "images": []
    },
    {
        "header": "Appendix EYT-Conversation Pseudo Annotation Quality Validation",
        "images": []
    },
    {
        "header": "Appendix FUse of AI Assistants",
        "images": []
    }
]