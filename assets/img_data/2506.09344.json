[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09344/x1.png",
                "caption": "Figure 1:Ming-Omniis a versatile, unified end-to-end model capable of processing images, text, video, and audio, and generating text, speech, and images. These capabilities enable the lite version of our model,Ming-Lite-Omni, to support a broad range of tasks, including visual perception, audio-visual interaction, and image generation, among others.",
                "position": 131
            }
        ]
    },
    {
        "header": "2Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09344/x2.png",
                "caption": "Figure 2:The overall framework ofMing-Omni.Ming-Omniextracts visual and audio tokens with dedicated encoders. These tokens are then combined with text tokens and processed through Ling (MoE architecture with modality-specific routers). Subsequently, it generates speech through an audio decoder and enables image generation via a diffusion model.",
                "position": 187
            },
            {
                "img": "https://arxiv.org/html/2506.09344/extracted/6507736/figures/f4.jpg",
                "caption": "Figure 3:Instruction based image style transfer results outputted byMing-Omni.",
                "position": 231
            },
            {
                "img": "https://arxiv.org/html/2506.09344/x3.png",
                "caption": "Figure 4:Instruction based T2I results outputted byMing-Omni.",
                "position": 241
            },
            {
                "img": "https://arxiv.org/html/2506.09344/x4.png",
                "caption": "Figure 5:Instruction based image editing results outputted byMing-Omni.",
                "position": 244
            }
        ]
    },
    {
        "header": "3Data Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09344/x5.png",
                "caption": "Figure 6:Overview of the data configurations during pre-training and instruction tuning. Note that the “Others” type of audio data includes AST, AAC, AAT, and SER data. The \"Others\" type of pre-training text data in (b) and (c) includes books, academic, news, and domain data. And the “Others” type of video-text data in (e) and (f) includes knowledge, captioning, and reasoning data.",
                "position": 284
            },
            {
                "img": "https://arxiv.org/html/2506.09344/x6.png",
                "caption": "Table 10:Performance ofMing-Lite-OmnionPUBLIC and IN-HOUSE Audio Understanding Benchmarkscompared to leading Models, including five different dimensions.",
                "position": 2781
            },
            {
                "img": "https://arxiv.org/html/2506.09344/x6.png",
                "caption": "Figure 7:Visualization results of OCR, Grounding, Information-Seeking, Reasoning, and Video-Audio Understanding tasks.",
                "position": 4370
            },
            {
                "img": "https://arxiv.org/html/2506.09344/x7.png",
                "caption": "Figure 8:Visualization results of Text→→\\rightarrow→Image tasks, including image generation task, image editing task, and style transfer task.",
                "position": 4378
            }
        ]
    },
    {
        "header": "4Evaluation",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Contributors",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Open-source image data",
        "images": []
    },
    {
        "header": "8Open-source audio data",
        "images": []
    },
    {
        "header": "9Open-source video data",
        "images": []
    }
]