[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19430/x1.png",
                "caption": "Figure 1:Comparison of different task completion schemes. An embodied agent is expected to use operations research knowledge to efficiently complete tasks through scheduling.",
                "position": 93
            },
            {
                "img": "https://arxiv.org/html/2511.19430/x2.png",
                "caption": "Figure 2:Illustration of the proposed Operations Research knowledge-based 3D Grounded Task Scheduling (ORS3D). When assigned a composite task by a human, the embodied agent needs to complete the subtasks efficiently by carefully scheduling using operations research knowledge and simultaneously locating the target objects in each step for navigation and manipulation.",
                "position": 102
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3The ORS3D-60K Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19430/x3.png",
                "caption": "Figure 3:Non-parallelizable subtask & parallelizable subtask.",
                "position": 274
            },
            {
                "img": "https://arxiv.org/html/2511.19430/x4.png",
                "caption": "Figure 4:(a) The ORS3D-60K dataset generation pipeline, which first generates subtask meta-information from 3D scene graphs, then uses this information to generate the structured dataset. (b) A composite task example from ORS3D-60K dataset. The green color mask indicates the ground-truth target object in the corresponding step.",
                "position": 277
            },
            {
                "img": "https://arxiv.org/html/2511.19430/x5.png",
                "caption": "Figure 5:Distributions of (a) subtask number in each composite task, and (b) the expected time of each subtask.",
                "position": 317
            },
            {
                "img": "https://arxiv.org/html/2511.19430/x6.png",
                "caption": "Figure 6:Overview of GRANT. The scene point cloud is processed by a 3D scene encoder into scene tokens. GRANT first infers task properties (stage 1), then uses a scheduling token to generate an optimal schedule (stage 2). The grounding tokens are fed to the 3D grounding head to generate object masks. The input task description is simplified for brevity.",
                "position": 320
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19430/x7.png",
                "caption": "Table 2:Experiment results on  ORS3D-60K test set.†We adapt LEO by replacing its LLM with Vicuna-1B for a fair comparison. * indicates that these methods require object point clouds from an external 3D detector like Mask3D(mask3d).‡Results are produced by directly providing step-wise schedules as input. Overall is the average of METEOR, ROUGE, TE, and Grounding Accuracy (treating unsupported metrics as 0).",
                "position": 478
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19430/x7.png",
                "caption": "Figure 7:A qualitative example of GRANT. In the visualized point clouds,yellowshows correct predictions,redindicates false positives, andgreenmarks missed ground truth regions.",
                "position": 955
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    }
]