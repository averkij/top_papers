[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2406.18516/x1.png",
                "caption": "Figure 1:(a)\nThe prediction error of a diffusion model is highly dependent on the quality of the conditional inputs. In this experiment, we introduce an additional condition alongside the original noisy input. This condition is the same target image but corrupted with additive white Gaussian noise at a noise levelœÉ‚àà[0,80]ùúé080\\sigma\\in[0,80]italic_œÉ ‚àà [ 0 , 80 ]. More details can be found in the AppendixA1.1.\n(b) The restoration network is optimized to provide ‚Äúgood‚Äù conditions to minimize the diffusion model‚Äôs noise prediction error, aiming for a clean target distribution.",
                "position": 92
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2406.18516/x2.png",
                "caption": "Figure 2:During the joint training, the restored synthetic images smoothly converge to the expected distribution over the epochs. However, the model tends to find a shortcut in real data by matching the similarity between the conditions and the paired clean image or remembering the channel index. Consequently, the restoration network learns to corrupt the high-frequency details in real-world images and the diffusion model tends to ignore them.",
                "position": 203
            },
            {
                "img": "https://arxiv.org/html/2406.18516/x3.png",
                "caption": "Figure 3:The proposed solution to eliminate the shortcut learning in diffusion.",
                "position": 209
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2406.18516/x4.png",
                "caption": "Figure 4:Visual comparison of the image denoising task on SIDD test dataset(Abdelhamed et¬†al.,2018).",
                "position": 305
            },
            {
                "img": "https://arxiv.org/html/2406.18516/x5.png",
                "caption": "Figure 5:Visual comparison of the image deraining and image deblurring tasks on SPAWang et¬†al. (2019)and RealBlur-JRim et¬†al. (2020)test datasets.",
                "position": 548
            },
            {
                "img": "https://arxiv.org/html/2406.18516/x6.png",
                "caption": "Table 4:Ablation studies of variant networks on the SIDD test image denoising dataset. CS and RS represent the proposed channel shuffling layer and residual-swapping contrastive learning strategies, respectively.",
                "position": 564
            },
            {
                "img": "https://arxiv.org/html/2406.18516/x6.png",
                "caption": "Figure 6:Visual comparison results of ablation studies. The complete version (f) of the proposed method achieves the best restoration results with visually pleasant appearances.",
                "position": 684
            },
            {
                "img": "https://arxiv.org/html/2406.18516/x6.png",
                "caption": "",
                "position": 687
            },
            {
                "img": "https://arxiv.org/html/2406.18516/x7.png",
                "caption": "Table 5:Ours* denotes using a more advanced restoration network with deeper layers, trained by our domain adaptation strategy. SS and DA represent the self-supervised and domain adaptation methods, respectively.‚Ä†The asymmetric pixel-shuffle downsampling for the blind-spot network is exploited.",
                "position": 701
            },
            {
                "img": "https://arxiv.org/html/2406.18516/x7.png",
                "caption": "Figure 7:Scalability of the proposed method on different network architectures.",
                "position": 768
            },
            {
                "img": "https://arxiv.org/html/2406.18516/x7.png",
                "caption": "",
                "position": 771
            },
            {
                "img": "https://arxiv.org/html/2406.18516/x8.png",
                "caption": "Figure 8:Visual results of the proposed method on unseen real-world datasets: the denoising test dataset DND(Plotz & Roth,2017)and deraining test dataset ‚ÄòReal-Internet‚Äô(Yang et¬†al.,2017).",
                "position": 807
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix A1Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2406.18516/x9.png",
                "caption": "Figure A1:Overview of different domain adaptation (DA) approaches. (a) Feature-space DA aligns the intermediate features across source and target domains. (b) Pixel-space DA translates source data to the ‚Äústyle\" of the target domain through adversarial learning. (c) The proposed noise-space DA is specifically designed for image restoration. It gradually adapts the results from both source and target domains to the target clean image distribution, via multi-step denoising. Particularly, the function network represents a restoration network in the context of image restoration.",
                "position": 2051
            }
        ]
    },
    {
        "header": "Appendix A2Discussion on Different Domain Adaptation Methods",
        "images": []
    },
    {
        "header": "Appendix A3Additional Analysis of the Ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2406.18516/x10.png",
                "caption": "Figure A2:Visual results on detailed textures and high-frequency components. The proposed method serves as a general learning strategy for the image restoration task, offering scalability across different restoration networks. It also enables performance improvements as the complexity of the restoration network increases (Ours*).",
                "position": 2157
            },
            {
                "img": "https://arxiv.org/html/2406.18516/x11.png",
                "caption": "Figure A3:Visual comparison of the image denoising task on SIDD test dataset(Abdelhamed et¬†al.,2018).",
                "position": 2176
            },
            {
                "img": "https://arxiv.org/html/2406.18516/x12.png",
                "caption": "Figure A4:Visual comparison of the image deraining task on SPA test dataset(Wang et¬†al.,2019).",
                "position": 2179
            },
            {
                "img": "https://arxiv.org/html/2406.18516/x13.png",
                "caption": "Figure A5:Visual comparison of the image deblurring task on RealBlur-J(Rim et¬†al.,2020)test dataset.",
                "position": 2182
            },
            {
                "img": "https://arxiv.org/html/2406.18516/x14.png",
                "caption": "Figure A6:Visual results of the proposed method on DND real-world denoising test dataset(Plotz & Roth,2017).",
                "position": 2185
            },
            {
                "img": "https://arxiv.org/html/2406.18516/x15.png",
                "caption": "Figure A7:Visual results of the proposed method on DND real-world denoising test dataset(Plotz & Roth,2017).",
                "position": 2188
            },
            {
                "img": "https://arxiv.org/html/2406.18516/x16.png",
                "caption": "Figure A8:Visual results of the proposed method on ‚ÄòReal-Internet‚Äô real-world deraining test dataset(Yang et¬†al.,2017).",
                "position": 2191
            }
        ]
    },
    {
        "header": "Appendix A4Additional Comparison Results",
        "images": []
    }
]