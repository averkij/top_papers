[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16280/x1.png",
                "caption": "Figure 1:Our approach\nMonoFormer\ntrains the autoregressive transformer\nand the diffusion transformer,\nwhich share the weights,\nand uses causal attention mask\nand bidirectional attention mask, respectively.\nDuring training,\nthe input of the transformer\nfor autoregression\nis the text token embeddings,\nand the output is embeddings\nthat are further processed for text generation.\nThe input for diffusion\nis the noised latent embeddings,\nand the output is embeddings\nthat are used to predict the noise.",
                "position": 195
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16280/x2.png",
                "caption": "Figure 2:Examples of MonoFormer for both image generation and text generation tasks. Left: Class-conditional image generation. Middle: Text-to-image generation. Right: Text-to-text generation.",
                "position": 262
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16280/x3.png",
                "caption": "Figure 3:Left:\nA single autoregressive transformer\nfor both text generation\nand visual generation.\nExample methods inlcude Chameleon(Team,2024)and LlamaGen(Sun et al.,2024b).\nRight:\nOne transformer is for autoregressive text generation, and the output embeddings are sent to another model\nfor diffusion-based text-to-image generation(Ge et al.,2024; Wu et al.,2023).",
                "position": 379
            },
            {
                "img": "https://arxiv.org/html/2409.16280/x4.png",
                "caption": "",
                "position": 382
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16280/x5.png",
                "caption": "Figure 4:(a) The effect of transformer initialization for image generation, measured using the FiD-10K metric on ImageNet. (b) The effect of transformer initialization for text generation, measured by the average commonsense reasoning score. (c) The effect of bidirectional attention mask for image generation.",
                "position": 878
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImage Generation Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16280/x6.png",
                "caption": "Figure 5:Example results of class-conditional image generation on ImageNet.We use 250 sampling steps and a classifier-free guidance scale of 4.0.",
                "position": 1650
            },
            {
                "img": "https://arxiv.org/html/2409.16280/x7.png",
                "caption": "Figure 6:Example results of text-to-image generation.The prompts are from Parti prompts(Yu et al.,2022). We use 250\nsampling steps and a classifier-free guidance scale of 4.0.",
                "position": 1653
            }
        ]
    },
    {
        "header": "Appendix BExtension to Multi-modality Understanding",
        "images": []
    },
    {
        "header": "Appendix CDiccussion with concurrent work",
        "images": []
    }
]