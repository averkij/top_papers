[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Hallucination Span Detection",
        "images": []
    },
    {
        "header": "3RL4HS: Reinforcement Learning for Hallucination Span Detection",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02173/x1.png",
                "caption": "Figure 1:Span-F1@K for different number of predictionsKK.Using CoT reasoning provides significant boost asKKincreases clearly demonstrating the potential of CoT reasoning.",
                "position": 185
            },
            {
                "img": "https://arxiv.org/html/2510.02173/x2.png",
                "caption": "Figure 2:Expected values of advantage given to Qwen2.5-7B-Instruct pretrained model predictions based on the prediction type.Values are shown separately for the three task-based splits of the RAGTruth dataset.",
                "position": 258
            },
            {
                "img": "https://arxiv.org/html/2510.02173/x3.png",
                "caption": "Figure 3:Advantage distribution by model predictions.Advantage distributions across tasks on Qwen2.5-7B-Instruct pretrained model.\nNon-hallucination predictions (red) receive higher advantages than hallucination predictions (blue), revealing a class imbalance issue.",
                "position": 268
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Results & Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02173/x4.png",
                "caption": "Figure 4:Training dynamics of GRPO (red) and CAPO (blue) on Qwen2.5-7B-Instruct model.While GRPO exhibits high precision but declining recall due to reward hacking, CAPO stabilizes recall without sacrificing precision, yielding consistently higher span F1. Shaded regions denote standard deviations across runs.",
                "position": 660
            },
            {
                "img": "https://arxiv.org/html/2510.02173/x5.png",
                "caption": "Figure 5:Out-of-domain evaluation on RAGTruth.Span-F1 scores on Ragtruth dataset. Our RL4HS-OOD-7B model performs competitively with larger reasoning models, showing the benefit of span-level reward fine-tuning. We use Instruct models for Qwen2.5 models.",
                "position": 680
            }
        ]
    },
    {
        "header": "6Related Works",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02173/x6.png",
                "caption": "Figure 6:Hallucination span detection with and without CoT reasoning.\nResults are shown for summarization, question answering, and data-to-text tasks on the RAGTruth benchmark.",
                "position": 1635
            },
            {
                "img": "https://arxiv.org/html/2510.02173/x7.png",
                "caption": "Figure 7:Hallucination span detection with and without CoT reasoning.\nResults are shown for summarization, question answering, and data-to-text tasks on the RAGTruth benchmark.",
                "position": 1642
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]