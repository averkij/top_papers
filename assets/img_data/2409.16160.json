[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16160/extracted/5876494/pic/teaser.png",
                "caption": "",
                "position": 75
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16160/extracted/5876494/pic/teaser_idea3.png",
                "caption": "Figure 2:The basic idea of MIMO. Controllable character video synthesis with desired attributes provided by multiple inputs (e.g., a single image for character, a pose sequence for motion, and a single video/image for scene) or a driving video. Target attributes are embedded into the latent space as the target codes and the driving video is spatially decomposed as the spatial codes. Target character videos can be generated in user control with the combined attribute codes.",
                "position": 103
            },
            {
                "img": "https://arxiv.org/html/2409.16160/extracted/5876494/pic/networks31.png",
                "caption": "Figure 3:An overview of the proposed framework. The video clip is decomposed to three spatial components (i.e., main human, underlying scene, and floating occlusion) in hierarchical layers based on 3D depth.\nThe human component is further disentangled for properties of identity and motion via canonical appearance transfer and structured body codes, and encoded to identity code𝒞i⁢dsubscript𝒞𝑖𝑑\\mathcal{C}_{id}caligraphic_C start_POSTSUBSCRIPT italic_i italic_d end_POSTSUBSCRIPTand motion code𝒞m⁢osubscript𝒞𝑚𝑜\\mathcal{C}_{mo}caligraphic_C start_POSTSUBSCRIPT italic_m italic_o end_POSTSUBSCRIPT.\nThe scene and occlusion components are embedded with a shared VAE encoder and re-organized as a full scene code𝒞s⁢osubscript𝒞𝑠𝑜\\mathcal{C}_{so}caligraphic_C start_POSTSUBSCRIPT italic_s italic_o end_POSTSUBSCRIPT. These latent codes are inserted into a diffusion-based decoder as conditions for video reconstruction.",
                "position": 108
            }
        ]
    },
    {
        "header": "2Method Description",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16160/extracted/5876494/pic/decoder1.png",
                "caption": "Figure 4:The architecture of the diffusion-based decoder.",
                "position": 199
            }
        ]
    },
    {
        "header": "3Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16160/extracted/5876494/pic/res_character3.png",
                "caption": "Figure 5:Results of animating diverse characters from a single reference image.",
                "position": 240
            },
            {
                "img": "https://arxiv.org/html/2409.16160/extracted/5876494/pic/res_motion1.png",
                "caption": "Figure 6:Results of synthesizing avatar animations with novel 3D motions, which are retrieved from the motion database or extracted from in-the-wild human videos.",
                "position": 243
            },
            {
                "img": "https://arxiv.org/html/2409.16160/extracted/5876494/pic/res_scene.png",
                "caption": "Figure 7:Results of synthesizing avatar animations with interactive scenes, which are extracted from in-the-wild videos.",
                "position": 248
            }
        ]
    },
    {
        "header": "4Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]