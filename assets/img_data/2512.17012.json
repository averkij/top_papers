[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17012/x1.png",
                "caption": "",
                "position": 123
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17012/x2.png",
                "caption": "Figure 2:Perceptual 4D Distillation (P4D) framework for 4D-RGPT.For each frameğ‘°(i){\\bm{I}}^{(i)}inğ‘½{\\bm{V}}, 4D-RGPT extracts 4D representations through training-only modules,i.e.,ğ‘«ğŸºâ€‹ğ™³â€‹ğ™¿{\\bm{\\mathsfit{D}}}_{\\tt 4DP}andğ‘«m{\\bm{\\mathsfit{D}}}_{m}formâˆˆâ„³m\\in{\\mathcal{M}}.\nThis includes both latent features,i.e.,ğ‘­^ğŸºâ€‹ğ™³\\hat{\\bm{F}}_{\\tt 4D}, and explicit signals,e.g., depthğ‘·^ğšğšğš™ğšğš‘\\hat{\\bm{P}}_{\\tt depth}or optical flow mapsğ‘·^ğšğš•ğš˜ğš \\hat{\\bm{P}}_{\\tt flow}.\nWe also incorporate timestamp positional encodings (TPE) to provide temporal cues for 4D-RGPT to be temporally aware.\nIn the P4D framework, the frozen teacher,i.e., 4D perception model, captures 4D expert knowledge fromğ‘½{\\bm{V}}.\nIt is then distilled to the student 4D-RGPT via two strategies.\n(a)Latent Distillation (LD): We align the latentğ‘­^ğŸºâ€‹ğ™³\\hat{\\bm{F}}_{\\tt 4D}with the teacherâ€™s intermediate 4D embeddingsğ‘­ğŸºâ€‹ğ™³{\\bm{F}}_{\\tt 4D}.\n(b)Explicit Distillation (ED): We align the explicitğ‘·^m\\hat{\\bm{P}}_{m}with the teacherâ€™s final 4D signalsğ‘·m{\\bm{P}}_{m}.\n4D-RGPT is optimized end-to-end using both SFT loss and the distillation losses,i.e.,â„’ğ™»ğ™³{\\mathcal{L}}_{\\tt LD}andâ„’ğ™´ğ™³{\\mathcal{L}}_{\\tt ED}.",
                "position": 367
            }
        ]
    },
    {
        "header": "4Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17012/x3.png",
                "caption": "Figure 3:Curation pipeline of our R4D-Bench.Given existing non-region 4D VQA benchmarks, we (a) first extract the noun keywords from the question as candidates for objects of interest.\n(b) Next, if ground truth segmentation masks are provided, we use them for step (d).\nOtherwise, we use off-the-shelf GroundingDINO[liu2024groundingdino]and SAM2[ravi2024sam2]to extract segmentation masks for each object of interest.\n(c) We generate a SoM[yang2023som]image for the first frame.\n(d) We prompt Qwen-2.5VL[alibab2025qwen25vl]with the SoM image and the processed question to match the objects referred to in the question with the regions.\n(e) Finally, the generated matching results are verified by human experts.",
                "position": 514
            }
        ]
    },
    {
        "header": "5R4D-Bench",
        "images": []
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17012/x4.png",
                "caption": "Figure 4:VQA comparison among baseline MLLMs and 4D-RGPT on R4D-Bench.For the baseline MLLMs, we use GPT-4o-20241120[openai2024gpt4o], Qwen-2.5VL-7B-Instruct[alibab2025qwen25vl], and NVILA-Lite-8B[liu2025nvila].\nWe note that the regions labeled with(*)or(*)are not provided in R4D-Bench; they are visualized for readability.",
                "position": 1149
            },
            {
                "img": "https://arxiv.org/html/2512.17012/x5.png",
                "caption": "Figure 5:Predicted depth maps at different training steps.We visualize the progress ofğ‘·^ğšğšğš™ğšğš‘\\hat{\\bm{P}}_{\\tt depth}throughout training.",
                "position": 1304
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Acknowledgment",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "A1Additional Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17012/x6.png",
                "caption": "Figure A1:An example from VSTI-Bench[fan2025vstibench]training data.\nThe corresponding conversation is as follows:\n(1)User: â€œThese are frames of a video. Approximately how far (in meters) did the camera move between frame 14 and frame 20 of 32? Please answer the question using a single word or phrase.â€;\n(2)GPT: â€œ1.6â€.",
                "position": 1629
            },
            {
                "img": "https://arxiv.org/html/2512.17012/x7.png",
                "caption": "Figure A2:An example from Wolf[li2024wolf]training data.\nThe corresponding conversation is as follows:\n(1)User: â€œWhat traffic participants are around the ego-centric vehicle in the video? Please count and list all of them.â€;\n(2)GPT: â€œ1 car is near the ego vehicleâ€™s expected path. 1 traffic cone is in the ego vehicleâ€™s lane and potentially blocking the ego vehicle. 4 barriers are in the ego vehicleâ€™s lane and potentially blocking the ego vehicle.â€.",
                "position": 1645
            },
            {
                "img": "https://arxiv.org/html/2512.17012/x8.png",
                "caption": "Figure A3:An example from RoboFAC[lu2025robofac]training data.\nThe corresponding conversation is as follows:\n(1)User: â€œFrom the video, what task is the robot engaged in?â€;\n(2)GPT: â€œPut the fork in the cup and put them in the microwave.â€.",
                "position": 1657
            },
            {
                "img": "https://arxiv.org/html/2512.17012/x9.png",
                "caption": "(a)First frame.",
                "position": 1671
            },
            {
                "img": "https://arxiv.org/html/2512.17012/x9.png",
                "caption": "(a)First frame.",
                "position": 1674
            },
            {
                "img": "https://arxiv.org/html/2512.17012/x10.png",
                "caption": "(b)Second frame.",
                "position": 1680
            }
        ]
    },
    {
        "header": "A2R4D-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17012/x11.png",
                "caption": "Figure A5:An example of SoM visual input in R4D-Bench.We apply SoM[yang2023som]onğ‘°(1){\\bm{I}}^{(1)}to generate intermediate region-based visual inputs.\nThe corresponding inputğ‘¸{\\bm{Q}}is â€œAt 9.00 sec, what is the positional relationship of thegreen truck modelrelative to theteddy bear?â€",
                "position": 1769
            },
            {
                "img": "https://arxiv.org/html/2512.17012/x12.png",
                "caption": "Figure A6:Translational questions in R4D-Bench.We note that the regions labeled with(*)are not provided in R4D-Bench; they are visualized for readability.",
                "position": 1923
            },
            {
                "img": "https://arxiv.org/html/2512.17012/x13.png",
                "caption": "Figure A7:Rotational questions in R4D-Bench.We note that the regions labeled with(*)are not provided in R4D-Bench; they are visualized for readability.",
                "position": 1929
            },
            {
                "img": "https://arxiv.org/html/2512.17012/x14.png",
                "caption": "Figure A8:Counting questions in R4D-Bench.We note that the regions labeled with(*),(*), or(*)are not provided in R4D-Bench; they are visualized for readability.",
                "position": 1935
            },
            {
                "img": "https://arxiv.org/html/2512.17012/x15.png",
                "caption": "Figure A9:False positive questions in R4D-Bench.We note that the regions labeled with(*)are not provided in R4D-Bench; they are visualized for readability.",
                "position": 1941
            },
            {
                "img": "https://arxiv.org/html/2512.17012/x16.png",
                "caption": "Figure A10:3D video grounding questions in R4D-Bench.We note that the regions labeled with(*)are not provided in R4D-Bench; they are visualized for readability.\nFor simplicity, we only show 1 correct option and 1 wrong option here, but there are 5 options for each 3D video grounding question in R4D-Bench.",
                "position": 1947
            },
            {
                "img": "https://arxiv.org/html/2512.17012/x17.png",
                "caption": "Figure A11:Spatial relation questions in R4D-Bench.The question asks about the spatial relationship at 7 seconds, which corresponds to the middle frame out of the three frames shown.\nWe note that the regions labeled with(*)or(*)are not provided in R4D-Bench; they are visualized for readability.",
                "position": 1954
            },
            {
                "img": "https://arxiv.org/html/2512.17012/x18.png",
                "caption": "Figure A12:Dimension measurement questions in R4D-Bench.We note that the regions labeled with(*)or(*)are not provided in R4D-Bench; they are visualized for readability.",
                "position": 1961
            },
            {
                "img": "https://arxiv.org/html/2512.17012/x19.png",
                "caption": "Figure A13:Displacement & path length questions in R4D-Bench.We note that the regions labeled with(*)are not provided in R4D-Bench; they are visualized for readability.",
                "position": 1967
            },
            {
                "img": "https://arxiv.org/html/2512.17012/x20.png",
                "caption": "Figure A14:Speed & acceleration questions in R4D-Bench.We note that the regions labeled with(*)are not provided in R4D-Bench; they are visualized for readability.",
                "position": 1973
            },
            {
                "img": "https://arxiv.org/html/2512.17012/x21.png",
                "caption": "Figure A15:TimeBench VQA.We curate a toy benchmark to evaluate MLLMsâ€™ temporal perception.\nWe note that the â€œ(MÃ—M\\times)â€ indicates the multiplier between the wrong option and the correct one.\nThey are not provided in the actual question but are shown here for clarity.",
                "position": 2280
            },
            {
                "img": "https://arxiv.org/html/2512.17012/",
                "caption": "Figure A16:More VQA comparison between GPT-4o[openai2024gpt4o]and 4D-RGPT (Ours) on R4D-Bench.We provide 2 examples for each of the following categories: Displacement & Path Length.",
                "position": 2443
            },
            {
                "img": "https://arxiv.org/html/2512.17012/x23.png",
                "caption": "Figure A17:More VQA comparison between GPT-4o[openai2024gpt4o]and 4D-RGPT (Ours) on R4D-Bench.We provide 2 examples for each of the following categories: Translational, Rotational, and Counting.",
                "position": 2449
            },
            {
                "img": "https://arxiv.org/html/2512.17012/x24.png",
                "caption": "Figure A18:More VQA comparison between GPT-4o[openai2024gpt4o]and 4D-RGPT (Ours) on R4D-Bench.We provide 2 examples for each of the following categories: False Positive and 3D Video Grounding.",
                "position": 2455
            },
            {
                "img": "https://arxiv.org/html/2512.17012/x25.png",
                "caption": "Figure A19:More VQA comparison between GPT-4o[openai2024gpt4o]and 4D-RGPT (Ours) on R4D-Bench.We provide 2 examples for each of the following categories: Spatial Relation, Dimension Measurement, and Speed & Acceleration.",
                "position": 2461
            },
            {
                "img": "https://arxiv.org/html/2512.17012/x26.png",
                "caption": "Figure A20:More visualizations of 4D-RGPT explicit signalsP^m\\hat{\\bm{P}}_{m}.Similar to the format of Fig.5,\nwe visualize the training progress ofğ‘·^ğšğšğš™ğšğš‘\\hat{\\bm{P}}_{\\tt depth},ğ‘·^ğšğš•ğš˜ğš \\hat{\\bm{P}}_{\\tt flow}, andğ‘·^ğš–ğš˜ğšğš’ğš˜ğš—\\hat{\\bm{P}}_{\\tt motion}.",
                "position": 2475
            }
        ]
    },
    {
        "header": "A3Additional Results",
        "images": []
    }
]