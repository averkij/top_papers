[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25084/x1.png",
                "caption": "(a)Task taxonomy used inDataMindfor fine-grained and diverse query synthesis.",
                "position": 134
            },
            {
                "img": "https://arxiv.org/html/2509.25084/x1.png",
                "caption": "(a)Task taxonomy used inDataMindfor fine-grained and diverse query synthesis.",
                "position": 137
            },
            {
                "img": "https://arxiv.org/html/2509.25084/x2.png",
                "caption": "(b)Performance comparison between proprietary models and open-source models on multiple datasets.",
                "position": 142
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Problem Definition",
        "images": []
    },
    {
        "header": "3Scaling Data-Analytic Agent Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25084/x3.png",
                "caption": "Figure 2:The Pipeline ofDataMind.DataMindapplies 1) a fine-grained task taxonomy and a recursive easy-to-hard task composition mechanism; 2) a knowledge-augmented trajectory sampling strategy followed by model-based and rule-based filtering; 3) a dynamically adjustable training objective including both SFT and RL losses; 4) a memory-frugal and stable code-based multi-turn rollout framework.",
                "position": 230
            }
        ]
    },
    {
        "header": "4Scaling Data-Analytic Agent Training",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25084/x4.png",
                "caption": "Figure 3:Analysis on Self-Consistency Filtering and Best Trajectory Selection.Con-selectis our original setting, including self-consistency filtering and best trajectory selection by a judge modelℳ\\mathcal{M}.Non-selectuses all the sampled trajectories without the best selection.Random-selectmeans randomly select a trajectory instead of the best selection.Non-condirectly leverages all the synthesized trajectories without self-consistency filtering.",
                "position": 798
            },
            {
                "img": "https://arxiv.org/html/2509.25084/x5.png",
                "caption": "Figure 4:The Influence of SFT Loss for RL Training.γ=0\\gamma=0denotes the absence of SFT loss,γ=0.2\\gamma=0.2corresponds to a low SFT-loss weight, and dynamicγ\\gammaindicates our naive setting.",
                "position": 817
            },
            {
                "img": "https://arxiv.org/html/2509.25084/x6.png",
                "caption": "Figure 5:Answer Reward and Entropy Dynamicsof differentγ\\gammasettings.",
                "position": 842
            },
            {
                "img": "https://arxiv.org/html/2509.25084/x7.png",
                "caption": "Figure 6:Performance Gap Between Cold Start and RLwith varying cold start epochs.",
                "position": 854
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AThe Use of Large Language Models",
        "images": []
    },
    {
        "header": "Appendix BLimitations",
        "images": []
    },
    {
        "header": "Appendix CA More Detailed Related Work",
        "images": []
    },
    {
        "header": "Appendix DDatasets and Evaluation Details",
        "images": []
    },
    {
        "header": "Appendix EBaselines and Reproduction Details",
        "images": []
    },
    {
        "header": "Appendix FTraining and Inference Details",
        "images": []
    },
    {
        "header": "Appendix GPrompts Used in Our Paper",
        "images": []
    }
]