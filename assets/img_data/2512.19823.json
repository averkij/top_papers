[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19823/x1.png",
                "caption": "Figure 1.We present a method for refocusing images using video diffusion models to predict focal stacks, allowing users to select their preferred focus. Trained on a large smartphone dataset captured in diverse real-world environments, our model generates realistic defocus effects and performs effectively on real scenes.",
                "position": 160
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19823/x2.png",
                "caption": "Figure 2.Illustration showing that video diffusion models have priors on generating focal stacks. The video sequence is generated by prompting a model(firefly)to generate “a focus pull of a bug on a leaf”.",
                "position": 175
            }
        ]
    },
    {
        "header": "2.Related work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19823/x3.png",
                "caption": "Figure 3.Eight sample focal stacks from our dataset consisting of 1637 total scenes. More focal stacks are visualized in a supplementary material montage.",
                "position": 264
            }
        ]
    },
    {
        "header": "3.Large focal stack dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19823/x4.png",
                "caption": "Figure 4.The original focal stack exhibits misalignment caused by focal breathing, evident from the noticeable edge movements. Our pre-processing corrects these distortions, resulting in a well-aligned stack that enables more accurate refocusing.",
                "position": 292
            }
        ]
    },
    {
        "header": "4.Method",
        "images": []
    },
    {
        "header": "5.Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19823/x5.png",
                "caption": "Figure 5.Illustration of our modified classifier-free guidance for a focal stack withF=3F=3frames. During training, the latent corresponding to a randomly selected focal position is passed to the video diffusion model at the matching position, while all other conditioning inputs are set to zero.",
                "position": 424
            },
            {
                "img": "https://arxiv.org/html/2512.19823/x6.png",
                "caption": "Figure 6.Refocusing results from various input focal positions to output focal positions. We compare our method to RGAN-NAF(refocusgan), NAFNet(naf), and ground truth (GT). Our method reconstructs coarse structures in the pine-cone (third row) and fine details in hair (second row).",
                "position": 440
            },
            {
                "img": "https://arxiv.org/html/2512.19823/x7.png",
                "caption": "Figure 7.Qualitative results of refocusing images provided by everyday iPhone users. We compare against the best baseline, NAFNet(naf). Our method has superior refocusing results and handles hard scenes with humans (bottom left) or thin structures (bottom right).",
                "position": 1039
            },
            {
                "img": "https://arxiv.org/html/2512.19823/x8.png",
                "caption": "Figure 8.Qualitative results showcasing our method’s ability to generalize across different cameras, including film cameras, DSLRs, and smartphones. We compare against the best baseline, NAFNet(naf). Our method produces more realistic results than NAFNet across all the cameras.",
                "position": 1057
            },
            {
                "img": "https://arxiv.org/html/2512.19823/x9.png",
                "caption": "Figure 9.We observe that our method can mitigate slight motion blur when refocusing. In the first image, both the hand and bird lie within the depth of field yet appear blurry due to motion. By refocusing to the background (focal position 9), these elements become noticeably sharper. When we subsequently refocus this output back to the original focal position, the hand and bird retain their improved clarity, indicating that the motion blur is substantially reduced.",
                "position": 1120
            },
            {
                "img": "https://arxiv.org/html/2512.19823/x10.png",
                "caption": "Figure 10.Our method struggles with extreme defocus levels that exceed those present in our training dataset. When attempting to refocus in such cases, it cannot accurately recover fine details—like the illuminated lights—due to the excessively large bokeh.",
                "position": 1123
            },
            {
                "img": "https://arxiv.org/html/2512.19823/x11.png",
                "caption": "Figure 11.The rig used to capture our dataset. For this work, we only use the central camera focal stacks.",
                "position": 1126
            },
            {
                "img": "https://arxiv.org/html/2512.19823/x12.png",
                "caption": "Figure 12.Illustration of generating all-in-focus (AiF) and depth-of-field (DoF) edited images via HeliconFocus applied to our reconstructed focal stacks. For AiF synthesis, we use all frames in the stack; for DoF edits, we use only focal positions 1-5. We compare these AiF/DoF edited outputs from our method against the best-performing baseline, NAFNet(naf), as well as the ground-truth (GT) reference.",
                "position": 1129
            }
        ]
    },
    {
        "header": "6.Conclusions",
        "images": []
    }
]