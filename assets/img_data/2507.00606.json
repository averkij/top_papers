[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.00606/x1.png",
                "caption": "Figure 1.Overview of our proposed MoR framework. The MoR framework can be divided into two stages: (1) Thought Generation. As shown in step 1, this involves generating a large number of reasoning chain templates using GPT. (2) SFT Dataset Construction. As depicted in steps 2, 3, and 4, this includes selecting optimal reasoning chains, creating prompts, and filtering for correct responses.",
                "position": 122
            }
        ]
    },
    {
        "header": "2.Related Work",
        "images": []
    },
    {
        "header": "3.Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.00606/x2.png",
                "caption": "Figure 2.Case study comparing the baseline model andMâ¢oâ¢R150ğ‘€ğ‘œsubscriptğ‘…150MoR_{150}italic_M italic_o italic_R start_POSTSUBSCRIPT 150 end_POSTSUBSCRIPTusing CoT prompts. The Qwen2.5-7B-instruct model follows the â€Letâ€™s think step by step.â€ approach but ultimately produces incorrect answers. In contrast, theMâ¢oâ¢R150ğ‘€ğ‘œsubscriptğ‘…150MoR_{150}italic_M italic_o italic_R start_POSTSUBSCRIPT 150 end_POSTSUBSCRIPTmodel adopts the MoR reasoning method, analyzing problems logically and ultimately arriving at the correct answer.",
                "position": 762
            }
        ]
    },
    {
        "header": "4.Experiment",
        "images": []
    },
    {
        "header": "5.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]