[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02359/x1.png",
                "caption": "Figure 1:Our self-supervised face forgery detection approach:We pre-train our audio-to-expression diffusion model on a large-scale, unlabeled video collection. Then, we personalize our pre-trained model on the reference videos of a person of interest (POI) by inserting a subject-specific adapter. Finally, we authenticate suspected videos of POI by the diffusion reconstruction distance.",
                "position": 80
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02359/x2.png",
                "caption": "Figure 2:ExposeAnyone framework for face forgery detection.(a) We pre-train an audio-to-expression diffusion model to predict the added noise sequenceœµ1:L\\bm{\\epsilon}^{1:L}from a noisy expression sequenceùíõt1:L{\\bm{z}}_{t}^{1:L}. Then, we personalize the pre-trained model on a specific subject by inserting an adapter token sequenceùíÑ1:K{\\bm{c}}^{1:K}. (b) After personalization, our model can authenticate videos by computing two reconstruction distances w/ and w/o the adapterùíÑ1:K{\\bm{c}}^{1:K}. (c) Our model is trained in a self-supervised fashion during both pre-training and personalization.",
                "position": 205
            }
        ]
    },
    {
        "header": "3Proposed Framework: ExposeAnyone",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02359/x3.png",
                "caption": "Figure 3:Robustness to common corruptions on IDForge.Severity levels are defined in DeeperForensics[deeperforensics]. Our method is highly consistent on the perturbations especially compression that detectors encounter frequently in real-world scenarios.",
                "position": 882
            },
            {
                "img": "https://arxiv.org/html/2601.02359/x4.png",
                "caption": "Figure 4:Effect of the content-agnostic authentication.The direct use of objectives for (a) pre-training and (b) personalization does not work for authentication, while (c) their quotient significantly distinguishes fake samples from real ones.",
                "position": 888
            },
            {
                "img": "https://arxiv.org/html/2601.02359/x5.png",
                "caption": "",
                "position": 894
            },
            {
                "img": "https://arxiv.org/html/2601.02359/x6.png",
                "caption": "",
                "position": 897
            },
            {
                "img": "https://arxiv.org/html/2601.02359/x7.png",
                "caption": "Figure 5:Analyses on ExposeAnyone.(a) Our model achieves a higher AUC on different durations of reference videos than the previous methods. (b) Detection accuracy converges well with 64 noise sequences. (c) The adapter with eight tokens per subject brings the most accurate results.",
                "position": 922
            },
            {
                "img": "https://arxiv.org/html/2601.02359/x8.png",
                "caption": "Figure 6:Temporal visualization of the authentication score.The solid blue and red lines represent the authentication scores over the frame index of a real video and a Sora2-generated video mimicking the subject, respectively. The dotted lines are the averaged values, which are statistically lower for real videos than for the corresponding fake videos.",
                "position": 960
            }
        ]
    },
    {
        "header": "5Limitations",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix AEvaluation Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02359/x9.png",
                "caption": "Figure 7:Pre-training dataset statistics.(a) We collect monocular videos from three sources: VoxCeleb2, AVSpeech, and Acappella datasets.\n(b) We visualize the ratio of languages spoken in our dataset.\n(c) All the sequences of our dataset are clipped into eight seconds.\n(d) We also visualize the Word Cloud for English.",
                "position": 1223
            }
        ]
    },
    {
        "header": "Appendix BCurated Dataset for Pre-training",
        "images": []
    },
    {
        "header": "Appendix CImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02359/x10.png",
                "caption": "(a)On DF-TIMIT",
                "position": 1800
            },
            {
                "img": "https://arxiv.org/html/2601.02359/x10.png",
                "caption": "(a)On DF-TIMIT",
                "position": 1803
            },
            {
                "img": "https://arxiv.org/html/2601.02359/x11.png",
                "caption": "(b)On DFDCP",
                "position": 1809
            },
            {
                "img": "https://arxiv.org/html/2601.02359/x12.png",
                "caption": "(c)On KoDF",
                "position": 1815
            },
            {
                "img": "https://arxiv.org/html/2601.02359/x13.png",
                "caption": "(d)On IDForge",
                "position": 1821
            },
            {
                "img": "https://arxiv.org/html/2601.02359/x14.png",
                "caption": "(e)On S2CFP",
                "position": 1827
            }
        ]
    },
    {
        "header": "Appendix DAdditional Experiments",
        "images": []
    }
]