[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IINTRODUCTION",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.01729/x1.png",
                "caption": "Figure 1:Learning manipulation policies through federated learning.In centralized training (top), a global model is trained iteratively over a large dataset stored in a central server. In contrast, federated learning (bottom) enables distributed robots to locally train models on their own data and periodically share updates with a central server, preserving data privacy. In this work, we contribute FLAME, the first benchmark of federated learning for robotic manipulation.",
                "position": 76
            }
        ]
    },
    {
        "header": "IIRelated Work",
        "images": []
    },
    {
        "header": "IIIFLAME",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.01729/x2.png",
                "caption": "Figure 2:Tasks, demonstrations and variations in FLAME. The figure showcases four robotic manipulation tasks:Slide Block to Target,Close Box,Insert Onto Square Peg, andScoop With Spatula. Each row represents a different environment instance (Env 0–3), introducing variations in background, object textures, lighting, and camera perspectives. Within each environment, three demonstrations (Demo 0–2) illustrate different executions of the same task, capturing the diversity in data collection used for training and evaluation in our federated learning framework.",
                "position": 142
            },
            {
                "img": "https://arxiv.org/html/2503.01729/x3.png",
                "caption": "Figure 3:The FLAME learning and evaluation framework. Our federated learning framework starts by sampling a set of clients for local training (1). After training for a user-defined number of local epochs, we evaluate each local policy on offline validation data (2) and in online interactions within the simulator (3). After the evaluation, we aggregate the weights of the local policies using a predetermined federated learning method and send back the aggregated weights to initialize the local clients in the new round of training (4). We repeat steps (1-4) for a user-defined number of aggregation rounds. Following this, we select the best-performing global policy in the validation dataset and evaluate the final policy using the test dataset (5) and in the test simulator environments (6).",
                "position": 188
            }
        ]
    },
    {
        "header": "IVEVALUATION",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.01729/x4.png",
                "caption": "(a)Local Clients",
                "position": 420
            },
            {
                "img": "https://arxiv.org/html/2503.01729/x4.png",
                "caption": "(a)Local Clients",
                "position": 423
            },
            {
                "img": "https://arxiv.org/html/2503.01729/x5.png",
                "caption": "(b)Demonstrations per Client",
                "position": 428
            },
            {
                "img": "https://arxiv.org/html/2503.01729/x6.png",
                "caption": "(c)Local Training Epochs",
                "position": 433
            },
            {
                "img": "https://arxiv.org/html/2503.01729/x7.png",
                "caption": "(d)Aggregation Rounds",
                "position": 438
            }
        ]
    },
    {
        "header": "VCONCLUSIONS",
        "images": []
    },
    {
        "header": "ACKNOWLEDGMENT",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]