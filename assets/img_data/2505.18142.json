[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18142/extracted/6480061/figs/github_logo.png",
                "caption": "",
                "position": 127
            },
            {
                "img": "https://arxiv.org/html/2505.18142/extracted/6480061/figs/huggingface_logo.png",
                "caption": "",
                "position": 129
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18142/x1.png",
                "caption": "Figure 1:Comparison of Different Metrics with Human Judgments.In each case, previous metrics (PSNR, SSIM, LPIPS) demonstrate discrepancies with human assessments, whereas our proposed face similarity and text accuracy effectively reflect the reconstruction quality. The reference image represents the original, while Patch 0 and Patch 1 show reconstruction results from different visual tokenizers. The same regions are cropped from the complete images for visualization.",
                "position": 134
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3TokBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18142/x2.png",
                "caption": "Figure 2:Statistics and Sample Diversity of TokBench-Image.TokBench features a balanced instance-scale distribution with particular emphasis on small-scale face and text instances, presenting significant challenges for existing visual reconstruction approaches.",
                "position": 254
            },
            {
                "img": "https://arxiv.org/html/2505.18142/x3.png",
                "caption": "Figure 3:Overview of the evaluation process of TokBench.",
                "position": 289
            },
            {
                "img": "https://arxiv.org/html/2505.18142/x4.png",
                "caption": "Figure 4:Comparison between reconstructed images (right) and original images (left) under different T-ACC and F-Sim metrics. Higher metric values indicate reconstructed images that more closely resemble the original. (Zoom in for better comparison.)",
                "position": 292
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18142/x5.png",
                "caption": "Figure 5:T-ACC and F-Sim metrics across reconstruction resolutions versus target scales.Smaller scales present greater challenges, and even the best-performing VAE show gap for improvement when compared to the “resize” upper bound.",
                "position": 1698
            }
        ]
    },
    {
        "header": "5Limitation",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AEvaluation Setting",
        "images": []
    },
    {
        "header": "Appendix BDetailed Comparison on Face Set",
        "images": []
    },
    {
        "header": "Appendix CMore Visualization",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18142/x6.png",
                "caption": "Figure 6:Visualization results of text and face reconstruction performance for different methods at 256 resolution. (Zoom in for better comparison.)",
                "position": 3903
            },
            {
                "img": "https://arxiv.org/html/2505.18142/x7.png",
                "caption": "Figure 7:Visualization results of text and face reconstruction performance for different methods at 1024 resolution. (Zoom in for better comparison.)",
                "position": 3906
            }
        ]
    },
    {
        "header": "Appendix DAblation Setting",
        "images": []
    }
]