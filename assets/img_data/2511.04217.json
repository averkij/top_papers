[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.04217/x1.png",
                "caption": "Figure 1:Comparison of the approximation techniques in conventional theories of the SLTH (top) and in our attention-specific approach (bottom).\nThis work demonstrates that an arbitrary attention mechanism can be approximated by pruning a randomly initialized one.",
                "position": 134
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.04217/x2.png",
                "caption": "Figure 2:The structure of an MHA.\nBy partitioning the output projection, the final result can be interpreted as the sum of outputs from all heads.",
                "position": 275
            }
        ]
    },
    {
        "header": "3Strong Lottery Ticket Hypothesisfor Transformers",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.04217/x3.png",
                "caption": "Figure 3:The diagram of our proof.\nBy merging the target projections and changing the calculation order of the source MHA,\nwe can apply a variant of the two-layers-for-one approximation technique and approximate the target MHA while keeping the original source and target structures.",
                "position": 494
            }
        ]
    },
    {
        "header": "4Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.04217/x4.png",
                "caption": "Figure 4:The approximation errorϵ\\epsilonof SLTs within a source MHA for the hidden dimensionsnK=nV{n_{\\mathrm{K}}=n_{\\mathrm{V}}}.\nThis result shows that the errorϵ\\epsilonsatisfiesϵ=O​(exp⁡(−n))\\epsilon=O(\\exp(-n)), consistently withTheorem3.",
                "position": 738
            },
            {
                "img": "https://arxiv.org/html/2511.04217/x5.png",
                "caption": "Figure 5:The approximation errorϵ\\epsilonof SLTs within an MHA for the sequence lengthTT.\nThis result suggests that the errorϵ\\epsilondoes not diverge asTTincreases, as implied byTheorem3.",
                "position": 752
            },
            {
                "img": "https://arxiv.org/html/2511.04217/x6.png",
                "caption": "Figure 6:The approximation errorϵ\\epsilonof SLTs within a randomly initialized transformer for the source hidden dimensionsnMHA=nFC{n_{\\mathrm{MHA}}=n_{\\mathrm{FC}}}.\nThis result suggests that error accumulates as the number of blocks increases, while each error holdsϵ=O​(exp⁡(−nMHA))\\epsilon=O(\\exp(-n_{\\textrm{MHA}})), consistently withTheorem3.",
                "position": 766
            },
            {
                "img": "https://arxiv.org/html/2511.04217/x7.png",
                "caption": "Figure 7:Loss comparison between SLTs with and without query and key weight scaling.\nBy introducing a scale based on our theoretical assumptions, we can obtain better SLTs.",
                "position": 782
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.04217/x8.png",
                "caption": "Figure 8:Loss comparison with respect to the weight scaling factor applied to query and key weights.\nInterestingly, in all models, the loss reaches its minimum near the weight scaling of our theoretical assumptions.",
                "position": 825
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Appendix AProofs of Main Theorems",
        "images": []
    },
    {
        "header": "Appendix BExperimental Details",
        "images": []
    }
]