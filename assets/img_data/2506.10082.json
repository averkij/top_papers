[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10082/x1.png",
                "caption": "Figure 1:We achieves high-quality first-frame guided video editing given a reference image (top row), while maintaining flexibility for incorporating additional reference conditions (bottom row).",
                "position": 89
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10082/x2.png",
                "caption": "Figure 2:Exploring different mask configurations as an input condition to image-to-video model. Left: Input conditions, which include a mask and a pseudo-video. Right: A video generation result under different mask configurations. From the top to bottom, we explore the four different cases. Case default: a default mask used in image-to-video training, which only preserves the first frame. Case 1: it uses no input condition and the problem falls back to a text-to-video generation. Case 2: use the entire video as condition without any masking, which results in artifacted videos. Case 3: use a video by masking the foreground as condition, which also fails.",
                "position": 184
            },
            {
                "img": "https://arxiv.org/html/2506.10082/x3.png",
                "caption": "Figure 3:Mask configuration for LoRA training and inference.",
                "position": 217
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10082/x4.png",
                "caption": "Figure 4:Comparisons with state-of-the-art reference-guided video editing methods.",
                "position": 285
            },
            {
                "img": "https://arxiv.org/html/2506.10082/x5.png",
                "caption": "Figure 5:Comparisons with state-of-the-art first-frame-guided video editing methods.",
                "position": 418
            },
            {
                "img": "https://arxiv.org/html/2506.10082/x6.png",
                "caption": "Figure 6:Ablation results of disentangling edits and background.",
                "position": 426
            },
            {
                "img": "https://arxiv.org/html/2506.10082/x7.png",
                "caption": "Figure 7:Ablation results of incorporating additional reference.",
                "position": 429
            },
            {
                "img": "https://arxiv.org/html/2506.10082/x8.png",
                "caption": "Figure 8:Results of our method applied to Wan2.1-I2V and HunyuanVideo-I2V",
                "position": 449
            },
            {
                "img": "https://arxiv.org/html/2506.10082/x9.png",
                "caption": "Figure 9:Comparison between the original and low-cost versions of our approach.",
                "position": 459
            }
        ]
    },
    {
        "header": "5Conclusion and Limitation",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]