[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.11393/x1.png",
                "caption": "",
                "position": 71
            },
            {
                "img": "https://arxiv.org/html/2504.11393/x2.png",
                "caption": "Figure 1:Which pretraining data to use? Ideally, compare performance of large models with fixed configurations averaged over random seeds (left). In practice, cheaper, smaller-scale experiments are used (center).\nHereDataDecidemeasures accuracy of pairwise decisions between 25 pretraining corpora to find efficient prediction methods (right).",
                "position": 105
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Methods",
        "images": []
    },
    {
        "header": "3Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.11393/x3.png",
                "caption": "Figure 2:Accuracy in pairwise decisions on best data when evaluating on the 10 OLMES tasks withAccuracy(shown aggregated in Figure1). Specific tasks have very distinct ranges of sensitivity, with some like ARC Easy being predictable at small scales and others like HellaSwag requiring substantially more compute to predict.",
                "position": 409
            },
            {
                "img": "https://arxiv.org/html/2504.11393/x4.png",
                "caption": "Figure 3:Decision accuracy over 8 baseline scaling law variants. At best, these approaches reach only the same compute to decision accuracy frontier as ranking single scale experiments.DataDecidecan be used to iterate on future scaling law prediction methods.",
                "position": 427
            },
            {
                "img": "https://arxiv.org/html/2504.11393/x5.png",
                "caption": "Figure 4:Per-task decision accuracy using character normalized proxy metrics forAccuracytargets. 5 tasks benefit at smaller scales from using raw likelihood of answers (Correct ProbandTotal Prob), as opposed to discreteAccuracyor continuous metrics that penalize probability on incorrect answers (Norm Correct Prob,Margin).",
                "position": 449
            },
            {
                "img": "https://arxiv.org/html/2504.11393/x6.png",
                "caption": "Figure 5:Why do some tasks or metrics get better or worse decision accuracy? At 150M withCorrect Probtasks like HellaSwag succeed with low run-to-run variance and tasks like SocialIQA widely spread the performance assigned to different pretraining data.",
                "position": 473
            },
            {
                "img": "https://arxiv.org/html/2504.11393/x7.png",
                "caption": "Figure 6:Code tasks such as humaneval and MBPP go from trivial decision accuracy to largely predictable when using using continuousCorrect Probinstead of discreteAccuracy. Meanwhile common math tasks remain near trivial decision accuracy regardless of metric.",
                "position": 489
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AHyperparameters",
        "images": []
    },
    {
        "header": "Appendix BProxy Metric Definitions",
        "images": []
    },
    {
        "header": "Appendix CScaling Law Variants",
        "images": []
    }
]