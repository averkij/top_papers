[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.22144/x1.png",
                "caption": "Figure 1:No-Language-Hallucination Decoding (NoLan).Given an LVLM, an imagevv, and a language questionxx, NoLan mitigates hallucinations in responses by comparing outputs generated from multimodal and unimodal (text-only) inputs. Step 2 can also be simplified by settingα\\alphato a fixed value of11.\nIn this example, the hallucinated object “whale” is suppressed by reducing the influence of language priors during token generation, while the ground truth object “bear” is effectively enhanced.",
                "position": 92
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.22144/x2.png",
                "caption": "Figure 2:Experimental pipeline to test whether LLaVA’s vision encoder can detect the presence of an object in an image.",
                "position": 180
            },
            {
                "img": "https://arxiv.org/html/2602.22144/x3.png",
                "caption": "Figure 3:An illustration of model prediction misdirected by the language priors. Given an image depicting six dwarfs in front of Snow White, LLaVA-1.5-13b provides the same token “seven” regardless of whether the image is provided as input or not.",
                "position": 218
            },
            {
                "img": "https://arxiv.org/html/2602.22144/x3.png",
                "caption": "",
                "position": 221
            },
            {
                "img": "https://arxiv.org/html/2602.22144/x4.png",
                "caption": "",
                "position": 226
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.22144/x5.png",
                "caption": "Figure 4:Illustration of hallucination mitigation by our proposed NoLan-Plus with two samples from LLaVA-Bench. Hallucinated objects from LVLM’s regular decoding are highlighted inred.",
                "position": 1211
            }
        ]
    },
    {
        "header": "5Conclusion and discussion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.22144/x6.png",
                "caption": "Figure 5:More examples from LLaVA-Bench of our proposed NoLan-Plus for hallucination corrections. Hallucinated objects from LVLM’s regular decoding are highlighted inred.",
                "position": 5450
            },
            {
                "img": "https://arxiv.org/html/2602.22144/x7.png",
                "caption": "Figure 6:More examples from LLaVA-Bench of our proposed NoLan-Plus for hallucination corrections. Hallucinated objects from LVLM’s regular decoding are highlighted inred.",
                "position": 5453
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]