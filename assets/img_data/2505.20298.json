[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20298/extracted/6481081/sections/figures/overview.jpg",
                "caption": "Figure 1:Overview of MangaVQA and MangaLMM.We present MangaVQA, a newly proposed benchmark for multimodal context understanding, consisting of 526 manually constructed question–answer pairs.\nWe also develop MangaLMM, a manga-specialized model jointly trained to handle both MangaOCR and MangaVQA tasks.",
                "position": 167
            }
        ]
    },
    {
        "header": "2Related Work: Comic Datasets and Tasks",
        "images": []
    },
    {
        "header": "3The Manga109 Dataset and Our Consolidated MangaOCR Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20298/extracted/6481081/sections/figures/manga109.jpg",
                "caption": "Figure 2:Illustration of a two-page spread from the Manga109 dataset.",
                "position": 201
            },
            {
                "img": "https://arxiv.org/html/2505.20298/x1.png",
                "caption": "(a)",
                "position": 320
            },
            {
                "img": "https://arxiv.org/html/2505.20298/x1.png",
                "caption": "(a)",
                "position": 323
            },
            {
                "img": "https://arxiv.org/html/2505.20298/x2.png",
                "caption": "(b)",
                "position": 329
            },
            {
                "img": "https://arxiv.org/html/2505.20298/x3.png",
                "caption": "(c)",
                "position": 335
            },
            {
                "img": "https://arxiv.org/html/2505.20298/x4.png",
                "caption": "(d)",
                "position": 341
            }
        ]
    },
    {
        "header": "4MangaVQA: A Novel Benchmark for Multimodal Context Understanding",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20298/extracted/6481081/sections/figures/manga_understanding.jpg",
                "caption": "Figure 4:Main categorization of MangaVQA questions.MangaVQA consists of (1) Exact Extraction, where the answer is directly extracted from the image; (2) Multimodal Understanding, where the answer requires comprehension of the story beyond simple extraction; and (3) Image Understanding, which can be answered without referring to the text.",
                "position": 365
            }
        ]
    },
    {
        "header": "5MangaLMM: A Specialized Model for MangaOCR and MangaVQA",
        "images": []
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20298/x5.png",
                "caption": "Figure 5:Category-wise score breakdown.Compared to the original model (Qwen2.5-VL-7B-Instruct), our trained MangaLMM improves scores across nearly every tag in every category.",
                "position": 831
            },
            {
                "img": "https://arxiv.org/html/2505.20298/x6.png",
                "caption": "Figure 6:Qualitative analysis on MangaVQA.The regions in the image relevant to the question or models’ answer are highlighted with boxes in corresponding colors.\nIn the left and middle examples, the model’s performance improves significantly after training, whereas in the right example, the trained model still struggles to produce an accurate answer.",
                "position": 837
            }
        ]
    },
    {
        "header": "7Conclusion and Discussion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AOCR Evaluation in Comics",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20298/x7.png",
                "caption": "(a)An example from the manga titled AppareKappore.",
                "position": 1400
            },
            {
                "img": "https://arxiv.org/html/2505.20298/x7.png",
                "caption": "(a)An example from the manga titled AppareKappore.",
                "position": 1403
            },
            {
                "img": "https://arxiv.org/html/2505.20298/x8.png",
                "caption": "(b)An example from the manga titled GarakutayaManta.",
                "position": 1409
            }
        ]
    },
    {
        "header": "Appendix BSynthetic VQA Examples",
        "images": []
    },
    {
        "header": "Appendix CSetup Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20298/x9.png",
                "caption": "Figure B:Category-wise analysis on MangaVQA.The regions in the image relevant to the question or models’ answer are highlighted with boxes in corresponding colors.",
                "position": 1561
            },
            {
                "img": "https://arxiv.org/html/2505.20298/x10.png",
                "caption": "Figure C:Comparison between GPT-Judge and Human Evaluation.Darker points indicate a higher concentration of points.",
                "position": 1580
            },
            {
                "img": "https://arxiv.org/html/2505.20298/extracted/6481081/sections/figures/gpt-ocr1.jpg",
                "caption": "(a)An example from the manga titled ShimatteIkouze.",
                "position": 1638
            },
            {
                "img": "https://arxiv.org/html/2505.20298/extracted/6481081/sections/figures/gpt-ocr1.jpg",
                "caption": "(a)An example from the manga titled ShimatteIkouze.",
                "position": 1641
            },
            {
                "img": "https://arxiv.org/html/2505.20298/extracted/6481081/sections/figures/gpt-ocr2.jpg",
                "caption": "(b)An example from the manga titled SaladDays.",
                "position": 1647
            },
            {
                "img": "https://arxiv.org/html/2505.20298/extracted/6481081/sections/figures/ocr_vis1.jpg",
                "caption": "(a)An example from the manga titled ShimatteIkouze.",
                "position": 1686
            },
            {
                "img": "https://arxiv.org/html/2505.20298/extracted/6481081/sections/figures/ocr_vis1.jpg",
                "caption": "(a)An example from the manga titled ShimatteIkouze.",
                "position": 1689
            },
            {
                "img": "https://arxiv.org/html/2505.20298/extracted/6481081/sections/figures/ocr_vis2.jpg",
                "caption": "(b)An example from the manga titled SaladDays.",
                "position": 1695
            }
        ]
    },
    {
        "header": "Appendix DAdditional Results",
        "images": []
    }
]