[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05842/x1.png",
                "caption": "Figure 1:We propose RWML as a scalable, self-supervised method to improve the world modeling ability of LLM-based agent by learning from next-states, prior to downstream policy RL which learns from task-success reward.",
                "position": 137
            },
            {
                "img": "https://arxiv.org/html/2602.05842/x2.png",
                "caption": "Figure 2:Overview of RWML. Given a target modelπθ\\pi_{\\theta}, we first collect training data for RWML by usingπθ\\pi_{\\theta}to gather rollouts(s0,a0,s1,a1,…​sT)(s_{0},a_{0},s_{1},a_{1},...s_{T})with the environment, and then convert these rollouts into⟨s≤t,at,st+1⟩\\left\\langle s_{\\leq t},a_{t},s_{t+1}\\right\\rangletriplets for alltt, after subsampling “too easy” samples defined inEquation˜1. We then trainπθ\\pi_{\\theta}to reason as a world model via GRPO, using lightweight reward functions (e.g., embedding-based cosine similarity) to compare the predicteds^t+1\\hat{s}_{t+1}with the realst+1s_{t+1}.",
                "position": 166
            }
        ]
    },
    {
        "header": "2Method",
        "images": []
    },
    {
        "header": "3Experiments",
        "images": []
    },
    {
        "header": "4Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05842/x3.png",
                "caption": "(a)ALFWorld",
                "position": 1027
            },
            {
                "img": "https://arxiv.org/html/2602.05842/x3.png",
                "caption": "(a)ALFWorld",
                "position": 1030
            },
            {
                "img": "https://arxiv.org/html/2602.05842/x4.png",
                "caption": "(b)τ2\\tau^{2}Bench",
                "position": 1035
            },
            {
                "img": "https://arxiv.org/html/2602.05842/x5.png",
                "caption": "Figure 4:RWML training with different base models onτ2\\tau^{2}Bench.",
                "position": 1060
            },
            {
                "img": "https://arxiv.org/html/2602.05842/x6.png",
                "caption": "Figure 5:After RWML, models produce more accurate and efficient decisions by leveraging its improved knowledge of the environment.",
                "position": 1104
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Impact Statements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALLM Usage",
        "images": []
    },
    {
        "header": "Appendix BMore Details on ALFWorld",
        "images": []
    },
    {
        "header": "Appendix CMore Details onτ2\\tau^{2}Bench",
        "images": []
    },
    {
        "header": "Appendix DMore Details on Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05842/x7.png",
                "caption": "(a)Layer-wise parameter change ratios by transformer layer (ALFWorld andτ2\\tau^{2}-Bench).",
                "position": 2459
            },
            {
                "img": "https://arxiv.org/html/2602.05842/x7.png",
                "caption": "(a)Layer-wise parameter change ratios by transformer layer (ALFWorld andτ2\\tau^{2}-Bench).",
                "position": 2462
            },
            {
                "img": "https://arxiv.org/html/2602.05842/x8.png",
                "caption": "(b)Module-wise parameter change ratios aggregated across layers (attention Q/K/V/O and MLP projections).",
                "position": 2468
            }
        ]
    },
    {
        "header": "Appendix EMore Details on Parameter Change Analysis",
        "images": []
    }
]