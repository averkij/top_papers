[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18619/x1.png",
                "caption": "Figure 1.Historical development of LMMs utilizing Next-Token Prediction. Models featuring vision and more modalities are set inblue backgroundwhile models that support audio modality are set ingreen backgrounds.",
                "position": 265
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18619/x2.png",
                "caption": "Figure 2.General pipeline of Multimodal Learning with Next Token Prediction (MMNTP).",
                "position": 275
            }
        ]
    },
    {
        "header": "2.Multimodal Tokenization",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18619/x3.png",
                "caption": "Figure 4.Illustrations of multimodal tokenization.",
                "position": 611
            },
            {
                "img": "https://arxiv.org/html/2412.18619/x4.png",
                "caption": "Figure 5.Illustrations of the training method of tokenizers.",
                "position": 1166
            },
            {
                "img": "https://arxiv.org/html/2412.18619/x5.png",
                "caption": "Figure 6.Illustration of Vector Quantization. Blue lines denote the encoding and quantization process, while the orange lines denote the reconstruction process. The encoder transforms the input image into a latent representation, which is quantized by mapping each vector inZùëçZitalic_Zto its nearest codebook entry inCùê∂Citalic_C. The quantized representationZ^^ùëç\\hat{Z}over^ start_ARG italic_Z end_ARGis then passed through the decoder to reconstruct the image.",
                "position": 1240
            },
            {
                "img": "https://arxiv.org/html/2412.18619/x6.png",
                "caption": "Figure 7.Illustration of Continuous Tokens. Blue lines denote the encoding process, where the encoder transforms the input image into a latent representationZùëçZitalic_Z, and alignerE‚Ä≤superscriptùê∏‚Ä≤E^{\\prime}italic_E start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPTgenerates continuous tokens as input for the LLM to understand image content. Orange lines denote the generation process, where the LLM produces continuous tokens via alignerD‚Ä≤superscriptùê∑‚Ä≤D^{\\prime}italic_D start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT, creating a latent representationZ^^ùëç\\hat{Z}over^ start_ARG italic_Z end_ARGfor the decoder to reconstruct or generate an image.",
                "position": 1417
            }
        ]
    },
    {
        "header": "3.Backbone Model for Multimodal Next Token Prediction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18619/x7.png",
                "caption": "Figure 8.Two types of Multimodal Next Token Prediction models. Compositional model utilizes powerful encoder and decoder for understanding and generation task, adding additional alignment layers. Unified model uses light-weighted encoder and decoder and leave most of the understanding and generation job to the backbone model.",
                "position": 1550
            },
            {
                "img": "https://arxiv.org/html/2412.18619/x8.png",
                "caption": "Figure 9.Input text, images, audio, or image/audio history are encoded into sequences of tokens which are concatenated and used as input to an multi-modal transformer model. The transformer outputs discrete tokens that can be decoded into text, an image, or an audio clip. Some image examples are referenced from(Lu et¬†al.,2023b).",
                "position": 2590
            },
            {
                "img": "https://arxiv.org/html/2412.18619/x9.png",
                "caption": "Figure 10.Attention patterns in a causal decoder, non-causal decoder, and encoder-decoder\narchitecture. In a causal decoder, each token attends to the previous tokens only. In a semi-causal decoder, each token could attends to part of future token and all past tokens(Tian et¬†al.,2024a). In both prefix-causal decoder and encoder-decoder, attention is allowed to be bidirectional on any conditioning information. For the encoder-decoder, that conditioning is fed into the encoder part of the model.",
                "position": 2594
            },
            {
                "img": "https://arxiv.org/html/2412.18619/x10.png",
                "caption": "Figure 11.Next token prediction based structures for (a) visual question answering, (b) text-to-image generation and (c) text guided image editing / image-to-image transform which require both image understanding and generation capabilities.",
                "position": 2619
            },
            {
                "img": "https://arxiv.org/html/2412.18619/x11.png",
                "caption": "Figure 12.Next token prediction based structures for (a) audio understanding, (b) audio generation and (c) full-duplex spoken dialogue, which requires both understanding and generation capabilities.",
                "position": 2623
            }
        ]
    },
    {
        "header": "4.Training with Unified Multimodal Task Representation",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18619/x12.png",
                "caption": "Figure 13.Training objectives example for text to image generation. (a) For the discrete token prediction task, the backbone model, typically a transformer decoder, processes text input to generate discrete image tokens. This is achieved using a vocabulary prediction head and a Softmax function. The resulting tokens are then passed to the VQVAE decoder to construct the final image. (b) For the continuous token prediction task, there is no application of the Softmax function to map the output hidden state to a fixed-size vocabulary within the feature prediction head. Instead, the output is directly processed by a vision generative module, such as a diffusion model.",
                "position": 2756
            },
            {
                "img": "https://arxiv.org/html/2412.18619/x13.png",
                "caption": "Figure 14.Training stage overview.",
                "position": 2816
            },
            {
                "img": "https://arxiv.org/html/2412.18619/x14.png",
                "caption": "Figure 15.Examples of Multimodal In-Context Learning and Chain-of-Thought reasoning.",
                "position": 3070
            },
            {
                "img": "https://arxiv.org/html/2412.18619/x15.png",
                "caption": "Figure 16.Multimodal inference enhancement methods.",
                "position": 3073
            }
        ]
    },
    {
        "header": "5.Datasets and Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18619/x16.png",
                "caption": "Figure 17.Performance evaluation of various models on the VQAv2(Goyal et¬†al.,2017)and MMMU(Yue et¬†al.,2023)benchmarks.\nColored representations signify the employment of the next token prediction architecture, whereas gray depictions denote alternative architectural frameworks.",
                "position": 4533
            },
            {
                "img": "https://arxiv.org/html/2412.18619/x17.png",
                "caption": "Figure 18.Performance evaluation of various models on the ImageNet(Russakovsky et¬†al.,2015)and GenEval(Ghosh et¬†al.,2023)benchmarks.\nColored representations signify the employment of the next token prediction architecture, whereas gray depictions denote alternative architectural frameworks.",
                "position": 4539
            }
        ]
    },
    {
        "header": "6.Challenges",
        "images": []
    },
    {
        "header": "7.Contributions and Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]