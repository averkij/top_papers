[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04220/x1.png",
                "caption": "",
                "position": 132
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background and Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04220/x2.png",
                "caption": "Figure 2:Overview:our One-DVA consists of an encoder, a diffusion decoder and a latent dropout module. The encoder utilizes a vision transformer with 1D queries to extract input video features and outputs low-dimensional latents.\nThe latent dropout module dynamically adjusts the length of 1D latents during training.\nThe diffusion decoder is a diffusion transformer generating videos in pixel space with the latents as the input condition.",
                "position": 234
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04220/x3.png",
                "caption": "(a)rFVD",
                "position": 563
            },
            {
                "img": "https://arxiv.org/html/2602.04220/x3.png",
                "caption": "(a)rFVD",
                "position": 566
            },
            {
                "img": "https://arxiv.org/html/2602.04220/x4.png",
                "caption": "(b)PSNR",
                "position": 572
            },
            {
                "img": "https://arxiv.org/html/2602.04220/x5.png",
                "caption": "Figure 4:Reconstructed videos with various 1D latent lengths. The first row shows the ground-truth (GT) videos, while the subsequent rows depict reconstructions with 1D latent lengths of0,200200,600600, and10001000, respectively. The red dashed boxes highlight regions where reconstruction quality varies noticeably across different 1D latent lengths. We sample frames at a 5-frame interval.",
                "position": 592
            },
            {
                "img": "https://arxiv.org/html/2602.04220/x6.png",
                "caption": "Figure 5:Quantitative reconstruction metrics using variable-length 1D latents. Videos with greater motion exhibit a steeper PSNR decline as the 1D latent length decreases.",
                "position": 597
            },
            {
                "img": "https://arxiv.org/html/2602.04220/x7.png",
                "caption": "Figure 6:Text-to-video results of our latent diffusion model trained on the latent space of our autoencoder.",
                "position": 602
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAutoencoder Details",
        "images": []
    },
    {
        "header": "Appendix BGenerative Model Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04220/x8.png",
                "caption": "Figure 7:Text-to-video results of our latent diffusion model trained on the structural latents of our autoencoder.",
                "position": 2308
            }
        ]
    },
    {
        "header": "Appendix CAutoencoder Adaptation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04220/x9.png",
                "caption": "(a)Pure 1D Latents",
                "position": 2316
            },
            {
                "img": "https://arxiv.org/html/2602.04220/x9.png",
                "caption": "(a)Pure 1D Latents",
                "position": 2319
            },
            {
                "img": "https://arxiv.org/html/2602.04220/x10.png",
                "caption": "(b)Hybrid Latents (First-frame Structural)",
                "position": 2325
            },
            {
                "img": "https://arxiv.org/html/2602.04220/x11.png",
                "caption": "(c)3D Structural Latents",
                "position": 2331
            },
            {
                "img": "https://arxiv.org/html/2602.04220/x12.png",
                "caption": "(a)Before alignment",
                "position": 2343
            },
            {
                "img": "https://arxiv.org/html/2602.04220/x12.png",
                "caption": "(a)Before alignment",
                "position": 2346
            },
            {
                "img": "https://arxiv.org/html/2602.04220/x13.png",
                "caption": "(b)After alignment",
                "position": 2352
            },
            {
                "img": "https://arxiv.org/html/2602.04220/x14.png",
                "caption": "(a)Decoded frames without decoder finetuning",
                "position": 2417
            },
            {
                "img": "https://arxiv.org/html/2602.04220/x14.png",
                "caption": "(a)Decoded frames without decoder finetuning",
                "position": 2420
            },
            {
                "img": "https://arxiv.org/html/2602.04220/x15.png",
                "caption": "(b)Decoded frames with decoder finetuning",
                "position": 2426
            },
            {
                "img": "https://arxiv.org/html/2602.04220/x16.png",
                "caption": "(a)The statistics without the latent alignment process",
                "position": 2469
            },
            {
                "img": "https://arxiv.org/html/2602.04220/x16.png",
                "caption": "(a)The statistics without the latent alignment process",
                "position": 2472
            },
            {
                "img": "https://arxiv.org/html/2602.04220/x17.png",
                "caption": "(b)The statistics after the latent alignment process",
                "position": 2478
            }
        ]
    },
    {
        "header": "Appendix DFurther Analysis on Autoencoder",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04220/x18.png",
                "caption": "Figure 12:Loss curves for autoencoders with 1B and 3B parameters. The two curves remain extremely close for the entire training process.",
                "position": 2495
            }
        ]
    },
    {
        "header": "Appendix ELimitation and Future Work",
        "images": []
    }
]