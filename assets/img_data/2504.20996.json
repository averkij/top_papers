[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.20996/x1.png",
                "caption": "Figure 1:We introduceX-Fusion- a novel framework that adapts pretrained LLMs (e.g., LLaMA) to new modalities (e.g., vision) while retaining their language capabilities and world knowledge.",
                "position": 109
            },
            {
                "img": "https://arxiv.org/html/2504.20996/x2.png",
                "caption": "Figure 2:Captions generated by X-Fusiondemonstrate high details and strong visual alignment with the image inputs.",
                "position": 114
            },
            {
                "img": "https://arxiv.org/html/2504.20996/x3.png",
                "caption": "Figure 3:Images generated by X-Fusiondemonstrate high visual quality and strong text alignment with the input prompts.",
                "position": 137
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.20996/x4.png",
                "caption": "Figure 4:Conceptual comparison of four model architecture baselines.Here, we illustrate how each layer processes the sequential multi-modal feature.(a) Single Tower:Directly fine-tuning pre-trained LLM.(b) Gated Layer:Duplicate Each LLM layer as the gated vision layer,(c) Dual Projection:Duplicate QKV matric and MLP layer for vision modality,(d) Dual Tower:Duplicated transformer block for vision modality.",
                "position": 179
            }
        ]
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4X-Fusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.20996/x5.png",
                "caption": "Figure 5:Performance of image generation and understanding at various data ratios.Increasing visual understanding data improves visual generation performance.",
                "position": 387
            },
            {
                "img": "https://arxiv.org/html/2504.20996/x6.png",
                "caption": "Figure 6:Performance of image generation and understanding at various noise limits in the image-to-text samples.Providing clear images for image-to-text samples enhances visual generation and understanding simultaneously.",
                "position": 391
            }
        ]
    },
    {
        "header": "5Architecture Design Choices for Adding Vision Abilities",
        "images": []
    },
    {
        "header": "6Effect of Data Ratios and Noise on Generation and Understanding Tasks",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.20996/x7.png",
                "caption": "Figure 7:Performance comparison of models of different sizes (1B, 3B, and 8B) with and without additional feature alignment loss.The effectiveness of alignment diminishes as model size increases.",
                "position": 572
            },
            {
                "img": "https://arxiv.org/html/2504.20996/x8.png",
                "caption": "Figure 8:Linear Probe Results.We use the trained model as a feature extractor and train an additional linear layer for image classification on ImageNet[74]. Models trained with our training strategy constantly obtain higher feature quality.",
                "position": 595
            },
            {
                "img": "https://arxiv.org/html/2504.20996/x9.png",
                "caption": "Figure 9:Qualitative comparisonbetween pretrained DiT model, X-Fusion(DiT) and vanilla X-Fusion on image generation and understanding task. By initializing the vision tower from pretrained text-to-image diffusion model, X-Fusion(DiT) achieves stronger image generation capability and competitive performance on image understanding compared to vanilla X-Fusion.",
                "position": 627
            }
        ]
    },
    {
        "header": "7Pretrained Representations for Vision Feature Regularization",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.20996/x10.png",
                "caption": "Figure 10:Interactive Generation.Our X-Fusion model can follow user instructions to understand, generate, or edit images.",
                "position": 658
            },
            {
                "img": "https://arxiv.org/html/2504.20996/x11.png",
                "caption": "Figure 11:Qualitative resultsof fine-tuned X-Fusion model on downstream tasks including: visual question answeringÂ (VQA), image editing, localization, and in/out-painting tasks.",
                "position": 661
            }
        ]
    },
    {
        "header": "8Extension of X-Fusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.20996/x12.png",
                "caption": "Figure 12:Ablation: X-Fuse layer. Our X-Fusion model with the X-Fuse layer outperforms the baseline X-Fusion model on image generation and understanding tasks.",
                "position": 672
            }
        ]
    },
    {
        "header": "9Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.20996/x13.png",
                "caption": "Figure A:Comparison of Different Evaluation Metrics.The BLIP score effectively differentiates between the two captions with varying levels of detail, whereas the other metrics do not.",
                "position": 1686
            }
        ]
    },
    {
        "header": "BAdditional Experiments Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.20996/x14.png",
                "caption": "Figure B:Limitations.Similar to other large multimodal models, our model is prone to hallucinating in its generated concepts.",
                "position": 1837
            }
        ]
    },
    {
        "header": "CSocial Impact",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.20996/x15.png",
                "caption": "Figure C:Additional qualitative results of X-Fusion model.Our generation samples demonstrated good visual quality and achieved great image-text alignment.",
                "position": 1844
            },
            {
                "img": "https://arxiv.org/html/2504.20996/x16.png",
                "caption": "Figure D:Additional qualitative results of X-Fusion model.Our generation captions demonstrated good text quality and achieved great image-text alignment.",
                "position": 1847
            }
        ]
    },
    {
        "header": "DLimitation",
        "images": []
    }
]