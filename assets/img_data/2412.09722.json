[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09722/extracted/5991147/figures/overview_greater.png",
                "caption": "Figure 1:Comparison of textual feedback-based prompt optimization andGReaTer. Left: textual feedback relies entirely on a larger language model‚Äôs judgments. Right:GReaTeravoids external large, proprietary models, using token suggestions from a small model and guiding prompt token selection with loss gradients.GReaTerincorporates model reasoning by first generating reasoning, then applying an extraction prompt to obtain answer logits for computing loss gradients. This ‚Äúgradient over reasoning‚Äù approach optimizes using direct signals rather than relying on language model feedback.",
                "position": 131
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Problem Definition",
        "images": []
    },
    {
        "header": "4Our Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09722/extracted/5991147/figures/greatprompt_methodology_v2.png",
                "caption": "Figure 2:Overall workflow ofGReaTer. (i) The language modelfLLMsubscriptùëìLLMf_{\\text{LLM}}italic_f start_POSTSUBSCRIPT LLM end_POSTSUBSCRIPTgenerates token candidates by conditioning on input samples. (ii)fLLMsubscriptùëìLLMf_{\\text{LLM}}italic_f start_POSTSUBSCRIPT LLM end_POSTSUBSCRIPTuses task input and current prompt to generate reasoning and extract final answer logits. (iii) The logits are used to calculate loss and compute gradient over generated reasoning with respect to the candidate tokens. These gradients determine the selection of candidate token to update the current position of the current prompt.",
                "position": 196
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09722/extracted/5991147/figures/impact_gradient_reasoning.png",
                "caption": "Figure 3:Ablation study on ‚ÄúGradient Over Reasoning‚Äù inGReaTer. Gradient calculation without reasoning causes notable performance drops, showing the importance of reasoning for gradients.",
                "position": 678
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ABenchmark Datasets, Models, and Baselines",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experimentation Details",
        "images": []
    },
    {
        "header": "Appendix CPerformance Comparison with Other Gradient-Based Methods",
        "images": []
    },
    {
        "header": "Appendix DComplexity Comparison:GReaTervs. Text-Based Feedback Approaches",
        "images": []
    },
    {
        "header": "Appendix EPrompt Optimization vs. Few-Shot In-Context Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09722/extracted/5991147/figures/five_vs_zero_shot.png",
                "caption": "Figure 4:Efficacy ofGReaTerin zero-shot setting compared to five-shot inference with Llama-3-8B-Instruct.",
                "position": 1894
            }
        ]
    },
    {
        "header": "Appendix FEffect of Initialization",
        "images": []
    },
    {
        "header": "Appendix GPrompt Optimization Performance in Very Small Language Model : Llama-3.2-1B-Instruct",
        "images": []
    },
    {
        "header": "Appendix HPrompt Optimization Results: Llama-3-8B-Instruct",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09722/extracted/5991147/figures/llama_wdl.png",
                "caption": "Figure 5:Win/Draw/Loss Comparison ofGReaTerand SOTA prompt optimization techniques APO, TextGrad, APE, and PE2 in optimization with Llama-3-8B-Instruct.GReaTermaintains a significant winning margin over these methods, highlighting its effectiveness in optimization.",
                "position": 2143
            },
            {
                "img": "https://arxiv.org/html/2412.09722/extracted/5991147/figures/llama_all_res.png",
                "caption": "Figure 6:Full performance breakdown across 21 BBH tasks ofGReaTerand SOTA prompt optimization techniques APO, TextGrad, APE, and PE2 in optimization with Llama-3-8B.",
                "position": 2147
            },
            {
                "img": "https://arxiv.org/html/2412.09722/extracted/5991147/figures/gemma_wdl.png",
                "caption": "Figure 7:Win/Draw/Loss Comparison ofGReaTerand SOTA prompt optimization techniques APO, TextGrad, APE, and PE2 in optimization with Gemma-2-9B-it.GReaTermaintains winning margin over these methods, highlighting its effectiveness in optimization.",
                "position": 3728
            },
            {
                "img": "https://arxiv.org/html/2412.09722/extracted/5991147/figures/gemma_allres.png",
                "caption": "Figure 8:Full performance breakdown across 21 BBH tasks ofGReaTerand SOTA prompt optimization techniques APO, TextGrad, APE, and PE2 in optimization with Gemma-2-9B.",
                "position": 3731
            }
        ]
    },
    {
        "header": "Appendix IPrompt Optimization Results: Gemma-2-9B-it",
        "images": []
    }
]