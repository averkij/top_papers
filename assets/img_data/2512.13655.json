[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background and Related Work",
        "images": []
    },
    {
        "header": "3Methods",
        "images": []
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13655/x1.png",
                "caption": "Figure 1:Distribution shift (KL divergence) after Heretic abliteration across eight instruction-tuned models. Lower values indicate better preservation of the original token distribution. DeepSeek-7B achieved the lowest divergence (0.043), while Qwen2.5-7B showed the highest (1.646).",
                "position": 563
            },
            {
                "img": "https://arxiv.org/html/2512.13655/x2.png",
                "caption": "Figure 2:Abliteration effectiveness trade-off: KL divergence (distribution preservation) vs remaining refusal rate. Models in the lower-left quadrant (e.g., Zephyr-7B, DeepSeek-7B) represent optimal outcomes with minimal distribution shift and effective refusal removal.",
                "position": 566
            },
            {
                "img": "https://arxiv.org/html/2512.13655/x3.png",
                "caption": "Figure 3:GSM8K math reasoning comparison across all four benchmarked models. DECCP preserved or improved GSM8K performance relative to Heretic on all models, with the most dramatic difference on Yi-1.5-9B (72.40% vs 52.08%). Zephyr-7B shows nearly identical scores between tools, lacking a base model for reference.",
                "position": 722
            },
            {
                "img": "https://arxiv.org/html/2512.13655/x4.png",
                "caption": "Figure 4:Benchmark change heatmap showing percentage change from baseline for each model-tool combination across MMLU, GSM8K, and HellaSwag. Green indicates improvement or minimal degradation; red indicates significant capability loss. Yi-1.5-9B with Heretic shows notable GSM8K degradation (−26.5%-26.5\\%), while DECCP and ErisForge maintain near-baseline performance.",
                "position": 725
            },
            {
                "img": "https://arxiv.org/html/2512.13655/x5.png",
                "caption": "Figure 5:Average benchmark change by tool across three models (DeepSeek-7B, Mistral-7B, Yi-1.5-9B). ErisForge demonstrated the best capability preservation across all benchmarks, followed by DECCP. Heretic showed significant GSM8K degradation on average (−7.81-7.81pp) driven primarily by Yi-1.5-9B results.",
                "position": 775
            },
            {
                "img": "https://arxiv.org/html/2512.13655/x6.png",
                "caption": "Figure 6:Tool compatibility matrix across sixteen models. Heretic achieved universal compatibility (16/16 models), followed by DECCP (11/16). FailSpy’s TransformerLens dependency limited compatibility to 5 models, while ErisForge succeeded on 9 models with some failures on models using non-standard architectures. Gray indicates not tested; Mamba SSM models are architecturally incompatible with transformer-only tools (see Table5).",
                "position": 936
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Data and Code Availability",
        "images": []
    },
    {
        "header": "Competing Interests",
        "images": []
    },
    {
        "header": "Appendix: Additional Uncertainty and Validity Checks",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]