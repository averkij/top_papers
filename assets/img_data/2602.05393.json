[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05393/x1.png",
                "caption": "Figure 1:Comparison of Average Downstream Task Performance: LET vs. Baseline (Standard Training) on 1.4B and 7B Models. LET models are trained under our proposed LET paradigm, whereas the baseline models utilize standard causal language modeling. Remarkably, LET delivers significant performance gains, even when aligned with a model 10√ó\\timessmaller than the target model.",
                "position": 156
            }
        ]
    },
    {
        "header": "2Methodology",
        "images": []
    },
    {
        "header": "3Empirical Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05393/x2.png",
                "caption": "Figure 2:Language modeling performance of LET across three different vocabulary settings. We evaluate the perplexity of models trained with different vocabulary: SmolLM, OPT, and Pythia. For fair comparison[gao2020pile], each subplot uses the same vocabulary. The results demonstrate that LET consistently achieves lower perplexity across all three settings.",
                "position": 552
            },
            {
                "img": "https://arxiv.org/html/2602.05393/x3.png",
                "caption": "Figure 3:Comparison of six layer-wise alignment strategies on average downstream task performance in one-shot evaluation. The proposed LET paradigm, corresponding to L2E, achieves the highest average performance across all downstream tasks, outperforming all alternative strategies.",
                "position": 578
            },
            {
                "img": "https://arxiv.org/html/2602.05393/x4.png",
                "caption": "Figure 4:Comparison of six layer-wise alignment strategies on language modeling performance, measured as test perplexity on the test split of The Pile dataset. Both M2E and L2E maintain robust performance throughout training, with L2E yielding the lowest final perplexity among all strategies.",
                "position": 581
            },
            {
                "img": "https://arxiv.org/html/2602.05393/x5.png",
                "caption": "Figure 5:Average downstream task performance (left) and test perplexity on the The Pile dataset (right) evaluated under differentŒª\\lambdavalues:0.010.01,0.10.1,0.30.3,1.01.0, and3.03.0. ‚ÄúBaseline‚Äù denotes training with standard causal language modeling, whereas all other configurations employ the proposed LET paradigm with differentŒª\\lambda.",
                "position": 593
            },
            {
                "img": "https://arxiv.org/html/2602.05393/x6.png",
                "caption": "Figure 6:Cosine similarity between the late-layer representations of the small pretrained modelùíØ\\mathcal{T}and the early-layer representations of the target model‚Ñ≥\\mathcal{M}under varyingŒª\\lambdavalues.",
                "position": 602
            },
            {
                "img": "https://arxiv.org/html/2602.05393/x7.png",
                "caption": "",
                "position": 619
            }
        ]
    },
    {
        "header": "4Discussion",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "6Experimental Settings and Details",
        "images": []
    },
    {
        "header": "7Related work",
        "images": []
    },
    {
        "header": "8Supplementary Empirical Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05393/x8.png",
                "caption": "Figure 8:A comparison of average downstream task performance when using Pythia-160M and OPT-125M as the modelùíØ\\mathcal{T}. Here, \"LET-opt125m\" and \"LET-pythia160m\" represent the use of the LET paradigm with OPT-125M and Pythia-160M as the small modelsùíØ\\mathcal{T}respectively.",
                "position": 777
            },
            {
                "img": "https://arxiv.org/html/2602.05393/x9.png",
                "caption": "Figure 9:A comparison of average downstream task performance between RKD, Baseline, and LET paradigm at both 1.4B and 7B model scales. We used SmolLM-135M and SmolLM-1.7B as the modelsùíØ\\mathcal{T}, respectively.",
                "position": 789
            },
            {
                "img": "https://arxiv.org/html/2602.05393/x10.png",
                "caption": "Figure 10:A comparison of average downstream task performance using different stopping thresholds in the LET paradigm. In this experiment,Ss‚Äãt‚Äão‚Äãp=1500S_{stop}=1500andSs‚Äãt‚Äão‚Äãp=3000S_{stop}=3000represent implementations of the LET paradigm where alignment was terminated after 1500 and 3000 steps respectively.",
                "position": 801
            },
            {
                "img": "https://arxiv.org/html/2602.05393/x11.png",
                "caption": "Figure 11:Average performance of the 7B model on downstream tasks (left) and perplexity on test split of The Pile dataset (right). TheLET-Llama_3.2_1Bmodel is trained using our proposed LET paradigm and leverages Llama-3.2-1B as the modelùíØ\\mathcal{T}. In contrast, the baselines are trained using standard causal language modeling. Both models have 7B parameters and share the Llama-3.2-1B vocabulary.",
                "position": 807
            }
        ]
    },
    {
        "header": "9Time Series Experiments",
        "images": []
    },
    {
        "header": "10LM Architecture and Throughput",
        "images": []
    },
    {
        "header": "11Hidden States Alignment",
        "images": []
    },
    {
        "header": "12LogSum Loss Setting",
        "images": []
    },
    {
        "header": "13Theoretical Analysis",
        "images": []
    },
    {
        "header": "14Failure Mode Analysis and Layer Selection Strategies",
        "images": []
    },
    {
        "header": "15Descriptions of Evaluation Tasks",
        "images": []
    }
]