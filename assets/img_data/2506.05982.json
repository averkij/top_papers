[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05982/x1.png",
                "caption": "Figure 1:Data samples from MCA-Benchincludes four categories and 20 sub-clusters of Point-and-Click Localization, Static Visual Recognition, Textual Logic Q&A and Interactive Manipulation.",
                "position": 108
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05982/x2.png",
                "caption": "Figure 2:Schematic overview of the MCA-Bench data-acquisition and annotation workflow.The four grey panels—from left to right—represent the dataset categories Static Visual Recognition, Interactive Manipulation, Point-and-Click Localization, and Textual Logic Q&A. The red labels (SVT, MIM-RU, CPTL, and LLM-GFP) denote the dedicated data-collection pipelines associated with each category. Each pipeline follows a unified four-stage paradigm: (i) determining the raw input format; (ii) applying task-specific geometric transformations, coordinate projections, or prompt-template generation; (iii) separating fine-grained annotation types; and (iv) saving the annotations to text files.",
                "position": 1245
            }
        ]
    },
    {
        "header": "3Dataset Construction Process",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05982/x3.png",
                "caption": "Figure 3:The schematic diagram illustrates the data flow and key module configuration across the four stages of the end-to-end framework: unified interface access, gent fine-tuning loading, collaborative inference execution, structured result feedback.",
                "position": 1385
            }
        ]
    },
    {
        "header": "4Agent Pipeline and Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05982/x4.png",
                "caption": "Figure 4:Violin plots of Passing-Rate (0–1) for Original Qwen-2.5 VL-7B-Instruct, Original ChatGPT-4-o, and Finetuned Qwen-2.5 VL-7B-Instruct on 20 CAPTCHA classes. Outlines show density; boxes, dots, and whiskers mark IQR, median, and range.",
                "position": 1403
            }
        ]
    },
    {
        "header": "5Design Principles for Next-Generation Cognitive-Secure CAPTCHAs",
        "images": []
    },
    {
        "header": "Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Details on MCA-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05982/x5.png",
                "caption": "Figure 5:Category distribution of CAPTCHA types in the datasetThe dataset covers a wide range of 3D-interactive, text-based, and visually complex CAPTCHA categories, with each type contributing approximately 5% of the total. The lowest-frequency categories (e.g., commonsense reasoning and text-based arithmetic) represent specialized reasoning-based challenges, while the most common types focus on perceptual and motor interactions.",
                "position": 3071
            },
            {
                "img": "https://arxiv.org/html/2506.05982/x6.png",
                "caption": "Figure 6:Distribution of image file sizes in the datasetOver 60% of the images are smaller than 100 kB, indicating a strong skew toward low-resolution or compressed images. The dashed line shows the estimated probability density function. Percentage labels denote the relative frequency per bin.",
                "position": 3081
            }
        ]
    },
    {
        "header": "Appendix BMarket Research on Mainstream CAPTCHA Datasets",
        "images": []
    },
    {
        "header": "Appendix CMore Extensive Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05982/x7.png",
                "caption": "Figure 7:Comparison of Multi-round Task Performance Between Models and Humans Across Task TypesEach subplot illustrates a specific task type, with the left side displaying the distribution of multiple performances by human participants (scatter plots), and the right side showing the model’s stable outputs (box plots). It can be observed that the model exhibits a smaller range of variability in most tasks, indicating higher execution stability.",
                "position": 4532
            },
            {
                "img": "https://arxiv.org/html/2506.05982/x8.png",
                "caption": "Figure 8:Comparison of Error Correction Capabilities Between Models and Humans Across TasksThe horizontal axis presents 20 types of interactive tasks (including representative scenarios such as Point and Click Location and State VR), while the vertical axis indicates the normalized correction capability index (ranging from 0 to 1.2). The light yellow bars represent the model’s error correction rate achieved through a multi-step feedback mechanism, whereas the ochre bars denote human participants’ spontaneous correction performance under the same tasks. The data show that in 75% of the task scenarios, the human correction index exceeds 0.8, while the model’s performance is mainly concentrated in the 0.5–0.7 range. Notably, the gap is most pronounced in TL-Q&A (dialogue-based interaction) and IM (immediate feedback) tasks. Error bars indicate the standard deviation across three independent experiments.",
                "position": 4542
            },
            {
                "img": "https://arxiv.org/html/2506.05982/x9.png",
                "caption": "Figure 9:Quantitative Comparison of Model Decision Paths in Multimodal CAPTCHA Scenarios.The horizontal axis includes 20 representative model architectures (e.g., 3-3G, HPR, J-S CC), with the left vertical axis indicating the normalized decision consistency index (ranging from 0 to 1.2), and the right vertical axis showing the variance fluctuation coefficient (ranging from 0 to 0.3). The red line represents the alignment of each model’s decision path with the human baseline, while the cobalt blue line reflects the degree of decision variability. Error bars denote standard deviations across three independent samples. The HPR model (Human-Pattern Recognition) demonstrates near-human performance in both consistency index (1.05±plus-or-minus\\pm±0.07) and variance coefficient (0.12±plus-or-minus\\pm±0.03). In contrast, the CD model (Cascade Decision) exhibits the highest decision variance (0.28±plus-or-minus\\pm±0.05), revealing instability in decision logic. Notably, in the CCC (Cross-Channel Correlation) task, over 70% of models exceed the 0.2 threshold in variance coefficient.",
                "position": 4553
            }
        ]
    },
    {
        "header": "Appendix DLimitations and Future Work",
        "images": []
    }
]