[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02427/sections/assets/nitrogen_overview.png",
                "caption": "Figure 1:NitroGenoverview.NitroGenconsists of three main components: (1)Multi-game foundation agent (center)- a generalist vision-action model that takes in game observations and generates gamepad actions, enabling zero-shot gameplay across multiple titles and serving as a foundation for fine-tuning on new games; (2)Universal simulator (left)- an environment wrapper that allows any commercial game to be controlled through a Gymnasium API; and (3)Internet-scale dataset (right)- the largest and most diverse open-source gaming dataset curated from 40,000 hours of publicly available gaming videos, spanning more than 1,000 games with extracted action labels.",
                "position": 135
            },
            {
                "img": "https://arxiv.org/html/2601.02427/sections/assets/data_pipeline_a.jpg",
                "caption": "(a)Examples of gamepad overlay videos.",
                "position": 163
            },
            {
                "img": "https://arxiv.org/html/2601.02427/sections/assets/data_pipeline_a.jpg",
                "caption": "(a)Examples of gamepad overlay videos.",
                "position": 166
            },
            {
                "img": "https://arxiv.org/html/2601.02427/sections/assets/action_extraction_pipeline-min.png",
                "caption": "(b)Action extraction pipeline.",
                "position": 172
            }
        ]
    },
    {
        "header": "2Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02427/x1.png",
                "caption": "Figure 3:Distribution of theNitroGendataset across games and genres.After filtering, theNitroGendataset contains 40,000 hours of gameplay videos spanning more than 1,000 games. (a) Hours per game shows broad coverage, with 846 games having over one hour of data, 91 games with over 100 hours, and 15 games exceeding 1,000 hours each. (b) Genre distribution reveals Action-RPG games are most common (34.9% of total hours), followed by Platformer (18.4%) and Action-Adventure (9.2%) games, with the remainder distributed across seven genres.",
                "position": 199
            },
            {
                "img": "https://arxiv.org/html/2601.02427/sections/assets/rollouts_figure-min.png",
                "caption": "Figure 4:In-game rollouts.We showNitroGenperforming tasks in diverse 2D and 3D environments. These tasks can take from a few seconds to a few minutes to perform. Some of them include memorization, while others are performed in procedurally generated worlds and require the model to adapt.",
                "position": 226
            },
            {
                "img": "https://arxiv.org/html/2601.02427/x2.png",
                "caption": "Figure 5:Gamepad parsing performance for different controller families.We verify the correctness of our action extraction pipeline by comparing performance across different controller families against ground-truth data. (a) shows joystick RÂ² correlation scores (averaged for both left and right joysticks) with an overall average of 0.84. (b) shows button frame accuracy with an overall average of 0.96.",
                "position": 248
            },
            {
                "img": "https://arxiv.org/html/2601.02427/x3.png",
                "caption": "Figure 6:NitroGen500M pre-training results across different games. We evaluateNitroGenafter behavior-cloning pre-training. The model is not fine-tuned for specific games. For each game, we measure the average task completion rate on 3 tasks with 5 rollouts per task. Despite being trained on a very noisy internet dataset,NitroGenis able to perform non-trivial tasks over games with different visual styles (3D, 2D top-down, 2D side-scrolling) and genres (platformer, action-RPG, roguelike, etc.).",
                "position": 270
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02427/x4.png",
                "caption": "Figure 7:Post-training experiments:NitroGenpre-training improves downstream agents in unseen environments.We pre-trainNitroGenon the dataset described in Section2.1, holding out one game. We then fine-tune the pre-trained checkpoint on the held-out game and compare the results with a model trained from scratch using the same architecture, data and compute budget. (a) When varying data quantity, task-completion rate scales with dataset size, and fine-tuning achieves on average a 10% relative improvement in task-completion rate. (b) When varying task type in the low-data regime (30h), fine-tuning achieves up to 52% relative improvement in task-completion rate.",
                "position": 299
            }
        ]
    },
    {
        "header": "4Limitations and future work",
        "images": []
    },
    {
        "header": "5Related works",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix ANitroGen model details",
        "images": []
    },
    {
        "header": "Appendix BEvaluation",
        "images": []
    }
]