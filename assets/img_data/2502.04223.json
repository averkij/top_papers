[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04223/extracted/6184210/images/teaser-page1.png",
                "caption": "(a)",
                "position": 112
            },
            {
                "img": "https://arxiv.org/html/2502.04223/extracted/6184210/images/teaser-page1.png",
                "caption": "(a)",
                "position": 115
            },
            {
                "img": "https://arxiv.org/html/2502.04223/extracted/6184210/images/teaser-page3.png",
                "caption": "(b)",
                "position": 120
            },
            {
                "img": "https://arxiv.org/html/2502.04223/extracted/6184210/images/teaser-page2.png",
                "caption": "(c)",
                "position": 125
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04223/extracted/6184210/images/imr.png",
                "caption": "Figure 2:Meta architecture for √âCLAIR showcasing the usage with two different (out of eight valid) prompts: Example a) uses the maximal information prompt to return bounding boxes along with their semantic class, markdown text, and tables and formulas. In b) we ask the model to return only markdown text without boxes or classes. All supported semantic classes are listed on the right.",
                "position": 176
            },
            {
                "img": "https://arxiv.org/html/2502.04223/extracted/6184210/images/im2.png",
                "caption": "",
                "position": 186
            }
        ]
    },
    {
        "header": "2√âCLAIR",
        "images": []
    },
    {
        "header": "3Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04223/extracted/6184210/images/test-set-examples-2x3.png",
                "caption": "Figure 3:Example pages from DROBS, our visually diverse document benchmark.",
                "position": 537
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Supplementary Material",
        "images": []
    },
    {
        "header": "S1Architecture Details",
        "images": []
    },
    {
        "header": "S2Training & Inference",
        "images": []
    },
    {
        "header": "S3Datasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04223/extracted/6184210/images/reading_order_img.png",
                "caption": "Figure S1:Illustrations of reading order over relevant text-like elements, i.e. Text, Section-header, List-item, Title and Formula. Other semantic classes (such as Picture, Footnote and Page-footer in the examples here) are not included in the reading order of the main body. (Note: We are not showing all the classes)",
                "position": 1936
            }
        ]
    },
    {
        "header": "S4Post-processing: Hallucinations and Bad-box detection.",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04223/extracted/6184210/images/hallucination_1.png",
                "caption": "(a)",
                "position": 1966
            },
            {
                "img": "https://arxiv.org/html/2502.04223/extracted/6184210/images/hallucination_1.png",
                "caption": "(a)",
                "position": 1969
            },
            {
                "img": "https://arxiv.org/html/2502.04223/extracted/6184210/images/hallucination_2.png",
                "caption": "(b)",
                "position": 1975
            }
        ]
    },
    {
        "header": "S5Object detection",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04223/extracted/6184210/images/None_confusion_matrix_avg.png",
                "caption": "Figure S3:Confusion matrix for √âCLAIR boxes matched with ground truth on the DocLayNet evaluation dataset, averaged over thresholds ofI‚Å¢o‚Å¢U‚â•{0.5,0.55,‚Ä¶,0.9,0.95}ùêºùëúùëà0.50.55‚Ä¶0.90.95IoU\\geq\\{0.5,0.55,...,0.9,0.95\\}italic_I italic_o italic_U ‚â• { 0.5 , 0.55 , ‚Ä¶ , 0.9 , 0.95 }.",
                "position": 2020
            },
            {
                "img": "https://arxiv.org/html/2502.04223/extracted/6184210/images/PR_Curve_allcat_iou0.5.png",
                "caption": "Figure S4:PR-Curves for individual classes and averaged over all classes forI‚Å¢o‚Å¢U‚â•0.5ùêºùëúùëà0.5IoU\\geq 0.5italic_I italic_o italic_U ‚â• 0.5and 1001 recall-bins evaluated on the DocLayNet evaluation dataset. For √âCLAIR, the scores are taken from the class-token logits. The mean precision and recall are taken fromTab.S1.",
                "position": 2039
            },
            {
                "img": "https://arxiv.org/html/2502.04223/extracted/6184210/images/PR_Curve_catall_iou0.50_areaall_maxDet100.png",
                "caption": "",
                "position": 2043
            }
        ]
    },
    {
        "header": "S6LLM Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04223/extracted/6184210/images/ex2.png",
                "caption": "Figure S5:Examples of pages with tables, formulae and pictures. On the left, predicted bounding boxes superimposed on the original sample image. On the right, the corresponding full predictions.",
                "position": 2313
            },
            {
                "img": "https://arxiv.org/html/2502.04223/extracted/6184210/images/ex1.png",
                "caption": "",
                "position": 2322
            },
            {
                "img": "https://arxiv.org/html/2502.04223/extracted/6184210/images/ex3.png",
                "caption": "",
                "position": 2328
            },
            {
                "img": "https://arxiv.org/html/2502.04223/extracted/6184210/images/ex4.png",
                "caption": "",
                "position": 2333
            }
        ]
    },
    {
        "header": "S7Examples of predictions",
        "images": []
    }
]