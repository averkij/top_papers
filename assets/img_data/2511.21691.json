[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21691/x1.png",
                "caption": "",
                "position": 112
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21691/x2.png",
                "caption": "Figure 2:Overview of Canvas-to-Image framework.(a)Multi-Task Canvas Training.We reformulate heterogeneous control tasks: spatial composition, pose guidance, and layout-constrained generation into a singlecanvas-to-imageformulation. Each training step samples one type of canvas (Spatial, Pose, or Box), where the target frame serves as supervision. All control signals are encoded as RGB canvases interpretable by the Vision-Language Model (VLM) for unified visual–spatial reasoning. The Multi-Modal DiT (MM-DiT) receives VLM embeddings, VAE latents, and noisy latents to predict the velocity for flow matching.\n(b)Inference.Although trained on single-control samples, the model generalizes to multi-control compositions, jointly leveraging pose, layout, and reference cues within a single generation process. This enables coherent multi-control reasoning without task-specific retraining.",
                "position": 168
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21691/x3.png",
                "caption": "Figure 3:Qualitative Comparisons on 4P Composition Benchmark.Under theSpatial Canvassetup, our Canvas-to-Image achieves the highest identity preservation for multi-subject insertion while respecting the spatial placement of each subject segment. FLUX Kontext[fluxkontext]-based approach[ilkerzgi2025overlay]fails to preserve identity, whereas NanoBanana[comanici2025gemini]consistently exhibits copy-pasting artifacts. Compared to our base model, Qwen-Image-Edit[wu2025qwen], our method maintains similar image quality but demonstrates significantly stronger identity preservation.",
                "position": 237
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21691/x4.png",
                "caption": "Figure 4:Qualitative Comparisons on Pose-Overlaid 4P Composition Benchmark.Our Canvas-to-Image achieves the highest identity preservation and most accurate pose alignment.\nNote how Canvas-to-Image closely follows the target poses defined in the prior generated by FLUX-Dev[FLUX](“Pose Prior” column), while maintaining subject identities more faithfully than the baselines.",
                "position": 247
            },
            {
                "img": "https://arxiv.org/html/2511.21691/x5.png",
                "caption": "Figure 5:Qualitative Comparisons on the Layout-Guided Composition Benchmark.Under theBox Canvassetup, our Canvas-to-Image achieves the highest fidelity in spatial layout control, even compared to the state-of-the-art CreatiDesign[zhang2025creatidesign]model trained for this task. Nano Banana[comanici2025gemini], while demonstrating good image quality, does not adhere to the bounding boxes as closely as our model. Compared to our base model Qwen-Image-Edit[wu2025qwen], we achieve the same level of image quality but significantly stronger spatial condition alignment.",
                "position": 276
            },
            {
                "img": "https://arxiv.org/html/2511.21691/x6.png",
                "caption": "Figure 6:Qualitative Comparisons on the Multi-Control Composition Benchmark.We compare Canvas-to-Image with state-of-the-art baselines under inputs containing multiple heterogeneous control signals. Existing methods[comanici2025gemini,wu2025qwen]fail to simultaneously satisfy all conditions, often neglecting spatial, pose, or identity constraints. In contrast, Canvas-to-Image accurately adheres to the bounding boxes for spatial placement, respects pose and interaction cues from overlaid skeletons, and maintains strong identity fidelity of the reference identity images across multi-control inputs.",
                "position": 280
            },
            {
                "img": "https://arxiv.org/html/2511.21691/x7.png",
                "caption": "Figure 7:Qualitative Ablation for Multi-Task Canvas Training.Starting from training with only the Spatial Canvas, the model struggles to follow pose and bounding-box annotations. As we incrementally add the Pose Canvas and Box Canvas tasks, the model progressively learns to respect these additional controls. The final model effectively handles complex multi-control inputs. Notably, during training, each sample contains only a single control type, yet the model exhibits strong generalization to multi-control scenarios at inference.",
                "position": 435
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "AComparisons with Personalization Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21691/fig/src/results/training_dynamics.jpg",
                "caption": "Figure I:Training Dynamics for Canvas-to-Image.The Control-QA score steadily improves during early training and converges around 50K iterations, indicating that the model effectively learns consistent control and composition. We further train up to 200K iterations to refine local details and enhance robustness in generation quality.",
                "position": 544
            },
            {
                "img": "https://arxiv.org/html/2511.21691/x8.png",
                "caption": "Figure II:Supplementary Qualitative Comparisons on the 4P Composition Benchmark with Personalization Approaches.Both Qwen-Image-Edit and our Canvas-to-Image significantly outperform the state-of-the-art multi-subject personalization baselines DreamO[DreamO], OmniGen2[wu2025omnigen2], and UNO[UNO]in terms of identity preservation. Compared to Qwen-Image-Edit, our method demonstrates further improvements in identity fidelity, particularly for the rightmost man in the1s​t1^{st}row, the leftmost woman in the2n​d2^{nd}row, and the second man in the3r​d3^{rd}row.",
                "position": 807
            },
            {
                "img": "https://arxiv.org/html/2511.21691/x9.png",
                "caption": "Figure III:Supplementary Qualitative Comparisons on the Pose-Overlaid 4P Composition Benchmark.We additionally include the relevant state-of-the-art personalization baseline, ID-Patch[ID-Patch], in our comparison. While ID-Patch follows poses to some extent, it performs significantly worse in identity preservation and image quality. In contrast, image editing baselines fail to accurately follow the target pose. Our Canvas-to-Image achieves both strong identity preservation and precise pose alignment.",
                "position": 810
            },
            {
                "img": "https://arxiv.org/html/2511.21691/x10.png",
                "caption": "Figure IV:Qualitative Results on the ID–Object Composition Benchmark.Our Canvas-to-Image generates coherent compositions that faithfully preserve both human identity and object fidelity, maintaining correct proportions and natural interactions between them. In contrast, existing baselines often fail to achieve realistic integration between the human and the object. For instance, the baseline Qwen-Image-Edit[wu2025qwen]fails to preserve both identity and object consistency, as illustrated in these examples.",
                "position": 813
            },
            {
                "img": "https://arxiv.org/html/2511.21691/x11.png",
                "caption": "Figure V:Qualitative Results on Pose-Overlaid 2P Composition.Under thePose Canvassetup, our Canvas-to-Image achieves superior identity preservation and accurate pose alignment. Notably, Canvas-to-Image closely follows the target poses defined by the prior generated from FLUX-Dev[FLUX](“Pose Prior” column), while producing coherent and high-quality images.",
                "position": 816
            },
            {
                "img": "https://arxiv.org/html/2511.21691/x12.png",
                "caption": "Figure VI:Qualitative Results on Pose-Overlaid 1P Composition.Under thePose Canvassetup, our Canvas-to-Image again achieves superior identity preservation and accurate pose alignment.",
                "position": 819
            }
        ]
    },
    {
        "header": "BSupplementary Ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21691/x13.png",
                "caption": "Figure VII:Qualitative Ablations on the Task Indicator.We visualize the impact of removing the task indicator prompt (cc) in training. Without this explicit signal, the model suffers from task mix-up, where the 4P Composition (Spatial Canvas) is impacted by the Box Canvas task. This results in unwanted text artifacts appearing in the background, as the model incorrectly transfers the text-rendering behavior required only in box-canvas settings to a spatial composition benchmark that does not require text rendering.",
                "position": 929
            },
            {
                "img": "https://arxiv.org/html/2511.21691/x14.png",
                "caption": "Figure VIII:Background-Aware Composition with Canvas-to-Image.Given a background image, Canvas-to-Image seamlessly integrates humans or objects into the scene through reference image pasting or bounding box annotations, producing natural spatial alignment and consistent lighting with the surrounding environment.",
                "position": 932
            }
        ]
    },
    {
        "header": "CLimitations",
        "images": []
    },
    {
        "header": "DAdditional Applications",
        "images": []
    },
    {
        "header": "EUser Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21691/fig/src/results/user-study-1-inst.png",
                "caption": "Figure IX:User Instructions for User Study “Control Following”.",
                "position": 1086
            },
            {
                "img": "https://arxiv.org/html/2511.21691/fig/src/results/user-study-q1.png",
                "caption": "Figure X:Sample Question for User Study “Control Following”.",
                "position": 1089
            },
            {
                "img": "https://arxiv.org/html/2511.21691/fig/src/results/user-study-2-instr.png",
                "caption": "Figure XI:User Instructions for User Study “Identity Preservation”.",
                "position": 1092
            },
            {
                "img": "https://arxiv.org/html/2511.21691/fig/src/results/user-study-q2.png",
                "caption": "Figure XII:Sample Question for User Study “Identity Preservation”.",
                "position": 1095
            }
        ]
    },
    {
        "header": "FBenchmark Details",
        "images": []
    }
]