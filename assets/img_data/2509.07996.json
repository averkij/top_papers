[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.07996/x1.png",
                "caption": "Figure 1:Outline of the survey.This work focuses onnative 3D and 4D representations: video streams, occupancy grids, and LiDAR point clouds, guided by geometric (ùíûgeo\\mathcal{C}_{\\mathrm{geo}}), action-based (ùíûact\\mathcal{C}_{\\mathrm{act}}), and semantic (ùíûsem\\mathcal{C}_{\\mathrm{sem}}) conditions (Sec.2). Methods are framed under two paradigms,generative(synthesis from observations and conditions) andpredictive(forecasting from history and actions), and grouped into four functional types (Sec.3). We cover three modality tracks and standardize evaluations (Sec.4), applications (Sec.5), and future trends (Sec.6) across diverse generation, forecasting, and downstream tasks.",
                "position": 179
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.07996/x2.png",
                "caption": "Figure 2:Summary of representative video-based generation(VideoGen), occupancy-based generation(OccGen), and LiDAR-based generation(LiDARGen)models from existing literature. For the complete list of related methods and discussions on their specifications, configurations, and technical details, kindly refer to Sec.3.1, Sec.3.2, and Sec.3.3, respectively.",
                "position": 252
            },
            {
                "img": "https://arxiv.org/html/2509.07996/x3.png",
                "caption": "Figure 3:Summary of existingdatasets and benchmarksused for training and evaluatingVideoGen,OccGen, andLiDARGenmodels. For detailed dataset configurations and statistics, kindly refer to TableV. Images adopted from the original papers.",
                "position": 255
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons_condition/camera-pose.png",
                "caption": "TABLE I:Summary of the rich collection of conditions used by existingVideoGen,OccGen, andLiDARGenmodels. The conditions are categorized intothreemain groups: geometric conditions, action-based conditions, and semantic conditions.",
                "position": 351
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons/number-1.png",
                "caption": "",
                "position": 378
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons_condition/depth-map.png",
                "caption": "",
                "position": 385
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons_condition/bev-map.png",
                "caption": "",
                "position": 400
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons/number-2.png",
                "caption": "",
                "position": 408
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons/number-3.png",
                "caption": "",
                "position": 408
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons_condition/hd-map.png",
                "caption": "",
                "position": 416
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons_condition/3d-bounding-box.png",
                "caption": "",
                "position": 433
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons_condition/flow-field.png",
                "caption": "",
                "position": 449
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons_condition/past-occupancy.png",
                "caption": "",
                "position": 466
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons_condition/lidar-pattern.png",
                "caption": "",
                "position": 482
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons_condition/object-coordinate.png",
                "caption": "",
                "position": 497
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons_condition/partial-point-cloud.png",
                "caption": "",
                "position": 513
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons_condition/rgb-frame.png",
                "caption": "",
                "position": 528
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons_condition/surface-mesh.png",
                "caption": "",
                "position": 544
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons_condition/ego-trajectory.png",
                "caption": "",
                "position": 561
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons_condition/ego-velocity.png",
                "caption": "",
                "position": 577
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons_condition/ego-acceleration.png",
                "caption": "",
                "position": 594
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons_condition/ego-steering.png",
                "caption": "",
                "position": 608
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons_condition/ego-command.png",
                "caption": "",
                "position": 623
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons_condition/route-plan.png",
                "caption": "",
                "position": 639
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons_condition/action-token.png",
                "caption": "",
                "position": 654
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons_condition/scan-path.png",
                "caption": "",
                "position": 668
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons_condition/semantic-mask.png",
                "caption": "",
                "position": 683
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons_condition/text-prompt.png",
                "caption": "",
                "position": 699
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons_condition/scene-graph.png",
                "caption": "",
                "position": 716
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons_condition/object-label.png",
                "caption": "",
                "position": 732
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons_condition/weather-tag.png",
                "caption": "",
                "position": 749
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons_condition/material-tag.png",
                "caption": "",
                "position": 765
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons/video-data-engine.png",
                "caption": "",
                "position": 789
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons/video-action-interpreter.png",
                "caption": "",
                "position": 809
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons/video-neural-simulator.png",
                "caption": "",
                "position": 829
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons/video-scene-reconstructor.png",
                "caption": "",
                "position": 849
            }
        ]
    },
    {
        "header": "3Methods: A Hierarchical Taxonomy",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.07996/x4.png",
                "caption": "Figure 4:The categorization ofVideoGenmodels based on functionalities, including data engines (Sec.3.1.1), action interpreters (Sec.3.1.2), and neural simulators (Sec.3.1.3).",
                "position": 958
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons/latent.png",
                "caption": "TABLE II:Summary of video-based generation (VideoGen) models.‚àô\\bulletDatasets:NnuScenes[10],KKITTI[11],WWaymo Open[95],YOpenDV-YouTube[96],AArgoverse 2[97],NnuPlan[98],NNAVSIM[99],CCARLA[100], andPPrivate (Internal) Data.‚àô\\bulletInput & Output:Noise Latent,Video (Single-View and/or Multi-View), andEgo-Action.‚àô\\bulletArchitectures (Arch.):AR: Autoregressive Models,MLLM: Multimodal Large Language Models,SD: Stable Diffusion Models,DiT: Diffusion Transformer,GPT: Generative Pre-trained Transformer.‚àô\\bulletTasks:VG: Video Generation,E2E: End-to-End Planning, and3SR: 3D Scene Reconstruction.‚àô\\bulletCategories:Data Engine (Sec.3.1.1),Action Interpreter (Sec.3.1.2), andNeural Simulator (Sec.3.1.3).",
                "position": 968
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons/action.png",
                "caption": "",
                "position": 1005
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons/latent.png",
                "caption": "TABLE III:Summary of occupancy-based generation (OccGen) models.‚àô\\bulletDatasets:SSemanticKITTI[16],CCarlaSC[163],NOcc3D-nuScenes[14],WWaymo Open[95],LLyft-Level5[164],AArgoverse 2[97],3KITTI-360[165],UNYUv2[9],\nandOOpenCOOD[166].‚àô\\bulletInput & Output:Noise Latent,Latent Codebook,Images,3D Occ,4D Occ, andEgo-Action.‚àô\\bulletArchitectures (Arch.):Enc-Dec: Encoder-Decoder,LDM: Latent Diffusion Model,MSSM: Memory State-Space Model,AR: Autoregressive Model,DiT: Diffusion Transformer,LLM: Large Language Model.‚àô\\bulletTasks:O3G: 3D Occupancy Generation,O4G: 4D Occupancy Generation,OF: 4D Occupancy Forecasting,PT: Pre-Training,SSC: Semantic Scene Completion, andE2E: End-to-End Planning.‚àô\\bulletCategories:Scene Representor (Sec.3.2.1),Occ Forecaster\n(Sec.3.2.2), andAutoregressive Simulator (Sec.3.2.3).",
                "position": 2958
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons/latent_codebook.png",
                "caption": "",
                "position": 3001
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons/occ3d.png",
                "caption": "",
                "position": 3002
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons/occ4d.png",
                "caption": "",
                "position": 3002
            },
            {
                "img": "https://arxiv.org/html/2509.07996/x5.png",
                "caption": "Figure 5:The categorization ofOccGenmodels based on functionalities, including scene representors (Sec.3.2.1), forecasters (Sec.3.2.2), and autoregressive simulators (Sec.3.2.3).",
                "position": 4061
            },
            {
                "img": "https://arxiv.org/html/2509.07996/x6.png",
                "caption": "Figure 6:The categorization ofLiDARGenmodels based on functionalities, including data engines (Sec.3.3.1), action forecasters (Sec.3.3.2), and autoregressive simulators (Sec.3.3.3).",
                "position": 4116
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons/latent.png",
                "caption": "TABLE IV:Summary of LiDAR-based generation (LiDARGen) models.‚àô\\bulletDatasets:KKITTI[11],SSemanticKITTI[16],NnuScenes[10],3KITTI-360[165],PPandaSet[217]CCarla[100],SSeeingThroughFog[218],WWaymo Open[95],NNAVSIM[99],AArgoverse 2[97]andOOmniDrive-nuScenes[219].‚àô\\bulletInput & Output:Noisy Latent,Latent Codebook,Noisy LiDAR Point Cloud,LiDAR Point Cloud,LiDAR Sequence, andImages/Videos (Single-View and/or Multi-View).‚àô\\bulletArchitectures (Arch.):GAN: Generative Adversarial Network,Enc-Dec: Encoder-Decoder,LDM: Latent Diffusion Model,AR: Autoregressive Model,DiT: Diffusion Transformer,LLM: Large Language Model.‚àô\\bulletTasks:LG: LiDAR Generation,L4G: 4D LiDAR Generation,SEG: 3D Semantic Segmentation,DET: 3D Object Detection,SC: Scene Completion,OP: Occupancy Prediction, andE2E: End-to-End Planning.‚àô\\bulletCategories:Data Engine (Sec.3.3.1),Action Forecaster\n(Sec.3.3.2), andAutoregressive Simulator (Sec.3.3.3).",
                "position": 4119
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons/lidar_noisy.png",
                "caption": "",
                "position": 4163
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons/lidar.png",
                "caption": "",
                "position": 4163
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons/lidar4d.png",
                "caption": "",
                "position": 4164
            }
        ]
    },
    {
        "header": "4Datasets & Evaluations",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons/video.png",
                "caption": "TABLE V:Summary of datasets and benchmarks used for trainingVideoGen,OccGen, andLiDARGenmodels.‚àô\\bulletColumn Keys:#= Total number of frames;#= Total number of occupancy scenes;#= Total number of LiDAR scenes;Freq= Annotation frequency;\nSymbol ‚Äú‚Äì‚Äù in a cell indicates the information is not provided.‚àô\\bulletTasked by:Video Generation Models (VideoGen,cf.Sec.3.1),Occupancy Generation Models (OccGen,cf.Sec.3.2), andLiDAR Generation Models (LiDARGen,cf.Sec.3.3). Kindly refer to TableIfor the definitions of conditions.",
                "position": 5167
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/logos/kitti.png",
                "caption": "",
                "position": 5212
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/logos/nyu.png",
                "caption": "",
                "position": 5243
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/logos/carla.png",
                "caption": "",
                "position": 5269
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/logos/boon.jpg",
                "caption": "",
                "position": 5298
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/logos/motional.png",
                "caption": "",
                "position": 5326
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/logos/waymo.png",
                "caption": "",
                "position": 5366
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/logos/argoverse.png",
                "caption": "",
                "position": 5460
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/logos/lyft.png",
                "caption": "",
                "position": 5491
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/logos/pandaset.png",
                "caption": "",
                "position": 5543
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/logos/ucla.jpg",
                "caption": "",
                "position": 5578
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/logos/tubingen.png",
                "caption": "",
                "position": 5604
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/logos/robo3d.png",
                "caption": "",
                "position": 5671
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/logos/navsim.png",
                "caption": "",
                "position": 5807
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/logos/nvidia.png",
                "caption": "",
                "position": 5860
            },
            {
                "img": "https://arxiv.org/html/2509.07996/figures/logos/nus.png",
                "caption": "",
                "position": 5915
            },
            {
                "img": "https://arxiv.org/html/2509.07996/x7.png",
                "caption": "Figure 7:Qualitative comparisons of state-of-the-artVideoGenmodels on the nuScenes[10]dataset. From top to bottom rows: Reference (from the dataset), MagicDrive[20], DreamForge[105], DriveDreamer-2[87], and OpenDWM[236].",
                "position": 6789
            },
            {
                "img": "https://arxiv.org/html/2509.07996/x8.png",
                "caption": "Figure 8:Qualitative comparisons of state-of-the-artVideoGenmodels on the nuScenes[10]dataset. From top to bottom rows: Reference (from the dataset), MagicDrive[20], DreamForge[105], DriveDreamer-2[87], and OpenDWM[236].",
                "position": 6792
            },
            {
                "img": "https://arxiv.org/html/2509.07996/x9.png",
                "caption": "Figure 9:Qualitative examples ofOccGenmodels on the nuScenes[10]dataset. From left to right columns: The input condition, the generated multi-view videos, and the generated occupancy grids. The results are generated usingùí≥\\mathcal{X}-Scene[198].",
                "position": 7362
            },
            {
                "img": "https://arxiv.org/html/2509.07996/x10.png",
                "caption": "Figure 10:Qualitative comparisons of state-of-the-artLiDARGenmodels on the nuScenes[10]dataset. From left to right columns: Reference (from the dataset), OpenDWM[236], UniScene[77], and LiDARCrafter[49].",
                "position": 7374
            }
        ]
    },
    {
        "header": "5Applications",
        "images": []
    },
    {
        "header": "6Challenges & Future Directions",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.07996/figures/icons/metrics.png",
                "caption": "TABLE XIV:Summary of the evaluation metrics used for evaluating the quality of1Generation,2Forecasting,3Planning,4Reconstruction, and5Downstream Tasksfor theVideoGen,OccGen, andLiDARGenmodels in 2D, 3D, and 4D tasks.",
                "position": 7639
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]