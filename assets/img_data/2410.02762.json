[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02762/x1.png",
                "caption": "Figure 1:Interpreting VLM internal image representations. (a) Given a VLM, (b) we unembed the latent representations from image embeddings to the vocabulary and classify hallucinations. We remove hallucinations by (c) linearly editing them out of the latent representations.",
                "position": 109
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Extracting Knowledge from VLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02762/x2.png",
                "caption": "Figure 2:Comparison of internal confidence in objects present and not present in the image. We examine the internal confidence of COCO objects that exist and do not exist in the image within intermediate VLM image representations. We observe that objects that do not exist in the image have lower internal confidence.",
                "position": 198
            },
            {
                "img": "https://arxiv.org/html/2410.02762/x3.png",
                "caption": "Figure 3:Localizing objects using internal confidence values. We find the probabilities of objects through layers of the language model for every image embedding in LLaVA. We use the highest layer probability per image embedding to localize an object within the image.",
                "position": 201
            }
        ]
    },
    {
        "header": "4Erasing knowledge from VLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02762/x4.png",
                "caption": "Figure 5:Qualitative results for mass object removal.We present example images and their captions after mass-removing hallucinations (red) withProjectAway., which can effectively remove hallucinations while preserving, even increasing, correctly detected objects (green).",
                "position": 373
            },
            {
                "img": "https://arxiv.org/html/2410.02762/x5.png",
                "caption": "Figure 6:Hidden layer ablationsfor LLaVA. We track hallucination reduction (%) across different layers to edit at and extract latent embeddings for the text embedding, crossing out (red) parameters from consideration where there is a decrease in correctly detected objects.",
                "position": 401
            },
            {
                "img": "https://arxiv.org/html/2410.02762/x6.png",
                "caption": "",
                "position": 415
            }
        ]
    },
    {
        "header": "5Applications",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02762/x7.png",
                "caption": "Figure 8:Object Presence Classification Curves for InstructBLIP and LLaVA.We show the Precision-Recall and ROC curves of our confidence measure for present object-hallucination classification on the COCO training subset. Classifying object presence with the internal confidence outperforms the baseline, indicating that the language modelâ€™s image representations know which objects are hallucinations and which are truly present.",
                "position": 448
            },
            {
                "img": "https://arxiv.org/html/2410.02762/x8.png",
                "caption": "Figure 9:Zero-shot segmentation.Warmer areas indicate higher internal confidence for the class at that image patch. We binarize these values with a threshold to generate segmentations.",
                "position": 555
            }
        ]
    },
    {
        "header": "6Discussion and limitations",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02762/x9.png",
                "caption": "Figure 10:Hidden layer ablations for InstructBLIP. We track hallucination reduction (%) across different layers to edit at and extract latent embeddings for the text embedding, crossing out (red) parameters from consideration where there is a decrease in correctly detected objects.",
                "position": 1423
            },
            {
                "img": "https://arxiv.org/html/2410.02762/x10.png",
                "caption": "",
                "position": 1437
            },
            {
                "img": "https://arxiv.org/html/2410.02762/x11.png",
                "caption": "Figure 12:LLaVA Object Presence Classification. Sample image captions from LLaVA and the internal confidence scores for objects in the caption used for classification as correctly detected objects or hallucinations.",
                "position": 1510
            },
            {
                "img": "https://arxiv.org/html/2410.02762/x12.png",
                "caption": "Figure 13:InstructBLIP Object Presence Classification.",
                "position": 1513
            },
            {
                "img": "https://arxiv.org/html/2410.02762/x13.png",
                "caption": "Figure 14:Qualitative results for LLaVA hallucination intervention. Our algorithm removes hallucinations and, at times, adds correctly detected objects.",
                "position": 1516
            },
            {
                "img": "https://arxiv.org/html/2410.02762/x14.png",
                "caption": "Figure 15:Qualitative results for InstructBLIP hallucination intervention.",
                "position": 1519
            },
            {
                "img": "https://arxiv.org/html/2410.02762/x15.png",
                "caption": "Figure 16:Object Localization Samples.",
                "position": 1522
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]