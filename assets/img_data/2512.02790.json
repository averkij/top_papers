[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02790/figures/logo_hf.png",
                "caption": "",
                "position": 143
            },
            {
                "img": "https://arxiv.org/html/2512.02790/x1.png",
                "caption": "Figure 1:UnicEdit-10Mcovers 22 edit tasks spanning basic and complex edits, with a unified post-verification stage that filters failures and refines instructions to yield high-quality triplets. We also introduceUnicBenchwith fine-grained metrics for comprehensive evaluation.",
                "position": 150
            },
            {
                "img": "https://arxiv.org/html/2512.02790/x2.png",
                "caption": "Figure 2:Representative examples of all sub-tasks fromUnicEdit-10M.",
                "position": 153
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02790/x3.png",
                "caption": "Figure 3:Data curation pipeline with three stages: (1) data preparation, (2) image editing, (3) post verification performing failed edits filtration and recaption.",
                "position": 318
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02790/x4.png",
                "caption": "Figure 4:Post-verification examples of the expert model.Basedenotes Qwen2.5-VL-7B;SFTdenotes Base model after Stage-1 SFT;Oursdenotes the dual-task expert modelQwen-Verify.",
                "position": 337
            }
        ]
    },
    {
        "header": "3UnicEdit-10M Dataset",
        "images": []
    },
    {
        "header": "4Qwen-Verify",
        "images": []
    },
    {
        "header": "5UnicBench",
        "images": []
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02790/x5.png",
                "caption": "Table 4:Overall performance of different model on UnicBench. The performance of open-source and closed-source models is separately marked with the best performance inbold, and the second bestunderlined.",
                "position": 707
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADataset Taxonomy",
        "images": []
    },
    {
        "header": "Appendix BData Pipeline Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02790/x5.png",
                "caption": "Figure 6:Examples ofNo Editfiltration. The left side shows four examples with detection results from SSIM[39]and Qwen-Verify. The top-left example has a clear edit (red dashed box) but receives a high SSIM[39]score, while the other three visually unchanged pairs receive lower scores. The right side provides a quantitative comparison, confirming that Qwen-Verify performs best at identifying failed edits.",
                "position": 1833
            }
        ]
    },
    {
        "header": "Appendix CExpert Model Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02790/x6.png",
                "caption": "Figure 7:Examples of our evaluation protocol on two complex edit cases. For each example, the edited image pair and instruction are shown on the left. The right side displays the scores for four evaluation dimensions, along with the detailed reasons from the VLM evaluator, which specifies points of credit and deduction.",
                "position": 1880
            }
        ]
    },
    {
        "header": "Appendix DValidation of Benchmark Metrics",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02790/x7.png",
                "caption": "Figure 8:Comparison between GEdit-Bench’s[23]metrics and UnicBench’s metrics. (a) compares scoring for a case with an unexpected removal. (b) compares scoring for a case with an unexpected text change.",
                "position": 2031
            },
            {
                "img": "https://arxiv.org/html/2512.02790/x8.png",
                "caption": "Figure 9:Qualitative comparison of facial consistency. (a) shows examples from GPT-Image-Edit-1.5M[38]. (b) shows examples from UnicEdit-10M.",
                "position": 2034
            }
        ]
    },
    {
        "header": "Appendix EAnalysis of Noisy Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02790/x9.png",
                "caption": "Figure 10:Overall score of each model on the sub-tasks in UnicBench, for EN (left) and CN (right) instructions. All results are evaluated byGPT-4.1.",
                "position": 2078
            },
            {
                "img": "https://arxiv.org/html/2512.02790/x10.png",
                "caption": "Table 9:Detailed performance across different editing tasks (EN). The performance of open-source and closed-source models is separately marked with the best performance inbold, and the second bestunderlined. All results are evaluated byGPT-4.1.",
                "position": 2081
            }
        ]
    },
    {
        "header": "Appendix FAnalysis of Facial Consistency",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02790/x10.png",
                "caption": "Figure 11:Performance of four evaluation dimensions for each sub-task. The top row shows results for EN tasks, and the bottom row shows results for CN tasks. All results are evaluated byGPT-4.1.",
                "position": 2490
            },
            {
                "img": "https://arxiv.org/html/2512.02790/x11.png",
                "caption": "Table 10:Detailed performance across different editing tasks (CN). The performance of open-source and closed-source models is separately marked with the best performance inbold, and the second bestunderlined. All results are evaluated byGPT-4.1.",
                "position": 2493
            }
        ]
    },
    {
        "header": "Appendix GAnalysis of Benchmark Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02790/x11.png",
                "caption": "Table 11:Detailed performance across different editing tasks (EN). The performance of open-source and closed-source models is separately marked with the best performance inbold, and the second bestunderlined. All results are evaluated byQwen2.5-VL-72B[1].",
                "position": 2771
            },
            {
                "img": "https://arxiv.org/html/2512.02790/x11.png",
                "caption": "Table 12:Detailed performance across different editing tasks (CN). The performance of open-source and closed-source models is separately marked with the best performance inbold, and the second bestunderlined. All results are evaluated byQwen2.5-VL-72B[1].",
                "position": 3173
            }
        ]
    },
    {
        "header": "Appendix HComparison with Other Benchmarks",
        "images": []
    },
    {
        "header": "Appendix IBenchmark Cases",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02790/x11.png",
                "caption": "Figure 12:Qualitative results for Attribute Editing tasks on UnicBench (EN).",
                "position": 3477
            },
            {
                "img": "https://arxiv.org/html/2512.02790/x12.png",
                "caption": "Figure 13:Qualitative results for Object Editing tasks on UnicBench (EN).",
                "position": 3480
            },
            {
                "img": "https://arxiv.org/html/2512.02790/x13.png",
                "caption": "Figure 14:Qualitative results for Scene Editing tasks on UnicBench (EN).",
                "position": 3483
            },
            {
                "img": "https://arxiv.org/html/2512.02790/x14.png",
                "caption": "Figure 15:Qualitative results for Reasoning Editing tasks on UnicBench (EN).",
                "position": 3486
            }
        ]
    },
    {
        "header": "Appendix JEvaluation Prompts",
        "images": []
    }
]