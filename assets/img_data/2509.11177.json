[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.11177/Figures/HF.png",
                "caption": "",
                "position": 59
            },
            {
                "img": "https://arxiv.org/html/2509.11177/Figures/github.png",
                "caption": "",
                "position": 60
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.11177/x1.png",
                "caption": "Figure 1:(a) Single compression techniques(Sun et¬†al.,2023; Ashkboos et¬†al.,2024)rapidly reaches limits under sub-4 bits while the joint counterpart can enable further compression. To enable a unified comparison in a single figure, pruning is represented using equivalent bit-widths. (b) INT4 + 2:4 sparse GEMM can achieve faster inference speed, higher throughput, and lower memory usage.",
                "position": 81
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Motivation",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.11177/x2.png",
                "caption": "Figure 2:Given original LLM weightsùêñ\\mathbf{W}, we first apply a rotation to smooth out outliers, followed by pruning to introduce sparsity. The proposed OBR is employed to compute optimal compensation, which is added to the unpruned elements to mitigate the conflict between pruning and quantization. Finally, quantization is applied to obtain the sparse and quantized LLM weights.",
                "position": 120
            }
        ]
    },
    {
        "header": "4Optimal Brain Restoration",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.11177/x3.png",
                "caption": "Figure 3:(a) Given a Hessian approximationùêá\\mathbf{H}, we extract the submatricesùêáR‚ÄãR\\mathbf{H}_{RR}andùêáR‚ÄãE\\mathbf{H}_{RE}based on the index setsRRandEE. (b) The rotated dense weights are partitioned intoR1R_{1}andE1E_{1}according to the binary pruning mask, followed by OBR to transfer information fromùê∞E1\\mathbf{w}_{E_{1}}toùê∞R1\\mathbf{w}_{R_{1}}. (c) The unpruned index setR1R_{1}is further divided into two groups: the firstŒ±\\alphafraction assigned to setE2E_{2}, the remaining1‚àíŒ±1-\\alphato setR2R_{2}. OBR is used to compensate for quantization error inE2E_{2}.",
                "position": 239
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.11177/x4.png",
                "caption": "Figure 4:Comparison on runtime, FLOPs, and TOPS across different sequence lengths. We evaluate the performance of FP16-Dense, INT4-Dense, and INT4 2:4 Sparse GEMM on a single NVIDIA A100-SXM4-80GB GPU. The GEMM computation follows a typical LLM inference setting, where the weight matrix isùêñ‚àà‚Ñù4096√ó4096\\mathbf{W}\\in\\mathbb{R}^{4096\\times 4096}and the input activation isùêó‚àà‚Ñù32√ós‚Äãe‚Äãq‚Äã_‚Äãl‚Äãe‚Äãn√ó4096\\mathbf{X}\\in\\mathbb{R}^{32\\times seq\\_len\\times 4096}.",
                "position": 855
            },
            {
                "img": "https://arxiv.org/html/2509.11177/x5.png",
                "caption": "Table 4:Comparison on 4:8 and 2:4 sparsity with Llama2-7B models. The included baselines have all been quantized using QuaRot W4A4KV4 configuration.",
                "position": 1065
            },
            {
                "img": "https://arxiv.org/html/2509.11177/x5.png",
                "caption": "Figure 5:Applying the proposed OBR to WANDA(Sun et¬†al.,2023)pruning algorithm in single compression tasks.",
                "position": 1145
            },
            {
                "img": "https://arxiv.org/html/2509.11177/x6.png",
                "caption": "Figure 6:Distribution visualization of different stages in the OBR framework. The weight matrix is taken from thelayer.0.q_projlayer from the Llama2-7B model. Due to the row-wise decoupling design in OBR, we visualize the distribution of the first row here and give full matrix visualization inAppendixD. Thexx-axis represents theCi‚ÄãnC_{in}channel index, and theyy-axis denotes the absolute value of weight elements.",
                "position": 1290
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ASummary of OBR Algorithm",
        "images": []
    },
    {
        "header": "Appendix BCoexistence of Quantization and Pruning.",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.11177/x7.png",
                "caption": "Figure 7:Distribution of layer-wise sparsity across LLMs under different rotation methods. All models are quantized with W4A4KV4 RTN quantizer.",
                "position": 1958
            }
        ]
    },
    {
        "header": "Appendix CMore Experiments",
        "images": []
    },
    {
        "header": "Appendix DMore Visualization",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.11177/x8.png",
                "caption": "Figure 8:Visualization of the full weight matrix at different stages in the proposed OBR pipeline. Thexx-axis corresponds to theCi‚ÄãnC_{in}dimension, and theyy-axis is theCo‚Äãu‚ÄãtC_{out}dimension. The weight matrix is taken from thelayer.0.q_projlayer from the Llama2-7B model, and absolute values are used to enhance visual clarity.",
                "position": 2756
            }
        ]
    },
    {
        "header": "Appendix ELimitation and Future Work",
        "images": []
    }
]