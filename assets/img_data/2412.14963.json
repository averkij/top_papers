[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14963/x1.png",
                "caption": "",
                "position": 135
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14963/x2.png",
                "caption": "Figure 2:Pipeline for constructing ourHuGe100K. Diverse attribute combinations from GPT-4 templates create text prompts, generating synthetic images via FLUX, combined with real images from DeepFashion. SMPL-X fitting produces multi-view pose sequences with 360-degree rotations and diverse animatable motions. MVChamp then converts these sequences into multi-view images, ensuring 3D consistency in the dataset.",
                "position": 333
            }
        ]
    },
    {
        "header": "3Dataset Creation",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14963/x3.png",
                "caption": "Figure 3:A paired example from the proposedHuGe100KDataset.For each reference image, we generate a set of multi-view images using an estimated shape and a specific pose. The figure shows the pose is well-aligned.",
                "position": 369
            },
            {
                "img": "https://arxiv.org/html/2412.14963/x4.png",
                "caption": "Figure 4:The architecture ofIDOL, a full-differentiable transformer-based framework for reconstructing animatable 3D human from a single image.\nThe model integrates a high-resolution (1024×1024102410241024\\times 10241024 × 1024) encoder[29]and fuses image tokens with learnable UV tokens through the UV-Alignment Transformer.\nA UV Decoder predicts Gaussian attribute maps as intermediate representations, capturing the human’s geometry and appearance in a structured 2D UV space defined by the SMPL-X model.\nThese maps, in conjunction with the SMPL-X model, represent a 3D human avatar in a canonical space, which can be animated using linear blend skinning (LBS). The model is optimized using multi-view images with diverse poses and identities, learning to disentangle pose, appearance, and shape.",
                "position": 380
            }
        ]
    },
    {
        "header": "4Large Human Reconstruction Model",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14963/x5.png",
                "caption": "Figure 5:Qualitative results of our MVChamp ablation study (left) and comparison experiment (right).",
                "position": 491
            },
            {
                "img": "https://arxiv.org/html/2412.14963/x6.png",
                "caption": "Figure 6:Comparisons on (a) the upper: novel-view synthesis given a single image, and (b) the lower: our animated results.",
                "position": 523
            },
            {
                "img": "https://arxiv.org/html/2412.14963/x7.png",
                "caption": "Figure 7:Qualitative Results of Ablation Study of IDOL.",
                "position": 561
            },
            {
                "img": "https://arxiv.org/html/2412.14963/x8.png",
                "caption": "Figure 8:Controllable Avatar Editing: (a) texture editing; (b) body shape editing.",
                "position": 611
            }
        ]
    },
    {
        "header": "6Conclusions and Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14963/x9.png",
                "caption": "Figure 9:The visualization of the reenactment.",
                "position": 1634
            }
        ]
    },
    {
        "header": "Appendix AMore Details ofHuGe100KData",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14963/extracted/6078595/fig/supp_dataset/fig_representation.png",
                "caption": "Figure 10:Visualization of different approaches for 3D human reconstruction. Unlike PIFu, which directly predicts the 3D human without a parametric prior, and GTA/SIFU, which relies on computationally expensive loop optimization for SMPL alignment, our IDOL method leverages SMPL-X as a prior. This enables more robust and accurate reconstruction while avoiding the pitfalls of error accumulation. Furthermore, our method supports direct animation and editing, enabling additional applications in digital content creation.",
                "position": 1770
            },
            {
                "img": "https://arxiv.org/html/2412.14963/extracted/6078595/fig/supp_dataset/flux_collections.jpg",
                "caption": "Figure 11:Visualization of diverse images generated by Flux[18].",
                "position": 1773
            },
            {
                "img": "https://arxiv.org/html/2412.14963/extracted/6078595/fig/supp_dataset/1.jpg",
                "caption": "Figure 12:Visualization of examples fromHuGe100K, where the images are generated by Flux and used to generate multi-view images.",
                "position": 1776
            },
            {
                "img": "https://arxiv.org/html/2412.14963/extracted/6078595/fig/supp_dataset/2.jpg",
                "caption": "",
                "position": 1780
            },
            {
                "img": "https://arxiv.org/html/2412.14963/extracted/6078595/fig/supp_dataset/3.jpg",
                "caption": "",
                "position": 1782
            },
            {
                "img": "https://arxiv.org/html/2412.14963/extracted/6078595/fig/supp_dataset/4.jpg",
                "caption": "",
                "position": 1784
            },
            {
                "img": "https://arxiv.org/html/2412.14963/extracted/6078595/fig/supp_dataset/5.jpg",
                "caption": "",
                "position": 1786
            },
            {
                "img": "https://arxiv.org/html/2412.14963/extracted/6078595/fig/supp_dataset/6.jpg",
                "caption": "",
                "position": 1788
            },
            {
                "img": "https://arxiv.org/html/2412.14963/extracted/6078595/fig/supp_dataset/7.jpg",
                "caption": "Figure 13:Visualization of examples fromHuGe100K, where the images are derived from the DeepFashion[38]dataset and used to generate multi-view images.",
                "position": 1792
            },
            {
                "img": "https://arxiv.org/html/2412.14963/extracted/6078595/fig/supp_dataset/8.jpg",
                "caption": "",
                "position": 1796
            },
            {
                "img": "https://arxiv.org/html/2412.14963/extracted/6078595/fig/supp_dataset/9.jpg",
                "caption": "",
                "position": 1798
            },
            {
                "img": "https://arxiv.org/html/2412.14963/extracted/6078595/fig/supp_dataset/10.jpg",
                "caption": "",
                "position": 1800
            },
            {
                "img": "https://arxiv.org/html/2412.14963/extracted/6078595/fig/supp_dataset/11.jpg",
                "caption": "",
                "position": 1802
            },
            {
                "img": "https://arxiv.org/html/2412.14963/extracted/6078595/fig/supp_dataset/12.jpg",
                "caption": "",
                "position": 1804
            }
        ]
    },
    {
        "header": "Appendix BMore Details ofIDOL",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14963/extracted/6078595/fig/supp_dataset/fig_visual_comparison.png",
                "caption": "Figure 14:More visualization for comparison in the in-the-wild cases. We compare with the reported results by HumanLRM[57].",
                "position": 1885
            },
            {
                "img": "https://arxiv.org/html/2412.14963/extracted/6078595/fig/supp_dataset/fig_visual_comp_sgd.jpg",
                "caption": "Figure 15:More visualization for comparison in the in-the-wild cases. We compare with the reported results by HumanSGD[2].",
                "position": 1888
            }
        ]
    },
    {
        "header": "Appendix CExperiment",
        "images": []
    }
]