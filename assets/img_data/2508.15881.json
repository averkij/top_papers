[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15881/x1.png",
                "caption": "Figure 1:Comparison of MLA, GLA, and TPLA. In MLA, each device must load the entire KV cache. In GLA, each attention head only accesses the portion of the KV cache stored on its own device. In TPLA, the prefilling phase follows MLA for efficiency and accuracy, while during the decoding phase, attention heads are distributed across devices, each relying on the KV cache stored locally on its assigned device.",
                "position": 108
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Preliminary",
        "images": []
    },
    {
        "header": "4Tensor Parallel Latent Attention (TPLA)",
        "images": []
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15881/x2.png",
                "caption": "Figure 2:Accuracy across multiple benchmarks under different tensor-parallelism methods (indicated by colors) and reparameterization strategies (indicated by textures).\nThe purple horizontal line marks the original DeepSeek-V2-Lite accuracy, and the vertical bars show each methodâ€™s accuracy drop relative to this MLA baseline.TPLA (norm only)parallelizes RMSNorm across two devices, followed by anallgatherbefore the softmax.TPLA (softmax only)applies RMSNorm normally and parallelizes the softmax.TPLAparallelizes both RMSNorm and softmax.Originalsplits parameters evenly;Hadamardbalances parts prior to splitting;PCAconcentrates information into earlier dimensions before splitting.\nTo better visualize method-induced loss, TPLA results are reported without PD separation in this figure.",
                "position": 1807
            },
            {
                "img": "https://arxiv.org/html/2508.15881/x3.png",
                "caption": "(a)H800 for DeepSeek-V3-0324",
                "position": 1853
            },
            {
                "img": "https://arxiv.org/html/2508.15881/x3.png",
                "caption": "(a)H800 for DeepSeek-V3-0324",
                "position": 1856
            },
            {
                "img": "https://arxiv.org/html/2508.15881/x4.png",
                "caption": "(b)H800 for Kimi-K2-Base",
                "position": 1861
            },
            {
                "img": "https://arxiv.org/html/2508.15881/x5.png",
                "caption": "(a)H800 for DeepSeek-V3-0324",
                "position": 1868
            },
            {
                "img": "https://arxiv.org/html/2508.15881/x5.png",
                "caption": "(a)H800 for DeepSeek-V3-0324",
                "position": 1871
            },
            {
                "img": "https://arxiv.org/html/2508.15881/x6.png",
                "caption": "(b)H800 for Kimi-K2-Base",
                "position": 1876
            }
        ]
    },
    {
        "header": "6Conclusion, Limitation and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]