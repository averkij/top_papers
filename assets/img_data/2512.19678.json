[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19678/x1.png",
                "caption": "",
                "position": 106
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19678/x2.png",
                "caption": "Figure 2:Training pipeline of our ST-Diff model.1) Spatially temporally-varying noisy latent:The process begins by rendering a warped image and a validity mask from an RGB point cloud (images are shown for illustration, as operations are in latent space). The warped image is encoded to getùê≥s‚Üít\\mathbf{z}_{s\\to t}, and the ground-truth image is encoded to getùê≥t\\mathbf{z}_{t}. A \"clean composite\" latentùê≥c,t\\mathbf{z}_{c,t}is created by combining the valid warped regions fromùê≥s‚Üít\\mathbf{z}_{s\\to t}with the blank regions fromùê≥t\\mathbf{z}_{t}, using the downsampled maskùêålatent\\mathbf{M}_{\\text{latent}}.2) Training ST-Diffusion:This composite latent sequence is noised according to our spatio-temporal schedule, resulting in a noisy latent sequence (visualized as a stack) where the noise level for each latent varies across different frames and spatial regions. The resulting noisy latents are fed into our modelùëÆŒ∏\\bm{G}_{\\theta}, which is trained to predict the target velocity (defined asœµt‚àíùê≥t\\mathbf{\\epsilon}_{t}-\\mathbf{z}_{t}), forcing it to learn the flow from the noisy composite latent back towards the original ground-truth latent sequenceùíµ\\mathcal{Z}.",
                "position": 179
            }
        ]
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19678/x3.png",
                "caption": "Figure 3:The autoregressive inference pipeline of WorldWarp.At each iterationkk, the available history (either the initial images or the previously generatedk‚àí1k-1chunk) is processed.\nFirst, TTT3R estimates camera poses and an initial 3D point cloud.\nThis geometry is used to optimize a 3D Gaussian Splatting (3DGS) representation, which serves as a high-fidelity 3D cache.\nConcurrently, a VLM generates a descriptive text prompt, and novel camera poses are extrapolated for the next chunk.\nThe optimized 3DGS renders forward-warped images at these new poses.\nThese warped priors, along with the VLM prompt, are fed into our non-causal ST-Diff model (GŒ∏G_{\\theta}) to denoise and generate thekk-th chunk of novel views.\nThe process then repeats, using the newly generated chunk as the history for the next iteration.",
                "position": 306
            },
            {
                "img": "https://arxiv.org/html/2512.19678/x4.png",
                "caption": "Figure 4:Qualitative comparisons on the RealEstate10K[re10k]and DL3DV[dl3dv]datasets.We visualize videos generated by our method against those by GenWarp[genwarp], CameraCtrl[cameractrl], and VMem[vmem]. Our WorldWarp generalizes to diverse camera motion, showcasing the spatial and temporal consistency.",
                "position": 344
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19678/x5.png",
                "caption": "Figure 5:Illustration of the ST-Diff‚Äôs generating process.We illustrate the GT images, the warped images which serve as the condition for ST-Diff, the corresponding validity mask, and our final generated frames.\nThe comparisons show that our ST-Diff successfully fills in the blank areas (initialized from a full noise level) while simultaneously revising distortions and enhancing details in the non-blank regions (initialized from a partial noise level) during the diffusion process.",
                "position": 988
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19678/x6.png",
                "caption": "Figure 6:Qualitative results on the RealEstate10K[re10k]datasets.",
                "position": 1239
            }
        ]
    },
    {
        "header": "7Implementation Details",
        "images": []
    },
    {
        "header": "8More Experiment Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19678/x7.png",
                "caption": "Figure 7:Qualitative results on the DL3DV[dl3dv]datasets.",
                "position": 1267
            },
            {
                "img": "https://arxiv.org/html/2512.19678/x8.png",
                "caption": "Figure 8:Generalization to Out-of-Distribution Artistic Styles.We evaluate the robustness of ST-Diff by generating video sequences conditioned on diverse artistic text prompts (e.g., ‚ÄúVan Gogh style‚Äù, ‚ÄúStudio Ghibli style‚Äù).\nThe visualized chunks demonstrate that our method successfully synthesizes high-quality stylized content while maintaining rigorous3D geometric consistencyacross the autoregressive generation.\nThis confirms that our geometry-aware fine-tuning strategy effectively incorporates structural control without compromising the semantic and aesthetic generalization capabilities of the pre-trained diffusion backbone.",
                "position": 1270
            },
            {
                "img": "https://arxiv.org/html/2512.19678/x9.png",
                "caption": "Figure 9:Visualization of the Spatially-Adaptive Noise Schedule.We visualize the schedule matrixùö∫ùí±\\mathbf{\\Sigma}_{\\mathcal{V}}across the reverse diffusion process. The horizontal axis represents the progression of denoising steps (fromT=999T=999to0), while the vertical axis corresponds to the sequence of temporal tokens (13 tokens derived from 49 frames). The top two rows represent theHistory Tokens(context), which are enforced as hard constraints (dark purple,œÉ=0\\sigma=0). The subsequent rows represent theGenerated Tokens, where the noise levels are spatially modulated: (1)Valid warped regionsare held at a reduced noise levelœÑ\\tau(intermediate green) to preserve explicit geometry; and (2)Occluded regionsare initialized with maximal noise (yellow,œÉ‚âà1.0\\sigma\\approx 1.0) to enable the generative synthesis of novel content.",
                "position": 1276
            }
        ]
    },
    {
        "header": "9Limitations",
        "images": []
    }
]