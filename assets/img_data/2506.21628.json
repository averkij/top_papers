[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "Framework Overview",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21628/x1.png",
                "caption": "Figure 1:Ark uses a unified configuration file to define action and observation channels, which are then instantiated within a distributed node-based network. This architecture supports both real and simulated hardware through interchangeable drivers and identical communication interfaces. The Ark Registry manages active nodes, while each component (e.g., sensors, actuators, policies) operates as an independent process. As a result, pipelines developed in simulation can also be used on physical systems without code modification, ensuring a consistent sim-real interface.",
                "position": 151
            },
            {
                "img": "https://arxiv.org/html/2506.21628/",
                "caption": "Figure 2:Technical diagram illustrating how Ark uses a unified configuration file to instantiate a distributed simulated system that mirrors real-world deployments. The YAML-based configuration specifies robots, sensors, environments, and networking parameters, which the Ark Simulator parses to launch corresponding simulated nodes. Each component, such as robot controllers, cameras, and sensor emulators, runs as an independent process, communicating through the same message passing protocol used in real deployments. This ensures that policies and pipelines developed in simulation operate identically when transferred to physical hardware, facilitating seamless sim-to-real transitions and reproducible experimentation",
                "position": 284
            },
            {
                "img": "https://arxiv.org/html/2506.21628/x3.png",
                "caption": "Figure 3:Graphical debugging tools provided by Ark. Ark Graph displays active nodes and their communication channels for network analysis. Ark Viewer renders live image streams to support camera calibration and inspection. Ark Plot visualizes real-time numerical data on any channel, aiding in system monitoring and debugging.",
                "position": 356
            }
        ]
    },
    {
        "header": "Use Cases",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21628/x4.png",
                "caption": "Figure 4:Seamless sim-real transition in Ark enabled by consistent observation and action space definitions. The environment configuration specifies sensor inputs (e.g., joint states, images) and actuator outputs (e.g., joint commands), which remain identical across both simulated and real systems. By toggling a single flag (sim = True/False), Ark automatically routes data through the different observation and action channels, allowing a single policy pipeline implementation to operate in both domains without modification. This unified interface simplifies development, debugging, and deployment across the sim-real boundary.",
                "position": 401
            },
            {
                "img": "https://arxiv.org/html/2506.21628/x5.png",
                "caption": "Figure 5:lcm-logger enables efficient data collection for imitation learning by recording demonstrations from a variety of control interfaces, including kinesthetic teaching, VR teleoperation, and gamepad input (left). Each demonstrations is saved as a separate CSV file (right), allowing users to accumulate diverse datasets across different input modalities rapidly.",
                "position": 431
            },
            {
                "img": "https://arxiv.org/html/2506.21628/x6.png",
                "caption": "Figure 6:Sequential snapshots of a Viper X 300 s arm executing a learned diffusion policy. The line overlay shows the sampled action trajectories converging toward the target object, while the arm autonomously refines its actions at each timestep.",
                "position": 499
            },
            {
                "img": "https://arxiv.org/html/2506.21628/x7.png",
                "caption": "Figure 7:Execution of an ACT-based policy on the OpenPyro humanoid robot for two distinct tasks: cloth manipulation (top row) and object handover (bottom row). The policy produces precise, contact-rich behaviors that enable the robot to flatten a shirt and place a banana into a bowl.",
                "position": 534
            },
            {
                "img": "https://arxiv.org/html/2506.21628/x8.png",
                "caption": "Figure 8:Using Ark’s integrated SLAM and visualization tools, a Husky robot equipped with a LiDAR sensor navigates a kitchen environment. The robot first constructs an occupancy map using FastSLAM (center), facilitated by Ark’s modular data streaming and map-building nodes. The final panel (right) shows an A* path generated within the occupancy map using Ark’s planning and rendering utilities. This setup demonstrates how Ark enables end-to-end navigation workflows—from sensor integration and SLAM to path planning and visualization—within a unified framework.",
                "position": 562
            },
            {
                "img": "https://arxiv.org/html/2506.21628/x9.png",
                "caption": "Figure 9:Deep Seek integration with Ark Framework to allow the Viper to play board games",
                "position": 607
            }
        ]
    },
    {
        "header": "Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]